[
    {
        "title": "4.1 Users can withdraw their funds immediately when they are over-leveraged ",
        "body": "  Description  Accounts.withdraw makes two checks before processing a withdrawal.  First, the method checks that the amount requested for withdrawal is not larger than the user s balance for the asset in question:  code/contracts/Accounts.sol:L197-L201  function withdraw(address _accountAddr, address _token, uint256 _amount) external onlyAuthorized returns(uint256) {  // Check if withdraw amount is less than user's balance  require(_amount <= getDepositBalanceCurrent(_token, _accountAddr), \"Insufficient balance.\");  uint256 borrowLTV = globalConfig.tokenInfoRegistry().getBorrowLTV(_token);  Second, the method checks that the withdrawal will not over-leverage the user. The amount to be withdrawn is subtracted from the user s current  borrow power  at the current price. If the user s total value borrowed exceeds this new borrow power, the method fails, as the user no longer has sufficient collateral to support their borrow positions. However, this require is only checked if a user is not already over-leveraged:  code/contracts/Accounts.sol:L203-L211  // This if condition is to deal with the withdraw of collateral token in liquidation.  // As the amount if borrowed asset is already large than the borrow power, we don't  // have to check the condition here.  if(getBorrowETH(_accountAddr) <= getBorrowPower(_accountAddr))  require(  getBorrowETH(_accountAddr) <= getBorrowPower(_accountAddr).sub(  _amount.mul(globalConfig.tokenInfoRegistry().priceFromAddress(_token))  .mul(borrowLTV).div(Utils.getDivisor(address(globalConfig), _token)).div(100)  ), \"Insufficient collateral when withdraw.\");  If the user has already borrowed more than their  borrow power  allows, they are allowed to withdraw regardless. This case may arise in several circumstances; the most common being price fluctuation.  Recommendation  Disallow withdrawals if the user is already over-leveraged.  From the comment included in the code sample above, this condition is included to support the liquidate method, but its inclusion creates an attack vector that may allow users to withdraw when they should not be able to do so. Consider adding an additional method to support liquidate, so that users may not exit without repaying debts.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/02/definer/"
    },
    {
        "title": "4.2 Users can borrow funds, deposit them, then borrow more   ",
        "body": "  Resolution  Comment from DeFiner team:  This is expected behaviour and our contracts are designed like that. Other lending protocols like Compound and AAVE allows this feature as well. So this is not a CRITICAL issue, as the user s funds are not at risk. The funds of the users are only at risk when their position is over-leveraged, which is expected behaviour.  Description  Users may deposit and borrow funds denominated in any asset supported by the TokenRegistry. Each time a user deposits or borrows a token, they earn FIN according to the difference in deposit / borrow rate indices maintained by Bank.  Borrowing funds  When users borrow funds, they may only borrow up to a certain amount: the user s  borrow power.  As long as the user is not requesting to borrow an amount that would cause their resulting borrowed asset value to exceed their available borrow power, the borrow is successful and the user receives the assets immediately. A user s borrow power is calculated in the following function:  code/contracts/Accounts.sol:L333-L353  /**  Calculate an account's borrow power based on token's LTV  /  function getBorrowPower(address _borrower) public view returns (uint256 power) {  for(uint8 i = 0; i < globalConfig.tokenInfoRegistry().getCoinLength(); i++) {  if (isUserHasDeposits(_borrower, i)) {  address token = globalConfig.tokenInfoRegistry().addressFromIndex(i);  uint divisor = INT_UNIT;  if(token != ETH_ADDR) {  divisor = 10**uint256(globalConfig.tokenInfoRegistry().getTokenDecimals(token));  // globalConfig.bank().newRateIndexCheckpoint(token);  power = power.add(getDepositBalanceCurrent(token, _borrower)  .mul(globalConfig.tokenInfoRegistry().priceFromIndex(i))  .mul(globalConfig.tokenInfoRegistry().getBorrowLTV(token)).div(100)  .div(divisor)  );  return power;  For each asset, borrow power is calculated from the user s deposit size, multiplied by the current chainlink price, multiplied and that asset s  borrow LTV.   Depositing borrowed funds  After a user borrows tokens, they can then deposit those tokens, increasing their deposit balance for that asset. As a result, their borrow power increases, which allows the user to borrow again.  By continuing to borrow, deposit, and borrow again, the user can repeatedly borrow assets. Essentially, this creates positions for the user where the collateral for their massive borrow position is entirely made up of borrowed assets.  Conclusion  There are several potential side-effects of this behavior.  First, as described in issue 4.6, the system is comprised of many different tokens, each of which is subject to price fluctuation. By borrowing and depositing repeatedly, a user may establish positions across all supported tokens. At this point, if price fluctuations cause the user s account to cross the liquidation threshold, their positions can be liquidated.  Liquidation is a complicated function of the protocol, but in essence, the liquidator purchases a target s collateral at a discount, and the resulting sale balances the account somewhat. However, when a user repeatedly deposits borrowed tokens, their collateral is made up of borrowed tokens: the system s liquidity! As a result, this may allow an attacker to intentionally create a massively over-leveraged account on purpose, liquidate it, and exit with a chunk of the system liquidity.  Another potential problem with this behavior is FIN token mining. When users borrow and deposit, they earn FIN according to the size of the deposit / borrow, and the difference in deposit / borrow rate indices since the last deposit / borrow. By repeatedly depositing / borrowing, users are able to artificially deposit and borrow far more often than normal, which may allow them to generate FIN tokens at will. This additional strategy may make attacks like the one described above much more economically feasible.  Recommendation  Due to the limited time available during this engagement, these possibilities and potential mitigations were not fully explored. Definer is encouraged to investigate this behavior more carefully.  ",
        "labels": [
            "Consensys",
            "Major",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/02/definer/"
    },
    {
        "title": "4.3 Stale Oracle prices might affect the rates ",
        "body": "  Description  It s possible that due to network congestion or other reasons, the price that the ChainLink oracle returns is old and not up to date. This is more extreme in lesser known tokens that have fewer ChainLink Price feeds to update the price frequently. The codebase as is, relies on chainLink().getLatestAnswer() and does not check the timestamp of the price.  Examples  /contracts/registry/TokenRegistry.sol#L291-L296  function priceFromAddress(address tokenAddress) public view returns(uint256) {  if(Utils._isETH(address(globalConfig), tokenAddress)) {  return 1e18;  return uint256(globalConfig.chainLink().getLatestAnswer(tokenAddress));  Recommendation  Do a sanity check on the price returned from the oracle. If the price is older than a threshold, revert or handle in other means.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/02/definer/"
    },
    {
        "title": "4.4 Overcomplicated unit conversions ",
        "body": "  Description  There are many instances of unit conversion in the system that are implemented in a confusing way. This could result in mistakes in the conversion and possibly failure in correct accounting. It s been seen in the ecosystem that these type of complicated unit conversions could result in calculation mistake and loss of funds.  Examples  Here are a few examples:  /contracts/Bank.sol#L216-L224  function getBorrowRatePerBlock(address _token) public view returns(uint) {  if(!globalConfig.tokenInfoRegistry().isSupportedOnCompound(_token))  // If the token is NOT supported by the third party, borrowing rate = 3% + U * 15%.  return getCapitalUtilizationRatio(_token).mul(globalConfig.rateCurveSlope()).div(INT_UNIT).add(globalConfig.rateCurveConstant()).div(BLOCKS_PER_YEAR);  // if the token is suppored in third party, borrowing rate = Compound Supply Rate * 0.4 + Compound Borrow Rate * 0.6  return (compoundPool[_token].depositRatePerBlock).mul(globalConfig.compoundSupplyRateWeights()).  add((compoundPool[_token].borrowRatePerBlock).mul(globalConfig.compoundBorrowRateWeights())).div(10);  /contracts/Bank.sol#L350-L351  compoundPool[_token].depositRatePerBlock = cTokenExchangeRate.mul(UNIT).div(lastCTokenExchangeRate[cToken])  .sub(UNIT).div(blockNumber.sub(lastCheckpoint[_token]));  /contracts/Bank.sol#L384-L385  return lastDepositeRateIndex.mul(getBlockNumber().sub(lcp).mul(depositRatePerBlock).add(INT_UNIT)).div(INT_UNIT);  Recommendation  Simplify the unit conversions in the system. This can be done either by using a function wrapper for units to convert all values to the same unit before including them in any calculation or by better documenting every line of unit conversion  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/02/definer/"
    },
    {
        "title": "4.5 Commented out code in the codebase ",
        "body": "  Description  There are many instances of code lines (and functions) that are commented out in the code base. Having commented out code increases the cognitive load on an already complex system. Also, it hides the important parts of the system that should get the proper attention, but that attention gets to be diluted.  The main problem is that commented code adds confusion with no real benefit. Code should be code, and comments should be comments.  Examples  Here s a few examples of such lines of code, note that there are more.  /contracts/SavingAccount.sol#L211-L218  struct LiquidationVars {  // address token;  // uint256 tokenPrice;  // uint256 coinValue;  uint256 borrowerCollateralValue;  // uint256 tokenAmount;  // uint256 tokenDivisor;  uint256 msgTotalBorrow;  contracts/Accounts.sol#L341-L345  if(token != ETH_ADDR) {  divisor = 10**uint256(globalConfig.tokenInfoRegistry().getTokenDecimals(token));  // globalConfig.bank().newRateIndexCheckpoint(token);  power = power.add(getDepositBalanceCurrent(token, _borrower)  Many usage of console.log() and also the commented import on most of the contracts  // import \"@nomiclabs/buidler/console.sol\";  ...  //console.log(\"tokenNum\", tokenNum);  /contracts/Accounts.sol#L426-L429  // require(  //     totalBorrow.mul(100) <= totalCollateral.mul(liquidationDiscountRatio),  //     \"Collateral is not sufficient to be liquidated.\"  // );  /contracts/registry/TokenRegistry.sol#L298-L306  // function _isETH(address _token) public view returns (bool) {  //     return globalConfig.constants().ETH_ADDR() == _token;  // }  // function getDivisor(address _token) public view returns (uint256) {  //     if(_isETH(_token)) return INT_UNIT;  //     return 10 ** uint256(getTokenDecimals(_token));  // }  /contracts/registry/TokenRegistry.sol#L118-L121  // require(_borrowLTV != 0, \"Borrow LTV is zero\");  require(_borrowLTV < SCALE, \"Borrow LTV must be less than Scale\");  // require(liquidationThreshold > _borrowLTV, \"Liquidation threshold must be greater than Borrow LTV\");  Recommendation  In many of the above examples, it s not clear if the commented code is for testing or obsolete code (e.g. in the last example, can _borrowLTV ==0?) . All these instances should be reviewed and the system should be fully tested for all edge cases after the code changes.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/02/definer/"
    },
    {
        "title": "4.6 Price volatility may compromise system integrity   ",
        "body": "  Resolution  Comment from DeFiner team:  The issue says that due to price volatility there could be an attack on DeFiner. However, price volatility is inherent in the Cryptocurrency ecosystem. All the other lending platforms like MakerDAO, Compound and AAVE also designed like that, in case of price volatility(downside) more liquidation happens on these platforms as well. Liquidations are in a sense good to keep the market stable. If there is no liquidation during those market crash, the system will be at risk. Due to this, it is always recommended to maintain the collateral and borrow ratio by the user. A user should keep checking his risk in the time when the market crashes.  Description  SavingAccount.borrow allows users to borrow funds from the bank. The funds borrowed may be denominated in any asset supported by the system-wide TokenRegistry. Borrowed funds come from the system s existing liquidity: other users  deposits.  Borrowing funds is an instant process. Assuming the user has sufficient collateral to service the borrow request (as well as any existing loans), funds are sent to the user immediately:  code/contracts/SavingAccount.sol:L130-L140  function borrow(address _token, uint256 _amount) external onlySupportedToken(_token) onlyEnabledToken(_token) whenNotPaused nonReentrant {  require(_amount != 0, \"Borrow zero amount of token is not allowed.\");  globalConfig.bank().borrow(msg.sender, _token, _amount);  // Transfer the token on Ethereum  SavingLib.send(globalConfig, _amount, _token);  emit Borrow(_token, msg.sender, _amount);  Users may borrow up to their  borrow power , which is the sum of their deposit balance for each token, multiplied by each token s borrowLTV, multiplied by the token price (queried from a chainlink oracle):  code/contracts/Accounts.sol:L344-L349  // globalConfig.bank().newRateIndexCheckpoint(token);  power = power.add(getDepositBalanceCurrent(token, _borrower)  .mul(globalConfig.tokenInfoRegistry().priceFromIndex(i))  .mul(globalConfig.tokenInfoRegistry().getBorrowLTV(token)).div(100)  .div(divisor)  );  If users borrow funds, their position may be liquidated via SavingAccount.liquidate. An account is considered liquidatable if the total value of borrowed funds exceeds the total value of collateral (multiplied by some liquidation threshold ratio). These values are calculated similarly to  borrow power:  the sum of the deposit balance for each token, multiplied by each token s borrowLTV, multiplied by the token price as determined by chainlink.  Conclusion  The instant-borrow approach, paired with the chainlink oracle represents a single point of failure for the Definer system. When the price of any single supported asset is sufficiently volatile, the entire liquidity held by the system is at risk as borrow power and collateral value become similarly volatile.  Some users may find their borrow power skyrocket and use this inflated value to drain large amounts of system liquidity they have no intention of repaying. Others may find their held collateral tank in value and be subject to sudden liquidations.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/02/definer/"
    },
    {
        "title": "4.7 Emergency withdrawal code present ",
        "body": "  Description  Code and functionality for emergency stop and withdrawal is present in this code base.  Examples  /contracts/lib/SavingLib.sol#L43-L48  // ============================================  // EMERGENCY WITHDRAWAL FUNCTIONS  // Needs to be removed when final version deployed  // ============================================  function emergencyWithdraw(GlobalConfig globalConfig, address _token) public {  address cToken = globalConfig.tokenInfoRegistry().getCToken(_token);  ...  /contracts/SavingAccount.sol#L307-L309  function emergencyWithdraw(address _token) external onlyEmergencyAddress {  SavingLib.emergencyWithdraw(globalConfig, _token);  /contracts/config/Constant.sol#L7-L8  ...  address payable public constant EMERGENCY_ADDR = 0xc04158f7dB6F9c9fFbD5593236a1a3D69F92167c;  ...  Recommendation  To remove the emergency code and fully test all the affected contracts.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/02/definer/"
    },
    {
        "title": "4.8 Accounts contains expensive looping ",
        "body": "  Description  Accounts.getBorrowETH performs multiple external calls to GlobalConfig and TokenRegistry within a for loop:  code/contracts/Accounts.sol:L381-L397  function getBorrowETH(  address _accountAddr  ) public view returns (uint256 borrowETH) {  uint tokenNum = globalConfig.tokenInfoRegistry().getCoinLength();  //console.log(\"tokenNum\", tokenNum);  for(uint i = 0; i < tokenNum; i++) {  if(isUserHasBorrows(_accountAddr, uint8(i))) {  address tokenAddress = globalConfig.tokenInfoRegistry().addressFromIndex(i);  uint divisor = INT_UNIT;  if(tokenAddress != ETH_ADDR) {  divisor = 10 ** uint256(globalConfig.tokenInfoRegistry().getTokenDecimals(tokenAddress));  borrowETH = borrowETH.add(getBorrowBalanceCurrent(tokenAddress, _accountAddr).mul(globalConfig.tokenInfoRegistry().priceFromIndex(i)).div(divisor));  return borrowETH;  The loop also makes additional external calls and delegatecalls from:  TokenRegistry.priceFromIndex:  code/contracts/registry/TokenRegistry.sol:L281-L289  function priceFromIndex(uint index) public view returns(uint256) {  require(index < tokens.length, \"coinIndex must be smaller than the coins length.\");  address tokenAddress = tokens[index];  // Temp fix  if(Utils._isETH(address(globalConfig), tokenAddress)) {  return 1e18;  return uint256(globalConfig.chainLink().getLatestAnswer(tokenAddress));  Accounts.getBorrowBalanceCurrent:  code/contracts/Accounts.sol:L313-L331  function getBorrowBalanceCurrent(  address _token,  address _accountAddr  ) public view returns (uint256 borrowBalance) {  AccountTokenLib.TokenInfo storage tokenInfo = accounts[_accountAddr].tokenInfos[_token];  uint accruedRate;  if(tokenInfo.getBorrowPrincipal() == 0) {  return 0;  } else {  if(globalConfig.bank().borrowRateIndex(_token, tokenInfo.getLastBorrowBlock()) == 0) {  accruedRate = INT_UNIT;  } else {  accruedRate = globalConfig.bank().borrowRateIndexNow(_token)  .mul(INT_UNIT)  .div(globalConfig.bank().borrowRateIndex(_token, tokenInfo.getLastBorrowBlock()));  return tokenInfo.getBorrowBalance(accruedRate);  In a worst case scenario, each iteration may perform a maximum of 25+ calls/delegatecalls. Assuming a maximum tokenNum of 128 (TokenRegistry.MAX_TOKENS), the gas cost for this method may reach upwards of 2 million for external calls alone.  Given that this figure would only be a portion of the total transaction gas cost, getBorrowETH may represent a DoS risk within the Accounts contract.  Recommendation  Avoid for loops unless absolutely necessary  Where possible, consolidate multiple subsequent calls to the same contract to a single call, and store the results of calls in local variables for re-use. For example,  Instead of this:  uint tokenNum = globalConfig.tokenInfoRegistry().getCoinLength();  for(uint i = 0; i < tokenNum; i++) {  if(isUserHasBorrows(_accountAddr, uint8(i))) {  address tokenAddress = globalConfig.tokenInfoRegistry().addressFromIndex(i);  uint divisor = INT_UNIT;  if(tokenAddress != ETH_ADDR) {  divisor = 10 ** uint256(globalConfig.tokenInfoRegistry().getTokenDecimals(tokenAddress));  borrowETH = borrowETH.add(getBorrowBalanceCurrent(tokenAddress, _accountAddr).mul(globalConfig.tokenInfoRegistry().priceFromIndex(i)).div(divisor));  Modify TokenRegistry to support a single call, and cache intermediate results like this:  TokenRegistry registry = globalConfig.tokenInfoRegistry();  uint tokenNum = registry.getCoinLength();  for(uint i = 0; i < tokenNum; i++) {  if(isUserHasBorrows(_accountAddr, uint8(i))) {  // here, getPriceFromIndex(i) performs all of the steps as the code above, but with only 1 ext call  borrowETH = borrowETH.add(getBorrowBalanceCurrent(tokenAddress, _accountAddr).mul(registry.getPriceFromIndex(i)).div(divisor));  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/02/definer/"
    },
    {
        "title": "4.9 Naming inconsistency ",
        "body": "  Description  There are some inconsistencies in the naming of some functions with what they do.  Examples  /contracts/registry/TokenRegistry.sol#L272-L274  function getCoinLength() public view returns (uint256 length) { //@audit-info coin vs token  return tokens.length;  Recommendation  Review the code for the naming inconsistencies.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/02/definer/"
    },
    {
        "title": "5.1 The virtual price may not correspond to the actual price in the pool ",
        "body": "  Description  A Curve pool has a function that returns a  virtual price  of the LP token; this price is resistant to flash-loan attacks and any manipulations in the Curve pool. While this price formula works well in some cases, there may be a significant period when a trade cannot be executed with this price. So the deposit or withdrawal will also be done under another price and will have a different result than the one estimated under the  virtual price .  When depositing into Curve, Brahma is doing it in 2 steps. First, when depositing the user s ETH to the Vault, the user s share is calculated according to the  virtual price . And then, in a different transaction, the funds are deposited into the Curve pool. These funds only consist of ETH, and if the deposit price does not correspond (with 0.3% slippage) to the virtual price, it will revert.  So we have multiple problems here:  If the chosen slippage parameter is very low, the funds will not be deposited/withdrawn for a long time due to reverts.  If the slippage is large enough, the attacker can manipulate the price to steal the slippage. Additionally, because of the 2-steps deposit, the amount of Vault s share minted to the users may not correspond to the LP tokens minted during the second step.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"
    },
    {
        "title": "5.2 ConvexPositionHandler._claimRewards incorrectly calculates amount of LP tokens to unstake ",
        "body": "  Description  ConvexPositionHandler._claimRewards is an internal function that harvests Convex reward tokens and takes the generated yield in ETH out of the Curve pool by calculating the difference in LP token price. To do so, it receives the current share price of the curve LP tokens and compares it to the last one stored in the contract during the last rewards claim. The difference in share price is then multiplied by the LP token balance to get the ETH yield via the yieldEarned variable:  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L293-L300  uint256 currentSharePrice = ethStEthPool.get_virtual_price();  if (currentSharePrice > prevSharePrice) {  // claim any gain on lp token yields  uint256 contractLpTokenBalance = lpToken.balanceOf(address(this));  uint256 totalLpBalance = contractLpTokenBalance +  baseRewardPool.balanceOf(address(this));  uint256 yieldEarned = (currentSharePrice - prevSharePrice) *  totalLpBalance;  However, to receive this ETH yield, LP tokens need to be unstaked from the Convex pool and then converted via the Curve pool. To do this, the contract introduces lpTokenEarned:  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L302  uint256 lpTokenEarned = yieldEarned / NORMALIZATION_FACTOR; // 18 decimal from virtual price  This calculation is incorrect. It uses yieldEarned which is denominated in ETH and simply divides it by the normalization factor to get the correct number of decimals, which still returns back an amount denominated in ETH, whereas an amount denominated in LP tokens should be returned instead.  This could lead to significant accounting issues including losses in the  no-loss  parts of the vault s strategy as 1 LP token is almost always guaranteed to be worth more than 1 ETH. So, when the intention is to withdraw X ETH worth of an LP token, withdrawing X LP tokens will actually withdraw Y ETH worth of an LP token, where Y>X. As a result, less than expected ETH will remain in the Convex handler part of the vault, and the ETH yield will go to the Lyra options, which are much riskier. In the event Lyra options don t work out and there is more ETH withdrawn than expected, there is a possibility that this would result in a loss for the vault.  Recommendation  The fix is straightforward and that is to calculate lpTokenEarned using the currentSharePrice already received from the Curve pool. That way, it is the amount of LP tokens that will be sent to be unwrapped and unstaked from the Convex and Curve pools. This will also take care of the normalization factor.  uint256 lpTokenEarned = yieldEarned / currentSharePrice;  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"
    },
    {
        "title": "5.3 The WETH tokens are not taken into account in the ConvexTradeExecutor.totalFunds function ",
        "body": "  Description  The totalFunds function of every executor should include all the funds that belong to the contract:  code/contracts/ConvexTradeExecutor.sol:L21-L23  function totalFunds() public view override returns (uint256, uint256) {  return ConvexPositionHandler.positionInWantToken();  The ConvexTradeExecutor uses this function for calculations:  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L121-L137  function positionInWantToken()  public  view  override  returns (uint256, uint256)  uint256 stakedLpBalanceInETH,  uint256 lpBalanceInETH,  uint256 ethBalance  ) = _getTotalBalancesInETH(true);  return (  stakedLpBalanceInETH + lpBalanceInETH + ethBalance,  block.number  );  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L337-L365  function _getTotalBalancesInETH(bool useVirtualPrice)  internal  view  returns (  uint256 stakedLpBalance,  uint256 lpTokenBalance,  uint256 ethBalance  uint256 stakedLpBalanceRaw = baseRewardPool.balanceOf(address(this));  uint256 lpTokenBalanceRaw = lpToken.balanceOf(address(this));  uint256 totalLpBalance = stakedLpBalanceRaw + lpTokenBalanceRaw;  // Here, in order to prevent price manipulation attacks via curve pools,  // When getting total position value -> its calculated based on virtual price  // During withdrawal -> calc_withdraw_one_coin() is used to get an actual estimate of ETH received if we were to remove liquidity  // The following checks account for this  uint256 totalLpBalanceInETH = useVirtualPrice  ? _lpTokenValueInETHFromVirtualPrice(totalLpBalance)  : _lpTokenValueInETH(totalLpBalance);  lpTokenBalance = useVirtualPrice  ? _lpTokenValueInETHFromVirtualPrice(lpTokenBalanceRaw)  : _lpTokenValueInETH(lpTokenBalanceRaw);  stakedLpBalance = totalLpBalanceInETH - lpTokenBalance;  ethBalance = address(this).balance;  This function includes ETH balance, LP balance, and staked balance. But WETH balance is not included here. WETH tokens are initially transferred to the contract, and before the withdrawal, the contract also stores WETH.  Recommendation  Include WETH balance into the totalFunds.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"
    },
    {
        "title": "5.4 LyraPositionHandlerL2 inaccurate modifier onlyAuthorized may lead to funds loss if keeper is compromised ",
        "body": "  Description  The LyraPositionHandlerL2 contract is operated either by the L2 keeper or by the L1 LyraPositionHandler via the L2CrossDomainMessenger. This is implemented through the onlyAuthorized modifier:  code/contracts/LyraL2/LyraPositionHandlerL2.sol:L187-L195  modifier onlyAuthorized() {  require(  ((msg.sender == L2CrossDomainMessenger &&  OptimismL2Wrapper.messageSender() == positionHandlerL1) ||  msg.sender == keeper),  \"ONLY_AUTHORIZED\"  );  _;  This is set on:  withdraw()  openPosition()  closePosition()  setSlippage()  deposit()  sweep()  setSocketRegistry()  setKeeper()  Functions 1-3 have a corresponding implementation on the L1 LyraPositionHandler, so they could indeed be called by it with the right parameters. However, 4-8 do not have an implemented way to call them from L1, and this modifier creates an unnecessarily expanded list of authorised entities that can call them.  Additionally, even if their implementation is provided, it needs to be done carefully because msg.sender in their case is going to end up being the L2CrossDomainMessenger. For example, the sweep() function sends any specified token to msg.sender, with the intention likely being that the recipient is under the team s or the governance s control   yet, it will be L2CrossDomainMessenger and the tokens will likely be lost forever instead.  On the other hand, the setKeeper() function would need a way to be called by something other than the keeper because it is intended to change the keeper itself. In the event that the access to the L2 keeper is compromised, and the L1 LyraPositionHandler has no way to call setKeeper() on the LyraPositionHandlerL2, the whole contract and its funds will be compromised as well. So, there needs to be some way to at least call the setKeeper() by something other than the keeper to ensure security of the funds on L2.  Examples  code/contracts/LyraL2/LyraPositionHandlerL2.sol:L153-L184  function closePosition(bool toSettle) public override onlyAuthorized {  LyraController._closePosition(toSettle);  UniswapV3Controller._estimateAndSwap(  false,  LyraController.sUSD.balanceOf(address(this))  );  /*///////////////////////////////////////////////////////////////  MAINTAINANCE FUNCTIONS  //////////////////////////////////////////////////////////////*/  /// @notice Sweep tokens  /// @param _token Address of the token to sweepr  function sweep(address _token) public override onlyAuthorized {  IERC20(_token).transfer(  msg.sender,  IERC20(_token).balanceOf(address(this))  );  /// @notice socket registry setter  /// @param _socketRegistry new address of socket registry  function setSocketRegistry(address _socketRegistry) public onlyAuthorized {  socketRegistry = _socketRegistry;  /// @notice keeper setter  /// @param _keeper new keeper address  function setKeeper(address _keeper) public onlyAuthorized {  keeper = _keeper;  Recommendation  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"
    },
    {
        "title": "5.5 Harvester.harvest swaps have no slippage parameters ",
        "body": "  Description  As part of the vault strategy, all reward tokens for staking in the Convex ETH-stETH pool are claimed and swapped into ETH. The swaps for these tokens are done with no slippage at the moment, i.e. the expected output amount for all of them is given as 0.  In particular, one reward token that is most susceptible to slippage is LDO, and its swap is implemented through the Uniswap router:  code/contracts/ConvexExecutor/Harvester.sol:L142-L155  function _swapLidoForWETH(uint256 amountToSwap) internal {  IUniswapSwapRouter.ExactInputSingleParams  memory params = IUniswapSwapRouter.ExactInputSingleParams({  tokenIn: address(ldo),  tokenOut: address(weth),  fee: UNISWAP_FEE,  recipient: address(this),  deadline: block.timestamp,  amountIn: amountToSwap,  amountOutMinimum: 0,  sqrtPriceLimitX96: 0  });  uniswapRouter.exactInputSingle(params);  The swap is called with amountOutMinimum: 0, meaning that there is no slippage protection in this swap. This could result in a significant loss of yield from this reward as MEV bots could  sandwich  this swap by manipulating the price before this transaction and immediately reversing their action after the transaction, profiting at the expense of our swap. Moreover, the Uniswap pools seem to have low liquidity for the LDO token as opposed to Balancer or Sushiswap, further magnifying slippage issues and susceptibility to frontrunning.  The other two tokens - CVX and CRV - are being swapped through their Curve pools, which have higher liquidity and are less susceptible to slippage. Nonetheless, MEV strategies have been getting more advanced and calling these swaps with 0 as expected output may place these transactions in danger of being frontrun and  sandwiched  as well.  code/contracts/ConvexExecutor/Harvester.sol:L120-L126  if (cvxBalance > 0) {  cvxeth.exchange(1, 0, cvxBalance, 0, false);  // swap CRV to WETH  if (crvBalance > 0) {  crveth.exchange(1, 0, crvBalance, 0, false);  In these calls .exchange , the last 0 is the min_dy argument in the Curve pools swap functions that represents the minimum expected amount of tokens received after the swap, which is 0 in our case.  Recommendation  Introduce some slippage parameters into the swaps.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"
    },
    {
        "title": "5.6 Harvester.rewardTokens doesn t account for LDO tokens ",
        "body": "  Description  As part of the vault s strategy, the reward tokens for participating in Curve s ETH-stETH pool and Convex staking are claimed and swapped for ETH. This is done by having the ConvexPositionHandler contract call the reward claims API from Convex via baseRewardPool.getReward(), which transfers the reward tokens to the handler s address. Then, the tokens are iterated through and sent to the harvester to be swapped from ConvexPositionHandler by getting their list from harvester.rewardTokens() and calling harvester.harvest()  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L274-L290  // get list of tokens to transfer to harvester  address[] memory rewardTokens = harvester.rewardTokens();  //transfer them  uint256 balance;  for (uint256 i = 0; i < rewardTokens.length; i++) {  balance = IERC20(rewardTokens[i]).balanceOf(address(this));  if (balance > 0) {  IERC20(rewardTokens[i]).safeTransfer(  address(harvester),  balance  );  // convert all rewards to WETH  harvester.harvest();  However, harvester.rewardTokens() doesn t have the LDO token s address in its list, so they will not be transferred to the harvester to be swapped.  code/contracts/ConvexExecutor/Harvester.sol:L77-L82  function rewardTokens() external pure override returns (address[] memory) {  address[] memory rewards = new address[](2);  rewards[0] = address(crv);  rewards[1] = address(cvx);  return rewards;  As a result, harvester.harvest() will not be able to execute its _swapLidoForWETH() function since its ldoBalance will be 0. This results in missed rewards and therefore yield for the vault as part of its normal flow.  There is a possible mitigation in the current state of the contract that would require governance to call sweep() on the LDO balance from the BaseTradeExecutor contract (that ConvexPositionHandler inherits) and then transferring those LDO tokens to the harvester contract to perform the swap at a later rewards claim. This, however, requires transactions separate from the intended flow of the system as well as governance intervention.  Recommendation  Add the LDO token address to the rewardTokens() function by adding the following line rewards[2] = address(ldo);  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"
    },
    {
        "title": "5.7 Keeper design complexity ",
        "body": "  Description  The current design of the protocol relies on the keeper being operated correctly in a complex manner. Since the offchain code for the keeper wasn t in scope of this audit, the following is a commentary on the complexity of the keeper operations in the context of the contracts. Keeper logic such as the order of operations and function argument parameters with log querying are some examples where if the keeper doesn t execute them correctly, there may be inconsistencies and issues with accounting of vault shares and vault funds resulting in unexpected behaviour. While it may represent little risk or issues to the current Brahma-fi team as the vault is recently live, the keeper logic and exact steps should be well documented so that public keepers (if and when they are enabled) can execute the logic securely and future iterations of the vault code can account for any intricacies of the keeper logic.  Examples  1. Order of operations: Convex rewards & new depositors profiting at the expense of old depositors  yielded reward tokens. As part of the vault s strategy, the depositors  ETH is provided to Curve and the LP tokens are staked in Convex, which yield rewards such as CRV, CVX, and LDO tokens. As new depositors provide their ETH, the vault shares minted for their deposits will be less compared to old deposits as they account for the increasing value of LP tokens staked in these pools. In other words, if the first depositor provides 1 ETH, then when a new depositor provides 1 ETH much later, the new depositor will get less shares back as the totalVaultFunds() will increase:  code/contracts/Vault.sol:L97-L99  shares = totalSupply() > 0  ? (totalSupply() * amountIn) / totalVaultFunds()  : amountIn;  code/contracts/Vault.sol:L127-L130  function totalVaultFunds() public view returns (uint256) {  return  IERC20(wantToken).balanceOf(address(this)) + totalExecutorFunds();  code/contracts/ConvexTradeExecutor.sol:L21-L23  function totalFunds() public view override returns (uint256, uint256) {  return ConvexPositionHandler.positionInWantToken();  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L121-L137  function positionInWantToken()  public  view  override  returns (uint256, uint256)  uint256 stakedLpBalanceInETH,  uint256 lpBalanceInETH,  uint256 ethBalance  ) = _getTotalBalancesInETH(true);  return (  stakedLpBalanceInETH + lpBalanceInETH + ethBalance,  block.number  );  However, this does not account for the reward tokens yielded throughout that time. From the smart contract logic alone, there is no requirement to first execute the reward token harvest. It is up to the keeper to execute ConvexTradeExecutor.claimRewards in order to claim and swap their rewards into ETH, which only then will be included into the yield in the above ConvexPositionHandler.positionInWantToken function. If this is not done prior to processing new deposits and minting new shares, new depositors would unfairly benefit from the reward tokens  yield that was generated before they deposited but accounted for in the vault funds only after they deposited.  2. Order of operations: closing Lyra options before processing new deposits.  The other part of the vault s strategy is utilising the yield from Convex to purchase options from Lyra on Optimism. While Lyra options are risky and can become worthless in the event of bad trades, only yield is used for them, therefore keeping user deposits  initial value safe. However, their value could also yield significant returns, increasing the overall funds of the vault. Just as with ConvexTradeExecutor, LyraTradeExecutor also has a totalFunds() function that feeds into the vault s totalVaultFunds() function. In Lyra s case, however, it is a manually set value by the keeper that is supposed to represent the value of Lyra L2 options:  code/contracts/LyraTradeExecutor.sol:L42-L53  function totalFunds()  public  view  override  returns (uint256 posValue, uint256 lastUpdatedBlock)  return (  positionInWantToken.posValue +  IERC20(vaultWantToken()).balanceOf(address(this)),  positionInWantToken.lastUpdatedBlock  );  code/contracts/LyraTradeExecutor.sol:L61-L63  function setPosValue(uint256 _posValue) public onlyKeeper {  LyraPositionHandler._setPosValue(_posValue);  code/contracts/LyraExecutor/LyraPositionHandler.sol:L218-L221  function _setPosValue(uint256 _posValue) internal {  positionInWantToken.posValue = _posValue;  positionInWantToken.lastUpdatedBlock = block.number;  Solely from the smart contract logic, there is a possibility that a user deposits when Lyra options are valued high, meaning the total vault funds are high as well, thus decreasing the amount of shares the user would have received if it weren t for the Lyra options  value. Consequently, if after the deposit the Lyra options become worthless, decreasing the total vault funds, the user s newly minted shares will now represent less than what they have deposited.  While this is not currently mitigated by smart contract logic, it may be worked around by the keeper first settling and closing all Lyra options and transferring all their yielded value in ETH, if any, to the Convex trade executor. Only then the keeper would process new deposits and mint new shares. This order of operations is critical to maintain the vault s intended safe strategy of maintaining the user s deposited value, and is dependent entirely on the keeper offchain logic.  Recommendation  Document the exact order of operations, steps, necessary logs and parameters that keepers need to keep track of in order for the vault strategy to succeed.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"
    },
    {
        "title": "5.8 Vault.deposit - Possible front running attack ",
        "body": "  Description  To determine the number of shares to mint to a depositor, (totalSupply() * amountIn) / totalVaultFunds() is used. Potential attackers can spot a call to Vault.deposit and front-run it with a transaction that sends tokens to the contract, causing the victim to receive fewer shares than what he expected.  In case totalVaultFunds() is greater than totalSupply() * amountIn, then the number of shares the depositor receives will be 0, although amountIn of tokens will be still pulled from the depositor s balance.  An attacker with access to enough liquidity and to the mem-pool data can spot a call to Vault.deposit(amountIn, receiver) and front-run it by sending at least totalSupplyBefore * (amountIn - 1) + 1 tokens to the contract . This way, the victim will get 0 shares, but amountIn will still be pulled from its account balance. Now the price for a share is inflated, and all shareholders can redeem this profit using Vault.withdraw.  Recommendation  The specific case that s mentioned in the last paragraph can be mitigated by adding a validation check to Vault.Deposit enforcing that shares > 0. However, it will not solve the general case since the victim can still lose value due to rounding errors. In order to fix that, Vault.Deposit should validate that shares >= amountMin where amountMin is an argument that should be determined by the depositor off-chain.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"
    },
    {
        "title": "5.9 Approving MAX_UINT amount of ERC20 tokens ",
        "body": "  Description  Approving the maximum value of uint256 is a known practice to save gas. However, this pattern was proven to increase the impact of an attack many times in the past, in case the approved contract gets hacked.  Examples  code/contracts/BaseTradeExecutor.sol:L19  IERC20(vaultWantToken()).approve(vault, MAX_INT);  code/contracts/Batcher/Batcher.sol:L48  IERC20(vaultInfo.tokenAddress).approve(vaultAddress, type(uint256).max);  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L106-L112  IERC20(LP_TOKEN).safeApprove(ETH_STETH_POOL, type(uint256).max);  // Approve max LP tokens to convex booster  IERC20(LP_TOKEN).safeApprove(  address(CONVEX_BOOSTER),  type(uint256).max  );  code/contracts/ConvexExecutor/Harvester.sol:L65-L69  crv.safeApprove(address(crveth), type(uint256).max);  // max approve CVX to CVX/ETH pool on curve  cvx.safeApprove(address(cvxeth), type(uint256).max);  // max approve LDO to uniswap swap router  ldo.safeApprove(address(uniswapRouter), type(uint256).max);  code/contracts/LyraL2/LyraPositionHandlerL2.sol:L63-L71  IERC20(wantTokenL2).safeApprove(  address(UniswapV3Controller.uniswapRouter),  type(uint256).max  );  // approve max susd balance to uniV3 router  LyraController.sUSD.safeApprove(  address(UniswapV3Controller.uniswapRouter),  type(uint256).max  );  Recommendation  Consider approving the exact amount that s needed to be transferred, or alternatively, add an external function that allows the revocation of approvals.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"
    },
    {
        "title": "5.10 Batcher.depositFunds may allow for more deposits than vaultInfo.maxAmount ",
        "body": "  Description  As part of a gradual rollout strategy, the Brahma-fi system of contracts has a limit of how much can be deposited into the protocol. This is implemented through the Batcher contract that allows users to deposit into it and keep the amount they have deposited in the depositLedger[recipient] state variable. In order to cap how much is deposited, the user s input amountIn is evaluated within the following statement:  code/contracts/Batcher/Batcher.sol:L109-L116  require(  IERC20(vaultInfo.vaultAddress).totalSupply() +  pendingDeposit -  pendingWithdrawal +  amountIn <=  vaultInfo.maxAmount,  \"MAX_LIMIT_EXCEEDED\"  );  However, while pendingDeposit, amountIn, and vaultInfo.maxAmount are denominated in the vault asset token (WETH in our case), IERC20(vaultInfo.vaultAddress).totalSupply() and pendingWithdrawal represent vault shares tokens, creating potential mismatches in this evaluation.  Recommendation  Consider either documenting this potential discrepancy or keeping track of all deposits in a state variable and using that inside the require statement..  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"
    },
    {
        "title": "5.11 The Deposit and Withdraw event are always emitted with zero amount ",
        "body": "  Description  The events emitted during the deposit or withdraw are supposed to contain the relevant amounts of tokens involved in these actions. But in fact the current balance of the address is used in both cases. These balances will be equal to zero by that time:  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L151-L155  IWETH9(address(wantToken)).withdraw(depositParams._amount);  _convertEthIntoLpToken(address(this).balance);  emit Deposit(address(this).balance);  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L207-L209  IWETH9(address(wantToken)).deposit{value: address(this).balance}();  emit Withdraw(address(this).balance);  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"
    },
    {
        "title": "5.12 BaseTradeExecutor.confirmDeposit | confirmWithdraw - Violation of the  checks-effects-interactions  pattern ",
        "body": "  Description  Both confirmDeposit, confirmWithdraw might be re-entered by the keeper (in case it is a contract), in case the derived contract allows the execution of untrusted code.  Examples  code/contracts/BaseTradeExecutor.sol:L57-L61  function confirmDeposit() public override onlyKeeper {  require(depositStatus.inProcess, \"DEPOSIT_COMPLETED\");  _confirmDeposit();  depositStatus.inProcess = false;  code/contracts/BaseTradeExecutor.sol:L69-L73  function confirmWithdraw() public override onlyKeeper {  require(withdrawalStatus.inProcess, \"WIHDRW_COMPLETED\");  _confirmWithdraw();  withdrawalStatus.inProcess = false;  Recommendation  Although the impact is very limited, it is recommended to implement the  checks-effects-interactions  in both functions.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"
    },
    {
        "title": "5.13 Batcher doesn t work properly with arbitrary tokens ",
        "body": "  Description  The Batcher and the Vault contracts initially operate with ETH and WETH. But the contracts are supposed to be compatible with any other ERC-20 tokens.  For example, in the Batcher.deposit function, there is an option to transfer ETH instead of the token, which should only be happening if the token is WETH. Also, the token is named WETH, but if the intention is to use the Batcher contract with arbitrary tokens token, it should be named differently.  code/contracts/Batcher/Batcher.sol:L89-L100  if (ethSent > 0) {  amountIn = ethSent;  WETH.deposit{value: ethSent}();  /// If no wei sent, use amountIn and transfer WETH from txn sender  else {  IERC20(vaultInfo.tokenAddress).safeTransferFrom(  msg.sender,  address(this),  amountIn  );  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"
    },
    {
        "title": "6.1 EOPBCTemplate - permission documentation inconsistencies    ",
        "body": "  Resolution   Fixed with   aragonone/fundraising@bafe100 by adding the undocumented and deviating permissions to the documentation.  Description  Undocumented  The template documentation provides an overview of the permissions set with the template. The following permissions are set by the template contract but are not documented in the accompanied fundraising/templates/externally_owned_presale_bonding_curve/README.md.  TokenManager  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L220-L221  _createPermissions(_acl, grantees, _fundraisingApps.bondedTokenManager, _fundraisingApps.bondedTokenManager.MINT_ROLE(), _owner);  _acl.createPermission(_fundraisingApps.marketMaker, _fundraisingApps.bondedTokenManager, _fundraisingApps.bondedTokenManager.BURN_ROLE(), _owner);  code/fundraising/templates/externally_owned_presale_bonding_curve/eopbc.yaml:L33-L44  Inconsistent  app: anj-token-manager  role: MINT_ROLE  grantee: market-maker  manager: owner  app: anj-token-manager  role: MINT_ROLE  grantee: presale  manager: owner  app: anj-token-manager  role: BURN_ROLE  grantee: market-maker  manager: owner  Inconsistent  The following permissions are set by the template but are inconsistent to the outline in the documentation:  Controller  owner has the following permissions even though they are documented as not being set https://github.com/ConsenSys/aragonone-presale-audit-2019-11/blob/9ddae8c7fde9dea3af3982b965a441239d81f370/code/fundraising/templates/externally_owned_presale_bonding_curve/README.md#controller.  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L239-L240  _acl.createPermission(_owner, _fundraisingApps.controller, _fundraisingApps.controller.UPDATE_BENEFICIARY_ROLE(), _owner);  _acl.createPermission(_owner, _fundraisingApps.controller, _fundraisingApps.controller.UPDATE_FEES_ROLE(), _owner);  Recommendation  For transparency, all permissions set-up by the template must be documented.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"
    },
    {
        "title": "6.2 EOPBCTemplate - AppId of BalanceRedirectPresale should be different from AragonBlack/Presale namehash to avoid collisions    ",
        "body": "  Resolution   Fixed with   aragonone/fundraising@bafe100 by generating a unique APMNameHash for  Description  The template references the new presale contract with apmNamehash 0x5de9bbdeaf6584c220c7b7f1922383bcd8bbcd4b48832080afd9d5ebf9a04df5. However, this namehash is already used by the aragonBlack/Presale contract. To avoid confusion and collision a unique apmNamehash should be used for this variant of the contract.  Note that the contract that is referenced from an apmNamehash is controlled by the ENS resolver that is configured when deploying the template contract. Using the same namehash for both variants of the contract does not allow a single registry to simultaneously provide both variants of the contract and might lead to confusion as to which application is actually deployed. This also raises the issue that the ENS registry must be verified before actually using the contract as a malicious registry could force the template to deploy potentially malicious applications.  aragonOne/Fundraising:  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L32  bytes32   private constant PRESALE_ID                    = 0x5de9bbdeaf6584c220c7b7f1922383bcd8bbcd4b48832080afd9d5ebf9a04df5;  aragonBlack/Fundraising:  templates/multisig/contracts/FundraisingMultisigTemplate.sol:L35  bytes32   private constant PRESALE_ID             = 0x5de9bbdeaf6584c220c7b7f1922383bcd8bbcd4b48832080afd9d5ebf9a04df5;  bytes32 private constant PRESALE_ID = 0x5de9bbdeaf6584c220c7b7f1922383bcd8bbcd4b48832080afd9d5ebf9a04df5;  Recommendation  Create a new apmNamehash for BalanceRedirectPresale.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"
    },
    {
        "title": "6.3 BalanceRedirectPresale - Presale can be extended indefinitely   ",
        "body": "  Resolution  This issue was addressed with the following statement:  It is a very reasonable concern, but this is the intended behavior. That modification is permissioned and that OPEN_ROLE is going to be held by the Aragon Network Dao, so we expect a reasonable use of it. We may document it and make it clear that this is possible.  Description  The OPEN_ROLE can indefinitely extend the Presale even after users contributed funds to it by adjusting the presale period. The period might be further manipulated to avoid that token trading in the MarketMaker is opened.  code/fundraising/apps/presale/contracts/BalanceRedirectPresale.sol:L136-L138  function setPeriod(uint64 _period) external auth(OPEN_ROLE) {  _setPeriod(_period);  code/fundraising/apps/presale/contracts/BalanceRedirectPresale.sol:L253-L257  function _setPeriod(uint64 _period) internal {  require(_period > 0, ERROR_TIME_PERIOD_ZERO);  require(openDate == 0 || openDate + _period > getTimestamp64(), ERROR_INVALID_TIME_PERIOD);  period = _period;  Recommendation  Do not allow to extend the presale after funds have been contributed to it or only allow period adjustments in State.PENDING.  ",
        "labels": [
            "Consensys",
            "Major",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"
    },
    {
        "title": "6.4 Repository structure - Create a clean repository containing one Aragon Application unless changes are contributed upstream    Deferred",
        "body": "  Resolution  The issue has been deferred pending internal discussion.  Description  The repository is a fork of AragonBlack/fundraising. The main development repository for Aragon Fundraising is the origin repository at AragonBlock. This repository duplicates a state of the upstream repository that can quickly get out of sync and therefore hard to maintain.  It is unclear if both repositories will live side-by-side or if the BalanceRedirectPresale variant is contributed upstream.  Recommendation  In case changes are not planned to be contributed upstream it is recommended to create a clean Aragon Application from scratch removing any unused or duplicated files.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"
    },
    {
        "title": "6.5 BalanceRedirectPresale - Tokens vest during the Presale phase   ",
        "body": "  Resolution  The issue was addressed with the following statement:  This presale version is intended to be used along with the Externally Owned Presale and Bonding Curve Template, which doesn t have a Voting app, therefore contributors doesn t have any voting power. The use case is the deployment of Aragon Network Jurors Token (ANJ) for the Aragon Court, which is not going to be active before the presale starts, so we don t see any potential issue here.  Description  Tokens are directly minted and assigned to contributors during the Presale. While this might not be an issue if the minted token does not give any voting power of some sort in a DAO it can be a problem for scenarios where contributors get stake in return for contributions.  Recommendation  Vest tokens for contributors after the presale finishes. In case this is the expected we suggest to add a note to the documentation to make potential users aware of this behaviour that might have security implications if contributors get stake in return for their investments.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"
    },
    {
        "title": "6.6 BalanceRedirectPresale - setPeriod uint64 overflow in validation check    ",
        "body": "  Resolution   Fixed with   aragonone/fundraising@bafe100 by performing the addition using  Description  code/fundraising/apps/presale/contracts/BalanceRedirectPresale.sol:L253-L257  function _setPeriod(uint64 _period) internal {  require(_period > 0, ERROR_TIME_PERIOD_ZERO);  require(openDate == 0 || openDate + _period > getTimestamp64(), ERROR_INVALID_TIME_PERIOD);  period = _period;  Recommendation  Use SafeMath which is already imported to protect from overflow scenarios.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"
    },
    {
        "title": "6.7 EOPBCTemplate - misleading method names _cacheFundraisingApps and _cacheFundraisingParams    ",
        "body": "  Resolution   Fixed with   aragonone/fundraising@0ce7c72 by renaming the functions.  Description  The methods _cacheFundraisingApps and _cacheFundraisingParams suggest that parameters are cached as state variables in the contract similar to the multi-step deployment contract used for AragonBlack/Fundraising. However, the methods are just returning memory structs.  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L254-L300  function _cacheFundraisingApps(  Agent          _reserve,  Presale        _presale,  MarketMaker    _marketMaker,  Tap            _tap,  Controller     _controller,  TokenManager   _tokenManager  internal  returns (FundraisingApps memory fundraisingApps)  fundraisingApps.reserve            = _reserve;  fundraisingApps.presale            = _presale;  fundraisingApps.marketMaker        = _marketMaker;  fundraisingApps.tap                = _tap;  fundraisingApps.controller         = _controller;  fundraisingApps.bondedTokenManager = _tokenManager;  function _cacheFundraisingParams(  address       _owner,  string        _id,  ERC20         _collateralToken,  MiniMeToken   _bondedToken,  uint64        _period,  uint256       _exchangeRate,  uint64        _openDate,  uint256       _reserveRatio,  uint256       _batchBlocks,  uint256       _slippage  internal  returns (FundraisingParams fundraisingParams)  fundraisingParams = FundraisingParams({  owner:           _owner,  id:              _id,  collateralToken: _collateralToken,  bondedToken:     _bondedToken,  period:          _period,  exchangeRate:    _exchangeRate,  openDate:        _openDate,  reserveRatio:    _reserveRatio,  batchBlocks:     _batchBlocks,  slippage:        _slippage  });  Recommendation  The functions are only called once throughout the deployment process. The structs can therefore be created directly in the main method. Otherwise rename the functions to properly reflect their purpose.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"
    },
    {
        "title": "6.8 EOPBCTemplate - Pool should be Agent or Reserve    ",
        "body": "  Resolution   Fixed with   aragonone/fundraising@bafe100 by replacing  Description  The documentation refers to an non-existent Pool application.  code/fundraising/templates/externally_owned_presale_bonding_curve/README.md:L58-L68  | App  | Permission             | Grantee          | Manager          |  | ---- | ---------------------- | ---------------- | ---------------- |  | Pool | SAFE_EXECUTE           | Owner            | Owner            |  | Pool | ADD_PROTECTED_TOKEN    | Controller       | Owner            |  | Pool | REMOVE_PROTECTED_TOKEN | NULL             | NULL             |  | Pool | EXECUTE                | NULL             | NULL             |  | Pool | DESIGNATE_SIGNER       | NULL             | NULL             |  | Pool | ADD_PRESIGNED_HASH     | NULL             | NULL             |  | Pool | RUN_SCRIPT             | NULL             | NULL             |  | Pool | TRANSFER               | MarketMaker      | Owner            |  Recommendation  Pool should be Agent or Reserve.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"
    },
    {
        "title": "6.9 EOPBCTemplate - inconsistent storage location declaration    ",
        "body": "  Resolution   Fixed with   aragonone/fundraising@bafe100 by adding the missing storage location declaration.  Description  _cacheFundraisingParams() does not explicitly declare the return value memory location.  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L273-L286  function _cacheFundraisingParams(  address       _owner,  string        _id,  ERC20         _collateralToken,  MiniMeToken   _bondedToken,  uint64        _period,  uint256       _exchangeRate,  uint64        _openDate,  uint256       _reserveRatio,  uint256       _batchBlocks,  uint256       _slippage  internal  returns (FundraisingParams fundraisingParams)  _cacheFundraisingApps() explicitly declares to return a copy of the storage struct.  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L254-L271  function _cacheFundraisingApps(  Agent          _reserve,  Presale        _presale,  MarketMaker    _marketMaker,  Tap            _tap,  Controller     _controller,  TokenManager   _tokenManager  internal  returns (FundraisingApps memory fundraisingApps)  fundraisingApps.reserve            = _reserve;  fundraisingApps.presale            = _presale;  fundraisingApps.marketMaker        = _marketMaker;  fundraisingApps.tap                = _tap;  fundraisingApps.controller         = _controller;  fundraisingApps.bondedTokenManager = _tokenManager;  Recommendation  Storage declarations should be consistent.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"
    },
    {
        "title": "6.10 EOPBCTemplate - Keep the template as closely aligned to the audited Company DAO-Template provided by Aragon    ",
        "body": "  Resolution   The issue was addressed with   aragonone/fundraising@bafe100 changing the main deployment method from  Description  The EOPBCTemplate is a simplified variant of the AragonBlack/FundraisingMultisigTemplate. The FundraisingMultisigTemplate is initially based on the Aragon/DAO-templates/company-board template.  Please note that the DAO-templates provided by Aragon have recently been audited.  The EOPBCTemplate is similar to the setup established with Aragon/DAO-templates/company. The scenario deploys in one step. However, interface names are different to the audited DAO-template variant (installFundraisingApps vs newInstance). We recommend the template and interface names to be kept as close as possible to the audited company template which established the entry point for deploying a one-step template as newInstance.  Recommendation  Take the Aragon/DAO-templates/company template as a starting point and add relevant parts for the presale variant.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"
    },
    {
        "title": "6.11 EOPBCTemplate - EtherTokenConstant is never used    ",
        "body": "  Resolution   Fixed with   aragonone/fundraising@bafe100 by removing the  Description  The constant value EtherTokenConstant.ETH is never used.  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L3  import \"@aragon/os/contracts/common/EtherTokenConstant.sol\";  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L21  contract EOPBCTemplate is EtherTokenConstant, BaseTemplate {  Recommendation  Remove all references to EtherTokenConstant.  7 Tool-Based Analysis  Several tools were used to perform an automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"
    },
    {
        "title": "7.1 MythX",
        "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"
    },
    {
        "title": "7.2 Ethlint",
        "body": "  Ethlint is an open-source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  $ solium --version  Solium version 1.2.5  $ solium -d contracts  contracts/EOPBCTemplate.sol  118:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  208:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  221:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  224:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  231:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  232:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  233:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  234:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  235:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  236:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  237:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  265:8     warning    Assignment operator must have exactly single space on both sides of it.                                   operator-whitespace  266:8     warning    Assignment operator must have exactly single space on both sides of it.                                   operator-whitespace  267:8     warning    Assignment operator must have exactly single space on both sides of it.                                   operator-whitespace  268:8     warning    Assignment operator must have exactly single space on both sides of it.                                   operator-whitespace  269:8     warning    Assignment operator must have exactly single space on both sides of it.                                   operator-whitespace  289:13    warning    Name 'owner': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.           whitespace  290:13    warning    Name 'id': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.              whitespace  292:13    warning    Name 'bondedToken': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.     whitespace  293:13    warning    Name 'period': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.          whitespace  294:13    warning    Name 'exchangeRate': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.    whitespace  295:13    warning    Name 'openDate': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.        whitespace  296:13    warning    Name 'reserveRatio': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.    whitespace  297:13    warning    Name 'batchBlocks': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.     whitespace  298:13    warning    Name 'slippage': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.        whitespace  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"
    },
    {
        "title": "7.3 Surya",
        "body": "  Surya is a utility tool for smart contract systems. It provides a number of visual outputs and information about the structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers (please use horizontal scroll to view all columns):  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"
    },
    {
        "title": "4.1 ERC1400ERC20 whitelist circumvents partition restrictions    ",
        "body": "  Resolution   This is fixed in   ConsenSys/ERC1400#13.  Description  ERC1400/1410 enable  partially fungible tokens  in that not all tokens are equivalent. A specific use case is placing restrictions on some tokens, such as lock-up periods.  The whitelist in ERC1400ERC20 circumvents these restrictions. When a token holder uses the ERC20 transfer function, tokens are transferred from that user s  default partitions , which a user can choose themselves by calling ERC1410.setDefaultPartitions. This means they can transfer tokens from any partition, and the only restriction that s placed on the transfer is that the recipient must be whitelisted.  It should be noted that the comment and error message around the whitelisting feature suggests that it is meant to be applied to both the sender and recipient:  code/contracts/token/ERC20/ERC1400ERC20.sol:L24-L30  /**  @dev Modifier to verify if sender and recipient are whitelisted.  /  modifier isWhitelisted(address recipient) {  require(_whitelisted[recipient], \"A3: Transfer Blocked - Sender lockup period not ended\");  _;  Remediation  There are many possibilities, but here are concrete suggestions for addressing this:  Require whitelisting both the sender and recipient, and make sure that whitelisted accounts only own (and will only own) unrestricted tokens.  Make sure that the only whitelisted recipients are those that apply partition restrictions when receiving tokens. (I.e. they implement the modified ERC777 receiving hook, examine the source partition, and reject transfers that should not occur.)  Instead of implementing the ERC20 interface on top of the ERC1400 token, support transferring out of the ERC1400 token and into a standard ERC20 token. Partition restrictions can then be applied on the ERC1400 transfer, and once ERC20 tokens are obtained, they can be transferred without restriction.  Don t allow token holders to set their own default partitions. Rather, have the token specify a single, unrestricted partition that is used for all ERC20 transfers.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"
    },
    {
        "title": "4.2 Certificate controllers do not always constrain the last argument    ",
        "body": "  Resolution   The existing back end already does its own ABI encoding, which means it s not vulnerable to this issue. Documentation has been added in   https://gitlab.com/ConsenSys/client/fr/dauriel/smart-contracts/certificate-controller/merge_requests/9 to ensure future maintainers understand this potential issue.  Description  The certificate controllers (CertificateControllerNonce and CertificateControllerSalt) are used by passing a signature as a final argument in a function call. This signature is over the other arguments to the function. Specifically, the signature must match the call data that precedes the signature.  The way this is implemented assumes standard ABI encoding of parameters, but there s actually some room for manipulation by a malicious user. This manipulation can allow the user to change some of the call data without invalidating the signature.  The following code is from CertificateControllerNonce, but similar logic applies to CertificateControllerSalt:  code2/contracts/CertificateControllerNonce.sol:L127-L134  bytes memory payload;  assembly {  let payloadsize := sub(calldatasize, 160)  payload := mload(0x40) // allocate new memory  mstore(0x40, add(payload, and(add(add(payloadsize, 0x20), 0x1f), not(0x1f)))) // boolean trick for padding to 0x40  mstore(payload, payloadsize) // set length  calldatacopy(add(add(payload, 0x20), 4), 4, sub(payloadsize, 4))  Here the signature is over all call data except the final 160 bytes. 160 bytes makes sense because the byte array is length 97, and it s preceded by a 32-byte size. This is a total of 129 bytes, and typical ABI encoded pads this to the next multiple of 32, which is 160.  If an attacker does not pad their arguments, they can use just 129 bytes for the signature or even 128 bytes if the v value happens to be 0. This means that when checking the signature, not only will the signature be excluded, but also the 31 or 32 bytes that come before the signature. This means the attacker can call a function with a different final argument than the one that was signed.  That final argument is, in many cases, the number of tokens to transfer, redeem, or issue.  Mitigating factors  For this to be exploitable, the attacker has to be able to obtain a signature over shortened call data.  If the signer accepts raw arguments and does its own ABI encoding with standard padding, then there s likely no opportunity for an attacker to exploit this vulnerability. (They can shorten the call data length when they make the function call later, but the signature won t match.)  Remediation  We have two suggestions for how to address this:  Instead of signatures being checked directly against call data, compute a new hash based on the decoded values, e.g. keccak256(abi.encode(argument1, argument2, ...)).  Address this at the signing layer (off chain) by doing the ABI encoding there and denying an attacker the opportunity to construct their own call data.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"
    },
    {
        "title": "4.3 Salt-based certificate controller is subject to signature replay    ",
        "body": "  Resolution   This is fixed in   https://gitlab.com/ConsenSys/client/fr/dauriel/smart-contracts/certificate-controller/merge_requests/8.  Description  The salt-based certificate controller prevents signature replay by storing each full signature. Only a signature that is exactly identical to a previously-used signature will be rejected.  For ECDSA signatures, each signature has a second S value (and flipped V to match) that will recover the same address. An attacker can produce such a second signature trivially without knowing the signer s private key. This gives an attacker a way to produce a new unique signature based on a previously used one. This effectively means every signature can be used twice.  code2/contracts/CertificateControllerSalt.sol:L25-L32  modifier isValidCertificate(bytes memory data) {  require(  _certificateSigners[msg.sender] || _checkCertificate(data, 0, 0x00000000),  \"A3: Transfer Blocked - Sender lockup period not ended\"  );  _usedCertificate[data] = true; // Use certificate  References  See https://smartcontractsecurity.github.io/SWC-registry/docs/SWC-117.  Remediation  Instead of rejecting used signatures based on the full signature value, keep track of used salts (which are then better referred to as  nonces ).  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"
    },
    {
        "title": "4.4 EIP-1400 is missing canTransfer* functions   ",
        "body": "  Description  The EIP-1400 states defines the interface to be implemented containing the 3 functions:  // Transfer Validity  function canTransfer(address _to, uint256 _value, bytes _data) external view returns (byte, bytes32);  function canTransferFrom(address _from, address _to, uint256 _value, bytes _data) external view returns (byte, bytes32);  function canTransferByPartition(address _from, address _to, bytes32 _partition, uint256 _value, bytes _data) external view returns (byte, bytes32, bytes32);  These functions were not implemented in ERC1400, thus making the implementation not completely compatible with EIP-1400.  In case the deployed contract needs to be added as a  lego block  part of a another application, there is a high chance that it will not correctly function. That external application could potentially call the EIP-1400 functions canTransfer, canTransferFrom or canTransferByPartition, in which case the transaction will likely fail.  This means that the current implementation will not be able to become part of external markets, exchanges or applications that need to interact with a generic EIP-1400 implementation.  Remediation  Even if the functions do not correctly reflect the transfer possibility, their omission can break other contracts interacting with the implementation.  A suggestion would be to add these functions and make them always return true. This way the contracts interacting with the current implementation do not break when they call these functions, while the actual transfer of the tokens is still limited by the current logic.  ",
        "labels": [
            "Consensys",
            "Major",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"
    },
    {
        "title": "4.5 ERC777 incompatibilities    ",
        "body": "  Resolution   This is fixed in   Description  As noted in the README, the ERC777 contract is not actually compatible with ERC 777.  Functions and events have been renamed, and the hooks ERC777TokensRecipient and ERC777TokensSender have been modified to add a partition parameter.  This means no tools that deal with standard ERC 777 contracts will work with this code s tokens.  Remediation  We suggest renaming these contracts to not use the term  ERC777 , as they lack compatibility. Most importantly, we recommend not using the interface names  ERC777TokensRecipient  and  ERC777TokensSender  when looking up the appropriate hook contracts via ERC 1820. Contracts that handle that interface will not be capable of handling the modified interface used here.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"
    },
    {
        "title": "4.6 Buffer over-read in ERC1410._getDestinationPartition    ",
        "body": "  Resolution   This is fixed in   ConsenSys/ERC1400#16.  Description  There s no check that data is at least 64 bytes long, so the following code can read past the end of data:  code/contracts/token/ERC1410/ERC1410.sol:L348-L361  function _getDestinationPartition(bytes32 fromPartition, bytes memory data) internal pure returns(bytes32 toPartition) {  bytes32 changePartitionFlag = 0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff;  bytes32 flag;  assembly {  flag := mload(add(data, 32))  if(flag == changePartitionFlag) {  assembly {  toPartition := mload(add(data, 64))  } else {  toPartition = fromPartition;  The only caller is _transferByPartition, which only checks that data.length > 0:  code/contracts/token/ERC1410/ERC1410.sol:L263-L264  if(operatorData.length != 0 && data.length != 0) {  toPartition = _getDestinationPartition(fromPartition, data);  Depending on how the compiler chooses to lay out memory, the next data in memory is probably the operatorData buffer, so data may inadvertently be read from there.  Remediation  Check for sufficient length (at least 64 bytes) before attempting to read it.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"
    },
    {
        "title": "4.7 ERC20/ERC777 compatibility: ERC20 transfer functions should not revert if the recipient is a contract without a registered ERC777TokensRecipient implementation    ",
        "body": "  Resolution   This is fixed in   ConsenSys/ERC1400#17.  Description  This will block transfers to a contract that doesn t have an ERC777TokensRecipient implementation. This is in violation of ERC 777, which says:  If the recipient is a contract, which has not registered an ERC777TokensRecipient implementation; then the token contract:  MUST revert if the tokensReceived hook is called from a mint or send call.  SHOULD continue processing the transaction if the tokensReceived hook is called from an ERC20 transfer or transferFrom call.  Remediation  Make sure that ERC20-compatible transfer calls do not set preventLocking to true.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"
    },
    {
        "title": "4.8 ERC777 compatibility: authorizeOperator and revokeOperator should revert when the caller and operator are the same account    ",
        "body": "  Resolution   This is fixed in   ConsenSys/ERC1400#19.  Description  From ERC 777:  The autohrizeOperator implementation does not do that:  code/contracts/token/ERC777/ERC777.sol:L144-L147  function authorizeOperator(address operator) external {  _authorizedOperator[operator][msg.sender] = true;  emit AuthorizedOperator(operator, msg.sender);  The same holds for revokeOperator:  code/contracts/token/ERC777/ERC777.sol:L155-L158  function revokeOperator(address operator) external {  _authorizedOperator[operator][msg.sender] = false;  emit RevokedOperator(operator, msg.sender);  Remediation  Add require(operator != msg.sender) to those two functions.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"
    },
    {
        "title": "4.9 Token receiver can mint gas tokens with sender s gas   ",
        "body": "  Description  When a transfer is executed, there are hooks activated on the sender s and on the receiver s side.  This is possible because the contract implements ERC1820Client which allows any address to define an implementation:  contracts/ERC1820Client.sol:L16-L19  function setInterfaceImplementation(string memory _interfaceLabel, address _implementation) internal {  bytes32 interfaceHash = keccak256(abi.encodePacked(_interfaceLabel));  ERC1820REGISTRY.setInterfaceImplementer(address(this), interfaceHash, _implementation);  Considering the receiver s side:  contracts/ERC1400.sol:L1016-L1020  recipientImplementation = interfaceAddr(to, ERC1400_TOKENS_RECIPIENT);  if (recipientImplementation != address(0)) {  IERC1400TokensRecipient(recipientImplementation).tokensReceived(msg.sig, partition, operator, from, to, value, data, operatorData);  The sender has to pay for the gas for the transaction to go through.  Because the receiver can define a contract to be called when receiving the tokens, and the sender has to pay for the gas, the receiver can mint gas tokens (or waste the gas).  Remediation  Because this is the way Ethereum works and the implementation allows calling external methods, there s no recommended remediation for this issue. It s just something the senders need to be aware of.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"
    },
    {
        "title": "4.10 Missing ERC Functions    ",
        "body": "  Resolution  This is fixed in https://github.com/ConsenSys/ERC1400/pull/18.  Description  There exist some functions, such as isOperator() ,that are part of the ERC1410 spec. Removing functions expected by ERC may break things like block explorers that expect to be able to query standard contracts for relevant metadata.  Remediation  It would be good to explicitly state any expected incompatibilities.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"
    },
    {
        "title": "4.11 Inaccurate error message in ERC777ERC20.approve    ",
        "body": "  Resolution   This is fixed in   ConsenSys/ERC1400#20.  Description  If the spender is address 0, the revert message says that the receiver is not eligible.  code/contracts/token/ERC20/ERC777ERC20.sol:L153  require(spender != address(0), \"A6: Transfer Blocked - Receiver not eligible\");  Remediation  Fix the revert message to match the actual issue.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"
    },
    {
        "title": "4.12 Non-standard treatment of a from address of 0    ",
        "body": "  Resolution   This is fixed in   ConsenSys/ERC1400#21.  Description  A number of functions throughout the system treat a from address of 0 as equivalent to msg.sender. In some cases, this seems to violate existing standards (e.g. in ERC20 transfers). In other cases, it is merely surprising.  ERC1400ERC20.transferFrom and ERC777ERC20.transferFrom both treat a from address as 0 as equivalent to msg.sender. This is unexpected behavior for an ERC20 token.  Examples  code/contracts/ERC1400.sol:L206-L214  function canOperatorTransferByPartition(bytes32 partition, address from, address to, uint256 value, bytes calldata data, bytes calldata operatorData)  external  view  returns (byte, bytes32, bytes32)  if(!_checkCertificate(operatorData, 0, 0x8c0dee9c)) { // 4 first bytes of keccak256(operatorTransferByPartition(bytes32,address,address,uint256,bytes,bytes))  return(hex\"A3\", \"\", partition); // Transfer Blocked - Sender lockup period not ended  } else {  address _from = (from == address(0)) ? msg.sender : from;  code/contracts/ERC1400.sol:L417-L421  function redeemFrom(address from, uint256 value, bytes calldata data, bytes calldata operatorData)  external  isValidCertificate(operatorData)  address _from = (from == address(0)) ? msg.sender : from;  code/contracts/token/ERC20/ERC1400ERC20.sol:L180-L181  function transferFrom(address from, address to, uint256 value) external isWhitelisted(to) returns (bool) {  address _from = (from == address(0)) ? msg.sender : from;  code/contracts/token/ERC20/ERC777ERC20.sol:L179-L180  function transferFrom(address from, address to, uint256 value) external isWhitelisted(to) returns (bool) {  address _from = (from == address(0)) ? msg.sender : from;  code/contracts/token/ERC777/ERC777.sol:L194-L198  function transferFromWithData(address from, address to, uint256 value, bytes calldata data, bytes calldata operatorData)  external  isValidCertificate(operatorData)  address _from = (from == address(0)) ? msg.sender : from;  code/contracts/token/ERC777/ERC777.sol:L226-L230  function redeemFrom(address from, uint256 value, bytes calldata data, bytes calldata operatorData)  external  isValidCertificate(operatorData)  address _from = (from == address(0)) ? msg.sender : from;  code/contracts/token/ERC1410/ERC1410.sol:L130-L142  function operatorTransferByPartition(  bytes32 partition,  address from,  address to,  uint256 value,  bytes calldata data,  bytes calldata operatorData  external  isValidCertificate(operatorData)  returns (bytes32)  address _from = (from == address(0)) ? msg.sender : from;  code/contracts/token/ERC1410/ERC1410.sol:L430-L434  function transferFromWithData(address from, address to, uint256 value, bytes calldata data, bytes calldata operatorData)  external  isValidCertificate(operatorData)  address _from = (from == address(0)) ? msg.sender : from;  Remediation  Remove this fallback logic and always use the from address that was passed in. This avoids surprises where, for example, an uninitialized value leads to loss of funds.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"
    },
    {
        "title": "4.13 ERC1410 s redeem and redeemFrom should revert    ",
        "body": "  Resolution   This is fixed in   ConsenSys/ERC1400#22.  Description  ERC1410 contains two functions: redeem and redeemFrom that  erase  the underlying ERC777 versions of these functions because those functions don t handle partitions.  These functions silently succeed, while they should probably fail by reverting.  Examples  code/contracts/token/ERC1410/ERC1410.sol:L441-L453  /**  [NOT MANDATORY FOR ERC1410 STANDARD][OVERRIDES ERC777 METHOD]  @dev Empty function to erase ERC777 redeem() function since it doesn't handle partitions.  /  function redeem(uint256 /*value*/, bytes calldata /*data*/) external { // Comments to avoid compilation warnings for unused variables.  /**  [NOT MANDATORY FOR ERC1410 STANDARD][OVERRIDES ERC777 METHOD]  @dev Empty function to erase ERC777 redeemFrom() function since it doesn't handle partitions.  /  function redeemFrom(address /*from*/, uint256 /*value*/, bytes calldata /*data*/, bytes calldata /*operatorData*/) external { // Comments to avoid compilation warnings for unused variables.  Remediation  Add a revert() (possibly with a reason) so callers know that the call failed.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"
    },
    {
        "title": "4.14 Unclear why operatorData.length is checked in _transferByPartition    ",
        "body": "  Resolution   This code is actually correct. When   Description  It s unclear why operatorData.length is being checked here:  code/contracts/token/ERC1410/ERC1410.sol:L263-L264  if(operatorData.length != 0 && data.length != 0) {  toPartition = _getDestinationPartition(fromPartition, data);  Remediation  Consider removing that check.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"
    },
    {
        "title": "4.15 Global partition enumeration can run into gas limits    ",
        "body": "  Resolution   This is fixed in   ConsenSys/ERC1400#25.  Description  In ERC1410, partitions are created on demand by issuing or transferring tokens, and these new partitions are added to the array _totalPartitions. When one of these partitions is later emptied, it s removed from that array with the following code in _removeTokenFromPartition:  code/contracts/token/ERC1410/ERC1410.sol:L303-L313  // If the total supply is zero, finds and deletes the partition.  if(_totalSupplyByPartition[partition] == 0) {  for (uint i = 0; i < _totalPartitions.length; i++) {  if(_totalPartitions[i] == partition) {  _totalPartitions[i] = _totalPartitions[_totalPartitions.length - 1];  delete _totalPartitions[_totalPartitions.length - 1];  _totalPartitions.length--;  break;  Finding the partition requires iterating over the entire array. This means that _removeTokenFromPartition can become very expensive and eventually bump up against the block gas limit if lots of partitions are created. This could be an attack vector for a malicious operator.  The same issue applies to a token holder s list of partitions, where transferring tokens in a large number of partitions to that token holder may block them from being able to transfer tokens out:  code/contracts/token/ERC1410/ERC1410.sol:L291-L301  // If the balance of the TokenHolder's partition is zero, finds and deletes the partition.  if(_balanceOfByPartition[from][partition] == 0) {  for (uint i = 0; i < _partitionsOf[from].length; i++) {  if(_partitionsOf[from][i] == partition) {  _partitionsOf[from][i] = _partitionsOf[from][_partitionsOf[from].length - 1];  delete _partitionsOf[from][_partitionsOf[from].length - 1];  _partitionsOf[from].length--;  break;  Remediation  Removing an item from a set can be accomplished in constant time if the set uses both an array (for storing the values) and a mapping of values to their index in that array. See https://programtheblockchain.com/posts/2018/06/03/storage-patterns-set/ for one example of doing this.  It also may be reasonable to cap the number of possible partitions or lock them down to a constant set of values on deployment, depending on the use case for the token.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"
    },
    {
        "title": "4.16 Optimization: redundant delete in ERC1400. _removeTokenFromPartition    ",
        "body": "  Resolution   This is fixed in   ConsenSys/ERC1400#23.  Description  Reducing the size of an array automatically deletes the removed elements, so the first of these two lines is redundant:  code/contracts/token/ERC1410/ERC1410.sol:L296-L297  delete _partitionsOf[from][_partitionsOf[from].length - 1];  _partitionsOf[from].length--;  The same applies here:  code/contracts/token/ERC1410/ERC1410.sol:L308-L309  delete _totalPartitions[_totalPartitions.length - 1];  _totalPartitions.length--;  Remediation  Remove the redundant deletions to save a little gas.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"
    },
    {
        "title": "4.17 Avoid hardcoding function selectors    ",
        "body": "  Resolution   This is fixed in   ConsenSys/ERC1400#24.  Description  In ERC1400, hardcoded function selectors can be replaced with this.transferByPartition.selector and this.operatorTransferByPartition.selector.  Examples  code/contracts/ERC1400.sol:L184  if(!_checkCertificate(data, 0, 0xf3d490db)) { // 4 first bytes of keccak256(transferByPartition(bytes32,address,uint256,bytes))  code/contracts/ERC1400.sol:L211  if(!_checkCertificate(operatorData, 0, 0x8c0dee9c)) { // 4 first bytes of keccak256(operatorTransferByPartition(bytes32,address,address,uint256,bytes,bytes))  Remediation  Replace the hardcoded function selectors with this.<method>.selector.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"
    },
    {
        "title": "3.1 Yearn: Re-entrancy attack during deposit ",
        "body": "  Description  During the deposit in the supplyTokenTo function, the token transfer is happening after the shares are minted and before tokens are deposited to the yearn vault:  code/pooltogether-yearnv2-yield-source/contracts/yield-source/YearnV2YieldSource.sol:L117-L128  function supplyTokenTo(uint256 _amount, address to) override external {  uint256 shares = _tokenToShares(_amount);  _mint(to, shares);  // NOTE: we have to deposit after calculating shares to mint  token.safeTransferFrom(msg.sender, address(this), _amount);  _depositInVault();  emit SuppliedTokenTo(msg.sender, shares, _amount, to);  If the token allows the re-entrancy (e.g., ERC-777), the attacker can do one more transaction during the token transfer and call the supplyTokenTo function again. This second call will be done with already modified shares from the first deposit but non-modified token balances. That will lead to an increased amount of shares minted during the supplyTokenTo. By using that technique, it s possible to steal funds from other users of the contract.  Recommendation  Have the re-entrancy guard on all the external functions.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"
    },
    {
        "title": "3.2 Yearn: Partial deposits are not processed properly ",
        "body": "  Description  The deposit is usually made with all the token balance of the contract:  code/pooltogether-yearnv2-yield-source/contracts/yield-source/YearnV2YieldSource.sol:L171-L172  // this will deposit full balance (for cases like not enough room in Vault)  return v.deposit();  The Yearn vault contract has a limit of how many tokens can be deposited there. If the deposit hits the limit, only part of the tokens is deposited (not to exceed the limit). That case is not handled properly, the shares are minted as if all the tokens are accepted, and the  change  is not transferred back to the caller:  code/pooltogether-yearnv2-yield-source/contracts/yield-source/YearnV2YieldSource.sol:L117-L128  function supplyTokenTo(uint256 _amount, address to) override external {  uint256 shares = _tokenToShares(_amount);  _mint(to, shares);  // NOTE: we have to deposit after calculating shares to mint  token.safeTransferFrom(msg.sender, address(this), _amount);  _depositInVault();  emit SuppliedTokenTo(msg.sender, shares, _amount, to);  Recommendation  Handle the edge cases properly.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"
    },
    {
        "title": "3.3 Sushi: redeemToken redeems less than it should ",
        "body": "  Description  The redeemToken function takes as argument the amount of SUSHI to redeem. Because the SushiBar s leave function   which has to be called to achieve this goal   takes an amount of xSUSHI that is to be burned in exchange for SUSHI, redeemToken has to compute the amount of xSUSHI that will result in a return of as many SUSHI tokens as were requested.  code/sushi-pooltogether/contracts/SushiYieldSource.sol:L62-L87  /// @notice Redeems tokens from the yield source from the msg.sender, it burn yield bearing tokens and return token to the sender.  /// @param amount The amount of `token()` to withdraw.  Denominated in `token()` as above.  /// @return The actual amount of tokens that were redeemed.  function redeemToken(uint256 amount) public override returns (uint256) {  ISushiBar bar = ISushiBar(sushiBar);  ISushi sushi = ISushi(sushiAddr);  uint256 totalShares = bar.totalSupply();  uint256 barSushiBalance = sushi.balanceOf(address(bar));  uint256 requiredShares = amount.mul(totalShares).div(barSushiBalance);  uint256 barBeforeBalance = bar.balanceOf(address(this));  uint256 sushiBeforeBalance = sushi.balanceOf(address(this));  bar.leave(requiredShares);  uint256 barAfterBalance = bar.balanceOf(address(this));  uint256 sushiAfterBalance = sushi.balanceOf(address(this));  uint256 barBalanceDiff = barBeforeBalance.sub(barAfterBalance);  uint256 sushiBalanceDiff = sushiAfterBalance.sub(sushiBeforeBalance);  balances[msg.sender] = balances[msg.sender].sub(barBalanceDiff);  sushi.transfer(msg.sender, sushiBalanceDiff);  return (sushiBalanceDiff);  Recommendation  Calculate requiredShares based on the formula above (x2). We also recommend dealing in a clean way with the special cases totalShares == 0 and barSushiBalance == 0.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"
    },
    {
        "title": "3.4 Sushi: balanceOfToken underestimates balance ",
        "body": "  Description  The balanceOfToken computation is too pessimistic, i.e., it can underestimate the current balance slightly.  code/sushi-pooltogether/contracts/SushiYieldSource.sol:L29-L45  /// @notice Returns the total balance (in asset tokens).  This includes the deposits and interest.  /// @return The underlying balance of asset tokens  function balanceOfToken(address addr) public override returns (uint256) {  if (balances[addr] == 0) return 0;  ISushiBar bar = ISushiBar(sushiBar);  uint256 shares = bar.balanceOf(address(this));  uint256 totalShares = bar.totalSupply();  uint256 sushiBalance =  shares.mul(ISushi(sushiAddr).balanceOf(address(sushiBar))).div(  totalShares  );  uint256 sourceShares = bar.balanceOf(address(this));  return (balances[addr].mul(sushiBalance).div(sourceShares));  Recommendation  The balanceOfToken function should use the formula above.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"
    },
    {
        "title": "3.5 Yearn: Redundant approve call ",
        "body": "  Description  The approval for token transfer is done in the following way:  code/pooltogether-yearnv2-yield-source/contracts/yield-source/YearnV2YieldSource.sol:L167-L170  if(token.allowance(address(this), address(v)) < token.balanceOf(address(this))) {  token.safeApprove(address(v), 0);  token.safeApprove(address(v), type(uint256).max);  Since the approval will be equal to the maximum value, there s no need to make zero-value approval first.  Recommendation  Change two safeApprove to one regular approve with the maximum value.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"
    },
    {
        "title": "3.6 Sushi: Some state variables should be immutable and have more specific types ",
        "body": "  Description  The state variables sushiBar and sushiAddr are initialized in the contract s constructor and never changed afterward.  code/sushi-pooltogether/contracts/SushiYieldSource.sol:L12-L21  contract SushiYieldSource is IYieldSource {  using SafeMath for uint256;  address public sushiBar;  address public sushiAddr;  mapping(address => uint256) public balances;  constructor(address _sushiBar, address _sushiAddr) public {  sushiBar = _sushiBar;  sushiAddr = _sushiAddr;  Recommendation  Make these two state variables immutable and change their types as indicated above. Remove the corresponding explicit type conversions in the rest of the contract, and add explicit conversions to type address where necessary.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"
    },
    {
        "title": "3.7 Sushi: Unnecessary balance queries ",
        "body": "  Description  code/sushi-pooltogether/contracts/SushiYieldSource.sol:L73-L84  uint256 barBeforeBalance = bar.balanceOf(address(this));  uint256 sushiBeforeBalance = sushi.balanceOf(address(this));  bar.leave(requiredShares);  uint256 barAfterBalance = bar.balanceOf(address(this));  uint256 sushiAfterBalance = sushi.balanceOf(address(this));  uint256 barBalanceDiff = barBeforeBalance.sub(barAfterBalance);  uint256 sushiBalanceDiff = sushiAfterBalance.sub(sushiBeforeBalance);  balances[msg.sender] = balances[msg.sender].sub(barBalanceDiff);  Recommendation  Use requiredShares instead of barBalanceDiff, and remove the unnecessary queries and variables.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"
    },
    {
        "title": "3.8 Sushi: Unnecessary function declaration in interface ",
        "body": "  Description  The ISushiBar interface declares a transfer function.  code/sushi-pooltogether/contracts/ISushiBar.sol:L5-L17  interface ISushiBar {  function enter(uint256 _amount) external;  function leave(uint256 _share) external;  function totalSupply() external view returns (uint256);  function balanceOf(address account) external view returns (uint256);  function transfer(address recipient, uint256 amount)  external  returns (bool);  However, this function is never used, so it could be removed from the interface. Other functions that the SushiBar provides but are not used (approve, for example) aren t part of the interface either.  Recommendation  Remove the transfer declaration from the ISushiBar interface.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"
    },
    {
        "title": "4.1 Missing Input Validation for WalletAddress    ",
        "body": "  Resolution   The client acknowledged the issue and fixed it by implementing a regex validation in PR#25   here - Snap shasum  Description  The snap prompts users to input the wallet address to be monitored. Users can set wallet addreses that do not adhere to the common Ethereum address format. The user input is not sanitized. This could lead to various injection vulnerabilities such as markdown or control character injections that could break other components. In particular, the address is sent to the API as a URL query parameter. A malicious attacker could try using that to mount URL injection attacks.  packages/snap/src/index.ts:L50-L61  if (  request.method === RpcRequestMethods.UpdateAccount &&  'walletAddress' in request.params &&  typeof request.params.walletAddress === 'string'  ) {  const { walletAddress } = request.params;  if (!walletAddress) {  throw new Error('no wallet address provided');  updateWalletAddress(walletAddress);  Recommendation  Sanitize the address string input by the user and reject all addresses that do not adhere to the Ethereum address format.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"
    },
    {
        "title": "4.2 Server Should Not Rely on Clients  Randomness    ",
        "body": "  Resolution  Severity decreased: Major  > Medium: The client acknowledged the issue, and let us know that the ID is only used for analytics purposes, to be compatible with the existing API. A future release of the API will improve UID handling.  Description  The snap code sends a request to the Wallet Guard API with a random UUID crypto.randomUUID() generated by the client. We would like to underline that the API should never trust clients  randomness nor assume any property about it. Relying on client-generated randomness for the API could lead to many vulnerabilities, such as replay attacks or collision issues due to the inability to ensure uniqueness. The varying algorithms used by clients may be subpar or even compromised. As this id is not used anywhere else in the snap code, we assume that it might be used on the API side. Because the API is not in scope for this review, we don t have access to the code and cannot tell whether this pseudo-random UUID is used in a safe way.  packages/snap/src/http/fetchTransaction.ts:L32-L40  const simulateRequest: SimulateRequestParams = {  id: crypto.randomUUID(),  chainID: mappedChainId,  signer: transaction.from as string,  origin: transactionOrigin as string,  method: transaction.method as string,  transaction,  source: 'SNAP',  };  Recommendation  Don t rely on clients  randomness on the API. Instead, the server should assign a unique ID to every incoming request.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"
    },
    {
        "title": "4.3 Properties of the transaction Object Might Be Undefined ",
        "body": "  Description  The Metamask Snaps API does not guarantee that the properties from and method of the transaction object are defined. Depending on the transaction type, it could happen that these properties are not defined. This would result in a runtime error when undefined is casted to string.  packages/snap/src/http/fetchTransaction.ts:L32-L40  const simulateRequest: SimulateRequestParams = {  id: crypto.randomUUID(),  chainID: mappedChainId,  signer: transaction.from as string,  origin: transactionOrigin as string,  method: transaction.method as string,  transaction,  source: 'SNAP',  };  Recommendation  One should check whether properties from, and method are defined, before explicitly casting them to a string. This could be done by introducing a hasProperty utility function for instance.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"
    },
    {
        "title": "4.4 AssetChangeComponent Displays a Change With Value 0 if fiatValue < 0.005 ",
        "body": "  Description  The toFixed(2) method rounds the transaction value string to 2 decimals. For transactions with fiatValue < 0.005, the function returns 0, meaning the component will display a transaction with zero value to the user, even if the transaction has a small yet non-zero value. This is not a good idea as it might trick the user. In that case, it would be better to default to the smallest value that can represented (i.e. 0.01) instead of 0.  packages/snap/src/components/stateChanges/AssetChangeComponent.ts:L18  const fiatValue = Number(stateChange.fiatValue).toFixed(2);  Recommendation  If fiatValue < 0.005, consider displaying a value of 0.01 to the user, instead of 0.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"
    },
    {
        "title": "4.5 Incomplete NatSpec and General Documentation ",
        "body": "  Description  The code is missing NatSpec documentation in many places. NatSpec documentation plays an important role in improving code comprehension and maintenance. Adding NatSpec documentation to functions with significant logic that provides clear explanations of behavior, inputs, and outputs enhances code readability, transparency, and maintainability of the codebase.  Recommendation  We recommend adding NatSpec documentation to every function that contains significant logic. Especially all the Snaps handlers. This will improve the readability, transparency, and maintainability of the codebase. We also recommend adding a detailed high-level documentation about the Snaps features, components, and permissions in the README.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"
    },
    {
        "title": "4.6 formatFiatValue() Can Be Simplified ",
        "body": "  Description  The function formatFiatValue formats a number to a string that is displayed to the user. The function formats numbers with at most 2 decimal digits, removes the trailing zeros, and adds commas as thousands separators.  The function first converts the number to a string representing the number in fixed-point notation. Then, it uses regex to remove the trailing zeros if they exist. Finally, it adds the thousands separators.  packages/snap/src/utils/helpers.ts:L16-L26  export const formatFiatValue = (  fiatValue: string,  maxDecimals: number,  ): string => {  const fiatWithRoundedDecimals = Number(fiatValue)  .toFixed(maxDecimals) // round to maxDecimals  .replace(/\\.00$/u, ''); // removes 00 if it exists  const fiatWithCommas = numberWithCommas(fiatWithRoundedDecimals); // add commas  return `$${fiatWithCommas}`;  };  The design of the function is unnecessarily complex. The whole design could be simplified using the native toLocaleString() function with appropriate parameters.  Recommendation  Simplify the design by using the native toLocaleString function. For instance, the function could be used as follows toLocaleString('en-US',{minimumFractionDigits: 0, maximumFractionDigits: 2})  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"
    },
    {
        "title": "4.7 No Way to Disable Approvals Checking, and Transaction Analytics ",
        "body": "  Description  Currently, there is no easy way to disable wallet approval monitoring and/or transaction simulation apart from uninstalling the snap. Users might want to opt out of wallet monitoring or disable transaction simulation selectively e.g., for privacy concerns.  Recommendation  We would recommend implementing a mechanism that allows users to selectively disable the snap features.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"
    },
    {
        "title": "4.8 devDependencies Erroneously Listed as dependencies ",
        "body": "  Description  The following dependencies are only used for development purpose and should therefore be listed as  devDependencies  instead of  dependencies  in the package.json file. Indeed, the TypeScript code is compiled into a bundle, which is released. Meaning the snap  production  code should not contain any external dependency.  packages/snap/package.json:L28-L31  \"dependencies\": {  \"@metamask/snaps-types\": \"^0.32.2\",  \"@metamask/snaps-ui\": \"^0.32.2\"  },  Recommendation  List the dependencies as  devDependencies .  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"
    },
    {
        "title": "4.9 package.json - Missing Author ",
        "body": "  Description  The package.json file is missing the author name, the link to the project homepage, and to the bug tracker.  Recommendation  According to package publishing best practices, we recommend adding those elements to the package.json file.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"
    },
    {
        "title": "4.10 Extra  If  Statement",
        "body": "  Description  The onRpcRequest() handler returns early if walletAddress is not defined.  packages/snap/src/index.ts:L57-L59  if (!walletAddress) {  throw new Error('no wallet address provided');  Thus, the extra  if  check before calling snap.request() is superfluous and can be removed.  packages/snap/src/index.ts:L57-L64  if (!walletAddress) {  throw new Error('no wallet address provided');  updateWalletAddress(walletAddress);  if (walletAddress) {  await snap.request({  Recommendation  Remove the extra  if  check.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"
    },
    {
        "title": "4.11 Misleading Comment",
        "body": "  Description  The NatSpec comment indicates that onRpcRequest() returns  the result of snap_dialog  while the method either does not return anything, or returns the Ethereum address of the monitored wallet.  packages/snap/src/index.ts:L24-L34  /**  Handle incoming JSON-RPC requests, sent through `wallet_invokeSnap`.  @param args - The request handler args as object.  @param args.origin - The origin of the request, e.g., the website that  invoked the snap.  @param args.request - A validated JSON-RPC request object.  @returns The result of `snap_dialog`.  @throws If the request method is not valid for this snap.  /  Recommendation  Fix the comment.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"
    },
    {
        "title": "4.12 Wallet Monitoring Improvements",
        "body": "  Description  The snap allows the user to set an arbitrary wallet address to be monitored for dangerous approvals. This feature is only of limited use and could be improved by:  Allowing to specify multiple addresses to monitor (a wallet typically consists of many accounts that are managed under the wallet key)  Allowing users to fetch connected addresses via the ethereum API directly instead of requiring the user to input valid accounts  For privacy reasons, allowing users to opt out of transaction analytics on a per-account basis (Currently, every transaction and transaction origin is sent to the API, even if no monitored wallet address is set).  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"
    },
    {
        "title": "4.13 Consider Submitting Snap Version With Backend API Requests",
        "body": "  Description  Consider adding the snap package version to the API requests in order to get insights about what snap versions are used in the field. This could be useful for future debugging and forensics when multiple snap versions will coexist.  packages/snap/src/http/fetchTransaction.ts:L32-L40  const simulateRequest: SimulateRequestParams = {  id: crypto.randomUUID(),  chainID: mappedChainId,  signer: transaction.from as string,  origin: transactionOrigin as string,  method: transaction.method as string,  transaction,  source: 'SNAP',  };  packages/snap/src/types/simulateApi.ts:L25-L35  export type SimulateRequestParams = {  id: string;  chainID: string;  signer: string;  origin: string;  method: string;  transaction: {  [key: string]: Json;  };  source: 'SNAP';  };  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"
    },
    {
        "title": "6.1 FAIR can be stolen using ERC-777 hooks    ",
        "body": "  Resolution  fixed by completely removing ERC-777 support.  Description  The sell() function calls out to user-configured hooks when burning incoming FAIR tokens. The buy() function does the same if the DAT s currency is ERC-777 compliant.  Either of these hooks might invoke malicious code to re-enter the DAT, allowing them to sell and/or buy FAIR tokens at an unintentionally favourable price.  Such attacks may leave the DAT undercollateralized, resulting in other investors being unable to redeem their FAIR for currency.  Example  Here are some ordered extracts from the code invoked when DAT.buy() is called, when the DAT s currency is an ERC-777 compliant token.  code/contracts/DecentralizedAutonomousTrust.vy:L629  tokenValue: uint256 = self.estimateBuyValue(_currencyValue)  The code above does a calculation using FAIR.totalSupply as input. The higher FAIR.totalSupply is, the more expensive FAIR tokens become.  code/contracts/DecentralizedAutonomousTrust.vy:L502-L503  if(self.isCurrencyERC777):  self.currency.operatorSend(_from, self, _quantityToInvest, \"\", \"\")  Per the ERC-777 standard, the code above invokes an arbitrary tokensToSend() hook configured by the buyer.  code/contracts/DecentralizedAutonomousTrust.vy:L654  self.fair.mint(msg.sender, _to, tokenValue, \"\", \"\")  The code above increments FAIR.totalSupply, effectively increasing the price of FAIR tokens. This happens after the other two code extracts have completed.  An attacker can exploit re-entrancy during the tokensToSend() hook, to purchase further tokens at a (perhaps extremely) favourable price before FAIR.totalSupply is incremented.  If the price at the time of the initial buy() is very low (as it will be when totalSupply is small or zero), then they may be able to buy huge amounts of FAIR at that very low price.  Recommendation  Prevent reentrancy by adding a mutex (using Vyper s @nonreentrant() decorator) across all functions that result in ERC-777 token transfers (of FAIR or an ERC-777 currency).  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"
    },
    {
        "title": "6.2 BigDiv does not prevent overflow in some cases where it should    ",
        "body": "  Resolution  Fixed in the Solidity implementation.  Description  BigDiv.vy has been created with the aim of allowing calculations like (a * b) / d to succeed where an intermediate step (e.g. a * b) might overflow but the end result is <= MAX_UINT256.  All of the functions sometimes fail in this aim if the numerators are large and of the same order of magnitude. (E.g. for bigDiv2x1, it fails if _numA / MAX_BEFORE_SQUARE = numB / MAX_BEFORE_SQUARE > 0)  The chances of this issue being hit accidentally or exploited deliberately in the current code will both greatly depend on the DAT s configuration and its state. (If the numbers are amenable, an attacker could conceivably front run transactions and adjust FAIR balances in a way that causes targeted transactions to fail.)  Having functions that unexpectedly fail is dangerous for future consumers of this code, and the (simplest possible) fix is small.  Examples  The following code overflows in the code as audited, but succeeds (returning MAX_INT) if MAX_BEFORE_SQUARE is altered as suggested in issue 6.4.  bigDiv2x1 also overflows for some simple cases where the result is far below MAX_UINT256. E.g.:  Recommendations  1. Fix overflows  The following code appears in each BigDiv function:  code/contracts/BigDiv.vy:L30-L31  if(factor == 0):  factor = 1  Replacing every instance of these two lines with simply factor += 1 will avoid overflows. It will also reduce the (currently undocumented) accuracy of the result in some cases, so see recommendations in issue 6.4.  2. Add automated regression tests for all BigDiv functions  We have already written some basic test code and can supply it on request.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"
    },
    {
        "title": "6.3 Square roots are not calculated accurately for inputs below ~10^30    ",
        "body": "  Resolution  This has been addressed.  Description  The rounding performed when calculating square roots results in an extreme loss of precision for numbers < ~10^30.  This may or may not be OK. Typically the numbers being square rooted will be significantly larger than 10^30, but when supply is low and the value of a buy / pay is also low, this rounding could have a dramatic effect.  In any case, the square rooting logic and its limitations could be be better documented and tested.  Examples  In both places where _toDecimalWithPlaces is used, it is surrounded by the same code, which combines with _toDecimalWithPlaces to calculate a square root of a uint256:  code/contracts/DecentralizedAutonomousTrust.vy:L792-L808  # Math: Truncates last 18 digits from tokenValue here  tokenValue /= DIGITS_UINT  # Math: Truncates another 8 digits from tokenValue (losing 26 digits in total)  # This will cause small values to round to 0 tokens for the payment (the payment is still accepted)  # Math: Max supported tokenValue is 1.7e+56. If supply is at the hard-cap tokenValue would be 1e38, leaving room  # for a _currencyValue up to 1.7e33 (or 1.7e15 after decimals)  decimalValue: decimal = self._toDecimalWithPlaces(tokenValue)  decimalValue = sqrt(decimalValue)  # Unshift results  # Math: decimalValue has a max value of 2^127 - 1 which after sqrt can always be multiplied  # here without overflow  decimalValue *= DIGITS_DECIMAL  tokenValue = convert(decimalValue, uint256)  This code casts the number to a decimal so that Vyper s sqrt can be used, after first doing some rounding to prevent overflow during the cast. After all of this is done, it casts back to a uint256.  The result of the rounding + casts is reasonably accurate square root for very large integers, but it loses a lot of accuracy for smaller integers.  E.g. an integer as  small  as 12345678901234567890123456 results in a  square root  value of 0.  Recommendations  1. Reduce code duplication and document assumption / limitations  By moving the common surrounding code inside the _toDecimalWithPlaces function, that function could be renamed (e.g. to integerSqrt) and the limitations of the whole square root calculation could be more easily documented.  2. Test the documented limitations of the integerSqrt operation  To verify the documented limitations, thereby reducing the chances of this code being misused by a different developer at a later stage of the same project.  3. If accuracy for smaller integers is important, improve it  Greater accuracy may be achievable by writing or importing a function that approximates square roots using integer arithmetic and Newton s Method, without ever casting to a decimal.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"
    },
    {
        "title": "6.4 BigDiv estimates some values that could be easily calculated    ",
        "body": "  Resolution  Fixed in the port to Solidity.  Description  The accuracy of BigDiv s functions is neither documented clearly nor directly tested. (The csv tests should exercise much of the BigDiv code, but it s hard to see exactly what calculations are being done.)  BigDiv returns estimates in some cases where it could easily calculate a precise answer. Having spoken offline about FAIR s requirements, we believe the lack of accuracy itself is probably not a problem right now, but it creates a small risk of BigDiv being accidentally misused in future scenarios where its level of accuracy is insufficient (perhaps by a different developer, during a new phase of the FAIR project).  In any case, BigDiv s behaviour could be better documented and tested.  Examples  For comparison, we define a simpler function:  @public  @constant  def simpleDiv(  _numA: uint256,  _numB: uint256,  _den: uint256  ) -> uint256:  return _numA * _numB / _den  In some cases where both bigDiv2x1 and simpleDiv both succeed, bigDiv2x1 is less accurate than simpleDiv:  a='1'  b='99993402823669209384634633746074317682114579999'  BigDiv.bigDiv2x1(a, b, '8', false) -- succeeds, approximate answer  simpleDiv(a, b, '8')               -- succeeds, exact answer  Also, the constants MAX_BEFORE_SQUARE and MAX_BEFORE_CUBE seem to have been miscalculated, resulting in estimation happening slightly more often than necessary.  Recommendations  1. Document expected accuracy / rounding  To prevent accidental misuse of these functions in the future.  2. Add automated regression tests for all BigDiv functions  We have written some basic unit test code as part of our audit, and can supply it on request.  3. If maximising accuracy is important, improve it  There is some low-hanging fruit here, such as:  increasing MAX_BEFORE_SQUARE and MAX_BEFORE_CUBE to 340282366920938463463374607431768211456 and 48740834812604276470692695, respectively.  Per code logic, these numbers are really the first numbers that cannot be squared and cubed, so you may also wish to rename the constants. Note that MAX_BEFORE_SQUARE is also defined in the DAT contract  Add a check for overflow before resorting to estimation. E.g. for bigDiv2x...:  if(MAX_UINT256 / _numA > _numB):  # No rounding required. Return exact result  return _numA * _numB / _den  This latter change may reduce gas consumption as well as improving accuracy.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"
    },
    {
        "title": "6.5 Unused code in BigDiv functions    ",
        "body": "  Resolution  Fixed.  Description  Some parameters and associated logic can be removed from BigDiv s functions. This would simplify the code, as well as the analysis and testing of the code.  Examples  The _roundUp parameter is always false in the following functions:  bigDiv2x1  bigDiv3x1  bigDiv3x3  Associated conditionals are numerous. E.g. bigDiv3x3 s code branches 7 times on the value of _roundUp, even though it is always false.  Recommendation  Remove unused code and associated logic.  Add tests for code that remains.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"
    },
    {
        "title": "6.6 FAIR - Calling transferFrom should not emit the Approval event   ",
        "body": "  Resolution  Closed as WontFix.  This behavior is a de facto standard based on it s usage in the OpenZeppelin implementation of ERC20.  Description  The method transferFrom() sends some already approved tokens to some address:  code/contracts/FAIR.vy:L427-L439  @public  def transferFrom(  _from: address,  _to: address,  _value: uint256  ) -> bool:  \"\"\"  @notice Transfers `_value` amount of tokens from address `_from` to address `_to` if authorized.  \"\"\"  self.allowances[_from][msg.sender] -= _value  self._send(msg.sender, _from, _to, _value, False, \"\", \"\")  log.Approval(_from, msg.sender, self.allowances[_from][msg.sender])  return True  But it also emits an Approval event.  code/contracts/FAIR.vy:L438  log.Approval(_from, msg.sender, self.allowances[_from][msg.sender])  The event does not seem to create problems, it basically updates the remaining approved tokens.  Examples  The EIP 20 documentation states that the event should be emitted when a successful call to approve happens. It does not say if it should (or should not) be used when successfully calling transferFrom().  https://eips.ethereum.org/EIPS/eip-20#approval  It does not seem to violate the EIP 20 or EIP 777 standard and it helps any off-chain service monitoring the contract, keep track of how many remaining approved tokens are left, without having any previous state.  However any re-entrancy issues will make the transaction emit multiple events, each event having different amounts approved, the last emitted event having the highest value, which will be the incorrect one.  Recommendation  We suggest removing the emitted log because it can create problems.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"
    },
    {
        "title": "6.7 On-chain logic cannot reliably prevent a malicious beneficiary from purchasing tokens at a discount    ",
        "body": "  Resolution  closed as WontFix. Fraudulent token purchases by the Beneficiary are prevented by the associated legal agreements, not by on-chain logic.  The extra code should actually be thought of as enabling a legitimate method for the Beneficiary purchase tokens at a fair price.  Description  The buy() function contains unique logic for identifying and processing an investment by the beneficiary:  code/contracts/DecentralizedAutonomousTrust.vy:L650-L658  elif(self.state == STATE_RUN):  if(_to != self.beneficiary):  self._distributeInvestment(_currencyValue)  self.fair.mint(msg.sender, _to, tokenValue, \"\", \"\")  if(self.state == STATE_RUN):  if(_to == self.beneficiary):  self._applyBurnThreshold() # must mint before this call  Because the beneficiary receives a portion amount invested, without this logic the beneficiary organization could purchase FAIRs for a fraction of the price compared to external investors.  However, this logic can be easily circumvented by a dishonest beneficiary using another address for investments.  The Fairmint team has explained that they are aware that this protection can be circumvented. The  legal layer  is necessary to enforce good behaviour, and the beneficiary would be committing fraud in case they purchased FAIRs using another address. Thus the extra code should actually be thought of as enabling the Beneficiary to legitimately purchase tokens.  Recommendation  This functionality introduces extra code. Consider reducing complexity by removing this functionality if it is not essential.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"
    },
    {
        "title": "6.8 FAIR is not ERC-777 compliant    ",
        "body": "  Resolution  fixed by removing ERC-777 support  Description  A comment at the top of FAIR.vy describes it as an  ERC-777 and ERC-20 compliant token .  But by the code s own acknowledgement, it is not fully ERC-777 compliant in its current state.  Examples  The contract is non-compliant with ERC-777 in at least the following ways:  Does not allow per-user revocation of the default operator (the DAT)  Does not call the ERC777 tokensToSend and tokensReceived hooks within transfer and transferFrom  It is (correctly, given the points above) not ERC820-registered as an ERC777Token  This list may not be exhaustive.  Recommendation  Implementing the standard fully may improve interop, so implementing all missing logic should be considered.  If not in full compliance:  avoid publishing any documentation that could be construed as claiming ERC-777 compliance, including code comments.  in accordance with other findings, consider removing ERC-777 compliance, and restricting the functionality to ERC-20.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"
    },
    {
        "title": "6.9 Not compliant with ERC1404    ",
        "body": "  Resolution  Fixed. Porting to solidity enabled compliance with ERC1404.  Description  The ERC1404 standard is an extension of the ERC20 standard. Here it has been implemented as a standalone contract, but does not contain all of the extra functions required by ERC1404.  As such, neither the FAIR contract nor the ERC1404 contract is ERC1404-compliant.  Recommendations  Rename the ERC1404 contract to be something more generic like Whitelist. This is more descriptive, and avoids confusion between Whitelist.approve() and the completely unrelated approve() function that an ERC1404-compliant contract should inherit from ERC20.  Fully implement ERC1404 in FAIR by adding messageForTransferRestriction(), if and only if the standard can be changed to accommodate Vyper s types. If it cannot, drop all claims or implications of ERC1404 support.  To further reduce confusion, consider renaming approve(), and perhaps splitting it into 2 separate functions. E.g. allow() and deny().  7 Code quality recommendations  This sections compiles suggestions which do not pose a direct threat to security, but would otherwise improve the quality of the code.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"
    },
    {
        "title": "7.1 DecentralizedAutonomousTrust.sol",
        "body": "  The method _authorizeTransfer could be rewritten as a modifier, if desired.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"
    },
    {
        "title": "7.2 Whitelist.sol",
        "body": "  The argument _isSell is not used in the method authorizeTransfer().  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"
    },
    {
        "title": "7.3 Sqrt.sol",
        "body": "  SafeMath.sol is imported to Sqrt.sol, but is not used.  8 Gas efficiency optimization recommendations  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"
    },
    {
        "title": "8.1 DecentralizedAutonomousTrust.sol",
        "body": "  The BigDiv.sol and Sqrt.sol contracts are deployed separately and their methods are accessed as external calls. This is more expensive than accessing the functions as libraries. Ie. library BigDiv and using BigDiv for uint.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"
    },
    {
        "title": "6.1 RocketDaoNodeTrusted - DAO takeover during deployment/bootstrapping ",
        "body": "  Resolution  The node registration is enabled by default (node.registration.enabled) but the client intends to change this to disabling the registration until bootstrap mode finished.  We are intending to set node registrations to false during deployment, then open it up when we need to register our oDAO nodes  Description  The initial deployer of the RocketStorage contract is set as the Guardian/Bootstrapping role. This guardian can bootstrap the TrustedNode and Protocol DAO, add members, upgrade components, change settings.  Right after deploying the DAO contract the member count is zero. The Guardian can now begin calling any of the bootstrapping functions to add members, change settings, upgrade components, interact with the treasury, etc. The bootstrapping configuration by the Guardian is unlikely to all happen within one transaction which might allow other parties to interact with the system while it is being set up.  RocketDaoNodeTrusted also implements a recovery mode that allows any registered node to invite themselves directly into the DAO without requiring approval from the Guardian or potential other DAO members as long as the total member count is below daoMemberMinCount (3). The Guardian itself is not counted as a DAO member as it is a supervisory role.  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrusted.sol:L202-L215  /**** Recovery ***************/  // In an explicable black swan scenario where the DAO loses more than the min membership required (3), this method can be used by a regular node operator to join the DAO  // Must have their ID, email, current RPL bond amount available and must be called by their current registered node account  function memberJoinRequired(string memory _id, string memory _email) override public onlyLowMemberMode onlyRegisteredNode(msg.sender) onlyLatestContract(\"rocketDAONodeTrusted\", address(this)) {  // Ok good to go, lets add them  (bool successPropose, bytes memory responsePropose) = getContractAddress('rocketDAONodeTrustedProposals').call(abi.encodeWithSignature(\"proposalInvite(string,string,address)\", _id, _email, msg.sender));  // Was there an error?  require(successPropose, getRevertMsg(responsePropose));  // Get the to automatically join as a member (by a regular proposal, they would have to manually accept, but this is no ordinary situation)  (bool successJoin, bytes memory responseJoin) = getContractAddress(\"rocketDAONodeTrustedActions\").call(abi.encodeWithSignature(\"actionJoinRequired(address)\", msg.sender));  // Was there an error?  require(successJoin, getRevertMsg(responseJoin));  This opens up a window during the bootstrapping phase where any Ethereum Address might be able to register as a node (RocketNodeManager.registerNode) if node registration is enabled (default=true) rushing into RocketDAONodeTrusted.memberJoinRequired adding themselves (up to 3 nodes) as trusted nodes to the DAO. The new DAO members can now take over the DAO by issuing proposals, waiting 2 blocks to vote/execute them (upgrade, change settings while Guardian is changing settings, etc.). The Guardian role can kick the new DAO members, however, they can invite themselves back into the DAO.  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettingsNode.sol:L19-L19  setSettingBool(\"node.registration.enabled\", true);  Recommendation  Disable the DAO recovery mode during bootstrapping. Disable node registration by default and require the guardian to enable it. Ensure that bootstrapDisable (in both DAO contracts) performs sanity checks as to whether the DAO bootstrapping finished and permissions can effectively be revoked without putting the DAO at risk or in an irrecoverable state (enough members bootstrapped, vital configurations like registration and other settings are configured, \u2026).  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.2 RocketTokenRETH - sandwiching opportunity on price updates    ",
        "body": "  Resolution  This issue is being addressed in a currently pending pull request. By introducing a delay between an rETH deposit and a subsequent transfer or burn, sandwiching a price update transaction is not possible anymore. Specifically, a deposit delay of circa one day is introduced:  https://github.com/rocket-pool/rocketpool/pull/201/files#diff-0387338dc5dd7edd0a03766cfdaaee42d021d4e781239d5ebbff359c81497839R146-R150  // This is called by the base ERC20 contract before all transfer, mint, and burns  function _beforeTokenTransfer(address from, address, uint256) internal override {  // Don't run check if this is a mint transaction  if (from != address(0)) {  // Check which block the user's last deposit was  bytes32 key = keccak256(abi.encodePacked(\"user.deposit.block\", from));  uint256 lastDepositBlock = getUint(key);  if (lastDepositBlock > 0) {  // Ensure enough blocks have passed  RocketDAOProtocolSettingsNetworkInterface rocketDAOProtocolSettingsNetwork = RocketDAOProtocolSettingsNetworkInterface(getContractAddress(\"rocketDAOProtocolSettingsNetwork\"));  uint256 blocksPassed = block.number.sub(lastDepositBlock);  require(blocksPassed > rocketDAOProtocolSettingsNetwork.getRethDepositDelay(), \"Not enough time has passed since deposit\");  // Clear the state as it's no longer necessary to check this until another deposit is made  deleteUint(key);  In the current version, it is correctly enforced that a deposit delay of zero is not possible.  Description  The rETH token price is not coupled to the amount of rETH tokens in circulation on the Ethereum chain. The price is reported by oracle nodes and committed to the system via a voting process. The price of rETH changes If 51% of nodes observe and submit the same price information. If nodes fail to find price consensus for a block, then the rETH price might be stale.  There is an opportunity for the user to front-run the price update right before it is committed. If the next price is higher than the previous (typical case), this gives an instant opportunity to perform a risk-free ETH -> rETH -> ETH exchange for profit. In the worst case, one could drain all the ETH held by the RocketTokenRETH contract + excess funds stored in the vault.  Note: there seems to be a \"network.submit.balances.frequency\" price and balance submission frequency of 24hrs. However, this frequency is not enforced, and it is questionable if it makes sense to pin the price for 24hrs.  Note: the total supply of the RocketTokenRETH contract may be completely disconnected from the reported total supply for RETH via oracle nodes.  Examples  The amount of ETH was only staked during this one process for the price update duration and unlikely to be useful to the system. This way, a whale (only limited by the max deposit amount set on deposit) can drain the RocketTokenRETH contract from all its ETH and excess eth funds.  mempool observed: submitPrice tx (an effective transaction that changes the price) wrapped with buying rETH and selling rETH for ETH:  RocketDepositPool.deposit() at old price => mints rETH at current rate  RocketNetworkPrices.submitPrices(newRate)  RocketTokenRETH.burn(balanceOf(msg.sender) => burns rETH for ETH at new rate  deposit (virtually no limit with 1000ETH being the limit right now)  rocketpool-2.5-Tokenomics-updates/contracts/contract/deposit/RocketDepositPool.sol:L63-L67  require(rocketDAOProtocolSettingsDeposit.getDepositEnabled(), \"Deposits into Rocket Pool are currently disabled\");  require(msg.value >= rocketDAOProtocolSettingsDeposit.getMinimumDeposit(), \"The deposited amount is less than the minimum deposit size\");  require(getBalance().add(msg.value) <= rocketDAOProtocolSettingsDeposit.getMaximumDepositPoolSize(), \"The deposit pool size after depositing exceeds the maximum size\");  // Mint rETH to user account  rocketTokenRETH.mint(msg.value, msg.sender);  trustedNodes submitPrice (changes params for getEthValue and getRethValue)  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkPrices.sol:L69-L72  RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  if (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {  updatePrices(_block, _rplPrice);  immediately burn at new rate (as params for getEthValue changed)  rocketpool-2.5-Tokenomics-updates/contracts/contract/token/RocketTokenRETH.sol:L107-L124  function burn(uint256 _rethAmount) override external {  // Check rETH amount  require(_rethAmount > 0, \"Invalid token burn amount\");  require(balanceOf(msg.sender) >= _rethAmount, \"Insufficient rETH balance\");  // Get ETH amount  uint256 ethAmount = getEthValue(_rethAmount);  // Get & check ETH balance  uint256 ethBalance = getTotalCollateral();  require(ethBalance >= ethAmount, \"Insufficient ETH balance for exchange\");  // Update balance & supply  _burn(msg.sender, _rethAmount);  // Withdraw ETH from deposit pool if required  withdrawDepositCollateral(ethAmount);  // Transfer ETH to sender  msg.sender.transfer(ethAmount);  // Emit tokens burned event  emit TokensBurned(msg.sender, _rethAmount, ethAmount, block.timestamp);  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.3 RocketDaoNodeTrustedActions - Incomplete implementation of member challenge process    ",
        "body": "  Resolution   As of the Smartnode s   Description  Nodes do not seem to monitor ActionChallengeMade events so that they could react to challenges  Nodes do not implement actionChallengeDecide and, therefore, cannot successfully stop a challenge  Funds/Tribute sent along with the challenge will be locked forever in the RocketDAONodeTrustedActions contract. There s no means to recover the funds.  It is questionable whether the incentives are aligned well enough for anyone to challenge stale nodes. The default of 1 eth compared to the risk of the  malicious  or  stale  node exiting themselves is quite high. The challenger is not incentivized to challenge someone other than for taking over the DAO. If the tribute is too low, this might incentivize users to grief trusted nodes and force them to close a challenge.  Requiring that the challenge initiator is a different registered node than the challenge finalized is a weak protection since the system is open to anyone to register as a node (even without depositing any funds.)  block time is subject to fluctuations. With the default of 43204 blocks, the challenge might expire at 5 days (10 seconds block time), 6.5 days (13 seconds Ethereum target median block time), 7 days (14 seconds), or more with historic block times going up to 20 seconds for shorter periods.  A minority of trusted nodes may use this functionality to boot other trusted node members off the DAO issuing challenges once a day until the DAO member number is low enough to allow them to reach quorum for their own proposals or until the member threshold allows them to add new nodes without having to go through the proposal process at all.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/settings/RocketDAONodeTrustedSettingsMembers.sol:L22-L24  setSettingUint('members.challenge.cooldown', 6172);              // How long a member must wait before performing another challenge, approx. 1 day worth of blocks  setSettingUint('members.challenge.window', 43204);               // How long a member has to respond to a challenge. 7 days worth of blocks  setSettingUint('members.challenge.cost', 1 ether);               // How much it costs a non-member to challenge a members node. It's free for current members to challenge other members.  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedActions.sol:L204-L206  // In the event that the majority/all of members go offline permanently and no more proposals could be passed, a current member or a regular node can 'challenge' a DAO members node to respond  // If it does not respond in the given window, it can be removed as a member. The one who removes the member after the challenge isn't met, must be another node other than the proposer to provide some oversight  // This should only be used in an emergency situation to recover the DAO. Members that need removing when consensus is still viable, should be done via the 'kick' method.  Recommendation  Implement the challenge-response process before enabling users to challenge other nodes. Implement means to detect misuse of this feature for griefing e.g. when one trusted node member forces another trusted node to defeat challenges over and over again (technical controls, monitoring).  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.4 RocketDAOProtocolSettings/RocketDAONodeTrustedSettings - anyone can set/overwrite settings until contract is declared  deployed    ",
        "body": "  Resolution  The client is aware of and acknowledges this potential issue. As with the current contracts the deployed flag is always set in the constructor and there will be no window for someone else to interact with the contract before this flag is set. The following statement was provided:  [\u2026] this method is purely to set the initial default vars. It shouldn t be run again due to the deployment flag being flagged incase that contract is upgraded and those default vars aren t removed.  Additionally, it was suggested to add safeguards to the access restricting modifier, to only allowing the guardian to change settings if a settings contract  forgets  to set the deployed flag in the constructor (Note: the deployed flag must be set with the deploing transaction or else there might be a window for someone to interact with the contract before it is fully configured).  Description  The onlyDAOProtocolProposal modifier guards all state-changing methods in this contract. However, analog to issue 6.5, the access control is disabled until the variable settingsNameSpace.deployed is set. If this contract is not deployed and configured in one transaction, anyone can update the contract while left unprotected on the blockchain.  See issue 6.5 for a similar issue.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettings.sol:L18-L23  modifier onlyDAOProtocolProposal() {  // If this contract has been initialised, only allow access from the proposals contract  if(getBool(keccak256(abi.encodePacked(settingNameSpace, \"deployed\")))) require(getContractAddress('rocketDAOProtocolProposals') == msg.sender, \"Only DAO Protocol Proposals contract can update a setting\");  _;  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/settings/RocketDAONodeTrustedSettings.sol:L18-L22  modifier onlyDAONodeTrustedProposal() {  // If this contract has been initialised, only allow access from the proposals contract  if(getBool(keccak256(abi.encodePacked(settingNameSpace, \"deployed\")))) require(getContractAddress('rocketDAONodeTrustedProposals') == msg.sender, \"Only DAO Node Trusted Proposals contract can update a setting\");  _;  There are at least 9 more occurrences of this pattern.  Recommendation  Restrict access to the methods to a temporary trusted account (e.g. guardian) until the system bootstrapping phase ends by setting deployed to true.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.5 RocketStorage - anyone can set/update values before the contract is initialized    ",
        "body": "  Resolution  Fixed by restricting access to the guardian while the contract is not yet initialized. The relevant changeset is rocket-pool/rocketpool@495a51f. The client provided the following statement:  tx.origin is only used in this deployment instance and should be safe since no external contracts are interacted with  The client is aware of the implication of using tx.origin and that the guardian should never be used to interact with third-party contracts as the contract may be able to impersonate the guardian changing settings in the storage contract during that transaction.  https://github.com/ConsenSys/rocketpool-audit-2021-03/blob/0a5f680ae0f4da0c5639a241bd1605512cba6004/rocketpool-rp3.0-updates/contracts/contract/RocketStorage.sol#L31-L32  Description  According to the deployment script, the contract is deployed, and settings are configured in multiple transactions. This also means that for a period of time, the contract is left unprotected on the blockchain. Anyone can delete/set any value in the centralized data store. An attacker might monitor the mempool for new deployments of the RocketStorage contract and front-run calls to contract.storage.initialised setting arbitrary values in the system.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketStorage.sol:L24-L31  modifier onlyLatestRocketNetworkContract() {  // The owner and other contracts are only allowed to set the storage upon deployment to register the initial contracts/settings, afterwards their direct access is disabled  if (boolStorage[keccak256(abi.encodePacked(\"contract.storage.initialised\"))] == true) {  // Make sure the access is permitted to only contracts in our Dapp  require(boolStorage[keccak256(abi.encodePacked(\"contract.exists\", msg.sender))], \"Invalid or outdated network contract\");  _;  Recommendation  Restrict access to the methods to a temporary trusted account (e.g. guardian) until the system bootstrapping phase ends by setting initialised to true.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.6 RocketDAOProposals - Unpredictable behavior due to short vote delay    Addressed",
        "body": "  Resolution   Addressed in branch   rocket-pool/rocketpool@b424ca1) by changing the default delay  Description  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/RocketDAOProposal.sol:L167-L170  require(_startBlock > block.number, \"Proposal start block must be in the future\");  require(_durationBlocks > 0, \"Proposal cannot have a duration of 0 blocks\");  require(_expiresBlocks > 0, \"Proposal cannot have a execution expiration of 0 blocks\");  require(_votesRequired > 0, \"Proposal cannot have a 0 votes required to be successful\");  The default vote delay configured in the system is 1 block.  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/settings/RocketDAONodeTrustedSettingsProposals.sol:L21-L21  setSettingUint('proposal.vote.delay.blocks', 1);                 // How long before a proposal can be voted on after it is created. Approx. Next Block  A vote is immediately passed when the required quorum is reached which allows it to be executed. This means that a group that is holding enough voting power can propose a change, wait for two blocks (block.number (of time of proposal creation) + configuredDelay (1) + 1 (for ACTIVE state), then vote and execute for the proposal to pass for it to take effect almost immediately after only 2 blocks (<30seconds).  Settings can be changed after 30 seconds which might be unpredictable for other DAO members and not give them enough time to oppose and leave the DAO.  Recommendation  The underlying issue is that users of the system can t be sure what the behavior of a function call will be, and this is because the behavior can change after two blocks. The only guarantee is that users can be sure the settings don t change for the next block if no proposal is active.  We recommend giving the user advance notice of changes with a delay. For example, all upgrades should require two steps with a mandatory time window between them. The first step merely broadcasts to users that a particular change is coming, and the second step commits that change after a suitable waiting period.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.7 RocketRewardPool - Unpredictable staking rewards as stake can be added just before claiming and rewards may be paid to to operators that do not provide a service to the system   Partially Addressed",
        "body": "  Resolution  Partially addressed in branch rp3.0-updates (rocket-pool/rocketpool@b424ca1) by changing the withdrawal requirements to 150% of the effective RPL.  The client provided the following statement:  Node operators can now only withdraw RPL above their 150% effective RPL stake.  Description  Nodes/TrustedNodes earn rewards based on the current share of the effective RPL stake provided backing the number of Minipools they run. The reward is paid out regardless of when the effective node stake was provided, as long as it is present just before the call to claim(). This means the reward does not take into account how long the stake was provided. The effective RPL stake is the nodes RPL stake capped at a maximum of halfDepositUserAmount * 150% * nr_of_minipools(node) / RPLPrice. If the node does not run any Minipools, the effective RPL stake is zero.  Since effective stake can be added just before calling the claim() method (effectively trying to get a reward for a period that passed without RPL being staked for the full duration), this might create an unpredictable outcome for other participants, as adding significant stake (requires creating Minipools and staking the max per pool; the stake is locked for at least the duration of a reward period rpl.rewards.claim.period.blocks) shifts the shares users get for the fixed total amount of rewards. This can be unfair if the first users claimed their reward, and then someone is artificially inflating the total amount of shares by adding more stake to get a bigger part of the remaining reward. However, this comes at the cost of the registered node having to create more Minipools to stake more, requiring an initial deposit (16ETH, or 0ETH under certain circumstances for trusted nodes) by the actor attempting to get a larger share of the rewards. The risk of losing funds for this actor, however, is rather low, as they can immediately dissolve() and close() the Minipool to refund their node deposit as NETH right after claiming the reward only losing the gas spent on the various transactions.  This can be extended to a node operator creating a Minipool and staking the maximum amount before calling claim to remove the Minipool right after, freeing up the ETH that was locked in the Minipool until the next reward period starts. The node operator is not providing any service to the network, loses some value in ETH for gas but may compensate that with the RPL staking rewards. If the node amassed a significant amount of RPL stake, they might even try to flash-loan enough ETH to spawn Minipools to inflate their effective stake and earn most of the rewards to return the loan RPL profit.  By staking just before claiming, the node effectively can earn rewards for 2 reward periods by only staking RPL for the duration of one period (claim the previous period, leave it in for 14 days, claim another period, withdraw).  The stake can be withdrawn at the earliest 14 days after staking. However, it can be added back at any time, and the stake addition takes effect immediately. This allows for optimizing the staking reward as follows (assuming we front-run other claimers to maximize profits and perform all transactions in one block):  Note that withdraw() can be called right at the time the new reward period starts:  rocketpool-2.5-Tokenomics-updates/contracts/contract/node/RocketNodeStaking.sol:L165-L166  require(block.number.sub(getNodeRPLStakedBlock(msg.sender)) >= rocketDAOProtocolSettingsRewards.getRewardsClaimIntervalBlocks(), \"The withdrawal cooldown period has not passed\");  // Get & check node's current RPL stake  Examples  A node may choose to register and stake some RPL to collect rewards but never actually provide registered node duties, e.g., operating a Minipool.  Node shares for a passed reward epoch are unpredictable as nodes may change their stake (adding) after/before users claim their rewards.  A node can maximize its rewards by adding stake just before claiming it  A node can stake to claim rewards, wait 14 days, withdraw, lend on a platform and return the stake in time to claim the next period.  Recommendation  Review the incentive model for the RPL rewards. Consider adjusting it so that nodes that provide a service get a better share of the rewards. Consider accruing rewards for the duration the stake was provided instead of taking a snapshot whenever the node calls claim(). Require stake to be locked for > 14 days instead of >=14 days (withdraw()) or have users skip the first reward period after staking.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.8 RocketNodeStaking - Node operators can reduce slashing impact by withdrawing excess staked RPL    ",
        "body": "  Resolution  The RocketNodeStaking.withdrawRPL method now reverts if a node operator attempts to withdraw an RPL amount that results in the leftover RPL stake being smaller than the maximum required stake. This prevents operators from withdrawing excess RPL to avoid the impact of a slashing.  https://github.com/rocket-pool/rocketpool/blob/rp3.0-updates/contracts/contract/node/RocketNodeStaking.sol#L187  Description  Oracle nodes update the Minipools  balance and progress it to the withdrawable state when they observe the minipools stake to become withdrawable. If the observed stakingEndBalance is less than the user deposit for that pool, the node operator is punished for the difference.  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolStatus.sol:L89-L94  rocketMinipoolManager.setMinipoolWithdrawalBalances(_minipoolAddress, _stakingEndBalance, nodeAmount);  // Apply node penalties by liquidating RPL stake  if (_stakingEndBalance < userDepositBalance) {  RocketNodeStakingInterface rocketNodeStaking = RocketNodeStakingInterface(getContractAddress(\"rocketNodeStaking\"));  rocketNodeStaking.slashRPL(minipool.getNodeAddress(), userDepositBalance - _stakingEndBalance);  The amount slashed is at max userDepositBalance - stakingEndBalance. The userDepositBalance is at least 16 ETH (minipool.half/.full) and at max 32 ETH (minipool.empty). The maximum amount to be slashed is therefore 32 ETH (endBalance = 0, minipool.empty).  https://github.com/ConsenSys/rocketpool-audit-2021-03/issues/32; note that the RPL token is potentially affected by a similar issue as one can stake RPL, wait for the cooldown period & wait for the price to change, and withdraw stake at higher RPL price/ETH). The  rocketpool-2.5-Tokenomics-updates/contracts/contract/node/RocketNodeStaking.sol:L188-L196  uint256 rplSlashAmount = calcBase.mul(_ethSlashAmount).div(rocketNetworkPrices.getRPLPrice());  // Cap slashed amount to node's RPL stake  uint256 rplStake = getNodeRPLStake(_nodeAddress);  if (rplSlashAmount > rplStake) { rplSlashAmount = rplStake; }  // Transfer slashed amount to auction contract  rocketVault.transferToken(\"rocketAuctionManager\", getContractAddress(\"rocketTokenRPL\"), rplSlashAmount);  // Update RPL stake amounts  decreaseTotalRPLStake(rplSlashAmount);  decreaseNodeRPLStake(_nodeAddress, rplSlashAmount);  If the node does not have a sufficient RPL stake to cover the losses, the slashing amount is capped at whatever amount of RPL the node has left staked.  The minimum amount of RPL a node needs to have staked if it operates minipools is calculated as follows:  rocketpool-2.5-Tokenomics-updates/contracts/contract/node/RocketNodeStaking.sol:L115-L120  // Calculate minimum RPL stake  return rocketDAOProtocolSettingsMinipool.getHalfDepositUserAmount()  .mul(rocketDAOProtocolSettingsNode.getMinimumPerMinipoolStake())  .mul(rocketMinipoolManager.getNodeMinipoolCount(_nodeAddress))  .div(rocketNetworkPrices.getRPLPrice());  With the current configuration, this would resolve in a minimum stake of 16 ETH * 0.1 (10% collateralization) * 1 (nr_minipools) * RPL_Price for a node operating 1 minipool. This means a node operator basically only needs to have 10% of 16 ETH staked to operate one minipool.  An operator can withdraw their stake at any time, but they have to wait at least 14 days after the last time they staked (cooldown period). They can, at max, withdraw all but the minimum stake required to run the pools (nr_of_minipools * 16 ETH * 10%). This also means that after the cooldown period, they can reduce their stake to 10% of the half deposit amount (16ETH), then perform a voluntary exit on ETH2 so that the minipool becomes withdrawable. If they end up with less than the userDepositBalance in staking rewards, they would only get slashed the 1.6 ETH at max (10% of 16ETH half deposit amount for 1 minipool) even though they incurred a loss that may be up to 32 ETH (empty Minipool empty amount).  Furthermore, if a node operator runs multiple minipools, let s say 5, then they would have to provide at least 5*16ETH*0.1 = 8ETH as a security guarantee in the form of staked RPL. If the node operator incurs a loss with one of their minipools, their 8 ETH RPL stake will likely be slashed in full. Their other - still operating - minipools are not backed by any RPL anymore, and they effectively cannot be slashed anymore. This means that a malicious node operator can create multiple minipools, stake the minimum amount of RPL, get slashed for one minipool, and still operate the others without having the minimum RPL needed to run the minipools staked (getNodeMinipoolLimit).  The RPL stake is donated to the RocketAuctionManager, where they can attempt to buy back RPL potentially at a discount.  Note: Staking more RPL (e.g., to add another Minipool) resets the cooldown period for the total RPL staked (not only for the newly added)  Recommendation  It is recommended to redesign the withdrawal process to prevent users from withdrawing their stake while slashable actions can still occur. A potential solution may be to add a locking period in the process. A node operator may schedule the withdrawal of funds, and after a certain time has passed, may withdraw them. This prevents the immediate withdrawal of funds that may need to be reduced while slashable events can still occur. E.g.:  A node operator requests to withdraw all but the minimum required stake to run their pools.  The funds are scheduled for withdrawal and locked until a period of X days has passed.  (optional) In this period, a slashable event occurs. The funds for compensation are taken from the user s stake including the funds scheduled for withdrawal.  After the time has passed, the node operator may call a function to trigger the withdrawal and get paid out.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.9 RocketTokenRPL - inaccurate inflation rate and potential for manipulation lowering the real APY    Addressed",
        "body": "  Resolution   The main issue was addressed in branch   rocket-pool/rocketpool@b424ca1) by recording the timestamp up to when inflation was updated to instead of the current block timestamp (  Description  RocketTokenRPL allows users to swap their fixed-rate tokens to the inflationary RocketTokenRPL ERC20 token via a swapToken function. The DAO defines the inflation rate of this token and is initially set to be 5% APY. This APY is configured as a daily inflation rate (APD) with the corresponding 1 day in blocks inflation interval in the rocketDAOProtocolSettingsInflation contract. The DAO members control the inflation settings.  Anyone can call inflationMintTokens to inflate the token, which mints tokens to the contracts RocketVault. Tokens are minted for discreet intervals since the last time inflationMintTokens was called (recorded as inflationCalcBlock). The inflation is then calculated for the passed intervals without taking the current not yet completed interval. However, the inflationCalcBlock is set to the current block.number, effectively skipping some  time /blocks of the APY calculation.  The more often inflationMintTokens is called, the higher the APY likelihood dropping below the configured 5%. In the worst case, one could manipulate the APY down to 2.45% (assuming that the APD for a 5% APY was configured) by calling inflationMintTokens close to the end of every second interval. This would essentially restart the APY interval at block.number, skipping blocks of the current interval that have not been accounted for.  Note: updating the inflation rate will directly affect past inflation intervals that have not been minted! this might be undesirable, and it could be considered to force an inflation mint if the APY changes  Note: if the interval is small enough and there is a history of unaccounted intervals to be minted, and the Ethereum network is congested, gas fees may be high and block limits hit, the calculations in the for loop might be susceptible to DoS the inflation mechanism because of gas constraints.  Note: The inflation seems only to be triggered regularly on RocketRewardsPool.claim (or at any point by external actors). If the price establishes based on the total supply of tokens, then this may give attackers an opportunity to front-run other users trading large amounts of RPL that may previously have calculated their prices based on the un-inflated supply.  Note: that the discrete interval-based inflation (e.g., once a day) might create dynamics that put pressure on users to trade their RPL in windows instead of consecutively  Examples  the inflation intervals passed is the number of completed intervals. The current interval that is started is not included.  rocketpool-2.5-Tokenomics-updates/contracts/contract/token/RocketTokenRPL.sol:L108-L119  function getInlfationIntervalsPassed() override public view returns(uint256) {  // The block that inflation was last calculated at  uint256 inflationLastCalculatedBlock = getInflationCalcBlock();  // Get the daily inflation in blocks  uint256 inflationInterval = getInflationIntervalBlocks();  // Calculate now if inflation has begun  if(inflationLastCalculatedBlock > 0) {  return (block.number).sub(inflationLastCalculatedBlock).div(inflationInterval);  }else{  return 0;  the inflation calculation calculates the to-be-minted tokens for the inflation rate at newTokens = supply * rateAPD^intervals - supply  rocketpool-2.5-Tokenomics-updates/contracts/contract/token/RocketTokenRPL.sol:L126-L148  function inflationCalculate() override public view returns (uint256) {  // The inflation amount  uint256 inflationTokenAmount = 0;  // Optimisation  uint256 inflationRate = getInflationIntervalRate();  // Compute the number of inflation intervals elapsed since the last time we minted infation tokens  uint256 intervalsSinceLastMint = getInlfationIntervalsPassed();  // Only update  if last interval has passed and inflation rate is > 0  if(intervalsSinceLastMint > 0 && inflationRate > 0) {  // Our inflation rate  uint256 rate = inflationRate;  // Compute inflation for total inflation intervals elapsed  for (uint256 i = 1; i < intervalsSinceLastMint; i++) {  rate = rate.mul(inflationRate).div(10 ** 18);  // Get the total supply now  uint256 totalSupplyCurrent = totalSupply();  // Return inflation amount  inflationTokenAmount = totalSupplyCurrent.mul(rate).div(10 ** 18).sub(totalSupplyCurrent);  // Done  return inflationTokenAmount;  Recommendation  Properly track inflationCalcBlock as the end of the previous interval, as this is up to where the inflation was calculated, instead of the block at which the method was invoked.  Ensure APY/APD and interval configuration match up. Ensure the interval is not too small (potential gas DoS blocking inflation mint and RocketRewardsPool.claim).  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.10 Trusted node participation risk and potential client  optimizations     ",
        "body": "  Resolution  The development team considers this issue fixed as monitoring on the correct behaviour of node software is added to the system.  Description  The system might end up in a stale state with minipools never being setWithdrawable or network and prices being severely outdated because trusted nodes don t fulfill their duty of providing oracle values. Minipools not being able to advance to the Withdrawable state will severely harm the system as no rewards can be paid out. Outdated balances and prices may affect token economics around the tokens involved (specifically rETH price depends on oracle observations).  There is an incentive to be an oracle node as you get paid to provide oracle node duties when enrolled with the DAO. However, it is not enforced that nodes actually fulfill their duty of calling the respective onlyTrustedNode oracle functions to submit prices/balances/minipool rewards.  Therefore, a smart Rocket Pool trusted node operator might consider patching their client software to not or only sporadically fulfill their duties to save considerable amounts of gas, making more profit than other trusted nodes would.  There is no means to directly incentivize trusted nodes to call certain functions as they get their rewards anyway. The only risk they run is that other trusted nodes might detect their antisocial behavior and attempt to kick them out of the DAO. To detect this, monitoring tools and processes need to be established; it is questionable whether users would participate in high maintenance DAO operators.  Furthermore, trusted nodes might choose to gas optimize their submissions to avoid calling the actual action once quorum was established. They can, for example, attempt to submit prices as early as possible, avoiding that they re the first to hit the 51% threshold.  Recommendation  Create monitoring tools and processes to detect participants that do not fulfill their trusted DAO duties. Create direct incentives for trusted nodes to provide oracle services by, e.g., recording their participation rate and only payout rewards based on how active they are.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.11 RocketDAONodeTrustedUpgrade - upgrade does not prevent the use of the same address multiple times creating an inconsistency where getContractAddress returns outdated information    ",
        "body": "  Resolution   A check has been introduced to make sure that the new contract address is not already in use by checking against the corresponding   Description  When adding a new contract, it is checked whether the address is already in use. This check is missing when upgrading a named contract to a new implementation, potentially allowing someone to register one address to multiple names creating an inconsistent configuration.  The crux of this is, that, getContractAddress() will now return a contract address that is not registered anymore (while getContractName may throw). getContractAddress can therefore not relied upon when checking ACL.  add contract name=test, address=0xfefe  >  sets contract.exists.0xfefe=true  sets contract.name.0xfefe=test  sets contract.address.test=0xfefe  sets contract.abi.test=abi  add another contract name=badcontract, address=0xbadbad  > sets contract.exists.0xbadbad=true sets contract.name.0xbadbad=badcontract sets contract.address.badcontract=0xbadbad sets contract.abi.badcontract=abi  update contract name=test, address=0xbadbad reusing badcontradcts address, the address is now bound to 2 names (test, badcontract) overwrites contract.exists.0xbadbad=true` (even though its already true) updates contract.name.0xbadbad=test (overwrites the reference to badcontract; badcontracts config is now inconsistent) updates contract.address.test=0xbadbad (ok, expected) updates contract.abi.test=abi (ok, expected) removes contract.name.0xfefe (ok) removes contract.exists.0xfefe (ok)  update contract name=test, address=0xc0c0 sets contract.exists.0xc0c0=true sets contract.name.0xc0c0=test (ok, expected) updates contract.address.test=0xc0c0 (ok, expected) updates contract.abi.test=abi (ok, expected) removes contract.name.0xbadbad (the contract is still registered as badcontract, but is indirectly removed now) removes contract.exists.0xbadbad (the contract is still registered as badcontract, but is indirectly removed now)  After this, badcontract is partially cleared, getContractName(0xbadbad) throws while getContractAddress(badcontract) returns 0xbadbad which is already unregistered (contract.exists.0xbadbad=false)  Examples  check in `_addContract``  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedUpgrade.sol:L76-L76  require(_contractAddress != address(0x0), \"Invalid contract address\");  no checks in upgrade.  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedUpgrade.sol:L53-L59  require(_contractAddress != address(0x0), \"Invalid contract address\");  require(_contractAddress != oldContractAddress, \"The contract address cannot be set to its current address\");  // Register new contract  setBool(keccak256(abi.encodePacked(\"contract.exists\", _contractAddress)), true);  setString(keccak256(abi.encodePacked(\"contract.name\", _contractAddress)), _name);  setAddress(keccak256(abi.encodePacked(\"contract.address\", _name)), _contractAddress);  setString(keccak256(abi.encodePacked(\"contract.abi\", _name)), _contractAbi);  Recommendation  Check that the address being upgraded to is not yet registered and properly clean up contract.address.<name|abi>.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.12 Rocketpool CLI - Lax data validation and output sanitation    Addressed",
        "body": "  Resolution  Addressed with v1.0.0-rc1 by sanitizing non-printables from strings stored in the smart contract.  This effectively mitigates terminal-based control character injection attacks. However, might still be used to inject context-sensitive information that may be consumed by different protocols/presentation layers (web, terminal by displaying falsified information next to fields).  E-mail and timezone format validation was introduced with https://github.com/rocket-pool/rocketpool-go/blob/c8738633ab973503b79c7dee5c2f78d7e44e48ae/dao/trustednode/proposals.go#L22 and rocket-pool/rocketpool-go@6e72501.  It is recommended to further tighten the checks on untrusted information enforcing an expected format of information and reject to interact with nodes/data that does not comply with the expected formats (e.g. email being in an email format, timezone information is a valid timezone, and does not contain extra information, \u2026).  Description  ValidateTimezoneLocation and ValidateDAOMemberEmail are only used to validate user input from the command line. Timezone location information and member email addresses are stored in the smart contract s string storage, e.g., using the setTimezoneLocation function of the RocketNodeManager contract. This function only validates that a minimum length of 4 has been given.  Through direct interaction with the contract, an attacker can submit arbitrary information, which is not validated on the CLI s side. With additional integrations of the Rocketpool smart contracts, the timezone location field may be used by an attacker to inject malicious code (e.g., for cross-site scripting attacks) or injecting false information (e.g. Balance: 1000 RPL or Status: Trusted), which is directly displayed on a user-facing application.  On the command line, control characters such as newline characters can be injected to alter how text is presented to the user, effectively exploiting user trust in the official application.  Examples  rocketpool-go-2.5-Tokenomics/node/node.go:L134-L153  wg.Go(func() error {  var err error  timezoneLocation, err = GetNodeTimezoneLocation(rp, nodeAddress, opts)  return err  })  // Wait for data  if err := wg.Wait(); err != nil {  return NodeDetails{}, err  // Return  return NodeDetails{  Address: nodeAddress,  Exists: exists,  WithdrawalAddress: withdrawalAddress,  TimezoneLocation: timezoneLocation,  }, nil  smartnode-2.5-Tokenomics/rocketpool-cli/odao/members.go:L34-L44  for _, member := range members.Members {  fmt.Printf(\"--------------------\\n\")  fmt.Printf(\"\\n\")  fmt.Printf(\"Member ID:            %s\\n\", member.ID)  fmt.Printf(\"Email address:        %s\\n\", member.Email)  fmt.Printf(\"Joined at block:      %d\\n\", member.JoinedBlock)  fmt.Printf(\"Last proposal block:  %d\\n\", member.LastProposalBlock)  fmt.Printf(\"RPL bond amount:      %.6f\\n\", math.RoundDown(eth.WeiToEth(member.RPLBondAmount), 6))  fmt.Printf(\"Unbonded minipools:   %d\\n\", member.UnbondedValidatorCount)  fmt.Printf(\"\\n\")  Recommendation  Validate user input before storing it on the blockchain. Validate and sanitize stored user tainted data before presenting it. Establish a register of data validation rules (e.g., email format, timezone format, etc.). Reject nodes operating with nodes that do not honor data validation rules.  Validate the correct format of variables (e.g., timezone location, email, name, \u2026) on the storage level (if applicable) and the lowest level of the go library to offer developers a strong foundation to build on and mitigate the risk in future integrations. Furthermore, on-chain validation might not be implemented (due to increased gas consumption) should be mentioned in the developer documentation security section as they need to be handled with special caution by consumer applications. Sanitize output before presenting it to avoid control character injections in terminal applications or other presentation technologies (e.g., SQL or HTML).  Review all usage of the fmt lib (especially Sprintf and string handling/concatenating functions). Ensure only sanitized data can reach this sink. Review the logging library and ensure it is hardened against control character injection by encoding non-printables and CR-LF.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.13 Rocketpool CLI - Various command injection vectors    Addressed",
        "body": "  Resolution   Initially, the client implemented the suggested fix using   https://github.com/rocket-pool/smartnode/compare/extra-escapes.  Description  Various commands in the Rocketpool CLI make use of the readOutput and printOutput functions. These do not perform sanitization of user-supplied inputs and allow an attacker to supply malicious values which can be used to execute arbitrary commands on the user s system.  Examples  All commands using the Client.readOutput, Client.printOutput and Client.compose functions are affected.  Furthermore, Client.callAPI is used for API-related calls throughout the Rocketpool service. However, it does not validate that the values passed into it are valid API commands. This can lead to arbitrary command execution, also inside the container using docker exec.  Recommendation  Perform strict validation on all user-supplied parameters. If parameter values need to be inserted into a command template string, the %q format string or other restrictive equivalents should be used.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.14 RocketStorage - Risk concentration by giving all registered contracts permissions to change any settings in RocketStorage   ",
        "body": "  Resolution  The client provided the following statement:  We ve looked at adding access control contracts using namespaces, but the increase in gas usage would be significant and could hinder upgrades.  Description  The ACL for changing settings in the centralized RocketStorage allows any registered contract (listed under contract.exists) to change settings that belong to other parts of the system.  The concern is that if someone finds a way to add their malicious contract to the registered contact list, they will override any setting in the system. The storage is authoritative when checking certain ACLs. Being able to set any value might allow an attacker to gain control of the complete system. Allowing any contract to overwrite other contracts  settings dramatically increases the attack surface.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketStorage.sol:L24-L32  modifier onlyLatestRocketNetworkContract() {  // The owner and other contracts are only allowed to set the storage upon deployment to register the initial contracts/settings, afterwards their direct access is disabled  if (boolStorage[keccak256(abi.encodePacked(\"contract.storage.initialised\"))] == true) {  // Make sure the access is permitted to only contracts in our Dapp  require(boolStorage[keccak256(abi.encodePacked(\"contract.exists\", msg.sender))], \"Invalid or outdated network contract\");  _;  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketStorage.sol:L78-L85  function setAddress(bytes32 _key, address _value) onlyLatestRocketNetworkContract override external {  addressStorage[_key] = _value;  /// @param _key The key for the record  function setUint(bytes32 _key, uint _value) onlyLatestRocketNetworkContract override external {  uIntStorage[_key] = _value;  Recommendation  Allow contracts to only change settings related to their namespace.  ",
        "labels": [
            "Consensys",
            "Major",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.15 RocketDAOProposals - require a minimum participation quorum for DAO proposals    Addressed",
        "body": "  Resolution  Addressed by requiring the DAO minimum viable user count as the minium quorum with rocket-pool/rocketpool@11bc18c (in bootstrap mode). The check for the bootstrap mode has since been removed following our remark  [\u2026] the problem here was not so much the bootstrap mode but rather that the dao membership may fall below the recovery mode threshold. The question is, whether it should still be allowed to propose and execute votes if the memberCount at proposal time is below that treshold (e.g. malicious member boots off other members, sends new proposals (quorum required=1), dao membrers rejoin but cannot reject that proposal anymore). Question is if quorum should be at least the recovery treshold.  And the following feedback from the client:  [\u2026] had that as allowed to happen if bootstrap mode was enabled. I ve just disabled the check for bootstrap mode now so that any proposals can t be made if the min member count is below the amount required. This means new members can only be added in this case via the emergency join function before new proposals can be added  Description  If the DAO falls below the minimum viable membership threshold, voting for proposals still continues as DAO proposals do not require a minimum participation quorum. In the worst case, this would allow the last standing DAO member to create a proposal that would be passable with only one vote even if new members would be immediately ready to join via the recovery mode (which has its own risks) as the minimum votes requirement for proposals is set as >0.  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/RocketDAOProposal.sol:L170-L170  require(_votesRequired > 0, \"Proposal cannot have a 0 votes required to be successful\");  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedProposals.sol:L57-L69  function propose(string memory _proposalMessage, bytes memory _payload) override public onlyTrustedNode(msg.sender) onlyLatestContract(\"rocketDAONodeTrustedProposals\", address(this)) returns (uint256) {  // Load contracts  RocketDAOProposalInterface daoProposal = RocketDAOProposalInterface(getContractAddress('rocketDAOProposal'));  RocketDAONodeTrustedInterface daoNodeTrusted = RocketDAONodeTrustedInterface(getContractAddress('rocketDAONodeTrusted'));  RocketDAONodeTrustedSettingsProposalsInterface rocketDAONodeTrustedSettingsProposals = RocketDAONodeTrustedSettingsProposalsInterface(getContractAddress(\"rocketDAONodeTrustedSettingsProposals\"));  // Check this user can make a proposal now  require(daoNodeTrusted.getMemberLastProposalBlock(msg.sender).add(rocketDAONodeTrustedSettingsProposals.getCooldown()) <= block.number, \"Member has not waited long enough to make another proposal\");  // Record the last time this user made a proposal  setUint(keccak256(abi.encodePacked(daoNameSpace, \"member.proposal.lastblock\", msg.sender)), block.number);  // Create the proposal  return daoProposal.add(msg.sender, 'rocketDAONodeTrustedProposals', _proposalMessage, block.number.add(rocketDAONodeTrustedSettingsProposals.getVoteDelayBlocks()), rocketDAONodeTrustedSettingsProposals.getVoteBlocks(), rocketDAONodeTrustedSettingsProposals.getExecuteBlocks(), daoNodeTrusted.getMemberQuorumVotesRequired(), _payload);  Sidenote: Since a proposals acceptance quorum is recorded on proposal creation, this may lead to another scenario where proposals acceptance quorum may never be reached if members leave the DAO. This would require a re-submission of the proposal.  Recommendation  Do not accept proposals if the member count falls below the minimum DAO membercount threshold.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.16 RocketDAONodeTrustedUpgrade - inconsistent upgrade blacklist    Addressed",
        "body": "  Resolution   Addressed in branch   rocket-pool/rocketpool@b424ca1) by updating the blacklist.  Description  upgradeContract defines a hardcoded list of contracts that cannot be upgraded because they manage their own settings (statevars) or they hold value in the system.  the list is hardcoded and cannot be extended when new contracts are added via addcontract. E.g. what if another contract holding value is added to the system? This would require an upgrade of the upgrade contract to update the whitelist (gas hungry, significant risk of losing access to the upgrade mechanisms if a bug is being introduced).  a contract named rocketPoolToken is blacklisted from being upgradeable but the system registers no contract called rocketPoolToken. This may be an oversight or artifact of a previous iteration of the code. However, it may allow a malicious group of nodes to add a contract that is not yet in the system which cannot be removed anymore as there is no removeContract functionality and upgradeContract to override the malicious contract will fail due to the blacklist.  Note that upgrading RocketTokenRPL requires an account balance migration as contracts in the system may hold value in RPL (e.g. a lot in AuctionManager) that may vanish after an upgrade. The contract is not exempt from upgrading. A migration may not be easy to perform as the system cannot be paused to e.g. snapshot balances.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedUpgrade.sol:L41-L49  function _upgradeContract(string memory _name, address _contractAddress, string memory _contractAbi) internal {  // Check contract being upgraded  bytes32 nameHash = keccak256(abi.encodePacked(_name));  require(nameHash != keccak256(abi.encodePacked(\"rocketVault\")),        \"Cannot upgrade the vault\");  require(nameHash != keccak256(abi.encodePacked(\"rocketPoolToken\")),    \"Cannot upgrade token contracts\");  require(nameHash != keccak256(abi.encodePacked(\"rocketTokenRETH\")),     \"Cannot upgrade token contracts\");  require(nameHash != keccak256(abi.encodePacked(\"rocketTokenNETH\")), \"Cannot upgrade token contracts\");  require(nameHash != keccak256(abi.encodePacked(\"casperDeposit\")),      \"Cannot upgrade the casper deposit contract\");  // Get old contract address & check contract exists  Recommendation  Consider implementing a whitelist of contracts that are allowed to be upgraded instead of a more error-prone blacklist of contracts that cannot be upgraded.  Provide documentation that outlines what contracts are upgradeable and why.  Create a process to verify the blacklist before deploying/operating the system.  Plan for migration paths when upgrading contracts in the system  Any proposal that reaches the upgrade contract must be scrutinized for potential malicious activity (e.g. as any registered contract can directly modify storage or may contain subtle backdoors. Upgrading without performing a thorough security inspection may easily put the DAO at risk)  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.17 RocketDAONodeTrustedActions - member cannot be kicked if the vault does not hold enough RPL to cover the bond    ",
        "body": "  Resolution   Addressed in branch   rocket-pool/rocketpool@b424ca1) by returning the bond if enough RPL is in the treasury and else continue without returning the bond. This way the member kick action does not block and the member can be kicked regardless of the RPL balance.  Description  If a DAO member behaves badly other DAO members may propose the node be evicted from the DAO. If for some reason, RocketVault does not hold enough RPL to pay back the DAO member bond actionKick will throw. The node is not evicted.  Now this is a somewhat exotic scenario as the vault should always hold the bond for the members in the system. However, if the node was kicked for stealing RPL (e.g. passing an upgrade proposal to perform an attack) it might be impossible to execute the eviction.  Recommendation  Ensure that there is no way a node can influence a succeeded kick proposal to fail. Consider burning the bond (by keeping it) as there is a reason for evicting the node or allow them to redeem it in a separate step.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.18 RocketMinipoolStatus - DAO Membership changes can result in votes getting stuck    ",
        "body": "  Resolution   This issue has been fixed in PR   https://github.com/ConsenSys/rocketpool-audit-2021-03/issues/204 by introducing a public method that allows anyone to manually trigger a DAO consensus threshold check and a subsequent balance update in case the issue s example scenario occurs.  Description  Changes in the DAO s trusted node members are reflected in the RocketDAONodeTrusted.getMemberCount() function. When compared with the vote on consensus threshold, a DAO-driven decision is made, e.g., when updating token price feeds and changing Minipool states.  Especially in the early phase of the DAO, the functions below can get stuck as execution is restricted to DAO members who have not voted yet. Consider the following scenario:  The DAO consists of five members  Two members vote to make a Minipool withdrawable  The other three members are inactive, the community votes, and they get kicked from the DAO  The two remaining members have no way to change the Minipool state now. All method calls to trigger the state update fails because the members have already voted before.  Note: votes of members that are kicked/leave are still count towards the quorum!  Examples  Setting a Minipool into the withdrawable state:  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolStatus.sol:L62-L65  RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  if (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {  setMinipoolWithdrawable(_minipoolAddress, _stakingStartBalance, _stakingEndBalance);  Submitting a block s network balances:  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkBalances.sol:L94-L97  RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  if (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {  updateBalances(_block, _totalEth, _stakingEth, _rethSupply);  Submitting a block s RPL price information:  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkPrices.sol:L69-L72  RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  if (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {  updatePrices(_block, _rplPrice);  Recommendation  The conditional check and update of price feed information, Minipool state transition, etc., should be externalized into a separate public function. This function is also called internally in the existing code. In case the DAO gets into the scenario above, anyone can call the function to trigger a reevaluation of the condition with updated membership numbers and thus get the process unstuck.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.19 Trusted/Oracle-Nodes can vote multiple times for different outcomes ",
        "body": "  Description  Trusted/oracle nodes submit various ETH2 observations to the RocketPool contracts. When 51% of nodes submitted the same observation, the result is stored in the contract. However, while it is recorded that a node already voted for a specific minipool (being withdrawable & balance) or block (price/balance), a re-submission with different parameters for the same minipool/block is not rejected.  Since the oracle values should be distinct, clear, and there can only be one valid value, it should not be allowed for trusted nodes to change their mind voting for multiple different outcomes within one block or one minipool  Examples  RocketMinipoolStatus - a trusted node can submit multiple different results for one minipool  Note that setBool(keccak256(abi.encodePacked(\"minipool.withdrawable.submitted.node\", msg.sender, _minipoolAddress)), true);  is recorded but never checked. (as for the other two instances)  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolStatus.sol:L48-L57  // Get submission keys  bytes32 nodeSubmissionKey = keccak256(abi.encodePacked(\"minipool.withdrawable.submitted.node\", msg.sender, _minipoolAddress, _stakingStartBalance, _stakingEndBalance));  bytes32 submissionCountKey = keccak256(abi.encodePacked(\"minipool.withdrawable.submitted.count\", _minipoolAddress, _stakingStartBalance, _stakingEndBalance));  // Check & update node submission status  require(!getBool(nodeSubmissionKey), \"Duplicate submission from node\");  setBool(nodeSubmissionKey, true);  setBool(keccak256(abi.encodePacked(\"minipool.withdrawable.submitted.node\", msg.sender, _minipoolAddress)), true);  // Increment submission count  uint256 submissionCount = getUint(submissionCountKey).add(1);  setUint(submissionCountKey, submissionCount);  RocketNetworkBalances - a trusted node can submit multiple different results for the balances at a specific block  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkBalances.sol:L80-L92  // Get submission keys  bytes32 nodeSubmissionKey = keccak256(abi.encodePacked(\"network.balances.submitted.node\", msg.sender, _block, _totalEth, _stakingEth, _rethSupply));  bytes32 submissionCountKey = keccak256(abi.encodePacked(\"network.balances.submitted.count\", _block, _totalEth, _stakingEth, _rethSupply));  // Check & update node submission status  require(!getBool(nodeSubmissionKey), \"Duplicate submission from node\");  setBool(nodeSubmissionKey, true);  setBool(keccak256(abi.encodePacked(\"network.balances.submitted.node\", msg.sender, _block)), true);  // Increment submission count  uint256 submissionCount = getUint(submissionCountKey).add(1);  setUint(submissionCountKey, submissionCount);  // Emit balances submitted event  emit BalancesSubmitted(msg.sender, _block, _totalEth, _stakingEth, _rethSupply, block.timestamp);  // Check submission count & update network balances  RocketNetworkPrices - a trusted node can submit multiple different results for the price at a specific block  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkPrices.sol:L55-L67  // Get submission keys  bytes32 nodeSubmissionKey = keccak256(abi.encodePacked(\"network.prices.submitted.node\", msg.sender, _block, _rplPrice));  bytes32 submissionCountKey = keccak256(abi.encodePacked(\"network.prices.submitted.count\", _block, _rplPrice));  // Check & update node submission status  require(!getBool(nodeSubmissionKey), \"Duplicate submission from node\");  setBool(nodeSubmissionKey, true);  setBool(keccak256(abi.encodePacked(\"network.prices.submitted.node\", msg.sender, _block)), true);  // Increment submission count  uint256 submissionCount = getUint(submissionCountKey).add(1);  setUint(submissionCountKey, submissionCount);  // Emit prices submitted event  emit PricesSubmitted(msg.sender, _block, _rplPrice, block.timestamp);  // Check submission count & update network prices  Recommendation  Only allow one vote per minipool/block. Don t give nodes the possibility to vote multiple times for different outcomes.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.20 RocketTokenNETH - Pot. discrepancy between minted tokens and deposited collateral    ",
        "body": "  Resolution  This issue is obsoleted by the fact that the nETH contract was removed completely. The client provided the following statement:  nETH has been removed completely.  Description  The nETH token is paid to node operators when minipool becomes withdrawable. nETH is supposed to be backed by ETH 1:1. However, in most cases, this will not be the case.  The nETH minting and deposition of collateral happens in two different stages of a minipool. nETH is minted in the minipool state transition from Staking to Withdrawable when the trusted/oracle nodes find consensus on the fact that the minipool became withdrawable (submitWinipoolWithdrawable).  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolStatus.sol:L63-L65  if (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {  setMinipoolWithdrawable(_minipoolAddress, _stakingStartBalance, _stakingEndBalance);  When consensus is found on the state of the minipool, nETH tokens are minted to the minipool address according to the withdrawal amount observed by the trusted/oracle nodes. At this stage, ETH backing the newly minted nETH was not yet provided.  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolStatus.sol:L80-L87  uint256 nodeAmount = getMinipoolNodeRewardAmount(  minipool.getNodeFee(),  userDepositBalance,  minipool.getStakingStartBalance(),  minipool.getStakingEndBalance()  );  // Mint nETH to minipool contract  if (nodeAmount > 0) { rocketTokenNETH.mint(nodeAmount, _minipoolAddress); }  The minipool.receive() function receives the ETH  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipool.sol:L109-L112  receive() external payable {  (bool success, bytes memory data) = getContractAddress(\"rocketMinipoolDelegate\").delegatecall(abi.encodeWithSignature(\"receiveValidatorBalance()\"));  if (!success) { revert(getRevertMessage(data)); }  and forwards it to minipooldelegate.receiveValidatorBalance  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolDelegate.sol:L227-L231  require(msg.sender == rocketDAOProtocolSettingsNetworkInterface.getSystemWithdrawalContractAddress(), \"The minipool's validator balance can only be sent by the eth1 system withdrawal contract\");  // Set validator balance withdrawn status  validatorBalanceWithdrawn = true;  // Process validator withdrawal for minipool  rocketNetworkWithdrawal.processWithdrawal{value: msg.value}();  Which calculates the nodeAmount based on the ETH received and submits it as collateral to back the previously minted nodeAmount of nETH.  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkWithdrawal.sol:L46-L60  uint256 totalShare = rocketMinipoolManager.getMinipoolWithdrawalTotalBalance(msg.sender);  uint256 nodeShare = rocketMinipoolManager.getMinipoolWithdrawalNodeBalance(msg.sender);  uint256 userShare = totalShare.sub(nodeShare);  // Get withdrawal amounts based on shares  uint256 nodeAmount = 0;  uint256 userAmount = 0;  if (totalShare > 0) {  nodeAmount = msg.value.mul(nodeShare).div(totalShare);  userAmount = msg.value.mul(userShare).div(totalShare);  // Set withdrawal processed status  rocketMinipoolManager.setMinipoolWithdrawalProcessed(msg.sender);  // Transfer node balance to nETH contract  if (nodeAmount > 0) { rocketTokenNETH.depositRewards{value: nodeAmount}(); }  // Transfer user balance to rETH contract or deposit pool  Looking at how the nodeAmount of nETH that was minted was calculated and comparing it to how nodeAmount of ETH is calculated, we can observe the following:  the nodeAmount of nETH minted is an absolute number of tokens based on the rewards observed by the trusted/oracle nodes. the nodeAmount is stored in the storage and later used to calculate the collateral deposit in a later step.  the nodeAmount calculated when depositing the collateral is first assumed to be a nodeShare (line 47), while it is actually an absolute number. the nodeShare is then turned into a nodeAmount relative to the ETH supplied to the contract.  Due to rounding errors, this might not always exactly match the nETH minted (see https://github.com/ConsenSys/rocketpool-audit-2021-03/issues/26).  The collateral calculation is based on the ETH value provided to the contract. If this value does not exactly match what was reported by the oracle/trusted nodes when minting nETH, less/more collateral will be provided.  Note: excess collateral will be locked in the nETH contract as it is unaccounted for in the nETH token contract and therefore cannot be redeemed. Note: providing less collateral will go unnoticed and mess up the 1:1 nETH:ETH peg. In the worst case, there will be less nETH than ETH. Not everybody will be able to redeem their ETH.  Note: keep in mind that the receive() function might be subject to gas restrictions depending on the implementation of the withdrawal contract (.call() vs. .transfer())  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolDelegate.sol:L201-L210  uint256 nethBalance = rocketTokenNETH.balanceOf(address(this));  if (nethBalance > 0) {  // Get node withdrawal address  RocketNodeManagerInterface rocketNodeManager = RocketNodeManagerInterface(getContractAddress(\"rocketNodeManager\"));  address nodeWithdrawalAddress = rocketNodeManager.getNodeWithdrawalAddress(nodeAddress);  // Transfer  require(rocketTokenNETH.transfer(nodeWithdrawalAddress, nethBalance), \"nETH balance was not successfully transferred to node operator\");  // Emit nETH withdrawn event  emit NethWithdrawn(nodeWithdrawalAddress, nethBalance, block.timestamp);  For reference, depositRewards (providing collateral) and mint are not connected at all, hence the risk of nETH being an undercollateralized token.  rocketpool-2.5-Tokenomics-updates/contracts/contract/token/RocketTokenNETH.sol:L28-L42  function depositRewards() override external payable onlyLatestContract(\"rocketNetworkWithdrawal\", msg.sender) {  // Emit ether deposited event  emit EtherDeposited(msg.sender, msg.value, block.timestamp);  // Mint nETH  // Only accepts calls from the RocketMinipoolStatus contract  function mint(uint256 _amount, address _to) override external onlyLatestContract(\"rocketMinipoolStatus\", msg.sender) {  // Check amount  require(_amount > 0, \"Invalid token mint amount\");  // Update balance & supply  _mint(_to, _amount);  // Emit tokens minted event  emit TokensMinted(_to, _amount, block.timestamp);  Recommendation  It looks like nETH might not be needed at all, and it should be discussed if the added complexity of having a potentially out-of-sync nETH token contract is necessary and otherwise remove it from the contract system as the nodeAmount of ETH can directly be paid out to the withdrawalAddress in the receiveValidatorBalance or withdraw transitions.  If nETH cannot be removed, consider minting nodeAmount of nETH directly to withdrawalAddress on withdraw instead of first minting uncollateralized tokens. This will also reduce the gas footprint of the Minipool.  Ensure that the initial nodeAmount calculation matches the minted nETH and deposited to the contract as collateral (absolute amount vs. fraction).  Enforce that nETH requires collateral to be provided when minting tokens.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.21 RocketMiniPoolDelegate - on destroy() leftover ETH is sent to RocketVault where it cannot be recovered    ",
        "body": "  Resolution  Leftover ETH is now sent to the node operator address as expected.  https://github.com/ConsenSys/rocketpool-audit-2021-03/blob/0a5f680ae0f4da0c5639a241bd1605512cba6004/rocketpool-rp3.0-updates/contracts/contract/minipool/RocketMinipoolDelegate.sol#L294  Description  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolDelegate.sol:L314-L321  // Destroy the minipool  function destroy() private {  // Destroy minipool  RocketMinipoolManagerInterface rocketMinipoolManager = RocketMinipoolManagerInterface(getContractAddress(\"rocketMinipoolManager\"));  rocketMinipoolManager.destroyMinipool();  // Self destruct & send any remaining ETH to vault  selfdestruct(payable(getContractAddress(\"rocketVault\")));  Recommendation  Implement means to recover and reuse ETH that was forcefully sent to the contract by MiniPool instances.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.22 RocketDAO - personally identifiable member information (PII) stored on-chain   ",
        "body": "  Resolution  Acknowledged with the following statement:  This is by design, need them to be publicly accountable. We ll advise their node should not be running on the same machine as their email software though.  Description  Like a DAO user s e-mail address, PII is stored on-chain and can, therefore, be accessed by anyone. This may allow de-pseudonymize users (and correlate Ethereum addresses to user email addresses) and be used for spamming or targeted phishing campaigns putting the DAO users at risk.  Examples  rocketpool-go-2.5-Tokenomics/dao/trustednode/dao.go:L173-L183  // Return  return MemberDetails{  Address: memberAddress,  Exists: exists,  ID: id,  Email: email,  JoinedBlock: joinedBlock,  LastProposalBlock: lastProposalBlock,  RPLBondAmount: rplBondAmount,  UnbondedValidatorCount: unbondedValidatorCount,  }, nil  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrusted.sol:L110-L112  function getMemberEmail(address _nodeAddress) override public view returns (string memory) {  return getString(keccak256(abi.encodePacked(daoNameSpace, \"member.email\", _nodeAddress)));  Recommendation  Avoid storing PII on-chain where it is readily available for anyone.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.23 Rocketpool CLI - Insecure SSH HostKeyCallback    ",
        "body": "  Resolution  A proper host key callback function to validate the remote party s authenticity is now defined.  https://github.com/ConsenSys/rocketpool-audit-2021-03/blob/0a5f680ae0f4da0c5639a241bd1605512cba6004/smartnode-1.0.0-rc1/shared/services/rocketpool/client.go#L114-L117  Description  The SSH client factory returns instances that have an insecure HostKeyCallback set. This means that SSH servers  public key will not be validated and thus initialize a potentially insecure connection. The function should not be used for production code.  Examples  smartnode-2.5-Tokenomics/shared/services/rocketpool/client.go:L87  HostKeyCallback: ssh.InsecureIgnoreHostKey(),  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.24 Deployment - Docker containers running as root ",
        "body": "  Description  By default, Docker containers run commands as the root user. This means that there is little to no resistance for an attacker who has managed to break into the container and execute commands. This effectively negates file permissions already set into the system, such as storing wallet-related information with 0600 as an attacker will most likely drop into the container as root already.  Examples  Missing USER instructions affect both SmartNode Dockerfiles:  smartnode-2.5-Tokenomics/docker/rocketpool-dockerfile:L25-L36  # Start from ubuntu image  FROM ubuntu:20.10  # Install OS dependencies  RUN apt-get update && apt-get install -y ca-certificates  # Copy binary  COPY --from=builder /go/bin/rocketpool /go/bin/rocketpool  # Container entry point  ENTRYPOINT [\"/go/bin/rocketpool\"]  smartnode-2.5-Tokenomics/docker/rocketpool-pow-proxy-dockerfile:L24-L35  # Start from ubuntu image  FROM ubuntu:20.10  # Install OS dependencies  RUN apt-get update && apt-get install -y ca-certificates  # Copy binary  COPY --from=builder /go/bin/rocketpool-pow-proxy /go/bin/rocketpool-pow-proxy  # Container entry point  ENTRYPOINT [\"/go/bin/rocketpool-pow-proxy\"]  Recommendation  In the Dockerfiles, create an unprivileged user and use the USER instruction to switch. Only then, the entrypoint launching the SmartNode or the POW Proxy should be defined.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.25 RocketPoolMinipool - should check for address(0x0)    ",
        "body": "  Resolution   Addressed in branch   rocket-pool/rocketpool@b424ca1) by changing requiring that the contract address is not  Description  The two implementations for getContractAddress() in Minipool/Delegate are not checking whether the requested contract s address was ever set before. If it were never set, the method would return address(0x0), which would silently make all delegatecalls succeed without executing any code. In contrast, RocketBase.getContractAddress() fails if the requested contract is not known.  It should be noted that this can happen if rocketMinipoolDelegate is not set in global storage, or it was cleared afterward, or if _rocketStorageAddress points to a contract that implements a non-throwing fallback function (may not even be storage at all).  Examples  Missing checks  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipool.sol:L170-L172  function getContractAddress(string memory _contractName) private view returns (address) {  return rocketStorage.getAddress(keccak256(abi.encodePacked(\"contract.address\", _contractName)));  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolDelegate.sol:L91-L93  function getContractAddress(string memory _contractName) private view returns (address) {  return rocketStorage.getAddress(keccak256(abi.encodePacked(\"contract.address\", _contractName)));  Checks implemented  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketBase.sol:L84-L92  function getContractAddress(string memory _contractName) internal view returns (address) {  // Get the current contract address  address contractAddress = getAddress(keccak256(abi.encodePacked(\"contract.address\", _contractName)));  // Check it  require(contractAddress != address(0x0), \"Contract not found\");  // Return  return contractAddress;  Recommendation  Similar to RocketBase.getContractAddress() require that the contract is set.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.26 RocketDAONodeTrustedAction - ambiguous event emitted in actionChallengeDecide    ",
        "body": "  Resolution  Instead of emitting an event even though the challenge period has not passed yet, the function call will now revert if the challenge window has not passed yet.  Description  actionChallengeDecide succeeds and emits challengeSuccess=False in case the challenged node defeats the challenge. It also emits the same event if another node calls actionChallengeDecided before the refute window passed. This ambiguity may make a defeated challenge indistinguishable from a challenge that was attempted to be decided too early (unless the component listening for the event also checks the refute window).  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedActions.sol:L244-L260  // Allow the challenged member to refute the challenge at anytime. If the window has passed and the challenge node does not run this method, any member can decide the challenge and eject the absent member  // Is it the node being challenged?  if(_nodeAddress == msg.sender) {  // Challenge is defeated, node has responded  deleteUint(keccak256(abi.encodePacked(daoNameSpace, \"member.challenged.block\", _nodeAddress)));  }else{  // The challenge refute window has passed, the member can be ejected now  if(getUint(keccak256(abi.encodePacked(daoNameSpace, \"member.challenged.block\", _nodeAddress))).add(rocketDAONodeTrustedSettingsMembers.getChallengeWindow()) < block.number) {  // Node has been challenged and failed to respond in the given window, remove them as a member and their bond is burned  _memberRemove(_nodeAddress);  // Challenge was successful  challengeSuccess = true;  // Log it  emit ActionChallengeDecided(_nodeAddress, msg.sender, challengeSuccess, block.timestamp);  Recommendation  Avoid ambiguities when emitting events. Consider throwing an exception in the else branch if the refute window has not passed yet (minimal gas savings; it s clear that the call failed; other components can rely on the event only being emitted if there was a decision.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.27 RocketDAOProtocolProposals, RocketDAONodeTrustedProposals - unused enum ProposalType    ",
        "body": "  Resolution   Addressed in branch   rocket-pool/rocketpool@b424ca1) by removing the unused code.  Description  The enum ProposalType is defined but never used.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedProposals.sol:L29-L35  enum ProposalType {  Invite,             // Invite a registered node to join the trusted node DAO  Leave,              // Leave the DAO  Replace,            // Replace a current trusted node with a new registered node, they take over their bond  Kick,               // Kick a member from the DAO with optional penalty applied to their RPL deposit  Setting             // Change a DAO setting (Quorum threshold, RPL deposit size, voting periods etc)  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/protocol/RocketDAOProtocolProposals.sol:L28-L31  enum ProposalType {  Setting             // Change a DAO setting (Node operator min/max fees, inflation rate etc)  Recommendation  Remove unnecessary code.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.28 RocketDaoNodeTrusted - Unused events    ",
        "body": "  Resolution   Addressed in branch   rocket-pool/rocketpool@b424ca1) by removing the unused code.  Description  The MemberJoined MemberLeave events are not used within RocketDaoNodeTrusted.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrusted.sol:L19-L23  // Events  event MemberJoined(address indexed _nodeAddress, uint256 _rplBondAmount, uint256 time);  event MemberLeave(address indexed _nodeAddress, uint256 _rplBondAmount, uint256 time);  Recommendation  Consider removing the events. Note: RocketDAONodeTrustedAction is emitting ActionJoin and ActionLeave event.s  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.29 RocketDAOProposal - expired, and defeated proposals can be canceled    ",
        "body": "  Resolution  Proposals can now only be cancelled if they are pending or active.  Description  The method emits an event that might trigger other components to perform actions.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/RocketDAOProposal.sol:L155-L159  } else {  // Check the votes, was it defeated?  // if (votesFor <= votesAgainst || votesFor < getVotesRequired(_proposalID))  return ProposalState.Defeated;  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/RocketDAOProposal.sol:L239-L250  function cancel(address _member, uint256 _proposalID) override public onlyDAOContract(getDAO(_proposalID)) {  // Firstly make sure this proposal that hasn't already been executed  require(getState(_proposalID) != ProposalState.Executed, \"Proposal has already been executed\");  // Make sure this proposal hasn't already been successful  require(getState(_proposalID) != ProposalState.Succeeded, \"Proposal has already succeeded\");  // Only allow the proposer to cancel  require(getProposer(_proposalID) == _member, \"Proposal can only be cancelled by the proposer\");  // Set as cancelled now  setBool(keccak256(abi.encodePacked(daoProposalNameSpace, \"cancelled\", _proposalID)), true);  // Log it  emit ProposalCancelled(_proposalID, _member, block.timestamp);  Recommendation  Preserve the true outcome. Do not allow to cancel proposals that are already in an end-state like canceled, expired, defeated.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.30 RocketDAOProposal - preserve the proposals correct state after expiration    ",
        "body": "  Resolution   Proposals that have been defeated now will show up as such even when expired. The new default value is   Description  The state of proposals is resolved to give a preference to a proposal being expired over the actual result which may be defeated. The preference for a proposal s status is checked in order: cancelled? -> executed? -> expired? -> succeeded? -> pending? -> active? -> defeated (default)  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/RocketDAOProposal.sol:L137-L159  if (getCancelled(_proposalID)) {  // Cancelled by the proposer?  return ProposalState.Cancelled;  // Has it been executed?  } else if (getExecuted(_proposalID)) {  return ProposalState.Executed;  // Has it expired?  } else if (block.number >= getExpires(_proposalID)) {  return ProposalState.Expired;  // Vote was successful, is now awaiting execution  } else if (votesFor >= getVotesRequired(_proposalID)) {  return ProposalState.Succeeded;  // Is the proposal pending? Eg. waiting to be voted on  } else if (block.number <= getStart(_proposalID)) {  return ProposalState.Pending;  // The proposal is active and can be voted on  } else if (block.number <= getEnd(_proposalID)) {  return ProposalState.Active;  } else {  // Check the votes, was it defeated?  // if (votesFor <= votesAgainst || votesFor < getVotesRequired(_proposalID))  return ProposalState.Defeated;  Recommendation  consider checking for voteAgainst explicitly and return defeated instead of expired if a proposal was defeated and is queried after expiration. Preserve the actual proposal result.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.31 RocketRewardsPool - registerClaimer should check if a node is already disabled before decrementing rewards.pool.claim.interval.claimers.total.next    ",
        "body": "  Resolution   In the case a submitted   Description  The other branch in registerClaimer does not check whether the provided _claimerAddress is already disabled (or invalid). This might lead to inconsistencies where rewards.pool.claim.interval.claimers.total.next is decremented because the caller provided an already deactivated address.  This issue is flagged as minor since we have not found an exploitable version of this issue in the current codebase. However, we recommend safeguarding the implementation instead of relying on the caller to provide sane parameters. Registered Nodes cannot unregister, and Trusted Nodes are unregistered when they leave.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/rewards/RocketRewardsPool.sol:L296-L316  function registerClaimer(address _claimerAddress, bool _enabled) override external onlyClaimContract {  // The name of the claiming contract  string memory contractName = getContractName(msg.sender);  // Record the block they are registering at  uint256 registeredBlock = 0;  // How many users are to be included in next interval  uint256 claimersIntervalTotalUpdate = getClaimingContractUserTotalNext(contractName);  // Ok register  if(_enabled) {  // Make sure they are not already registered  require(getClaimingContractUserRegisteredBlock(contractName, _claimerAddress) == 0, \"Claimer is already registered\");  // Update block number  registeredBlock = block.number;  // Update the total registered claimers for next interval  setUint(keccak256(abi.encodePacked(\"rewards.pool.claim.interval.claimers.total.next\", contractName)), claimersIntervalTotalUpdate.add(1));  }else{  setUint(keccak256(abi.encodePacked(\"rewards.pool.claim.interval.claimers.total.next\", contractName)), claimersIntervalTotalUpdate.sub(1));  // Save the registered block  setUint(keccak256(abi.encodePacked(\"rewards.pool.claim.contract.registered.block\", contractName, _claimerAddress)), registeredBlock);  Recommendation  Ensure that getClaimingContractUserRegisteredBlock(contractName, _claimerAddress) returns !=0 before decrementing the .total.next.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.32 RocketNetworkPrices - Price feed update lacks block number sanity check    ",
        "body": "  Resolution   Addressed in branch   rocket-pool/rocketpool@b424ca1) by only allowing price submissions for blocks in the range of  Description  Trusted nodes submit the RPL price feed. The function is called specifying a block number and the corresponding RPL price for that block. If a DAO vote goes through for that block-price combination, it is written to storage. In the unlikely scenario that a vote confirms a very high block number such as uint(-1), all future price updates will fail due to the require check below.  This issue becomes less likely the more active members the DAO has. Thus, it s considered a minor issue that mainly affects the initial bootstrapping process.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkPrices.sol:L53-L54  // Check block  require(_block > getPricesBlock(), \"Network prices for an equal or higher block are set\");  Recommendation  The function s _block parameter should be checked to prevent large block numbers from being submitted. This check could, e.g., specify that node operators are only allowed to submit price updates for a maximum of x blocks ahead of block.number.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.33 RocketDepositPool - Potential gasDoS in assignDeposits   ",
        "body": "  Resolution  The client acknowledges this issue.  Description  assignDeposits seems to be a gas heavy function, with many external calls in general, and few of them are inside the for loop itself. By default, rocketDAOProtocolSettingsDeposit.getMaximumDepositAssignments() returns 2, which is not a security concern. Through a DAO vote, the settings key deposit.assign.maximum can be set to a value that exhausts the block gas limit and effectively deactivates the deposit assignment process.  rocketpool-2.5-Tokenomics-updates/contracts/contract/deposit/RocketDepositPool.sol:L115-L116  for (uint256 i = 0; i < rocketDAOProtocolSettingsDeposit.getMaximumDepositAssignments(); ++i) {  // Get & check next available minipool capacity  Recommendation  The rocketDAOProtocolSettingsDeposit.getMaximumDepositAssignments() return value could be cached outside the loop. Additionally, a check should be added that prevents unreasonably high values.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.34 RocketNetworkWithdrawal - ETH dust lockup due to rounding errors    ",
        "body": "  Resolution   Addressed in branch   rocket-pool/rocketpool@b424ca1) by calculating  Description  There s a potential ETH dust lockup when processing a withdrawal due to rounding errors when performing a division.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkWithdrawal.sol:L46-L55  uint256 totalShare = rocketMinipoolManager.getMinipoolWithdrawalTotalBalance(msg.sender);  uint256 nodeShare = rocketMinipoolManager.getMinipoolWithdrawalNodeBalance(msg.sender);  uint256 userShare = totalShare.sub(nodeShare);  // Get withdrawal amounts based on shares  uint256 nodeAmount = 0;  uint256 userAmount = 0;  if (totalShare > 0) {  nodeAmount = msg.value.mul(nodeShare).div(totalShare);  userAmount = msg.value.mul(userShare).div(totalShare);  Recommendation  Calculate userAmount as msg.value - nodeAmount instead. This should also save some gas.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.35 RocketAuctionManager - calcBase should be declared constant    ",
        "body": "  Resolution   Addressed in branch   rocket-pool/rocketpool@b424ca1) by declaring  Description  Declaring the same constant value calcBase multiple times as local variables to some methods in RocketAuctionManager carries the risk that if that value is ever updated, one of the value assignments might be missed. It is therefore highly recommended to reduce duplicate code and declare the value as a public constant. This way, it is clear that the same calcBase is used throughout the contract, and there is a single point of change in case it ever needs to be changed.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/auction/RocketAuctionManager.sol:L136-L139  function getLotPriceByTotalBids(uint256 _index) override public view returns (uint256) {  uint256 calcBase = 1 ether;  return calcBase.mul(getLotTotalBidAmount(_index)).div(getLotTotalRPLAmount(_index));  rocketpool-2.5-Tokenomics-updates/contracts/contract/auction/RocketAuctionManager.sol:L151-L154  function getLotClaimedRPLAmount(uint256 _index) override public view returns (uint256) {  uint256 calcBase = 1 ether;  return calcBase.mul(getLotTotalBidAmount(_index)).div(getLotCurrentPrice(_index));  rocketpool-2.5-Tokenomics-updates/contracts/contract/auction/RocketAuctionManager.sol:L173-L174  // Calculation base value  uint256 calcBase = 1 ether;  rocketpool-2.5-Tokenomics-updates/contracts/contract/auction/RocketAuctionManager.sol:L216-L217  uint256 bidAmount = msg.value;  uint256 calcBase = 1 ether;  rocketpool-2.5-Tokenomics-updates/contracts/contract/auction/RocketAuctionManager.sol:L247-L249  // Calculate RPL claim amount  uint256 calcBase = 1 ether;  uint256 rplAmount = calcBase.mul(bidAmount).div(currentPrice);  Recommendation  Consider declaring calcBase as a private const state var instead of re-declaring it with the same value in multiple, multiple functions. Constant, literal state vars are replaced in a preprocessing step and do not require significant additional gas when accessed than normal state vars.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.36 RocketDAO* - daoNamespace is missing a trailing dot; should be declared constant/immutable    ",
        "body": "  Resolution   Addressed in branch   rocket-pool/rocketpool@b424ca1) by adding a trailing dot to the  Description  string private daoNameSpace = 'dao.trustednodes' is missing a trailing dot, or else there s no separator when concatenating the namespace with the vars.  Examples  requests dao.trustednodesmember.index instead of dao.trustednodes.member.index  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrusted.sol:L83-L86  function getMemberAt(uint256 _index) override public view returns (address) {  AddressSetStorageInterface addressSetStorage = AddressSetStorageInterface(getContractAddress(\"addressSetStorage\"));  return addressSetStorage.getItem(keccak256(abi.encodePacked(daoNameSpace, \"member.index\")), _index);  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedActions.sol:L32-L33  // The namespace for any data stored in the trusted node DAO (do not change)  string private daoNameSpace = 'dao.trustednodes';  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedProposals.sol:L22-L26  // Calculate using this as the base  uint256 private calcBase = 1 ether;  // The namespace for any data stored in the trusted node DAO (do not change)  string private daoNameSpace = 'dao.trustednodes';  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/protocol/RocketDAOProtocol.sol:L12-L13  // The namespace for any data stored in the network DAO (do not change)  string private daoNameSpace = 'dao.protocol';  Recommendation  Remove the daoNameSpace and add the prefix to the respective variables directly.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.37 RocketVault - consider rejecting zero amount deposit/withdrawal requests    ",
        "body": "  Resolution  Addressed in branch rp3.0-updates (rocket-pool/rocketpool@b424ca1) by requiring that ETH and tokenAmounts are not zero.  Note that components that used to raise no exception when attempting to deposit/withdraw/transfer zero amount tokens/ETH may now throw which can be used to block certain functionalities (slashAmount==0).  The client provided the following statement:  We ll double check this. Currently the only way a slashAmount is 0 is if we allow node operators to not stake RPL (min 10% required currently). Though there isn t a check for 0 in the slash function atm, I ll add one now just as a safety check.  Description  Consider disallowing zero amount token transfers unless the system requires this to work. In most cases, zero amount token transfers will emit an event (that potentially triggers off-chain components). In some cases, they allow the caller without holding any balance to call back to themselves (pot. reentrancy) or the caller provided token address.  depositEther allows to deposit zero ETH  emits EtherDeposited  withdrawEther allows to withdraw zero ETH  calls back to withdrawer (msg.sender)! emits EtherWithdrawn  (depositToken checks for amount >0)  withdrawToken allows zero amount token withdrawals  calls into user provided (actually a network contract) tokenAddress) emits TokenWithdrawn  transferToken allows zero amount token transfers  emits TokenTransfer  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketVault.sol:L50-L57  function depositEther() override external payable onlyLatestNetworkContract {  // Get contract key  bytes32 contractKey = keccak256(abi.encodePacked(getContractName(msg.sender)));  // Update contract balance  etherBalances[contractKey] = etherBalances[contractKey].add(msg.value);  // Emit ether deposited event  emit EtherDeposited(contractKey, msg.value, block.timestamp);  Recommendation  Zero amount transfers are no-operation calls in most cases and should be avoided. However, as all vault actions are authenticated (to registered system contracts), the risk of something going wrong is rather low. Nevertheless, it is recommended to deny zero amount transfers to avoid running code unnecessarily (gas consumption), emitting unnecessary events, or potentially call back to callers/token address for ineffective transfers.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.38 RocketVault - methods returning static return values and unchecked return parameters    ",
        "body": "  Resolution  The unused boolean return values have been removed and reverts have been introduced instead.  Description  The Token* methods in RocketVault either throw or return true, but they can never return false. If the method fails, it will always throw. Therefore, it is questionable if the static return value is needed at all. Furthermore, callees are in most cases not checking the return value of  Examples  static return value true  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketVault.sol:L93-L96  // Emit token transfer  emit TokenDeposited(contractKey, _tokenAddress, _amount, block.timestamp);  // Done  return true;  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketVault.sol:L113-L115  emit TokenWithdrawn(contractKey, _tokenAddress, _amount, block.timestamp);  // Done  return true;  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketVault.sol:L134-L137  // Emit token withdrawn event  emit TokenTransfer(contractKeyFrom, contractKeyTo, _tokenAddress, _amount, block.timestamp);  // Done  return true;  return value not checked  rocketpool-2.5-Tokenomics-updates/contracts/contract/node/RocketNodeStaking.sol:L149-L150  rocketVault.depositToken(\"rocketNodeStaking\", rplTokenAddress, _amount);  // Update RPL stake amounts & node RPL staked block  rocketpool-2.5-Tokenomics-updates/contracts/contract/auction/RocketAuctionManager.sol:L252-L252  rocketVault.withdrawToken(msg.sender, getContractAddress(\"rocketTokenRPL\"), rplAmount);  rocketpool-2.5-Tokenomics-updates/contracts/contract/node/RocketNodeStaking.sol:L172-L172  rocketVault.withdrawToken(msg.sender, getContractAddress(\"rocketTokenRPL\"), _amount);  rocketpool-2.5-Tokenomics-updates/contracts/contract/node/RocketNodeStaking.sol:L193-L193  rocketVault.transferToken(\"rocketAuctionManager\", getContractAddress(\"rocketTokenRPL\"), rplSlashAmount);  Recommendation  Define a clear interface for these functions. Remove the static return value in favor of having the method throw on failure (which is already the current behavior).  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.39 Deployment - Overloaded Ubuntu base image ",
        "body": "  Description  The SmartNode and the corresponding proxy Dockerfiles base their builds on the ubuntu:20.10 image. This image introduces many unrelated tools that significantly increase the container s attack surface and the tools an attacker has at their disposal once they have gained access to the container. Some of these tools include:  apt  bash/sh  perl  Examples  smartnode-2.5-Tokenomics/docker/rocketpool-dockerfile:L26-L26  FROM ubuntu:20.10  smartnode-2.5-Tokenomics/docker/rocketpool-pow-proxy-dockerfile:L25-L25  FROM ubuntu:20.10  Recommendation  Consider using a smaller and more restrictive base image such as Alpine. Additionally, AppArmor or Seccomp policies should be used to prevent unexpected and potentially malicious activities during the container s lifecycle. As an illustrative example, a SmartNode container does not need to load/unload kernel modules or loading a BPF to capture network traffic.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "6.40 RocketMinipoolDelegate - enforce that the delegate contract cannot be called directly    ",
        "body": "  Resolution   Addressed in branch   rocket-pool/rocketpool@b424ca1) by removing the constructor and therefore the initialization code from the RocketMinipoolDelegate contract. The contract cannot be used directly anymore as all relevant methods are decorated  Description  This contract is not meant to be consumed directly and will only be delegate called from Minipool. Being able to call it directly might even create the problem that, in the worst case, someone might be able to selfdestruct the contract rendering all other contracts that link to it dysfunctional. This might even not be easily detectable because delegatecall to an EOA will act as a NOP.  The access control checks on the methods currently prevent methods from being called directly on the delegate. They require state variables to be set correctly, or the delegate is registered as a valid minipool in the system. Both conditions are improbable to be fulfilled, hence, mitigation any security risk. However, it looks like this is more of a side-effect than a design decision, and we would recommend not explicitly stating that the delegate contract cannot be used directly.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolDelegate.sol:L65-L70  constructor(address _rocketStorageAddress) {  // Initialise RocketStorage  require(_rocketStorageAddress != address(0x0), \"Invalid storage address\");  rocketStorage = RocketStorageInterface(_rocketStorageAddress);  Recommendation  Remove the initialization from the constructor in the delegate contract. Consider adding a flag that indicates that the delegate contract is initialized and only set in the Minipool contract and not in the logic contract (delegate). On calls, check that the contract is initialized.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"
    },
    {
        "title": "5.1 TimeLock spam prevention can be bypassed    Addressed",
        "body": "  Resolution   This was addressed in   commit aa6fc49fbf3230d7f02956b33a3150c6885ee93f by parsing the input evm script and ensuring only a single external call is made. Additionally,  commit 453179e98159413d38196b6a5373cdd729483567 added  Description  The TimeLock app is a forwarder that requires users to lock some token before forwarding an EVM callscript. Its purpose is to introduce a  spam penalty  to hamper repeat actions within an Aragon org. In the context of a Dandelion org, this spam penalty is meant to stop users from repeatedly creating votes in DandelionVoting, as subsequent votes are buffered by a configurable number of blocks (DandelionVoting.bufferBlocks). Spam prevention is important, as the more votes are buffered, the longer it takes before  non-spam  votes are able to be executed.  By allowing arbitrary calls to be executed, the TimeLock app opens several potential vectors for bypassing spam prevention.  Examples  Using a callscript to transfer locked tokens to the sender  By constructing a callscript that executes a call to the lock token address, the sender execute calls to the lock token on behalf of TimeLock. Any function can be executed, making it possible to not only transfer  locked  tokens back to the sender, but also steal other users  locked tokens by way of transfer.  Using a batched callscript to call DandelionVoting.newVote repeatedly  Callscripts can be batched, meaning they can execute multiple calls before finishing. Within a Dandelion org, the spam prevention mechanism is used for the DandelionVoting.newVote function. A callscript that batches multiple calls to this function can execute newVote several times per call to TimeLock.forward. Although multiple new votes are created, only one spam penalty is incurred, making it trivial to extend the buffer imposed on  non-spam  votes.  Using a callscript to re-enter TimeLock and forward or withdrawAllTokens to itself  A callscript can be used to re-enter TimeLock.forward, as well as any other TimeLock functions. Although this may not be directly exploitable, it does seem unintentional that many of the TimeLock contract functions are accessible to itself in this manner.  Recommendation  Add the TimeLock contract s own address to the evmscript blacklist  Add the TimeLock lock token address to the evmscript blacklist  To fix spamming through batched callscripts, one option is to have users pass in a destination and calldata, and manually perform a call. Alternatively, CallsScript can be forked and altered to only execute a single external call to a single destination.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/12/dandelion-organizations/"
    },
    {
        "title": "5.2 Passing duplicate tokens to Redemptions and TokenRequest may have unintended consequences    Addressed",
        "body": "  Resolution  This was addressed in Redemptions commit 2b0034206a5b9cdf239da7a51900e89d9931554f by checking redeemableTokenAdded[token] == false for each subsequent token added during initialization. Note that ordering is not enforced.  Additionally, the issue in TokenRequest was addressed in commit eb4181961093439f142f2e74eb706b7f501eb5c0 by requiring that each subsequent token added during initialization has a value strictly greater than the previous token added.  Description  Both Redemptions and TokenRequest are initialized with a list of acceptable tokens to use with each app. For Redemptions, the list of tokens corresponds to an organization s treasury assets. For TokenRequest, the list of tokens corresponds to tokens accepted for payment to join an organization. Neither contract makes a uniqueness check on input tokens during initialization, which can lead to unintended behavior.  Examples  In Redemptions, each of an organization s assets are redeemed according to the sender s proportional ownership in the org. The redemption process iterates over the redeemableTokens list, paying out the sender their proportion of each token listed:  code/redemptions-app/contracts/Redemptions.sol:L112-L121  for (uint256 i = 0; i < redeemableTokens.length; i++) {  vaultTokenBalance = vault.balance(redeemableTokens[i]);  redemptionAmount = _burnableAmount.mul(vaultTokenBalance).div(burnableTokenTotalSupply);  totalRedemptionAmount = totalRedemptionAmount.add(redemptionAmount);  if (redemptionAmount > 0) {  vault.transfer(redeemableTokens[i], msg.sender, redemptionAmount);  If a token address is included more than once, the sender will be paid out more than once, potentially earning many times more than their proportional share of the token.  In TokenRequest, this behavior does not allow for any significant deviation from expected behavior. It was included because the initialization process is similar to that of Redemptions.  Recommendation  During initialization in both apps, check that input token addresses are unique. One simple method is to require that token addresses are submitted in ascending order, and that each subsequent address added is greater than the one before.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/12/dandelion-organizations/"
    },
    {
        "title": "5.3 The Delay app allows scripts to be paused even after execution time has elapsed    Addressed",
        "body": "  Resolution   This was addressed in   commit 46d8fa414cc3e68c68a5d9bc1174be5f32970611 by requiring that the current timestamp is before the delayed script s execution time.  Description  The Delay app is used to configure a delay between when an evm script is created and when it is executed. The entry point for this process is Delay.delayExecution, which stores the input script with a future execution date:  code/delay-app/contracts/Delay.sol:L153-L162  function _delayExecution(bytes _evmCallScript) internal returns (uint256) {  uint256 delayedScriptIndex = delayedScriptsNewIndex;  delayedScriptsNewIndex++;  delayedScripts[delayedScriptIndex] = DelayedScript(getTimestamp64().add(executionDelay), 0, _evmCallScript);  emit DelayedScriptStored(delayedScriptIndex);  return delayedScriptIndex;  An auxiliary capability of the Delay app is the ability to  pause  the delayed script, which sets the script s pausedAt value to the current block timestamp:  code/delay-app/contracts/Delay.sol:L80-L85  function pauseExecution(uint256 _delayedScriptId) external auth(PAUSE_EXECUTION_ROLE) {  require(!_isExecutionPaused(_delayedScriptId), ERROR_CAN_NOT_PAUSE);  delayedScripts[_delayedScriptId].pausedAt = getTimestamp64();  emit ExecutionPaused(_delayedScriptId);  A paused script cannot be executed until resumeExecution is called, which extends the script s executionTime by the amount of time paused. Essentially, the delay itself is paused:  code/delay-app/contracts/Delay.sol:L91-L100  function resumeExecution(uint256 _delayedScriptId) external auth(RESUME_EXECUTION_ROLE) {  require(_isExecutionPaused(_delayedScriptId), ERROR_CAN_NOT_RESUME);  DelayedScript storage delayedScript = delayedScripts[_delayedScriptId];  uint64 timePaused = getTimestamp64().sub(delayedScript.pausedAt);  delayedScript.executionTime = delayedScript.executionTime.add(timePaused);  delayedScript.pausedAt = 0;  emit ExecutionResumed(_delayedScriptId);  A delayed script whose execution time has passed and is not currently paused should be able to be executed via the execute function. However, the pauseExecution function still allows the aforementioned script to be paused, halting execution.  Recommendation  Add a check to pauseExecution to ensure that execution is not paused if the script s execution delay has already transpired.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/12/dandelion-organizations/"
    },
    {
        "title": "5.4 Misleading intentional misconfiguration possible through misuse of newToken and newBaseInstance    Addressed",
        "body": "  Resolution   This was addressed in   commit b68d89ab0deb22161987e19d1ff0bb9d7303f0a9 by making  Description  The instantiation process for a Dandelion organization requires two separate external calls to DandelionOrg. There are two primary functions: installDandelionApps, and newTokenAndBaseInstance.  installDandelionApps relies on cached results from prior calls to newTokenAndBaseInstance and completes the initialization step for a Dandelion org.  newTokenAndBaseInstance is a wrapper around two publicly accessible functions: newToken and newBaseInstance. Called together, the functions:  Deploy a new MiniMeToken used to represent shares in an organization, and cache the address of the created token:  code/dandelion-org/contracts/DandelionOrg.sol:L128-L137  /**  @dev Create a new MiniMe token and save it for the user  @param _name String with the name for the token used by share holders in the organization  @param _symbol String with the symbol for the token used by share holders in the organization  /  function newToken(string memory _name, string memory _symbol) public returns (MiniMeToken) {  MiniMeToken token = _createToken(_name, _symbol, TOKEN_DECIMALS);  _saveToken(token);  return token;  Create a new dao instance using Aragon s BaseTemplate contract:  code/dandelion-org/contracts/DandelionOrg.sol:L139-L160  /**  @dev Deploy a Dandelion Org DAO using a previously saved MiniMe token  @param _id String with the name for org, will assign `[id].aragonid.eth`  @param _holders Array of token holder addresses  @param _stakes Array of token stakes for holders (token has 18 decimals, multiply token amount `* 10^18`)  @param _useAgentAsVault Boolean to tell whether to use an Agent app as a more advanced form of Vault app  /  function newBaseInstance(  string memory _id,  address[] memory _holders,  uint256[] memory _stakes,  uint64 _financePeriod,  bool _useAgentAsVault  public  _validateId(_id);  _ensureBaseSettings(_holders, _stakes);  (Kernel dao, ACL acl) = _createDAO();  _setupBaseApps(dao, acl, _holders, _stakes, _financePeriod, _useAgentAsVault);  Set up prepackaged Aragon apps, like Vault, TokenManager, and Finance:  code/dandelion-org/contracts/DandelionOrg.sol:L162-L182  function _setupBaseApps(  Kernel _dao,  ACL _acl,  address[] memory _holders,  uint256[] memory _stakes,  uint64 _financePeriod,  bool _useAgentAsVault  internal  MiniMeToken token = _getToken();  Vault agentOrVault = _useAgentAsVault ? _installDefaultAgentApp(_dao) : _installVaultApp(_dao);  TokenManager tokenManager = _installTokenManagerApp(_dao, token, TOKEN_TRANSFERABLE, TOKEN_MAX_PER_ACCOUNT);  Finance finance = _installFinanceApp(_dao, agentOrVault, _financePeriod == 0 ? DEFAULT_FINANCE_PERIOD : _financePeriod);  _mintTokens(_acl, tokenManager, _holders, _stakes);  _saveBaseApps(_dao, finance, tokenManager, agentOrVault);  _saveAgentAsVault(_dao, _useAgentAsVault);  Note that newToken and newBaseInstance can be called separately. The token created in newToken is cached in _saveToken, which overwrites any previously-cached value:  code/dandelion-org/contracts/DandelionOrg.sol:L413-L417  function _saveToken(MiniMeToken _token) internal {  DeployedContracts storage senderDeployedContracts = deployedContracts[msg.sender];  senderDeployedContracts.token = address(_token);  Cached tokens are retrieved in _getToken:  code/dandelion-org/contracts/DandelionOrg.sol:L441-L447  function _getToken() internal returns (MiniMeToken) {  DeployedContracts storage senderDeployedContracts = deployedContracts[msg.sender];  require(senderDeployedContracts.token != address(0), ERROR_MISSING_TOKEN_CONTRACT);  MiniMeToken token = MiniMeToken(senderDeployedContracts.token);  return token;  By exploiting the overwriteable caching mechanism, it is possible to intentionally misconfigure Dandelion orgs.  Examples  installDandelionApps uses _getToken to associate a token with the DandelionVoting app. The value returned from _getToken depends on the sender s previous call to newToken, which overwrites any previously-cached value. The steps for intentional misconfiguration are as follows:  Sender calls newTokenAndBaseInstance, creating token m0 and DAO A.  The TokenManager app in A is automatically configured to be the controller of m0.  m0 is cached using _saveToken.  DAO A apps are cached for future use using _saveBaseApps and _saveAgentAsVault.  Sender calls newToken, creating token m1, and overwriting the cache of m0.  Future calls to _getToken will retrieve m1.  The DandelionOrg contract is the controller of m1.  Sender calls installDandelionApps, which installs Dandelion apps in DAO A  The DandelionVoting app is configured to use the current cached token, m1, rather than the token associated with A.TokenManager, m0  Many different misconfigurations are possible, and some may be underhandedly abusable.  Recommendation  Make newToken and newBaseInstance internal so they are only callable via newTokenAndBaseInstance.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/12/dandelion-organizations/"
    },
    {
        "title": "5.5 Delay.execute can re-enter and re-execute the same script twice    Addressed",
        "body": "  Resolution   This was addressed in   commit f049e978f93765e27783a3ecac4830498bb779ba by deleting the delayed script before it is run. 1Hive elected to keep an empty script blacklist in order to allow delayed actions to be taken on the  Description  Delay.execute does not follow the  checks-effects-interactions  pattern, and deletes a delayed script only after the script is run. Because the script being run executes arbitrary external calls, a script can be created that re-enters Delay and executes itself multiple times before being deleted:  code/delay-app/contracts/Delay.sol:L112-L123  /**  @notice Execute the script with ID `_delayedScriptId`  @param _delayedScriptId The ID of the script to execute  /  function execute(uint256 _delayedScriptId) external {  require(canExecute(_delayedScriptId), ERROR_CAN_NOT_EXECUTE);  runScript(delayedScripts[_delayedScriptId].evmCallScript, new bytes(0), new address[](0));  delete delayedScripts[_delayedScriptId];  emit ExecutedScript(_delayedScriptId);  Recommendation  Add the Delay contract address to the runScript blacklist, or delete the delayed script from storage before it is run.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/12/dandelion-organizations/"
    },
    {
        "title": "5.6 Delay.cancelExecution should revert on a non-existent script id    Addressed",
        "body": "  Resolution   This was addressed in   commit d99c94f5138a9af1fd5f0cd6990c140b46a55925 by adding the  Description  cancelExecution makes no existence check on the passed-in script ID, clearing its storage slot and emitting an event:  code/delay-app/contracts/Delay.sol:L102-L110  /**  @notice Cancel script execution with ID `_delayedScriptId`  @param _delayedScriptId The ID of the script execution to cancel  /  function cancelExecution(uint256 _delayedScriptId) external auth(CANCEL_EXECUTION_ROLE) {  delete delayedScripts[_delayedScriptId];  emit ExecutionCancelled(_delayedScriptId);  Recommendation  Add a check that the passed-in script exists.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/12/dandelion-organizations/"
    },
    {
        "title": "5.7 ID validation check missing for installDandelionApps    Addressed",
        "body": "  Resolution   This was addressed in   commit 8d1ecb1bc892d6ea1d34c7234e35de031db2bebd by removing the  Description  DandelionOrg allows users to kickstart an Aragon organization by using a dao template. There are two primary functions to instantiate an org: newTokenAndBaseInstance, and installDandelionApps. Both functions accept a parameter, string _id, meant to represent an ENS subdomain that will be assigned to the new org during the instantiation process. The two functions are called independently, but depend on each other.  In newTokenAndBaseInstance, a sanity check is performed on the _id parameter, which ensures the _id length is nonzero:  code/dandelion-org/contracts/DandelionOrg.sol:L155  _validateId(_id);  Note that the value of _id is otherwise unused in newTokenAndBaseInstance.  In installDandelionApps, this check is missing. The check is only important in this function, since it is in installDandelionApps that the ENS subdomain registration is actually performed.  Recommendation  Use _validateId in installDandelionApps rather than newTokenAndBaseInstance. Since the _id parameter is otherwise unused in newTokenAndBaseInstance, it can be removed.  Alternatively, the value of the submitted _id could be cached between calls and validated in newTokenAndBaseInstance, similarly to newToken.  6 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/12/dandelion-organizations/"
    },
    {
        "title": "6.1 Ethlint",
        "body": "  Ethlint is an open source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  $ solium -V  Solium version 1.2.5  $ solium -d .  dandelion-org/contracts/DandelionOrg.sol  86:1     warning    Line contains trailing whitespace           no-trailing-whitespace  226:8    warning    Line exceeds the limit of 145 characters    max-len  dandelion-voting-app/contracts/DandelionVoting.sol  272:8    warning    Line exceeds the limit of 145 characters    max-len  token-request-app/contracts/TokenRequest.sol  62:4      warning    Line exceeds the limit of 145 characters                 max-len  104:1     warning    Line contains trailing whitespace                        no-trailing-whitespace  token-request-app/contracts/lib/UintArrayLib.sol  6:3    error    Only use indent of 4 spaces.    indentation  \u2716 1 error, 5 warnings found.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/12/dandelion-organizations/"
    },
    {
        "title": "6.2 Surya",
        "body": "  Surya is a utility tool for smart contract systems. It provides a number of visual outputs and information about the structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  Contracts Description Table  Function Name  Visibility  Mutability  Modifiers  AddressArrayLib  Library  deleteItem  Internal \ud83d\udd12  contains  Internal \ud83d\udd12  ArrayUtils  Library  deleteItem  Internal \ud83d\udd12  DandelionOrg  Implementation  BaseTemplate  <Constructor>  Public    BaseTemplate  newTokenAndBaseInstance  External    NO   installDandelionApps  External    NO   newToken  Public    NO   newBaseInstance  Public    NO   _setupBaseApps  Internal \ud83d\udd12  _installDandelionApps  Internal \ud83d\udd12  _installDandelionVotingApp  Internal \ud83d\udd12  _installDandelionVotingApp  Internal \ud83d\udd12  _createDandelionVotingPermissions  Internal \ud83d\udd12  _installRedemptionsApp  Internal \ud83d\udd12  _createRedemptionsPermissions  Internal \ud83d\udd12  _installTokenRequestApp  Internal \ud83d\udd12  _createTokenRequestPermissions  Internal \ud83d\udd12  _installTimeLockApp  Internal \ud83d\udd12  _installTimeLockApp  Internal \ud83d\udd12  _createTimeLockPermissions  Internal \ud83d\udd12  _installTokenBalanceOracle  Internal \ud83d\udd12  _createTokenBalanceOraclePermissions  Internal \ud83d\udd12  _setupBasePermissions  Internal \ud83d\udd12  _setupDandelionPermissions  Internal \ud83d\udd12  _saveToken  Internal \ud83d\udd12  _saveBaseApps  Internal \ud83d\udd12  _saveAgentAsVault  Internal \ud83d\udd12  _getDao  Internal \ud83d\udd12  _getToken  Internal \ud83d\udd12  _getBaseApps  Internal \ud83d\udd12  _getAgentAsVault  Internal \ud83d\udd12  _clearDeployedContracts  Internal \ud83d\udd12  _ensureBaseAppsDeployed  Internal \ud83d\udd12  _ensureBaseSettings  Private \ud83d\udd10  _ensureDandelionSettings  Private \ud83d\udd10  _registerApp  Private \ud83d\udd10  _setOracle  Private \ud83d\udd10  _paramsTo256  Private \ud83d\udd10  DandelionVoting  Implementation  IForwarder, IACLOracle, AragonApp  initialize  External    onlyInit  changeSupportRequiredPct  External    authP  changeMinAcceptQuorumPct  External    authP  changeBufferBlocks  External    auth  changeExecutionDelayBlocks  External    auth  newVote  External    auth  vote  External    voteExists  executeVote  External    NO   isForwarder  External    NO   forward  Public    NO   canForward  Public    NO   canPerform  External    NO   canExecute  Public    NO   canVote  Public    voteExists  getVote  Public    voteExists  getVoterState  Public    voteExists  _newVote  Internal \ud83d\udd12  _vote  Internal \ud83d\udd12  _canExecute  Internal \ud83d\udd12  voteExists  _votePassed  Internal \ud83d\udd12  _canVote  Internal \ud83d\udd12  _voterStake  Internal \ud83d\udd12  _isVoteOpen  Internal \ud83d\udd12  _isValuePct  Internal \ud83d\udd12  Delay  Implementation  AragonApp, IForwarder  initialize  External    onlyInit  setExecutionDelay  External    auth  delayExecution  External    auth  isForwarder  External    NO   pauseExecution  External    auth  resumeExecution  External    auth  cancelExecution  External    auth  execute  External    NO   canExecute  Public    NO   canForward  Public    NO   forward  Public    NO   _isExecutionPaused  Internal \ud83d\udd12  scriptExists  _delayExecution  Internal \ud83d\udd12  Redemptions  Implementation  AragonApp  initialize  External    onlyInit  addRedeemableToken  External    auth  removeRedeemableToken  External    auth  redeem  External    authP  getRedeemableTokens  External    NO   getToken  External    NO   getETHAddress  External    NO   TimeLock  Implementation  AragonApp, IForwarder, IForwarderFee  initialize  External    onlyInit  changeLockDuration  External    auth  changeLockAmount  External    auth  changeSpamPenaltyFactor  External    auth  withdrawAllTokens  External    NO   withdrawTokens  External    NO   forwardFee  External    NO   isForwarder  External    NO   canForward  Public    NO   forward  Public    NO   getWithdrawLocksCount  Public    NO   getSpamPenalty  Public    NO   _withdrawTokens  Internal \ud83d\udd12  TokenBalanceOracle  Implementation  AragonApp, IACLOracle  initialize  External    onlyInit  setToken  External    auth  setMinBalance  External    auth  canPerform  External    NO   TokenRequest  Implementation  AragonApp  initialize  External    onlyInit  setTokenManager  External    auth  setVault  External    auth  addToken  External    auth  removeToken  External    auth  createTokenRequest  External    NO   refundTokenRequest  External    nonReentrant tokenRequestExists  finaliseTokenRequest  External    nonReentrant tokenRequestExists auth  getAcceptedDepositTokens  Public    NO   getTokenRequest  Public    NO   getToken  Public    NO   UintArrayLib  Library  deleteItem  Internal \ud83d\udd12  Legend  Function can modify state  Function is payable  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/12/dandelion-organizations/"
    },
    {
        "title": "5.1 Tokens with no decimals can be locked in Niftyswap   ",
        "body": "  Resolution  This will be addressed by only listing tokens with at least 2 decimals. This should be well documented in the Niftyswap repository and code comments.  Description  Assume the Niftyswap exchange has:  wrapped DAI as the base currency, and  it s ERC1155 contract has a token called  Blue Dragons , which are a  low fungibility  token, with zero decimals, and a total supply of 100.  Consider the following scenario on the Niftyswap exchange:  10 people each add 1,000 DAI, and 1 BlueDragon. They get 1,000 pool tokens each.  Someone buys 1 BlueDragon, at a price of 1,117 base Tokens (per the constant product pricing model).  Niftyswap s balances are now 11,117 baseTokens, 9 Blue Dragons.  Someone removes liquidity by burning 1,000 pool tokens:  They would get 1111 base tokens (1000 * 11,117/ 10000). They would get 0 Blue Dragons due to the rounding on integer math.  Recommendation  Through conversation with the developers, we agreed the right approach is for tokens to have at least 2 decimals to minimize the negative effects of rounding down.  ",
        "labels": [
            "Consensys",
            "Major",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/horizon-games/"
    },
    {
        "title": "5.2 Incorrect response from price feed if called during an onERC1155Received callback   ",
        "body": "  Resolution  The design will not be modified. Horizon Games should clearly document this risk for 3rd parties seeking to use Niftyswap as a price feed.  Description  The ERC 1155 standard requires that smart contracts must implement onERC1155Received and onERC1155BatchReceived to accept transfers.  This means that on any token received, code run on the receiving smart contract.  In NiftyswapExchange when adding / removing liquidity or buying tokens, the methods mentioned above are called when the tokens are sent. When this happens, the state of the contract is changed but not completed, the tokens are sent to the receiving smart contract but the state is not completely updated.  This happens in these cases  _baseToToken (when buying tokens)  code/niftyswap/contracts/exchange/NiftyswapExchange.sol:L163-L169  // // Refund Base Token if any  if (totalRefundBaseTokens > 0) {  baseToken.safeTransferFrom(address(this), _recipient, baseTokenID, totalRefundBaseTokens, \"\");  // Send Tokens all tokens purchased  token.safeBatchTransferFrom(address(this), _recipient, _tokenIds, _tokensBoughtAmounts, \"\");  _removeLiquidity  code/niftyswap/contracts/exchange/NiftyswapExchange.sol:L485-L487  // Transfer total Base Tokens and all Tokens ids  baseToken.safeTransferFrom(address(this), _provider, baseTokenID, totalBaseTokens, \"\");  token.safeBatchTransferFrom(address(this), _provider, _tokenIds, tokenAmounts, \"\");  _addLiquidity  code/niftyswap/contracts/exchange/NiftyswapExchange.sol:L403-L407  // Mint liquidity pool tokens  _batchMint(_provider, _tokenIds, liquiditiesToMint, \"\");  // Transfer all Base Tokens to this contract  baseToken.safeTransferFrom(_provider, address(this), baseTokenID, totalBaseTokens, abi.encode(DEPOSIT_SIG));  Each of these examples send some tokens to the smart contract, which triggers calling some code on the receiving smart contract.  While these methods have the nonReentrant modifier which protects them from re-netrancy, the result of the methods getPrice_baseToToken and getPrice_tokenToBase is affected. These 2 methods do not have the nonReentrant modifier.  The price reported by the getPrice_baseToToken and getPrice_tokenToBase methods is incorrect (until after the end of the transaction) because they rely on the number of tokens owned by the NiftyswapExchange; which between the calls is not finalized. Hence the price reported will be incorrect.  This gives the smart contract which receives the tokens, the opportunity to use other systems (if they exist) that rely on the result of getPrice_baseToToken and getPrice_tokenToBase to use the returned price to its advantage.  It s important to note that this is a bug only if other systems rely on the price reported by this NiftyswapExchange. Also the current contract is not affected, nor its balances or internal ledger, only other systems relying on its reported price will be fooled.  Recommendation  Because there is no way to enforce how other systems work, a restriction can be added on NiftyswapExchange to protect other systems (if any) that rely on NiftyswapExchange for price discovery.  Adding a nonReentrant modifier on the view methods getPrice_baseToToken and getPrice_tokenToBase will add a bit of protection for the ecosystem.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/horizon-games/"
    },
    {
        "title": "6.1 Test code present in the code base    ",
        "body": "  Resolution   Fixed in   lukso-network/rICO-smart-contracts@edb880c.  Description  Test code are present in the code base. This is mainly a reminder to fix those before production.  Examples  rescuerAddress and freezerAddress are not even in the function arguments.  code/contracts/ReversibleICO.sol:L243-L247  whitelistingAddress = _whitelistingAddress;  projectAddress = _projectAddress;  freezerAddress = _projectAddress; // TODO change, here only for testing  rescuerAddress = _projectAddress; // TODO change, here only for testing  Recommendation  Make sure all the variable assignments are ready for production before deployment to production.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/04/rico/"
    },
    {
        "title": "6.2 FreezerAddress has more power than required   ",
        "body": "  Resolution   This issue is acknowledged by the client and the behaviour has been documented in   security measurements.  Description  FreezerAddress is designed to have the ability of freezing the contract in case of emergency. However, indirectly, there are other changes in the system that can result from the freeze.  Examples  FreezerAddress can extend the rICO time frame. Given that the frozenPeriod is deducted from the blockNumber in stage calculations, the buyPhaseEndBlock is technically equals to buyPhaseEndBlock + frozenPeriod  FreezerAddress can call disableEscapeHatch(), which disables the escape hatch and rendering RescuerAddress useless.  Recommendation  If these behaviors are intentional they should be well documented and specified. If not, they should be removed.  In the case they are, indeed, intentional the audit team believes that, for Example 1., there should be some event fired to serve as notification for the participants (possibly followed by off-chain infrastructure to warn them through email or other communication channel).  ",
        "labels": [
            "Consensys",
            "Medium",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/04/rico/"
    },
    {
        "title": "6.3 frozenPeriod is subtracted twice for calculating the current price    ",
        "body": "  Resolution   Found in parallel to the audit team and has been mitigated in   lukso-network/rICO-smart-contracts@ebc4bce . The issue was further simplified by adding  lukso-network/rICO-smart-contracts@e4c9ed5 to remove ambiguity when calculating current block number.  Description  If the contract had been frozen, the current stage price will calculate the price by subtracting the frozenPeriod twice and result in wrong calculation.  getCurrentBlockNumber() subtracts frozenPeriod once, and then getStageAtBlock() will also subtract the same number again.  Examples  code/contracts/ReversibleICO.sol:L617-L619  function getCurrentStage() public view returns (uint8) {  return getStageAtBlock(getCurrentBlockNumber());  code/contracts/ReversibleICO.sol:L711-L714  function getCurrentBlockNumber() public view returns (uint256) {  return uint256(block.number)  .sub(frozenPeriod); // make sure we deduct any frozenPeriod from calculations  code/contracts/ReversibleICO.sol:L654-L656  function getStageAtBlock(uint256 _blockNumber) public view returns (uint8) {  uint256 blockNumber = _blockNumber.sub(frozenPeriod); // adjust the block by the frozen period  Recommendation  Make sure frozenPeriod calculation is done correctly. It could be solved by renaming getCurrentBlockNumber() to reflect the calculation done inside the function.  e.g. :  getCurrentBlockNumber() : gets current block number  getCurrentEffectiveBlockNumber() : calculates the effective block number deducting frozenPeriod  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/04/rico/"
    },
    {
        "title": "6.4 Lockup condition in getStageAtBlock()    ",
        "body": "  Resolution  Even though the freeze pattern does indeed create a lot of additional complexity to the protocol, the particular require mentioned in the issue corpus by the audit team was found to never be triggered in a harmful way by rICO s development team.  In the light of this new discovery, we are greatly reducing the severity of the issue to  Minor . The reason why it is still kept as an issue is that the implementation of the freezing mechanism could still be greatly improved as we saw in the presented fixes here:  lukso-network/rICO-smart-contracts@e4c9ed5  The changes resulted in a much more resilient rICO implementation.  Description  Given that the contract has been frozen at least once, if the frozenPeriod is longer than the period before the freeze event (starting from commitPhaseStartBlock till the freezeStart), the following require in getStageAtBlock() will revert due to the fact that blockNumber < commitPhaseStartBlock:  uint256 blockNumber = _blockNumber.sub(frozenPeriod); // adjust the block by the frozen period  require(blockNumber >= commitPhaseStartBlock && blockNumber <= buyPhaseEndBlock, \"Block outside of rICO period.\");  Note that the issue here is also related to the way currentBlockNumber is calculated (See issue 6.3 and Separate currentBlock from currentEffectiveBlock.  getCurrentStage() is called for every accept or cancelation of contributions and this lockup can result in total system halt.  Recommendation  Given that in the init function, the following condition is checked:  require(_commitPhaseStartBlock > getCurrentBlockNumber(), \"Start block cannot be set in the past.\");  The check in the getStageAtBlock() can be removed. However this is assuming that the correct calculation of the currentEffectiveBlockNumber is used.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/04/rico/"
    },
    {
        "title": "6.5 emit events for significant state changes    ",
        "body": "  Resolution   This issue was discussed in the code walk through meeting and was fixed, by adding proper events to the code base in   lukso-network/rICO-smart-contracts@77517a4, before the end of the audit.  Description  Events are useful for UI changes and user notifications. The code base overall can use more use of events to update the UI and participants.  One of the most important aspects that must emit events, are when system state and functionality are changed. These functions require to emit events for better visibility to the participants:  freeze()  unfreeze()  disableEscapeHatch()  escapeHatch()  Recommendation  emit events when system state is changed.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/04/rico/"
    },
    {
        "title": "6.1 An account that confirms a transaction via AssetProxyOwner can indefinitely block that transaction    ",
        "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2297 by allowing transactions to be  over confirmed  without resetting the confirmation time. As long as there are enough honest signers, this prevents a malicious signer from blocking transactions.  Description  When a transaction reaches the required number of confirmations in confirmTransaction(), its confirmation time is recorded:  code/contracts/multisig/contracts/src/MultiSigWalletWithTimeLock.sol:L86-L100  /// @dev Allows an owner to confirm a transaction.  /// @param transactionId Transaction ID.  function confirmTransaction(uint256 transactionId)  public  ownerExists(msg.sender)  transactionExists(transactionId)  notConfirmed(transactionId, msg.sender)  notFullyConfirmed(transactionId)  confirmations[transactionId][msg.sender] = true;  emit Confirmation(msg.sender, transactionId);  if (isConfirmed(transactionId)) {  _setConfirmationTime(transactionId, block.timestamp);  Before the time lock has elapsed and the transaction is executed, any of the owners that originally confirmed the transaction can revoke their confirmation via revokeConfirmation():  code/contracts/multisig/contracts/src/MultiSigWallet.sol:L249-L259  /// @dev Allows an owner to revoke a confirmation for a transaction.  /// @param transactionId Transaction ID.  function revokeConfirmation(uint256 transactionId)  public  ownerExists(msg.sender)  confirmed(transactionId, msg.sender)  notExecuted(transactionId)  confirmations[transactionId][msg.sender] = false;  emit Revocation(msg.sender, transactionId);  Immediately after, that owner can call confirmTransaction() again, which will reset the confirmation time and thus the time lock.  This is especially troubling in the case of a single compromised key, but it s also an issue for disagreement among owners, where any m of the n owners should be able to execute transactions but could be blocked.  Mitigations  Only an owner can do this, and that owner has to be part of the group that originally confirmed the transaction. This means the malicious owner may have to front run the others to make sure they re in that initial confirmation set.  Even once a malicious owner is in position to execute this perpetual delay, they need to call revokeConfirmation() and confirmTransaction() again each time. Another owner can attempt to front the attacker and execute their own confirmTransaction() immediately after the revokeConfirmation() to regain control.  Recommendation  There are several ways to address this, but to best preserve the original MultiSigWallet semantics, once a transaction has reached the required number of confirmations, it should be impossible to revoke confirmations. In the original implementation, this is enforced by immediately executing the transaction when the final confirmation is received.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"
    },
    {
        "title": "6.2 Orders with signatures that require regular validation can have their validation bypassed if the order is partially filled    ",
        "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2246. Signatures are now always validated each time, regardless of type.  Description  This re-validation step for Wallet, Validator, and EIP1271Wallet signatures is intended to facilitate their use with contracts whose validation depends on some state that may change over time. For example, a validating contract may call into a price feed and determine that some order is invalid if its price deviates from some expected range. In this case, the repeated validation allows 0x users to make orders with custom fill conditions which are evaluated at run-time.  We found that if the sender provides the contract with an invalid signature after the order in question has already been partially filled, the regular validation check required for Wallet, Validator, and EIP1271Wallet signatures can be bypassed entirely.  Examples  Signature validation takes place in MixinExchangeCore._assertFillableOrder. A signature is only validated if it passes the following criteria:  code/contracts/exchange/contracts/src/MixinExchangeCore.sol:L372-L381  // Validate either on the first fill or if the signature type requires  // regular validation.  address makerAddress = order.makerAddress;  if (orderInfo.orderTakerAssetFilledAmount == 0 ||  _doesSignatureRequireRegularValidation(  orderInfo.orderHash,  makerAddress,  signature  ) {  In effect, signature validation only occurs if:  orderInfo.orderTakerAssetFilledAmount == 0 OR  _doesSignatureRequireRegularValidation(orderHash, makerAddress, signature)  If an order is partially filled, the first condition will evaluate to false. Then, that order s signature will only be validated if _doesSignatureRequireRegularValidation evaluates to true:  code/contracts/exchange/contracts/src/MixinSignatureValidator.sol:L183-L206  function _doesSignatureRequireRegularValidation(  bytes32 hash,  address signerAddress,  bytes memory signature  internal  pure  returns (bool needsRegularValidation)  // Read the signatureType from the signature  SignatureType signatureType = _readSignatureType(  hash,  signerAddress,  signature  );  // Any signature type that makes an external call needs to be revalidated  // with every partial fill  needsRegularValidation =  signatureType == SignatureType.Wallet ||  signatureType == SignatureType.Validator ||  signatureType == SignatureType.EIP1271Wallet;  return needsRegularValidation;  The result is that an order whose signature requires regular validation can be forced to skip validation if it has been partially filled, by passing in an invalid signature.  Recommendation  There are a few options for remediation:  Have the Exchange validate the provided signature every time an order is filled.  Record the first seen signature type or signature hash for each order, and check that subsequent actions are submitted with a matching signature.  The first option requires the fewest changes, and does not require storing additional state. While this does mean some additional cost validating subsequent signatures, we feel the increase in flexibility is well worth it, as a maker could choose to create multiple valid signatures for use across different order books.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"
    },
    {
        "title": "6.3 Changing the owners or required confirmations in the AssetProxyOwner can unconfirm a previously confirmed transaction    ",
        "body": "  Resolution  This issue is somewhat inaccurate: isConfirmed() breaks out of the loop once it s found the correct number of confirmations. That means that lowering the number of required confirmations is not a problem.  Further, 0xProject/0x-monorepo#2297 allows signers to confirm transactions that have already been confirmed.  Increasing signing requirements or changing signers can still unconfirm previously confirmed transactions, but the development team is happy with that behavior.  Description  Once a transaction has been confirmed in the AssetProxyOwner, it cannot be executed until a lock period has passed. During that time, any change to the number of required confirmations will cause this transaction to no longer be executable.  If the number of required confirmations was decreased, then one or more owners will have to revoke their confirmation before the transaction can be executed.  If the number of required confirmations was increased, then additional owners will have to confirm the transaction, and when the new required number of confirmations is reached, a new confirmation time will be recorded, and thus the time lock will restart.  Similarly, if an owner that had previously confirmed the transaction is replaced, the number of confirmations will drop for existing transactions, and they will need to be confirmed again.  This is not disastrous, but it s almost certainly unintended behavior and may make it difficult to make changes to the multisig owners and parameters.  Examples  executeTransaction() requires that at the time of execution, the transaction is confirmed:  code/contracts/multisig/contracts/src/AssetProxyOwner.sol:L115-L118  function executeTransaction(uint256 transactionId)  public  notExecuted(transactionId)  fullyConfirmed(transactionId)  isConfirmed() checks for exact equality with the number of required confirmations. Having too many confirmations is just as bad as too few:  code/contracts/multisig/contracts/src/MultiSigWallet.sol:L318-L335  /// @dev Returns the confirmation status of a transaction.  /// @param transactionId Transaction ID.  /// @return Confirmation status.  function isConfirmed(uint256 transactionId)  public  view  returns (bool)  uint256 count = 0;  for (uint256 i = 0; i < owners.length; i++) {  if (confirmations[transactionId][owners[i]]) {  count += 1;  if (count == required) {  return true;  If additional confirmations are required to reconfirm a transaction, that resets the time lock:  code/contracts/multisig/contracts/src/MultiSigWalletWithTimeLock.sol:L86-L100  /// @dev Allows an owner to confirm a transaction.  /// @param transactionId Transaction ID.  function confirmTransaction(uint256 transactionId)  public  ownerExists(msg.sender)  transactionExists(transactionId)  notConfirmed(transactionId, msg.sender)  notFullyConfirmed(transactionId)  confirmations[transactionId][msg.sender] = true;  emit Confirmation(msg.sender, transactionId);  if (isConfirmed(transactionId)) {  _setConfirmationTime(transactionId, block.timestamp);  Recommendation  As in issue 6.1, the semantics of the original MultiSigWallet were that once a transaction is fully confirmed, it s immediately executed. The time lock means this is no longer possible, but it is possible to record that the transaction is confirmed and never allow this to change. In fact, the confirmation time already records this. Once the confirmation time is non-zero, a transaction should always be considered confirmed.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"
    },
    {
        "title": "6.4 Reentrancy in executeTransaction()   ",
        "body": "  Resolution  From the development team:  Reentrancy would be dangerous in executeTransaction if combined with updating the currentContextAddress. However, this is is prevented by checking currentContextAddress_ != address(0) when validating a transaction.  executeTransaction also inherits a lot of the safety from the reentrancy protection on other individual functions in the Exchange contract.  Setting transactionsExecuted before making the delegatecall also prevents the same transaction from being executed multiple times.  Description  In MixinTransactions, executeTransaction() and batchExecuteTransactions() do not have the nonReentrant modifier. Because of that, it is possible to execute nested transactions or call these functions during other reentrancy attacks on the exchange. The reason behind that decision is to be able to call functions with nonReentrant modifier as delegated transactions.  Nested transactions are partially prevented with a separate check that does not allow transaction execution if the exchange is currently in somebody else s context:  code/contracts/exchange/contracts/src/MixinTransactions.sol:L155-L162  // Prevent `executeTransaction` from being called when context is already set  address currentContextAddress_ = currentContextAddress;  if (currentContextAddress_ != address(0)) {  LibRichErrors.rrevert(LibExchangeRichErrors.TransactionInvalidContextError(  transactionHash,  currentContextAddress_  ));  This check still leaves some possibility of reentrancy. Allowing that behavior is dangerous and may create possible attack vectors in the future.  Recommendation  Add a new modifier to executeTransaction() and batchExecuteTransactions() which is similar to nonReentrant but uses different storage slot.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"
    },
    {
        "title": "6.5  Poison  order that consumes gas can block market trades   ",
        "body": "  Resolution  From the development team:  This can be prevented fairly easily by performing an eth_call off-chain before attempting to fill any orders (which is pretty standard practice). Hard coding gas limits reduces flexibility and may ultimately prevent some use cases from developing in the future.  (Note from the audit team: Hardcoding is not necessary. A parameter would do.)  Description  The market buy/sell functions gather a list of orders together for the same asset and try to fill them in order until a target amount has been traded.  These functions use MixinWrapperFunctions._fillOrderNoThrow() to attempt to fill each order but ignore failures. This way, if one order is unfillable for some reason, the overall market order can still succeed by filling other orders.  Orders can still force _fillOrderNoThrow() to revert by using an external contract for signature validation and having that contract consume all available gas.  This makes it possible to advertise a  poison  order for a low price that will block all market orders from succeeding. It s reasonable to assume that off-chain order books will automatically include the best prices when constructing market orders, so this attack would likely be quite effective. Note that such an attack costs the attacker nothing because all they need is an on-chain contract that consumers all available gas (maybe via an assert). This makes it a very appealing attack vector for, e.g., an order book that wants to temporarily disable a competitor.  Details  _fillOrderNoThrow() forwards all available gas when filling the order:  code/contracts/exchange/contracts/src/MixinWrapperFunctions.sol:L340-L348  // ABI encode calldata for `fillOrder`  bytes memory fillOrderCalldata = abi.encodeWithSelector(  IExchangeCore(address(0)).fillOrder.selector,  order,  takerAssetFillAmount,  signature  );  (bool didSucceed, bytes memory returnData) = address(this).delegatecall(fillOrderCalldata);  Similarly, when the Exchange attempts to fill an order that requires external signature validation (Wallet, Validator, or EIP1271Wallet signature types), it forwards all available gas:  code/contracts/exchange/contracts/src/MixinSignatureValidator.sol:L642  (bool didSucceed, bytes memory returnData) = verifyingContractAddress.staticcall(callData);  If the verifying contract consumes all available gas, it can force the overall transaction to revert.  Pedantic Note  Technically, it s impossible to consume all remaining gas when called by another contract because the EVM holds back a small amount, but even at the block gas limit, the amount held back would be insufficient to complete the transaction.  Recommendation  Constrain the gas that is forwarded during signature validation. This can be constrained either as a part of the signature or as a parameter provided by the taker.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"
    },
    {
        "title": "6.6 Front running in matchOrders()   ",
        "body": "  Resolution  From the development team:  Front-running is typically prevented with a combination of external contracts and various off-chain mechanics.  These functions are primarily intended to be used with  matching relayers . In this model, orders must set their takerAddress or senderAddress to the address of the matcher, who is the only party allowed to actually fill the orders. This prevents any other address from participating in a gas auction.  A commit-reveal scheme would be difficult to take advantage of in practice, since orders could be filled through a number of other functions on the Exchange contract. All of these functions would have to adhere to the commit-reveal scheme in order to be effective.  Description  Calls to matchOrders() are made to extract profit from the price difference between two opposite orders: left and right.  code/contracts/exchange/contracts/src/MixinMatchOrders.sol:L106-L111  function matchOrders(  LibOrder.Order memory leftOrder,  LibOrder.Order memory rightOrder,  bytes memory leftSignature,  bytes memory rightSignature  The caller only pays protocol and transaction fees, so it s almost always profitable to front run every call to matchOrders(). That would lead to gas auctions and would make matchOrders() difficult to use.  Recommendation  Consider adding a commit-reveal scheme to matchOrders() to stop front running altogether.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"
    },
    {
        "title": "6.7 The Exchange owner should not be able to call executeTransaction or batchExecuteTransaction   ",
        "body": "  Resolution  From the development team:  While this is a minor inconsistency in the logic of these functions, it is in no way dangerous. currentContextAddress is not used when calling any admin functions, so the address of the transaction signer will be completely disregarded.  Description  Examples  _executeTransaction sets the context address to the signer address, which is not msg.sender in this case: code/contracts/exchange/contracts/src/MixinTransactions.sol:L102-L104 // Set the current transaction signer address signerAddress = transaction.signerAddress; _setCurrentContextAddressIfRequired(signerAddress, signerAddress);  The resulting delegatecall could target an admin function like this one: code/contracts/exchange/contracts/src/MixinAssetProxyDispatcher.sol:L38-L61 /// @dev Registers an asset proxy to its asset proxy id. ///      Once an asset proxy is registered, it cannot be unregistered. /// @param assetProxy Address of new asset proxy to register. function registerAssetProxy(address assetProxy)     external     onlyOwner {     // Ensure that no asset proxy exists with current id.     bytes4 assetProxyId = IAssetProxy(assetProxy).getProxyId();     address currentAssetProxy = _assetProxies[assetProxyId];     if (currentAssetProxy != address(0)) {         LibRichErrors.rrevert(LibExchangeRichErrors.AssetProxyExistsError(             assetProxyId,             currentAssetProxy         ));     }      // Add asset proxy and log registration.     _assetProxies[assetProxyId] = assetProxy;     emit AssetProxyRegistered(         assetProxyId,         assetProxy     ); }  The onlyOwner modifier does not check the context address, but checks msg.sender: code/contracts/utils/contracts/src/Ownable.sol:L35-L45 function _assertSenderIsOwner()     internal     view {     if (msg.sender != owner) {         LibRichErrors.rrevert(LibOwnableRichErrors.OnlyOwnerError(             msg.sender,             owner         ));     } }  Recommendation  Add a check to _executeTransaction that prevents the owner from calling this function.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"
    },
    {
        "title": "6.8 Anyone can front run MixinExchangeCore.cancelOrder()   ",
        "body": "  Resolution  From the development team:  Front-running is typically prevented with a combination of external contracts and various off-chain mechanics.  It is not possible to cancel an order by providing less data to the cancelOrder function without drastically changing the logic of the fill functions. However, this type of behavior could possibly be enforced by using external contracts that are set to the senderAddress of the related orders.  Description  In order to cancel an order, an authorized address (maker or sender) calls cancelOrder(LibOrder.Order memory order). When calling that function, all data for the order becomes visible to everyone on the network, and anyone can fill that order before it s canceled.  Usually, a maker is canceling an order because it s no longer profitable for them, so an attacker is likely to profit from front running the cancelOrder() transaction.  Recommendation  Make it impossible to front run order cancelation by providing less data to the cancelOrder() function such that this data is insufficient to execute the order.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"
    },
    {
        "title": "6.9 By manipulating the gas limit, relayers can affect the outcome of ZeroExTransactions   ",
        "body": "  Resolution  From the development team:  While this is an annoyance when used in combination with marketBuyOrdersNoThrow and marketSellOrdersNoThrow, it does not seem worth it to add a gasLimit to 0x transactions for this reason alone. Instead, this quirk should be documented along with a recommendation to use the fillOrKill variants of each market fill function when used in combination with 0x transactions.  Description  ZeroExTransactions are meta transactions supported by the Exchange. They do not require that they are executed with a specific amount of gas, so the transaction relayer can choose how much gas to provide. By choosing a low gas limit, a relayer can affect the outcome of the transaction.  A ZeroExTransaction specifies a signer, an expiration, and call data for the transaction:  code/contracts/exchange-libs/contracts/src/LibZeroExTransaction.sol:L41-L47  struct ZeroExTransaction {  uint256 salt;                   // Arbitrary number to ensure uniqueness of transaction hash.  uint256 expirationTimeSeconds;  // Timestamp in seconds at which transaction expires.  uint256 gasPrice;               // gasPrice that transaction is required to be executed with.  address signerAddress;          // Address of transaction signer.  bytes data;                     // AbiV2 encoded calldata.  In MixinTransactions._executeTransaction(), all available gas is forwarded in the delegate call, and the transaction is marked as executed:  code/contracts/exchange/contracts/src/MixinTransactions.sol:L107-L108  transactionsExecuted[transactionHash] = true;  (bool didSucceed, bytes memory returnData) = address(this).delegatecall(transaction.data);  Examples  A likely attack vector for this is front running a ZeroExTransaction that ultimately invokes _fillNoThrow(). In this scenario, an attacker sees the call to executeTransaction() and makes their own call with a lower gas limit, causing the order being filled to run out of gas but allowing the transaction as a whole to succeed.  If such an attack is successful, the ZeroExTransaction cannot be replayed, so the signer must produce a new signature and try again, ad infinitum.  Recommendation  Add a gasLimit field to ZeroExTransaction and forward exactly that much gas via delegatecall. (Note that you must explicitly check that sufficient gas is available because the EVM allows you to supply a gas parameter that exceeds the actual remaining gas.)  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"
    },
    {
        "title": "6.10 Front running market orders   ",
        "body": "  Resolution  From the development team:  Front-running is typically prevented with a combination of external contracts and various off-chain mechanics.  Users should always understand the risk of using market orders in any market or exchange structure. Although they increase convenience and arguably have a better UX, they almost always carry more risk than other order types.  Users can always enforce a worst price by padding a market fill with an appropriate number of orders that do not exceed the worst acceptable price.  Description  MixinWrapperFunctions defines a number of functions for market buy/sell orders. These functions take a list of orders and a target asset amount to buy or sell. They fill each order in turn until the target has been reached.  These functions provide an appealing opportunity for front running because of the near-guaranteed profit to be had. This is most easily explained with an example:  Alice wishes to buy 10 FOO tokens. She creates a market buy order to purchase tokens first from Bob, who is selling 4 FOO tokens at $9 each, and then from Eve, who is selling 20 tokens at $10 each.  Eve front runs this market order with a transaction that buys all 4 FOO tokens from Bob for $9 each.  Alice s transaction goes through, but because Bob s inventory has been depleted, all 10 FOO tokens are purchased from Eve at a price of $10 each. By front running, Eve gained $4.  In a more traditional front running scheme, Alice would have just been trying to make a simple purchase of FOO tokens at $9 each, and Eve would be taking on non-trivial risk by buying them first and hoping Alice (or another buyer) would be willing to pay a higher price later.  With a market order, however, Eve s front running is nearly risk free because she knows the market order already commits Alice to buying at the higher price.  Recommendation  For the most part, traders will simply have to understand the risks of market orders and take care to only authorize trades they will be happy with.  That said, each order in a market order could specify a maximum quantity, e.g.  I want 10 FOO tokens, and I m willing to buy up to 10 from Bob but only up to 5 from Eve.  This would limit the trader s exposure to increased prices due to front running, but it would retain the convenience and efficiency of market orders.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"
    },
    {
        "title": "6.11 Modifier ordering plays a significant role in modifier efficacy    ",
        "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2228 by introducing a new modifier that combines the two:  Description  The nonReentrant and refundFinalBalance modifiers always appear together across the 0x monorepo. When used, they invariably appear with nonReentrant listed first, followed by refundFinalBalance. This specific order appears inconsequential at first glance but is actually important. The order of execution is as follows:  The nonReentrant modifier runs (_lockMutexOrThrowIfAlreadyLocked).  If refundFinalBalance had a prefix, it would run now.  The function itself runs.  The refundFinalBalance modifier runs (_refundNonZeroBalanceIfEnabled).  The nonReentrant modifier runs (_unlockMutex).  The fact that the refundFinalBalance modifier runs before the mutex is unlocked is of particular importance because it potentially invokes an external call, which may reenter. If the order of the two modifiers were flipped, the mutex would unlock before the external call, defeating the purpose of the reentrancy guard.  Examples  code/contracts/exchange/contracts/src/MixinExchangeCore.sol:L64-L65  nonReentrant  refundFinalBalance  Recommendation  Although the order of the modifiers is correct as-is, this pattern introduces cognitive overhead when making or reviewing changes to the 0x codebase. Because the two modifiers always appear together, it may make sense to combine the two into a single modifier where the order of operations is explicit.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"
    },
    {
        "title": "6.12 Several overflows in LibBytes    Addressed",
        "body": "  Resolution   This is addressed in   0xProject/0x-monorepo#2265. Unused functions have been removed. The remaining functions are only used with safe parameters (ones guaranteed not to overflow).  Description  Several functions in LibBytes have integer overflows.  Examples  LibBytes.readBytesWithLength returns a pointer to a bytes array within an existing bytes array at some given index. The length of the nested array is added to the given index and checked against the parent array to ensure the data in the nested array is within the bounds of the parent. However, because the addition can overflow, the bounds check can be bypassed to return an array that points to data out of bounds of the parent array.  code/contracts/utils/contracts/src/LibBytes.sol:L546-L553  if (b.length < index + nestedBytesLength) {  LibRichErrors.rrevert(LibBytesRichErrors.InvalidByteOperationError(  LibBytesRichErrors  .InvalidByteOperationErrorCodes.LengthGreaterThanOrEqualsNestedBytesLengthRequired,  b.length,  index + nestedBytesLength  ));  The following functions have similar issues:  readAddress  writeAddress  readBytes32  writeBytes32  readBytes4  Recommendation  An overflow check should be added to the function. Alternatively, because readBytesWithLength does not appear to be used anywhere in the 0x project, the function should be removed from LibBytes. Additionally, the following functions in LibBytes are also not used and should be considered for removal:  popLast20Bytes  writeAddress  writeBytes32  writeUint256  writeBytesWithLength  deepCopyBytes  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"
    },
    {
        "title": "6.13 NSignatureTypes enum value bypasses Solidity safety checks   ",
        "body": "  Resolution  From the development team:  This has been left unchanged in order to provide more context with a revert when an invalid signature type is used.  Description  The ISignatureValidator contract defines an enum SignatureType to represent the different types of signatures recognized within the exchange. The final enum value, NSignatureTypes, is not a valid signature type. Instead, it is used by MixinSignatureValidator to check that the value read from the signature is a valid enum value. However, Solidity now includes its own check for enum casting, and casting a value over the maximum enum size to an enum is no longer possible.  Because of the added NSignatureTypes value, Solidity s check now recognizes 0x08 as a valid SignatureType value.  Examples  The check is made here:  code/contracts/exchange/contracts/src/MixinSignatureValidator.sol:L441-L449  // Ensure signature is supported  if (uint8(signatureType) >= uint8(SignatureType.NSignatureTypes)) {  LibRichErrors.rrevert(LibExchangeRichErrors.SignatureError(  LibExchangeRichErrors.SignatureErrorCodes.UNSUPPORTED,  hash,  signerAddress,  signature  ));  Recommendation  The check should be removed, as should the SignatureTypes.NSignatureTypes value.  7 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issues section.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"
    },
    {
        "title": "7.1 MythX",
        "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  The full set of MythX results for both the exchange and staking contracts are available in a separate report.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"
    },
    {
        "title": "7.2 Surya",
        "body": "  Surya is an utility tool for smart contract systems. It provides a number of visual outputs and information about structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  S\u016brya s Description Report  Files Description Table  exchange/contracts/src/Exchange.sol  cb6733c32d3306348791b83a9ae76460b75555df  exchange/contracts/src/MixinAssetProxyDispatcher.sol  ee5492092ebea3397d53163cad5cfe8b8050f88e  exchange/contracts/src/MixinExchangeCore.sol  87f9d192c0d75569ee95705baa9c1cdfd129d7a5  exchange/contracts/src/MixinMatchOrders.sol  42868be4aea9327a636766682a8655686af3fc72  exchange/contracts/src/MixinProtocolFees.sol  4982d287aaa206897698039fb34f95f53deda0b5  exchange/contracts/src/MixinSignatureValidator.sol  a69bf0916642b2abaf7e2705d704c00bf2e79150  exchange/contracts/src/MixinTransactions.sol  c3108f751ef627e171ad35c445c8e38cbe0c4d2c  exchange/contracts/src/MixinTransferSimulator.sol  b3ceb9d2e4a8cc1c55648548b950ad1114d29961  exchange/contracts/src/MixinWrapperFunctions.sol  69ea7edd94fc6fd1ede6c6bad139e3e61472c3df  exchange/contracts/src/interfaces/IAssetProxy.sol  21860ce6d0fe6286966dab04b39784f6e2d23857  exchange/contracts/src/interfaces/IAssetProxyDispatcher.sol  f3022084eee2e1a87d4bc023d2aa58d44a3bc3c3  exchange/contracts/src/interfaces/IEIP1271Data.sol  3e98264aa000a238a3f954b17acb6c6606fb3104  exchange/contracts/src/interfaces/IEIP1271Wallet.sol  d99b3b52044cba515a1eebbee67cf5db4f0ae280  exchange/contracts/src/interfaces/IExchange.sol  82d342133ab823431dc07255853f99da8cd49b10  exchange/contracts/src/interfaces/IExchangeCore.sol  48b0562a46653734202a40cc2ce7fcf0e653327a  exchange/contracts/src/interfaces/IMatchOrders.sol  db34eec2bf4bc41c3b51ec35803e1c5aaae4a6fb  exchange/contracts/src/interfaces/IProtocolFees.sol  bcc0151ed53fa72a87102f18015b3bcbf604b4cc  exchange/contracts/src/interfaces/ISignatureValidator.sol  e2304c3b8612ec7b7899d163b82a1bb1145c191a  exchange/contracts/src/interfaces/ITransactions.sol  a2f67b8a9e047c0dc7c33efda4223e087a6e90b4  exchange/contracts/src/interfaces/ITransferSimulator.sol  02ea8f864e3277e1f7c30e0ea38aac177625177d  exchange/contracts/src/interfaces/IWallet.sol  81fbaee73e754cfbc57882e1cd81be5fbf70b9de  exchange/contracts/src/interfaces/IWrapperFunctions.sol  d1b20adfa9b2639aff21e8a0d8f864a5b9435fa4  exchange/contracts/src/libs/LibExchangeRichErrorDecoder.sol  02c13f0e1c57b12da14b0384bebb38d1039bc7c1  exchange-libs/contracts/src/IWallet.sol  d3c769706e00d8a68175a261d79c04a8750b6118  exchange-libs/contracts/src/LibEIP712ExchangeDomain.sol  823955e1f1b21a34ad3fda91c7e691dd68e9a62e  exchange-libs/contracts/src/LibExchangeRichErrors.sol  e58712de5e18edfe951ea694124859ca1a1c05f5  exchange-libs/contracts/src/LibFillResults.sol  49422e7a81067b52f6acc8fe5de1acf21134ee7a  exchange-libs/contracts/src/LibMath.sol  ca6e24ec1de03bdea83351ce5f96082f8f5a9976  exchange-libs/contracts/src/LibMathRichErrors.sol  7f3b0be62d7a8d6f3026018aad08dcc9cbb41825  exchange-libs/contracts/src/LibOrder.sol  114be366ad7a0a711a0c2e552500a2c9fb1bbddf  exchange-libs/contracts/src/LibZeroExTransaction.sol  95ea4427d1df12aef259e07ac6215f2e2d9bd6d9  multisig/contracts/src/AssetProxyOwner.sol  df9ed7cba84c1362fee9de80d7774592323a86df  multisig/contracts/src/MultiSigWallet.sol  33b84d070486847dcc86a140fd682a1d8c953164  multisig/contracts/src/MultiSigWalletWithTimeLock.sol  c54d8b6631eacb20fe6bfad6ee268ab81c112614  utils/contracts/src/Authorizable.sol  2ae731a21730cfdd30feb5d20da4d4d2fa194e1d  utils/contracts/src/LibAddress.sol  33eef1855488fbbbfd1eed92101f379343a8f0f7  utils/contracts/src/LibAddressArray.sol  b13d0359922c04fadb4b24abd3d5318462c62d8e  utils/contracts/src/LibAddressArrayRichErrors.sol  883bc123ba699ba1efc11a75f806e1150e8af1ba  utils/contracts/src/LibAuthorizableRichErrors.sol  abfba41b1c63ba91803721d4d0ec6a7a1678752b  utils/contracts/src/LibBytes.sol  7a0c37b1577f5a12378fbf529177ca62314a4e62  utils/contracts/src/LibBytesRichErrors.sol  611b4e660351ee4e24140074ee1df49756e496ec  utils/contracts/src/LibEIP1271.sol  2fe0c70163677ea228d9bcfecdbba2627a5be77f  utils/contracts/src/LibEIP712.sol  3b486180d6ee3e6d5e1f2fa57c1ca060a1bdca9b  utils/contracts/src/LibFractions.sol  552a637f32edb135942cd1ea25e88d6972b8cf79  utils/contracts/src/LibOwnableRichErrors.sol  dfda0c5639f5fc994712421dc92b284071fc9e56  utils/contracts/src/LibReentrancyGuardRichErrors.sol  8af2504839d0b9a4a7a4694886704bee31fb43ad  utils/contracts/src/LibRichErrors.sol  3be89d9503f6fb6aee08aa515119af83d63f7d29  utils/contracts/src/LibSafeMath.sol  f095f7330b0d2b0d85370b47bd5ac98360ed5b48  utils/contracts/src/LibSafeMathRichErrors.sol  7785c4a4076e3f0be3319ec4bc17aca0090c2ce0  utils/contracts/src/Ownable.sol  8ede7b82d2ee0ed63b2162709d8afa7250efc3cf  utils/contracts/src/ReentrancyGuard.sol  5364694b8a2bba36861bfdd8d5886ece26e301a4  utils/contracts/src/Refundable.sol  0fe9acae963bb683b6c3539de8377ed05240bae0  utils/contracts/src/SafeMath.sol  5b675f9c12bf862a72c7dc71d00839214d970d34  utils/contracts/src/interfaces/IAuthorizable.sol  3a438f74bdb79cf6bff4dbe52a31651928601022  utils/contracts/src/interfaces/IOwnable.sol  5fe3a74b7d5948bba5644db684459d87e84fb5c6  Contracts Description Table  Function Name  Visibility  Mutability  Modifiers  Exchange  Implementation  LibEIP712ExchangeDomain, MixinMatchOrders, MixinWrapperFunctions, MixinTransferSimulator  <Constructor>  Public    LibEIP712ExchangeDomain  MixinAssetProxyDispatcher  Implementation  Ownable, IAssetProxyDispatcher  registerAssetProxy  External    onlyOwner  getAssetProxy  External    NO   _dispatchTransferFrom  Internal \ud83d\udd12  MixinExchangeCore  Implementation  IExchangeCore, Refundable, LibEIP712ExchangeDomain, MixinAssetProxyDispatcher, MixinProtocolFees, MixinSignatureValidator  cancelOrdersUpTo  External    refundFinalBalanceNoReentry  fillOrder  Public    refundFinalBalanceNoReentry  cancelOrder  Public    refundFinalBalanceNoReentry  getOrderInfo  Public    NO   _fillOrder  Internal \ud83d\udd12  _cancelOrder  Internal \ud83d\udd12  _updateFilledState  Internal \ud83d\udd12  _updateCancelledState  Internal \ud83d\udd12  _assertFillableOrder  Internal \ud83d\udd12  _assertValidCancel  Internal \ud83d\udd12  _settleOrder  Internal \ud83d\udd12  _getOrderHashAndFilledAmount  Internal \ud83d\udd12  MixinMatchOrders  Implementation  MixinExchangeCore, IMatchOrders  batchMatchOrders  Public    refundFinalBalanceNoReentry  batchMatchOrdersWithMaximalFill  Public    refundFinalBalanceNoReentry  matchOrders  Public    refundFinalBalanceNoReentry  matchOrdersWithMaximalFill  Public    refundFinalBalanceNoReentry  _assertValidMatch  Internal \ud83d\udd12  _batchMatchOrders  Internal \ud83d\udd12  _matchOrders  Internal \ud83d\udd12  _settleMatchedOrders  Internal \ud83d\udd12  MixinProtocolFees  Implementation  IProtocolFees, Ownable  setProtocolFeeMultiplier  External    onlyOwner  setProtocolFeeCollectorAddress  External    onlyOwner  _paySingleProtocolFee  Internal \ud83d\udd12  _payTwoProtocolFees  Internal \ud83d\udd12  _payProtocolFeeToFeeCollector  Internal \ud83d\udd12  MixinSignatureValidator  Implementation  LibEIP712ExchangeDomain, LibEIP1271, ISignatureValidator, MixinTransactions  preSign  External    refundFinalBalanceNoReentry  setSignatureValidatorApproval  External    refundFinalBalanceNoReentry  isValidHashSignature  Public    NO   isValidOrderSignature  Public    NO   isValidTransactionSignature  Public    NO   _isValidOrderWithHashSignature  Internal \ud83d\udd12  _isValidTransactionWithHashSignature  Internal \ud83d\udd12  _validateHashSignatureTypes  Private \ud83d\udd10  _readSignatureType  Private \ud83d\udd10  _readValidSignatureType  Private \ud83d\udd10  _encodeEIP1271OrderWithHash  Private \ud83d\udd10  _encodeEIP1271TransactionWithHash  Private \ud83d\udd10  _validateHashWithWallet  Private \ud83d\udd10  _validateBytesWithWallet  Private \ud83d\udd10  _validateBytesWithValidator  Private \ud83d\udd10  _staticCallEIP1271WalletWithReducedSignatureLength  Private \ud83d\udd10  MixinTransactions  Implementation  Refundable, LibEIP712ExchangeDomain, ISignatureValidator, ITransactions  executeTransaction  Public    disableRefundUntilEnd  batchExecuteTransactions  Public    disableRefundUntilEnd  _executeTransaction  Internal \ud83d\udd12  _assertExecutableTransaction  Internal \ud83d\udd12  _setCurrentContextAddressIfRequired  Internal \ud83d\udd12  _getCurrentContextAddress  Internal \ud83d\udd12  MixinTransferSimulator  Implementation  MixinAssetProxyDispatcher  simulateDispatchTransferFromCalls  Public    NO   MixinWrapperFunctions  Implementation  IWrapperFunctions, MixinExchangeCore  fillOrKillOrder  Public    refundFinalBalanceNoReentry  batchFillOrders  Public    refundFinalBalanceNoReentry  batchFillOrKillOrders  Public    refundFinalBalanceNoReentry  batchFillOrdersNoThrow  Public    disableRefundUntilEnd  marketSellOrdersNoThrow  Public    disableRefundUntilEnd  marketBuyOrdersNoThrow  Public    disableRefundUntilEnd  marketSellOrdersFillOrKill  Public    NO   marketBuyOrdersFillOrKill  Public    NO   batchCancelOrders  Public    refundFinalBalanceNoReentry  _fillOrKillOrder  Internal \ud83d\udd12  _fillOrderNoThrow  Internal \ud83d\udd12  IAssetProxy  Implementation  transferFrom  External    NO   getProxyId  External    NO   IAssetProxyDispatcher  Implementation  registerAssetProxy  External    NO   getAssetProxy  External    NO   IEIP1271Data  Implementation  OrderWithHash  External    NO   ZeroExTransactionWithHash  External    NO   IEIP1271Wallet  Implementation  LibEIP1271  isValidSignature  External    NO   IExchange  Implementation  IProtocolFees, IExchangeCore, IMatchOrders, ISignatureValidator, ITransactions, IAssetProxyDispatcher, ITransferSimulator, IWrapperFunctions  IExchangeCore  Implementation  cancelOrdersUpTo  External    NO   fillOrder  Public    NO   cancelOrder  Public    NO   getOrderInfo  Public    NO   IMatchOrders  Implementation  batchMatchOrders  Public    NO   batchMatchOrdersWithMaximalFill  Public    NO   matchOrders  Public    NO   matchOrdersWithMaximalFill  Public    NO   IProtocolFees  Implementation  setProtocolFeeMultiplier  External    NO   setProtocolFeeCollectorAddress  External    NO   protocolFeeMultiplier  External    NO   protocolFeeCollector  External    NO   ISignatureValidator  Implementation  preSign  External    NO   setSignatureValidatorApproval  External    NO   isValidHashSignature  Public    NO   isValidOrderSignature  Public    NO   isValidTransactionSignature  Public    NO   _isValidOrderWithHashSignature  Internal \ud83d\udd12  _isValidTransactionWithHashSignature  Internal \ud83d\udd12  ITransactions  Implementation  executeTransaction  Public    NO   batchExecuteTransactions  Public    NO   _getCurrentContextAddress  Internal \ud83d\udd12  ITransferSimulator  Implementation  simulateDispatchTransferFromCalls  Public    NO   IWallet  Implementation  isValidSignature  External    NO   IWrapperFunctions  Implementation  fillOrKillOrder  Public    NO   batchFillOrders  Public    NO   batchFillOrKillOrders  Public    NO   batchFillOrdersNoThrow  Public    NO   marketSellOrdersNoThrow  Public    NO   marketBuyOrdersNoThrow  Public    NO   marketSellOrdersFillOrKill  Public    NO   marketBuyOrdersFillOrKill  Public    NO   batchCancelOrders  Public    NO   LibExchangeRichErrorDecoder  Implementation  decodeSignatureError  Public    NO   decodeEIP1271SignatureError  Public    NO   decodeSignatureValidatorNotApprovedError  Public    NO   decodeSignatureWalletError  Public    NO   decodeOrderStatusError  Public    NO   decodeExchangeInvalidContextError  Public    NO   decodeFillError  Public    NO   decodeOrderEpochError  Public    NO   decodeAssetProxyExistsError  Public    NO   decodeAssetProxyDispatchError  Public    NO   decodeAssetProxyTransferError  Public    NO   decodeNegativeSpreadError  Public    NO   decodeTransactionError  Public    NO   decodeTransactionExecutionError  Public    NO   decodeIncompleteFillError  Public    NO   _assertSelectorBytes  Private \ud83d\udd10  IWallet  Implementation  isValidSignature  External    NO   LibEIP712ExchangeDomain  Implementation  <Constructor>  Public    LibExchangeRichErrors  Library  SignatureErrorSelector  Internal \ud83d\udd12  SignatureValidatorNotApprovedErrorSelector  Internal \ud83d\udd12  EIP1271SignatureErrorSelector  Internal \ud83d\udd12  SignatureWalletErrorSelector  Internal \ud83d\udd12  OrderStatusErrorSelector  Internal \ud83d\udd12  ExchangeInvalidContextErrorSelector  Internal \ud83d\udd12  FillErrorSelector  Internal \ud83d\udd12  OrderEpochErrorSelector  Internal \ud83d\udd12  AssetProxyExistsErrorSelector  Internal \ud83d\udd12  AssetProxyDispatchErrorSelector  Internal \ud83d\udd12  AssetProxyTransferErrorSelector  Internal \ud83d\udd12  NegativeSpreadErrorSelector  Internal \ud83d\udd12  TransactionErrorSelector  Internal \ud83d\udd12  TransactionExecutionErrorSelector  Internal \ud83d\udd12  IncompleteFillErrorSelector  Internal \ud83d\udd12  BatchMatchOrdersErrorSelector  Internal \ud83d\udd12  TransactionGasPriceErrorSelector  Internal \ud83d\udd12  TransactionInvalidContextErrorSelector  Internal \ud83d\udd12  PayProtocolFeeErrorSelector  Internal \ud83d\udd12  BatchMatchOrdersError  Internal \ud83d\udd12  SignatureError  Internal \ud83d\udd12  SignatureValidatorNotApprovedError  Internal \ud83d\udd12  EIP1271SignatureError  Internal \ud83d\udd12  SignatureWalletError  Internal \ud83d\udd12  OrderStatusError  Internal \ud83d\udd12  ExchangeInvalidContextError  Internal \ud83d\udd12  FillError  Internal \ud83d\udd12  OrderEpochError  Internal \ud83d\udd12  AssetProxyExistsError  Internal \ud83d\udd12  AssetProxyDispatchError  Internal \ud83d\udd12  AssetProxyTransferError  Internal \ud83d\udd12  NegativeSpreadError  Internal \ud83d\udd12  TransactionError  Internal \ud83d\udd12  TransactionExecutionError  Internal \ud83d\udd12  TransactionGasPriceError  Internal \ud83d\udd12  TransactionInvalidContextError  Internal \ud83d\udd12  IncompleteFillError  Internal \ud83d\udd12  PayProtocolFeeError  Internal \ud83d\udd12  LibFillResults  Library  calculateFillResults  Internal \ud83d\udd12  calculateMatchedFillResults  Internal \ud83d\udd12  addFillResults  Internal \ud83d\udd12  _calculateMatchedFillResults  Private \ud83d\udd10  _calculateMatchedFillResultsWithMaximalFill  Private \ud83d\udd10  _calculateCompleteFillBoth  Private \ud83d\udd10  _calculateCompleteRightFill  Private \ud83d\udd10  LibMath  Library  safeGetPartialAmountFloor  Internal \ud83d\udd12  safeGetPartialAmountCeil  Internal \ud83d\udd12  getPartialAmountFloor  Internal \ud83d\udd12  getPartialAmountCeil  Internal \ud83d\udd12  isRoundingErrorFloor  Internal \ud83d\udd12  isRoundingErrorCeil  Internal \ud83d\udd12  LibMathRichErrors  Library  DivisionByZeroError  Internal \ud83d\udd12  RoundingError  Internal \ud83d\udd12  LibOrder  Library  getTypedDataHash  Internal \ud83d\udd12  getStructHash  Internal \ud83d\udd12  LibZeroExTransaction  Library  getTypedDataHash  Internal \ud83d\udd12  getStructHash  Internal \ud83d\udd12  TestLibEIP712ExchangeDomain  Implementation  LibEIP712ExchangeDomain  <Constructor>  Public    LibEIP712ExchangeDomain  TestLibFillResults  Implementation  calculateFillResults  Public    NO   calculateMatchedFillResults  Public    NO   addFillResults  Public    NO   TestLibMath  Implementation  safeGetPartialAmountFloor  Public    NO   safeGetPartialAmountCeil  Public    NO   getPartialAmountFloor  Public    NO   getPartialAmountCeil  Public    NO   isRoundingErrorFloor  Public    NO   isRoundingErrorCeil  Public    NO   TestLibOrder  Implementation  getTypedDataHash  Public    NO   getStructHash  Public    NO   TestLibZeroExTransaction  Implementation  getTypedDataHash  Public    NO   getStructHash  Public    NO   AssetProxyOwner  Implementation  MultiSigWalletWithTimeLock  <Constructor>  Public    MultiSigWalletWithTimeLock  registerFunctionCall  External    onlyWallet  executeTransaction  Public    notExecuted fullyConfirmed  _registerFunctionCall  Internal \ud83d\udd12  _assertValidFunctionCall  Internal \ud83d\udd12  MultiSigWallet  Implementation  <Fallback>  External    NO   <Constructor>  Public    validRequirement  addOwner  Public    onlyWallet ownerDoesNotExist notNull validRequirement  removeOwner  Public    onlyWallet ownerExists  replaceOwner  Public    onlyWallet ownerExists ownerDoesNotExist  changeRequirement  Public    onlyWallet validRequirement  submitTransaction  Public    NO   confirmTransaction  Public    ownerExists transactionExists notConfirmed  revokeConfirmation  Public    ownerExists confirmed notExecuted  executeTransaction  Public    ownerExists confirmed notExecuted  _externalCall  Internal \ud83d\udd12  isConfirmed  Public    NO   _addTransaction  Internal \ud83d\udd12  notNull  getConfirmationCount  Public    NO   getTransactionCount  Public    NO   getOwners  Public    NO   getConfirmations  Public    NO   getTransactionIds  Public    NO   MultiSigWalletWithTimeLock  Implementation  MultiSigWallet  <Constructor>  Public    MultiSigWallet  changeTimeLock  Public    onlyWallet  confirmTransaction  Public    ownerExists transactionExists notConfirmed notFullyConfirmed  executeTransaction  Public    notExecuted fullyConfirmed pastTimeLock  _setConfirmationTime  Internal \ud83d\udd12  Authorizable  Implementation  Ownable, IAuthorizable  <Constructor>  Public    Ownable  addAuthorizedAddress  External    onlyOwner  removeAuthorizedAddress  External    onlyOwner  removeAuthorizedAddressAtIndex  External    onlyOwner  getAuthorizedAddresses  External    NO   _assertSenderIsAuthorized  Internal \ud83d\udd12  _addAuthorizedAddress  Internal \ud83d\udd12  _removeAuthorizedAddressAtIndex  Internal \ud83d\udd12  LibAddress  Library  isContract  Internal \ud83d\udd12  LibAddressArray  Library  append  Internal \ud83d\udd12  contains  Internal \ud83d\udd12  indexOf  Internal \ud83d\udd12  LibAddressArrayRichErrors  Library  MismanagedMemoryError  Internal \ud83d\udd12  LibAuthorizableRichErrors  Library  AuthorizedAddressMismatchError  Internal \ud83d\udd12  IndexOutOfBoundsError  Internal \ud83d\udd12  SenderNotAuthorizedError  Internal \ud83d\udd12  TargetAlreadyAuthorizedError  Internal \ud83d\udd12  TargetNotAuthorizedError  Internal \ud83d\udd12  ZeroCantBeAuthorizedError  Internal \ud83d\udd12  LibBytes  Library  rawAddress  Internal \ud83d\udd12  contentAddress  Internal \ud83d\udd12  memCopy  Internal \ud83d\udd12  slice  Internal \ud83d\udd12  sliceDestructive  Internal \ud83d\udd12  popLastByte  Internal \ud83d\udd12  equals  Internal \ud83d\udd12  readAddress  Internal \ud83d\udd12  writeAddress  Internal \ud83d\udd12  readBytes32  Internal \ud83d\udd12  writeBytes32  Internal \ud83d\udd12  readUint256  Internal \ud83d\udd12  writeUint256  Internal \ud83d\udd12  readBytes4  Internal \ud83d\udd12  writeLength  Internal \ud83d\udd12  LibBytesRichErrors  Library  InvalidByteOperationError  Internal \ud83d\udd12  LibEIP1271  Implementation  LibEIP712  Library  hashEIP712Domain  Internal \ud83d\udd12  hashEIP712Message  Internal \ud83d\udd12  LibFractions  Library  add  Internal \ud83d\udd12  normalize  Internal \ud83d\udd12  normalize  Internal \ud83d\udd12  scaleDifference  Internal \ud83d\udd12  LibOwnableRichErrors  Library  OnlyOwnerError  Internal \ud83d\udd12  TransferOwnerToZeroError  Internal \ud83d\udd12  LibReentrancyGuardRichErrors  Library  IllegalReentrancyError  Internal \ud83d\udd12  LibRichErrors  Library  StandardError  Internal \ud83d\udd12  rrevert  Internal \ud83d\udd12  LibSafeMath  Library  safeMul  Internal \ud83d\udd12  safeDiv  Internal \ud83d\udd12  safeSub  Internal \ud83d\udd12  safeAdd  Internal \ud83d\udd12  max256  Internal \ud83d\udd12  min256  Internal \ud83d\udd12  LibSafeMathRichErrors  Library  Uint256BinOpError  Internal \ud83d\udd12  Uint256DowncastError  Internal \ud83d\udd12  Ownable  Implementation  IOwnable  <Constructor>  Public    transferOwnership  Public    onlyOwner  _assertSenderIsOwner  Internal \ud83d\udd12  ReentrancyGuard  Implementation  _lockMutexOrThrowIfAlreadyLocked  Internal \ud83d\udd12  _unlockMutex  Internal \ud83d\udd12  Refundable  Implementation  ReentrancyGuard  _refundNonZeroBalanceIfEnabled  Internal \ud83d\udd12  _refundNonZeroBalance  Internal \ud83d\udd12  _disableRefund  Internal \ud83d\udd12  _enableAndRefundNonZeroBalance  Internal \ud83d\udd12  _areRefundsDisabled  Internal \ud83d\udd12  SafeMath  Implementation  _safeMul  Internal \ud83d\udd12  _safeDiv  Internal \ud83d\udd12  _safeSub  Internal \ud83d\udd12  _safeAdd  Internal \ud83d\udd12  _max256  Internal \ud83d\udd12  _min256  Internal \ud83d\udd12  IAuthorizable  Implementation  IOwnable  addAuthorizedAddress  External    NO   removeAuthorizedAddress  External    NO   removeAuthorizedAddressAtIndex  External    NO   getAuthorizedAddresses  External    NO   IOwnable  Implementation  transferOwnership  Public    NO   Legend  Function can modify state  Function is payable  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"
    },
    {
        "title": "5.1 Superfluous Permission endowment:ethereum-provider    ",
        "body": "  Resolution   fixed in   ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by removing the ethereum provider permission from the manifest.  Description  The snap requests permission endowment:ethereum-provider but window.ethereum is never accessed from within the snap s context.  snap/snap.manifest.json:L39  \"endowment:ethereum-provider\": {}  Recommendation  Remove superfluous permissions.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"
    },
    {
        "title": "5.2 A Trusted Website Can Add Any Address to the Snaps Address Storage; No Control Over Added Addresses; Confirmation Is a Notification    ",
        "body": "  Resolution  partially addressed in ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by only allowing trusted origins to interact with the snap.  Update: user confirmation for address management (add/remove current account) added with ethereum-push-notification-service/push-protocol-snaps@7ee018947303014e8c14e9413a5edd9fd29f9829  Description  Trusted websites can add addresses to the list of addresses the user wants to receive notifications for. However, the user has no control over the addresses, and even though the code suggests that the snap user must confirm new address addition, this confirmation is merely a notification that the address has been added.  The lack of address management may lead to a self-DoS when too many addresses are added to the extension.  await window.ethereum?.request({  method: \"wallet_invokeSnap\",  params: {  snapId: \"local:http://localhost:8080\",  request: { method: 'hello', params: { address: \"\\nhi\\nho\" } },  }})  push-snap-site/components/buttons/ConfirmButton.tsx:L6-L35  export default function ConfirmButton() {  const { address, isConnecting, isDisconnected } = useAccount();  const defaultSnapOrigin = `local:http://localhost:8080`;  const sendHello = async (address: string) => {  await window.ethereum?.request({  method: \"wallet_invokeSnap\",  params: {  snapId: defaultSnapOrigin,  request: { method: 'hello', params: { address: address } },  },  });  };  const { data, isError, isLoading, isSuccess, signMessage } = useSignMessage({  message:  `Confirm your Address ${address}, \\n this will be added to MetaMask for sending notifications`,  });  function sleep(ms: number) {  return new Promise((resolve) => setTimeout(resolve, ms));  const confirmAddition=async()=>{  signMessage();  if(isSuccess){  await sleep(5000);  await sendHello(String(address));  The same is true for configuration settings. Any connected dap may set togglepopup. This may be problematic in multi-dapp scenarios where multiple dapps request to set togglepopup.  Recommendation  Note that dapps are not necessarily completely trusted. They can be modified, or malicious behavior may be added later by the dapp deployer (unless used locally or via IPFS). Therefore, the snap should always notify the wallet owner of important state changes and allow them to reject them or, in this case, manage addresses that ve been added previously.  Consider checking the origin in onRPC if this dapp is only meant to be called from a specific dapp address. Otherwise, any connected dapp may change configuration settings.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"
    },
    {
        "title": "5.3 Lax Input Validation, Control Char, URI, and Markdown Injection    ",
        "body": "  Resolution  addressed in ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 validating the address with ethers.utils.isAddress.  Update 1:  Markdown rendering of newlines fixed with: ethereum-push-notification-service/push-protocol-snaps@7ee018947303014e8c14e9413a5edd9fd29f9829  Major: Markdown Injection in Confirmation Dialogue re-introduced with ethereum-push-notification-service/push-protocol-snaps@7ee018947303014e8c14e9413a5edd9fd29f9829  Update 2:  Markdown Injection in Confirmation Dialogue fixed with ethereum-push-notification-service/push-protocol-snaps@b40e141243c77bfd7ec109408b326607b19314c8  Description  There is no input validation on the address to be added. The input may be an ethereum address but can be anything, potentially breaking security assumptions in the code and leading to unwanted side effects.  request.params may be null, and  request.params.address may not be an ethereum address.  snap/src/index.ts:L18  await addAddress(request.params.address || \"0x0\");  Example  await window.ethereum?.request({  method: \"wallet_invokeSnap\",  params: {  snapId: \"local:http://localhost:8080\",  request: { method: 'hello', params: { address: \"Hi \ud83d\ude4c\\n\\n \ud83d\udd38 **boom**\" } },  }})  URI injection if address contains ?#/  snap/src/utils/fetchnotifs.ts:L3-L13  export const getNotifications=async(address:string)=>{  const url = `https://backend-prod.epns.io/apis/v1/users/eip155:5:${address}/feeds`;  const response = await fetch(url, {  method: 'get',  headers: {  'Content-Type': 'application/json',  },  });  const data = await response.json();  return data;  Injection in notifications  snap/src/utils/popupHelper.ts:L3-L12  export const popupHelper = (notifs: String[]) => {  let msg = [];  if (notifs.length > 0) {  notifs.forEach((notif) => {  let str = `\\n\ud83d\udd14` + notif + \"\\n\";  msg.push(str);  });  return msg;  };  Markdown injection  snap/src/utils/fetchAddress.ts:L45-L52  const data = persistedData.addresses;  const popup = persistedData.popuptoggle;  let msg='';  for(let i = 0; i < data!.length; i++){  msg = msg + '\ud83d\udd39' + data![i] + '\\n';  return snap.request({  method: 'snap_dialog',  Also, note that the currently rendered markdown that lists addresses appears wrong, as markdown newlines require \\n\\n instead of \\n.  Recommendation  Strictly validate inputs from external origins. Ensure that the provided address is a valid ethereum address. Optionally check the addresses checksum to detect typos. Ensure that inputs may not lead to renderable markdown. Fix the rendered list of addresses to properly display as a newline d list. Ensure untrusted inputs cannot inject context-sensitive information into fetch urls.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"
    },
    {
        "title": "5.4 persistedData Race Where snap_manageState.get Returnsnull    ",
        "body": "  Resolution   addressed with   ethereum-push-notification-service/push-protocol-snaps@7ee018947303014e8c14e9413a5edd9fd29f9829 by introducing a wrapper function that ensures that snapstate returns sane defaults. This function is not used everywhere, but in places where it is not, custom checks are employed.  Description  Metamask Error:  snap.request(, {method: 'snap_manageState', params: {operation: 'get'}}) may return null. Snap state is only initialized on rpc request method hello via addAddress().  This is the only method that checks if the retrieved state is null:  snap/src/utils/fetchAddress.ts:L5-L20  export const addAddress = async (address:string) => {  const persistedData = await snap.request({  method: 'snap_manageState',  params: { operation: 'get' },  });  if(persistedData == null){  const data = {  addresses: [address],  popuptoggle: 0,  };  await snap.request({  method: 'snap_manageState',  params: { operation: 'update', newState:data },  });  snap/src/index.ts:L12-L21  export const onRpcRequest: OnRpcRequestHandler = async ({  origin,  request,  }) => {  switch (request.method) {  case \"hello\": {  await addAddress(request.params.address || \"0x0\");  await confirmAddress();  break;  If the state was never initialized or there was a race where rpc-hello() was not called first, then the snap may run into a null deref exception (here rpc-togglepopup):  snap/src/utils/toggleHelper.ts:L2-L12  let persistedData = await snap.request({  method: 'snap_manageState',  params: { operation: 'get' },  });  let popuptoggle = notifcount;  const data = {  addresses: persistedData.addresses,  popuptoggle: popuptoggle,  };  Recommendation  Wrap snap_manageState with a function that always falls back to safe defaults if the snap state was never set. This also obsoleted the future need to check if persistedData is null as the new method ensures safe non-null defaults.  This should also silence some of the type errors reported by tslint that warn that attributes of persistentdata are read while it might be null (see issue 5.6 ).  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"
    },
    {
        "title": "5.5 User Flow - Request to Sign Message Does Not Provide Security Guarantee    ",
        "body": "  Resolution   obsolete, removed with   ethereum-push-notification-service/push-protocol-snaps@7ee018947303014e8c14e9413a5edd9fd29f9829.  Description  A connected dapp can add any address to the snap via the RPC method hello. There is no added security by requesting the user to sign with their address as the backend API gives access to any address notification (they are not private) and the dapps request is a front-end-only solution. A user may add any other address by creating their dapp which allows custom addresses.  In light of this, the front-end (dapp) security check requiring the user to prove that they are in possession of the private key appears not to add any security guarantees to the snap. Instead, the snap may want to enumerate wallet account addresses internally instead and remove the hello API altogether, or, allow any address to be added without requiring a proof of ownership of an address.  Examples  push-snap-site/components/buttons/ConfirmButton.tsx:L20-L23  const { data, isError, isLoading, isSuccess, signMessage } = useSignMessage({  message:  `Confirm your Address ${address}, \\n this will be added to MetaMask for sending notifications`,  });  Recommendation  Remove the signature check, and add linked accounts from within the snaps context. Be transparent that notification texts are not private, and anyone can subscribe to the back-end API. If notifications are private to the recipient, we suggest encrypting them for the target account and adding logic in the snap to allow the recipient to decrypt them within the context of the snap.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"
    },
    {
        "title": "5.6 TypeScript Errors    ",
        "body": "  Resolution  partially addressed in ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761:  toggleHelper not addressed.  popupHelper addressed as per recommendation.  fetchAllAddrNotifs fixed by forcing fetchAddress() to return empty array instead.  persistedData partially addressed. might still null-deref at persistedData.addresses in index.ts  Update: toggleHelper and persistedData addressed with the snap data check wrapper function in ethereum-push-notification-service/push-protocol-snaps@7ee018947303014e8c14e9413a5edd9fd29f9829  Description  toggleHelper  persistedData should be checked for null and default to a sane initial config. notifcount:Number should be notifcount:number.  Type '{ addresses: Json; popuptoggle: Number; }' is not assignable to type 'Record<string, Json>'.  Property 'popuptoggle' is incompatible with index signature.  Type 'Number' is not assignable to type 'Json'.  Type 'Number' is not assignable to type '{ [prop: string]: Json; }'.  Index signature for type 'string' is missing in type 'Number'.ts(2322)  snap/src/utils/toggleHelper.ts:L7-L16  let popuptoggle = notifcount;  const data = {  addresses: persistedData.addresses,  popuptoggle: popuptoggle,  };  await snap.request({  method: 'snap_manageState',  params: { operation: 'update', newState:data },  });  popupHelper  let msg = [] should be let msg = [] as String[];  Variable 'msg' implicitly has an 'any[]' type.ts(7005)  addresses can be null  snap/src/utils/fetchnotifs.ts:L34-L37  export const fetchAllAddrNotifs = async () => {  const addresses = await fetchAddress();  let notifs:String[] = [];  for(let i = 0; i < addresses.length; i++){  persistedData can be null  snap/src/index.ts:L63-L68  let persistedData = await snap.request({  method: \"snap_manageState\",  params: { operation: \"get\" },  });  let popuptoggle = Number(persistedData.popuptoggle) + msgs.length;  Recommendation  Fix the typescript configuration (see issue 5.13 ). Fix all reported ts-lint errors. Avoid using any types and use safe types instead.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"
    },
    {
        "title": "5.7 Avoid Hardcoding the Local Snap ID ",
        "body": "  Description  The local snap-id is hardcoded in various places. Local snap IDs should not be used in production. Hence, we recommend defining and importing the snap id from a single source file within the project, setting it to local:http://localhost:8080 and npm:push-v1 depending on whether the build is set to be production or development (e.g., using an environment variable).  push-snap-site/components/buttons/ConfirmButton.tsx:L6-L8  export default function ConfirmButton() {  const { address, isConnecting, isDisconnected } = useAccount();  const defaultSnapOrigin = `local:http://localhost:8080`;  push-snap-site/components/buttons/ReconnectButton.tsx:L4-L6  export default function ReconnectButton() {  const defaultSnapOrigin = `local:http://localhost:8080`;  push-snap-site/components/buttons/SendMessageButton.tsx:L1-L3  export default function ReconnectButton() {  const defaultSnapOrigin = `local:http://localhost:8080`;  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"
    },
    {
        "title": "5.8 package.json - Invalid License    ",
        "body": "  Resolution   fixed in   ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by changing the license to GPLv2.  Description  The license field in package.json is invalid.  snap/package.json:L9  \"license\": \"(MIT-0 OR Apache-2.0)\",  Recommendation  Update the license field.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"
    },
    {
        "title": "5.9 fetchAddress  - Inaccurate Function Name ",
        "body": "  Description  Function fetchAddress returns an array of addresses and should, therefore, be named fetchAddresses  snap/src/utils/fetchAddress.ts:L66-L73  export const fetchAddress = async () => {  const persistedData = await snap.request({  method: 'snap_manageState',  params: { operation: 'get' },  });  const addresses = persistedData!.addresses;  return addresses;  };  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"
    },
    {
        "title": "5.10 currentepoch - Unnecesary Conversion From/to String    ",
        "body": "  Resolution   fixed in   ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by not converting  Description  It is unclear why currentepoch is declared as String while calculations require it to be numerical.  Examples  snap/src/utils/fetchnotifs.ts:L15-L31  export const filterNotifications=async(address:string)=>{  let fetchedNotifications = await getNotifications(address);  fetchedNotifications = fetchedNotifications?.feeds;  let notiffeeds:String[] = [];  const currentepoch:string = Math.floor(Date.now() / 1000).toString();  if(fetchedNotifications.length > 0){  for(let i = 0; i < fetchedNotifications.length; i++){  let feedepoch = fetchedNotifications[i].payload.data.epoch;  feedepoch = Number(feedepoch).toFixed(0);  if(feedepoch > parseInt(currentepoch)-60) {  let msg = fetchedNotifications[i].payload.data.app+' : '+fetchedNotifications[i].payload.data.amsg;  notiffeeds.push(msg);  notiffeeds = notiffeeds.reverse();  return notiffeeds;  Recommendation  currentepoch should be numerical.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"
    },
    {
        "title": "5.11 Dead Code popup    ",
        "body": "  Resolution   fixed in   ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by removing the used code.  Description  const popup is retrieved from the snap state but never used within the context of confirmAddress(). This might be an indicator of an incomplete implementation of the togglePopup setting or dead code.  snap/src/utils/fetchAddress.ts:L40-L47  export const confirmAddress = async () => {  const persistedData = await snap.request({  method: 'snap_manageState',  params: { operation: 'get' },  });  const data = persistedData.addresses;  const popup = persistedData.popuptoggle;  let msg='';  Recommendation  Double check if this setting is meant to be read (unlikely) or else clean up and remove unused code.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"
    },
    {
        "title": "5.12 Unused Import ethers, @metamask/snaps-ui    ",
        "body": "  Resolution   fixed in   ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by using  Description  ethers  ethers is listed as a dependency and imported by fetchAddress.ts but is never used.  snap/src/utils/fetchAddress.ts:L3  const {ethers} = require('ethers');  @metamask/snaps-ui  @metamask/snaps-ui is imported in popupHelper but the imported components are never used.  snap/src/utils/popupHelper.ts:L1  import { heading, panel, text } from \"@metamask/snaps-ui\";  Recommendation  Remove the unused import/dependency.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"
    },
    {
        "title": "5.13 Non-Existent Base Config (Eslint, Tsconfig) ",
        "body": "  Description  .eslintrc.js points to a base configuration outside of this repository.  snap/.eslintrc.js:L2  extends: ['../../.eslintrc.js'],  .eslintrc.js  tsconfig.json  snap/tsconfig.json:L2  \"extends\": \"../../tsconfig.json\",  Recommendation  Provide the eslint base configuration with the repository to allow for reproducible lint runs. Run the linter as part of github commit checks.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"
    },
    {
        "title": "5.14 Performance - await in for Loop   ",
        "body": "  Resolution   fixed in   ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by using  Description  Performing an await as part of each operation is an indication that the program is not taking full advantage of the parallelization benefits of async/await:  snap/src/utils/fetchnotifs.ts:L38  let temp = await filterNotifications(addresses[i]);  Recommendation  Using Promise.all() fully utilizes parallelism and improves performance  ",
        "labels": [
            "Consensys",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"
    },
    {
        "title": "5.15 API Design - Consider Using Consistent RPC Method Names   ",
        "body": "  Resolution   fixed in   ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by changing the rpc method names as per recommendation.  Description  Consider using descriptive RPC method names with a distinct prefix, e.g. pushproto_initialize, pushproto_addaddress, pushprotoc_togglepopup:  snap/src/index.ts:L17  case \"hello\": {  snap/src/index.ts:L22  case \"init\": {  snap/src/index.ts:L36  case \"togglepopup\": {  Note that init can be called multiple times and is not initializing anything.  ",
        "labels": [
            "Consensys",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"
    },
    {
        "title": "7.1 readOnlyMode is ineffective and may result in a false sense of security    Addressed",
        "body": "  Resolution   This was addressed in   PegaSysEng/permissioning-smart-contracts@ed2d4a2 by adding comments to clarify that  Description  AccountRules and NodeRules can both enter and exit a mode of operation called readOnlyMode.  The only effect of readOnlyMode is to prevent admins (who are the only users able to change rules) from changing rules.  Those same admins can disable readOnlyMode, so this mode will not prevent a determined actor from doing something they want to do.  Recommendation  Either readOnlyMode should be removed to prevent it from providing a false sense of security, or the authorization required to toggle readOnlyMode should be separated from the authorization required to change rules.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"
    },
    {
        "title": "7.2 Ingress.setContractAddress() can cause duplicate entries in contractKeys    ",
        "body": "  Resolution   This is fixed in   PegaSysEng/permissioning-smart-contracts@faff726.  Description  code/contracts/Ingress.sol:L39-L62  function setContractAddress(bytes32 name, address addr) public returns (bool) {  require(name > 0x0000000000000000000000000000000000000000000000000000000000000000, \"Contract name must not be empty.\");  require(isAuthorized(msg.sender), \"Not authorized to update contract registry.\");  ContractDetails memory info = registry[name];  // create info if it doesn't exist in the registry  if (info.contractAddress == address(0)) {  info = ContractDetails({  owner: msg.sender,  contractAddress: addr  });  // Update registry indexing  contractKeys.push(name);  } else {  info.contractAddress = addr;  // update record in the registry  registry[name] = info;  emit RegistryUpdated(addr,name);  return true;  If, however, a contract is actually added with the address 0, which is currently allowed in the code, then the contract does already exists, and adding the name to contractKeys again will result in a duplicate.  Mitigation  An admin can call removeContract repeatedly with the same name to remove multiple duplicate entries.  Recommendation  Either disallow a contract address of 0 or check for existence via the owner field instead (which can never be 0).  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"
    },
    {
        "title": "7.3 Use specific contract types instead of address where possible    ",
        "body": "  Resolution   This is fixed in   PegaSysEng/permissioning-smart-contracts@05d33ae and  PegaSysEng/permissioning-smart-contracts@2728bac.  Description  For clarity and to get more out of the Solidity type checker, it s generally preferred to use a specific contract type for variables rather than the generic address.  Examples  AccountRules.ingressContractAddress could instead be AccountRules.ingressContract and use the type IngressContract:  code/contracts/AccountRules.sol:L16  address private ingressContractAddress;  code/contracts/AccountRules.sol:L24  AccountIngress ingressContract = AccountIngress(ingressContractAddress);  code/contracts/AccountRules.sol:L32  constructor (address ingressAddress) public {  This same pattern is found in NodeRules:  code/contracts/NodeRules.sol:L32  address private nodeIngressContractAddress;  Recommendation  Where possible, use a specific contract type rather than address.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"
    },
    {
        "title": "7.4 Ingress should use a set    ",
        "body": "  Resolution   This is fixed in   PegaSysEng/permissioning-smart-contracts@2978bd0 and  PegaSysEng/permissioning-smart-contracts@f973035.  Description  The AdminList, AccountRulesList, and NodeRulesList contracts have been recently rewritten to use a set. Ingress has the semantics of a set but has not been written the same way.  This leads to some inefficiencies. In particular, Ingress.removeContract is an O(n) operation:  code/contracts/Ingress.sol:L68-L74  for (uint i = 0; i < contractKeys.length; i++) {  // Delete the key from the array + mapping if it is present  if (contractKeys[i] == name) {  delete registry[contractKeys[i]];  contractKeys[i] = contractKeys[contractKeys.length - 1];  delete contractKeys[contractKeys.length - 1];  contractKeys.length--;  Recommendation  Use the same set implementation for Ingress: an array of ContractDetails and a mapping of names to indexes in that array.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"
    },
    {
        "title": "7.5 Use a specific Solidity compiler version    ",
        "body": "  Resolution   This is fixed in   PegaSysEng/permissioning-smart-contracts@acf5a22 by pinning to Solidity 0.5.9 everywhere except the  Description  A number of files use a  floating  pragma as follows:  pragma solidity >=0.4.22 <0.6.0;  It s better to use a specific Solidity compiler version (preferably a current version). This removes any confusion about which compiler was used when the contract is deployed, and it makes sure the code is never subjected to older compiler bugs.  It s still a good idea to upgrade the compiler version in the future as compiler bugs are fixed, but this way you must explicitly choose the new compiler version in your code when you do so.  Recommendation  Based on the Truffle configuration, the code is currently compiled with Solidity 0.5.9. Consider changing the existing pragmas to the following:  pragma solidity 0.5.9;  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"
    },
    {
        "title": "7.6 ContractDetails.owner is never read    ",
        "body": "  Resolution   This is fixed in   PegaSysEng/permissioning-smart-contracts@d3f505e.  Description  The ContractDetails struct used by Ingress contracts has an owner field that is written to, but it is never read.  code/contracts/Ingress.sol:L14-L19  struct ContractDetails {  address owner;  address contractAddress;  mapping(bytes32 => ContractDetails) registry;  Recommendation  If owner is not (yet) needed, the ContractDetails struct should be removed altogether and the type of Ingress.registry should change to mapping(bytes32 => address)  8 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"
    },
    {
        "title": "8.1 MythX",
        "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  Below is the raw output of the MythX vulnerability scan:  Summary  40 problems (0 errors, 40 warnings)  Warnings  SWC-108  XXXXX  SWC-131  27  XXXXXXXXXXXXXXXXXXXX  SWC-110  XXX  SWC-128  XXX  SWC-123  XX  Details  AccountRules.sol - 7 problems (0 errors, 7 warnings)  Warning  12:9  The state variable visibility is not set. It is best practice to set the visibility of state variables explicitly. The default visibility for \"readOnlyMode\" is internal. Other possible visibility values are public and private.  SWC-108  Warning  14:9  The state variable visibility is not set. It is best practice to set the visibility of state variables explicitly. The default visibility for \"version\" is internal. Other possible visibility values are public and private.  SWC-108  Warning  61:8  Unused local variable \"\" The local variable \"\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  62:8  Unused local variable \"\" The local variable \"\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  63:8  Unused local variable \"\" The local variable \"\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  64:8  Unused local variable \"\" The local variable \"\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  65:8  Unused local variable \"\" The local variable \"\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  AccountRulesList.sol - 2 problems (0 errors, 2 warnings)  Warning  15:4  A reachable exception has been detected. It is possible to trigger an exception (opcode 0xfe). Exceptions can be caused by type errors, division by zero, out-of-bounds array access, or assert violations. Note that explicit assert() should only be used to check invariants. Use require() for regular input checking.  SWC-110  Warning  36:8  Potential denial-of-service if block gas limit is reached. A storage modification is executed in a loop. Be aware that the transaction may fail to execute if the loop is unbounded and the necessary gas exceeds the block gas limit.  SWC-128  AccountRulesProxy.sol - 12 problems (0 errors, 12 warnings)  Warning  5:8  Unused local variable \"sender\" The local variable \"sender\" is created within the contract \"AccountRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  5:8  Unused local variable \"sender\" The local variable \"sender\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  6:8  Unused local variable \"target\" The local variable \"target\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  6:8  Unused local variable \"target\" The local variable \"target\" is created within the contract \"AccountRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  7:8  Unused local variable \"value\" The local variable \"value\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  7:8  Unused local variable \"value\" The local variable \"value\" is created within the contract \"AccountRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  8:8  Unused local variable \"gasPrice\" The local variable \"gasPrice\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  8:8  Unused local variable \"gasPrice\" The local variable \"gasPrice\" is created within the contract \"AccountRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  9:8  Unused local variable \"gasLimit\" The local variable \"gasLimit\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  9:8  Unused local variable \"gasLimit\" The local variable \"gasLimit\" is created within the contract \"AccountRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  10:8  Unused local variable \"payload\" The local variable \"payload\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  10:8  Unused local variable \"payload\" The local variable \"payload\" is created within the contract \"AccountRulesProxy\" but does not seem to be used anywhere.  SWC-131  AdminList.sol - 3 problems (0 errors, 3 warnings)  Warning  17:4  A reachable exception has been detected. It is possible to trigger an exception (opcode 0xfe). Exceptions can be caused by type errors, division by zero, out-of-bounds array access, or assert violations. Note that explicit assert() should only be used to check invariants. Use require() for regular input checking.  SWC-110  Warning  38:8  Potential denial-of-service if block gas limit is reached. A storage modification is executed in a loop. Be aware that the transaction may fail to execute if the loop is unbounded and the necessary gas exceeds the block gas limit.  SWC-128  Warning  42:23  Potential denial-of-service if block gas limit is reached. A storage modification is executed in a loop. Be aware that the transaction may fail to execute if the loop is unbounded and the necessary gas exceeds the block gas limit.  SWC-128  AdminProxy.sol - 3 problems (0 errors, 3 warnings)  Warning  4:10  precondition violation A precondition was violated. Make sure valid inputs are provided to both callees (e.g, via passed arguments) and callers (e.g., via return values).  SWC-123  Warning  4:26  Unused local variable \"source\" The local variable \"source\" is created within the contract \"Admin\" but does not seem to be used anywhere.  SWC-131  Warning  4:26  Unused local variable \"source\" The local variable \"source\" is created within the contract \"AdminProxy\" but does not seem to be used anywhere.  SWC-131  Ingress.sol - 3 problems (0 errors, 3 warnings)  Warning  12:14  The state variable visibility is not set. It is best practice to set the visibility of state variables explicitly. The default visibility for \"contractKeys\" is internal. Other possible visibility values are public and private.  SWC-108  Warning  19:40  The state variable visibility is not set. It is best practice to set the visibility of state variables explicitly. The default visibility for \"registry\" is internal. Other possible visibility values are public and private.  SWC-108  Warning  35:19  precondition violation A precondition was violated. Make sure valid inputs are provided to both callees (e.g, via passed arguments) and callers (e.g., via return values).  SWC-123  NodeRulesList.sol - 1 problem (0 errors, 1 warning)  Warning  15:4  assertion violation An assertion was violated. Make sure your program logic is correct (e.g., no division by zero) and that you add appropriate validation for inputs from both callers (e.g, passed arguments) and callees (e.g., return values).  SWC-110  NodeIngress.sol - 1 problem (0 errors, 1 warning)  Warning  9:9  The state variable visibility is not set. It is best practice to set the visibility of state variables explicitly. The default visibility for \"version\" is internal. Other possible visibility values are public and private.  SWC-108  NodeRulesProxy.sol - 8 problems (0 errors, 8 warnings)  Warning  5:8  Unused local variable \"sourceEnodeHigh\" The local variable \"sourceEnodeHigh\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  6:8  Unused local variable \"sourceEnodeLow\" The local variable \"sourceEnodeLow\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  7:8  Unused local variable \"sourceEnodeIp\" The local variable \"sourceEnodeIp\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  8:8  Unused local variable \"sourceEnodePort\" The local variable \"sourceEnodePort\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  9:8  Unused local variable \"destinationEnodeHigh\" The local variable \"destinationEnodeHigh\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  10:8  Unused local variable \"destinationEnodeLow\" The local variable \"destinationEnodeLow\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  11:8  Unused local variable \"destinationEnodeIp\" The local variable \"destinationEnodeIp\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  12:8  Unused local variable \"destinationEnodePort\" The local variable \"destinationEnodePort\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  AccountIngress.sol - 0 problems  Admin.sol - 0 problems  ExposedAccountRulesList.sol - 0 problems  ExposedAdminList.sol - 0 problems  ExposedNodeRulesList.sol - 0 problems  Generated on Thu Aug 29 2019 15:16:37 GMT-0700 (Pacific Daylight Time)  MythX Logs:  AccountRules.sol  UUID: 6db36465-5d19-43b8-8318-20d038616ffb  info: skipped automated fuzz testing due to incompatible bytecode input  AccountRulesList.sol  UUID: 17faa2da-60ed-4e9c-8f76-c9d87ebfa025  AccountRulesProxy.sol  UUID: 0579eb33-82ef-4ac7-99e1-948ba46955df  Admin.sol  UUID: da4012ea-98e3-4116-9ee9-896da7904e7c  AdminList.sol  UUID: 6a5da947-d87d-4f3a-b3e6-94d76712aa73  AdminProxy.sol  UUID: ef18baac-d986-4bcd-aefc-1d0801e214d2  ExposedAccountRulesList.sol  UUID: 706738e2-35a4-4def-b7c4-f680920db1a1  ExposedAdminList.sol  UUID: ea78f05f-c0f2-46e0-84eb-0168d35fccc4  ExposedNodeRulesList.sol  UUID: 33d4d1a4-2f85-4d6d-99c7-dd76c738d305  Ingress.sol  UUID: 162745bf-308e-4cc8-a07b-b5e1564f7764  NodeIngress.sol  UUID: a40eebe4-d51e-40fd-8b0b-913491e63411  NodeRulesList.sol  UUID: c5d7bd11-591d-4bd6-8364-883d8db35bb9  NodeRulesProxy.sol  UUID: 851c3349-b9f2-459c-a3ec-5bbd7cb6d616  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"
    },
    {
        "title": "8.2 Ethlint",
        "body": "  Ethlint is an open source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Ethlint didn t find any issues.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"
    },
    {
        "title": "8.3 Surya",
        "body": "  Surya is an utility tool for smart contract systems. It provides a number of visual outputs and information about structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  Files Description Table  AccountIngress.sol  57207a6878535bc2f3d40216d96f07eef9bbdfd9  AccountRulesList.sol  73ffd92be5b6c3b1e18d1b860344dac578c9aa31  Admin.sol  e13931323093f1555f4dfcc74fad6a2c457c1082  AdminProxy.sol  eecd073b4e05a4445fb00888074b48c443c5bbf4  Ingress.sol  b0fcff06fa7d55136cfe483331280e4e9bb9def4  NodeIngress.sol  3f46f78e4c1b9a546287135a13ffa303f62a826b  NodeRulesList.sol  fa9382c4cf3f4d800aa3d0e89bb9a712d5aa5f0c  AccountRules.sol  c730212300e070ed22b1490f6e67347d1f36c051  AccountRulesProxy.sol  1024d00149ee0258f5ee4c0671a09ada723c3645  AdminList.sol  0304e06bfc4c87abc4d2f4c0361633590c5ef830  NodeRules.sol  8f0dc9efd5bc09a8c6346495e23a398c907baf21  NodeRulesProxy.sol  01967d8481a3f1497ecdfcfcd5e7dd2ea9f9c17e  Contracts Description Table  Function Name  Visibility  Mutability  Modifiers  AccountIngress  Implementation  Ingress  getContractVersion  Public    NO   emitRulesChangeEvent  Public    NO   transactionAllowed  Public    NO   AccountRulesList  Implementation  size  Internal \ud83d\udd12  exists  Internal \ud83d\udd12  add  Internal \ud83d\udd12  addAll  Internal \ud83d\udd12  remove  Internal \ud83d\udd12  Admin  Implementation  AdminProxy, AdminList  <Constructor>  Public    isAuthorized  Public    NO   addAdmin  Public    onlyAdmin  removeAdmin  Public    onlyAdmin notSelf  getAdmins  Public    NO   addAdmins  Public    onlyAdmin  AdminProxy  Interface  isAuthorized  External    NO   Ingress  Implementation  getContractAddress  Public    NO   isAuthorized  Public    NO   setContractAddress  Public    NO   removeContract  Public    NO   getAllContractKeys  Public    NO   NodeIngress  Implementation  Ingress  getContractVersion  Public    NO   emitRulesChangeEvent  Public    NO   connectionAllowed  Public    NO   NodeRulesList  Implementation  calculateKey  Internal \ud83d\udd12  size  Internal \ud83d\udd12  exists  Internal \ud83d\udd12  add  Internal \ud83d\udd12  remove  Internal \ud83d\udd12  AccountRules  Implementation  AccountRulesProxy, AccountRulesList  <Constructor>  Public    getContractVersion  Public    NO   isReadOnly  Public    NO   enterReadOnly  Public    onlyAdmin  exitReadOnly  Public    onlyAdmin  transactionAllowed  Public    NO   accountInWhitelist  Public    NO   addAccount  Public    onlyAdmin onlyOnEditMode  removeAccount  Public    onlyAdmin onlyOnEditMode  getSize  Public    NO   getByIndex  Public    NO   getAccounts  Public    NO   addAccounts  Public    onlyAdmin  AccountRulesProxy  Interface  transactionAllowed  External    NO   AdminList  Implementation  size  Internal \ud83d\udd12  exists  Internal \ud83d\udd12  add  Internal \ud83d\udd12  addAll  Internal \ud83d\udd12  remove  Internal \ud83d\udd12  NodeRules  Implementation  NodeRulesProxy, NodeRulesList  <Constructor>  Public    getContractVersion  Public    NO   isReadOnly  Public    NO   enterReadOnly  Public    onlyAdmin  exitReadOnly  Public    onlyAdmin  connectionAllowed  Public    NO   enodeInWhitelist  Public    NO   addEnode  Public    onlyAdmin onlyOnEditMode  removeEnode  Public    onlyAdmin onlyOnEditMode  getSize  Public    NO   getByIndex  Public    NO   triggerRulesChangeEvent  Public    NO   NodeRulesProxy  Interface  connectionAllowed  External    NO   Legend  Function can modify state  Function is payable  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"
    },
    {
        "title": "8.4 Slither",
        "body": "  Slither is a Solidity static analysis framework written in Python 3. It runs a suite of vulnerability detectors.  Below is the raw output of the Slither scan:  INFO:Detectors:  Pragma version \">=0.4.22<0.6.0\" allows old versions (ExposedAdminList.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (AccountRules.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (AccountRulesList.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (Ingress.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (NodeRules.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (AdminProxy.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (NodeRulesProxy.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (ExposedNodeRulesList.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (AdminList.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (ExposedAccountRulesList.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (Admin.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (Migrations.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (NodeIngress.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (AccountRulesProxy.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (AccountIngress.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (NodeRulesList.sol#1)  Reference: https://github.com/crytic/slither/wiki/Detector-Documentation#incorrect-versions-of-solidity  INFO:Detectors:  Function 'ExposedAdminList._size()' (ExposedAdminList.sol#9-11) is not in mixedCase  Function 'ExposedAdminList._exists(address)' (ExposedAdminList.sol#13-15) is not in mixedCase  Parameter '_address' of _address (ExposedAdminList.sol#13) is not in mixedCase  Function 'ExposedAdminList._add(address)' (ExposedAdminList.sol#17-19) is not in mixedCase  Parameter '_address' of _address (ExposedAdminList.sol#17) is not in mixedCase  Function 'ExposedAdminList._remove(address)' (ExposedAdminList.sol#21-23) is not in mixedCase  Parameter '_address' of _address (ExposedAdminList.sol#21) is not in mixedCase  Function 'ExposedAdminList._addBatch(address[])' (ExposedAdminList.sol#25-27) is not in mixedCase  Parameter '_addresses' of _addresses (ExposedAdminList.sol#25) is not in mixedCase  Parameter '_account' of _account (AccountRules.sol#77) is not in mixedCase  Parameter '_account' of _account (AccountRulesList.sol#22) is not in mixedCase  Parameter '_account' of _account (AccountRulesList.sol#26) is not in mixedCase  Parameter '_account' of _account (AccountRulesList.sol#45) is not in mixedCase  Variable 'Ingress.RULES_CONTRACT' (Ingress.sol#8) is not in mixedCase  Variable 'Ingress.ADMIN_CONTRACT' (Ingress.sol#9) is not in mixedCase  Function 'ExposedNodeRulesList._calculateKey(bytes32,bytes32,bytes16,uint16)' (ExposedNodeRulesList.sol#8-10) is not in mixedCase  Parameter '_enodeHigh' of _enodeHigh (ExposedNodeRulesList.sol#8) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (ExposedNodeRulesList.sol#8) is not in mixedCase  Parameter '_ip' of _ip (ExposedNodeRulesList.sol#8) is not in mixedCase  Parameter '_port' of _port (ExposedNodeRulesList.sol#8) is not in mixedCase  Function 'ExposedNodeRulesList._size()' (ExposedNodeRulesList.sol#12-14) is not in mixedCase  Function 'ExposedNodeRulesList._exists(bytes32,bytes32,bytes16,uint16)' (ExposedNodeRulesList.sol#16-18) is not in mixedCase  Parameter '_enodeHigh' of _enodeHigh (ExposedNodeRulesList.sol#16) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (ExposedNodeRulesList.sol#16) is not in mixedCase  Parameter '_ip' of _ip (ExposedNodeRulesList.sol#16) is not in mixedCase  Parameter '_port' of _port (ExposedNodeRulesList.sol#16) is not in mixedCase  Function 'ExposedNodeRulesList._add(bytes32,bytes32,bytes16,uint16)' (ExposedNodeRulesList.sol#20-22) is not in mixedCase  Parameter '_enodeHigh' of _enodeHigh (ExposedNodeRulesList.sol#20) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (ExposedNodeRulesList.sol#20) is not in mixedCase  Parameter '_ip' of _ip (ExposedNodeRulesList.sol#20) is not in mixedCase  Parameter '_port' of _port (ExposedNodeRulesList.sol#20) is not in mixedCase  Function 'ExposedNodeRulesList._remove(bytes32,bytes32,bytes16,uint16)' (ExposedNodeRulesList.sol#24-26) is not in mixedCase  Parameter '_enodeHigh' of _enodeHigh (ExposedNodeRulesList.sol#24) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (ExposedNodeRulesList.sol#24) is not in mixedCase  Parameter '_ip' of _ip (ExposedNodeRulesList.sol#24) is not in mixedCase  Parameter '_port' of _port (ExposedNodeRulesList.sol#24) is not in mixedCase  Parameter '_account' of _account (AdminList.sol#24) is not in mixedCase  Parameter '_account' of _account (AdminList.sol#28) is not in mixedCase  Parameter '_account' of _account (AdminList.sol#56) is not in mixedCase  Function 'ExposedAccountRulesList._size()' (ExposedAccountRulesList.sol#8-10) is not in mixedCase  Function 'ExposedAccountRulesList._exists(address)' (ExposedAccountRulesList.sol#12-14) is not in mixedCase  Parameter '_account' of _account (ExposedAccountRulesList.sol#12) is not in mixedCase  Function 'ExposedAccountRulesList._add(address)' (ExposedAccountRulesList.sol#16-18) is not in mixedCase  Parameter '_account' of _account (ExposedAccountRulesList.sol#16) is not in mixedCase  Function 'ExposedAccountRulesList._addAll(address[])' (ExposedAccountRulesList.sol#20-22) is not in mixedCase  Function 'ExposedAccountRulesList._remove(address)' (ExposedAccountRulesList.sol#24-26) is not in mixedCase  Parameter '_account' of _account (ExposedAccountRulesList.sol#24) is not in mixedCase  Parameter '_address' of _address (Admin.sol#22) is not in mixedCase  Parameter '_address' of _address (Admin.sol#26) is not in mixedCase  Parameter '_address' of _address (Admin.sol#38) is not in mixedCase  Parameter 'new_address' of new_address (Migrations.sol#20) is not in mixedCase  Variable 'Migrations.last_completed_migration' (Migrations.sol#6) is not in mixedCase  Struct 'NodeRulesList.enode' (NodeRulesList.sol#8-13) is not in CapWords  Parameter '_enodeHigh' of _enodeHigh (NodeRulesList.sol#18) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (NodeRulesList.sol#18) is not in mixedCase  Parameter '_ip' of _ip (NodeRulesList.sol#18) is not in mixedCase  Parameter '_port' of _port (NodeRulesList.sol#18) is not in mixedCase  Parameter '_enodeHigh' of _enodeHigh (NodeRulesList.sol#26) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (NodeRulesList.sol#26) is not in mixedCase  Parameter '_ip' of _ip (NodeRulesList.sol#26) is not in mixedCase  Parameter '_port' of _port (NodeRulesList.sol#26) is not in mixedCase  Parameter '_enodeHigh' of _enodeHigh (NodeRulesList.sol#30) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (NodeRulesList.sol#30) is not in mixedCase  Parameter '_ip' of _ip (NodeRulesList.sol#30) is not in mixedCase  Parameter '_port' of _port (NodeRulesList.sol#30) is not in mixedCase  Parameter '_enodeHigh' of _enodeHigh (NodeRulesList.sol#39) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (NodeRulesList.sol#39) is not in mixedCase  Parameter '_ip' of _ip (NodeRulesList.sol#39) is not in mixedCase  Parameter '_port' of _port (NodeRulesList.sol#39) is not in mixedCase  Reference: https://github.com/crytic/slither/wiki/Detector-Documentation#conformance-to-solidity-naming-conventions  INFO:Detectors:  AccountRules.slitherConstructorVariables (AccountRules.sol#9-113) uses literals with too many digits:  version = 1000000  NodeRules.slitherConstructorVariables (NodeRules.sol#9-170) uses literals with too many digits:  version = 1000000  NodeIngress.getContractAddress (Ingress.sol#26-29) uses literals with too many digits:  require(bool,string)(name > 0x0000000000000000000000000000000000000000000000000000000000000000,Contract name must not be empty.)  NodeIngress.setContractAddress (Ingress.sol#39-62) uses literals with too many digits:  require(bool,string)(name > 0x0000000000000000000000000000000000000000000000000000000000000000,Contract name must not be empty.)  NodeIngress.removeContract (Ingress.sol#64-81) uses literals with too many digits:  require(bool,string)(name > 0x0000000000000000000000000000000000000000000000000000000000000000,Contract name must not be empty.)  NodeIngress.slitherConstructorVariables (NodeIngress.sol#7-49) uses literals with too many digits:  RULES_CONTRACT = 0x72756c6573000000000000000000000000000000000000000000000000000000  NodeIngress.slitherConstructorVariables (NodeIngress.sol#7-49) uses literals with too many digits:  ADMIN_CONTRACT = 0x61646d696e697374726174696f6e000000000000000000000000000000000000  NodeIngress.slitherConstructorVariables (NodeIngress.sol#7-49) uses literals with too many digits:  version = 1000000  AccountIngress.getContractAddress (Ingress.sol#26-29) uses literals with too many digits:  require(bool,string)(name > 0x0000000000000000000000000000000000000000000000000000000000000000,Contract name must not be empty.)  AccountIngress.setContractAddress (Ingress.sol#39-62) uses literals with too many digits:  require(bool,string)(name > 0x0000000000000000000000000000000000000000000000000000000000000000,Contract name must not be empty.)  AccountIngress.removeContract (Ingress.sol#64-81) uses literals with too many digits:  require(bool,string)(name > 0x0000000000000000000000000000000000000000000000000000000000000000,Contract name must not be empty.)  AccountIngress.slitherConstructorVariables (AccountIngress.sol#7-40) uses literals with too many digits:  RULES_CONTRACT = 0x72756c6573000000000000000000000000000000000000000000000000000000  AccountIngress.slitherConstructorVariables (AccountIngress.sol#7-40) uses literals with too many digits:  ADMIN_CONTRACT = 0x61646d696e697374726174696f6e000000000000000000000000000000000000  AccountIngress.slitherConstructorVariables (AccountIngress.sol#7-40) uses literals with too many digits:  version = 1000000  Reference: https://github.com/crytic/slither/wiki/Detector-Documentation#too-many-digits  INFO:Detectors:  AccountIngress.version should be constant (AccountIngress.sol#9)  AccountRules.version should be constant (AccountRules.sol#14)  Ingress.ADMIN_CONTRACT should be constant (Ingress.sol#9)  Ingress.RULES_CONTRACT should be constant (Ingress.sol#8)  NodeIngress.version should be constant (NodeIngress.sol#9)  NodeRules.version should be constant (NodeRules.sol#30)  Reference: https://github.com/crytic/slither/wiki/Detector-Documentation#state-variables-that-could-be-declared-constant  INFO:Detectors:  ExposedAdminList._size() (ExposedAdminList.sol#9-11) should be declared external  ExposedAdminList._exists(address) (ExposedAdminList.sol#13-15) should be declared external  ExposedAdminList._add(address) (ExposedAdminList.sol#17-19) should be declared external  ExposedAdminList._remove(address) (ExposedAdminList.sol#21-23) should be declared external  ExposedAdminList._addBatch(address[]) (ExposedAdminList.sol#25-27) should be declared external  AccountRules.getContractVersion() (AccountRules.sol#38-40) should be declared external  AccountRules.isReadOnly() (AccountRules.sol#43-45) should be declared external  AccountRules.enterReadOnly() (AccountRules.sol#47-51) should be declared external  AccountRules.exitReadOnly() (AccountRules.sol#53-57) should be declared external  AccountRules.transactionAllowed(address,address,uint256,uint256,uint256,bytes) (AccountRules.sol#59-74) should be declared external  AccountRulesProxy.transactionAllowed(address,address,uint256,uint256,uint256,bytes) (AccountRulesProxy.sol#4-11) should be declared external  AccountRules.addAccount(address) (AccountRules.sol#82-88) should be declared external  AccountRules.removeAccount(address) (AccountRules.sol#90-96) should be declared external  AccountRules.getSize() (AccountRules.sol#98-100) should be declared external  AccountRules.getByIndex(uint256) (AccountRules.sol#102-104) should be declared external  AccountRules.getAccounts() (AccountRules.sol#106-108) should be declared external  AccountRules.addAccounts(address[]) (AccountRules.sol#110-112) should be declared external  Ingress.setContractAddress(bytes32,address) (Ingress.sol#39-62) should be declared external  Ingress.removeContract(bytes32) (Ingress.sol#64-81) should be declared external  Ingress.getAllContractKeys() (Ingress.sol#83-85) should be declared external  NodeRules.getContractVersion() (NodeRules.sol#53-55) should be declared external  NodeRules.isReadOnly() (NodeRules.sol#58-60) should be declared external  NodeRules.enterReadOnly() (NodeRules.sol#62-66) should be declared external  NodeRules.exitReadOnly() (NodeRules.sol#68-72) should be declared external  NodeRulesProxy.connectionAllowed(bytes32,bytes32,bytes16,uint16,bytes32,bytes32,bytes16,uint16) (NodeRulesProxy.sol#4-13) should be declared external  NodeRules.connectionAllowed(bytes32,bytes32,bytes16,uint16,bytes32,bytes32,bytes16,uint16) (NodeRules.sol#74-101) should be declared external  NodeRules.addEnode(bytes32,bytes32,bytes16,uint16) (NodeRules.sol#112-132) should be declared external  NodeRules.removeEnode(bytes32,bytes32,bytes16,uint16) (NodeRules.sol#134-154) should be declared external  NodeRules.getSize() (NodeRules.sol#156-158) should be declared external  NodeRules.getByIndex(uint256) (NodeRules.sol#160-165) should be declared external  ExposedNodeRulesList._calculateKey(bytes32,bytes32,bytes16,uint16) (ExposedNodeRulesList.sol#8-10) should be declared external  ExposedNodeRulesList._size() (ExposedNodeRulesList.sol#12-14) should be declared external  ExposedNodeRulesList._exists(bytes32,bytes32,bytes16,uint16) (ExposedNodeRulesList.sol#16-18) should be declared external  ExposedNodeRulesList._add(bytes32,bytes32,bytes16,uint16) (ExposedNodeRulesList.sol#20-22) should be declared external  ExposedNodeRulesList._remove(bytes32,bytes32,bytes16,uint16) (ExposedNodeRulesList.sol#24-26) should be declared external  ExposedAccountRulesList._size() (ExposedAccountRulesList.sol#8-10) should be declared external  ExposedAccountRulesList._exists(address) (ExposedAccountRulesList.sol#12-14) should be declared external  ExposedAccountRulesList._add(address) (ExposedAccountRulesList.sol#16-18) should be declared external  ExposedAccountRulesList._addAll(address[]) (ExposedAccountRulesList.sol#20-22) should be declared external  ExposedAccountRulesList._remove(address) (ExposedAccountRulesList.sol#24-26) should be declared external  Admin.addAdmin(address) (Admin.sol#26-36) should be declared external  Admin.removeAdmin(address) (Admin.sol#38-42) should be declared external  Admin.getAdmins() (Admin.sol#44-46) should be declared external  Admin.addAdmins(address[]) (Admin.sol#48-50) should be declared external  Migrations.setCompleted(uint256) (Migrations.sol#16-18) should be declared external  Migrations.upgrade(address) (Migrations.sol#20-23) should be declared external  NodeIngress.getContractVersion() (NodeIngress.sol#15-17) should be declared external  NodeIngress.emitRulesChangeEvent(bool) (NodeIngress.sol#19-22) should be declared external  NodeIngress.connectionAllowed(bytes32,bytes32,bytes16,uint16,bytes32,bytes32,bytes16,uint16) (NodeIngress.sol#24-48) should be declared external  AccountIngress.getContractVersion() (AccountIngress.sol#15-17) should be declared external  AccountIngress.emitRulesChangeEvent(bool) (AccountIngress.sol#19-22) should be declared external  AccountIngress.transactionAllowed(address,address,uint256,uint256,uint256,bytes) (AccountIngress.sol#24-39) should be declared external  Reference: https://github.com/crytic/slither/wiki/Detector-Documentation#public-function-that-could-be-declared-as-external  INFO:Slither:. analyzed (16 contracts), 157 result(s) found  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"
    },
    {
        "title": "5.1 Oracle s _sanityCheck for prices will not work with slashing ",
        "body": "  Description  The _sanityCheck is verifying that the new price didn t change significantly:  code/contracts/Portal/utils/OracleUtilsLib.sol:L405-L417  uint256 maxPrice = curPrice +  ((curPrice *  self.PERIOD_PRICE_INCREASE_LIMIT *  _periodsSinceUpdate) / PERCENTAGE_DENOMINATOR);  uint256 minPrice = curPrice -  ((curPrice *  self.PERIOD_PRICE_DECREASE_LIMIT *  _periodsSinceUpdate) / PERCENTAGE_DENOMINATOR);  require(  _newPrice >= minPrice && _newPrice <= maxPrice,  \"OracleUtils: price is insane\"  While the rewards of staking can be reasonably predicted, the balances may also be changed due to slashing. So any slashing event should reduce the price, and if enough ETH is slashed, the price will drop heavily. The oracle will not be updated because of a sanity check. After that, there will be an arbitrage opportunity, and everyone will be incentivized to withdraw as soon as possible. That process will inevitably devaluate gETH to zero. The severity of this issue is also amplified by the fact that operators have no skin in the game and won t lose anything from slashing.  Recommendation  Make sure that slashing can be adequately processed when updating the price.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"
    },
    {
        "title": "5.2 Multiple calculation mistakes in the _findPricesClearBuffer function ",
        "body": "  Description  The _findPricesClearBuffer function is designed to calculate the gETH/ETH prices. The first one (oracle price) is the price at the reference point, for ease of calculation let s assume it is midnight. The second price is the price at the time the reportOracle is called.  code/contracts/Portal/utils/OracleUtilsLib.sol:L388  return (unbufferedEther / unbufferedSupply, totalEther / supply);  To calculate the oracle price at midnight, the current ETH balance is reduced by all the minted gETH (converted to ETH with the old price) and increased by all the burnt gETH (converted to ETH with the old price) starting from midnight to the time transaction is being executed:  code/contracts/Portal/utils/OracleUtilsLib.sol:L368-L374  uint256 unbufferedEther = totalEther -  (DATASTORE.readUintForId(_poolId, _dailyBufferMintKey) * price) /  self.gETH.totalSupply(_poolId);  unbufferedEther +=  (DATASTORE.readUintForId(_poolId, _dailyBufferBurnKey) * price) /  self.gETH.denominator();  But in the first calculation, the self.gETH.totalSupply(_poolId) is mistakenly used instead of self.gETH.denominator(). This can lead to the unbufferedEther being much larger, and the eventual oracle price will be much larger too.  There is another serious calculation mistake. In the end, the function returns the following line:  code/contracts/Portal/utils/OracleUtilsLib.sol:L388  return (unbufferedEther / unbufferedSupply, totalEther / supply);  But none of these values are multiplied by self.gETH.denominator(); so they are in the same range. Both values will usually be around 1. While the actual price value should be multiplied by self.gETH.denominator();.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"
    },
    {
        "title": "5.3 New interfaces can add malicious code without any delay or check ",
        "body": "  Description  Geode Finance uses an interesting system of contracts for each individual staked ETH derivative. At the base of it all is an ERC1155 gETH contract where planet id acts as a token id. To make it more compatible with the rest of DeFi the Geode team pairs it up with an ERC20 contract that users would normally interact with and where all the allowances are stored. Naturally, since the balances are stored in the gETH contract, ERC20 interfaces need to ask gETH contract to update the balance. It is done in a way where the gETH contract will perform any transfer requested by the interface since the interface is expected to do all the checks and accountings. The issue comes with the fact that planet maintainers can whitelist new interfaces and that process does not require any approval. Planet maintainers could whitelist an interface that will send all the available tokens to the maintainer s wallet for example. This essentially allows Planet maintainers to steal all derivative tokens in circulation in one transaction.  Examples  code/contracts/Portal/utils/StakeUtilsLib.sol:L165-L173  function setInterface(  StakePool storage self,  DataStoreUtils.DataStore storage DATASTORE,  uint256 id,  address _interface  ) external {  DATASTORE.authenticate(id, true, [false, true, true]);  _setInterface(self, DATASTORE, id, _interface);  Recommendation  gETH.sol contract has a concept of avoiders. One of the ways to fix this issue is to have the avoidance be set on a per-interface basis and avoiding new interfaces by default. This way users will need to allow the new tokens to access the balances.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"
    },
    {
        "title": "5.4 MiniGovernance - fetchUpgradeProposal will always revert ",
        "body": "  Description  In the function fetchUpgradeProposal(), newProposal() is called with a hard coded duration of 4 weeks. This means the function will always revert since newProposal() checks that the proposal duration is not more than the constant MAX_PROPOSAL_DURATION of 2 weeks. Effectively, this leaves MiniGovernance non-upgradeable.  Examples  code/contracts/Portal/MiniGovernance/MiniGovernance.sol:L183  GEM.newProposal(proposal.CONTROLLER, 2, proposal.NAME, 4 weeks);  code/contracts/Portal/utils/GeodeUtilsLib.sol:L328-L331  require(  duration <= MAX_PROPOSAL_DURATION,  \"GeodeUtils: duration exceeds MAX_PROPOSAL_DURATION\"  );  Recommendation  Switch the hard coded proposal duration to 2 weeks.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"
    },
    {
        "title": "5.5 reportOracle can be sandwiched for profit. ",
        "body": "  Description  The fact that price update happens in an on-chain transaction gives the searches the ability to see the future price and then act accordingly.  Examples  MEV searcher can find the reportOracle transaction in the mem-pool and if the price is about to increase he could proceed to mint as much gETH as he can with a flash loan. They would then bundle the reportOracle transaction. Finally, they would redeem all the gETH for ETH at a higher price per share value as the last transaction in the bundle.  This paired with the fact that oracle might be updated less frequently than once per day, could lead to the fact that profits from this attack will outweigh the fees for performing it.  Fortunately, due to the nature of the protocol, the price fluctuations from day to day will most likely be smaller than the fees encountered during this arbitrage, but this is still something to be aware of when updating the values for DWP donations and fees. But it also makes it crucial to update the oracle every day not to increase the profit margins for this attack.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"
    },
    {
        "title": "5.6 Updating interfaces of derivatives is done in a dangerous and unpredictable manner. ",
        "body": "  Description  Geode Finance codebase provides planet maintainers with the ability to enable or disable different contracts to act as the main token contract. In fact, multiple separate contracts can be used at the same time if decided so by the planet maintainer. Those contracts will have shared balances but will not share the allowances as you can see below:  code/contracts/Portal/helpers/ERC1155SupplyMinterPauser.sol:L47  mapping(uint256 => mapping(address => uint256)) private _balances;  code/contracts/Portal/gETHInterfaces/ERC20InterfaceUpgradable.sol:L60  mapping(address => mapping(address => uint256)) private _allowances;  Unfortunately, this approach comes with some implications that are very hard to predict as they involve interactions with other systems, but is possible to say that the consequences of those implications will most always be negative. We will not be able to outline all the implications of this issue, but we can try and outline the pattern that they all would follow.  Examples  There are really two ways to update an interface: set the new one and immediately unset the old one, or have them both run in parallel for some time. Let s look at them one by one.  in the first case, the old interface is disabled immediately. Given that interfaces share balances that will lead to some very serious consequences. Imagine the following sequence:  Alice deposits her derivatives into the DWP contract for liquidity mining.  Planet maintainer updates the interface and immediately disables the old one.  DWP contract now has the old tokens and the new ones. But only the new ones are accounted for in the storage and thus can be withdrawn. Unfortunately, the old tokens are disabled meaning that now both old and new tokens are lost.  This can happen in pretty much any contract and not just the DWP token. Unless the holders had enough time to withdraw the derivatives back to their wallets all the funds deposited into contracts could be lost.  This leads us to the second case where the two interfaces are active in parallel. This would solve the issue above by allowing Alice to withdraw the old tokens from the DWP and make the new tokens follow. Unfortunately, there is an issue in that case as well.  Some DeFi contracts allow their owners to withdraw any tokens that are not accounted for by the internal accounting. DWP allows the withdrawal of admin fees if the contract has more tokens than balances[] store. Some contracts even allow to withdraw funds that were accidentally sent to the contract by people. Either to recover them or just as a part of dust collection. Let s call such contracts  dangerous contracts  for our purposes.  Alice deposits her derivatives into the dangerous contract.  Planet maintainer sets a new interface.  Owner of the dangerous contract sees that some odd and unaccounted tokens landed in the contract. He learns those are real and are part of Geode ecosystem. So he takes them.  Old tokens will follow the new tokens. That means Alice now has no claim to them and the contract that they just left has broken accounting since numbers there are not backed by tokens anymore.  One other issue we would like to highlight here is that despite the contracts being expected to have separate allowances, if the old contract has the allowance set, the initial 0 value of the new one will be ignored. Here is an example:  Alice approves Bob for 100 derivatives.  Planet maintainer sets a new interface. The new interface has no allowance from Alice to Bob.  Bob still can transfer new tokens from Alice to himself by transferring the old tokens for which he still has the allowance. New token balances will be updated accordingly.  Alice could also give Bob an allowance of 100 tokens in the new contract since that was her original intent, but this would mean that Bob now has 200 token allowance.  This is extremely convoluted and will most likely result in errors made by the planet maintainers when updating the interfaces.  Recommendation  The safest option is to only allow a list of whitelisted interfaces to be used that are well-documented and audited. Planet maintainers could then choose the once that they see fit.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"
    },
    {
        "title": "5.7 A sandwich attack on fetchUnstake ",
        "body": "  Description  Operators are incentivized to withdraw the stake when there is a debt in the system. Withdrawn ETH will be sold in the DWP, and a portion of the arbitrage profit will be sent to the operator. But the operators cannot unstake and earn the arbitrage boost instantly. Node operator will need to start the withdrawal process, signal unstake, and only then, after some time, potentially days, Oracle will trigger fetchUnstake and will take the arbitrage opportunity if it is still there.  code/contracts/Portal/utils/StakeUtilsLib.sol:L1276-L1288  function fetchUnstake(  StakePool storage self,  DataStoreUtils.DataStore storage DATASTORE,  uint256 poolId,  uint256 operatorId,  bytes[] calldata pubkeys,  uint256[] calldata balances,  bool[] calldata isExit  ) external {  require(  msg.sender == self.TELESCOPE.ORACLE_POSITION,  \"StakeUtils: sender NOT ORACLE\"  );  In reality, the DWP contract s swap function is external and can be used by anyone, so anyone could try and take the arbitrage.  code/contracts/Portal/withdrawalPool/Swap.sol:L341-L358  function swap(  uint8 tokenIndexFrom,  uint8 tokenIndexTo,  uint256 dx,  uint256 minDy,  uint256 deadline  external  payable  virtual  override  nonReentrant  whenNotPaused  deadlineCheck(deadline)  returns (uint256)  return swapStorage.swap(tokenIndexFrom, tokenIndexTo, dx, minDy);  In fact, one could take this arbitrage with no risk or personal funds. This is due to the fact that fetchUnstake() could get sandwiched. Consider the following case:  There is a debt in the DWP and the node operator decides to withdraw the stake to take the arbitrage opportunity.  After some time the Oracle will actually finalize the withdrawal by calling fecthUnstake.  If debt is still there MEV searcher will see that transaction in the mem-pool and will take an ETH loan to buy cheap gETH.  fetchUnstake() will execute and since the debt was repaid in the previous step all of the withdrawn ETH will go into surplus.  Searcher will redeem gETH that they bought for the oracle price from surplus and will get all of the profit.  At the end of the day, the goal of regaining the peg will be accomplished, but node operators will not be interested in withdrawing early later. This will potentially create unhealthy situations when withdrawals are required in case of a serious de-peg.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"
    },
    {
        "title": "5.8 Only the GOVERNANCE can initialize the Portal ",
        "body": "  Description  In the Portal s initialize function, the _GOVERNANCE is passed as a parameter:  code/contracts/Portal/Portal.sol:L156-L196  function initialize(  address _GOVERNANCE,  address _gETH,  address _ORACLE_POSITION,  address _DEFAULT_gETH_INTERFACE,  address _DEFAULT_DWP,  address _DEFAULT_LP_TOKEN,  address _MINI_GOVERNANCE_POSITION,  uint256 _GOVERNANCE_TAX,  uint256 _COMET_TAX,  uint256 _MAX_MAINTAINER_FEE,  uint256 _BOOSTRAP_PERIOD  ) public virtual override initializer {  __ReentrancyGuard_init();  __Pausable_init();  __ERC1155Holder_init();  __UUPSUpgradeable_init();  GEODE.SENATE = _GOVERNANCE;  GEODE.GOVERNANCE = _GOVERNANCE;  GEODE.GOVERNANCE_TAX = _GOVERNANCE_TAX;  GEODE.MAX_GOVERNANCE_TAX = _GOVERNANCE_TAX;  GEODE.SENATE_EXPIRY = type(uint256).max;  STAKEPOOL.GOVERNANCE = _GOVERNANCE;  STAKEPOOL.gETH = IgETH(_gETH);  STAKEPOOL.TELESCOPE.gETH = IgETH(_gETH);  STAKEPOOL.TELESCOPE.ORACLE_POSITION = _ORACLE_POSITION;  STAKEPOOL.TELESCOPE.MONOPOLY_THRESHOLD = 20000;  updateStakingParams(  _DEFAULT_gETH_INTERFACE,  _DEFAULT_DWP,  _DEFAULT_LP_TOKEN,  _MAX_MAINTAINER_FEE,  _BOOSTRAP_PERIOD,  type(uint256).max,  type(uint256).max,  _COMET_TAX,  3 days  );  But then it calls the updateStakingParams function, which requires the msg.sender to be the governance:  code/contracts/Portal/Portal.sol:L651-L665  function updateStakingParams(  address _DEFAULT_gETH_INTERFACE,  address _DEFAULT_DWP,  address _DEFAULT_LP_TOKEN,  uint256 _MAX_MAINTAINER_FEE,  uint256 _BOOSTRAP_PERIOD,  uint256 _PERIOD_PRICE_INCREASE_LIMIT,  uint256 _PERIOD_PRICE_DECREASE_LIMIT,  uint256 _COMET_TAX,  uint256 _BOOST_SWITCH_LATENCY  ) public virtual override {  require(  msg.sender == GEODE.GOVERNANCE,  \"Portal: sender not GOVERNANCE\"  );  So only the future governance can initialize the Portal. In the case of the Geode protocol, the governance will be represented by a token contract, making it hard to initialize promptly. Initialization should be done by an actor that is more flexible than governance.  Recommendation  Split the updateStakingParams function into public and private ones and use them accordingly.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"
    },
    {
        "title": "5.9 The maintainer of the MiniGovernance can block the changeMaintainer function ",
        "body": "  Description  Every entity with an ID has a controller and a maintainer. The controller tends to have more control, and the maintainer is mostly used for operational purposes. So the controller should be able to change the maintainer if that is required. Indeed we see that it is possible in the MiniGovernance too:  code/contracts/Portal/MiniGovernance/MiniGovernance.sol:L224-L246  function changeMaintainer(  bytes calldata password,  bytes32 newPasswordHash,  address newMaintainer  external  virtual  override  onlyPortal  whenNotPaused  returns (bool success)  require(  SELF.PASSWORD_HASH == bytes32(0) ||  SELF.PASSWORD_HASH ==  keccak256(abi.encodePacked(SELF.ID, password))  );  SELF.PASSWORD_HASH = newPasswordHash;  _refreshSenate(newMaintainer);  success = true;  Here the changeMaintainer function can only be called by the Portal, and only the controller can initiate that call. But the maintainer can pause the MiniGovernance, which will make this call revert because the _refreshSenate function has the whenNotPaused modifier. Thus maintainer could intentionally prevent the controller from replacing it by another maintainer.  Recommendation  Make sure that the controller can always change the malicious maintainer.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"
    },
    {
        "title": "5.10 Entities are not required to be initiated ",
        "body": "  Description  Every entity (Planet, Comet, Operator) has a 3-step creation process:  Creation of the proposal.  Approval of the proposal.  Initiation of the entity.  The last step is crucial, but it is never explicitly checked that the entity is initialized. The initiation always includes the initiator modifier that works with the \"initiated\" slot on DATASTORE:  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L46-L72  modifier initiator(  DataStoreUtils.DataStore storage DATASTORE,  uint256 _TYPE,  uint256 _id,  address _maintainer  ) {  require(  msg.sender == DATASTORE.readAddressForId(_id, \"CONTROLLER\"),  \"MaintainerUtils: sender NOT CONTROLLER\"  );  require(  DATASTORE.readUintForId(_id, \"TYPE\") == _TYPE,  \"MaintainerUtils: id NOT correct TYPE\"  );  require(  DATASTORE.readUintForId(_id, \"initiated\") == 0,  \"MaintainerUtils: already initiated\"  );  DATASTORE.writeAddressForId(_id, \"maintainer\", _maintainer);  _;  DATASTORE.writeUintForId(_id, \"initiated\", block.timestamp);  emit IdInitiated(_id, _TYPE);  But this slot is never actually checked when the entities are used. While we did not find any profitable attack vector using uninitiated entities, the code will be upgraded, which may allow for possible attack vectors related to this issue.  Recommendation  Make sure the entities are initiated before they are used.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"
    },
    {
        "title": "5.11 Node operators are not risking anything when abandoning their activity or performing malicious actions ",
        "body": "  Description  During the staking process, the node operators need to provide 1 ETH as a deposit for every validator that they would like to initiate. After that is done, Oracle needs to ensure that validator creation has been done correctly and then deposit the remaining 31 ETH on chain as well as reimburse 1 ETH back to the node operator. The node operator can then proceed to withdraw the funds that were used as initial deposits. As the result, node operators operate nodes that have 32 ETH each and none of which originally belonged to the operator. They essentially have no skin in the game to continue managing the validators besides a potential share in staking rewards. Instead, node operators could stop operation, or try to get slashed on purpose to create turmoil around derivatives on the market and try to capitalize while shorting the assets elsewhere.  Recommendation  Senate will need to be extra careful when approving operator onboarding proposals or potentially only reimburse the node operators the initial deposit after the funds were withdrawn from the MiniGovernance.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"
    },
    {
        "title": "5.12 Planets should not act as operators ",
        "body": "  Description  The system stores every entity (e.g., planet, comet, and operator) separately in DATASTORE under different IDs. But there is one exception, every planet can also act as an operator by default. This exception bypasses the general rule and goes against some expectations readers might have about the code:  Every entity with ID has fees; they are stored in DATASTORE for each entity DATASTORE.readUintForId(id, \"fee\"). The fees for a planet and an operator should be able to be different. But if a planet acts like an operator, both fees are stored under the same variable.  The same problem arises with the maintainer address. Since there will probably be different scripts for maintaining a planet and an operator, having separate addresses for the maintainers would make sense.  Every operator should be initialized before usage, but it is impossible to initialize a planet as an operator. There are two reasons behind it. First, only the original  Operator type  can call initiateOperator, while the planet will have a  Planet type . Second, an entity cannot be initialized twice; even different initialization functions use the same  initiated  storage slot.  Recommendation  Do not allow planets to be operators in the code. If every planet should be able to act as an operator simultaneously, it is better to create separate operator entities for every planet.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"
    },
    {
        "title": "5.13 The blameOperator can be called for an alienated validator ",
        "body": "  Description  The blameOperator  function is designed to be called by anyone. If some operator did not signal to exit in time, anyone can blame and imprison this operator.  code/contracts/Portal/utils/StakeUtilsLib.sol:L1205-L1224  /**  @notice allows improsening an Operator if the validator have not been exited until expectedExit  @dev anyone can call this function  @dev if operator has given enough allowence, they can rotate the validators to avoid being prisoned  /  function blameOperator(  StakePool storage self,  DataStoreUtils.DataStore storage DATASTORE,  bytes calldata pk  ) external {  if (  block.timestamp > self.TELESCOPE._validators[pk].expectedExit &&  self.TELESCOPE._validators[pk].state != 3  ) {  OracleUtils.imprison(  DATASTORE,  self.TELESCOPE._validators[pk].operatorId  );  The problem is that it can be called for any state that is not 3 (self.TELESCOPE._validators[pk].state != 3). But it should only be called for active validators whose state equals 2. So the blameOperator can be called an infinite amount of time for alienated or not approved validators. These types of validators cannot switch to state 3.  The severity of the issue is mitigated by the fact that this function is currently unavailable for users to call. But it is intended to be external once the withdrawal process is in place.  Recommendation  Make sure that you can only blame the operator of an active validator.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"
    },
    {
        "title": "5.14 Latency timelocks on certain functions can be bypassed ",
        "body": "  Description  The functions switchMaintainerFee() and switchWithdrawalBoost() add a latency of typically three days to the current timestamp at which the new value is meant to be valid. However, they don t limit the number of times this value can be changed within the latency period. This allows a malicious maintainer to set their desired value twice and effectively make the change immediately. Let s take the first function as an example. The first call to it sets a value as the newFee, moving the old value to priorFee, which is effectively the fee in use until the time lock is up. A follow-up call to the function with the same value as a parameter would mean the  new  value overwrites the old priorFee while remaining in the queue for the switch.  Examples  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L311-L333  function switchMaintainerFee(  DataStoreUtils.DataStore storage DATASTORE,  uint256 id,  uint256 newFee  ) external {  DATASTORE.writeUintForId(  id,  \"priorFee\",  DATASTORE.readUintForId(id, \"fee\")  );  DATASTORE.writeUintForId(  id,  \"feeSwitch\",  block.timestamp + FEE_SWITCH_LATENCY  );  DATASTORE.writeUintForId(id, \"fee\", newFee);  emit MaintainerFeeSwitched(  id,  newFee,  block.timestamp + FEE_SWITCH_LATENCY  );  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L296-L304  function getMaintainerFee(  DataStoreUtils.DataStore storage DATASTORE,  uint256 id  ) internal view returns (uint256 fee) {  if (DATASTORE.readUintForId(id, \"feeSwitch\") > block.timestamp) {  return DATASTORE.readUintForId(id, \"priorFee\");  return DATASTORE.readUintForId(id, \"fee\");  Recommendation  Add a check to make sure only one value can be set between time lock periods.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"
    },
    {
        "title": "5.15 MiniGovernance s senate has almost unlimited validity ",
        "body": "  Description  A new senate for the MiniGovernance contract is set in the following line:  code/contracts/Portal/MiniGovernance/MiniGovernance.sol:L201  GEM._setSenate(newSenate, block.timestamp + SENATE_VALIDITY);  The validity period argument should not include block.timestamp, because it is going to be added a bit later in the code:  code/contracts/Portal/utils/GeodeUtilsLib.sol:L496  self.SENATE_EXPIRY = block.timestamp + _senatePeriod;  So currently, every senate of MiniGovernance will have much longer validity than it is supposed to.  Recommendation  Pass onlySENATE_VALIDITY in the _refreshSenate function.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"
    },
    {
        "title": "5.16 Proposed validators not accounted for in the monopoly check. ",
        "body": "  Description  The Geode team introduced a check that makes sure that node operators do not initiate more validators than a threshold called MONOPOLY_THRESHOLD allows. It is used on call to proposeStake(...) which the operator would call in order to propose new validators. It is worth mentioning that onboarding new validator nodes requires 2 steps: a proposal from the node operator and approval from the planet maintainer. After the first step validators get a status of proposed. After the second step validators get the status of active and all eth accounting is done. The issue we found is that the proposed validators step performs the monopoly check but does not account for previously proposed but not active validators.  Examples  Assume that MONOPOLY_THRESHOLD is set to 5. The node operator could propose 4 new validators and pass the monopoly check and label those validators as proposed. The node operator could then suggest 4 more validators in a separate transaction and since the monopoly check does not check for the proposed validators, that would pass as well. Then in beaconStake or the step of maintainer approval, there is no monopoly check at all, so 8 validators could be activated at once.  code/contracts/Portal/utils/StakeUtilsLib.sol:L978-L982  require(  (DATASTORE.readUintForId(operatorId, \"totalActiveValidators\") +  pubkeys.length) <= self.TELESCOPE.MONOPOLY_THRESHOLD,  \"StakeUtils: IceBear does NOT like monopolies\"  );  Recommendation  Include the (DATASTORE.readUintForId(poolId,DataStoreUtils.getKey(operatorId, \"proposedValidators\")) into the require statement, just like in the check for the node operator allowance check.  code/contracts/Portal/utils/StakeUtilsLib.sol:L983-L995  require(  (DATASTORE.readUintForId(  poolId,  DataStoreUtils.getKey(operatorId, \"proposedValidators\")  ) +  DATASTORE.readUintForId(  poolId,  DataStoreUtils.getKey(operatorId, \"activeValidators\")  ) +  pubkeys.length) <=  operatorAllowance(DATASTORE, poolId, operatorId),  \"StakeUtils: NOT enough allowance\"  );  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"
    },
    {
        "title": "5.17 Comparison operator used instead of assignment operator ",
        "body": "  Description  A common typo is present twice in the OracleUtilsLib.sol where == is used instead of = resulting in incorrect storage updates.  Examples  code/contracts/Portal/utils/OracleUtilsLib.sol:L250  self._validators[_pk].state == 2;  code/contracts/Portal/utils/OracleUtilsLib.sol:L269  self._validators[_pk].state == 3;  Recommendation  Replace == with =.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"
    },
    {
        "title": "5.18 initiator modifier will not work in the context of one transaction ",
        "body": "  Description  Each planet, comet or operator must be initialized after the onboarding proposal is approved. In order to make sure that these entities are not initialized more than once initiateOperator, initiateComet and initiatePlanet have the initiator modifier.  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L135-L147  function initiatePlanet(  DataStoreUtils.DataStore storage DATASTORE,  uint256[3] memory uintSpecs,  address[5] memory addressSpecs,  string[2] calldata interfaceSpecs  external  initiator(DATASTORE, 5, uintSpecs[0], addressSpecs[1])  returns (  address miniGovernance,  address gInterface,  address withdrawalPool  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L184-L189  function initiateComet(  DataStoreUtils.DataStore storage DATASTORE,  uint256 id,  uint256 fee,  address maintainer  ) external initiator(DATASTORE, 6, id, maintainer) {  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L119-L124  function initiateOperator(  DataStoreUtils.DataStore storage DATASTORE,  uint256 id,  uint256 fee,  address maintainer  ) external initiator(DATASTORE, 4, id, maintainer) {  Inside that modifier, we check that the initiated flag is 0 and if so we proceed to initialization. We later update it to the current timestamp.  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L46-L72  modifier initiator(  DataStoreUtils.DataStore storage DATASTORE,  uint256 _TYPE,  uint256 _id,  address _maintainer  ) {  require(  msg.sender == DATASTORE.readAddressForId(_id, \"CONTROLLER\"),  \"MaintainerUtils: sender NOT CONTROLLER\"  );  require(  DATASTORE.readUintForId(_id, \"TYPE\") == _TYPE,  \"MaintainerUtils: id NOT correct TYPE\"  );  require(  DATASTORE.readUintForId(_id, \"initiated\") == 0,  \"MaintainerUtils: already initiated\"  );  DATASTORE.writeAddressForId(_id, \"maintainer\", _maintainer);  _;  DATASTORE.writeUintForId(_id, \"initiated\", block.timestamp);  emit IdInitiated(_id, _TYPE);  Unfortunately, this does not follow the checks-effects-interractions pattern. If one for example would call initiatePlanet again from the body of the modifier, this check will still pass making it susceptible to a reentrancy attack. While we could not find a way to exploit this in the current engagement, given that system is designed to be upgradable this could become a risk in the future. For example, if during the initialization of the planet the maintainer will be allowed to pass a custom interface that could potentially allow reentering.  Recommendation  Bring the line that updated the initiated flag to the current timestamp before the _;.  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L69  DATASTORE.writeUintForId(_id, \"initiated\", block.timestamp);  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"
    },
    {
        "title": "5.19 Incorrect accounting for the burned gEth ",
        "body": "  Description  Geode Portal records the amount of minted and burned gETH on any given day during the active period of the oracle. One case where some gETH is burned is when the users redeem gETH for ETH. In the burn function we burn the spentGeth - gEthDonation but in the accounting code we do not account for gEthDonation so the code records more assets burned than was really burned.  Examples  code/contracts/Portal/utils/StakeUtilsLib.sol:L823-L832  DATASTORE.subUintForId(poolId, \"surplus\", spentSurplus);  self.gETH.burn(address(this), poolId, spentGeth - gEthDonation);  if (self.TELESCOPE._isOracleActive()) {  bytes32 dailyBufferKey = DataStoreUtils.getKey(  block.timestamp - (block.timestamp % OracleUtils.ORACLE_PERIOD),  \"burnBuffer\"  );  DATASTORE.addUintForId(poolId, dailyBufferKey, spentGeth);  Recommendation  Record the spentGeth - gEthDonation instead of just spentGeth in the burn buffer.  code/contracts/Portal/utils/StakeUtilsLib.sol:L831  DATASTORE.addUintForId(poolId, dailyBufferKey, spentGeth);  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"
    },
    {
        "title": "5.20 Boost calculation on fetchUnstake should not be using the cumBalance when it is larger than debt. ",
        "body": "  Description  The Geode team implemented the 2-step withdrawal mechanism for the staked ETH. First, node operators signal their intent to withdraw the stake, and then the oracle will trigger all of the accounting of rewards, balances, and buybacks if necessary. Buybacks are what we are interested in at this time. Buybacks are performed by checking if the derivative asset is off peg in the Dynamic Withdrawal Pool contract. Once the debt is larger than some ignorable threshold an arbitrage buyback will be executed. A portion of the arbitrage profit will go to the node operator. The issue here is that when simulating the arbitrage swap in the calculateSwap call we use the cumulative un-stake balance rather than ETH debt preset in the DWP. In the case where the withdrawal cumulative balance is higher than the debt node operator will receive a higher reward than intended.  Examples  code/contracts/Portal/utils/StakeUtilsLib.sol:L1353-L1354  uint256 arb = withdrawalPoolById(DATASTORE, poolId)  .calculateSwap(0, 1, cumBal);  Recommendation  Use the debt amount of ETH in the boost reward calculation when the cumulative balance is larger than the debt.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"
    },
    {
        "title": "5.21 DataStore struct not having the _gap for upgrades. ",
        "body": "  Description  Geode Finance codebase follows a structure where most of the storage variables are stored in the structs. You can see an example of that in the Portal.sol.  code/contracts/Portal/Portal.sol:L152-L154  DataStoreUtils.DataStore private DATASTORE;  GeodeUtils.Universe private GEODE;  StakeUtils.StakePool private STAKEPOOL;  It is worth mentioning that Geode contracts are meant to support the upgradability pattern. Given that information, one should be careful not to overwrite the storage variables by reordering the old ones or adding the new once not at the end of the list of variables when upgrading. The issue comes with the fact that structs seem to give a false sense of security making it feel like they are an isolated set of storage variables that will not override anything else. In reality, struts are just tuples that are expanded in storage sequentially just like all the other storage variables. For that reason, if you have two struct storage variables listed back to back like in the code above, you either need to make sure not to change the order or the number of variables in the structs other than the last one between upgrades or you need to add a uint256[N] _gap array of fixed size to reserve some storage slots for the future at the end of each struct. The Geode Finance team is missing the gap in the DataStrore struct making it non-upgradable.  code/contracts/Portal/utils/DataStoreUtilsLib.sol:L34-L39  struct DataStore {  mapping(uint256 => uint256[]) allIdsByType;  mapping(bytes32 => uint256) uintData;  mapping(bytes32 => bytes) bytesData;  mapping(bytes32 => address) addressData;  Recommendation  We suggest that gap is used in DataStore as well. Since it was used for all the other structs we consider it just a typo.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"
    },
    {
        "title": "4.1 Gold order size should be limited    Addressed",
        "body": "  Resolution   Addressed in   horizon-games/SkyWeaver-contracts#9 by adding a limit for cold cards amount in one order.  Description  When a user submits an order to buy gold cards, it s possible to buy a huge amount of cards. _commit function uses less gas than mineGolds, which means that the user can successfully commit to buying this amount of cards and when it s time to collect them, mineGolds function may run out of gas because it iterates over all card IDs and mints them:  code/contracts/shop/GoldCardsFactory.sol:L375-L376  // Mint gold cards  skyweaverAssets.batchMint(_order.cardRecipient, _ids, amounts, \"\");  Recommendation  Limit a maximum gold card amount in one order.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"
    },
    {
        "title": "4.2 Price and refund changes may cause failures    Addressed",
        "body": "  Resolution  Addressed in horizon-games/SkyWeaver-contracts#3.  Fix involves burning the weave when the commit occurs instead of when the minting of the gold cards occur.  Description  Price and refund for gold cards are used in 3 different places: commit, mint, refund.  Weave tokens spent during the commit phase  code/contracts/shop/GoldCardsFactory.sol:L274-L279  function _commit(uint256 _weaveAmount, GoldOrder memory _order)  internal  // Check if weave sent is sufficient for order  uint256 total_cost = _order.cardAmount.mul(goldPrice).add(_order.feeAmount);  uint256 refund_amount = _weaveAmount.sub(total_cost); // Will throw if insufficient amount received  but they are burned rngDelay blocks after  code/contracts/shop/GoldCardsFactory.sol:L371-L373  // Burn the non-refundable weave  uint256 weave_to_burn = (_order.cardAmount.mul(goldPrice)).sub(_order.cardAmount.mul(goldRefund));  weaveContract.burn(weaveID, weave_to_burn);  If the price is increased between these transactions, mining cards may fail because it should burn more weave tokens than there are tokens in the smart contract. Even if there are enough tokens during this particular transaction, someone may fail to melt a gold card later.  If the price is decreased, some weave tokens will be stuck in the contract forever without being burned.  Recommendation  Store goldPrice and goldRefund in GoldOrder.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"
    },
    {
        "title": "4.3 Re-entrancy attack allows to buy EternalHeroes cheaper    Addressed",
        "body": "  Resolution  Addressed in horizon-games/SkyWeaver-contracts#4.  Minting tokens before sending refunds. Subsequent PR will also add re-entrancy guard for all shops.  And re-entrancy guard added here: horizon-games/SkyWeaver-contracts#10  Description  When buying eternal heroes in _buy  function of EternalHeroesFactory contract, a buyer can do re-entracy before items are minted.  code/contracts/shop/EternalHeroesFactory.sol:L278-L284  uint256 refundAmount = _arcAmount.sub(total_cost);  if (refundAmount > 0) {  arcadeumCoin.safeTransferFrom(address(this), _recipient, arcadeumCoinID, refundAmount, \"\");  // Mint tokens to recipient  factoryManager.batchMint(_recipient, _ids, amounts_to_mint, \"\");  Since price should increase after every N items are minted, it s possible to buy more items with the old price.  Recommendation  Add re-entrancy protection or mint items before sending the refund.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"
    },
    {
        "title": "4.4 Supply limitation misbehaviors    Addressed",
        "body": "  Resolution   Logic remains unchanged as it s the desired behaviour. But the issue is mitigated in   horizon-games/SkyWeaver-contracts#5 by renaming the term  currentSupply  to  currentIssuance  and  maxSupply  to  maxIssuance  for maximum clarity.  Description  In SWSupplyManager contract, the owner can limit supply for any token ID by setting maxSupply:  code/contracts/shop/SWSupplyManager.sol:L149-L165  function setMaxSupplies(uint256[] calldata _ids, uint256[] calldata _newMaxSupplies) external onlyOwner() {  require(_ids.length == _newMaxSupplies.length, \"SWSupplyManager#setMaxSupply: INVALID_ARRAYS_LENGTH\");  // Can only *decrease* a max supply  // Can't set max supply back to 0  for (uint256 i = 0; i < _ids.length; i++ ) {  if (maxSupply[_ids[i]] > 0) {  require(  0 < _newMaxSupplies[i] && _newMaxSupplies[i] < maxSupply[_ids[i]],  \"SWSupplyManager#setMaxSupply: INVALID_NEW_MAX_SUPPLY\"  );  maxSupply[_ids[i]] = _newMaxSupplies[i];  emit MaxSuppliesChanged(_ids, _newMaxSupplies);  The problem is that you can set maxSupply that is lower than currentSupply, which would be an unexpected state to have.  Also, if some tokens are burned, their currentSupply is not decreasing:  code/contracts/shop/SWSupplyManager.sol:L339-L345  function burn(  uint256 _id,  uint256 _amount)  external  _burn(msg.sender, _id, _amount);  This unexpected behaviour may lead to burning all of the tokens without being able to mint more.  Recommendation  Properly track currentSupply by modifying it in burn function. Consider having a following restriction require(_newMaxSupplies[i] > currentSupply[_ids[i]]) in setMaxSupplies function.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"
    },
    {
        "title": "4.5 Owner can modify gold cards distribution after someone committed to buy   ",
        "body": "  Resolution  The client decided not to fix this issue with the following comment:  This issue will be addressed by having the owner be a delayed multisig, such that users will have time to witness a change in the distribution that is about to occur.  Description  When a user commits to buying a gold card (and sends weave), there is an expected distribution of possible outcomes. But the problem is that owner can change distribution by calling registerIDs and deregisterIDs  functions.  Additionally, owner can buy any specific gold card avoiding RNG mechanism. It can be done by deleting all the unwanted cards, mining the card and then returning them back. And if owner removes every card from the list, nothing is going to be minted.  Recommendation  There are a few possible recommendations:  Fix a distribution for every order after commit(costly solution).  Make it an explicit part of the trust model (increases trust to the admins).  Cancel pending orders if gold cards IDs are changed.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"
    },
    {
        "title": "4.6 A buyer of a gold card can manipulate randomness   ",
        "body": "  Resolution  The client decided not to fix this issue with the following comment:  We hereby assume that Horizon will always be willing to mine gold cards even at a loss considering the amount of gold cards that can be created per week is limited. If in practice this becomes a problem, we can upgrade this factory.  Description  When a user is buying a gold card, _commit function is called. After rngDelay number of blocks, someone should call mineGolds function to actually mint the card. If this function is not called during 255 blocks (around 1 hour), a user should call recommit to try to mint a gold card again with a new random seed. So if the user doesn t like a card that s going to be minted (randomly), user can try again until a card is good. The issue is medium because anyone can call mineGolds function in order to prevent this behaviour. But it costs money and there s no incentive for anyone to do so.  Recommendation  Create a mechanism to avoid this kind of manipulation. For example, make sure there is an incentive for someone to call mineGolds function  ",
        "labels": [
            "Consensys",
            "Medium",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"
    },
    {
        "title": "4.7 A refund is sent to recipient   ",
        "body": "  Resolution  The client decided not to fix this issue with the following comment:  It s unlikely users will send inexact amount since price is fixed. If this becomes a problem in practice we can re-deploy the factory with this added functionality.  Description  When a refund is sent, it s sent to recipient. In case if a user wants to keep game items and money separate, it makes sense to send a refund back to from address.  Recommendation  Since there may be different use cases, consider adding refundAddress to order structure.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"
    },
    {
        "title": "4.8 Randomness can be manipulated by miners   ",
        "body": "  Resolution  The client decided not to fix this issue with the following comment:  For miners to be able to profit, they would have to forfeit multiple blocks and the desired gold cards would have to be very expensive in the first place (e.g. in the $10k) for it to be worth it for them. In practice, there are also other oppotunities for miners that offer better returns, but if it ever turned out to be a problem, we would see it coming and we can then use a more secure and expensive source of RNG as the gold cards would be very expensive and the additional cost would be worth it.  Description  Random number generator uses future blockhash as a seed. So it s possible for miners to manipulate that value in order to get a better gold card. The issue is minor because it only makes sense if the cost of the card is high enough to do the extra work on the miner side.  Recommendation  Use better RNG algorithms if the price of gold cards is high enough for the miners to start manipulation.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"
    },
    {
        "title": "5.1 TokenFaucet refill can have an unexpected outcome ",
        "body": "  Description  The TokenFaucet contract can only disburse tokens to the users if it has enough balance. When the contract is running out of tokens, it stops dripping.  code/pool-contracts/contracts/token-faucet/TokenFaucet.sol:L119-L138  uint256 assetTotalSupply = asset.balanceOf(address(this));  uint256 availableTotalSupply = assetTotalSupply.sub(totalUnclaimed);  uint256 newSeconds = currentTimestamp.sub(lastDripTimestamp);  uint256 nextExchangeRateMantissa = exchangeRateMantissa;  uint256 newTokens;  uint256 measureTotalSupply = measure.totalSupply();  if (measureTotalSupply > 0 && availableTotalSupply > 0 && newSeconds > 0) {  newTokens = newSeconds.mul(dripRatePerSecond);  if (newTokens > availableTotalSupply) {  newTokens = availableTotalSupply;  uint256 indexDeltaMantissa = measureTotalSupply > 0 ? FixedPoint.calculateMantissa(newTokens, measureTotalSupply) : 0;  nextExchangeRateMantissa = nextExchangeRateMantissa.add(indexDeltaMantissa);  emit Dripped(  newTokens  );  The owners of the faucet can decide to refill the contract so it can disburse tokens again. If there s been a lot of time since the faucet was drained, the lastDripTimestamp value can be far behind the currentTimestamp. In that case, the users can instantly withdraw some amount (up to all the balance) right after the refill.  Recommendation  To avoid uncertainty, it s essential to call the drip function before the refill. If this call is made in a separate transaction, the owner should make sure that this transaction was successfully mined before sending tokens for the refill.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/02/pooltogether/"
    },
    {
        "title": "5.2 Gas Optimization on transfers ",
        "body": "  Description  In TokenFaucet, on every transfer _captureNewTokensForUser is called twice. This function does a few calculations and writes the latest UserState to the storage. However, if lastExchangeRateMantissa == exchangeRateMantissa, or in other words, two transfers happen in the same block, there are no changes in the newToken amounts, so there is an extra storage store with the same values.  Examples  deltaExchangeRateMantissa will be 0 in case two transfers ( no matter from or to) are in the same block for a user.  /pool-contracts/contracts/token-faucet/TokenFaucet.sol  uint256 deltaExchangeRateMantissa = uint256(exchangeRateMantissa).sub(userState.lastExchangeRateMantissa);  uint128 newTokens = FixedPoint.multiplyUintByMantissa(userMeasureBalance, deltaExchangeRateMantissa).toUint128();  userStates[user] = UserState({  lastExchangeRateMantissa: exchangeRateMantissa,  balance: uint256(userState.balance).add(newTokens).toUint128()  });  Recommendation  Return without storage update if lastExchangeRateMantissa == exchangeRateMantissa, or by another method if deltaExchangeRateMantissa == 0. This reduces the gas cost for active users (high number of transfers that might be in the same block)  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/02/pooltogether/"
    },
    {
        "title": "5.3 Handle transfer tokens where from == to ",
        "body": "  Description  In TokenFaucet, when calling beforeTokenTransfer it should also be optimized when to == from. This is to prevent any possible issues with internal accounting and token drip calculations.  /pool-contracts/contracts/token-faucet/TokenFaucet.sol  ...  if (token == address(measure) && from != address(0)) {  //add && from != to  drip();  ...  Recommendation  As ERC20 standard, from == to can be allowed but check in beforeTokenTransfer that if to == from, then do not call _captureNewTokensForUser(from); again.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/02/pooltogether/"
    },
    {
        "title": "5.4 Redundant/Duplicate checks ",
        "body": "  Description  There are a few checks (require) in TokenFaucet that are redundant and/or checked twice.  Examples  _dripRatePerSecond > 0 checked twice, no need to check it in initialize pool-contracts/contracts/token-faucet/TokenFaucet.sol  require(_dripRatePerSecond > 0, \"TokenFaucet/dripRate-gt-zero\");  asset = _asset;  measure = _measure;  setDripRatePerSecond(_dripRatePerSecond);  function setDripRatePerSecond(uint256 _dripRatePerSecond) public onlyOwner {  require(_dripRatePerSecond > 0, \"TokenFaucet/dripRate-gt-zero\");  lastDripTimestamp == uint32(currentTimestamp) and newSeconds > 0 are basically the same check.  measureTotalSupply can never be < 0, as in the if statement enforces that /pool-contracts/contracts/token-faucet/TokenFaucet.sol#L111-L117  function drip() public returns (uint256) {  uint256 currentTimestamp = _currentTime();  // this should only run once per block.  if (lastDripTimestamp == uint32(currentTimestamp)) {  return 0;  ...  uint256 newSeconds = currentTimestamp.sub(lastDripTimestamp);  ...  if (measureTotalSupply > 0 && availableTotalSupply > 0 && newSeconds > 0) {  ...  uint256 indexDeltaMantissa = measureTotalSupply > 0 ? FixedPoint.calculateMantissa(newTokens, measureTotalSupply) : 0;  Recommendation  Remove the redundant checks to reduce the code size and complexity.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/02/pooltogether/"
    },
    {
        "title": "5.5 Unnecessary use of upgradability   ",
        "body": "  Resolution   These contracts are part of   OpenZeppelin Contracts Upgradeable.  Description  Libraries such as SafeMath and SafeCast should not be upgradable as they should be used as pure functions.  Upgradable libraries used in TokenFaucet contract:  SafeMathUpgradeable  SafeCastUpgradeable  IERC20Upgradeable  Recommendation  Remove the upgradability functionality from any part of the system that is unnecessary, as they add complexity and centralization power to the admins.  ",
        "labels": [
            "Consensys",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/02/pooltogether/"
    },
    {
        "title": "3.1 Memory corruption in Buffer    ",
        "body": "  Resolution   Issue has been closed in   ensdomains/buffer#3  Description  Although out of scope for this audit, the audit team noticed a memory corruption issue in the Buffer library. The init function is as follows:  contracts/Buffer.sol:L22-L41  /**  @dev Initializes a buffer with an initial capacity.  @param buf The buffer to initialize.  @param capacity The number of bytes of space to allocate the buffer.  @return The buffer, for chaining.  /  function init(buffer memory buf, uint capacity) internal pure returns(buffer memory) {  if (capacity % 32 != 0) {  capacity += 32 - (capacity % 32);  // Allocate space for the buffer data  buf.capacity = capacity;  assembly {  let ptr := mload(0x40)  mstore(buf, ptr)  mstore(ptr, 0)  mstore(0x40, add(32, add(ptr, capacity)))  return buf;  Note that memory is reserved only for capacity bytes, but the bytes actually requires capacity + 32 bytes to account for the prefixed array length. Other functions in Buffer assume correct allocation and therefore corrupt nearby memory.  Although we didn t immediately spot an ENS exploit for this vulnerability, we consider any memory corruption issue to be important to address.  Example  A simple test shows the memory corruption issue:  contract Test {  using Buffer for Buffer.buffer;  function test() external pure {  Buffer.buffer memory buffer;  buffer.init(1);  // foo immediately follows buffer.buf in memory  bytes memory foo = new bytes(0);  assert(foo.length == 0);  buffer.append(\"A\");  // \"A\" == 65, gets written to the high order byte of foo.length  assert(foo.length == 65 * 256**31);  Remediation  Allocate an additional 32 bytes as follows, to account for storing the uint256 size of the bytes array:  mstore(0x40, add(ptr, add(capacity, 32)))  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"
    },
    {
        "title": "3.2 SimplePriceOracle.price is susceptible to integer overflow    ",
        "body": "  Resolution   Issue has been closed in   ensdomains/ethregistrar#17 by using  Description  SimplePriceOracle.price is as follows:  ethregistrar/contracts/SimplePriceOracle.sol:L26-L28  function price(string calldata /*name*/, uint /*expires*/, uint duration) external view returns(uint) {  return duration * rentPrice;  This is susceptible to a simple overflow attack, e.g. setting the duration to 2**256/rentPrice to give yourself a price of 0.  Severity note: It s unclear whether the SimplePriceOracle is expected to be used in practice, but the severity is set here under the assumption that the code may be used somewhere.  Remediation  Use SafeMath or explicitly check for the overflow.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"
    },
    {
        "title": "3.3 ETHRegistrarController.register is vulnerable to front running    ",
        "body": "  Resolution   Issue has been closed in   ensdomains/ethregistrar#18  Description  commit() and then register() appears to serve the purpose of preventing front running. However, because the commitment is not tied to a specific owner, it serves equally well as a commitment for a front-running attacker.  Example  Alice calls commit(makeCommitment(\"mydomain\", <secret>)).  10 minutes later, Alice submits a transaction to register(\"mydomain\", Alice, ..., <secret>).  Eve observes this transaction in the transaction pool.  Eve submits register(\"mydomain\", Eve, ..., <secret>) with a higher gas price and wins the race.  Remediation  Commitments should commit to owners in addition to names. This way an attacker can t repurpose a previous commitment. (They would have to buy on behalf of the original committer.)  As an alternative, if it s undesirable to pin down owner, the commitment could include msg.sender instead (only allowing the original committer to call register).  E.g. the following (and corresponding changes to callers):  function makeCommitment(  string memory name,  address owner, /* or perhaps committer/sender */  bytes32 secret  pure  public  returns(bytes32)  bytes32 label = keccak256(bytes(name));  return keccak256(abi.encodePacked(label, owner, secret));  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"
    },
    {
        "title": "3.4 SOA record check on the wrong domain    ",
        "body": "  Resolution   During the audit, this issue was discovered by the client development team and already fixed in   ensdomains/root#25.  Description  The SOA record check in Root.getAddress is meant to happen on the root TLD, but in the version of the code audited, it is performed instead on _ens.nic.<tld>.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"
    },
    {
        "title": "3.5 Work towards a trustless model for ENS   ",
        "body": "  Resolution  Acknowledged by client team. As stated, this is a long-term issue for which there is no immediate fix, but work is already in progress.  Description  The ENS registry itself is owned by a multisig wallet owned by a number of reputable Ethereum community members. That multisig wallet can do just about anything, up to and including directly taking over any existing or future registered names.  It s important to note that even if we as a community trust the current owners of the multisig wallet, we also need to consider the possibility of their Ethereum private keys being compromised by malicious actors.  Remediation  This centralized control is by design, and the multisig owners have been chosen carefully. However, we do recommend\u2014as is already the plan\u2014that the multisig wallet s power be reduced in future updates to the system. Changes made by that wallet are already quite transparent to the community, but future enhancements might include requiring a waiting period for any changes or disallowing certain types of changes altogether.  In the meantime, wherever possible, the trust model should be made clear so that users understand what guarantees they do and do not have when interacting with ENS.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"
    },
    {
        "title": "3.6 Consider replacing the Buffer implementation    ",
        "body": "  Resolution   There will be no immediate fix for this, but the client team is working on collaborating to get a better audited   Description  The audit team uncovered two bugs in the Buffer library, one each in the only two functions that were looked at. (The library was in general not in scope for this audit.) One bug was a critical memory corruption bug. This calls into question how safe this library is to use in general.  Remediation  Consider using a different library, ideally one that has been fully tested and audited and that minimizes the use of inline assembly, particularly around memory allocation.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"
    },
    {
        "title": "3.7 Overzealous resizing in Buffer    ",
        "body": "  Resolution   Issue has been closed in   ensdomains/buffer#4  Description  In the following code, the buffer is resized even when sufficient capacity is available to perform the write. The buf.buf.length term is unnecessary and leads to unnecessary resizing:  contracts/Buffer.sol:L91-L95  function write(buffer memory buf, uint off, bytes memory data, uint len) internal pure returns(buffer memory) {  require(len <= data.length);  if (off + len > buf.capacity) {  resize(buf, max(buf.capacity, len + off) * 2);  Contrast with the calculation in a similar function:  contracts/Buffer.sol:L206-L209  function write(buffer memory buf, uint off, bytes32 data, uint len) private pure returns(buffer memory) {  if (len + off > buf.capacity) {  resize(buf, (len + off) * 2);  Remediation  Check just the condition if (off + len > buf.capacity) when deciding whether to resize the buffer. This will be a significant gas savings in the common case of reserving exactly the right capacity and then performing two append operations.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"
    },
    {
        "title": "3.8 Pending auctions in the legacy registrar don t result in proper ownership in ENS    ",
        "body": "  Resolution   Addressed in   ensdomains/ethregistrar#23 by reducing the waiting period to 28 days.  Description  If an auction has yet to be finalized in the legacy HashRegistrar at the time that the new, permanent .eth registrar is put in place, the auction winner doesn t get actual ownership of the ENS entry.  The sequence of events would look like:  Auction is started in the HashRegistrar for the name something.eth  The new BaseRegistrarImplementation becomes the owner of the .eth root node in ENS.  The auction is won.  The auction winner calls finalizeAuction, which calls trySetSubnodeOwner, which fails to actually set subnode ownership (as the HashRegistrar no longer has ownership of the .eth root node).  At this point, there s an owner of the deed for the name something.eth in the HashRegistrar, but the ENS subnode is unowned. It can t be transferred to the new registrar for 183 days, and the name can t be registered in the new registrar.  The owner can get themselves out of this situation by calling releaseDeed in the HashRegistrar. If they want to avoid potentially losing their domain in the process, they can transfer the deed to a smart contract which can then release the deed and rent the same name in the new registrar atomically.  Remediation  Here are a few ideas of improvements to help in this situation:  Discourage (or prevent, if possible) new auctions very close to the launch of the new registrar.  Allow domains to be transferred before the 183-day waiting period but require rent payment in those cases. (Perhaps just use the existing grace period to have people renew?)  Document the process for rescuing names that get stuck in this state, or better yet provide a tool for doing so.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"
    },
    {
        "title": "3.9 BaseRegistrarImplementation.acceptRegistrarTransfer should probably use the live modifier    ",
        "body": "  Resolution   Issue has been closed in   ensdomains/ethregistrar#19.  Description  Most external functions in BaseRegistrarImplementation have the live modifier, which ensures that they can only be called on the current ENS owner of the registrar s base address. The acceptRegistrarTransfer function does not have this modifier, which means names can be transferred to the new registrar even if it s not the proper registry owner.  It s hard to think of a real-world example of why this is problematic, especially because the interim registrar appears to protect against this by only transferring to the ens.owner, but it seems safer to include the live modifier unless there s a specific reason not to.  Remediation  Add the live modifier to acceptRegistrarTransfer.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"
    },
    {
        "title": "3.10 Reconsider use of inline assembly in BytesUtils.sol    ",
        "body": "  Resolution   Issue has been closed in   ensdomains/dnssec-oracle#55  Description  Root.sol imports and uses @ensdomains/dnssec-oracle/contracts/BytesUtils.sol for byte operations.  BytesUtils.sol is mainly written in assembly. In general, inline assembly is concerning from a security perspective because it bypasses compiler checks and inhibits human code reasoning.  e.g.readUint8():  function readUint8(bytes memory self, uint idx) internal pure returns (uint8 ret) {  require(idx + 1 <= self.length);  assembly {  ret := and(mload(add(add(self, 1), idx)), 0xFF)  Remediation  Some of the functions in BytesUtil.sol can be written in Solidity without affecting the gas costs.  readUint8() can be written as following Solidity code which functions the same:  function readUint8(bytes memory self, uint idx) internal pure returns (uint8 ret) {  return uint8(self[idx]);  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"
    },
    {
        "title": "3.11 BaseRegistrarImplementation.acceptRegistrarTransfer does not check for invalid names    ",
        "body": "  Resolution  Short names will be manually canceled in the old registrar during the migration period. Note that this is still feasible with the reduced 28-day lock-up period.  Description  BaseRegistrarImplementation.acceptRegistrarTransfer does not explicitly check for invalid names.  In the old registrar it is possible to register domain names with length less than 7 characters. However anyone can call HashRegistrar.invalidateName() to invalidate the registration and get half of the deed amount as an incentive.  Assume that an invalid domain is registered in the old registrar and no one invalidates the registration (within the 183 days between the registrationDate and the transfer ETHRegistrarController.acceptRegistrarTransfer), it is possible to transfer the invalid domain to the new ENS registrar.  Remediation  Given that it is easy to check for invalid domains using a rainbow table for all possible <7 character domains, anyone can invalidate them before the new registrar goes live. Note that for the auctions starting right before the new registrar goes live, there will be a 183 days window in which anyone can call HashRegistrar.invalidateName() to invalidate the domain names.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"
    },
    {
        "title": "3.12 Sanity check around transferPeriodEnds    ",
        "body": "  Resolution   Issue has been closed in   ensdomains/ethregistrar#23  Description  BaseRegistrarImplementation.acceptRegistrarTransfer has a hardcoded limit such that only domains registered 183 days ago can be transferred in.  This imposes an implicit constraint on the transferPeriodEnds state variable. If the transfer period ends too soon after the new registrar is put in place, names that were just registered won t be transferrable during the transfer period (and will thus become available to be rented by another user).  Remediation  A sanity check in the constructor would help here, e.g.:  require(_transferPeriodEnds > now + 183 days);  Note that the true requirement is something more like  The time between when this registrar becomes the ENS node owner of the .eth domain and the time of transferPeriodEnds must be at least 183 days plus a sufficient time window for late registrants to have a chance to perform the transfer.  But it s hard to see a way to encode this precisely. A broad sanity check will at least avoid simple timing mistakes.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"
    },
    {
        "title": "3.13 StablePriceOracle.price has an unimportant integer underflow    ",
        "body": "  Resolution   Issue has been closed in   ensdomains/ethregistrar#20  Description  ethregistrar/contracts/StablePriceOracle.sol:L57-L63  function price(string calldata name, uint /*expires*/, uint duration) view external returns(uint) {  uint len = name.strlen();  require(len > 0);  if(len > rentPrices.length) {  len = rentPrices.length;  uint priceUSD = rentPrices[len - 1].mul(duration);  If the length of the rentPrices array is 0, then the last line above attempts to access rentPrices[2**256-1]. This will assert, but it might be more friendly (from a gas perspective) to revert in this case.  Remediation  A simple fix would be to move the require(len > 0) down until just before the array access.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"
    },
    {
        "title": "3.14 ETHRegistrarController.register should revert rather than silently fail    ",
        "body": "  Resolution   Issue has been closed in   ensdomains/ethregistrar#22  Description  When called with an invalid commitment or unavailable domain, ETHRegistrarController.register refunds the sent ether and silently fails rather than reverting:  ethregistrar/contracts/ETHRegistrarController.sol:L56-L64  function register(string calldata name, address owner, uint duration, bytes32 secret) external payable {  // Require a valid commitment  bytes32 commitment = makeCommitment(name, secret);  require(commitments[commitment] + MIN_COMMITMENT_AGE <= now);  // If the commitment is too old, or the name is registered, stop  if(commitments[commitment] + MAX_COMMITMENT_AGE < now || !available(name))  {  msg.sender.transfer(msg.value);  return;  register also has no return value, so it s difficult for a caller to know whether the register action succeeded or failed.  Remediation  It s probably better to use require(...) to handle these invalid cases. This is roughly equivalent because no state changes have been made before this early return, but it seems less error prone and clearer to callers about what happened.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"
    },
    {
        "title": "3.15 StringUtils.strlen could be rewritten without assembly    ",
        "body": "  Resolution   Issue has been closed in   ensdomains/ethregistrar#21  Description  StringUtils.strlen uses inline assembly to walk through a UTF-8 string and count its character length. In general, inline assembly is concerning from a security perspective because it bypasses compiler checks and inhibits human code reasoning.  Remediation  Consider rewriting in Solidity, something similar to the following:  function strlen(string memory s) internal pure returns (uint256) {  uint256 i = 0;  uint256 len;  for (len = 0; i < bytes(s).length; len++) {  byte b = bytes(s)[i];  if (b < 0x80) {  i += 1;  } else if (b < 0xE0) {  i += 2;  ...  return len;  4 Threat Model  The creation of a threat model is beneficial when building smart contract systems as it helps to understand the potential security threats, assess risk, and identify appropriate mitigation strategies. This is especially useful during the design and development of a contract system as it allows to create a more resilient design which is more difficult to change post-development.  A threat model was created during the audit process in order to analyze the attack surface of the contract system and to focus review and testing efforts on key areas that a malicious actor would likely also attack. It consists of two parts: a high-level analysis that help to understand the attack surface and a list of threats that exist for the contract system.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"
    },
    {
        "title": "4.1 Overview",
        "body": "  The following assets are managed by contracts and likely targets for an attacker:  Registered domain names (e.g. foo.eth)  Ether, in the form of rent paid to the ETHRegistrarController  The following actors have access to the system to perform an attack:  System owners (ENS itself, registrars, controllers, price oracles)  DNS domain/subdomain owners, who can update DNSSEC records  Users who are registering, renewing, and transferring domains  The following describes the surface area available to attackers:  DNSSEC records  Registrars and controllers  Root contract  Ethereum private keys  Because they were out of scope for this audit, we did not consider some interesting targets such as the DNSSEC oracle, DNSSEC-based registrar, the interim .eth registrar, or the multisig wallet used for ENS ownership.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"
    },
    {
        "title": "4.2 Threat Analysis",
        "body": "  The following table contains a list of identified threats, along with their mitigations:  user may try to register/renew a domain for less than the expected price  overflow on the rent price / manipulate the price oracle  SafeMath mitigates some potential math errors  user may try to mount denial-of-service attacks on other users (e.g. censor their purchases/renewals)  network DoS  long purchase windows and grace periods  user may try to snipe a domain  front-running  a commit/reveal scheme attempts to prevent this but is ineffective (see section 3), a generous grace period prevents race conditions on expiration  user may try to register .eth TLD  update DNSSEC records  Root disallows changes to that node  Root owner may steal domains, manipulate prices, etc.  ENS root swaps the controller/registrar with malicious code  such manipulation would be transparent today, and future updates may limit the root owners  powers  domain owners may take over already-owned subdomains  change DNSSEC to replace registrar for a domain  this is allowed by design  5 Tool-based analysis  The issues from the tool based analysis have been reviewed and the relevant issues have been listed in chapter 3 - Issues.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"
    },
    {
        "title": "5.1 MythX",
        "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  Where possible, we ran the full MythX analysis. MythX is still in beta, and where analysis failed, we fell back to running Mythril Classic, a large subset of the functionality of MythX.  Below is the raw output of MythX and Mythril Classic vulnerability scans:  In order to run MythX, Root.sol contract was flattened. flat_root.sol line numbers reflect on the output of truffle-flattener contracts/Root.sol.  Title: Floating Pragma  Head: A floating pragma is set.  Description: It is recommended to make a conscious choice on what version of Solidity is used for compilation. Currently any version equal or greater than \"0.4.24\" is allowed.  Source code:  flat_root.sol 1:0  --------------------------------------------------  pragma solidity ^0.4.24;  --------------------------------------------------  ==================================================  Title: Shadowing State Variables  Head: State variable shadows another state variable.  Description: The state variable \"CLASS_INET\" in contract \"DNSClaimChecker\" shadows another state variable with the same name \"CLASS_INET\" in contract \"Root\".  Source code:  flat_root.sol 869:4  --------------------------------------------------  uint16 constant CLASS_INET = 1  --------------------------------------------------  ==================================================  Title: Shadowing State Variables  Head: State variable shadows another state variable.  Description: The state variable \"TYPE_TXT\" in contract \"DNSClaimChecker\" shadows another state variable with the same name \"TYPE_TXT\" in contract \"Root\".  Source code:  flat_root.sol 870:4  --------------------------------------------------  uint16 constant TYPE_TXT = 16  --------------------------------------------------  ==================================================  Title: Shadowing State Variables  Head: Local variable shadows a state variable.  Description: The local variable \"owner\" in contract \"ENS\" shadows the state variable with the same name \"owner\" in contract \"Ownable\".  Source code:  flat_root.sol 20:58  --------------------------------------------------  address owner  --------------------------------------------------  flat_root.sol 22:36  --------------------------------------------------  address owner  --------------------------------------------------  ==================================================  Title: Shadowing State Variables  Head: Local variable shadows a state variable.  Description: The local variable \"owner\" in contract \"Root\" shadows the state variable with the same name \"owner\" in contract \"Ownable\".  Source code:  flat_root.sol 1012:44  --------------------------------------------------  address owner  --------------------------------------------------  flat_root.sol 1037:36  --------------------------------------------------  address owner  --------------------------------------------------  ==================================================  Title: Shadowing State Variables  Head: Local variable shadows a state variable.  Description: The local variable \"oracle\" in contract \"DNSClaimChecker\" shadows the state variable with the same name \"oracle\" in contract \"Root\".  Source code:  flat_root.sol 881:29  --------------------------------------------------  DNSSEC oracle  --------------------------------------------------  ==================================================  BaseRegistrarImplementation  Mythril Classic results for BaseRegistrarImplementation are as follows. flat_BaseRegistrarImplementation.sol line numbers reflect on the output of truffle-flattener contracts/BaseRegistrarImplementation.sol  ETHRegistrarController  Mythril Classic results for ETHRegistrarController are as follows. flat_ETHRegistrarController.sol line numbers reflect on the output of truffle-flattener contracts/ETHRegistrarController.sol.  ==== Multiple Calls in a Single Transaction ====  SWC ID: 113  Severity: Medium  Contract: ETHRegistrarController  Function name: rentPrice(string,uint256)  PC address: 996  Estimated Gas Usage: 5179 - 79151  Multiple sends are executed in one transaction.  Consecutive calls are executed at the following bytecode offsets:  Offset: 2947  Offset: 3202  Try to isolate each external call into its own transaction, as external calls can fail accidentally or deliberately.  --------------------  In file: flat_ETHRegistrarController.sol:1467  function rentPrice(string memory name, uint duration) view public returns(uint) {  bytes32 hash = keccak256(bytes(name));  return prices.price(name, base.nameExpires(uint256(hash)), duration);  --------------------  ==== Dependence on predictable environment variable ====  SWC ID: 116  Severity: Low  Contract: ETHRegistrarController  Function name: register(string,address,uint256,bytes32)  PC address: 3552  Estimated Gas Usage: 2056 - 6247  Sending of Ether depends on a predictable variable.  The contract sends Ether depending on the values of the following variables:  block.timestamp  block.timestamp  block.timestamp  Note that the values of variables like coinbase, gaslimit, block number and timestamp are predictable and/or can be manipulated by a malicious miner. Don't use them for random number generation or to make critical decisions.  --------------------  In file: flat_ETHRegistrarController.sol:1498  msg.sender.transfer(msg.value)  --------------------  ==== Integer Overflow ====  SWC ID: 101  Severity: High  Contract: ETHRegistrarController  Function name: register(string,address,uint256,bytes32)  PC address: 5332  Estimated Gas Usage: 2333 - 9254  The binary addition can overflow.  The operands of the addition operation are not sufficiently constrained. The addition could therefore result in an integer overflow. Prevent the overflow by checking inputs or ensure sure that the overflow is caught by an assertion.  --------------------  In file: flat_ETHRegistrarController.sol:1411  add(mload(s), ptr)  --------------------  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"
    },
    {
        "title": "5.2 Ethlint",
        "body": "  Ethlint is an open source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  ethregistrar  contracts/BaseRegistrarImplementation.sol  36:36     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  60:42     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  65:15     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  73:16     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  73:48     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  75:23     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  83:39     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  85:15     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  89:47     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  114:37    warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  118:35    warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  contracts/ETHRegistrarController.sol  53:63    warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  54:34    warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  61:64    warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  64:58    warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  contracts/StringUtils.sol  15:8     error    Avoid using Inline Assembly.    security/no-inline-assembly  22:12    error    Avoid using Inline Assembly.    security/no-inline-assembly  \u2716 2 errors, 15 warnings found.  root  No issues found.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"
    },
    {
        "title": "5.3 Surya",
        "body": "  Surya is an utility tool for smart contract systems. It provides a number of visual outputs and information about structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  Files Description Table  root/contracts/Migrations.sol  eac3bb098bace681296263c037b30123fd46e01a  root/contracts/Ownable.sol  b596da7ad9b5c92a119268e05a5f2190659de8d3  root/contracts/Root.sol  94c5fd45635c6d78cae15903bdf565e2c587bdfa  ethregistrar/contracts/BaseRegistrar.sol  dfadfc8a35024069ff66cbc4a82b67dc48129eab  ethregistrar/contracts/BaseRegistrarImplementation.sol  a1e04ce66a9588063155591b59cd695d2d35cabe  ethregistrar/contracts/DummyOracle.sol  e1dab33211d55e02874ae2510e5e773e13056939  ethregistrar/contracts/ETHRegistrarController.sol  7cb180a1d5102efd2acc04b0b518848b6127846e  ethregistrar/contracts/Migrations.sol  b6732a145e4cb6841945488f591b1cf383a6441e  ethregistrar/contracts/PriceOracle.sol  3257acda730f294f19984163f9fe4a19eabdef4d  ethregistrar/contracts/SafeMath.sol  5effc6db2209b2bf2d49abe4ad1ac247e106f8d9  ethregistrar/contracts/SimplePriceOracle.sol  fc11bff8c93e8471b8d8478f1a14b7f43fff2eef  ethregistrar/contracts/StablePriceOracle.sol  892333542a757ba6089c5c3d19d00b337cb0da78  ethregistrar/contracts/StringUtils.sol  4d784bb26b409cfd8ed841f43c4e0ffbfddc450b  ethregistrar/contracts/_TestDeps.sol  2077d541fedbd889d2f814c5c51aa046078f566d  Contracts Description Table  Function Name  Visibility  Mutability  Modifiers  Migrations  Implementation  <Constructor>  Public    setCompleted  Public    restricted  upgrade  Public    restricted  Ownable  Implementation  <Constructor>  Public    transferOwnership  Public    onlyOwner  isOwner  Public    NO   Root  Implementation  Ownable  <Constructor>  Public    proveAndRegisterTLD  External    NO   setSubnodeOwner  External    onlyOwner  setRegistrar  External    onlyOwner  registerTLD  Public    NO   setResolver  Public    onlyOwner  setOwner  Public    onlyOwner  setTTL  Public    onlyOwner  getLabel  Internal \ud83d\udd12  getAddress  Internal \ud83d\udd12  getSOAHash  Internal \ud83d\udd12  BaseRegistrar  Implementation  ERC721, Ownable  addController  External    NO   removeController  External    NO   nameExpires  External    NO   available  Public    NO   register  External    NO   renew  External    NO   reclaim  External    NO   acceptRegistrarTransfer  External    NO   BaseRegistrarImplementation  Implementation  BaseRegistrar  <Constructor>  Public    ownerOf  Public    NO   addController  External    onlyOwner  removeController  External    onlyOwner  nameExpires  External    NO   available  Public    NO   register  External    live onlyController  renew  External    live onlyController  reclaim  External    live  acceptRegistrarTransfer  External    NO   DummyOracle  Implementation  <Constructor>  Public    set  Public    NO   read  External    NO   ETHRegistrarController  Implementation  Ownable  <Constructor>  Public    rentPrice  Public    NO   valid  Public    NO   available  Public    NO   makeCommitment  Public    NO   commit  Public    NO   register  External    NO   renew  External    NO   setPriceOracle  Public    onlyOwner  withdraw  Public    onlyOwner  Migrations  Implementation  <Constructor>  Public    setCompleted  Public    restricted  upgrade  Public    restricted  PriceOracle  Interface  price  External    NO   SafeMath  Library  mul  Internal \ud83d\udd12  div  Internal \ud83d\udd12  sub  Internal \ud83d\udd12  add  Internal \ud83d\udd12  mod  Internal \ud83d\udd12  SimplePriceOracle  Implementation  Ownable, PriceOracle  <Constructor>  Public    setPrice  Public    onlyOwner  price  External    NO   DSValue  Interface  read  External    NO   StablePriceOracle  Implementation  Ownable, PriceOracle  <Constructor>  Public    setOracle  Public    onlyOwner  setPrices  Public    onlyOwner  price  External    NO   StringUtils  Library  strlen  Internal \ud83d\udd12  Legend  Function can modify state  Function is payable  Root Control Flow  6 Test Coverage Measurement  Testing is implemented using Truffle. 12 tests are included for the Root contract, and they all pass. 30 tests are included for the .eth permanent registrar, and they all pass.  We were unable to obtain code coverage numbers for the tests, but the audit team s overall impression is that testing covers a high percentage of code branches. That said, the testing is weak, in particular regarding negative test cases and edge cases. As a specific example, changing the following in ETHRegistrarController.renew causes no test failures, which shows a serious lack of coverage:  // OLD: require(msg.value >= cost);  // NEW:  require(msg.value > 0);  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"
    },
    {
        "title": "5.1 didTransferShares function has no access control modifier    ",
        "body": "  Resolution   The concerned function has now been restricted to be only called by   146 with final commit hash as  Description  The staked tokens (shares) in Forta are meant to be transferable. Similarly, the rewards allocation for these shares for delegated staking is meant to be transferable as well. This allocation for the shares  owner is tracked in the StakeAllocator. To enable this, the Forta staking contract FortaStaking implements a _beforeTokenTransfer() function that calls _allocator.didTransferShares() when it is appropriate to transfer the underlying allocation.  code/contracts/components/staking/FortaStaking.sol:L572-L585  function _beforeTokenTransfer(  address operator,  address from,  address to,  uint256[] memory ids,  uint256[] memory amounts,  bytes memory data  ) internal virtual override {  for (uint256 i = 0; i < ids.length; i++) {  if (FortaStakingUtils.isActive(ids[i])) {  uint8 subjectType = FortaStakingUtils.subjectTypeOfShares(ids[i]);  if (subjectType == DELEGATOR_NODE_RUNNER_SUBJECT && to != address(0) && from != address(0)) {  _allocator.didTransferShares(ids[i], subjectType, from, to, amounts[i]);  Due to this, the StakeAllocator.didTransferShares() has an external visibility so it can be called from the FortaStaking contract to perform transfers. However, there is no access control modifier to allow only the staking contract to call this. Therefore, anyone can call this function with whatever parameters they want.  code/contracts/components/staking/allocation/StakeAllocator.sol:L341-L349  function didTransferShares(  uint256 sharesId,  uint8 subjectType,  address from,  address to,  uint256 sharesAmount  ) external {  _rewardsDistributor.didTransferShares(sharesId, subjectType, from, to, sharesAmount);  Recommendation  Apply access control modifiers as appropriate for this contract, for example onlyRole().  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"
    },
    {
        "title": "5.2 Incorrect reward epoch start date calculation    ",
        "body": "  Resolution   The suggested recommendations have been implemented in a pull request   144 with a final hash as  Description  The Forta rewards system is based on epochs. A privileged address with the role REWARDER_ROLE calls the reward() function with a parameter for a specific epochNumber that consequently distributes the rewards for that epoch. Additionally, as users stake and delegate their stake, accounts in the Forta system accrue weight that is based on the active stake to distribute these rewards. Since accounts can modify their stake as well as delegate or un-delegate it, the rewards weight for each account can be modified, as seen, for example, in the didAllocate() function. In turn, this modifies the DelegatedAccRewards storage struct that stores the accumulated rewards for each share id. To keep track of changes done to the accumulated rewards, epochs with checkpoints are used to manage the accumulated rate of rewards, their value at the checkpoint, and the timestamp of the checkpoint.  For example, in the didAllocate() function the addRate() function is being called to modify the accumulated rewards.  code/contracts/components/staking/rewards/RewardsDistributor.sol:L89-L101  function didAllocate(  uint8 subjectType,  uint256 subject,  uint256 stakeAmount,  uint256 sharesAmount,  address staker  ) external onlyRole(ALLOCATOR_CONTRACT_ROLE) {  bool delegated = getSubjectTypeAgency(subjectType) == SubjectStakeAgency.DELEGATED;  if (delegated) {  uint8 delegatorType = getDelegatorSubjectType(subjectType);  uint256 shareId = FortaStakingUtils.subjectToActive(delegatorType, subject);  DelegatedAccRewards storage s = _rewardsAccumulators[shareId];  s.delegated.addRate(stakeAmount);  Then the function flow goes into setRate() that checks the existing accumulated rewards storage and modifies it based on the current timestamp.  code/contracts/components/staking/rewards/Accumulators.sol:L34-L36  function addRate(Accumulator storage acc, uint256 rate) internal {  setRate(acc, latest(acc).rate + rate);  code/contracts/components/staking/rewards/Accumulators.sol:L42-L50  function setRate(Accumulator storage acc, uint256 rate) internal {  EpochCheckpoint memory ckpt = EpochCheckpoint({ timestamp: SafeCast.toUint32(block.timestamp), rate: SafeCast.toUint224(rate), value: getValue(acc) });  uint256 length = acc.checkpoints.length;  if (length > 0 && isCurrentEpoch(acc.checkpoints[length - 1].timestamp)) {  acc.checkpoints[length - 1] = ckpt;  } else {  acc.checkpoints.push(ckpt);  Namely, it pushes epoch checkpoints to the list of account checkpoints based on its timestamp. If the last checkpoint s timestamp is during the current epoch, then the last checkpoint is replaced with the new one altogether. If the last checkpoint s timestamp is different from the current epoch, a new checkpoint is added to the list. However, the isCurrentEpoch() function calls a function getCurrentEpochTimestamp() that incorrectly determines the start date of the current epoch. In particular, it doesn t take the offset into account when calculating how many epochs have already passed.  code/contracts/components/staking/rewards/Accumulators.sol:L103-L110  function getCurrentEpochTimestamp() internal view returns (uint256) {  return ((block.timestamp / EPOCH_LENGTH) * EPOCH_LENGTH) + TIMESTAMP_OFFSET;  function isCurrentEpoch(uint256 timestamp) internal view returns (bool) {  uint256 currentEpochStart = getCurrentEpochTimestamp();  return timestamp > currentEpochStart;  Instead of ((block.timestamp / EPOCH_LENGTH) * EPOCH_LENGTH) + TIMESTAMP_OFFSET, it should be (((block.timestamp - TIMESTAMP_OFFSET) / EPOCH_LENGTH) * EPOCH_LENGTH) + TIMESTAMP_OFFSET. In fact, it should simply call the getEpochNumber() function that correctly provides the epoch number for any timestamp.  code/contracts/components/staking/rewards/Accumulators.sol:L95-L97  function getEpochNumber(uint256 timestamp) internal pure returns (uint32) {  return SafeCast.toUint32((timestamp - TIMESTAMP_OFFSET) / EPOCH_LENGTH);  In other words, the resulting function would look something like the following:  code/contracts/components/staking/rewards/Accumulators.sol:L45-L48  if (length > 0 && isCurrentEpoch(acc.checkpoints[length - 1].timestamp)) {  acc.checkpoints[length - 1] = ckpt;  } else {  acc.checkpoints.push(ckpt);  This causes several checkpoints to be stored for the same epoch, which would cause issues in functions such as getAtEpoch(), that feeds into getValueAtEpoch() function that provides data for the rewards  share calculation. In the end, this would cause issues in the accounting for the rewards calculation resulting in incorrect distributions.  During the discussion with the Forta Foundation team, it was additionally discovered that there are edge cases around the limits of epochs. Specifically, epoch s end time and the subsequent epoch s start time are exactly the same, although it should be that it is only the start of the next epoch. Similarly, that start time isn t recognized as part of the epoch due to > sign instead of >=. In particular, the following changes need to be made:  Recommendation  A refactor of the epoch timestamp calculation functions is recommended to account for:  The correct epoch number to calculate the start and end timestamps of epochs.  The boundaries of epochs coinciding.  Clarity in functions  intent. For example, adding a function just to calculate any epoch s start time and renaming getCurrentEpochTimestamp() to getCurrentEpochStartTimestamp().  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"
    },
    {
        "title": "5.3 A single unfreeze dismisses all other slashing proposal freezes    ",
        "body": "  Resolution   As per the recommendation, the Forta team modified the logic in favor of open proposals. Now, every   149 with a final hash  Description  In order to retaliate against malicious actors, the Forta staking system allows users to submit slashing proposals that are guarded by submitting along a deposit with a slashing reason. These proposals immediately freeze the proposal s subject s stake, blocking them from withdrawing that stake.  At the same time, there can be multiple proposals submitted against the same subject, which works out with freezing   the subject remains frozen with each proposal submitted. However, once any one of the active proposals against the subject gets to the end of its lifecycle, be it REJECTED, DISMISSED, EXECUTED, or REVERTED, the subject gets unfrozen altogether. The other proposals might still be active, but the stake is no longer frozen, allowing the subject to withdraw it if they would like.  In terms of impact, this allows bad actors to avoid punishment intended by the slashes and freezes. A malicious actor could, for example, submit a faulty proposal against themselves in the hopes that it will get quickly rejected or dismissed while the existing, legitimate proposals against them are still being considered. This would allow them to get unfrozen quickly and withdraw their stake. Similarly, in the event a bad staker has several proposals against them, they could withdraw right after a single slashing proposal goes through.  Examples  code/contracts/components/staking/slashing/SlashingController.sol:L174-L179  function dismissSlashProposal(uint256 _proposalId, string[] calldata _evidence) external onlyRole(SLASHING_ARBITER_ROLE) {  _transition(_proposalId, DISMISSED);  _submitEvidence(_proposalId, DISMISSED, _evidence);  _returnDeposit(_proposalId);  _unfreeze(_proposalId);  code/contracts/components/staking/slashing/SlashingController.sol:L187-L192  function rejectSlashProposal(uint256 _proposalId, string[] calldata _evidence) external onlyRole(SLASHING_ARBITER_ROLE) {  _transition(_proposalId, REJECTED);  _submitEvidence(_proposalId, REJECTED, _evidence);  _slashDeposit(_proposalId);  _unfreeze(_proposalId);  code/contracts/components/staking/slashing/SlashingController.sol:L215-L229  function reviewSlashProposalParameters(  uint256 _proposalId,  uint8 _subjectType,  uint256 _subjectId,  bytes32 _penaltyId,  string[] calldata _evidence  ) external onlyRole(SLASHING_ARBITER_ROLE) onlyInState(_proposalId, IN_REVIEW) onlyValidSlashPenaltyId(_penaltyId) onlyValidSubjectType(_subjectType) notAgencyType(_subjectType, SubjectStakeAgency.DELEGATOR) {  // No need to check for proposal existence, onlyInState will revert if _proposalId is in undefined state  if (!subjectGateway.isRegistered(_subjectType, _subjectId)) revert NonRegisteredSubject(_subjectType, _subjectId);  _submitEvidence(_proposalId, IN_REVIEW, _evidence);  if (_subjectType != proposals[_proposalId].subjectType || _subjectId != proposals[_proposalId].subjectId) {  _unfreeze(_proposalId);  _freeze(_subjectType, _subjectId);  code/contracts/components/staking/slashing/SlashingController.sol:L254-L259  function revertSlashProposal(uint256 _proposalId, string[] calldata _evidence) external {  _authorizeRevertSlashProposal(_proposalId);  _transition(_proposalId, REVERTED);  _submitEvidence(_proposalId, REVERTED, _evidence);  _unfreeze(_proposalId);  code/contracts/components/staking/slashing/SlashingController.sol:L267-L272  function executeSlashProposal(uint256 _proposalId) external onlyRole(SLASHER_ROLE) {  _transition(_proposalId, EXECUTED);  Proposal memory proposal = proposals[_proposalId];  slashingExecutor.slash(proposal.subjectType, proposal.subjectId, getSlashedStakeValue(_proposalId), proposal.proposer, slashPercentToProposer);  slashingExecutor.freeze(proposal.subjectType, proposal.subjectId, false);  code/contracts/components/staking/slashing/SlashingController.sol:L337-L339  function _unfreeze(uint256 _proposalId) private {  slashingExecutor.freeze(proposals[_proposalId].subjectType, proposals[_proposalId].subjectId, false);  Recommendation  Introduce a check in the unfreezing mechanics to first ensure there are no other active proposals for that subject.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"
    },
    {
        "title": "5.4 Storage gap variables slightly off from the intended size    ",
        "body": "  Resolution  The Forta Team worked on the storage layout to maintain a consistent storage buffer in the inheritance tree. The changes were made through multiple pull requests, also an easy-to-understand layout description has been added through a pull request 157. However, we still found some inconsistencies and recommend doing a thorough review of the buffer space again.  For instance, in FortaStaking (considering the latest commit)  the above-mentioned storage variables will be taking a single slot, however, separate slots are considered for the buffer space(referring to the storage layout description to determine __gap buffer).  Description  The Forta staking system is using upgradeable proxies for its deployment strategy. To avoid storage collisions between contract versions during upgrades, uint256[] private __gap array variables are introduced that create a storage buffer. Together with contract state variables, the storage slots should sum up to 50. For example, the __gap variable is present in the BaseComponentUpgradeable component, which is the base of most Forta contracts, and there is a helpful comment in AgentRegistryCore that describes how its relevant __gap variable size was calculated:  code/contracts/components/BaseComponentUpgradeable.sol:L62  uint256[50] private __gap;  code/contracts/components/agents/AgentRegistryCore.sol:L196  uint256[41] private __gap; // 50 - 1 (frontRunningDelay) - 3 (_stakeThreshold) - 5 StakeSubjectUpgradeable  However, there are a few places where the __gap size was not computed correctly to get the storage slots up to 50. Some of these are:  code/contracts/components/scanners/ScannerRegistry.sol:L234  uint256[49] private __gap;  code/contracts/components/dispatch/Dispatch.sol:L333  uint256[47] private __gap;  code/contracts/components/node_runners/NodeRunnerRegistryCore.sol:L452  uint256[44] private __gap;  While these still provide large storage buffers, it is best if the __gap variables are calculated to hold the same buffer within contracts of similar types as per the initial intentions to avoid confusion.  During conversations with the Forta Foundation team, it appears that some contracts like ScannerRegistry and AgentRegistry should instead add up to 45 with their __gap variable due to the StakeSubject contracts they inherit from adding 5 from themselves. This is something to note and be careful with as well for future upgrades.  Recommendation  Provide appropriate sizes for the __gap variables to have a consistent storage layout approach that would help avoid storage issues with future versions of the system.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"
    },
    {
        "title": "5.5 AgentRegistryCore - Agent Creation DoS    ",
        "body": "  Resolution  The Forta team as per the recommendations modified the minting logic to allow users to mint an agentId only for their own address in a pull request 155 with final hash as 7426891222e2bcdf2bbbec669905d5041f9fb58e. Also, the team claims that the Agent Ids are generated through the Forta Bot SDK to minimize the collision risk. However, this has not been verified by the auditing team.  We still recommend notifying users to check whether an ID is already registered prior to making any commitment if a front-running delay is enabled, to avoid unintended DoS.  Description  AgentRegistryCore allows anyone to mint an agentID for the desired owner address. However, in some cases, it may fall prey to DoS, either deliberately or unintentionally.  For instance, let s assume the Front Running Protection is disabled or the frontRunningDelay is 0. It means anyone can directly create an agent without any prior commitment. Thus, anyone can observe pending transactions and try to front run them to mint an agentID prior to the victim s restricting it to mint a desired agentID.  Also, it may be possible that a malicious actor succeeds in frontrunning a transaction with manipulated data/chainIDs but with the same owner address and agentID. There is a good chance that victim still accepts the attacker s transaction as valid, even though its own transaction reverted, due to the fact that the victim is still seeing itself as the owner of that ID.  Taking an instance where let s assume the frontrunning protection is enabled. Still, there is a good chance that two users vouch for the same agentIDs and commits in the same block, thus getting the same frontrunning delay. Then, it will be a game of luck, whoever creates that agent first will get the ID minted to its address, and the other user s transaction will be reverted wasting the time they have spent on the delay.  As the agentIDs can be picked by users, the chances of collisions with an already minted ID will increase over time causing unnecessary reverts for others.  Adding to the fact that there is no restriction for owner address, anyone can spam mint any agentID to any address for any profitable reason.  Examples  code/contracts/components/agents/AgentRegistryCore.sol:L68-L77  function createAgent(uint256 agentId, address owner, string calldata metadata, uint256[] calldata chainIds)  public  onlySorted(chainIds)  frontrunProtected(keccak256(abi.encodePacked(agentId, owner, metadata, chainIds)), frontRunningDelay)  _mint(owner, agentId);  _beforeAgentUpdate(agentId, metadata, chainIds);  _agentUpdate(agentId, metadata, chainIds);  _afterAgentUpdate(agentId, metadata, chainIds);  Recommendation  Modify function prepareAgent to not commit an already registered agentID.  A better approach could be to allow sequential minting of agentIDs using some counters.  Only allow users to mint an agentID, either for themselves or for someone they are approved to.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"
    },
    {
        "title": "5.6 Lack of checks for rewarding an epoch that has already been rewarded    ",
        "body": "  Resolution   The suggested recommendations have been implemented in a pull request   150 with final hash  Description  To give rewards to the participating stakers, the Forta system utilizes reward epochs for each shareId, i.e. a delegated staking share. Each epoch gets their own reward distribution, and then StakeAllocator and RewardsDistributor contracts along with the Forta staking shares determine how much the users get.  Although totalRewardsDistributed is essentially isolated to the sweep() function to allow transferring out the reward tokens without taking away those tokens reserved for the reward distribution, this still creates an inconsistency, albeit a minor one in the context of the current system.  Examples  code/contracts/components/staking/rewards/RewardsDistributor.sol:L155-L167  function reward(  uint8 subjectType,  uint256 subjectId,  uint256 amount,  uint256 epochNumber  ) external onlyRole(REWARDER_ROLE) {  if (subjectType != NODE_RUNNER_SUBJECT) revert InvalidSubjectType(subjectType);  if (!_subjectGateway.isRegistered(subjectType, subjectId)) revert RewardingNonRegisteredSubject(subjectType, subjectId);  uint256 shareId = FortaStakingUtils.subjectToActive(getDelegatorSubjectType(subjectType), subjectId);  _rewardsPerEpoch[shareId][epochNumber] = amount;  totalRewardsDistributed += amount;  emit Rewarded(subjectType, subjectId, amount, epochNumber);  Recommendation  Implement checks as appropriate to the reward() function to ensure correct behavior of totalRewardsDistributed tracking. Also, implement necessary changes to the tracking of pending rewards, if necessary.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"
    },
    {
        "title": "5.7 Reentrancy in FortaStaking during ERC1155 mints    ",
        "body": "  Resolution   The Forta team implemented a Reentrancy Guard in a pull request   151 with a final hash  Description  In the Forta staking system, the staking shares (both  active  and  inactive ) are represented as tokens implemented according to the ERC1155 standard. The specific implementation that is being used utilizes a smart contract acceptance check _doSafeTransferAcceptanceCheck() upon mints to the recipient.  code/contracts/components/staking/FortaStaking.sol:L54  contract FortaStaking is BaseComponentUpgradeable, ERC1155SupplyUpgradeable, SubjectTypeValidator, ISlashingExecutor, IStakeMigrator {  The specific implementation for ERC1155SupplyUpgradeable contracts can be found here, and the smart contract check can be found here.  This opens up reentrancy into the system s flow. In fact, the reentrancy occurs on all mints that happen in the below functions, and it happens before a call to another Forta contract for allocation is made via either _allocator.depositAllocation or _allocator.withdrawAllocation:  code/contracts/components/staking/FortaStaking.sol:L273-L295  function deposit(  uint8 subjectType,  uint256 subject,  uint256 stakeValue  ) external onlyValidSubjectType(subjectType) notAgencyType(subjectType, SubjectStakeAgency.MANAGED) returns (uint256) {  if (address(subjectGateway) == address(0)) revert ZeroAddress(\"subjectGateway\");  if (!subjectGateway.isStakeActivatedFor(subjectType, subject)) revert StakeInactiveOrSubjectNotFound();  address staker = _msgSender();  uint256 activeSharesId = FortaStakingUtils.subjectToActive(subjectType, subject);  bool reachedMax;  (stakeValue, reachedMax) = _getInboundStake(subjectType, subject, stakeValue);  if (reachedMax) {  emit MaxStakeReached(subjectType, subject);  uint256 sharesValue = stakeToActiveShares(activeSharesId, stakeValue);  SafeERC20.safeTransferFrom(stakedToken, staker, address(this), stakeValue);  _activeStake.mint(activeSharesId, stakeValue);  _mint(staker, activeSharesId, sharesValue, new bytes(0));  emit StakeDeposited(subjectType, subject, staker, stakeValue);  _allocator.depositAllocation(activeSharesId, subjectType, subject, staker, stakeValue, sharesValue);  return sharesValue;  code/contracts/components/staking/FortaStaking.sol:L303-L326  function migrate(  uint8 oldSubjectType,  uint256 oldSubject,  uint8 newSubjectType,  uint256 newSubject,  address staker  ) external onlyRole(SCANNER_2_NODE_RUNNER_MIGRATOR_ROLE) {  if (oldSubjectType != SCANNER_SUBJECT) revert InvalidSubjectType(oldSubjectType);  if (newSubjectType != NODE_RUNNER_SUBJECT) revert InvalidSubjectType(newSubjectType);  if (isFrozen(oldSubjectType, oldSubject)) revert FrozenSubject();  uint256 oldSharesId = FortaStakingUtils.subjectToActive(oldSubjectType, oldSubject);  uint256 oldShares = balanceOf(staker, oldSharesId);  uint256 stake = activeSharesToStake(oldSharesId, oldShares);  uint256 newSharesId = FortaStakingUtils.subjectToActive(newSubjectType, newSubject);  uint256 newShares = stakeToActiveShares(newSharesId, stake);  _activeStake.burn(oldSharesId, stake);  _activeStake.mint(newSharesId, stake);  _burn(staker, oldSharesId, oldShares);  _mint(staker, newSharesId, newShares, new bytes(0));  emit StakeDeposited(newSubjectType, newSubject, staker, stake);  _allocator.depositAllocation(newSharesId, newSubjectType, newSubject, staker, stake, newShares);  code/contracts/components/staking/FortaStaking.sol:L365-L387  function initiateWithdrawal(  uint8 subjectType,  uint256 subject,  uint256 sharesValue  ) external onlyValidSubjectType(subjectType) returns (uint64) {  address staker = _msgSender();  uint256 activeSharesId = FortaStakingUtils.subjectToActive(subjectType, subject);  if (balanceOf(staker, activeSharesId) == 0) revert NoActiveShares();  uint64 deadline = SafeCast.toUint64(block.timestamp) + _withdrawalDelay;  _lockingDelay[activeSharesId][staker].setDeadline(deadline);  uint256 activeShares = Math.min(sharesValue, balanceOf(staker, activeSharesId));  uint256 stakeValue = activeSharesToStake(activeSharesId, activeShares);  uint256 inactiveShares = stakeToInactiveShares(FortaStakingUtils.activeToInactive(activeSharesId), stakeValue);  SubjectStakeAgency agency = getSubjectTypeAgency(subjectType);  _activeStake.burn(activeSharesId, stakeValue);  _inactiveStake.mint(FortaStakingUtils.activeToInactive(activeSharesId), stakeValue);  _burn(staker, activeSharesId, activeShares);  _mint(staker, FortaStakingUtils.activeToInactive(activeSharesId), inactiveShares, new bytes(0));  if (agency == SubjectStakeAgency.DELEGATED || agency == SubjectStakeAgency.DELEGATOR) {  _allocator.withdrawAllocation(activeSharesId, subjectType, subject, staker, stakeValue, activeShares);  Although this doesn t seem to be an issue in the current Forta system of contracts since the allocator s logic doesn t seem to be manipulable, this could still be dangerous as it opens up an external execution flow.  Recommendation  Consider introducing a reentrancy check or emphasize this behavior in the documentation, so that both other projects using this system later and future upgrades along with maintenance work on the Forta staking system itself are implemented safely.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"
    },
    {
        "title": "5.8 Unnecessary code blocks that check the same condition    ",
        "body": "  Resolution   The code block has been refactored under a single conditional block as per the suggested recommendation in a pull request   152 with a final hash as  Description  In the RewardsDistributor there is a function that allows to set delegation fees for a NodeRunner. It adjusts the fees[] array for that node as appropriate. However, during its checks, it performs the same check twice in a row.  Examples  code/contracts/components/staking/rewards/RewardsDistributor.sol:L259-L264  if (fees[1].sinceEpoch != 0) {  if (Accumulators.getCurrentEpochNumber() < fees[1].sinceEpoch + delegationParamsEpochDelay) revert SetDelegationFeeNotReady();  if (fees[1].sinceEpoch != 0) {  fees[0] = fees[1];  Recommendation  Consider refactoring this under a single code block.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"
    },
    {
        "title": "5.9 Event spam in RewardsDistributor.claimRewards    ",
        "body": "  Resolution  Forta team has implemented the recommended check in a pull request 153, as: if (epochRewards == 0) revert ZeroAmount(\"epochRewards\");  The implemented check will now be reverting the transaction if there exists no reward for an epoch number. However, it may not be a gas-efficient approach for the user claiming rewards and accidentally passing an incorrect epoch number. A better approach could be to transfer any reward and emit any event only for a non-zero epochReward.  Description  The RewardsDistributor contract allows users to claim their rewards through the claimRewards() function. It does check to see whether or not the user has already claimed the rewards for a specific epoch that they are claiming for, but it does not check to see if the user has any associated rewards at all. This could lead to event ClaimedRewards being spammed by malicious users, especially on low gas chains.  Examples  code/contracts/components/staking/rewards/RewardsDistributor.sol:L224-L229  for (uint256 i = 0; i < epochNumbers.length; i++) {  if (_claimedRewardsPerEpoch[shareId][epochNumbers[i]][_msgSender()]) revert AlreadyClaimed();  _claimedRewardsPerEpoch[shareId][epochNumbers[i]][_msgSender()] = true;  uint256 epochRewards = _availableReward(shareId, isDelegator, epochNumbers[i], _msgSender());  SafeERC20.safeTransfer(rewardsToken, _msgSender(), epochRewards);  emit ClaimedRewards(subjectType, subjectId, _msgSender(), epochNumbers[i], epochRewards);  Recommendation  Add a check for rewards amounts being greater than 0.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"
    },
    {
        "title": "5.10 SubjectTypes.sol files unused    ",
        "body": "  Resolution   The unused file has now been removed in commit   2548e0a4f7b38926362a759f4fa0611394348d6e  Description  There is a rogue file SubjectTypes.sol that is not being utilized. It appears that its intended functionality is being done by the SubjectTypeValidator.sol file as it even has a contract with the same name implemented there.  Examples  code/contracts/components/staking/SubjectTypes.sol:L4-L10  pragma solidity ^0.8.9;  uint8 constant SCANNER_SUBJECT = 0;  uint8 constant AGENT_SUBJECT = 1;  uint8 constant NODE_RUNNER_SUBJECT = 3;  contract SubjectTypeValidator {  Recommendation  Remove the SubjectTypes.sol file.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"
    },
    {
        "title": "5.11 Lack of a check for the subject s stake for reviewSlashProposalParameters    ",
        "body": "  Resolution   The recommended check has now been added in a pull request   154 with final hash as  Description  While it may be assumed that the review function will be called by a privileged and knowledgeable actor, this additional check may avoid accidental mistakes.  Examples  code/contracts/components/staking/slashing/SlashingController.sol:L153  if (subjectGateway.totalStakeFor(_subjectType, _subjectId) == 0) revert ZeroAmount(\"subject stake\");  code/contracts/components/staking/slashing/SlashingController.sol:L226-L229  if (_subjectType != proposals[_proposalId].subjectType || _subjectId != proposals[_proposalId].subjectId) {  _unfreeze(_proposalId);  _freeze(_subjectType, _subjectId);  Recommendation  Add a check for the new subject having stake to slash.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"
    },
    {
        "title": "5.12 Comment and code inconsistencies    ",
        "body": "  Resolution   The comments have now been found fixed as per the implemented logic, primarily in the pull request   156 and in another commit with hash  f4ee799ee192084965643b09b69f3cbeababd5ae  Description  During the audit a few inconsistencies were found between what the comments say and what the implemented code actually did.  Examples  Subject Type Agency for Scanner Subjects  In the SubjectTypeValidator, the comment says that the SCANNER_SUBJECT is of type DIRECT agency type, i.e. it can be directly staked on by multiple different stakers. However, we found a difference in the implementation, where the concerned subject is defined as type MANAGED agency type, which says that it cannot be staked on directly; instead it s a delegated type and the allocation is supposed to be managed by its manager.  code/contracts/components/staking/SubjectTypeValidator.sol:L21  code/contracts/components/staking/SubjectTypeValidator.sol:L66-L67  - SCANNER_SUBJECT --> DIRECT  code/contracts/components/staking/SubjectTypeValidator.sol:L66-L67  } else if (subjectType == SCANNER_SUBJECT) {  return SubjectStakeAgency.MANAGED;  Dispatch refers to ERC721 tokens as ERC1155  One of the comments describing the functionality to link and unlink agents and scanners refers to them as ERC1155 tokens, when in reality they are ERC721.  code/contracts/components/dispatch/Dispatch.sol:L179-L185  /**  @notice Assigns the job of running an agent to a scanner.  @dev currently only allowed for DISPATCHER_ROLE (Assigner software).  @dev emits Link(agentId, scannerId, true) event.  @param agentId ERC1155 token id of the agent.  @param scannerId ERC1155 token id of the scanner.  /  NodeRunnerRegistryCore comment that implies the reverse of what happens  A comment describing a helper function that returns address for a given scanner ID describes the opposite behavior. It is the same comment for the function just above that actually does what the comment says.  code/contracts/components/node_runners/NodeRunnerRegistryCore.sol:L259-L262  /// Converts scanner address to uint256 for FortaStaking Token Id.  function scannerIdToAddress(uint256 scannerId) public pure returns (address) {  return address(uint160(scannerId));  ScannerToNodeRunnerMigration comment that says that no NodeRunner tokens must be owned  For the migration from Scanners to NodeRunners, a comment in the beginning of the file implies that for the system to work correctly, there must be no NodeRunner tokens owned prior to migration. After a conversation with the Forta Foundation team, it appears that this was an early design choice that is no longer relevant.  code/contracts/components/scanners/ScannerToNodeRunnerMigration.sol:L69  code/contracts/components/scanners/ScannerToNodeRunnerMigration.sol:L91  @param nodeRunnerId If set as 0, a new NodeRunnerRegistry ERC721 will be minted to nodeRunner (but it must not own any prior),  code/contracts/components/scanners/ScannerToNodeRunnerMigration.sol:L91  Recommendation  @param nodeRunnerId If set as 0, a new NodeRunnerRegistry ERC721 will be minted to nodeRunner (but it must not own any prior),  Recommendation  Verify the operational logic and fix either the concerned comments or defined logic as per the need.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"
    },
    {
        "title": "5.1 zNS - Domain bid might be approved by non owner account    ",
        "body": "  Resolution  Addressed with zer0-os/zNS@ab7d62a by storing the domain request data on-chain.  Description  The spec allows anyone to place a bid for a domain, while only parent domain owners are allowed to approve a bid. Bid placement is actually enforced and purely informational. In practice, approveDomainBid allows any parent domain owner to approve bids (signatures) for any other domain even if they do not own it. Once approved, anyone can call fulfillDomainBid to create a domain.  Examples  zNS/contracts/StakingController.sol:L95-L103  function approveDomainBid(  uint256 parentId,  string memory bidIPFSHash,  bytes memory signature  ) external authorizedOwner(parentId) {  bytes32 hashOfSig = keccak256(abi.encode(signature));  approvedBids[hashOfSig] = true;  emit DomainBidApproved(bidIPFSHash);  Recommendation  Consider adding a validation check that allows only the parent domain owner to approve bids on one of its domains. Reconsider the design of the system introducing more on-chain guarantees for bids.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"
    },
    {
        "title": "5.2 zAuction, zNS - Bids cannot be cancelled, never expire, and the auction lifecycle is unclear    ",
        "body": "  Resolution  Addressed with zer0-os/zNS@ab7d62a by refactoring the StakingController to control the lifecycle of bids instead of handling this off-chain.  Addressed with zer0-os/zAuction@135b2aa for zAuction by adding a bid/saleOffer expiration for bids. The client also provided the following statement:  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"
    },
    {
        "title": "5.6 added expireblock and startblock to zauction, expireblock to zsale",
        "body": " Decided not to add a cancel function. Paying gas to cancel isn t ideal, and it can be used as a griefing function. though that s still possible to do by moving weth but differently  The stateless nature of auctions may make it hard to enforce bid/sale expirations and it is not possible to cancel a bid/offer that should not be valid anymore. The expiration reduces the risk of old offers being used as they now automatically invalidate after time, however, it is still likely that multiple valid offers may be present at the same time. As outlined in the recommendation, one option would be to allow someone who signed a commitment to explicitly cancel it in the contract. Another option would be to create a stateful auction where the entity that puts up something for  starts  an auction, creating an auction id, requiring bidders to bid on that auction id. Once a bid is accepted the auction id is invalidated which invalidates all bids that might be floating around.  zer0-os/zAuction@2f92aa1 for  Description  The lifecycle of a bid both for zAuction and zNS is not clear, and has many flaws.  zAuction - Consider the case where a bid is placed, then the underlying asset in being transferred to a new owner. The new owner can now force to sell the asset even though it s might not be relevant anymore.  zAuction - Once a bid was accepted and the asset was transferred, all other bids need to be invalidated automatically, otherwise and old bid might be accepted even after the formal auction is over.  zAuction, zNS - There is no way for the bidder to cancel an old bid. That might be useful in the event of a significant change in market trend, where the old pricing is no longer relevant. Currently, in order to cancel a bid, the bidder can either withdraw his ether balance from the zAuctionAccountant, or disapprove WETH which requires an extra transaction that might be front-runned by the seller.  Examples  zAuction/contracts/zAuction.sol:L35-L45  function acceptBid(bytes memory signature, uint256 rand, address bidder, uint256 bid, address nftaddress, uint256 tokenid) external {  address recoveredbidder = recover(toEthSignedMessageHash(keccak256(abi.encode(rand, address(this), block.chainid, bid, nftaddress, tokenid))), signature);  require(bidder == recoveredbidder, 'zAuction: incorrect bidder');  require(!randUsed[rand], 'Random nonce already used');  randUsed[rand] = true;  IERC721 nftcontract = IERC721(nftaddress);  accountant.Exchange(bidder, msg.sender, bid);  nftcontract.transferFrom(msg.sender, bidder, tokenid);  emit BidAccepted(bidder, msg.sender, bid, nftaddress, tokenid);  zNS/contracts/StakingController.sol:L120-L152  function fulfillDomainBid(  uint256 parentId,  uint256 bidAmount,  uint256 royaltyAmount,  string memory bidIPFSHash,  string memory name,  string memory metadata,  bytes memory signature,  bool lockOnCreation,  address recipient  ) external {  bytes32 recoveredBidHash = createBid(parentId, bidAmount, bidIPFSHash, name);  address recoveredBidder = recover(recoveredBidHash, signature);  require(recipient == recoveredBidder, \"ZNS: bid info doesnt match/exist\");  bytes32 hashOfSig = keccak256(abi.encode(signature));  require(approvedBids[hashOfSig] == true, \"ZNS: has been fullfilled\");  infinity.safeTransferFrom(recoveredBidder, controller, bidAmount);  uint256 id = registrar.registerDomain(parentId, name, controller, recoveredBidder);  registrar.setDomainMetadataUri(id, metadata);  registrar.setDomainRoyaltyAmount(id, royaltyAmount);  registrar.transferFrom(controller, recoveredBidder, id);  if (lockOnCreation) {  registrar.lockDomainMetadataForOwner(id);  approvedBids[hashOfSig] = false;  emit DomainBidFulfilled(  metadata,  name,  recoveredBidder,  id,  parentId  );  Recommendation  Consider adding an expiration field to the message signed by the bidder both for zAuction and zNS. Consider adding auction control, creating an auctionId, and have users bid on specific auctions. By adding this id to the signed message, all other bids are invalidated automatically and users would have to place new bids for a new auction. Optionally allow users to cancel bids explicitly.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"
    },
    {
        "title": "5.3 zNS - Insufficient protection against replay attacks    ",
        "body": "  Resolution  Addressed with zer0-os/zNS@ab7d62a by avoiding the use of digital signatures and storing the domain request data on-chain.  Description  There is no dedicated data structure to prevent replay attacks on StakingController. approvedBids mapping offers only partial mitigation, due to the fact that after a domain bid is fulfilled, the only mechanism in place to prevent a replay attack is the Registrar contract that might be replaced in the case where StakingController is being re-deployed with a different Registrar instance. Additionally, the digital signature used for domain bids does not identify the buyer request uniquely enough. The bidder s signature could be replayed in future similar contracts that are deployed with a different registrar or in a different network.  Examples  zNS/contracts/StakingController.sol:L176-L183  function createBid(  uint256 parentId,  uint256 bidAmount,  string memory bidIPFSHash,  string memory name  ) public pure returns(bytes32) {  return keccak256(abi.encode(parentId, bidAmount, bidIPFSHash, name));  Recommendation  Consider adding a dedicated mapping to store the a unique identifier of a bid, as well as adding address(this), block.chainId, registrar and nonce to the message that is being signed by the bidder.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"
    },
    {
        "title": "5.4 zNS - domain name collisions    ",
        "body": "  Resolution   Addressed with   zer0-os/ZNS@ab7d62a by disallowing empty names for domain registrations. The name validation in off-chain components (e.g. subgraph components) has not been verified.  Description  Domain registration accepts an empty (zero-length) name. This may allow a malicious entity to register two different NFT s for the same visually indinstinguishable text representation of a domain. Similar to this the domain name is mapped to an NFT via a subgraph that connects parent names to the new subdomain using a domain separation character (dot/slash/\u2026). Someone might be able to register a.b to cats.cool which might resolve to the same domain as if someone registers cats.cool.a and then cats.cool.a.b.  Examples  0/cats/ = 0xfe  0/cats/<empty-string/ = 0xfe.keccak(\"\")  zNS/contracts/Registrar.sol:L76-L96  function registerDomain(  uint256 parentId,  string memory name,  address domainOwner,  address minter  ) external override onlyController returns (uint256) {  // Create the child domain under the parent domain  uint256 labelHash = uint256(keccak256(bytes(name)));  address controller = msg.sender;  // Domain parents must exist  require(_exists(parentId), \"Zer0 Registrar: No parent\");  // Calculate the new domain's id and create it  uint256 domainId =  uint256(keccak256(abi.encodePacked(parentId, labelHash)));  _createDomain(domainId, domainOwner, minter, controller);  emit DomainCreated(domainId, name, labelHash, parentId, minter, controller);  return domainId;  Recommendation  Disallow empty subdomain names. Disallow domain separators in names (in the offchain component or smart contract).  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"
    },
    {
        "title": "5.5 zAuction, zNS - gas griefing by spamming offchain fake bids   ",
        "body": "  Resolution  Addressed and acknowledged with changes from zer0-os/zAuction@135b2aa. The client provided the following remark:  ",
        "labels": [
            "Consensys",
            "Medium",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"
    },
    {
        "title": "5.19 I have attempted to order the requires sensibly, putting the least expensive first. Please advise if the ordering is optimal. gas griefing will be mitigated in the dapp with off-client checks",
        "body": "  Description  The execution status of both zAuction.acceptBid and StakingController.fulfillDomainBid transactions depend on the bidder, as his approval is needed, his signature is being validated, etc. However, these transactions can be submitted by accounts that are different from the bidder account, or for accounts that do not have the required funds/deposits available, luring the account that has to perform the on-chain call into spending gas on a transaction that is deemed to fail (gas griefing). E.g. posting high-value fake bids for zAuction without having funds deposited or WETH approved.  Examples  zNS/contracts/StakingController.sol:L120-L152  function fulfillDomainBid(  uint256 parentId,  uint256 bidAmount,  uint256 royaltyAmount,  string memory bidIPFSHash,  string memory name,  string memory metadata,  bytes memory signature,  bool lockOnCreation,  address recipient  ) external {  bytes32 recoveredBidHash = createBid(parentId, bidAmount, bidIPFSHash, name);  address recoveredBidder = recover(recoveredBidHash, signature);  require(recipient == recoveredBidder, \"ZNS: bid info doesnt match/exist\");  bytes32 hashOfSig = keccak256(abi.encode(signature));  require(approvedBids[hashOfSig] == true, \"ZNS: has been fullfilled\");  infinity.safeTransferFrom(recoveredBidder, controller, bidAmount);  uint256 id = registrar.registerDomain(parentId, name, controller, recoveredBidder);  registrar.setDomainMetadataUri(id, metadata);  registrar.setDomainRoyaltyAmount(id, royaltyAmount);  registrar.transferFrom(controller, recoveredBidder, id);  if (lockOnCreation) {  registrar.lockDomainMetadataForOwner(id);  approvedBids[hashOfSig] = false;  emit DomainBidFulfilled(  metadata,  name,  recoveredBidder,  id,  parentId  );  zAuction/contracts/zAuction.sol:L35-L44  function acceptBid(bytes memory signature, uint256 rand, address bidder, uint256 bid, address nftaddress, uint256 tokenid) external {  address recoveredbidder = recover(toEthSignedMessageHash(keccak256(abi.encode(rand, address(this), block.chainid, bid, nftaddress, tokenid))), signature);  require(bidder == recoveredbidder, 'zAuction: incorrect bidder');  require(!randUsed[rand], 'Random nonce already used');  randUsed[rand] = true;  IERC721 nftcontract = IERC721(nftaddress);  accountant.Exchange(bidder, msg.sender, bid);  nftcontract.transferFrom(msg.sender, bidder, tokenid);  emit BidAccepted(bidder, msg.sender, bid, nftaddress, tokenid);  Recommendation  Revert early for checks that depend on the bidder before performing gas-intensive computations.  Consider adding a dry-run validation for off-chain components before transaction submission.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"
    },
    {
        "title": "5.6 zNS - anyone can front-run fulfillDomainBid to lock the domain setting or set different metadata    ",
        "body": "  Resolution   Addressed with   zer0-os/ZNS@ab7d62a by restricting the method to only be callable by the requester.  Description  Anyone observing a call to fulfillDomainBid can front-run this call for the original bidder, provide different metadata/royalty amount, or lock the metadata, as these parameters are not part of the bidder s signature. The impact is limited as both metadata, royalty amount, and lock state can be changed by the domain owner after creation.  Examples  zNS/contracts/StakingController.sol:L120-L143  function fulfillDomainBid(  uint256 parentId,  uint256 bidAmount,  uint256 royaltyAmount,  string memory bidIPFSHash,  string memory name,  string memory metadata,  bytes memory signature,  bool lockOnCreation,  address recipient  ) external {  bytes32 recoveredBidHash = createBid(parentId, bidAmount, bidIPFSHash, name);  address recoveredBidder = recover(recoveredBidHash, signature);  require(recipient == recoveredBidder, \"ZNS: bid info doesnt match/exist\");  bytes32 hashOfSig = keccak256(abi.encode(signature));  require(approvedBids[hashOfSig] == true, \"ZNS: has been fullfilled\");  infinity.safeTransferFrom(recoveredBidder, controller, bidAmount);  uint256 id = registrar.registerDomain(parentId, name, controller, recoveredBidder);  registrar.setDomainMetadataUri(id, metadata);  registrar.setDomainRoyaltyAmount(id, royaltyAmount);  registrar.transferFrom(controller, recoveredBidder, id);  if (lockOnCreation) {  registrar.lockDomainMetadataForOwner(id);  Recommendation  Consider adding metadata, royaltyAmount, and lockOnCreation to the message signed by the bidder if the parent should have some control over metadata and lockstatus and restrict access to this function to msg.sender==recoveredbidder.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"
    },
    {
        "title": "5.7 zNS- Using a digital signature as a hash preimage    ",
        "body": "  Resolution  Addressed with zer0-os/zNS@ab7d62a by avoiding the use of digital signatures.  Description  Using the encoded signature (r,s,v) or the hash of the signature to prevent replay or track if signatures have been seen/used is not recommended in general, as it may introduce signature malleability issues, as two different signature params (r,s,v) may be producable that validly sign the same data.  The impact for this codebase, however, is limited, due to the fact that openzeppelins ECDSA wrapper library is used which checks for malleable ECDSA signatures (high s value). We still decided to keep this as a medium issue to raise awareness, that it is bad practice to rely on the hash of signatures instead of the hash of the actual signed data for checks.  In another instance in zAuction, a global random nonce is used to prevent replay attacks. This is suboptimal and instead, the hash of the signed data (including a nonce) should be used.  Examples  zNS/contracts/StakingController.sol:L120-L152  function fulfillDomainBid(  uint256 parentId,  uint256 bidAmount,  uint256 royaltyAmount,  string memory bidIPFSHash,  string memory name,  string memory metadata,  bytes memory signature,  bool lockOnCreation,  address recipient  ) external {  bytes32 recoveredBidHash = createBid(parentId, bidAmount, bidIPFSHash, name);  address recoveredBidder = recover(recoveredBidHash, signature);  require(recipient == recoveredBidder, \"ZNS: bid info doesnt match/exist\");  bytes32 hashOfSig = keccak256(abi.encode(signature));  require(approvedBids[hashOfSig] == true, \"ZNS: has been fullfilled\");  infinity.safeTransferFrom(recoveredBidder, controller, bidAmount);  uint256 id = registrar.registerDomain(parentId, name, controller, recoveredBidder);  registrar.setDomainMetadataUri(id, metadata);  registrar.setDomainRoyaltyAmount(id, royaltyAmount);  registrar.transferFrom(controller, recoveredBidder, id);  if (lockOnCreation) {  registrar.lockDomainMetadataForOwner(id);  approvedBids[hashOfSig] = false;  emit DomainBidFulfilled(  metadata,  name,  recoveredBidder,  id,  parentId  );  zAuction/contracts/zAuction.sol:L35-L39  function acceptBid(bytes memory signature, uint256 rand, address bidder, uint256 bid, address nftaddress, uint256 tokenid) external {  address recoveredbidder = recover(toEthSignedMessageHash(keccak256(abi.encode(rand, address(this), block.chainid, bid, nftaddress, tokenid))), signature);  require(bidder == recoveredbidder, 'zAuction: incorrect bidder');  require(!randUsed[rand], 'Random nonce already used');  randUsed[rand] = true;  Recommendation  Consider creating the bid identifier by hashing the concatenation of all bid parameters instead. Ensure to add replay protection https://github.com/ConsenSys/zer0-zns-audit-2021-05/issues/19. Always check for the hash of the signed data instead of the hash of the encoded signature to track whether a signature has been seen before.  Consider implementing Ethereum typed structured data hashing and signing according to EIP-712.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"
    },
    {
        "title": "5.8 zNS - Registrar skips __ERC721Pausable_init()    ",
        "body": "  Resolution   Addressed with   zer0-os/ZNS@ab7d62a  Description  The initialization function of registrar skips the chained initializer __ERC721Pausable_init to initialize __ERC721_init(\"Zer0 Name Service\", \"ZNS\"). This basically skips the following initialization calls:  abstract contract ERC721PausableUpgradeable is Initializable, ERC721Upgradeable, PausableUpgradeable {  function __ERC721Pausable_init() internal initializer {  __Context_init_unchained();  __ERC165_init_unchained();  __Pausable_init_unchained();  __ERC721Pausable_init_unchained();  Examples  zNS/contracts/Registrar.sol:L39-L45  function initialize() public initializer {  __Ownable_init();  __ERC721_init(\"Zer0 Name Service\", \"ZNS\");  // create the root domain  _createDomain(0, msg.sender, msg.sender, address(0));  Recommendation  consider calling the missing initializers to register the interface for erc165 if needed.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"
    },
    {
        "title": "5.9 zNS - Registrar is ERC721PausableUpgradeable but there is no way to actually pause it    ",
        "body": "  Resolution   Addressed with   zer0-os/ZNS@ab7d62a by exposing pausable functionality to the contract owner.  Description  The registrar is ownable and pausable but the functionality to pause the contract is not implemented.  zNS/contracts/Registrar.sol:L8-L12  contract Registrar is  IRegistrar,  OwnableUpgradeable,  ERC721PausableUpgradeable  Recommendation  Simplification is key. Remove the pausable functionality if the contract is not meant to be paused or consider implementing an external pause() function decorated onlyOwner.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"
    },
    {
        "title": "5.10 zNS - Avoid no-ops    ",
        "body": "  Resolution   Addressed with   zer0-os/ZNS@ab7d62a.  Description  Code paths that are causing transactions to be ended with an ineffective outcome or no-operation (no actual state changes) are not advisable, as they consume more gas, hide misconfiguration or error cases (e.g. adding the same controller multiple times), and may impact other processes that rely upon transaction s logs.  Examples  Reject adding an already existing controller, and removing non existing controller.  zNS/contracts/Registrar.sol:L51-L67  /**  @notice Authorizes a controller to control the registrar  @param controller The address of the controller  /  function addController(address controller) external override onlyOwner {  controllers[controller] = true;  emit ControllerAdded(controller);  /**  @notice Unauthorizes a controller to control the registrar  @param controller The address of the controller  /  function removeController(address controller) external override onlyOwner {  controllers[controller] = false;  emit ControllerRemoved(controller);  Recommendation  Consider reverting code paths that end up in ineffective outcomes (i.e. no-operation) as early as possible.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"
    },
    {
        "title": "5.1 RocketNodeDistributorDelegate - Reentrancy in distribute() allows node owner to drain distributor funds    ",
        "body": "  Resolution  Fixed in https://github.com/rocket-pool/rocketpool/tree/77d7cca65b7c0557cfda078a4fc45f9ac0cc6cc6 by implementing a custom reentrancy guard via a new state variable lock that is appended to the end of the storage layout. The reentrancy guard is functionally equivalent to the OpenZeppelin implementation. The method was not refactored to give user funds priority over the node share. Additionally, the client provided the following statement:  We acknowledge this as a critical issue and have solved with a reentrancy guard.  We followed OpenZeppelin s design for a reentrancy guard. We were unable to use it directly as it is hardcoded to use storage slot 0 and because we already have deployment of this delegate in the wild already using storage slot 0 for another purpose, we had to append it to the end of the existing storage layout.  Description  The distribute() function distributes the contract s balance between the node operator and the user. The node operator is returned their initial collateral, including a fee. The rest is returned to the RETH token contract as user collateral.  After determining the node owner s share, the contract transfers ETH to the node withdrawal address, which can be the configured withdrawal address or the node address. Both addresses may potentially be a malicious contract that recursively calls back into the distribute() function to retrieve the node share multiple times until all funds are drained from the contract. The distribute() function is not protected against reentrancy:  code/contracts/contract/node/RocketNodeDistributorDelegate.sol:L59-L73  /// @notice Distributes the balance of this contract to its owners  function distribute() override external {  // Calculate node share  uint256 nodeShare = getNodeShare();  // Transfer node share  address withdrawalAddress = rocketStorage.getNodeWithdrawalAddress(nodeAddress);  (bool success,) = withdrawalAddress.call{value : nodeShare}(\"\");  require(success);  // Transfer user share  uint256 userShare = address(this).balance;  address rocketTokenRETH = rocketStorage.getAddress(rocketTokenRETHKey);  payable(rocketTokenRETH).transfer(userShare);  // Emit event  emit FeesDistributed(nodeAddress, userShare, nodeShare, block.timestamp);  We also noticed that any address could set a withdrawal address as there is no check for the caller to be a registered node. In fact, the caller can be the withdrawal address or node operator.  code/contracts/contract/RocketStorage.sol:L118-L133  // Set a node's withdrawal address  function setWithdrawalAddress(address _nodeAddress, address _newWithdrawalAddress, bool _confirm) external override {  // Check new withdrawal address  require(_newWithdrawalAddress != address(0x0), \"Invalid withdrawal address\");  // Confirm the transaction is from the node's current withdrawal address  address withdrawalAddress = getNodeWithdrawalAddress(_nodeAddress);  require(withdrawalAddress == msg.sender, \"Only a tx from a node's withdrawal address can update it\");  // Update immediately if confirmed  if (_confirm) {  updateWithdrawalAddress(_nodeAddress, _newWithdrawalAddress);  // Set pending withdrawal address if not confirmed  else {  pendingWithdrawalAddresses[_nodeAddress] = _newWithdrawalAddress;  Recommendation  Add a reentrancy guard to functions that interact with untrusted contracts. Adhere to the checks-effects pattern and send user funds to the  trusted  RETH contract first. Only then send funds to the node s withdrawal address.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"
    },
    {
        "title": "5.2 RocketMinipoolDelegateOld - Node operator may reenter finalise() to manipulate accounting    ",
        "body": "  Resolution  The client acknowledges the finding and provided the following statement:  We are aware of this issue (it was reported via our Immunefi bug bounty). It is live but that code path is inaccessible. It requires the oDAO to mark a minipool as Withdrawable which we don t do and have removed from the withdrawal process moving forward.  In a later revision, the development team fixed the issue in the following commit: 73d5792a671db5d2f4dcbd35737e729f9e01aa11  Description  node.minipools.finalised.count<NodeAddress>: NodeAddress finalised count increased twice instead  minipools.finalised.count: global finalised count increased twice  eth.matched.node.amount<NodeAddress> - NodeAddress eth matched amount potentially reduced too many times; has an impact on getNodeETHCollateralisationRatio -> GetNodeShare, getNodeETHProvided -> getNodeEffectiveRPLStake and getNodeETHProvided->getNodeMaximumRPLStake->withdrawRPL and is the limiting factor when withdrawing RPL to ensure the pools stay collateralized.  Note: RocketMinipoolDelegateOld is assumed to be the currently deployed MiniPool implementation. Users may upgrade from this delegate to the new version and can roll back at any time and re-upgrade, even within the same transaction (see issue 5.3 ).  The following is an annotated call stack from a node operator calling minipool.finalise() reentering finalise() once more on their Minipool:  code/contracts/contract/old/minipool/RocketMinipoolDelegateOld.sol:L182-L191  // Called by node operator to finalise the pool and unlock their RPL stake  function finalise() external override onlyInitialised onlyMinipoolOwnerOrWithdrawalAddress(msg.sender) {  // Can only call if withdrawable and can only be called once  require(status == MinipoolStatus.Withdrawable, \"Minipool must be withdrawable\");  // Node operator cannot finalise the pool unless distributeBalance has been called  require(withdrawalBlock > 0, \"Minipool balance must have been distributed at least once\");  // Finalise the pool  _finalise();  _refund() handing over control flow to nodeWithdrawalAddress  code/contracts/contract/old/minipool/RocketMinipoolDelegateOld.sol:L311-L341  // Perform any slashings, refunds, and unlock NO's stake  function _finalise() private {  // Get contracts  RocketMinipoolManagerInterface rocketMinipoolManager = RocketMinipoolManagerInterface(getContractAddress(\"rocketMinipoolManager\"));  // Can only finalise the pool once  require(!finalised, \"Minipool has already been finalised\");  // If slash is required then perform it  if (nodeSlashBalance > 0) {  _slash();  // Refund node operator if required  if (nodeRefundBalance > 0) {  _refund();  // Send any left over ETH to rETH contract  if (address(this).balance > 0) {  // Send user amount to rETH contract  payable(rocketTokenRETH).transfer(address(this).balance);  // Trigger a deposit of excess collateral from rETH contract to deposit pool  RocketTokenRETHInterface(rocketTokenRETH).depositExcessCollateral();  // Unlock node operator's RPL  rocketMinipoolManager.incrementNodeFinalisedMinipoolCount(nodeAddress);  // Update unbonded validator count if minipool is unbonded  if (depositType == MinipoolDeposit.Empty) {  RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  rocketDAONodeTrusted.decrementMemberUnbondedValidatorCount(nodeAddress);  // Set finalised flag  finalised = true;  code/contracts/contract/old/minipool/RocketMinipoolDelegateOld.sol:L517-L528  function _refund() private {  // Update refund balance  uint256 refundAmount = nodeRefundBalance;  nodeRefundBalance = 0;  // Get node withdrawal address  address nodeWithdrawalAddress = rocketStorage.getNodeWithdrawalAddress(nodeAddress);  // Transfer refund amount  (bool success,) = nodeWithdrawalAddress.call{value : refundAmount}(\"\");  require(success, \"ETH refund amount was not successfully transferred to node operator\");  // Emit ether withdrawn event  emit EtherWithdrawn(nodeWithdrawalAddress, refundAmount, block.timestamp);  Methods adjusting system settings called twice:  code/contracts/contract/old/minipool/RocketMinipoolManagerOld.sol:L265-L272  // Increments _nodeAddress' number of minipools that have been finalised  function incrementNodeFinalisedMinipoolCount(address _nodeAddress) override external onlyLatestContract(\"rocketMinipoolManager\", address(this)) onlyRegisteredMinipool(msg.sender) {  // Update the node specific count  addUint(keccak256(abi.encodePacked(\"node.minipools.finalised.count\", _nodeAddress)), 1);  // Update the total count  addUint(keccak256(bytes(\"minipools.finalised.count\")), 1);  code/contracts/contract/dao/node/RocketDAONodeTrusted.sol:L139-L142  function decrementMemberUnbondedValidatorCount(address _nodeAddress) override external onlyLatestContract(\"rocketDAONodeTrusted\", address(this)) onlyRegisteredMinipool(msg.sender) {  subUint(keccak256(abi.encodePacked(daoNameSpace, \"member.validator.unbonded.count\", _nodeAddress)), 1);  Recommendation  We recommend setting the finalised = true flag immediately after checking for it. Additionally, the function flow should adhere to the checks-effects-interactions pattern whenever possible. We recommend adding generic reentrancy protection whenever the control flow is handed to an untrusted entity.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"
    },
    {
        "title": "5.3 RocketMinipoolDelegate - Sandwiching of Minipool calls can have unintended side effects ",
        "body": "  Resolution  The client provided the following statement:  The slashed value is purely for NO informational purposes and not used in any logic in the contracts so this example is benign as you say. We have fixed this particular issue by moving the slashed boolean out of the delegate and into RocketMinipooLManager. It is now set on any call to rocketNodeStaking.slashRPL which covers both old delegate and new.  We appreciate that the finding was more a classification of potential issues with upgrades and rollbacks. At this stage, we cannot change this functionality as it is already deployed in a non-upgradable way to over 12,000 contracts.  As this is more of a guidance and there is no immediate threat, we don t believe this should be considered a  major  finding.  With https://github.com/rocket-pool/rocketpool/tree/77d7cca65b7c0557cfda078a4fc45f9ac0cc6cc6 the slashed flag was moved to RocketNodeStaking.slashRPL() (minipool.rpl.slashed|<msg.sender> = true).  The audit team acknowledges that this issue does not provide a concrete exploit that puts funds at risk. However, due to the sensitive nature and potential for issues regarding future updates, we stand by the initial severity rating as it stands for security vulnerabilities that may not be directly exploitable or require certain conditions to be exploited.  Description  The RocketMinipoolBase contract exposes the functions delegateUpgrade and delegateRollback, allowing the minipool owner to switch between delegate implementations. While giving the minipool owner a chance to roll back potentially malfunctioning upgrades, the fact that upgrades and rollback are instantaneous also gives them a chance to alternate between executing old and new code (e.g. by utilizing callbacks) and sandwich user calls to the minipool.  Examples  Assuming the latest minipool delegate implementation, any user can call RocketMinipoolDelegate.slash, which slashes the node operator s RPL balance if a slashing has been recorded on their validator. To mark the minipool as having been slashed, the slashed contract variable is set to true. A minipool owner can avoid this flag from being set By sandwiching the user calls:  Minipool owner rolls back to the old implementation from RocketMinipoolDelegateOld.sol  User calls slash on the now old delegate implementation (where slashed is not set)  Minipool owner upgrades to the latest delegate implementation again  In detail, the new slash implementation:  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L687-L696  function _slash() private {  // Get contracts  RocketNodeStakingInterface rocketNodeStaking = RocketNodeStakingInterface(getContractAddress(\"rocketNodeStaking\"));  // Slash required amount and reset storage value  uint256 slashAmount = nodeSlashBalance;  nodeSlashBalance = 0;  rocketNodeStaking.slashRPL(nodeAddress, slashAmount);  // Record slashing  slashed = true;  Compared to the old slash implementation:  code/contracts/contract/old/minipool/RocketMinipoolDelegateOld.sol:L531-L539  function _slash() private {  // Get contracts  RocketNodeStakingInterface rocketNodeStaking = RocketNodeStakingInterface(getContractAddress(\"rocketNodeStaking\"));  // Slash required amount and reset storage value  uint256 slashAmount = nodeSlashBalance;  nodeSlashBalance = 0;  rocketNodeStaking.slashRPL(nodeAddress, slashAmount);  While the bypass of slashed being set is a benign example, the effects of this issue, in general, could result in a significant disruption of minipool operations and potentially affect the system s funds. The impact highly depends on the changes introduced by future minipool upgrades.  Recommendation  We recommend limiting upgrades and rollbacks to prevent minipool owners from switching implementations with an immediate effect. A time lock can fulfill this purpose when a minipool owner announces an upgrade to be done at a specific block. A warning can precede user-made calls that an upgrade is pending, and their interaction can have unintended side effects.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"
    },
    {
        "title": "5.4 RocketDAONodeTrustedActions - No way to access ETH provided by non-member votes   ",
        "body": "  Resolution  According to the client, this is the intended behavior. The client provided the following statement:  This is by design.  Description  DAO members can challenge nodes to prove liveliness for free. Non-DAO members must provide members.challenge.cost = 1 eth to start a challenge. However, the provided challenge cost is locked within the contract instead of being returned or recycled as system collateral.  Examples  code/contracts/contract/dao/node/RocketDAONodeTrustedActions.sol:L181-L192  // In the event that the majority/all of members go offline permanently and no more proposals could be passed, a current member or a regular node can 'challenge' a DAO members node to respond  // If it does not respond in the given window, it can be removed as a member. The one who removes the member after the challenge isn't met, must be another node other than the proposer to provide some oversight  // This should only be used in an emergency situation to recover the DAO. Members that need removing when consensus is still viable, should be done via the 'kick' method.  function actionChallengeMake(address _nodeAddress) override external onlyTrustedNode(_nodeAddress) onlyRegisteredNode(msg.sender) onlyLatestContract(\"rocketDAONodeTrustedActions\", address(this)) payable {  // Load contracts  RocketDAONodeTrustedInterface rocketDAONode = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  RocketDAONodeTrustedSettingsMembersInterface rocketDAONodeTrustedSettingsMembers = RocketDAONodeTrustedSettingsMembersInterface(getContractAddress(\"rocketDAONodeTrustedSettingsMembers\"));  // Members can challenge other members for free, but for a regular bonded node to challenge a DAO member, requires non-refundable payment to prevent spamming  if(rocketDAONode.getMemberIsValid(msg.sender) != true) require(msg.value == rocketDAONodeTrustedSettingsMembers.getChallengeCost(), \"Non DAO members must pay ETH to challenge a members node\");  // Can't challenge yourself duh  require(msg.sender != _nodeAddress, \"You cannot challenge yourself\");  // Is this member already being challenged?  Recommendation  We recommend locking the ETH inside the contract during the challenge process. If a challenge is refuted, we recommend feeding the locked value back into the system as protocol collateral. If the challenge succeeds and the node is kicked, it is assumed that the challenger will be repaid the amount they had to lock up to prove non-liveliness.  ",
        "labels": [
            "Consensys",
            "Major",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"
    },
    {
        "title": "5.5 Multiple checks-effects violations ",
        "body": "  Resolution  The client provided the following statement:  In many of the cited examples, the  external call  is a call to another network contract that has the same privileges as the caller. Preventing reentrancy against our own internal contracts provides no additional security. If a malicious contract is introduced via a malicious oDAO they already have full keys to the kingdom.  None of the examples provide an attack surface and so we don t believe this to be a  major  finding and should be downgraded.  This finding highlights our concerns about a dangerous pattern used throughout the codebase that may eventually lead to exploitable scenarios if continued to be followed, especially on codebases that do not employ protective measures against reentrant calls. This report also flagged one such exploitable instance, leading to a critical exploitable issue in one of the components.  This repeated occurrence led us to flag this as a major issue to highlight a general error and attack surface present in several places.  From our experience, there are predominantly positive side-effects of adhering to safe coding patterns, even for trusted contract interactions, as developers indirectly follow or pick up the coding style from existing code, reducing the likelihood of following a pattern that may be prone to be taken advantage of.  For example, to a developer, it might not always be directly evident that control flow is passed to potentially untrusted components/addresses from the code itself, especially when calling multiple  trusted  components in the system. Furthermore, individual components down the call stack may be updated at later times, introducing an untrusted external call (i.e., because funds are refunded) and exposing the initially calling contract to a reentrancy-type issue. Therefore, we highly recommend adhering to a safe checks-effects pattern even though the contracts mainly interact with other trusted components and build secure code based on defense-in-depth principles to contain potential damage in favor of assuming worst-case scenarios.  Description  Throughout the system, there are various violations of the checks-effects-interactions pattern where the contract state is updated after an external call. Since large parts of the Rocket Pool system s smart contracts are not guarded against reentrancy, the external call s recipient may reenter and potentially perform malicious actions that can impact the overall accounting and, thus, system funds.  Examples  distributeToOwner() sends the contract s balance to the node or the withdrawal address before clearing the internal accounting:  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L564-L581  /// @notice Withdraw node balances from the minipool and close it. Only accepts calls from the owner  function close() override external onlyMinipoolOwner(msg.sender) onlyInitialised {  // Check current status  require(status == MinipoolStatus.Dissolved, \"The minipool can only be closed while dissolved\");  // Distribute funds to owner  distributeToOwner();  // Destroy minipool  RocketMinipoolManagerInterface rocketMinipoolManager = RocketMinipoolManagerInterface(getContractAddress(\"rocketMinipoolManager\"));  require(rocketMinipoolManager.getMinipoolExists(address(this)), \"Minipool already closed\");  rocketMinipoolManager.destroyMinipool();  // Clear state  nodeDepositBalance = 0;  nodeRefundBalance = 0;  userDepositBalance = 0;  userDepositBalanceLegacy = 0;  userDepositAssignedTime = 0;  The withdrawal block should be set before any other contracts are called:  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L498-L499  // Save block to prevent multiple withdrawals within a few blocks  withdrawalBlock = block.number;  The slashed state should be set before any external calls are made:  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L686-L696  /// @dev Slash node operator's RPL balance based on nodeSlashBalance  function _slash() private {  // Get contracts  RocketNodeStakingInterface rocketNodeStaking = RocketNodeStakingInterface(getContractAddress(\"rocketNodeStaking\"));  // Slash required amount and reset storage value  uint256 slashAmount = nodeSlashBalance;  nodeSlashBalance = 0;  rocketNodeStaking.slashRPL(nodeAddress, slashAmount);  // Record slashing  slashed = true;  In the bond reducer, the accounting values should be cleared before any external calls are made:  code/contracts/contract/minipool/RocketMinipoolBondReducer.sol:L120-L134  // Get desired to amount  uint256 newBondAmount = getUint(keccak256(abi.encodePacked(\"minipool.bond.reduction.value\", msg.sender)));  require(rocketNodeDeposit.isValidDepositAmount(newBondAmount), \"Invalid bond amount\");  // Calculate difference  uint256 existingBondAmount = minipool.getNodeDepositBalance();  uint256 delta = existingBondAmount.sub(newBondAmount);  // Get node address  address nodeAddress = minipool.getNodeAddress();  // Increase ETH matched or revert if exceeds limit based on current RPL stake  rocketNodeDeposit.increaseEthMatched(nodeAddress, delta);  // Increase node operator's deposit credit  rocketNodeDeposit.increaseDepositCreditBalance(nodeAddress, delta);  // Clean up state  deleteUint(keccak256(abi.encodePacked(\"minipool.bond.reduction.time\", msg.sender)));  deleteUint(keccak256(abi.encodePacked(\"minipool.bond.reduction.value\", msg.sender)));  The counter for reward snapshot execution should be incremented before RPL gets minted:  code/contracts/contract/rewards/RocketRewardsPool.sol:L210-L213  // Execute inflation if required  rplContract.inflationMintTokens();  // Increment the reward index and update the claim interval timestamp  incrementRewardIndex();  Recommendation  We recommend following the checks-effects-interactions pattern and adjusting any contract state variables before making external calls. With the upgradeable nature of the system, we also recommend strictly adhering to this practice when all external calls are being made to trusted network contracts.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"
    },
    {
        "title": "5.6 Minipool state machine design and pseudo-states   ",
        "body": "  Resolution  The client acknowledges the finding and provided the following statement.  We agree that the state machine is complicated. This is a symptom of technical debt and backwards compatibility.  There is no actionable response to this finding as we cannot make changes to the existing 12,000 contracts already deployed.  We want to emphasize that this finding strongly suggests that there are design deficits in the minipool state machine that, sooner or later, may impact the overall system s security. We suggest refactoring a clean design with clear transitions and states for the current iteration removing technical debt from future versions. This may mean that it may be warranted to release a new major Rocketpool version as a standalone system with a clean migration path avoiding potential problems otherwise introduced by dealing with the current technical debt.  Description  Recommendation  We strongly discourage the use of pseudo-states in state machines as they make the state machine less intuitive and present challenges in mapping state transitions to the code base. Real states and transitions should be used where possible.  Generally, we recommend the following when designing state machines:  Using clear and descriptive transition names,  Avoiding having multiple transitions with the same trigger,  Modeling decisions in the form of state transitions rather than states themselves.  In any case, every Minipool should terminate in a clear end state.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"
    },
    {
        "title": "5.7 RocketMinipoolDelegate - Redundant refund() call on forced finalization    ",
        "body": "  Resolution  Fixed in https://github.com/rocket-pool/rocketpool/tree/77d7cca65b7c0557cfda078a4fc45f9ac0cc6cc6 by refactoring refund() to avoid a double invocation of _refund() in the _finalise() codepath.  Fixed per the recommendation. Thanks.  Description  The RocketMinipoolDelegate.refund function will force finalization if a user previously distributed the pool. However, _finalise already calls _refund() if there is a node refund balance to transfer, making the additional call to _refund() in refund() obsolete.  Examples  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L200-L209  function refund() override external onlyMinipoolOwnerOrWithdrawalAddress(msg.sender) onlyInitialised {  // Check refund balance  require(nodeRefundBalance > 0, \"No amount of the node deposit is available for refund\");  // If this minipool was distributed by a user, force finalisation on the node operator  if (!finalised && userDistributed) {  _finalise();  // Refund node  _refund();  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L445-L459  function _finalise() private {  // Get contracts  RocketMinipoolManagerInterface rocketMinipoolManager = RocketMinipoolManagerInterface(getContractAddress(\"rocketMinipoolManager\"));  // Can only finalise the pool once  require(!finalised, \"Minipool has already been finalised\");  // Set finalised flag  finalised = true;  // If slash is required then perform it  if (nodeSlashBalance > 0) {  _slash();  // Refund node operator if required  if (nodeRefundBalance > 0) {  _refund();  Recommendation  We recommend refactoring the if condition to contain _refund() in the else branch.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"
    },
    {
        "title": "5.8 Sparse documentation and accounting complexity   ",
        "body": "  Resolution  The client acknowledges the finding and provided the following statement:  Acknowledged and agree.  Description  Throughout the project, inline documentation is either sparse or missing altogether. Furthermore, few technical documents about the system s design rationale are available. The recent releases  increased complexity makes it significantly harder to trace the flow of funds through the system as components change semantics, are split into separate contracts, etc.  It is essential that documentation not only outlines what is being done but also why and what a function s role in the system s  bigger picture  is. Many comments in the code base fail to fulfill this requirement and are thus redundant, e.g.  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L292-L293  // Sanity check that refund balance is zero  require(nodeRefundBalance == 0, \"Refund balance not zero\");  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L333-L334  // Remove from vacant set  rocketMinipoolManager.removeVacantMinipool();  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L381-L383  if (ownerCalling) {  // Finalise the minipool if the owner is calling  _finalise();  The increased complexity and lack of documentation can increase the likelihood of developer error. Furthermore, the time spent maintaining the code and introducing new developers to the code base will drastically increase. This effect can be especially problematic in the system s accounting of funds as the various stages of a Minipool imply different flows of funds and interactions with external dependencies. Documentation should explain the rationale behind specific hardcoded values, such as the magic 8 ether boundary for withdrawal detection. An example of a lack of documentation and distribution across components is the calculation and influence of ethMatched as it plays a role in:  the minipool bond reducer,  the node deposit contract,  the node manager, and  the node staking contract.  Recommendation  As the Rocketpool system grows in complexity, we highly recommend significantly increasing the number of inline comments and general technical documentation and exploring ways to centralize the system s accounting further to provide a clear picture of which funds move where and at what point in time. Where the flow of funds is obscured because multiple components or multi-step processes are involved, we recommend adding extensive inline documentation to give context.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"
    },
    {
        "title": "5.9 RocketNodeDistributor - Missing extcodesize check in dynamic proxy   ",
        "body": "  Resolution  The client decided not to address the finding with the upcoming update. As per their assessment, the scenario outlined would require a series of misconfigurations/failures and hence is unlikely to happen. Following a defense-in-depth approach we, nevertheless, urge to implement safeguards on multiple layers as a condition like this can easily go undetected. However, after reviewing the feedback provided by the client we share the assessment that the finding should be downgraded from Major to Medium as funds are not at immediate risk and they can recover from this problem by fixing the delegate. For transparency, the client provided the following statement:  Agree that an extcodesize check here would add safety against a future mistake. But it does require a failure at many points for it to actually lead to an issue. Beacuse this contract is not getting upgraded in Atlas, we will leave it as is. We will make note to add a safety check on it in a future update of this contract.  We don t believe this consitutes a  major  finding given that it requires a future significant failure. If such a failure were to happen, the impact is also minimal as any calls to distribute() would simply do nothing. A contract upgrade would fix the problem and no funds would be at risk.  Description  Examples  code/contracts/contract/node/RocketNodeDistributor.sol:L23-L31  fallback() external payable {  address _target = rocketStorage.getAddress(distributorStorageKey);  assembly {  calldatacopy(0x0, 0x0, calldatasize())  let result := delegatecall(gas(), _target, 0x0, calldatasize(), 0x0, 0)  returndatacopy(0x0, 0x0, returndatasize())  switch result case 0 {revert(0, returndatasize())} default {return (0, returndatasize())}  code/contracts/contract/RocketStorage.sol:L153-L155  function getAddress(bytes32 _key) override external view returns (address r) {  return addressStorage[_key];  Recommendation  Before delegate-calling into the target contract, check if it exists.  assembly {  codeSize := extcodesize(_target)  require(codeSize > 0);  ",
        "labels": [
            "Consensys",
            "Medium",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"
    },
    {
        "title": "5.10 Kicked oDAO members  votes taken into account   ",
        "body": "  Resolution  The client acknowledges the finding and provided the following statement:  We are aware of this limitation but the additional changes required to implement a fix outweigh the concern in our opinion.  Description  oDAO members can vote on proposals or submit external data to the system, acting as an oracle. Data submission is based on a vote by itself, and multiple oDAO members must submit the same data until a configurable threshold (51% by default) is reached for the data to be confirmed.  When a member gets kicked or leaves the oDAO after voting, their vote is still accounted for while the total number of oDAO members decreases.  A (group of) malicious oDAO actors may exploit this fact to artificially lower the consensus threshold by voting for a proposal and then leaving the oDAO. This will leave excess votes with the proposal while the total member count decreases.  For example, let s assume there are 17 oDAO members. 9 members must vote for the proposal for it to pass (52.9%). Let s assume 8 members voted for, and the rest abstained and is against the proposal (47%, threshold not met). The proposal is unlikely to pass unless two malicious oDAO members leave the DAO, lowering the member count to 15 in an attempt to manipulate the vote, suddenly inflating vote power from 8/17 (47%; rejected) to 8/15 (53.3%; passed).  The crux is that the votes of ex-oDAO members still count, while the quorum is based on the current oDAO member number.  Here are some examples, however, this is a general pattern used for oDAO votes in the system.  Example: RocketNetworkPrices  Members submit votes via submitPrices(). If the threshold is reached, the proposal is executed. Quorum is based on the current oDAO member count, votes of ex-oDAO members are still accounted for. If a proposal is a near miss, malicious actors can force execute it by leaving the oDAO, lowering the threshold, and then calling executeUpdatePrices() to execute it.  code/contracts/contract/network/RocketNetworkPrices.sol:L75-L79  RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  if (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {  // Update the price  updatePrices(_block, _rplPrice);  code/contracts/contract/network/RocketNetworkPrices.sol:L85-L86  function executeUpdatePrices(uint256 _block, uint256 _rplPrice) override external onlyLatestContract(\"rocketNetworkPrices\", address(this)) {  // Check settings  RocketMinipoolBondReducer  The RocketMinipoolBondReducer contract s voteCancelReduction function takes old votes of previously kicked oDAO members into account. This results in the vote being significantly higher and increases the potential for malicious actors, even after their removal, to sway the vote. Note that a canceled bond reduction cannot be undone.  code/contracts/contract/minipool/RocketMinipoolBondReducer.sol:L94-L98  RocketDAONodeTrustedSettingsMinipoolInterface rocketDAONodeTrustedSettingsMinipool = RocketDAONodeTrustedSettingsMinipoolInterface(getContractAddress(\"rocketDAONodeTrustedSettingsMinipool\"));  uint256 quorum = rocketDAONode.getMemberCount().mul(rocketDAONodeTrustedSettingsMinipool.getCancelBondReductionQuorum()).div(calcBase);  bytes32 totalCancelVotesKey = keccak256(abi.encodePacked(\"minipool.bond.reduction.vote.count\", _minipoolAddress));  uint256 totalCancelVotes = getUint(totalCancelVotesKey).add(1);  if (totalCancelVotes > quorum) {  RocketNetworkPenalties  code/contracts/contract/network/RocketNetworkPenalties.sol:L47-L51  RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  if (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodePenaltyThreshold()) {  setBool(executedKey, true);  incrementMinipoolPenaltyCount(_minipoolAddress);  code/contracts/contract/network/RocketNetworkPenalties.sol:L54-L58  // Executes incrementMinipoolPenaltyCount if consensus threshold is reached  function executeUpdatePenalty(address _minipoolAddress, uint256 _block) override external onlyLatestContract(\"rocketNetworkPenalties\", address(this)) {  // Get contracts  RocketDAOProtocolSettingsNetworkInterface rocketDAOProtocolSettingsNetwork = RocketDAOProtocolSettingsNetworkInterface(getContractAddress(\"rocketDAOProtocolSettingsNetwork\"));  // Get submission keys  Recommendation  Track oDAO members  votes and remove them from the tally when the removal from the oDAO is executed.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"
    },
    {
        "title": "5.11 RocketDAOProtocolSettingsRewards - settings key collission   ",
        "body": "  Resolution  The client acknowledges the finding and provided the following statement:  We are aware of this limitation but making this change now with an existing deployment outweighs the concern in our opinion.  Description  A malicious user may craft a DAO protocol proposal to set a rewards claimer for a specific contract, thus overwriting another contract s settings. This issue arises due to lax requirements when choosing safe settings keys.  code/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettingsRewards.sol:L36-L49  function setSettingRewardsClaimer(string memory _contractName, uint256 _perc) override public onlyDAOProtocolProposal {  // Get the total perc set, can't be more than 100  uint256 percTotal = getRewardsClaimersPercTotal();  // If this group already exists, it will update the perc  uint256 percTotalUpdate = percTotal.add(_perc).sub(getRewardsClaimerPerc(_contractName));  // Can't be more than a total claim amount of 100%  require(percTotalUpdate <= 1 ether, \"Claimers cannot total more than 100%\");  // Update the total  setUint(keccak256(abi.encodePacked(settingNameSpace,\"rewards.claims\", \"group.totalPerc\")), percTotalUpdate);  // Update/Add the claimer amount  setUint(keccak256(abi.encodePacked(settingNameSpace, \"rewards.claims\", \"group.amount\", _contractName)), _perc);  // Set the time it was updated at  setUint(keccak256(abi.encodePacked(settingNameSpace, \"rewards.claims\", \"group.amount.updated.time\", _contractName)), block.timestamp);  The method updates the rewards claimer for a specific contract by writing to the following two setting keys:  settingNameSpace.rewards.claimsgroup.amount<_contractName>  settingNameSpace.rewards.claimsgroup.amount.updated.time<_contractName>  Due to the way the settings hierarchy was chosen in this case, a malicious proposal might define a <_contractName> = .updated.time<targetContract> that overwrites the settings of a different contract with an invalid value.  Note that the issue of delimiter consistency is also discussed in issue 5.12.  The severity rating is based on the fact that this should be detectable by DAO members. However, following a defense-in-depth approach means that such collisions should be avoided wherever possible.  Recommendation  We recommend enforcing a unique prefix and delimiter when concatenating user-provided input to setting keys. In this specific case, the settings could be renamed as follows:  settingNameSpace.rewards.claimsgroup.amount.value<_contractName>  settingNameSpace.rewards.claimsgroup.amount.updated.time<_contractName>  ",
        "labels": [
            "Consensys",
            "Medium",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"
    },
    {
        "title": "5.12 RocketDAOProtocolSettingsRewards - missing setting delimiters   ",
        "body": "  Resolution  The client acknowledges the finding and provided the following statement:  We are aware of this limitation but making this change now with an existing deployment outweighs the concern in our opinion.  Description  Settings in the Rocket Pool system are hierarchical, and namespaces are prefixed using dot delimiters.  Calling abi.encodePacked(<string>, <string>) on strings performs a simple concatenation. According to the settings  naming scheme, it is suggested that the following example writes to a key named: <settingNameSpace>.rewards.claims.group.amount.<_contractName>. However, due to missing delimiters, the actual key written to is: <settingNameSpace>.rewards.claimsgroup.amount<_contractName>.  Note that there is no delimiter between claims|group and amount|<_contractName>.  code/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettingsRewards.sol:L36-L49  function setSettingRewardsClaimer(string memory _contractName, uint256 _perc) override public onlyDAOProtocolProposal {  // Get the total perc set, can't be more than 100  uint256 percTotal = getRewardsClaimersPercTotal();  // If this group already exists, it will update the perc  uint256 percTotalUpdate = percTotal.add(_perc).sub(getRewardsClaimerPerc(_contractName));  // Can't be more than a total claim amount of 100%  require(percTotalUpdate <= 1 ether, \"Claimers cannot total more than 100%\");  // Update the total  setUint(keccak256(abi.encodePacked(settingNameSpace,\"rewards.claims\", \"group.totalPerc\")), percTotalUpdate);  // Update/Add the claimer amount  setUint(keccak256(abi.encodePacked(settingNameSpace, \"rewards.claims\", \"group.amount\", _contractName)), _perc);  // Set the time it was updated at  setUint(keccak256(abi.encodePacked(settingNameSpace, \"rewards.claims\", \"group.amount.updated.time\", _contractName)), block.timestamp);  Recommendation  We recommend adding the missing intermediate delimiters. The system should enforce delimiters after the last setting key before user input is concatenated to reduce the risk of accidental namespace collisions.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"
    },
    {
        "title": "5.13 Use of address instead of specific contract types   ",
        "body": "  Resolution  The client acknowledges the finding, removed the unnecessary casts from canReduceBondAmount and voteCancelReduction with https://github.com/rocket-pool/rocketpool/tree/77d7cca65b7c0557cfda078a4fc45f9ac0cc6cc6, and provided the following statement:  Acknowledged. We will migrate to this pattern as we upgrade contracts.  Description  Rather than using a low-level address type and then casting to the safer contract type, it s better to use the best type available by default so the compiler can eventually check for type safety and contract existence and only downcast to less secure low-level types (address) when necessary.  Examples  RocketStorageInterface _rocketStorage should be declared in the arguments, removing the need to cast the address explicitly.  code/contracts/contract/minipool/RocketMinipoolBase.sol:L39-L47  /// @notice Sets up starting delegate contract and then delegates initialisation to it  function initialise(address _rocketStorage, address _nodeAddress) external override notSelf {  // Check input  require(_nodeAddress != address(0), \"Invalid node address\");  require(storageState == StorageState.Undefined, \"Already initialised\");  // Set storage state to uninitialised  storageState = StorageState.Uninitialised;  // Set rocketStorage  rocketStorage = RocketStorageInterface(_rocketStorage);  RocketMinipoolInterface _minipoolAddress should be declared in the arguments, removing the need to cast the address explicitly. Downcast to low-level address if needed. The event can be redeclared with the contract type.  code/contracts/contract/minipool/RocketMinipoolBondReducer.sol:L33-L34  function beginReduceBondAmount(address _minipoolAddress, uint256 _newBondAmount) override external onlyLatestContract(\"rocketMinipoolBondReducer\", address(this)) {  RocketMinipoolInterface minipool = RocketMinipoolInterface(_minipoolAddress);  code/contracts/contract/minipool/RocketMinipoolBondReducer.sol:L69-L76  /// @notice Returns whether owner of given minipool can reduce bond amount given the waiting period constraint  /// @param _minipoolAddress Address of the minipool  function canReduceBondAmount(address _minipoolAddress) override public view returns (bool) {  RocketMinipoolInterface minipool = RocketMinipoolInterface(_minipoolAddress);  RocketDAONodeTrustedSettingsMinipoolInterface rocketDAONodeTrustedSettingsMinipool = RocketDAONodeTrustedSettingsMinipoolInterface(getContractAddress(\"rocketDAONodeTrustedSettingsMinipool\"));  uint256 reduceBondTime = getUint(keccak256(abi.encodePacked(\"minipool.bond.reduction.time\", _minipoolAddress)));  return rocketDAONodeTrustedSettingsMinipool.isWithinBondReductionWindow(block.timestamp.sub(reduceBondTime));  code/contracts/contract/minipool/RocketMinipoolBondReducer.sol:L80-L84  function voteCancelReduction(address _minipoolAddress) override external onlyTrustedNode(msg.sender) onlyLatestContract(\"rocketMinipoolBondReducer\", address(this)) {  // Prevent calling if consensus has already been reached  require(!getReduceBondCancelled(_minipoolAddress), \"Already cancelled\");  // Get contracts  RocketMinipoolInterface minipool = RocketMinipoolInterface(_minipoolAddress);  Note that abi.encode*(contractType) assumes address for contract types by default. An explicit downcast is not required.  More examples of address _minipool declarations:  code/contracts/contract/minipool/RocketMinipoolManager.sol:L449-L455  /// @dev Internal logic to set a minipool's pubkey  /// @param _pubkey The pubkey to set for the calling minipool  function _setMinipoolPubkey(address _minipool, bytes calldata _pubkey) private {  // Load contracts  AddressSetStorageInterface addressSetStorage = AddressSetStorageInterface(getContractAddress(\"addressSetStorage\"));  // Initialize minipool & get properties  RocketMinipoolInterface minipool = RocketMinipoolInterface(_minipool);  code/contracts/contract/minipool/RocketMinipoolManager.sol:L474-L478  function getMinipoolDetails(address _minipoolAddress) override external view returns (MinipoolDetails memory) {  // Get contracts  RocketMinipoolInterface minipoolInterface = RocketMinipoolInterface(_minipoolAddress);  RocketMinipoolBase minipool = RocketMinipoolBase(payable(_minipoolAddress));  RocketNetworkPenaltiesInterface rocketNetworkPenalties = RocketNetworkPenaltiesInterface(getContractAddress(\"rocketNetworkPenalties\"));  More examples of RocketStorageInterface _rocketStorage casts:  code/contracts/contract/node/RocketNodeDistributor.sol:L8-L13  contract RocketNodeDistributor is RocketNodeDistributorStorageLayout {  bytes32 immutable distributorStorageKey;  constructor(address _nodeAddress, address _rocketStorage) {  rocketStorage = RocketStorageInterface(_rocketStorage);  nodeAddress = _nodeAddress;  Recommendation  We recommend using more specific types instead of address where possible. Downcast if necessary. This goes for parameter types as well as state variable types.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"
    },
    {
        "title": "5.14 Redundant double casts   ",
        "body": "  Resolution  The client acknowledges the finding and provided the following statement:  Acknowledged. These contracts are non-upgradable.  Description  _rocketStorageAddress  is already of contract type RocketStorageInterface.  code/contracts/contract/RocketBase.sol:L78-L82  /// @dev Set the main Rocket Storage address  constructor(RocketStorageInterface _rocketStorageAddress) {  // Update the contract address  rocketStorage = RocketStorageInterface(_rocketStorageAddress);  _tokenAddress  is already of contract type ERC20Burnable.  code/contracts/contract/RocketVault.sol:L132-L138  function burnToken(ERC20Burnable _tokenAddress, uint256 _amount) override external onlyLatestNetworkContract {  // Get contract key  bytes32 contractKey = keccak256(abi.encodePacked(getContractName(msg.sender), _tokenAddress));  // Update balances  tokenBalances[contractKey] = tokenBalances[contractKey].sub(_amount);  // Get the token ERC20 instance  ERC20Burnable tokenContract = ERC20Burnable(_tokenAddress);  _rocketTokenRPLFixedSupplyAddress is already of contract type IERC20.  code/contracts/contract/token/RocketTokenRPL.sol:L47-L51  constructor(RocketStorageInterface _rocketStorageAddress, IERC20 _rocketTokenRPLFixedSupplyAddress) RocketBase(_rocketStorageAddress) ERC20(\"Rocket Pool Protocol\", \"RPL\") {  // Version  version = 1;  // Set the mainnet RPL fixed supply token address  rplFixedSupplyContract = IERC20(_rocketTokenRPLFixedSupplyAddress);  Recommendation  We recommend removing the unnecessary double casts and copies of local variables.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"
    },
    {
        "title": "5.15 RocketMinipoolDelegate - Missing event in prepareVacancy    ",
        "body": "  Resolution  Fixed in https://github.com/rocket-pool/rocketpool/tree/77d7cca65b7c0557cfda078a4fc45f9ac0cc6cc6 by emitting a new event MinipoolVacancyPrepared.  Agreed. Added event per recommendation. Thanks.  Description  The function prepareVacancy updates multiple contract state variables and should therefore emit an event.  Examples  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L286-L309  /// @dev Sets the bond value and vacancy flag on this minipool  /// @param _bondAmount The bond amount selected by the node operator  /// @param _currentBalance The current balance of the validator on the beaconchain (will be checked by oDAO and scrubbed if not correct)  function prepareVacancy(uint256 _bondAmount, uint256 _currentBalance) override external onlyLatestContract(\"rocketMinipoolManager\", msg.sender) onlyInitialised {  // Check status  require(status == MinipoolStatus.Initialised, \"Must be in initialised status\");  // Sanity check that refund balance is zero  require(nodeRefundBalance == 0, \"Refund balance not zero\");  // Check balance  RocketDAOProtocolSettingsMinipoolInterface rocketDAOProtocolSettingsMinipool = RocketDAOProtocolSettingsMinipoolInterface(getContractAddress(\"rocketDAOProtocolSettingsMinipool\"));  uint256 launchAmount = rocketDAOProtocolSettingsMinipool.getLaunchBalance();  require(_currentBalance >= launchAmount, \"Balance is too low\");  // Store bond amount  nodeDepositBalance = _bondAmount;  // Calculate user amount from launch amount  userDepositBalance = launchAmount.sub(nodeDepositBalance);  // Flag as vacant  vacant = true;  preMigrationBalance = _currentBalance;  // Refund the node whatever rewards they have accrued prior to becoming a RP validator  nodeRefundBalance = _currentBalance.sub(launchAmount);  // Set status to preLaunch  setStatus(MinipoolStatus.Prelaunch);  Recommendation  Emit the missing event.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"
    },
    {
        "title": "5.16 Compiler error due to missing RocketMinipoolBaseInterface    ",
        "body": "  Resolution   Fixed in   https://github.com/rocket-pool/rocketpool/tree/77d7cca65b7c0557cfda078a4fc45f9ac0cc6cc6 by adding the missing interface file.  Description  The interface RocketMinipoolBaseInterface is missing from the code repository. Manually generating the interface and adding it to the repository fixes the error.  Recommendation  Add the missing source unit to the repository.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"
    },
    {
        "title": "5.17 Unused Imports   Partially Addressed",
        "body": "  Resolution  Addressed in https://github.com/rocket-pool/rocketpool/tree/77d7cca65b7c0557cfda078a4fc45f9ac0cc6cc6 by removing all but the following two mentioned unused imports:  RocketRewardsPoolInterface  RocketSmoothingPoolInterface  Description  The following source units are imported but not referenced in the importing source unit:  code/contracts/contract/rewards/RocketMerkleDistributorMainnet.sol:L11  import \"../../interface/rewards/RocketSmoothingPoolInterface.sol\";  code/contracts/contract/minipool/RocketMinipoolFactory.sol:L12-L18  import \"../../interface/minipool/RocketMinipoolManagerInterface.sol\";  import \"../../interface/minipool/RocketMinipoolQueueInterface.sol\";  import \"../../interface/node/RocketNodeStakingInterface.sol\";  import \"../../interface/util/AddressSetStorageInterface.sol\";  import \"../../interface/node/RocketNodeManagerInterface.sol\";  import \"../../interface/network/RocketNetworkPricesInterface.sol\";  import \"../../interface/dao/protocol/settings/RocketDAOProtocolSettingsMinipoolInterface.sol\";  code/contracts/contract/minipool/RocketMinipoolFactory.sol:L8-L10  import \"../../types/MinipoolStatus.sol\";  import \"../../types/MinipoolDeposit.sol\";  import \"../../interface/dao/node/RocketDAONodeTrustedInterface.sol\";  code/contracts/contract/minipool/RocketMinipoolBase.sol:L7-L8  import \"../../types/MinipoolDeposit.sol\";  import \"../../types/MinipoolStatus.sol\";  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L13-L14  import \"../../interface/network/RocketNetworkPricesInterface.sol\";  import \"../../interface/node/RocketNodeManagerInterface.sol\";  code/contracts/contract/node/RocketNodeManager.sol:L13  import \"../../interface/rewards/claims/RocketClaimNodeInterface.sol\";  code/contracts/contract/rewards/RocketClaimDAO.sol:L7  import \"../../interface/rewards/RocketRewardsPoolInterface.sol\";  Duplicate Import:  code/contracts/contract/minipool/RocketMinipoolFactory.sol:L19-L20  import \"../../interface/dao/protocol/settings/RocketDAOProtocolSettingsNodeInterface.sol\";  import \"../../interface/dao/protocol/settings/RocketDAOProtocolSettingsNodeInterface.sol\";  The above list is exemplary, and there are likely more occurrences across the code base.  Recommendation  We recommend checking all imports and removing unused/unreferenced and unnecessary imports.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"
    },
    {
        "title": "5.18 RocketMinipool - Inconsistent access control modifier declaration onlyMinipoolOwner   ",
        "body": "  Resolution  Acknowledged by the client. Not addressed within rocket-pool/rocketpool@77d7cca  Agreed. This would change a lot of contracts just for a minor improvement in readbility.  Description  The access control modifier onlyMinipoolOwner should be renamed to onlyMinipoolOwnerOrWithdrawalAddress to be consistent with the actual check permitting the owner or the withdrawal address to interact with the function. This would also be consistent with other declarations in the codebase.  Example  The onlyMinipoolOwner modifier in RocketMinipoolBase is the same as onlyMinipoolOwnerOrWithdrawalAddress in other modules.  code/contracts/contract/minipool/RocketMinipoolBase.sol:L31-L37  /// @dev Only allow access from the owning node address  modifier onlyMinipoolOwner() {  // Only the node operator can upgrade  address withdrawalAddress = rocketStorage.getNodeWithdrawalAddress(nodeAddress);  require(msg.sender == nodeAddress || msg.sender == withdrawalAddress, \"Only the node operator can access this method\");  _;  code/contracts/contract/old/minipool/RocketMinipoolOld.sol:L21-L27  // Only allow access from the owning node address  modifier onlyMinipoolOwner() {  // Only the node operator can upgrade  address withdrawalAddress = rocketStorage.getNodeWithdrawalAddress(nodeAddress);  require(msg.sender == nodeAddress || msg.sender == withdrawalAddress, \"Only the node operator can access this method\");  _;  Other declarations:  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L97-L107  /// @dev Only allow access from the owning node address  modifier onlyMinipoolOwner(address _nodeAddress) {  require(_nodeAddress == nodeAddress, \"Invalid minipool owner\");  _;  /// @dev Only allow access from the owning node address or their withdrawal address  modifier onlyMinipoolOwnerOrWithdrawalAddress(address _nodeAddress) {  require(_nodeAddress == nodeAddress || _nodeAddress == rocketStorage.getNodeWithdrawalAddress(nodeAddress), \"Invalid minipool owner\");  _;  code/contracts/contract/old/minipool/RocketMinipoolDelegateOld.sol:L82-L92  // Only allow access from the owning node address  modifier onlyMinipoolOwner(address _nodeAddress) {  require(_nodeAddress == nodeAddress, \"Invalid minipool owner\");  _;  // Only allow access from the owning node address or their withdrawal address  modifier onlyMinipoolOwnerOrWithdrawalAddress(address _nodeAddress) {  require(_nodeAddress == nodeAddress || _nodeAddress == rocketStorage.getNodeWithdrawalAddress(nodeAddress), \"Invalid minipool owner\");  _;  Recommendation  We recommend renaming RocketMinipoolBase.onlyMinipoolOwner to RocketMinipoolBase.onlyMinipoolOwnerOrWithdrawalAddress.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"
    },
    {
        "title": "5.19 RocketDAO*Settings - settingNameSpace should be immutable   ",
        "body": "  Resolution  Acknowledged by the client. Not addressed within rocket-pool/rocketpool@77d7cca  Acknowledged. We can fix this as we upgrade the related contracts.  Description  The settingNameSpace in the abstract contract RocketDAONodeTrustedSettings is only set on contract deployment. Hence, the fields should be declared immutable to make clear that the settings namespace cannot change after construction.  Examples  RocketDAONodeTrustedSettings  code/contracts/contract/dao/node/settings/RocketDAONodeTrustedSettings.sol:L13-L16  // The namespace for a particular group of settings  bytes32 settingNameSpace;  code/contracts/contract/dao/node/settings/RocketDAONodeTrustedSettings.sol:L25-L30  // Construct  constructor(RocketStorageInterface _rocketStorageAddress, string memory _settingNameSpace) RocketBase(_rocketStorageAddress) {  // Apply the setting namespace  settingNameSpace = keccak256(abi.encodePacked(\"dao.trustednodes.setting.\", _settingNameSpace));  RocketDAOProtocolSettings  code/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettings.sol:L13-L14  // The namespace for a particular group of settings  bytes32 settingNameSpace;  code/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettings.sol:L25-L29  // Construct  constructor(RocketStorageInterface _rocketStorageAddress, string memory _settingNameSpace) RocketBase(_rocketStorageAddress) {  // Apply the setting namespace  settingNameSpace = keccak256(abi.encodePacked(\"dao.protocol.setting.\", _settingNameSpace));  code/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettingsAuction.sol:L13-L15  constructor(RocketStorageInterface _rocketStorageAddress) RocketDAOProtocolSettings(_rocketStorageAddress, \"auction\") {  // Set version  version = 1;  Recommendation  We recommend using the immutable annotation in Solidity (see Immutable).  ",
        "labels": [
            "Consensys",
            "Minor",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"
    },
    {
        "title": "5.20 Inefficiencies with the onlyMinipoolOwner modifier  ",
        "body": "  Resolution  Acknowledged by the client. No further actions.  Correct. This change would change every single contract we have and so the benefit does not outweigh the change.  Description  If a withdrawal address has not been set (or has been zeroed out), rocketStorage.getNodeWithdrawalAddress(nodeAddress) returns nodeAddress. This outcome leads to the modifier checking the same address twice (msg.sender == nodeAddress || msg.sender == nodeAddress):  code/contracts/contract/minipool/RocketMinipoolBase.sol:L31-L37  /// @dev Only allow access from the owning node address  modifier onlyMinipoolOwner() {  // Only the node operator can upgrade  address withdrawalAddress = rocketStorage.getNodeWithdrawalAddress(nodeAddress);  require(msg.sender == nodeAddress || msg.sender == withdrawalAddress, \"Only the node operator can access this method\");  _;  code/contracts/contract/RocketStorage.sol:L103-L111  // Get a node's withdrawal address  function getNodeWithdrawalAddress(address _nodeAddress) public override view returns (address) {  // If no withdrawal address has been set, return the nodes address  address withdrawalAddress = withdrawalAddresses[_nodeAddress];  if (withdrawalAddress == address(0)) {  return _nodeAddress;  return withdrawalAddress;  ",
        "labels": [
            "Consensys",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"
    },
    {
        "title": "5.21 RocketNodeDeposit - Duplicate check to avoid revert   ",
        "body": "  Resolution  Fixed with rocket-pool/rocketpool@3ab7af1 by introducing a new method maybeAssignDeposits() that does not revert by default but returns a boolean instead. This way, RocketNodeDeposit directly call the maybeAssignDeposits() function, avoiding the duplicate check.  This finding does not present a security-related problem in the code base, which is why we downgrade its severity to informational. However, we opted to keep this recommendation present in the report since it underlines a form of technical debt where old functionality is wrapped by new functionality using a workaround.  Description  When receiving and subsequently assigning deposits, the RocketNodeDeposit contract s assignDeposits function calls RocketDAOProtocolSettingsDeposit.getAssignDepositsEnabled and skips the assignment of funds. This is done because the RocketDepositPool.assignDeposits function reverts if the setting is disabled:  code/contracts/contract/deposit/RocketDepositPool.sol:L207-L212  function assignDeposits() override external onlyThisLatestContract {  // Load contracts  RocketDAOProtocolSettingsDepositInterface rocketDAOProtocolSettingsDeposit = RocketDAOProtocolSettingsDepositInterface(getContractAddress(\"rocketDAOProtocolSettingsDeposit\"));  // Revert if assigning is disabled  require(_assignDeposits(rocketDAOProtocolSettingsDeposit), \"Deposit assignments are currently disabled\");  However, the underlying _assignDeposits function already performs a check for the setting and returns prematurely to avoid assignment.  code/contracts/contract/deposit/RocketDepositPool.sol:L217-L219  if (!_rocketDAOProtocolSettingsDeposit.getAssignDepositsEnabled()) {  return false;  The rocketDAOProtocolSettingsDeposit.getAssignDepositsEnabled() setting is checked twice. The first occurrence is in RocketNodeDeposit.assignDeposits and the second one in the same flow is contained in RocketDepositPool._assignDeposits. The second check is performed in a reverting fashion, thus requiring the top-level check in the RocketNodeDeposit contract to preemptively fetch and check the setting before continuing.  Recommendation  Since Rocketpool v1.2 already aims to perform an upgrade on the RocketDepositPool contract, we do recommend adding a separate, non-reverting version of the RocketDepositPool.assignDeposits function to the code base and removing the redundant preemptive check in RocketNodeDeposit.assignDeposits. This will improve readability and maintainability of future versions of the code, and save gas cost on deposit assignment operations.  ",
        "labels": [
            "Consensys",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"
    },
    {
        "title": "5.22 Inconsistent Coding Style  ",
        "body": "  Resolution  The client provided the following statement:  Acknolwedge your recommendation but we are dealing with an existing deployed codebase and if we change codestyle on only the contracts we update we will end up with a codebase with different code styles which is worse than one that is internally consistent but not consistent with best practice.  Description  Deviations from the Solidity Style Guide were identified throughout the codebase. Considering how much value a consistent coding style adds to the project s readability, enforcing a standard coding style with the help of linter tools is recommended.  Inconsistent Function naming scheme for external and internal interfaces  Throughout the codebase, private/internal functions are generally prefixed with an underscore (_<name>). This allows for an easy way to see if an external party can interact with a function without having to scan the declaration line for the corresponding visibility keywords. However, this naming scheme is not enforced consistently. Many internal function names are indistinguishable from external function names. It is therefore highly recommended to implement a consistent naming scheme and prefix internal functions with an underscore (_<name>).  code/contracts/contract/node/RocketNodeDeposit.sol:L268-L283  /// @dev Reverts if vacant minipools are not enabled  function checkVacantMinipoolsEnabled() private view {  // Get contracts  RocketDAOProtocolSettingsNodeInterface rocketDAOProtocolSettingsNode = RocketDAOProtocolSettingsNodeInterface(getContractAddress(\"rocketDAOProtocolSettingsNode\"));  // Check node settings  require(rocketDAOProtocolSettingsNode.getVacantMinipoolsEnabled(), \"Vacant minipools are currently disabled\");  /// @dev Executes an assignDeposits call on the deposit pool  function assignDeposits() private {  RocketDAOProtocolSettingsDepositInterface rocketDAOProtocolSettingsDeposit = RocketDAOProtocolSettingsDepositInterface(getContractAddress(\"rocketDAOProtocolSettingsDeposit\"));  if (rocketDAOProtocolSettingsDeposit.getAssignDepositsEnabled()) {  RocketDepositPoolInterface rocketDepositPool = RocketDepositPoolInterface(getContractAddress(\"rocketDepositPool\"));  rocketDepositPool.assignDeposits();  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L339-L345  /// @dev Stakes the balance of this minipool into the deposit contract to set withdrawal credentials to this contract  /// @param _validatorSignature A signature over the deposit message object  /// @param _depositDataRoot The hash tree root of the deposit data object  function preStake(bytes calldata _validatorPubkey, bytes calldata _validatorSignature, bytes32 _depositDataRoot) internal {  // Load contracts  DepositInterface casperDeposit = DepositInterface(getContractAddress(\"casperDeposit\"));  RocketMinipoolManagerInterface rocketMinipoolManager = RocketMinipoolManagerInterface(getContractAddress(\"rocketMinipoolManager\"));  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L651-L654  /// @dev Distributes the current contract balance based on capital ratio and node fee  function distributeSkimmedRewards() internal {  uint256 rewards = address(this).balance.sub(nodeRefundBalance);  uint256 nodeShare = calculateNodeRewards(nodeDepositBalance, getUserDepositBalance(), rewards);  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L661-L663  /// @dev Set the minipool's current status  /// @param _status The new status  function setStatus(MinipoolStatus _status) private {  code/contracts/contract/node/RocketNodeDeposit.sol:L202-L206  /// @dev Adds a minipool to the queue  function enqueueMinipool(address _minipoolAddress) private {  // Add minipool to queue  RocketMinipoolQueueInterface(getContractAddress(\"rocketMinipoolQueue\")).enqueueMinipool(_minipoolAddress);  code/contracts/contract/node/RocketNodeDeposit.sol:L208-L213  /// @dev Reverts if node operator has not initialised their fee distributor  function checkDistributorInitialised() private view {  // Check node has initialised their fee distributor  RocketNodeManagerInterface rocketNodeManager = RocketNodeManagerInterface(getContractAddress(\"rocketNodeManager\"));  require(rocketNodeManager.getFeeDistributorInitialised(msg.sender), \"Fee distributor not initialised\");  code/contracts/contract/node/RocketNodeDeposit.sol:L215-L218  /// @dev Creates a minipool and returns an instance of it  /// @param _salt The salt used to determine the minipools address  /// @param _expectedMinipoolAddress The expected minipool address. Reverts if not correct  function createMinipool(uint256 _salt, address _expectedMinipoolAddress) private returns (RocketMinipoolInterface) {  code/contracts/contract/auction/RocketAuctionManager.sol:L58-L60  function setLotCount(uint256 _amount) private {  setUint(keccak256(\"auction.lots.count\"), _amount);  ",
        "labels": [
            "Consensys",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"
    },
    {
        "title": "5.1 uint overflow may lead to stealing funds    Addressed",
        "body": "  Resolution  safeMath was added in SKALE-215. At the time of the writing this comment, the review has not been comprehensive to all arithmetic calculations in the scope.  Note that in some cases usage of safeMath due to reverts can result in unexpected halting of the system, that too should be reviewed again.  Description  It s possible to create a delegation with a very huge amount which may result in a lot of critically bad malicious usages:  code/contracts/delegation/DelegationRequestManager.sol:L74-L76  uint holderBalance = SkaleToken(contractManager.getContract(\"SkaleToken\")).balanceOf(holder);  uint lockedToDelegate = tokenState.getLockedCount(holder) - tokenState.getPurchasedAmount(holder);  require(holderBalance >= amount + lockedToDelegate, \"Delegator hasn't enough tokens to delegate\");  amount is passed by a user as a parameter, so if it s close to uint max value, amount + lockedToDelegate would overflow and this requirement would pass.  Having delegation with an almost infinite amount of tokens can lead to many various attacks on the system up to stealing funds and breaking everything.  Recommendation  Using SafeMath everywhere should prevent this and other similar issues. There should be more critical attacks caused by overflows/underflows, so SafeMath should be used everywhere in the codebase.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "5.2 Holders can burn locked funds    Addressed",
        "body": "  Resolution   Fixed in   SKALE-2144 by adding proper checks in  Description  Skale token is a modified ERC-777 that allows locking some part of the balance. Locking is checked during every transfer:  code/contracts/ERC777/LockableERC777.sol:L433-L441  // Property of the company SKALE Labs inc.---------------------------------  uint locked = _getLockedOf(from);  if (locked > 0) {  require(_balances[from] >= locked + amount, \"Token should be unlocked for transferring\");  //-------------------------------------------------------------------------  _balances[from] = _balances[from].sub(amount);  _balances[to] = _balances[to].add(amount);  But it s not checked during burn function and it s possible to  burn  locked tokens. Tokens will be burned, but locked amount will remain the same. That will result in having more locked tokens than the balance which may have very unpredictable behaviour.  Recommendation  Allow burning only unlocked tokens.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "5.3 Node can unlink validator    Addressed",
        "body": "  Resolution   Fixed in   SKALE-2145-unlink-node by adding a check in  Description  Validators can link a node address to them by calling linkNodeAddress function:  code/contracts/delegation/ValidatorService.sol:L109-L119  function linkNodeAddress(address validatorAddress, address nodeAddress) external allow(\"DelegationService\") {  uint validatorId = getValidatorId(validatorAddress);  require(_validatorAddressToId[nodeAddress] == 0, \"Validator cannot override node address\");  _validatorAddressToId[nodeAddress] = validatorId;  function unlinkNodeAddress(address validatorAddress, address nodeAddress) external allow(\"DelegationService\") {  uint validatorId = getValidatorId(validatorAddress);  require(_validatorAddressToId[nodeAddress] == validatorId, \"Validator hasn't permissions to unlink node\");  _validatorAddressToId[nodeAddress] = 0;  After that, the node has the same rights and is almost indistinguishable from the validator. So the node can even remove validator s address from _validatorAddressToId list and take over full control over validator. Additionally, the node can even remove itself by calling unlinkNodeAddress, leaving validator with no control at all forever.  Also, even without nodes, a validator can initially call unlinkNodeAddress to remove itself.  Recommendation  Linked nodes (and validator) should not be able to unlink validator s address from the _validatorAddressToId mapping.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "5.4 Unlocking funds after slashing    Addressed",
        "body": "  Resolution   Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  The initial funds can be unlocked if 51+% of them are delegated. However if any portion of the funds are slashed, the rest of the funds will not be unlocked at the end of the delegation period.  code/contracts/delegation/TokenState.sol:L258-L263  if (_isPurchased[delegationId]) {  address holder = delegation.holder;  _totalDelegated[holder] += delegation.amount;  if (_totalDelegated[holder] >= _purchased[holder]) {  purchasedToUnlocked(holder);  Recommendation  Consider slashed tokens as delegated, or include them in the calculation for process to unlock in endingDelegatedToUnlocked  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "5.5 Bounties and fees should only be locked for the first 3 months    Addressed",
        "body": "  Resolution   Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  Bounties are currently locked for the first 3 months after delegation:  code/contracts/delegation/DelegationService.sol:L315  skaleBalances.lockBounty(shares[i].holder, timeHelpers.addMonths(delegationStarted, 3));  Instead, they should be locked for the first 3 months after the token launch.  Recommendation  It s better just to forbid any withdrawals for the first 3 months, no need to track it separately for every delegation. This recommendation is mainly to simplify the process.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "5.6 getLockedCount is iterating over all history of delegations    Addressed",
        "body": "  Resolution   Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  getLockedCount is iterating over all delegations of a specific holder and may even change the state of these delegations by calling getState.  code/contracts/delegation/TokenState.sol:L60-L71  function getLockedCount(address holder) external returns (uint amount) {  amount = 0;  DelegationController delegationController = DelegationController(contractManager.getContract(\"DelegationController\"));  uint[] memory delegationIds = delegationController.getDelegationsByHolder(holder);  for (uint i = 0; i < delegationIds.length; ++i) {  uint id = delegationIds[i];  if (isLocked(getState(id))) {  amount += delegationController.getDelegation(id).amount;  return amount + getPurchasedAmount(holder) + this.getSlashedAmount(holder);  This problem is major because delegations number is growing over time and may even potentially grow more than the gas limit and lock all tokens forever. getLockedCount is called during every transfer which makes any token transfer much more expensive than it should be.  Recommendation  Remove iterations over a potentially unlimited amount of tokens. All the necessary data can be precalculated before and getLockedCount function can have O(1) complexity.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "5.7 Tokens are unlocked only when delegation ends    Addressed",
        "body": "  Resolution   Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  After the first 3 months since at least 50% of tokens are delegated, all tokens should be unlocked. In practice, they are only unlocked if at least 50% of tokens, that were bought on the initial launch, are undelegated.  code/contracts/delegation/TokenState.sol:L258-L264  if (_isPurchased[delegationId]) {  address holder = delegation.holder;  _totalDelegated[holder] += delegation.amount;  if (_totalDelegated[holder] >= _purchased[holder]) {  purchasedToUnlocked(holder);  Recommendation  Implement lock mechanism according to the legal requirement.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "5.8 Tokens after delegation should not be unlocked automatically    Addressed",
        "body": "  Resolution   Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  When some amount of tokens are delegated to a validator when the delegation period ends, these tokens are unlocked. However these tokens should be added to _purchased as they were in that state before their delegation.  code/contracts/delegation/TokenState.sol:L258-L264  if (_isPurchased[delegationId]) {  address holder = delegation.holder;  _totalDelegated[holder] += delegation.amount;  if (_totalDelegated[holder] >= _purchased[holder]) {  purchasedToUnlocked(holder);  Recommendation  Tokens should only be unlocked if the main legal requirement (_totalDelegated[holder] >= _purchased[holder]) is satisfied, which in the above case this has not happened.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "5.9 Some unlocked tokens can become locked after delegation is rejected    Addressed",
        "body": "  Resolution   Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  When some amount of tokens are requested to be delegated to a validator, the validator can reject the request. The previous status of these tokens should be intact and not changed (locked or unlocked).  Here the initial status of tokens gets stored and it s either completely locked or unlocked:  code/contracts/delegation/TokenState.sol:L205-L214  if (_purchased[delegation.holder] > 0) {  _isPurchased[delegationId] = true;  if (_purchased[delegation.holder] > delegation.amount) {  _purchased[delegation.holder] -= delegation.amount;  } else {  _purchased[delegation.holder] = 0;  } else {  _isPurchased[delegationId] = false;  The problem is that if some amount of these tokens are locked at the time of the request and the rest tokens are unlocked, they will all be considered as locked after the delegation was rejected.  code/contracts/delegation/TokenState.sol:L272-L278  function _cancel(uint delegationId, DelegationController.Delegation memory delegation) internal returns (State state) {  if (_isPurchased[delegationId]) {  state = purchasedProposedToPurchased(delegationId, delegation);  } else {  state = proposedToUnlocked(delegationId);  Recommendation  Don t change the status of the rejected tokens.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "5.10 Gas limit for bounty and slashing distribution    Addressed",
        "body": "  Resolution   Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  After every bounty payment (should be once per month) to a validator, the bounty is distributed to all delegators. In order to do that, there is a for loop that iterates over all active delegators and sends their bounty to SkaleBalances contract:  code/contracts/delegation/DelegationService.sol:L310-L316  for (uint i = 0; i < shares.length; ++i) {  skaleToken.send(address(skaleBalances), shares[i].amount, abi.encode(shares[i].holder));  uint created = delegationController.getDelegation(shares[i].delegationId).created;  uint delegationStarted = timeHelpers.getNextMonthStartFromDate(created);  skaleBalances.lockBounty(shares[i].holder, timeHelpers.addMonths(delegationStarted, 3));  There are also few more loops over all the active delegators. This leads to a huge gas cost of distribution mechanism. A number of active delegators that can be processed before hitting the gas limit is limited and not big enough.  The same issue is with slashing:  code/contracts/delegation/DelegationService.sol:L95-L106  function slash(uint validatorId, uint amount) external allow(\"SkaleDKG\") {  ValidatorService validatorService = ValidatorService(contractManager.getContract(\"ValidatorService\"));  require(validatorService.validatorExists(validatorId), \"Validator does not exist\");  Distributor distributor = Distributor(contractManager.getContract(\"Distributor\"));  TokenState tokenState = TokenState(contractManager.getContract(\"TokenState\"));  Distributor.Share[] memory shares = distributor.distributePenalties(validatorId, amount);  for (uint i = 0; i < shares.length; ++i) {  tokenState.slash(shares[i].delegationId, shares[i].amount);  Recommendation  The best solution would require major changes to the codebase, but would eventually make it simpler and safer. Instead of distributing and centrally calculating bounty for each delegator during one call it s better to just store all the necessary values, so delegator would be able to calculate the bounty on withdrawal. Amongst the necessary values, there should be history of total delegated amounts per validator during each bounty payment and history of all delegations with durations of their active state.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "5.11 ERC-777 callback issue    Partially fixed",
        "body": "  Resolution  SKALE-2153 to  skalenetwork/skale-manager#128  This report raises this as an unfixed minor issue. This issue will be fixed if the upgrade capability for _getAndUpdateLockedAmount() is revoked by SKALE network governance in the future.  Description  ERC-777 token comes with callback functions to the receiver and the sender on every token transfer. This gives re-entrancy opportunities for everyone who s using this token. There is a chance that other systems might not handle ERC-777 correctly.  Examples  Uniswap reentrancy critical bug: https://medium.com/consensys-diligence/uniswap-audit-b90335ac007  Recommendation  Use ERC-20 standard or remove callback function calls.  Remove callback function usage from the system and replace them with a standard ERC-20 flow:  code/contracts/delegation/SkaleBalances.sol:L55-L68  function tokensReceived(  address operator,  address from,  address to,  uint256 amount,  bytes calldata userData,  bytes calldata operatorData  external  allow(\"SkaleToken\")  address recipient = abi.decode(userData, (address));  stashBalance(recipient, amount);  code/contracts/delegation/DelegationService.sol:L275-L289  function tokensReceived(  address operator,  address from,  address to,  uint256 amount,  bytes calldata userData,  bytes calldata operatorData  external  allow(\"SkaleToken\")  require(userData.length == 32, \"Data length is incorrect\");  uint validatorId = abi.decode(userData, (uint));  distributeBounty(amount, validatorId);  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "5.12 Rename functions    Addressed",
        "body": "  Resolution   Fixed in   SKALE-2154-naming by renaming the functions. The functions that are not solely getters and update the state of the smart contract are renamed to have  Description  The naming of the functions should reflect their nature, such as functions starting with  get  should be only getters and do not change state. This will result in confusion developments and the implicit state changes might not be noticed.  Other than getters, some other function or variable names are misleading.  Examples  The following functions are a few examples that are named as getters but they change the state.  getState -> updateState  getDelegationsTotal getDelegationsForValidator getDelegationsByHolder  Some other naming that does not reflect the nature of the functionality:  getPurchasedAmount -> getPurchasedUnlocked  tokenState.Sold -> lock  Recommendation  For functions that get and update variables use getAndUpdate naming. Similarly use variable names that reflect the nature of the values they store.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "5.13 Delegations might stuck in non-active validator   Pending",
        "body": "  Resolution  Skale team acknowledged this issue and will address this in future versions.  Description  If a validator does not get enough funds to run a node (MSR - Minimum staking requirement), all token holders that delegated tokens to the validator cannot switch to a different validator, and might result in funds getting stuck with the nonfunctioning validator for up to 12 months.  Example  code/contracts/delegation/ValidatorService.sol:L166  require((validatorNodes.length + 1) * msr <= delegationsTotal, \"Validator has to meet Minimum Staking Requirement\");  Recommendation  Allow token holders to withdraw delegation earlier if the validator didn t get enough funds for running nodes.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "5.14 Disabled Validators still have delegated funds   Pending",
        "body": "  Resolution  Skale team acknowledged this issue and will address this in future versions.  Description  The owner of ValidatorService contract can enable and disable validators. The issue is that when a validator is disabled, it still has its delegations, and delegated funds will be locked until the end of their delegation period (up to 12 months).  code/contracts/delegation/ValidatorService.sol:L84-L90  function enableValidator(uint validatorId) external checkValidatorExists(validatorId) onlyOwner {  trustedValidators[validatorId] = true;  function disableValidator(uint validatorId) external checkValidatorExists(validatorId) onlyOwner {  trustedValidators[validatorId] = false;  Recommendation  It might make sense to release all delegations and stop validator s nodes if it s not trusted anymore. However, the rationale behind disabling the validators might be different that what we think, in any case there should be a way to handle this scenario, where the validator is disabled but there are funds delegated to it.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "5.15 Fees can be > 100%    Addressed",
        "body": "  Resolution   Added a check to prevent fee rates equal or higher than 100% in   SKALE-2157-fee-check.  Description  A validator can be created with feeRate > 1000 which would mean that the fee rate would be higher than 100%. Severity is not high because that validator will most likely be not whitelisted.  Also, 100%+ fees would still somehow work and not revert because of the absence of SafeMath.  Recommendation  Add sanity check for the input values in registerValidator, and do not allow adding a validator with a fee rate higher than 100%.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "5.16 getState changes state implicitly    Addressed",
        "body": "  Resolution   Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  getState function is checking and changing the state of a delegation struct. This function is called in many places in the codebase. Every delegation has a lot of different possible states and all of them are changed implicitly during other transactions, which makes it hard to track the logic in the code and make future changes in the code close to impossible without breaking some functionalities.  Recommendation  The general suggestion would be to minimize the number of implicit storage changes. Many states can be either changed explicitly or be calculated without additional storage changes.  As an option, it s possible to get rid of state storage slot at all. startDate and endDate fields may set the current state:  initProposed can be called during the creation of the proposal.  no need to explicitly change states between ACCEPTED and DELEGATED, you can set the start date on acceptance and no further changes are required.  no need to switch states between DELEGATED and ENDING_DELEGATED, when delegation is set to end, it s fine to just have end_date storage slot and make assign the date there when undelegate function is called.  unlocking funds from delegation (or not accepted request) can be explicit.  Also see issue 5.19 for other suggestions regarding getState usage in the code  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "5.17 _endingDelegations list is redundant    Addressed",
        "body": "  Resolution   Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  _endingDelegations is a list of delegations that is created for optimisation purposes. But the only place it s used is in getPurchasedAmount function, so only a subset of all delegations is going to be updated.  code/contracts/delegation/TokenState.sol:L159-L164  function getPurchasedAmount(address holder) public returns (uint amount) {  // check if any delegation was ended  for (uint i = 0; i < _endingDelegations[holder].length; ++i) {  getState(_endingDelegations[holder][i]);  return _purchased[holder];  But getPurchasedAmount function is mostly used after iterating over all delegations of the holder.  Recommendation  Remove _endingDelegations and switch to a mechanism that does not require looping through delegations list of potentially unlimited size.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "5.18 Some functions are defined but not implemented    Addressed",
        "body": "  Resolution   Fixed by removing the empty functions and implementing some others in   SKALE-2160. At the time of the writing this comment, the review has not been comprehensive to all functions in the scope.  Description  There are many functions that are defined but not implemented. They have a revert with a message as not implemented.  This results in complex code and reduces readability. Here is a some of these functions within the scope of this audit:  DelegationService.setMinimumStakingRequirement()  DelegationService.getAllDelegationRequests()  DelegationService.getDelegationRequestsForValidator()  DelegationService.listDelegationRequests()  DelegationService.getDelegationRequestsForValidator() Many more functions in DelegationService.sol  Examples  code/contracts/delegation/DelegationService.sol:L152-L158  function getAllDelegationRequests() external returns(uint[] memory) {  revert(\"Not implemented\");  function getDelegationRequestsForValidator(uint validatorId) external returns (uint[] memory) {  revert(\"Not implemented\");  Recommendation  If these functions are needed for this release, they must be implemented. If they are for future plan, it s better to remove the extra code in the smart contracts.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "5.19 tokenState.setState redundant checks    Addressed",
        "body": "  Resolution   Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  tokenState.setState is used to change the state of the token from:  PROPOSED to ACCEPTED (in accept())  DELEGATED to ENDING_DELEGATED (in requestUndelegation()  The if/else statement in setState is too complicated and can be simplified, both to optimize gas usage and to increase readability.  Examples  code/contracts/delegation/TokenState.sol:L173-L197  function setState(uint delegationId, State newState) internal {  TimeHelpers timeHelpers = TimeHelpers(contractManager.getContract(\"TimeHelpers\"));  DelegationController delegationController = DelegationController(contractManager.getContract(\"DelegationController\"));  require(newState != State.PROPOSED, \"Can't set state to proposed\");  if (newState == State.ACCEPTED) {  State currentState = getState(delegationId);  require(currentState == State.PROPOSED, \"Can't set state to accepted\");  _state[delegationId] = State.ACCEPTED;  _timelimit[delegationId] = timeHelpers.getNextMonthStart();  } else if (newState == State.DELEGATED) {  revert(\"Can't set state to delegated\");  } else if (newState == State.ENDING_DELEGATED) {  require(getState(delegationId) == State.DELEGATED, \"Can't set state to ending delegated\");  DelegationController.Delegation memory delegation = delegationController.getDelegation(delegationId);  _state[delegationId] = State.ENDING_DELEGATED;  _timelimit[delegationId] = timeHelpers.calculateDelegationEndTime(delegation.created, delegation.delegationPeriod, 3);  _endingDelegations[delegation.holder].push(delegationId);  } else {  revert(\"Unknown state\");  Recommendation  Some of the changes that do not change the functionality of the setState function:  Remove reverts() and add the valid states to the require() at the beginning of the function  Remove multiple calls to getState()  Remove final else/revert as this is an internal function and States passed should be valid More optimization can be done which requires further understanding of the system and the state machine.  function setState(uint delegationId, State newState) internal {  TimeHelpers timeHelpers = TimeHelpers(contractManager.getContract(\"TimeHelpers\"));  DelegationController delegationController = DelegationController(contractManager.getContract(\"DelegationController\"));  require(newState != State.PROPOSED || newState != State.DELEGATED, \"Invalid state change\");  State currentState = getState(delegationId);  if (newState == State.ACCEPTED) {  require(currentState == State.PROPOSED, \"Can't set state to accepted\");  _state[delegationId] = State.ACCEPTED;  _timelimit[delegationId] = timeHelpers.getNextMonthStart();  } else if (newState == State.ENDING_DELEGATED) {  require(currentState == State.DELEGATED, \"Can't set state to ending delegated\");  DelegationController.Delegation memory delegation = delegationController.getDelegation(delegationId);  _state[delegationId] = State.ENDING_DELEGATED;  _timelimit[delegationId] = timeHelpers.calculateDelegationEndTime(delegation.created, delegation.delegationPeriod, 3);  _endingDelegations[delegation.holder].push(delegationId);  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "5.20 Validator should be able to remove delegator    Addressed",
        "body": "  Resolution   Code added in   SKALE-2162, If the delegation is not in  Description  In order to delegate tokens to a validator, the validator should accept the delegation request, however it s not possible to remove the delegator for the next period.  Recommendation  For consistency, either allow a validator to undelegate delegators for the next period or remove acceptance mechanism if it s not needed.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "5.21 Lack of logs and events on state changes   Pending",
        "body": "  Resolution  Skale team acknowledged this issue and will address this in future versions, but given the minor level and need to begin remediation, it s left out of scope from the re-mediation tag.  Description  Events in Solidity are used to log major state changes in the system, as for tracebility and also trigger UI changes or user notifications. It is a good practice to use events for every value storage change to be able to trace back the system.  Recommendation  emit events whenever a state change happens. As an example slashing does not emit any events and cannot notify a user unless a service is polling the system state regularly.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "5.22 DelegationService redundancy    Addressed",
        "body": "  Resolution  pull/114 and the functionality is distributed in  Description  DelegationService acts as a gateway for every external call. The problem is that it adds extra complexity to the code, which makes it harder to read and add a new code. Also, it costs more gas because of extra calls between contracts.  Recommendation  The same functionality of DelegationService can be added through UI to allow direct calls to each contract. However, as the whole system is modular and upgradable, it is understandable why using one main contract as the point of interaction might make sense as well.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "5.23 Add timelock for some onlyOwner functions   Pending",
        "body": "  Resolution  Skale team acknowledged this issue and gave us the following response:  The SKALE Network Upgrade key will soon transition to an on-chain voting mechanism therefore making the ownership a function of community governance. It will be centrally managed through a multi-sig process for the initial 3 months to prioritize agility for resolving critical issues prior to becoming a community owned on-chain function. Successful Ethereum projects such as Maker have given clear data points on successful voting mechanism and community control which the SKALE Network will employ as soon as possible.  Description  The system is trusted in a way that there are some owners have the power to do major changes in the system. The most powerful is owner of ContractManager which can update any contract in any way. Even though the system is trusted and this is intended behaviour, it s possible to mitigate this trust a bit.  Recommendation  Add timelock to major admin functions, so people would know about it beforehand (2 weeks before) and would be able to react somehow.  Severity is minor because if owners of SKALE would want to attack the system in that way, tokens would lose the value anyway, and security of SKALE chains would be unreliable. So it s unclear what can be done even having that knowledge beforehand.  6 Mitigation issues  This section lists the issues found in the mitigation phase. The audit team, reviewed the code fixes after the initial report was delivered,  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "6.1 Users can burn delegated tokens using re-entrancy attack    Addressed",
        "body": "  Resolution   Mitigated in   skalenetwork/skale-manager#128  Description  When a user burns tokens, the following code is called:  new_code/contracts/ERC777/LockableERC777.sol:L413-L426  uint locked = _getAndUpdateLockedAmount(from);  if (locked > 0) {  require(_balances[from] >= locked.add(amount), \"Token should be unlocked for burning\");  //-------------------------------------------------------------------------  _callTokensToSend(  operator, from, address(0), amount, data, operatorData  );  // Update state variables  _totalSupply = _totalSupply.sub(amount);  _balances[from] = _balances[from].sub(amount);  There is a callback function right after the check that there are enough unlocked tokens to burn. In this callback, the user can delegate all the tokens right before burning them without breaking the code flow.  Recommendation  _callTokensToSend  should be called before checking for the unlocked amount of tokens, which is better defined as Checks-Effects-Interactions Pattern.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "6.2 Rounding errors after slashing    Addressed",
        "body": "  Resolution   Mitigated in   skalenetwork/skale-manager#130.  Description  When slashing happens _delegatedToValidator and _effectiveDelegatedToValidator values are reduced.  new_code/contracts/delegation/DelegationController.sol:L349-L355  function confiscate(uint validatorId, uint amount) external {  uint currentMonth = getCurrentMonth();  Fraction memory coefficient = reduce(_delegatedToValidator[validatorId], amount, currentMonth);  reduce(_effectiveDelegatedToValidator[validatorId], coefficient, currentMonth);  putToSlashingLog(_slashesOfValidator[validatorId], coefficient, currentMonth);  _slashes.push(SlashingEvent({reducingCoefficient: coefficient, validatorId: validatorId, month: currentMonth}));  When holders process slashings, they reduce _delegatedByHolderToValidator, _delegatedByHolder, _effectiveDelegatedByHolderToValidator values.  new_code/contracts/delegation/DelegationController.sol:L892-L904  if (oldValue > 0) {  reduce(  _delegatedByHolderToValidator[holder][validatorId],  _delegatedByHolder[holder],  _slashes[index].reducingCoefficient,  month);  reduce(  _effectiveDelegatedByHolderToValidator[holder][validatorId],  _slashes[index].reducingCoefficient,  month);  slashingSignals[index.sub(begin)].holder = holder;  slashingSignals[index.sub(begin)].penalty = oldValue.sub(getAndUpdateDelegatedByHolderToValidator(holder, validatorId, month));  Also when holders are undelegating, they are calculating how many tokens from delegations[delegationId].amount were slashed.  new_code/contracts/delegation/DelegationController.sol:L316  uint amountAfterSlashing = calculateDelegationAmountAfterSlashing(delegationId);  If rounding error reduces amount not that much as other values, we can have uint underflow. This is especially dangerous because all calculations are delayed and we will know about underflow and SafeMath revert in the next month or later. Developers already made sure that rounding errors are aligned in a correct way, and that the reduced value should always be larger than the subtracted, so there should not be underflow. This solution is very unstable because it s hard to verify it and keep in mind even during a small code change.  If rounding errors make amount smaller then it should be, when other values should be zero (for example, when all the delegations are undelegated), these values will become some very small values. The problem here is that it would be impossible to compare values to zero.  Recommendation  Consider not calling revert on these subtractions and make result value be equals to zero if underflow happens.  Consider comparing to some small epsilon value instead of zero. Or similar to the previous point, on every subtraction check if the value is smaller then epsilon, and make it zero if it is.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "6.3 Slashes do not affect bounty distribution    Addressed",
        "body": "  Resolution   Mitigated in   skalenetwork/skale-manager#118  Description  When slashes are processed by a holder, only _delegatedByHolderToValidator and _delegatedByHolder values are reduced. But _effectiveDelegatedByHolderToValidator value remains the same. This value is used to distribute bounties amongst delegators. So slashing will not affect that distribution.  contracts/delegation/DelegationController.sol:L863-L873  uint oldValue = getAndUpdateDelegatedByHolderToValidator(holder, validatorId);  if (oldValue > 0) {  uint month = _slashes[index].month;  reduce(  _delegatedByHolderToValidator[holder][validatorId],  _delegatedByHolder[holder],  _slashes[index].reducingCoefficient,  month);  slashingSignals[index.sub(begin)].holder = holder;  slashingSignals[index.sub(begin)].penalty = oldValue.sub(getAndUpdateDelegatedByHolderToValidator(holder, validatorId));  Recommendation  Reduce _effectiveDelegatedByHolderToValidator and _effectiveDelegatedToValidator when slashes are processed.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "6.4 Iterations over slashes    Addressed",
        "body": "  Resolution   Partially mitigated in   skalenetwork/skale-manager#163 .  Description  Every user should iterate over each slash (but only once) and process them in order to determine whether this slash impacted his delegations or not.  However, the check is done during almost every action that the user does because it updates the current state of the user s balance. The downside of this method is that if there are a lot of slashes in the system, every user would be forced to iterate over all of them even if the user is only trading tokens and only calls transfer function.  If the number of slashes is huge, checking them all in one function would impossible due to the block gas limit. It s possible to call the checking function separately and process slashes in batches. So this attack should not result in system halt and can be mitigated with manual intervention.  Also, there are two separate pipelines for iterating over slashes. One pipeline is for iterating over months to determine amount of slashed tokens in separate delegations. This one can potentially hit gas limit in many-many years. The other one is for modifying aggregated delegation values.  Recommendation  Try to avoid all the unnecessary iterations over a potentially unlimited number of items. Additionally, it s possible to optimize some calculations:  When slashing signals are processed, all of them always have the same holder. There s no reason for having an array of signals with the same holder (always with predefined length and values will most likely be zero). It seems possible to remove signals functionality and just aggregate the changes for the Punisher.  Try merge two pipelines into one.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "6.5 Storage operations optimization    Addressed",
        "body": "  Resolution   Mitigated in   skalenetwork/skale-manager#179  Description  There are a lot of operations that write some value to the storage (uses SSTORE opcode) without actually changing it.  Examples  In getAndUpdateValue  function of DelegationController and TokenLaunchLocker:  new_code/contracts/delegation/DelegationController.sol:L711-L715  for (uint i = sequence.firstUnprocessedMonth; i <= month; ++i) {  sequence.value = sequence.value.add(sequence.addDiff[i]).sub(sequence.subtractDiff[i]);  delete sequence.addDiff[i];  delete sequence.subtractDiff[i];  In handleSlash function of Punisher contract amount will be zero in most cases:  new_code/contracts/delegation/Punisher.sol:L66-L68  function handleSlash(address holder, uint amount) external allow(\"DelegationController\") {  _locked[holder] = _locked[holder].add(amount);  Recommendation  Check if the value is the same and don t write it to the storage in that case.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "6.6 Duplicate function implementation addMonths()    Addressed",
        "body": "  Resolution   Fixed in   skalenetwork/skale-manager#127  Description  TimeHelpers.addMonths() implementation is redundant as it can directly use BokkyPooBahsDateTimeLibrary.addMonths() function.  Recommendation  Simply use return BokkyPooBahsDateTimeLibrary.addMonths() on the same function to prevent further code changes, it s still a good idea to call addMonth through TimeHelpers contract.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "6.7 Function overloading    Addressed",
        "body": "  Resolution   Fixed in   skalenetwork/skale-manager#181  Description  Some functions in the codebase are overloaded. That makes code less readable and increases the probability of missing bugs.  For example, there are a lot of reduce function implementations in DelegationController:  new_code/contracts/delegation/DelegationController.sol:L722-L820  function reduce(PartialDifferencesValue storage sequence, uint amount, uint month) internal returns (Fraction memory) {  require(month.add(1) >= sequence.firstUnprocessedMonth, \"Can't reduce value in the past\");  if (sequence.firstUnprocessedMonth == 0) {  return createFraction(0);  uint value = getAndUpdateValue(sequence, month);  if (value == 0) {  return createFraction(0);  uint _amount = amount;  if (value < amount) {  _amount = value;  Fraction memory reducingCoefficient = createFraction(value.sub(_amount), value);  reduce(sequence, reducingCoefficient, month);  return reducingCoefficient;  function reduce(PartialDifferencesValue storage sequence, Fraction memory reducingCoefficient, uint month) internal {  reduce(  sequence,  sequence,  reducingCoefficient,  month,  false);  function reduce(  PartialDifferencesValue storage sequence,  PartialDifferencesValue storage sumSequence,  Fraction memory reducingCoefficient,  uint month) internal  reduce(  sequence,  sumSequence,  reducingCoefficient,  month,  true);  function reduce(  PartialDifferencesValue storage sequence,  PartialDifferencesValue storage sumSequence,  Fraction memory reducingCoefficient,  uint month,  bool hasSumSequence) internal  require(month.add(1) >= sequence.firstUnprocessedMonth, \"Can't reduce value in the past\");  if (hasSumSequence) {  require(month.add(1) >= sumSequence.firstUnprocessedMonth, \"Can't reduce value in the past\");  require(reducingCoefficient.numerator <= reducingCoefficient.denominator, \"Increasing of values is not implemented\");  if (sequence.firstUnprocessedMonth == 0) {  return;  uint value = getAndUpdateValue(sequence, month);  if (value == 0) {  return;  uint newValue = sequence.value.mul(reducingCoefficient.numerator).div(reducingCoefficient.denominator);  if (hasSumSequence) {  subtract(sumSequence, sequence.value.sub(newValue), month);  sequence.value = newValue;  for (uint i = month.add(1); i <= sequence.lastChangedMonth; ++i) {  uint newDiff = sequence.subtractDiff[i].mul(reducingCoefficient.numerator).div(reducingCoefficient.denominator);  if (hasSumSequence) {  sumSequence.subtractDiff[i] = sumSequence.subtractDiff[i].sub(sequence.subtractDiff[i].sub(newDiff));  sequence.subtractDiff[i] = newDiff;  function reduce(  PartialDifferences storage sequence,  Fraction memory reducingCoefficient,  uint month) internal  require(month.add(1) >= sequence.firstUnprocessedMonth, \"Can't reduce value in the past\");  require(reducingCoefficient.numerator <= reducingCoefficient.denominator, \"Increasing of values is not implemented\");  if (sequence.firstUnprocessedMonth == 0) {  return;  uint value = getAndUpdateValue(sequence, month);  if (value == 0) {  return;  sequence.value[month] = sequence.value[month].mul(reducingCoefficient.numerator).div(reducingCoefficient.denominator);  for (uint i = month.add(1); i <= sequence.lastChangedMonth; ++i) {  sequence.subtractDiff[i] = sequence.subtractDiff[i].mul(reducingCoefficient.numerator).div(reducingCoefficient.denominator);  Recommendation  Avoid function overloading as a general guideline.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"
    },
    {
        "title": "6.1 Exchange - CancelOrder has no effect   Pending",
        "body": "  Resolution  This issue has been addressed with mai-protocol-v2/2fcbf4b44f4595e5879ff5efea4e42c529ef0ce1 by verifying that an order has not been cancelled in method validateOrderParam.  cancelOrder still does not verify the order signature.  Description  The exchange provides means for the trader or broker to cancel the order. The cancelOrder method, however, only stores the hash of the canceled order in mapping but the mapping is never checked. It is therefore effectively impossible for a trader to cancel an order.  Examples  code/contracts/exchange/Exchange.sol:L179-L187  function cancelOrder(LibOrder.Order memory order) public {  require(msg.sender == order.trader || msg.sender == order.broker, \"invalid caller\");  bytes32 orderHash = order.getOrderHash();  cancelled[orderHash] = true;  emit Cancel(orderHash);  Recommendation  matchOrders* or validateOrderParam should check if cancelled[orderHash] == true and abort fulfilling the order.  Verify the order params (Signature) before accepting it as canceled.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.2 AMM - funding can be called in emergency mode   Pending",
        "body": "  Resolution   This issue was addressed by silently skipping   Description  specification for  Recommendation  According to the specification, forceFunding should not be allowed in EMERGENCY mode. However, it is assumed that this method should only be callable in NORMAL mode.  The assessment team would like to note that the specification appears to be inconsistent and dated (method names, variable names, \u2026).  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.3 Perpetual - withdraw should only be available in NORMAL state   Pending",
        "body": "  Resolution   This issue was resolved by requiring   Description  According to the specification withdraw can only be called in NORMAL state. However, the implementation allows it to be called in NORMAL and SETTLED mode.  Examples  Withdraw only checks for !SETTLING state which resolves to NORMAL and SETTLED.  code/contracts/perpetual/Perpetual.sol:L175-L178  function withdraw(uint256 amount) public {  withdrawFromAccount(msg.sender, amount);  code/contracts/perpetual/Perpetual.sol:L156-L169  function withdrawFromAccount(address payable guy, uint256 amount) private {  require(guy != address(0), \"invalid guy\");  require(status != LibTypes.Status.SETTLING, \"wrong perpetual status\");  uint256 currentMarkPrice = markPrice();  require(isSafeWithPrice(guy, currentMarkPrice), \"unsafe before withdraw\");  remargin(guy, currentMarkPrice);  address broker = currentBroker(guy);  bool forced = broker == address(amm.perpetualProxy()) || broker == address(0);  withdraw(guy, amount, forced);  require(isSafeWithPrice(guy, currentMarkPrice), \"unsafe after withdraw\");  require(availableMarginWithPrice(guy, currentMarkPrice) >= 0, \"withdraw margin\");  In contrast, withdrawFor requires the state to be NORMAL:  code/contracts/perpetual/Perpetual.sol:L171-L174  function withdrawFor(address payable guy, uint256 amount) public onlyWhitelisted {  require(status == LibTypes.Status.NORMAL, \"wrong perpetual status\");  withdrawFromAccount(guy, amount);  Recommendation  withdraw should only be available in the NORMAL operation mode.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.4 Perpetual - withdrawFromInsuranceFund should check wadAmount instead of rawAmount   Pending",
        "body": "  Resolution   This issue was addressed by checking   Description  withdrawFromInsurance checks that enough funds are in the insurance fund before allowing withdrawal by an admin by checking the provided rawAmount <= insuranceFundBalance.toUint256(). rawAmount is the ETH (18 digit precision) or collateral token amount (can be less than 18 digit precision) to be withdrawn while insuranceFundBalance is a WAD-denominated value (18 digit precision).  The check does not hold if the configured collateral has different precision and may have unwanted consequences, e.g. the withdrawal of more funds than expected.  Note: there is another check for insuranceFundBalance staying positive after the potential external call to collateral.  Examples  code/contracts/perpetual/Perpetual.sol:L204-L216  function withdrawFromInsuranceFund(uint256 rawAmount) public onlyWhitelistAdmin {  require(rawAmount > 0, \"invalid amount\");  require(insuranceFundBalance > 0, \"insufficient funds\");  require(rawAmount <= insuranceFundBalance.toUint256(), \"insufficient funds\");  int256 wadAmount = toWad(rawAmount);  insuranceFundBalance = insuranceFundBalance.sub(wadAmount);  withdrawFromProtocol(msg.sender, rawAmount);  require(insuranceFundBalance >= 0, \"negtive insurance fund\");  emit UpdateInsuranceFund(insuranceFundBalance);  When looking at the test-cases there seems to be a misconception about what unit of amount withdrawFromInsuranceFund is taking. For example, the insurance fund withdrawal and deposit are not tested for collateral that specifies a precision that is not 18. The test-cases falsely assume that the input to withdrawFromInsuranceFund is a WAD value, while it is taking the collateral s rawAmount which is then converted to a WAD number.  code/test/test_perpetual.js:L471-L473  await perpetual.withdrawFromInsuranceFund(toWad(10.111));  fund = await perpetual.insuranceFundBalance();  assert.equal(fund.toString(), 0);  Recommendation  Check that require(wadAmount <= insuranceFundBalance.toUint256(), \"insufficient funds\");, add a test-suite testing the insurance fund with collaterals with different precision and update existing tests that properly provide the expected input to withdraFromInsurance.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.5 Perpetual - liquidateFrom should not have public visibility   Pending",
        "body": "  Resolution   This issue has been resolved by removing the   Description  Perpetual.liquidate is used to liquidate an account that is  unsafe,  determined by the relative sizes of marginBalanceWithPrice and maintenanceMarginWithPrice:  code/contracts/perpetual/Perpetual.sol:L248-L253  // safe for liquidation  function isSafeWithPrice(address guy, uint256 currentMarkPrice) public returns (bool) {  return  marginBalanceWithPrice(guy, currentMarkPrice) >=  maintenanceMarginWithPrice(guy, currentMarkPrice).toInt256();  Perpetual.liquidate allows the caller to assume the liquidated account s position, as well as a small amount of  penalty collateral.  The steps to liquidate are, roughly:  Close the liquidated account s position  Perform a trade on the liquidated assets with the liquidator acting as counter-party  Grant the liquidator a portion of the liquidated assets as a reward. An additional portion is added to the insurance fund.  Handle any losses  We found several issues in Perpetual.liquidate:  Examples  liquidateFrom has public visibility:  code/contracts/perpetual/Perpetual.sol:L270  function liquidateFrom(address from, address guy, uint256 maxAmount) public returns (uint256, uint256) {  Given that liquidate only calls liquidateFrom after checking the current contract s status, this oversight allows anyone to call liquidateFrom during the SETTLED stage:  code/contracts/perpetual/Perpetual.sol:L291-L294  function liquidate(address guy, uint256 maxAmount) public returns (uint256, uint256) {  require(status != LibTypes.Status.SETTLED, \"wrong perpetual status\");  return liquidateFrom(msg.sender, guy, maxAmount);  Additionally, directly calling liquidateFrom allows anyone to liquidate on behalf of other users, forcing other accounts to assume liquidated positions.  Finally, neither liquidate nor liquidateFrom check that the liquidated account and liquidator are the same. Though the liquidation accounting process is hard to follow, we believe this is unintended and could lead to large errors in internal contract accounting.  Recommendation  Make liquidateFrom an internal function  In liquidate or liquidateFrom, check that msg.sender != guy  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.6 Unpredictable behavior due to front running or general bad timing   Pending",
        "body": "  Resolution  This issue was addressed by the client providing the following statement:  Not fixed in the perpetual. But later a voting system will take over the administration key. We intent to add a waiting period before voted changes applying.  Description  In a number of cases, administrators of contracts can update or upgrade things in the system without warning. This has the potential to violate a security goal of the system.  Specifically, privileged roles could use front running to make malicious changes just ahead of incoming transactions, or purely accidental negative effects could occur due to unfortunate timing of changes.  Some instances of this are more important than others, but in general users of the system should have assurances about the behavior of the action they re about to take.  Examples  Updating governance and global configuration parameters are not protected by a time-lock and take effect immediately. This, therefore, creates an opportunity for administrators to front-run users on the exchange by changing parameters for orders. It may also allow an administrator to temporarily lift restrictions for themselves (e.g. withdrawalLockBlockCount).  GlobalConfig  withdrawalLockBlockCount is queried when applying for withdrawal. This value can be set zero enabling allowing immediate withdrawal. brokerLockBlockCount is queried when setting a new broker. This value can e set to zero effectively enabling immediate broker changes.  code/contracts/global/GlobalConfig.sol:L18-L27  function setGlobalParameter(bytes32 key, uint256 value) public onlyWhitelistAdmin {  if (key == \"withdrawalLockBlockCount\") {  withdrawalLockBlockCount = value;  } else if (key == \"brokerLockBlockCount\") {  brokerLockBlockCount = value;  } else {  revert(\"key not exists\");  emit UpdateGlobalParameter(key, value);  PerpetualGovernance  e.g. Admin can front-run specific matchOrder calls and set arbitrary dev fees or curve parameters\u2026  code/contracts/perpetual/PerpetualGovernance.sol:L39-L80  function setGovernanceParameter(bytes32 key, int256 value) public onlyWhitelistAdmin {  if (key == \"initialMarginRate\") {  governance.initialMarginRate = value.toUint256();  require(governance.initialMarginRate > 0, \"require im > 0\");  require(governance.initialMarginRate < 10**18, \"require im < 1\");  require(governance.maintenanceMarginRate < governance.initialMarginRate, \"require mm < im\");  } else if (key == \"maintenanceMarginRate\") {  governance.maintenanceMarginRate = value.toUint256();  require(governance.maintenanceMarginRate > 0, \"require mm > 0\");  require(governance.maintenanceMarginRate < governance.initialMarginRate, \"require mm < im\");  require(governance.liquidationPenaltyRate < governance.maintenanceMarginRate, \"require lpr < mm\");  require(governance.penaltyFundRate < governance.maintenanceMarginRate, \"require pfr < mm\");  } else if (key == \"liquidationPenaltyRate\") {  governance.liquidationPenaltyRate = value.toUint256();  require(governance.liquidationPenaltyRate < governance.maintenanceMarginRate, \"require lpr < mm\");  } else if (key == \"penaltyFundRate\") {  governance.penaltyFundRate = value.toUint256();  require(governance.penaltyFundRate < governance.maintenanceMarginRate, \"require pfr < mm\");  } else if (key == \"takerDevFeeRate\") {  governance.takerDevFeeRate = value;  } else if (key == \"makerDevFeeRate\") {  governance.makerDevFeeRate = value;  } else if (key == \"lotSize\") {  require(  governance.tradingLotSize == 0 || governance.tradingLotSize.mod(value.toUint256()) == 0,  \"require tls % ls == 0\"  );  governance.lotSize = value.toUint256();  } else if (key == \"tradingLotSize\") {  require(governance.lotSize == 0 || value.toUint256().mod(governance.lotSize) == 0, \"require tls % ls == 0\");  governance.tradingLotSize = value.toUint256();  } else if (key == \"longSocialLossPerContracts\") {  require(status == LibTypes.Status.SETTLING, \"wrong perpetual status\");  socialLossPerContracts[uint256(LibTypes.Side.LONG)] = value;  } else if (key == \"shortSocialLossPerContracts\") {  require(status == LibTypes.Status.SETTLING, \"wrong perpetual status\");  socialLossPerContracts[uint256(LibTypes.Side.SHORT)] = value;  } else {  revert(\"key not exists\");  emit UpdateGovernanceParameter(key, value);  Admin can set devAddress or even update to a new amm and globalConfig  code/contracts/perpetual/PerpetualGovernance.sol:L82-L94  function setGovernanceAddress(bytes32 key, address value) public onlyWhitelistAdmin {  require(value != address(0x0), \"invalid address\");  if (key == \"dev\") {  devAddress = value;  } else if (key == \"amm\") {  amm = IAMM(value);  } else if (key == \"globalConfig\") {  globalConfig = IGlobalConfig(value);  } else {  revert(\"key not exists\");  emit UpdateGovernanceAddress(key, value);  AMMGovernance  code/contracts/liquidity/AMMGovernance.sol:L22-L43  function setGovernanceParameter(bytes32 key, int256 value) public onlyWhitelistAdmin {  if (key == \"poolFeeRate\") {  governance.poolFeeRate = value.toUint256();  } else if (key == \"poolDevFeeRate\") {  governance.poolDevFeeRate = value.toUint256();  } else if (key == \"emaAlpha\") {  require(value > 0, \"alpha should be > 0\");  governance.emaAlpha = value;  emaAlpha2 = 10**18 - governance.emaAlpha;  emaAlpha2Ln = emaAlpha2.wln();  } else if (key == \"updatePremiumPrize\") {  governance.updatePremiumPrize = value.toUint256();  } else if (key == \"markPremiumLimit\") {  governance.markPremiumLimit = value;  } else if (key == \"fundingDampener\") {  governance.fundingDampener = value;  } else {  revert(\"key not exists\");  emit UpdateGovernanceParameter(key, value);  Recommendation  The underlying issue is that users of the system can t be sure what the behavior of a function call will be, and this is because the behavior can change at any time.  We recommend giving the user advance notice of changes with a time lock. For example, make all updates to system parameters or upgrades require two steps with a mandatory time window between them. The first step merely broadcasts to users that a particular change is coming, and the second step commits that change after a suitable waiting period.  Additionally, users should verify the whitelist setup before using the contract system and monitor it for new additions to the whitelist. Documentation should clearly outline what roles are owned by whom to support suitability. Sane parameter bounds should be enforced (e.g. min. disallow block delay of zero )  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.7 AMM - Governance is able to set an invalid alpha value   Pending",
        "body": "  Resolution   This issue was addressed by checking that the provided   Description  According to https://en.wikipedia.org/wiki/Moving_average  The coefficient \u03b1 represents the degree of weighting decrease, a constant smoothing factor between 0 and 1. A higher \u03b1 discounts older observations faster.  However, the code does not check upper bounds. An admin may, therefore, set an invalid alpha that puts emaAlpha2 out of bounds or negative.  Examples  code/contracts/liquidity/AMMGovernance.sol:L27-L31  } else if (key == \"emaAlpha\") {  require(value > 0, \"alpha should be > 0\");  governance.emaAlpha = value;  emaAlpha2 = 10**18 - governance.emaAlpha;  emaAlpha2Ln = emaAlpha2.wln();  Recommendation  Ensure that the system configuration is always within safe bounds. Document expected system variable types and their safe operating ranges. Enforce that bounds are checked every time a value is set. Enforce safe defaults when deploying contracts.  Ensure emaAlpha is 0 < value < 1 WAD  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.8 AMM - Amount of collateral spent or shares received may be unpredictable for liquidity provider   ",
        "body": "  Resolution  The client acknowledges this issue without providing further information or implementing the recommended fixes.  Description  When providing liquidity with addLiquidity(), the amount of collateral required is based on the current price and the amount of shares received depends on the total amount of shares in circulation. This price can fluctuate at a moment s notice, making the behavior of the function unpredictable for the user.  The same is true when removing liquidity via removeLiquidity().  Recommendation  Unpredictability can be introduced by someone front-running the transaction, or simply by poor timing. For example, adjustments to global variable configuration by the system admin will directly impact subsequent actions by the user. In order to ensure users know what to expect:  Allow the caller to specify a price limit or maximum amount of collateral to be spent  Allow the caller to specify the minimum amount of shares expected to be received  ",
        "labels": [
            "Consensys",
            "Medium",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.9 Exchange - insufficient input validation in matchOrders   Pending",
        "body": "  Resolution   This issue was addressed by following the recommendation to verify that   Description  Additionally, the method allows the sender to provide no makerOrderParams at all, resulting in no state changes.  matchOrders also does not reject trades with an amount set to zero. Such orders should be rejected because they do not comply with the minimum tradingLotSize configured for the system. As a side-effect, events may be emitted for zero-amount trades and unexpected state changes may occur.  Examples  code/contracts/exchange/Exchange.sol:L34-L39  function matchOrders(  LibOrder.OrderParam memory takerOrderParam,  LibOrder.OrderParam[] memory makerOrderParams,  address _perpetual,  uint256[] memory amounts  ) public {  code/contracts/exchange/Exchange.sol:L113-L113  function matchOrderWithAMM(LibOrder.OrderParam memory takerOrderParam, address _perpetual, uint256 amount) public {  Recommendation  Require makerOrderParams.length > 0 && amounts.length == makerOrderParams.length  Require that amount or any of the amounts[i] provided to matchOrders is >=tradingLotSize.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.10 AMM - Liquidity provider may lose up to lotSize when removing liquidity   ",
        "body": "  Resolution  The client acknowledges this issue without providing further information.  Description  When removing liquidity, the amount of collateral received is calculated from the shareAmount (ShareToken) of the liquidity provider. The liquidity removal process registers a trade on the amount, with the liquidity provider and AMM taking opposite sides. Because trading only accepts multiple of the lotSize, the leftover is discarded. The amount discarded may be up to lotSize - 1.  The expectation is that this value should not be too high, but as lotSize can be set to arbitrary values by an admin, it is possible that this step discards significant value. Additionally, see issue 6.6 for how this can be exploited by an admin.  Note that similar behavior is present in Perpetual.liquidateFrom, where the liquidatableAmount calculated undergoes a similar modulo operation:  code/contracts/perpetual/Perpetual.sol:L277-L278  uint256 liquidatableAmount = totalPositionSize.sub(totalPositionSize.mod(governance.lotSize));  liquidationAmount = liquidationAmount.ceil(governance.lotSize).min(maxAmount).min(liquidatableAmount);  Examples  lotSize can arbitrarily be set up to pos_int256_max as long as tradingLotSize % lotSize == 0  code/contracts/perpetual/PerpetualGovernance.sol:L61-L69  } else if (key == \"lotSize\") {  require(  governance.tradingLotSize == 0 || governance.tradingLotSize.mod(value.toUint256()) == 0,  \"require tls % ls == 0\"  );  governance.lotSize = value.toUint256();  } else if (key == \"tradingLotSize\") {  require(governance.lotSize == 0 || value.toUint256().mod(governance.lotSize) == 0, \"require tls % ls == 0\");  governance.tradingLotSize = value.toUint256();  amount is derived from shareAmount rounded down to the next multiple of the lotSize. The leftover is discarded.  code/contracts/liquidity/AMM.sol:L289-L294  uint256 amount = shareAmount.wmul(oldPoolPositionSize).wdiv(shareToken.totalSupply());  amount = amount.sub(amount.mod(perpetualProxy.lotSize()));  perpetualProxy.transferBalanceOut(trader, price.wmul(amount).mul(2));  burnShareTokenFrom(trader, shareAmount);  uint256 opened = perpetualProxy.trade(trader, LibTypes.Side.LONG, price, amount);  Recommendation  Ensure that documentation makes users aware of the fact that they may lose up to lotsize-1 in value.  Alternatively, track accrued value and permit trades on values that exceed lotSize. Note that this may add significant complexity.  Ensure that similar system behavior, like the liquidatableAmount calculated in Perpetual.liquidateFrom, is also documented and communicated clearly to users.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.11 Oracle - Unchecked oracle response timestamp and integer over/underflow    ",
        "body": "  Resolution  This issue was resolved by following the recommendations,  using LibMath for arithmetic operations to guard against over/underflows,  checking that newPrice != 0  verifying that the timestamp is within a configurable range,  duplicating the code and combining the reverse oracle into one contract  The assessment team would like to note that the acceptable time-frame for answers can vary, the price may be outdated, and it is totally up to the deployer to configure the acceptable timeout. The timeout can be changed by the account deploying the oracle feed without a delay allowing the price-feed owner to arbitrarily make calls to AMM.indexPrice fail (front-running). A timeout may be set to an arbitrarily high value to bypass the check. User s of the system are advised to validate that they trust the account operating the feeder and that the timeout is set correctly.  Description  The external Chainlink oracle, which provides index price information to the system, introduces risk inherent to any dependency on third-party data sources. For example, the oracle could fall behind or otherwise fail to be maintained, resulting in outdated data being fed to the index price calculations of the AMM. Oracle reliance has historically resulted in crippled on-chain systems, and complications that lead to these outcomes can arise from things as simple as network congestion.  Ensuring that unexpected oracle return values are properly handled will reduce reliance on off-chain components and increase the resiliency of the smart contract system that depends on them.  Examples  The ChainlinkAdapter and InversedChainlinkAdapter take the oracle s (int256) latestAnswer and convert the result using chainlinkDecimalsAdapter. This arithmetic operation can underflow/overflow if the Oracle provides a large enough answer:  code/contracts/oracle/ChainlinkAdapter.sol:L10-L19  int256 public constant chainlinkDecimalsAdapter = 10**10;  constructor(address _feeder) public {  feeder = IChainlinkFeeder(_feeder);  function price() public view returns (uint256 newPrice, uint256 timestamp) {  newPrice = (feeder.latestAnswer() * chainlinkDecimalsAdapter).toUint256();  timestamp = feeder.latestTimestamp();  code/contracts/oracle/InversedChainlinkAdapter.sol:L11-L20  int256 public constant chainlinkDecimalsAdapter = 10**10;  constructor(address _feeder) public {  feeder = IChainlinkFeeder(_feeder);  function price() public view returns (uint256 newPrice, uint256 timestamp) {  newPrice = ONE.wdiv(feeder.latestAnswer() * chainlinkDecimalsAdapter).toUint256();  timestamp = feeder.latestTimestamp();  The oracle provides a timestamp for the latestAnswer that is not validated and may lead to old oracle timestamps being accepted (e.g. caused by congestion on the blockchain or a directed censorship attack).  code/contracts/oracle/InversedChainlinkAdapter.sol:L19-L20  timestamp = feeder.latestTimestamp();  Recommendation  Use SafeMath for mathematical computations  Verify latestAnswer is within valid bounds (!=0)  Verify latestTimestamp is within accepted bounds (not in the future, was updated within a reasonable amount of time)  Deduplicate code by combining both Adapters into one as the only difference is that the InversedChainlinkAdapter returns ONE.wdiv(price).  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.12 AMM - Liquidity pools can be initialized with zero collateral   Pending",
        "body": "  Resolution  This issue was addressed by checking that amount > 0. The assessment team would like to note that;  The client chose to verify that amount is non-zero when calling createPool instead of requiring a minimum of a lotSize.  The client did not address the issues about removeLiquidity and addLiquidity allowing to remove and add zero liquidity.  Description  createPool can be initialized with amount == 0. Because a subsequent call to initFunding can only happen once, the contract is now initialized with a zero size pool that does not allow any liquidity to be added.  Trying to recover by calling createPool again fails as the funding state is already initialized. The specification also states the following about createPool:  Open asset pool by deposit to AMM. Only available when pool is empty.  This is inaccurate, as createPool can only be called once due to a check in initFunding, but this call may leave the pool empty.  Furthermore, the contract s liquidity management functionality (addLiquidity and removeLiquidity) allows adding zero liquidity (amount == 0) and removing zero shares (shareAmount == 0). As these actions do not change the liquidity of the pool, they should be rejected.  Recommendation  Require a minimum amount lotSize to be provided when creating a Pool and adding liquidity via addLiquidity  Require a minimum amount of shares to be provided when removing liquidity via removeLiquidity  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.13 Perpetual - Administrators can put the system into emergency mode indefinitely   Pending",
        "body": "  Resolution  The client provided the following statement addressing the issue:  It should be solved by voting. Moreover, we add two roles who is able to disable withdrawing /pause the system.  The duration of the emergency phase is still unrestricted.  Description  There is no limitation on how long an administrator can put the Perpetual contract into emergency mode. Users cannot trade or withdraw funds in emergency mode and are effectively locked out until the admin chooses to put the contract in SETTLED mode.  Examples  code/contracts/perpetual/PerpetualGovernance.sol:L96-L101  function beginGlobalSettlement(uint256 price) public onlyWhitelistAdmin {  require(status != LibTypes.Status.SETTLED, \"already settled\");  settlementPrice = price;  status = LibTypes.Status.SETTLING;  emit BeginGlobalSettlement(price);  code/contracts/perpetual/Perpetual.sol:L146-L154  function endGlobalSettlement() public onlyWhitelistAdmin {  require(status == LibTypes.Status.SETTLING, \"wrong perpetual status\");  address guy = address(amm.perpetualProxy());  settleFor(guy);  status = LibTypes.Status.SETTLED;  emit EndGlobalSettlement();  Recommendation  Set a time-lock when entering emergency mode that allows anyone to set the system to SETTLED after a fixed amount of time.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.14 Signed data may be usable cross-chain    ",
        "body": "  Resolution   This issue was addressed by adding the   order data and verifying it as part of  Description  Signed order data may be re-usable cross-chain as the chain-id is not explicitly part of the signed data.  Examples  The signed order data currently includes the EIP712 Domain Name Mai Protocol and the following information:  code/contracts/lib/LibOrder.sol:L23-L48  struct Order {  address trader;  address broker;  address perpetual;  uint256 amount;  uint256 price;  /**  Data contains the following values packed into 32 bytes  \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557  \u2551                    \u2502 length(bytes)   desc                                      \u2551  \u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562  \u2551 version            \u2502 1               order version                             \u2551  \u2551 side               \u2502 1               0: buy (long), 1: sell (short)            \u2551  \u2551 isMarketOrder      \u2502 1               0: limitOrder, 1: marketOrder             \u2551  \u2551 expiredAt          \u2502 5               order expiration time in seconds          \u2551  \u2551 asMakerFeeRate     \u2502 2               maker fee rate (base 100,000)             \u2551  \u2551 asTakerFeeRate     \u2502 2               taker fee rate (base 100,000)             \u2551  \u2551 (d) makerRebateRate\u2502 2               rebate rate for maker (base 100)          \u2551  \u2551 salt               \u2502 8               salt                                      \u2551  \u2551 isMakerOnly        \u2502 1               is maker only                             \u2551  \u2551 isInversed         \u2502 1               is inversed contract                      \u2551  \u2551                    \u2502 8               reserved                                  \u2551  \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d  /  bytes32 data;  Signature verification:  code/contracts/lib/LibSignature.sol:L24-L47  function isValidSignature(OrderSignature memory signature, bytes32 hash, address signerAddress)  internal  pure  returns (bool)  uint8 method = uint8(signature.config[1]);  address recovered;  uint8 v = uint8(signature.config[0]);  if (method == uint8(SignatureMethod.ETH_SIGN)) {  recovered = ecrecover(  keccak256(abi.encodePacked(\"\\x19Ethereum Signed Message:\\n32\", hash)),  v,  signature.r,  signature.s  );  } else if (method == uint8(SignatureMethod.EIP712)) {  recovered = ecrecover(hash, v, signature.r, signature.s);  } else {  revert(\"invalid sign method\");  return signerAddress == recovered;  Recommendation  Include the chain-id in the signature to avoid cross-chain validity of signatures  verify s is within valid bounds to avoid signature malleability  if (uint256(s) > 0x7FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF5D576E7357A4501DDFE92F46681B20A0) {  revert(\"ECDSA: invalid signature 's' value\");  verify v is within valid bounds  if (v != 27 && v != 28) {  revert(\"ECDSA: invalid signature 'v' value\");  return invalid if the result of ecrecover() is 0x0  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.15 Exchange - validateOrderParam does not check against SUPPORTED_ORDER_VERSION    ",
        "body": "  Resolution   This issue was resolved by checking against   Description  validateOrderParam verifies the signature and version of a provided order. Instead of checking against the contract constant SUPPORTED_ORDER_VERSION it, however, checks against a hardcoded version 2 in the method itself.  This might be a problem if SUPPORTED_ORDER_VERSION is seen as the configuration parameter for the allowed version. Changing it would not change the allowed order version for validateOrderParam as this constant literal is never used.  At the time of this audit, however, the SUPPORTED_ORDER_VERSION value equals the hardcoded value in the validateOrderParam method.  Examples  code/contracts/exchange/Exchange.sol:L155-L170  function validateOrderParam(IPerpetual perpetual, LibOrder.OrderParam memory orderParam)  internal  view  returns (bytes32)  address broker = perpetual.currentBroker(orderParam.trader);  require(broker == msg.sender, \"invalid broker\");  require(orderParam.getOrderVersion() == 2, \"unsupported version\");  require(orderParam.getExpiredAt() >= block.timestamp, \"order expired\");  bytes32 orderHash = orderParam.getOrderHash(address(perpetual), broker);  require(orderParam.signature.isValidSignature(orderHash, orderParam.trader), \"invalid signature\");  require(filled[orderHash] < orderParam.amount, \"fullfilled order\");  return orderHash;  Recommendation  Check against SUPPORTED_ORDER_VERSION instead of the hardcoded value 2.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.16 LibMathSigned - wpowi returns an invalid result for a negative exponent   Pending",
        "body": "  Resolution   This issue was addressed by requiring that   here). The method is still lacking proper natspec documentation outlining expected argument types and valid ranges. The client chose not to implement a check to detect the case where a user accidentally provides  Description  LibMathSigned.wpowi(x,n) calculates Wad value x (base) to the power of n (exponent). The exponent is declared as a signed int, however, the method returns wrong results when calculating x ^(-n).  The comment for the wpowi method suggests that n is a normal integer instead of a Wad-denominated value. This, however, is not being enforced.  Examples  LibMathSigned.wpowi(8000000000000000000, 2) = 64000000000000000000  (wrong) LibMathSigned.wpowi(8000000000000000000, -2) = 64000000000000000000  code/contracts/lib/LibMath.sol:L103-L116  // x ^ n  // NOTE: n is a normal integer, do not shift 18 decimals  // solium-disable-next-line security/no-assign-params  function wpowi(int256 x, int256 n) internal pure returns (int256 z) {  z = n % 2 != 0 ? x : _WAD;  for (n /= 2; n != 0; n /= 2) {  x = wmul(x, x);  if (n % 2 != 0) {  z = wmul(z, x);  Recommendation  Make wpowi support negative exponents or use the proper type for n (uint) and reject negative values.  Enforce that the exponent bounds are within sane ranges and less than a Wad to detect potential misuse where someone accidentally provides a Wad value as n.  Add positive and negative unit-tests to fully cover this functionality.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.17 Outdated solidity version and floating pragma   Pending",
        "body": "  Resolution   This issue was addressed by removing the floating pragma and fixing the compiler version to v0.5.15. The assessment team would like to note, that the latest   ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "0.5.17 with 0.5.16 addressing an ABIEncoder issue.",
        "body": "  Description  Using an outdated compiler version can be problematic especially if there are publicly disclosed bugs and issues (see also https://github.com/ethereum/solidity/releases) that affect the current compiler version.  The codebase specifies a floating version of ^0.5.2 and makes use of the experimental feature ABIEncoderV2.  It should be noted, that ABIEncoderV2 was subject to multiple bug-fixes up until the latest 0.6.xversion and contracts compiled with earlier versions are - for example - susceptible to the following issues:  ImplicitConstructorCallvalueCheck  TupleAssignmentMultiStackSlotComponents  MemoryArrayCreationOverflow  privateCanBeOverridden  YulOptimizerRedundantAssignmentBreakContinue0.5  ABIEncoderV2CalldataStructsWithStaticallySizedAndDynamicallyEncodedMembers  SignedArrayStorageCopy  ABIEncoderV2StorageArrayWithMultiSlotElement  DynamicConstructorArgumentsClippedABIV2  Examples  Codebase declares compiler version ^0.5.2:  code/contracts/liquidity/AMM.sol:L1-L2  pragma solidity ^0.5.2;  pragma experimental ABIEncoderV2; // to enable structure-type parameters  According to etherscan.io, the currently deployed main-net AMM contract is compiled with solidity version 0.5.8:  https://etherscan.io/address/0xb95B9fb0539Ec84DeD2855Ed1C9C686Af9A4e8b3#code  Recommendation  It is recommended to settle on the latest stable 0.6.x or 0.5.x version of the Solidity compiler and lock the pragma version to a specifically tested compiler release.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.18 AMM - ONE_WAD_U is never used    ",
        "body": "  Resolution   This issue is resolved by removing   Description  The const ONE_WAD_U is declared but never used. Avoid re-declaring the same constants in multiple source-units (and unit-test cases) as this will be hard to maintain.  Examples  code/contracts/liquidity/AMM.sol:L17-L17  uint256 private constant ONE_WAD_U = 10**18;  Recommendation  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.19 Perpetual - Variable shadowing in constructor    ",
        "body": "  Resolution  This issue was addressed by following the recommendation.  Description  Perpetual inherits from PerpetualGovernance and Collateral, which declare state variables that are shadowed in the Perpetual constructor.  Examples  Local constructor argument shadows PerpetualGovernance.globalConfig, PerpetualGovernance.devAddress, Collateral.collateral  Note: Confusing name: Collateral is an inherited contract and a state variable.  code/contracts/perpetual/Perpetual.sol:L34-L41  constructor(address globalConfig, address devAddress, address collateral, uint256 collateralDecimals)  public  Position(collateral, collateralDecimals)  setGovernanceAddress(\"globalConfig\", globalConfig);  setGovernanceAddress(\"dev\", devAddress);  emit CreatePerpetual();  Recommendation  Rename the parameter or state variable.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.20 Perpetual - The specified decimals for the collateral may not reflect the token s actual decimals   ",
        "body": "  Resolution  The client acknowledges this issue without providing further information.  Description  When initializing the Perpetual contract, the deployer can decide to use either ETH, or an ERC20-compliant collateral. In the latter case, the deployer must provide a nonzero address for the token, as well as the number of decimals used by the token:  code/contracts/perpetual/Collateral.sol:L28-L34  constructor(address _collateral, uint256 decimals) public {  require(decimals <= MAX_DECIMALS, \"decimals out of range\");  require(_collateral != address(0x0) || (_collateral == address(0x0) && decimals == 18), \"invalid decimals\");  collateral = _collateral;  scaler = (decimals == MAX_DECIMALS ? 1 : 10**(MAX_DECIMALS - decimals)).toInt256();  The provided decimals value is not checked for validity and can differ from the actual token s decimals.  Recommendation  Ensure to establish documentation that makes users aware of the fact that the decimals configured are not enforced to match the actual tokens decimals. This is to allow users to audit the system configuration and decide whether they want to participate in it.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.21 AMM - Unchecked return value in ShareToken.mint   Pending",
        "body": "  Resolution   This issue was addressed by adding checks to   Description  ShareToken is an extension of the Openzeppelin ERC20Mintable pattern which exposes a method called mint() that allows accounts owning the minter role to mint new tokens. The return value of ShareToken.mint() is not checked.  Since the ERC20 standard does not define whether this method should return a value or revert it may be problematic to assume that all tokens revert. If, for example, an implementation is used that does not revert on error but returns a boolean error indicator instead the caller might falsely continue without the token minted.  We would like to note that the functionality is intended to be used with the provided ShareToken and therefore the contract is safe to use assuming ERC20Mintable.mint reverts on error. The issue arises if the system is used with a different ShareToken implementation that is not implemented in the same way.  Examples  Openzeppelin implementation  function mint(address account, uint256 amount) public onlyMinter returns (bool) {  _mint(account, amount);  return true;  Call with unchecked return value  code/contracts/liquidity/AMM.sol:L499-L502  function mintShareTokenTo(address guy, uint256 amount) internal {  shareToken.mint(guy, amount);  Recommendation  Consider wrapping the mint statement in a require clause, however, this way only tokens that are returning a boolean error indicator are supported. Document the specification requirements for the ShareToken and clearly state if the token is expected to revert or return an error indicator.  It should also be documented that the Token exposes a burn method that does not adhere to the Openzeppelin ERC20Burnable implementation. The ERC20Burnable import is unused as noted in issue 6.23.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.22 Perpetual - beginGlobalSettlement can be called multiple times   ",
        "body": "  Resolution  The client addressed this issue with the following statement:  Acknowledged. It sill can be call multiple times to correct the settlement price. Voting and pausemay improve the situation.When pause, no liquidation which may leading to losing position happens event in theemergency mode.  beginGlobalSettlement can still be called multiple times.  Description  The system can be put into emergency mode by an admin calling beginGlobalSettlement and providing a fixed settlementPrice. The method can be invoked even when the contract is already in SETTLING (emergency) mode, allowing an admin to selectively adjust the settlement price again. This does not seem to be the intended behavior as calling the method again re-sets the status to SETTLING. Furthermore, it may affect users  behavior during the SETTLING phase.  Examples  code/contracts/perpetual/PerpetualGovernance.sol:L96-L101  function beginGlobalSettlement(uint256 price) public onlyWhitelistAdmin {  require(status != LibTypes.Status.SETTLED, \"already settled\");  settlementPrice = price;  status = LibTypes.Status.SETTLING;  emit BeginGlobalSettlement(price);  Recommendation  Emergency mode should only be allowed to be set once  ",
        "labels": [
            "Consensys",
            "Minor",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.23 Unused Imports    ",
        "body": "  Resolution  This issue was addressed by removing the listed imports.  Description  The following source units are imported but not referenced in the contract:  Examples  code/contracts/perpetual/Perpetual.sol:L4-L5  import \"@openzeppelin/contracts/token/ERC20/IERC20.sol\";  import \"@openzeppelin/contracts/token/ERC20/SafeERC20.sol\";  code/contracts/perpetual/Perpetual.sol:L14-L15  import \"../interface/IPriceFeeder.sol\";  import \"../interface/IGlobalConfig.sol\";  code/contracts/token/ShareToken.sol:L5-L5  import \"@openzeppelin/contracts/token/ERC20/ERC20Burnable.sol\";  code/contracts/token/ShareToken.sol:L3-L3  import \"@openzeppelin/contracts/token/ERC20/ERC20.sol\";  Recommendation  Check all imports and remove all unused/unreferenced and unnecessary imports.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.24 Exchange - OrderStatus is never used    ",
        "body": "  Resolution  This issue was resolved by removing the unused code.  Description  The enum OrderStatus is declared but never used.  Examples  code/contracts/exchange/Exchange.sol:L20-L20  enum OrderStatus {EXPIRED, CANCELLED, FILLABLE, FULLY_FILLED}  Recommendation  Remove unused code.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.25 LibMath - Inaccurate declaration of _UINT256_MAX    ",
        "body": "  Resolution   This issue was addressed by renaming   Description  LibMathUnsigned declares _UINT256_MAX as 2^255-1 while this value actually represents _INT256_MAX. This appears to just be a naming issue.  Examples  (UINT256_MAX/2-1 => pos INT256_MAX; 2**256/2-1==2**255-1)  code/contracts/lib/LibMath.sol:L228-L230  library LibMathUnsigned {  uint256 private constant _WAD = 10**18;  uint256 private constant _UINT256_MAX = 2**255 - 1;  Recommendation  Rename _UINT256_MAX to _INT256MAX or _SIGNED_INT256MAX.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.26 LibMath - inconsistent assertion text and improve representation of literals with many digits   ",
        "body": "  Resolution  The client acknowledges this issue without providing further information.  Description  The assertion below states that logE only accepts v <= 1e22 * 1e18 while the argument name is x. In addition to that we suggest representing large literals in scientific notation.  Examples  code/contracts/lib/LibMath.sol:L153-L157  function wln(int256 x) internal pure returns (int256) {  require(x > 0, \"logE of negative number\");  require(x <= 10000000000000000000000000000000000000000, \"logE only accepts v <= 1e22 * 1e18\"); // in order to prevent using safe-math  int256 r = 0;  uint8 extra_digits = longer_digits - fixed_digits;  Recommendation  Update the inconsistent assertion text v -> x and represent large literals in scientific notation as they are otherwise difficult to read and review.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.27 LibMath - roundHalfUp returns unfinished result    ",
        "body": "  Resolution  This issue was addressed by adding the following comment to the function signature. Please note that the code documentation does not adhere to the natspec format.  // ROUND_HALF_UP rule helper. You have to call roundHalfUp(x, y) / y to finish the rounding operation  There is still the residual risk that someone might miss the comment and wrongly assume that the method finishes rounding. This is, however, accepted by the client.  Description  It is assumed that the final rounding step is not executed for performance reasons. However, this might easily introduce errors when the caller assumes the result is rounded for base while it is not.  Examples  roundHalfUp(-4700, 1000) = -4700 instead of 5000  roundHalfUp(4700, 1000) = 4700 instead of 5000  code/contracts/lib/LibMath.sol:L126-L133  // ROUND_HALF_UP rule helper. 0.5 \u2248 1, 0.4 \u2248 0, -0.5 \u2248 -1, -0.4 \u2248 0  function roundHalfUp(int256 x, int256 y) internal pure returns (int256) {  require(y > 0, \"roundHalfUp only supports y > 0\");  if (x >= 0) {  return add(x, y / 2);  return sub(x, y / 2);  Recommendation  We have verified the current code-base and the callers for roundHalfUp are correctly finishing the rounding step. However, it is recommended to finish the rounding within the method or document this behavior to prevent errors caused by code that falsely assumes that the returned value finished rounding.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.28 LibMath/LibOrder - unused named return value    ",
        "body": "  Resolution  This issue was resolved by either returning a named value or using the return statement.  Description  The following methods declare a named return value but explicitly return a value instead. The named return value is not used.  LibMathSigned.min()  LibMathSigned.max()  LibMathUnsigned.min()  LibMathUnsigned.max()  LibOrder.getOrderHash()  LibOrder.hashOrder()  Examples  code/contracts/lib/LibMath.sol:L90-L96  function min(int256 x, int256 y) internal pure returns (int256 z) {  return x <= y ? x : y;  function max(int256 x, int256 y) internal pure returns (int256 z) {  return x >= y ? x : y;  code/contracts/lib/LibMath.sol:L285-L292  function min(uint256 x, uint256 y) internal pure returns (uint256 z) {  return x <= y ? x : y;  function max(uint256 x, uint256 y) internal pure returns (uint256 z) {  return x >= y ? x : y;  code/contracts/lib/LibOrder.sol:L68-L71  function getOrderHash(Order memory order) internal pure returns (bytes32 orderHash) {  orderHash = LibEIP712.hashEIP712Message(hashOrder(order));  return orderHash;  code/contracts/lib/LibOrder.sol:L86-L97  function hashOrder(Order memory order) internal pure returns (bytes32 result) {  bytes32 orderType = EIP712_ORDER_TYPE;  // solium-disable-next-line security/no-inline-assembly  assembly {  let start := sub(order, 32)  let tmp := mload(start)  mstore(start, orderType)  result := keccak256(start, 224)  mstore(start, tmp)  return result;  Recommendation  Remove the named return value and explicitly return the value.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "6.29 Where possible, a specific contract type should be used rather than address   Pending",
        "body": "  Resolution   This issue was partially addressed by changing some state variable declarations from   Description  Rather than storing addresses and then casting to the known contract type, it s better to use the best type available so the compiler can check for type safety.  Examples  Collateral. collateral is of type address, but it could be type IERC20 instead. Not only would this give a little more type safety when deploying new modules, but it would avoid repeated casts throughout the codebase of the form IERC20(collateral), IPerpetual(_perpetual) and others. The following is an incomplete list of examples:  declare collateral as IERC20  code/contracts/perpetual/Collateral.sol:L19-L19  address public collateral;  code/contracts/perpetual/Collateral.sol:L51-L51  IERC20(collateral).safeTransferFrom(guy, address(this), rawAmount);  declare argument perpetual as IPerpetual  code/contracts/exchange/Exchange.sol:L34-L42  function matchOrders(  LibOrder.OrderParam memory takerOrderParam,  LibOrder.OrderParam[] memory makerOrderParams,  address _perpetual,  uint256[] memory amounts  ) public {  require(!takerOrderParam.isMakerOnly(), \"taker order is maker only\");  IPerpetual perpetual = IPerpetual(_perpetual);  declare argument feeder as IChainlinkFeeder  code/contracts/oracle/ChainlinkAdapter.sol:L12-L14  constructor(address _feeder) public {  feeder = IChainlinkFeeder(_feeder);  Remediation  Where possible, use more specific types instead of address. This goes for parameter types as well as state variable types.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"
    },
    {
        "title": "5.1 Oracle updates can be manipulated to perform atomic front-running attack    Addressed",
        "body": "  Resolution  The issue was mitigated by updating the Oracle price only once per block and consistently only using the old value throughout the block instead of querying the Oracle when adding or removing liquidity. Arbitrageurs can now no longer do the profitable trade within a single transaction which also precludes the possibility of using flash loans to amplify the attack.  Description  It is possible to atomically arbitrage rate changes in a risk-free way by  sandwiching  the Oracle update between two transactions. The attacker would send the following 2 transactions at the moment the Oracle update appears in the mempool:  The first transaction, which is sent with a higher gas price than the Oracle update transaction, converts a very small amount. This  locks in  the conversion weights for the block since handleExternalRateChange() only updates weights once per block. By doing this, the arbitrageur ensures that the stale Oracle price is initially used when doing the first conversion in the following transaction.  The second transaction, which is sent at a slightly lower gas price than the transaction that updates the Oracle, does the following:  Perform a large conversion at the old weight;  Add a small amount of Liquidity to trigger rebalancing;  Convert back at the new rate.  The attacker can also leverage the incentive generated by the formula by converting such that primary reserve balance == primary reserve staked balance.  The attacker can obtain liquidity for step 2 using a flash loan. The attack will deplete the reserves of the pool. An example is shown in section 5.4.  Recommendation  Do not allow users to trade at a stale Oracle rate and trigger an Oracle price update in the same transaction.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/bancor-v2-amm-security-audit/"
    },
    {
        "title": "5.2 Slippage and fees can be manipulated by a trader    Addressed",
        "body": "  Resolution  The issue was addressed by introducing an exit fee mechanism. When a liquidity provider wants to withdraw some liquidity, the smart contract returns fewer tokens if the primary reserve is not in the balanced state. So in most cases, the manipulations described in the issue should potentially be non-profitable anymore. Although, in some cases, the traders still may have some incentive to add liquidity before making the trade and remove it after to get a part of the fees (i.e., if the pool is going to be in a balanced state after the trade).  Description  Users are making trades against the liquidity pool (converter) with slippage and fees defined in the converter contract and Bancor formula. The following steps can be done to optimize trading costs:  Instead of just making a trade, a user can add a lot of liquidity (of both tokens, or only one of them) to the pool after taking a flash loan, for example.  Make the trade.  Remove the added liquidity.  Because the liquidity is increased on the first step, slippage is getting smaller for this trade. Additionally, the trader receives a part of the fees for this trade by providing liquidity.  One of the reasons why this is possible is described in another issue issue 5.3.  This technique of reducing slippage could be used by the trader to get more profit from any frontrunning/arbitrage opportunity and can help to deplete the reserves.  Example  Consider the initial state with an amplification factor of 20 and zero fees:  Here a user can make a trade with the following rate:  > Convert 9000000 TKN into 8612440 BNT.  But if the user adds 100% of the liquidity in both tokens before the trade, the slippage will be lower:  > Convert 9000000 TKN into 8801955 BNT.  Recommendation  Fixing this issue requires some modification of the algorithm.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/bancor-v2-amm-security-audit/"
    },
    {
        "title": "5.3 Loss of the liquidity pool is not equally distributed    Addressed",
        "body": "  Resolution  The issue was addressed by adding a new fee mechanism called  adjusted fees . This mechanism aims to decrease the deficit of the reserves over time. If there is a deficit of reserves, it is usually present on the secondary token side, because there is a strong incentive to bring the primary token to the balanced state. Roughly speaking, the idea is that if the secondary token has a deficit in reserves, there are additional fees for trading that token. These fees are not distributed across the liquidity providers like the regular fees. Instead, they are just populating the reserve, decreasing the existing deficit.  Loss is still not distributed across the liquidity providers, and there is a possibility that there are not enough funds for everyone to withdraw them. In the case of a run on reserves, LPs will be able to withdraw funds on a first-come-first-serve basis.  Description  All stakeholders in the liquidity pool should be able to withdraw the same amount as they staked plus a share of fees that the converter earned during their staking period.  code/contracts/converter/LiquidityPoolV2Converter.sol:L491-L505  IPoolTokensContainer(anchor).burn(_poolToken, msg.sender, _amount);  // calculate how much liquidity to remove  // if the entire supply is liquidated, the entire staked amount should be sent, otherwise  // the price is based on the ratio between the pool token supply and the staked balance  uint256 reserveAmount = 0;  if (_amount == initialPoolSupply)  reserveAmount = balance;  else  reserveAmount = _amount.mul(balance).div(initialPoolSupply);  // sync the reserve balance / staked balance  reserves[reserveToken].balance = reserves[reserveToken].balance.sub(reserveAmount);  uint256 newStakedBalance = stakedBalances[reserveToken].sub(reserveAmount);  stakedBalances[reserveToken] = newStakedBalance;  The problem is that sometimes there might not be enough funds in reserve (for example, due to this issue https://github.com/ConsenSys/bancor-audit-2020-06/issues/4). So the first ones who withdraw their stakes receive all the tokens they own. But the last stakeholders might not be able to get their funds back because the pool is empty already.  So under some circumstances, there is a chance that users can lose all of their staked funds.  This issue also has the opposite side: if the liquidity pool makes an extra profit, the stakers do not owe this profit and cannot withdraw it.  Recommendation  Distribute losses evenly across the liquidity providers.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/bancor-v2-amm-security-audit/"
    },
    {
        "title": "5.4 Oracle front-running could deplete reserves over time    Addressed",
        "body": "  Resolution  To mitigate this issue, the Bancor team has added a mechanism that adjusts the effective weights once per block based on its internal price feed. The conversion rate re-anchors to the external oracle price once the next oracle update comes in. This mechanism should help to cause the weight rebalancing caused by the external Oracle update to be less pronounced, thereby limiting the profitability of Oracle frontrunning. It should be noted that it also adds another layer of complexity to the system. It is difficult to predict the actual effectiveness and impact of this mitigation measure without simulating the system under real-world conditions.  Description  Bancor s weight rebalancing mechanism uses Chainlink price oracles to dynamically update the weights of the assets in the pool to track the market price. Due to Oracle price updates being visible in the mempool before they are included in a block, it is always possible to know about Oracle updates in advance and attempt to make a favourable conversion which takes the future rebalancing into account, followed by the reverse conversion after the rebalancing has occurred. This can be done with high liquidity and medium risk since transaction ordering on the Ethereum blockchain is largely predictable.  Over time, this could deplete the secondary reserve as the formula compensates by rebalancing the weights such that the secondary token is sold slightly below its market rate (this is done to create an incentive to bring the primary reserve back to the amount staked by liquidity providers).  Example  Consider the initial state with an amplification factor of 20 and zero fees:  The frontrunner sees a Chainlink transaction in the mempool that changes Oracle B rate to 10,500. He sends a transaction with a slightly higher gas price than the Oracle update.  Convert 1,000,000 TKN into 999,500 BNT.  The intermediate state:  In the following block, the frontrunner sends another transaction with a high gas price (the goal is to be first to convert at the new rate set by the Oracle update):  Convert 999,500 BNT back into TKN.  The state is:  The frontrunner can now leverage the incentive created by the formula to bring back TKN reserve balance to staked TKN balance by converting TKN back to BNT:  Convert 4,994 TKN to BNT  The final state is:  The pool is now balanced and the frontrunner has gained 4,969 BNT.  Recommendation  This appears to be a fundamental problem caused by the fact that rebalancing is predictable. It is difficult to assess the actual impact of this issue without also reviewing components external to the scope of this audit (Chainlink) and extensively testing the system under real-world conditions.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/bancor-v2-amm-security-audit/"
    },
    {
        "title": "5.5 Use of external calls with a fixed amount of gas   ",
        "body": "  Resolution  It was decided to accept this minor risk as the usage of .call() might introduce other unexpected behavior.  Description  The converter smart contract uses the Solidity transfer() function to transfer Ether.  .transfer() and .send() forward exactly 2,300 gas to the recipient. The goal of this hardcoded gas stipend was to prevent reentrancy vulnerabilities, but this only makes sense under the assumption that gas costs are constant. Recently EIP 1884 was included in the Istanbul hard fork. One of the changes included in EIP 1884 is an increase to the gas cost of the SLOAD operation, causing a contract s fallback function to cost more than 2300 gas.  Examples  code/contracts/converter/ConverterBase.sol:L228  _to.transfer(address(this).balance);  code/contracts/converter/LiquidityPoolV2Converter.sol:L370  if (_targetToken == ETH_RESERVE_ADDRESS)  code/contracts/converter/LiquidityPoolV2Converter.sol:L509  msg.sender.transfer(reserveAmount);  Recommendation  It s recommended to stop using .transfer() and .send() and instead use .call(). Note that .call() does nothing to mitigate reentrancy attacks, so other precautions must be taken. To prevent reentrancy attacks, it is recommended that you use the checks-effects-interactions pattern.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/bancor-v2-amm-security-audit/"
    },
    {
        "title": "5.6 Use of assert statement for input validation    Addressed",
        "body": "  Resolution  Assertions are no longer used in the final version reviewed.  Description  Solidity assertion should only be used to assert invariants, i.e. statements that are expected to always hold if the code behaves correctly. Note that all available gas is consumed when an assert-style exception occurs.  Examples  It appears that assert() is used in one location within the test scope to catch invalid user inputs:  code/contracts/converter/LiquidityPoolV2Converter.sol:L354  assert(amount < targetReserveBalance);  Recommendation  Using require() instead of assert().  6 Bytecode Verification  Bytecode-level checking helps to ensure that the code behaves correctly for all input values. In this audit we used Mythx deep analysis to verify a small number of basic properties on the weight rebalancing and conversion functions and to detect conditions that would cause runtime exceptions. MythX uses symbolic execution and input fuzzing to explore a large amount of possible inputs and program states.  Note that the Bancor formula is compiled with solc-0.4.25 / 20,000 optimization passes.  We checked whether the following properties hold for all inputs:  [P1] Function balancedWeights: Sum of weights returned by must equal MAX_WEIGHT  [P2a] Function crossReserveTargetAmount: Output amount must not be greater than target reserve balance  [P2b] Function crossReserveTargetAmount: If reserve balances are equal and source weight < target weight, target amount must be lower than input amount  Note that balancedWeights is known to revert when (t * p) / (r * q) * log( s / t) is not in the range [-1/e, 1/e], where:  t is the primary reserve staked balance  s is the primary reserve current balance  r is the secondary reserve current balance  q is the primary reserve rate  p is the secondary reserve rate  The following preconditions were set on the input to reflect realistic input ranges. For balancedWeights:  For crossReserveTargetAmount:  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/bancor-v2-amm-security-audit/"
    },
    {
        "title": "6.1 Results",
        "body": "  No violations of the properties tested were found. Our tools also did not identify any cases that would cause the function to revert for the given input ranges.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/bancor-v2-amm-security-audit/"
    },
    {
        "title": "5.1 TokenStaking.recoverStake allows instant stake undelegation    Addressed",
        "body": "  Resolution   Addressed with   keep-network/keep-core#1521 by adding a non-zero check for the undelegation block.  Description  TokenStaking.recoverStake is used to recover stake that has been designated to be undelegated. It contains a single check to ensure that the undelegation period has passed:  keep-core/contracts/solidity/contracts/TokenStaking.sol:L182-L187  function recoverStake(address _operator) public {  uint256 operatorParams = operators[_operator].packedParams;  require(  block.number > operatorParams.getUndelegationBlock().add(undelegationPeriod),  \"Can not recover stake before undelegation period is over.\"  );  However, if an undelegation period is never set, this will always return true, allowing any operator to instantly undelegate stake at any time.  Recommendation  Require that the undelegation period is nonzero before allowing an operator to recover stake.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.2 Improper length validation in BLS signature library allows RNG manipulation    Addressed",
        "body": "  Resolution   Addressed with   keep-network/keep-core#1523 by adding input length checks to  Description  KeepRandomBeaconOperator.relayEntry(bytes memory _signature) is used to submit random beacon results:  keep-core/contracts/solidity/contracts/KeepRandomBeaconOperator.sol:L418-L433  function relayEntry(bytes memory _groupSignature) public nonReentrant {  require(isEntryInProgress(), \"Entry was submitted\");  require(!hasEntryTimedOut(), \"Entry timed out\");  bytes memory groupPubKey = groups.getGroupPublicKey(signingRequest.groupIndex);  require(  BLS.verify(  groupPubKey,  signingRequest.previousEntry,  _groupSignature  ),  \"Invalid signature\"  );  emit RelayEntrySubmitted();  The function calls BLS.verify, which validates that the submitted signature correctly signs the previous recorded random beacon entry. BLS.verify calls AltBn128.g1Unmarshal(signature):  keep-core/contracts/solidity/contracts/cryptography/BLS.sol:L31-L37  function verify(  bytes memory publicKey,  bytes memory message,  bytes memory signature  ) public view returns (bool) {  AltBn128.G1Point memory _signature = AltBn128.g1Unmarshal(signature);  AltBn128.g1Unmarshal(signature) reads directly from memory without making any length checks:  keep-core/contracts/solidity/contracts/cryptography/AltBn128.sol:L214-L228  /**  @dev Unmarshals a point on G1 from bytes in an uncompressed form.  /  function g1Unmarshal(bytes memory m) internal pure returns(G1Point memory) {  bytes32 x;  bytes32 y;  /* solium-disable-next-line */  assembly {  x := mload(add(m, 0x20))  y := mload(add(m, 0x40))  return G1Point(uint256(x), uint256(y));  There are two potential issues with this:  g1Unmarshal may be reading out-of-bounds of the signature from dirty memory.  g1Unmarshal may not be reading all of the signature. If more than 64 bytes are supplied, they are ignored for the purposes of signature validation.  These issues are important because the hash of the signature is the  random number  supplied to user contracts:  keep-core/contracts/solidity/contracts/KeepRandomBeaconOperator.sol:L435-L448  // Spend no more than groupSelectionGasEstimate + 40000 gas max  // This will prevent relayEntry failure in case the service contract is compromised  signingRequest.serviceContract.call.gas(groupSelectionGasEstimate.add(40000))(  abi.encodeWithSignature(  \"entryCreated(uint256,bytes,address)\",  signingRequest.relayRequestId,  _groupSignature,  msg.sender  );  if (signingRequest.callbackFee > 0) {  executeCallback(signingRequest, uint256(keccak256(_groupSignature)));  An attacker can use this behavior to game random number generation by frontrunning a valid signature submission with additional byte padding.  Recommendation  Ensure each function in BLS.sol properly validates input lengths for all parameters; the same length validation issue exists in BLS.verifyBytes.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.3 tbtc - the tecdsa keep is never closed, signer bonds are not released    Addressed",
        "body": "  Resolution  Addressed with https://github.com/keep-network/tbtc/issues/473, https://github.com/keep-network/tbtc/issues/490, keep-network/tbtc#534, and keep-network/tbtc#520.  failed_setup:  notifySignerSetupFailure \u2705closed by seizing funds with issue 5.10 notifyFundingTimeout \u2705closed with keep-network/tbtc#534 provideFundingECDSAFraudProof, \u2705slashes stake, distributes signer bonds to funder (push payment -> should be pull or funder may block), closes keep. provideFraudBTCFundingProof \u2705 removed with keep-network/tbtc#534 notifyFraudFundingTimeout \u2705 removed with keep-network/tbtc#534  liquidated:  provideSPVFraudProof \u2705removed purchaseSignerBondsAtAuction \u2705 via startSignerAbortLiquidation, \u2705 via startSignerFraudLiquidation (implicitly via seizebonds)  redeemed:  provideRedemptionProof \u2705  Description  At the end of the TBTC deposit lifecycle happy path, the deposit is supposed to close the keep in order to release the signer bonds. However, there is no call to closeKeep in any of the code-bases under audit.  Recommendation  Close the keep releasing the signer bonds.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.4 tbtc - No access control in TBTCSystem.requestNewKeep    Addressed",
        "body": "  Resolution   Issue addressed in   keep-network/tbtc#514. Each call to  Description  TBTCSystem.requestNewKeep is used by each new Deposit contract on creation. It calls BondedECDSAKeepFactory.openKeep, which sets the Deposit contract as the  owner,  a permissioned role within the created keep. openKeep also automatically allocates bonds from members registered to the application. The  application  from which member bonds are allocated is the tbtc system itself.  Because requestNewKeep has no access controls, anyone can request that a keep be opened with msg.sender as the  owner,  and arbitrary signing threshold values:  tbtc/implementation/contracts/system/TBTCSystem.sol:L231-L243  /// @notice Request a new keep opening.  /// @param _m Minimum number of honest keep members required to sign.  /// @param _n Number of members in the keep.  /// @return Address of a new keep.  function requestNewKeep(uint256 _m, uint256 _n, uint256 _bond)  external  payable  returns (address)  IBondedECDSAKeepVendor _keepVendor = IBondedECDSAKeepVendor(keepVendor);  IBondedECDSAKeepFactory _keepFactory = IBondedECDSAKeepFactory(_keepVendor.selectFactory());  return _keepFactory.openKeep.value(msg.value)(_n, _m, msg.sender, _bond);  Given that the owner of a keep is able to seize signer bonds, close the keep, and more, having control of this role could be detrimental to group members.  Recommendation  Add access control to requestNewKeep, so that it can only be called as a part of the Deposit creation and initialization process.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.5 Unpredictable behavior due to front running or general bad timing    Addressed",
        "body": "  Resolution  This issue has been addressed with https://github.com/keep-network/tbtc/issues/493 and the following set of PRs:  https://github.com/keep-network/tbtc/issues/493  https://github.com/keep-network/keep-tecdsa/issues/296 - note: initializeImplementation should be done in completeUpgrade otherwise this could be used as a backdoor.  fixed by keep-network/keep-ecdsa#327 - fixed: initialization moved to complete upgrade step  https://github.com/keep-network/keep-core/issues/1423 - note: initializeImplementationshould be done incompleteUpgrade` otherwise this could be used as a backdoor.  fixed by keep-network/keep-core#1517 - fixed: initialization moved to complete upgrade step  The client also provided the following statements:  In general, our current stance on frontrunning proofs that lead to rewards is that as long as it doesn t significantly compromise an incentive on the primary actors of the system, we re comfortable with having it present. In particular, frontrunnable actions that include rewards in several cases have additional incentives\u2014for tBTC deposit owners, for example, claiming bonds in case of misbehavior; for signers, reclaiming bonds in case of deposit owner absence or other misbehavior. We consider signer reclamation of bonds to be a strong incentive, as bond value is expected to be large enough that there is ongoing expected value to having the bond value liquid rather than bonded.  Some of the frontrunning cases (e.g. around beacon signing) did not have this additional incentive, and in those cases we ve taken up the recommendations in the audit.  Description  In a number of cases, administrators of contracts can update or upgrade things in the system without warning. This has the potential to violate a security goal of the system.  Specifically, privileged roles could use front running to make malicious changes just ahead of incoming transactions, or purely accidental negative effects could occur due to unfortunate timing of changes.  Some instances of this are more important than others, but in general users of the system should have assurances about the behavior of the action they re about to take.  Examples  System Parameters  The owner of the TBTCSystem contract can change system parameters at any time with changes taking effect immediately.  setSignerFeeDivisor - stored in the deposit contract when creating a new deposit. emits an event.  setLotSizes - stored in the deposit contract when creating a new deposit. emits an event.  setCollateralizationThresholds - stored in the deposit contract when creating a new deposit. emits an event.  This also opens up an opportunity for malicious owner to:  interfere with other participants deposit creation attempts (front-running transactions)  craft a series of transactions that allow the owner to set parameters that are more beneficial to them, then create a deposit and reset the parameters to the systems  initial settings.  tbtc/implementation/contracts/system/TBTCSystem.sol:L113-L121  /// @notice Set the system signer fee divisor.  /// @param _signerFeeDivisor The signer fee divisor.  function setSignerFeeDivisor(uint256 _signerFeeDivisor)  external onlyOwner  require(_signerFeeDivisor > 9, \"Signer fee divisor must be greater than 9, for a signer fee that is <= 10%.\");  signerFeeDivisor = _signerFeeDivisor;  emit SignerFeeDivisorUpdated(_signerFeeDivisor);  Upgradables  The proxy pattern used in many places throughout the system allows the operator to set a new implementation which takes effect immediately.  keep-core/contracts/solidity/contracts/KeepRandomBeaconService.sol:L67-L80  /**  @dev Upgrade current implementation.  @param _implementation Address of the new implementation contract.  /  function upgradeTo(address _implementation)  public  onlyOwner  address currentImplementation = implementation();  require(_implementation != address(0), \"Implementation address can't be zero.\");  require(_implementation != currentImplementation, \"Implementation address must be different from the current one.\");  setImplementation(_implementation);  emit Upgraded(_implementation);  keep-tecdsa/solidity/contracts/BondedECDSAKeepVendor.sol:L57-L71  /// @notice Upgrades the current vendor implementation.  /// @param _implementation Address of the new vendor implementation contract.  function upgradeTo(address _implementation) public onlyOwner {  address currentImplementation = implementation();  require(  _implementation != address(0),  \"Implementation address can't be zero.\"  );  require(  _implementation != currentImplementation,  \"Implementation address must be different from the current one.\"  );  setImplementation(_implementation);  emit Upgraded(_implementation);  Registry  keep-tecdsa/solidity/contracts/BondedECDSAKeepVendorImplV1.sol:L43-L50  function registerFactory(address payable _factory) external onlyOperatorContractUpgrader {  require(_factory != address(0), \"Incorrect factory address\");  require(  registry.isApprovedOperatorContract(_factory),  \"Factory contract is not approved\"  );  keepFactory = _factory;  Recommendation  The underlying issue is that users of the system can t be sure what the behavior of a function call will be, and this is because the behavior can change at any time.  We recommend giving the user advance notice of changes with a time lock. For example, make all upgrades require two steps with a mandatory time window between them. The first step merely broadcasts to users that a particular change is coming, and the second step commits that change after a suitable waiting period.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.6 keep-core - reportRelayEntryTimeout creates an incentive for nodes to race for rewards potentially wasting gas and it creates an opportunity for front-running    Addressed",
        "body": "  Resolution   Following the discussion at   https://github.com/keep-network/keep-core/issues/1404 it was verified that the method throws as early as possible in an attempt to safe gas in case many nodes call out the timeout in the same block. The client is currently comfortable with this tradeoff. We would like to note that this issue cannot easily be addressed (e.g. allowing nodes to disable calling out timeouts impacts the security of the system; a commit/reveal proxy adds overhead and is unlikely to make the situation better as nodes are programmed to call out timeouts) and we therefore recommend to monitor the network for this scenario.  Description  The incentive on reportRelayEntryTimeout for being rewarded with 5% of the seized amount creates an incentive to call the method but might also kick off a race for front-running this call. This method is being called from the keep node which is unlikely to adjust the gasPrice and might always lose the race against a front-running bot collecting rewards for all timeouts and fraud proofs (issue 5.7)  Examples  keep-core/contracts/solidity/contracts/KeepRandomBeaconOperator.sol:L600-L626  /**  @dev Function used to inform about the fact the currently ongoing  new relay entry generation operation timed out. As a result, the group  which was supposed to produce a new relay entry is immediately  terminated and a new group is selected to produce a new relay entry.  All members of the group are punished by seizing minimum stake of  their tokens. The submitter of the transaction is rewarded with a  tattletale reward which is limited to min(1, 20 / group_size) of the  maximum tattletale reward.  /  function reportRelayEntryTimeout() public {  require(hasEntryTimedOut(), \"Entry did not time out\");  groups.reportRelayEntryTimeout(signingRequest.groupIndex, groupSize, minimumStake);  // We could terminate the last active group. If that's the case,  // do not try to execute signing again because there is no group  // which can handle it.  if (numberOfGroups() > 0) {  signRelayEntry(  signingRequest.relayRequestId,  signingRequest.previousEntry,  signingRequest.serviceContract,  signingRequest.entryVerificationAndProfitFee,  signingRequest.callbackFee  );  Recommendation  Make sure that reportRelayEntryTimeout throws as early as possible if the group was previously terminated (isGroupTerminated) to avoid that keep-nodes spend gas on a call that will fail. Depending on the reward for calling out the timeout this might create a front-running opportunity that cannot be resolved.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.7 keep-core - reportUnauthorizedSigning fraud proof is not bound to reporter and can be front-run    Addressed",
        "body": "  Resolution   Addressed with   https://github.com/keep-network/keep-core/issues/1405 by binding the proof to  Description  An attacker can monitor reportUnauthorizedSigning() for fraud reports and attempt to front-run the original call in an effort to be the first one reporting the fraud and be rewarded 5% of the total seized amount.  Examples  keep-core/contracts/solidity/contracts/KeepRandomBeaconOperator.sol:L742-L755  /**  @dev Reports unauthorized signing for the provided group. Must provide  a valid signature of the group address as a message. Successful signature  verification means the private key has been leaked and all group members  should be punished by seizing their tokens. The submitter of this proof is  rewarded with 5% of the total seized amount scaled by the reward adjustment  parameter and the rest 95% is burned.  /  function reportUnauthorizedSigning(  uint256 groupIndex,  bytes memory signedGroupPubKey  ) public {  groups.reportUnauthorizedSigning(groupIndex, signedGroupPubKey, minimumStake);  Recommendation  Require the reporter to include msg.sender in the signature proving the fraud or implement a two-step commit/reveal scheme to counter front-running opportunities by forcing a reporter to secretly commit the fraud parameters in one block and reveal them in another.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.8 keep-core - operator contracts disabled via panic button can be re-enabled by RegistryKeeper    Addressed",
        "body": "  Resolution  Addressed by https://github.com/keep-network/keep-core/issues/1406 with changes from keep-network/keep-core#1463:  the contract is now using enums instead of int literals  only new operator contracts can be approved  only approved contracts can be disabled  disabled contracts cannot be re-enabled  disabling an operator contract does not yield an event  changes take effect immediately  Description  The keep specification states the following:  Panic Button The Panic Button can disable malicious or malfunctioning contracts that have been previously approved by the Registry Keeper. When a contract is disabled by the Panic Button, its status on the registry changes to reflect this, and it becomes ineligible to penalize operators. Contracts disabled by the Panic Button can not be reactivated. The Panic Button can be rekeyed by Governance.  With the current implementation of the Registry the registryKeeper account can re-enable an operator contract that has previously been disabled by the panicButton account.  We would also like to note the following:  The contract should use enums instead of integer literals when working with contract states.  Changes to the contract take effect immediately, allowing an administrative account to selectively front-run calls to the Registry ACL and interfere with user activity.  The operator contract state can be set to the current value without raising an error.  The panic button can be called for operator contracts that are not yet active.  Examples  keep-core/contracts/solidity/contracts/Registry.sol:L67-L75  function approveOperatorContract(address operatorContract) public onlyRegistryKeeper {  operatorContracts[operatorContract] = 1;  function disableOperatorContract(address operatorContract) public onlyPanicButton {  operatorContracts[operatorContract] = 2;  Recommendation  The keep specification states:  The Panic Button can be used to set the status of an APPROVED contract to DISABLED. Operator Contracts disabled with the Panic Button cannot be re-enabled, and disabled contracts may not punish operators nor be selected by service contracts to perform work.  All three accounts are typically trusted. We recommend requiring the Governance or paniceButton accounts to reset the contract operator state before registryKeeper can change the state or disallow re-enabling of disabled operator contracts as stated in the specification.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.9 tbtc - State transitions are not always enforced    Addressed",
        "body": "  Resolution  This issue was addressed with https://github.com/keep-network/tbtc/issues/494 and accepted by the client with the following statement. Deposits that are timed out can still be pushed to an active state.  For 5.7 around state transitions, our stance (specifically for the upcoming release) is that a skipped state is acceptable as long as it does not result in data loss or incentive skew. Taken in turn, the listed examples:   A TDT holder can choose not to call out notifySignerSetupFailure hoping that the signing group still forms after the signer setup timeout passes.  -> we consider this fine. If the TDT holder wishes to hold out hope, it is their choice. Signers should be incentivized to call notifySignerSetupFailure in case of actual failure to release their bond.   The deposit can be pushed to active state even after notifySignerSetupFailure, notifyFundingTimeout have passed but nobody called it out.  -> again, we consider this fine. A deposit that is funded and proven past its timeout is still a valid deposit, since the two players in question (the depositor and the signing group) were willing to wait longer to complete the flow. The timeouts in question are largely a matter of allowing signers to release their bond in case there is an issue setting up the deposit.   Members of the signing group might decide to call notifyFraudFundingTimeout in a race to avoid late submissions for provideFraudBTCFundingProof to succeed in order to contain funds lost due to fraud.  -> We are intending to change the mechanic here so that signers lose their whole bond in either case.   A malicious signing group observes BTC funding on the bitcoin chain in an attempt to commit fraud at the time the provideBTCFundingProof transition becomes available to front-run provideFundingECDSAFraudProof forcing the deposit into active state.  -> this one is tough, and we re working on changing the liquidation initiator reward so it is no longer a useful attack. In particular, we re looking at the suggestion in 2.4 for this.   If oracle price slippage occurs for one block (flash-crash type of event) someone could call an undercollateralization transition.  -> We are still investigating this possibility.   A deposit term expiration courtesy call can be exit in the rare case where _d.fundedAt + TBTCConstants.getDepositTerm() == block.timestamp  -> Deposit term expiration courtsey calls should no longer apply; see keep-network/tbtc@6344892 . Courtesy call after deposit term is identical to courtsey call pre-term.  Description  State transitions from one deposit state to another require someone calling the corresponding transition method on the deposit and actually spend gas on it. The incentive to call a transition varies and is analyzed in more detail in the security-specification section of this report.  This issue assumes that participants are not always pushing forward through the state machine as soon as a new state becomes available, opening up the possibility of having multiple state transitions being a valid option for a deposit (e.g. pushing a deposit to active state even though a timeout should have been called on it).  Examples  A TDT holder can choose not to call out notifySignerSetupFailure hoping that the signing group still forms after the signer setup timeout passes.  there is no incentive for the TDT holder to terminate its own deposit after a timeout.  the deposit might end up never being in a final error state.  there is no incentive for the signing group to terminate the deposit.  This affects all states that can time out.  The deposit can be pushed to active state even after notifySignerSetupFailure, notifyFundingTimeout have passed but nobody called it out.  There is no timeout check in retrieveSignerPubkey, provideBTCFundingProof.  tbtc/implementation/contracts/deposit/DepositFunding.sol:L108-L117  /// @notice             we poll the Keep contract to retrieve our pubkey  /// @dev                We store the pubkey as 2 bytestrings, X and Y.  /// @param  _d          deposit storage pointer  /// @return             True if successful, otherwise revert  function retrieveSignerPubkey(DepositUtils.Deposit storage _d) public {  require(_d.inAwaitingSignerSetup(), \"Not currently awaiting signer setup\");  bytes memory _publicKey = IBondedECDSAKeep(_d.keepAddress).getPublicKey();  require(_publicKey.length == 64, \"public key not set or not 64-bytes long\");  tbtc/implementation/contracts/deposit/DepositFunding.sol:L263-L278  function provideBTCFundingProof(  DepositUtils.Deposit storage _d,  bytes4 _txVersion,  bytes memory _txInputVector,  bytes memory _txOutputVector,  bytes4 _txLocktime,  uint8 _fundingOutputIndex,  bytes memory _merkleProof,  uint256 _txIndexInBlock,  bytes memory _bitcoinHeaders  ) public returns (bool) {  require(_d.inAwaitingBTCFundingProof(), \"Not awaiting funding\");  bytes8 _valueBytes;  bytes memory  _utxoOutpoint;  Members of the signing group might decide to call notifyFraudFundingTimeout in a race to avoid late submissions for provideFraudBTCFundingProof to succeed in order to contain funds lost due to fraud.  It should be noted that even after the fraud funding timeout passed the TDT holder could provideFraudBTCFundingProof as it does not check for the timeout.  A malicious signing group observes BTC funding on the bitcoin chain in an attempt to commit fraud at the time the provideBTCFundingProof transition becomes available to front-run provideFundingECDSAFraudProof forcing the deposit into active state.  The malicious users of the signing group can then try to report fraud, set themselves as liquidationInitiator to be awarded part of the signer bond (in addition to taking control of the BTC collateral).  The TDT holders fraud-proof can be front-run, see issue 5.15  If oracle price slippage occurs for one block (flash-crash type of event) someone could call an undercollateralization transition.  For severe oracle errors deposits might be liquidated by calling notifyUndercollateralizedLiquidation. The TDT holder cannot exit liquidation in this case.  For non-severe under collateralization someone could call notifyCourtesyCall to impose extra effort on TDT holders to exitCourtesyCall deposits.  A deposit term expiration courtesy call can be exit in the rare case where _d.fundedAt + TBTCConstants.getDepositTerm() == block.timestamp  tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L289-L298  /// @notice     Goes from courtesy call to active  /// @dev        Only callable if collateral is sufficient and the deposit is not expiring  /// @param  _d  deposit storage pointer  function exitCourtesyCall(DepositUtils.Deposit storage _d) public {  require(_d.inCourtesyCall(), \"Not currently in courtesy call\");  require(block.timestamp <= _d.fundedAt + TBTCConstants.getDepositTerm(), \"Deposit is expiring\");  require(getCollateralizationPercentage(_d) >= _d.undercollateralizedThresholdPercent, \"Deposit is still undercollateralized\");  _d.setActive();  _d.logExitedCourtesyCall();  tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L318-L327  /// @notice     Notifies the contract that its term limit has been reached  /// @dev        This initiates a courtesy call  /// @param  _d  deposit storage pointer  function notifyDepositExpiryCourtesyCall(DepositUtils.Deposit storage _d) public {  require(_d.inActive(), \"Deposit is not active\");  require(block.timestamp >= _d.fundedAt + TBTCConstants.getDepositTerm(), \"Deposit term not elapsed\");  _d.setCourtesyCall();  _d.logCourtesyCalled();  _d.courtesyCallInitiated = block.timestamp;  Allow exiting the courtesy call only if the deposit is not expired: block.timestamp < _d.fundedAt + TBTCConstants.getDepositTerm()  Recommendation  Ensure that there are no competing interests between participants of the system to favor one transition over the other, causing race conditions, front-running opportunities or stale deposits that are not pushed to end-states.  Note: Please find an analysis of incentives to call state transitions in the security section of this document.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.10 tbtc - Funder loses payment to keep if signing group is not established in time   Pending",
        "body": "  Resolution  This issue was addressed with https://github.com/keep-network/tbtc/issues/495 by refunding the cost of creating a new keep. We recommend using the pull instead of a push payment pattern to avoid that the funder can block the call.  Additionally, the client provided the following statement:  The remaining push vs pull question is being tracked in https://github.com/keep-network/tbtc/issues/551, part of recommendation 2.7.  Description  The funder had to provide payment for the keep but the signing group failed to establish. Payment for the keep is not returned even though one could assume that the signing group tried to play unfairly. The signing group might intentionally try to cause this scenario to interfere with the system.  Examples  retrieveSignerPubkey fails if keep provided pubkey is empty or of an unexpected length  tbtc/implementation/contracts/deposit/DepositFunding.sol:L108-L127  /// @notice             we poll the Keep contract to retrieve our pubkey  /// @dev                We store the pubkey as 2 bytestrings, X and Y.  /// @param  _d          deposit storage pointer  /// @return             True if successful, otherwise revert  function retrieveSignerPubkey(DepositUtils.Deposit storage _d) public {  require(_d.inAwaitingSignerSetup(), \"Not currently awaiting signer setup\");  bytes memory _publicKey = IBondedECDSAKeep(_d.keepAddress).getPublicKey();  require(_publicKey.length == 64, \"public key not set or not 64-bytes long\");  _d.signingGroupPubkeyX = _publicKey.slice(0, 32).toBytes32();  _d.signingGroupPubkeyY = _publicKey.slice(32, 32).toBytes32();  require(_d.signingGroupPubkeyY != bytes32(0) && _d.signingGroupPubkeyX != bytes32(0), \"Keep returned bad pubkey\");  _d.fundingProofTimerStart = block.timestamp;  _d.setAwaitingBTCFundingProof();  _d.logRegisteredPubkey(  _d.signingGroupPubkeyX,  _d.signingGroupPubkeyY);  notifySignerSetupFailure can be called by anyone after a timeout of 3hrs  tbtc/implementation/contracts/deposit/DepositFunding.sol:L93-L106  /// @notice     Anyone may notify the contract that signing group setup has timed out  /// @dev        We rely on the keep system punishes the signers in this case  /// @param  _d  deposit storage pointer  function notifySignerSetupFailure(DepositUtils.Deposit storage _d) public {  require(_d.inAwaitingSignerSetup(), \"Not awaiting setup\");  require(  block.timestamp > _d.signingGroupRequestedAt + TBTCConstants.getSigningGroupFormationTimeout(),  \"Signing group formation timeout not yet elapsed\"  );  _d.setFailedSetup();  _d.logSetupFailed();  fundingTeardown(_d);  Recommendation  It should be ensured that a keep group always establishes or otherwise the funder is refunded the fee for the keep.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.11 tbtc - Ethereum block gas limit imposes a fundamental limitation on SPV proofs    Addressed",
        "body": "  Resolution   SPV fraud proofs were removed in   keep-network/tbtc#521. Remember to continue exploring this limitation of the EVM with benchmarking and gas estimates in the tBTC UI.  Description  Several components of the tBTC system rely on SPV proofs to prove the existence of transactions on Bitcoin. Because an SPV proof must provide the entire Bitcoin transaction to the proving smart contract, the Ethereum block gas limit imposes an upper bound on the size of the transaction in question. Although an exact upper bound is subject to several variables, reasonable estimates show that even a moderately-sized Bitcoin transaction may not be able to be successfully validated on Ethereum.  This limitation is significant for two reasons:  Depositors may deposit BTC to the signers by way of a legitimate Bitcoin transaction, only to find that this transaction is unable to be verified on Ethereum. Although the depositor in question was not acting maliciously, they may lose their deposit entirely.  In case signers collude to spend a depositor s BTC unprompted, the system allows depositors to prove a fraudulent spend occurred by way of SPV fraud proof. Given that signers can easily spend BTC with a transaction that is too large to validate by way of SPV proof, this method of fraud proof is unreliable at best. Deposit owners should instead prove fraud by using an ECDSA fraud proof, which operates on a hash of the signed message.  Recommendation  It s important that prospective depositors are able to guarantee that their deposit transaction will be verified successfully. To that end, efforts should be made to provide a deposit UI that checks whether or not a given transaction will be verified successfully before it is submitted. Several variables can affect transaction verification:  Current Ethereum block gas limits  Number of zero-bytes in the Bitcoin transaction in question  Size of the merkle proof needed to prove the transaction s existence  Given that not all of these can be calculated before the transaction is submitted to the Bitcoin blockchain, calculations should attempt to provide a margin of error for the process. Additionally, users should be well-educated about the process, including how to perform a deposit with relatively low risk.  Understanding the relative limitations of the EVM will help this process significantly. Consider benchmarking the gas cost of verifying Bitcoin transactions of various sizes.  Finally, because SPV fraud proofs can be gamed by colluding signers, they should be removed from the system entirely. Deposit owners should always be directed towards ECDSA fraud proofs, as these require relatively fewer assumptions and stronger guarantees.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.12 bitcoin-spv - SPV proofs do not support transactions with larger numbers of inputs and outputs   Pending",
        "body": "  Resolution  The client provided the following statement:  Benchmarks and takeaways are being tracked in issue https://github.com/keep-network/tbtc/issues/556.  Description  There is no explicit restriction on the number of inputs and outputs a Bitcoin transaction can have - as long as the transaction fits into a block. The number of inputs and outputs in a transaction is denoted by a leading  varint  - a variable length integer. In BTCUtils.validateVin and BTCUtils.validateVout, the value of this varint is restricted to under 0xFD, or 253:  bitcoin-spv/solidity/contracts/BTCUtils.sol:L404-L415  /// @notice      Checks that the vin passed up is properly formatted  /// @dev         Consider a vin with a valid vout in its scriptsig  /// @param _vin  Raw bytes length-prefixed input vector  /// @return      True if it represents a validly formatted vin  function validateVin(bytes memory _vin) internal pure returns (bool) {  uint256 _offset = 1;  uint8 _nIns = uint8(_vin.slice(0, 1)[0]);  // Not valid if it says there are too many or no inputs  if (_nIns >= 0xfd || _nIns == 0) {  return false;  Transactions that include more than 252 inputs or outputs will not pass this validation, leading to some legitimate deposits being rejected by the tBTC system.  Examples  The 252-item limit exists in a few forms throughout the system, outside of the aforementioned BTCUtils.validateVin and BTCUtils.validateVout:  BTCUtils.determineOutputLength:  bitcoin-spv/solidity/contracts/BTCUtils.sol:L294-L303  /// @notice          Determines the length of an output  /// @dev             5 types: WPKH, WSH, PKH, SH, and OP_RETURN  /// @param _output   The output  /// @return          The length indicated by the prefix, error if invalid length  function determineOutputLength(bytes memory _output) internal pure returns (uint256) {  uint8 _len = uint8(_output.slice(8, 1)[0]);  require(_len < 0xfd, \"Multi-byte VarInts not supported\");  return _len + 8 + 1; // 8 byte value, 1 byte for _len itself  DepositUtils.findAndParseFundingOutput:  tbtc/implementation/contracts/deposit/DepositUtils.sol:L150-L154  function findAndParseFundingOutput(  DepositUtils.Deposit storage _d,  bytes memory _txOutputVector,  uint8 _fundingOutputIndex  ) public view returns (bytes8) {  DepositUtils.validateAndParseFundingSPVProof:  tbtc/implementation/contracts/deposit/DepositUtils.sol:L181-L191  function validateAndParseFundingSPVProof(  DepositUtils.Deposit storage _d,  bytes4 _txVersion,  bytes memory _txInputVector,  bytes memory _txOutputVector,  bytes4 _txLocktime,  uint8 _fundingOutputIndex,  bytes memory _merkleProof,  uint256 _txIndexInBlock,  bytes memory _bitcoinHeaders  ) public view returns (bytes8 _valueBytes, bytes memory _utxoOutpoint){  DepositFunding.provideFraudBTCFundingProof:  tbtc/implementation/contracts/deposit/DepositFunding.sol:L213-L223  function provideFraudBTCFundingProof(  DepositUtils.Deposit storage _d,  bytes4 _txVersion,  bytes memory _txInputVector,  bytes memory _txOutputVector,  bytes4 _txLocktime,  uint8 _fundingOutputIndex,  bytes memory _merkleProof,  uint256 _txIndexInBlock,  bytes memory _bitcoinHeaders  ) public returns (bool) {  DepositFunding.provideBTCFundingProof:  tbtc/implementation/contracts/deposit/DepositFunding.sol:L263-L273  function provideBTCFundingProof(  DepositUtils.Deposit storage _d,  bytes4 _txVersion,  bytes memory _txInputVector,  bytes memory _txOutputVector,  bytes4 _txLocktime,  uint8 _fundingOutputIndex,  bytes memory _merkleProof,  uint256 _txIndexInBlock,  bytes memory _bitcoinHeaders  ) public returns (bool) {  DepositLiquidation.provideSPVFraudProof:  tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L150-L160  function provideSPVFraudProof(  DepositUtils.Deposit storage _d,  bytes4 _txVersion,  bytes memory _txInputVector,  bytes memory _txOutputVector,  bytes4 _txLocktime,  bytes memory _merkleProof,  uint256 _txIndexInBlock,  uint8 _targetInputIndex,  bytes memory _bitcoinHeaders  ) public {  Recommendation  Incorporate varint parsing in BTCUtils.validateVin and BTCUtils.validateVout. Ensure that other components of the system reflect the removal of the 252-item limit.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.13 bitcoin-spv - multiple integer under-/overflows    Addressed",
        "body": "  Resolution  This was partially addressed in summa-tx/bitcoin-spv#118, summa-tx/bitcoin-spv#119, and summa-tx/bitcoin-spv#122.  Summa opted not to fix the underflow in extractTarget.  In summa-tx/bitcoin-spv#118, the determineOutputLength overflow was addressed by casting _len to a uint256 before addition.  In summa-tx/bitcoin-spv#119, the extractHash underflow was addressed by returning an empty bytes array if the extracted length would cause underflow. Note that an explicit error and transaction revert is favorable in these cases, in order to avoid returning unusable data to the calling function.  Underflow and overflow in BytesLib was addressed in summa-tx/bitcoin-spv#122. Multiple requires were added to the mentioned functions, ensuring memory reads stayed in-bounds for each array. A later change in summa-tx/bitcoin-spv#128 added support for slice with a length of 0.  Description  The bitcoin-spv library allows for multiple integer under-/overflows while processing or converting potentially untrusted or user-provided data.  Examples  uint8 underflow uint256(uint8(_e - 3))  Note: _header[75] will throw consuming all gas if out of bounds while the majority of the library usually uses slice(start, 1) to handle this more gracefully.  bitcoin-spv/solidity/contracts/BTCUtils.sol:L483-L494  /// @dev             Target is a 256 bit number encoded as a 3-byte mantissa and 1 byte exponent  /// @param _header   The header  /// @return          The target threshold  function extractTarget(bytes memory _header) internal pure returns (uint256) {  bytes memory _m = _header.slice(72, 3);  uint8 _e = uint8(_header[75]);  uint256 _mantissa = bytesToUint(reverseEndianness(_m));  uint _exponent = _e - 3;  return _mantissa * (256 ** _exponent);  uint8 overflow uint256(uint8(_len + 8 + 1))  Note: might allow a specially crafted output to return an invalid determineOutputLength <= 9.  Note: while type VarInt is implemented for inputs, it is not for the output length.  bitcoin-spv/solidity/contracts/BTCUtils.sol:L295-L304  /// @dev             5 types: WPKH, WSH, PKH, SH, and OP_RETURN  /// @param _output   The output  /// @return          The length indicated by the prefix, error if invalid length  function determineOutputLength(bytes memory _output) internal pure returns (uint256) {  uint8 _len = uint8(_output.slice(8, 1)[0]);  require(_len < 0xfd, \"Multi-byte VarInts not supported\");  return _len + 8 + 1; // 8 byte value, 1 byte for _len itself  uint8 underflow uint256(uint8(extractOutputScriptLen(_output)[0]) - 2)  bitcoin-spv/solidity/contracts/BTCUtils.sol:L366-L378  /// @dev             Determines type by the length prefix and validates format  /// @param _output   The output  /// @return          The hash committed to by the pk_script, or null for errors  function extractHash(bytes memory _output) internal pure returns (bytes memory) {  if (uint8(_output.slice(9, 1)[0]) == 0) {  uint256 _len = uint8(extractOutputScriptLen(_output)[0]) - 2;  // Check for maliciously formatted witness outputs  if (uint8(_output.slice(10, 1)[0]) != uint8(_len)) {  return hex\"\";  return _output.slice(11, _len);  } else {  bytes32 _tag = _output.keccak256Slice(8, 3);  BytesLib input validation multiple start+length overflow  Note: multiple occurrences. should check start+length > start && bytes.length >= start+length  bitcoin-spv/solidity/contracts/BytesLib.sol:L246-L248  function slice(bytes memory _bytes, uint _start, uint _length) internal  pure returns (bytes memory res) {  require(_bytes.length >= (_start + _length), \"Slice out of bounds\");  BytesLib input validation multiple start overflow  bitcoin-spv/solidity/contracts/BytesLib.sol:L280-L281  function toUint(bytes memory _bytes, uint _start) internal  pure returns (uint256) {  require(_bytes.length >= (_start + 32), \"Uint conversion out of bounds.\");  bitcoin-spv/solidity/contracts/BytesLib.sol:L269-L270  function toAddress(bytes memory _bytes, uint _start) internal  pure returns (address) {  require(_bytes.length >= (_start + 20), \"Address conversion out of bounds.\");  bitcoin-spv/solidity/contracts/BytesLib.sol:L246-L248  function slice(bytes memory _bytes, uint _start, uint _length) internal  pure returns (bytes memory res) {  require(_bytes.length >= (_start + _length), \"Slice out of bounds\");  bitcoin-spv/solidity/contracts/BytesLib.sol:L410-L412  function keccak256Slice(bytes memory _bytes, uint _start, uint _length) pure internal returns (bytes32 result) {  require(_bytes.length >= (_start + _length), \"Slice out of bounds\");  Recommendation  We believe that a general-purpose parsing and verification library for bitcoin payments should be very strict when processing untrusted user input. With strict we mean, that it should rigorously validate provided input data and only proceed with the processing of the data if it is within a safe-to-use range for the method to return valid results. Relying on the caller to provide pre-validate data can be unsafe especially if the caller assumes that proper input validation is performed by the library.  Given the risk profile for this library, we recommend a conservative approach that balances security instead of gas efficiency without relying on certain calls or instructions to throw on invalid input.  For this issue specifically, we recommend proper input validation and explicit type expansion where necessary to prevent values from wrapping or processing data for arguments that are not within a safe-to-use range.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.14 tbtc - Unreachable state LIQUIDATION_IN_PROGRESS    Addressed",
        "body": "  Resolution   Addressed with   https://github.com/keep-network/tbtc/issues/497 with commits from  keep-network/tbtc#517 changing all non-fraud transitions to end up in  Description  According to the specification (overview, states, version 2020-02-06), a deposit can be in one of two liquidation_in_progress states.  LIQUIDATION_IN_PROGRESS  LIQUIDATION_IN_PROGRESS Liquidation due to undercollateralization or an abort has started Automatic (on-chain) liquidation was unsuccessful  FRAUD_LIQUIDATION_IN_PROGRESS  FRAUD_LIQUIDATION_IN_PROGRESS Liquidation due to fraud has started Automatic (on-chain) liquidation was unsuccessful  However, LIQUIDATION_IN_PROGRESS is unreachable and instead, FRAUD_LIQUIDATION_IN_PROGRESS is always called. This means that all non-fraud state transitions end up in the fraud liquidation path and will perform actions as if fraud was detected even though it might be caused by an undercollateralized notification or courtesy timeout.  Examples  startSignerAbortLiquidation transitions to FRAUD_LIQUIDATION_IN_PROGRESS on non-fraud events notifyUndercollateralizedLiquidation and notifyCourtesyTimeout  tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L96-L108  /// @notice         Starts signer liquidation due to abort or undercollateralization  /// @dev            We first attempt to liquidate on chain, then by auction  /// @param  _d      deposit storage pointer  function startSignerAbortLiquidation(DepositUtils.Deposit storage _d) internal {  _d.logStartedLiquidation(false);  // Reclaim used state for gas savings  _d.redemptionTeardown();  _d.seizeSignerBonds();  _d.liquidationInitiated = block.timestamp;  // Store the timestamp for auction  _d.liquidationInitiator = msg.sender;  _d.setFraudLiquidationInProgress();  Recommendation  Verify state transitions and either remove LIQUIDATION_IN_PROGRESS if it is redundant or fix the state transitions for non-fraud liquidations.  Note that Deposit states can be simplified by removing redundant states by setting a flag (e.g. fraudLiquidation) in the deposit instead of adding a state to track the fraud liquidation path.  According to the specification, we assume the following state transitions are desired:  LIQUIDATION_IN_PROGRESS  In case of liquidation due to undercollateralization or abort, the remaining bond value is split 50-50 between the account which triggered the liquidation and the signers.  FRAUD_LIQUIDATION_IN_PROGRESS  In case of liquidation due to fraud, the remaining bond value in full goes to the account which triggered the liquidation by proving fraud.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.15 tbtc - various deposit state transitions can be front-run (e.g. fraud proofs, timeouts)   ",
        "body": "  Resolution  Addressed with the discussion at https://github.com/keep-network/tbtc/issues/498. It is accepted that a malicious entity may be able to front-run certain fraud proofs as long as fraud is being called out. It is also accepted that calls to certain timeouts may be front-run which could lead to a scenario where the client implementation is always front-run by a malicious actor.  Additionally, the client provided the following statement:  In general, we are comfortable with front-runnable interactions that ensure system integrity, as long as such front-running does not remove the original incentive of the submitter. We believe remaining front-runnable interactions have clear benefits to system actors, such that even if they are front-run, they have reason to submit the transaction.  Description  An entity that can provide proof for fraudulent ECDSA signatures or SPV proofs in the liquidation flow is rewarded with part of the deposit contract ETH value.  Specification: Liquidation Any signer bond left over after the deposit owner is compensated is distributed to the account responsible for reporting the misbehavior (for fraud) or between the signers and the account that triggered liquidation (for collateralization issues).  However, the methods under which proof is provided are not protected from front-running allowing anyone to observe transactions to provideECDSAFraudProof/ provideSPVFraudProof and submit the same proofs with providing a higher gas value.  Please note that a similar issue exists for timeout states providing rewards for calling them out (i.e. they set the liquidationInitiator address).  Examples  provideECDSAFraudProof verifies the fraudulent proof  r,s,v,signedDigest appear to be the fraudulent signature. _preimage is the correct value.  tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L117-L137  /// @param _preimage        The sha256 preimage of the digest  function provideECDSAFraudProof(  DepositUtils.Deposit storage _d,  uint8 _v,  bytes32 _r,  bytes32 _s,  bytes32 _signedDigest,  bytes memory _preimage  ) public {  require(  !_d.inFunding() && !_d.inFundingFailure(),  \"Use provideFundingECDSAFraudProof instead\"  );  require(  !_d.inSignerLiquidation(),  \"Signer liquidation already in progress\"  );  require(!_d.inEndState(), \"Contract has halted\");  require(submitSignatureFraud(_d, _v, _r, _s, _signedDigest, _preimage), \"Signature is not fraud\");  startSignerFraudLiquidation(_d);  startSignerFraudLiquidation sets the address that provides the proof as the beneficiary  tbtc/implementation/contracts/deposit/DepositFunding.sol:L153-L179  function provideFundingECDSAFraudProof(  DepositUtils.Deposit storage _d,  uint8 _v,  bytes32 _r,  bytes32 _s,  bytes32 _signedDigest,  bytes memory _preimage  ) public {  require(  _d.inAwaitingBTCFundingProof(),  \"Signer fraud during funding flow only available while awaiting funding\"  );  bool _isFraud = _d.submitSignatureFraud(_v, _r, _s, _signedDigest, _preimage);  require(_isFraud, \"Signature is not fraudulent\");  _d.logFraudDuringSetup();  // If the funding timeout has elapsed, punish the funder too!  if (block.timestamp > _d.fundingProofTimerStart + TBTCConstants.getFundingTimeout()) {  address(0).transfer(address(this).balance);  // Burn it all down (fire emoji)  _d.setFailedSetup();  } else {  /* NB: This is reuse of the variable */  _d.fundingProofTimerStart = block.timestamp;  _d.setFraudAwaitingBTCFundingProof();  purchaseSignerBondsAtAuction pays out the funds  tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L260-L276  uint256 contractEthBalance = address(this).balance;  address payable initiator = _d.liquidationInitiator;  if (initiator == address(0)){  initiator = address(0xdead);  if (contractEthBalance > 1) {  if (_wasFraud) {  initiator.transfer(contractEthBalance);  } else {  // There will always be a liquidation initiator.  uint256 split = contractEthBalance.div(2);  _d.pushFundsToKeepGroup(split);  initiator.transfer(split);  Recommendation  For fraud proofs, it should be required that the reporter uses a commit/reveal scheme to lock in a proof in one block, and reveal the details in another.  ",
        "labels": [
            "Consensys",
            "Major",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.16 tbtc - Anyone can emit log events due to missing access control    Addressed",
        "body": "  Resolution   Addressed with   https://github.com/keep-network/tbtc/issues/477,  keep-network/tbtc#467 and  keep-network/tbtc#537 by restricting log calls to known  Description  Examples  tbtc/implementation/contracts/DepositLog.sol:L95-L99  function approvedToLog(address _caller) public pure returns (bool) {  /* TODO: auth via system */  _caller;  return true;  Recommendation  Log events are typically initiated by the Deposit contract. Make sure only Deposit contracts deployed by an approved factory can emit logs on TBTCSystem.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.17 DKGResultVerification.verify unsafe packing in signed data    Addressed",
        "body": "  Resolution   Addressed with   keep-network/keep-core#1525 by adding additional checks for  Description  DKGResultVerification.verify allows the sender to arbitrarily move bytes between groupPubKey and misbehaved:  keep-core/contracts/solidity/contracts/libraries/operator/DKGResultVerification.sol:L80  bytes32 resultHash = keccak256(abi.encodePacked(groupPubKey, misbehaved));  Recommendation  Validate the expected length of both and add a salt between the two.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.18 keep-core - Service contract callbacks can be abused to call into other contracts    Addressed",
        "body": "  Resolution  Addressed with keep-network/keep-core#1532 by hardcoding the callback method signature and the following statement:  We still allow specifying an address of the callback contract. This could be beneficial in a situations where one contract pays for a random number for another contract.  A subsequent change in keep-network/keep-ecdsa#339 updated keep-tecdsa to use the new, hardcoded callback function: __beaconCallback(uint256).  Description  KeepRandomBeaconServiceImplV1 allows senders to specify an arbitrary method and contract that will receive a callback once the beacon generates a relay entry:  keep-core/contracts/solidity/contracts/KeepRandomBeaconServiceImplV1.sol:L228-L245  /**  @dev Creates a request to generate a new relay entry, which will include  a random number (by signing the previous entry's random number).  @param callbackContract Callback contract address. Callback is called once a new relay entry has been generated.  @param callbackMethod Callback contract method signature. String representation of your method with a single  uint256 input parameter i.e. \"relayEntryCallback(uint256)\".  @param callbackGas Gas required for the callback.  The customer needs to ensure they provide a sufficient callback gas  to cover the gas fee of executing the callback. Any surplus is returned  to the customer. If the callback gas amount turns to be not enough to  execute the callback, callback execution is skipped.  @return An uint256 representing uniquely generated relay request ID. It is also returned as part of the event.  /  function requestRelayEntry(  address callbackContract,  string memory callbackMethod,  uint256 callbackGas  ) public nonReentrant payable returns (uint256) {  Once an operator contract receives the relay entry, it calls executeCallback:  keep-core/contracts/solidity/contracts/KeepRandomBeaconServiceImplV1.sol:L314-L335  /**  @dev Executes customer specified callback for the relay entry request.  @param requestId Request id tracked internally by this contract.  @param entry The generated random number.  @return Address to receive callback surplus.  /  function executeCallback(uint256 requestId, uint256 entry) public returns (address payable surplusRecipient) {  require(  _operatorContracts.contains(msg.sender),  \"Only authorized operator contract can call execute callback.\"  );  require(  _callbacks[requestId].callbackContract != address(0),  \"Callback contract not found\"  );  _callbacks[requestId].callbackContract.call(abi.encodeWithSignature(_callbacks[requestId].callbackMethod, entry));  surplusRecipient = _callbacks[requestId].surplusRecipient;  delete _callbacks[requestId];  Arbitrary callbacks can be used to force the service contract to execute many functions within the keep contract system. Currently, the KeepRandomBeaconOperator includes an onlyServiceContract modifier:  keep-core/contracts/solidity/contracts/KeepRandomBeaconOperator.sol:L150-L159  /**  @dev Checks if sender is authorized.  /  modifier onlyServiceContract() {  require(  serviceContracts.contains(msg.sender),  \"Caller is not an authorized contract\"  );  _;  The functions it protects cannot be targeted by the aforementioned service contract callbacks due to Solidity s CALLDATASIZE checking. However, the presence of the modifier suggests that the service contract is expected to be a permissioned actor within some contracts.  Recommendation  Stick to a constant callback method signature, rather than allowing users to submit an arbitrary string. An example is __beaconCallback__(uint256).  Consider disallowing arbitrary callback destinations. Instead, rely on contracts making requests directly, and default the callback destination to msg.sender. Ensure the sender is not an EOA.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.19 tbtc - Disallow signatures with high-s values in DepositRedemption.provideRedemptionSignature    Addressed",
        "body": "  Resolution   Issue addressed in   keep-network/tbtc#518  Description  DepositRedemption.provideRedemptionSignature is used by signers to publish a signature that can be used to redeem a deposit on Bitcoin. The function accepts a signature s value in the upper half of the secp256k1 curve:  tbtc/implementation/contracts/deposit/DepositRedemption.sol:L183-L202  function provideRedemptionSignature(  DepositUtils.Deposit storage _d,  uint8 _v,  bytes32 _r,  bytes32 _s  ) public {  require(_d.inAwaitingWithdrawalSignature(), \"Not currently awaiting a signature\");  // If we're outside of the signature window, we COULD punish signers here  // Instead, we consider this a no-harm-no-foul situation.  // The signers have not stolen funds. Most likely they've just inconvenienced someone  // The signature must be valid on the pubkey  require(  _d.signerPubkey().checkSig(  _d.lastRequestedDigest,  _v, _r, _s  ),  \"Invalid signature\"  );  Although ecrecover accepts signatures with these s values, they are no longer used in Bitcoin. As such, the signature will appear to be valid to the Ethereum smart contract, but will likely not be accepted on Bitcoin. If no users watching malleate the signature, the redemption process will likely enter a fee increase loop, incurring a cost on the deposit owner.  Recommendation  Ensure the passed-in s value is restricted to the lower half of the secp256k1 curve, as done in BondedECDSAKeep:  keep-tecdsa/solidity/contracts/BondedECDSAKeep.sol:L333-L340  // Validate `s` value for a malleability concern described in EIP-2.  // Only signatures with `s` value in the lower half of the secp256k1  // curve's order are considered valid.  require(  uint256(_s) <=  0x7FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF5D576E7357A4501DDFE92F46681B20A0,  \"Malleable signature - s should be in the low half of secp256k1 curve's order\"  );  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.20 Consistent use of SafeERC20 for external tokens    Addressed",
        "body": "  Resolution   Addressed with   https://github.com/keep-network/keep-core/issues/1407 and  https://github.com/keep-network/keep-tecdsa/issues/272.  Description  Use SafeERC20 features to interact with potentially broken tokens used in the system. E.g. TokenGrant.receiveApproval() is using safeTransferFrom while other contracts aren t.  Examples  TokenGrant.receiveApproval using safeTransferFrom  keep-core/contracts/solidity/contracts/TokenGrant.sol:L200-L200  token.safeTransferFrom(_from, address(this), _amount);  TokenStaking.receiveApproval not using safeTransferFrom while safeTransfer is being used.  keep-core/contracts/solidity/contracts/TokenStaking.sol:L75-L75  token.transferFrom(_from, address(this), _value);  keep-core/contracts/solidity/contracts/TokenStaking.sol:L103-L103  token.safeTransfer(owner, amount);  keep-core/contracts/solidity/contracts/TokenStaking.sol:L193-L193  token.transfer(tattletale, tattletaleReward);  distributeERC20ToMembers not using safeTransferFrom  keep-tecdsa/solidity/contracts/BondedECDSAKeep.sol:L459-L463  token.transferFrom(  msg.sender,  tokenStaking.magpieOf(members[i]),  dividend  );  Recommendation  Consistently use SafeERC20 to support potentially broken tokens external to the system.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.21 Initialize implementations for proxy contracts and protect initialization methods    Addressed",
        "body": "  Resolution   This issue is addressed with the following changesets that ensure that the logic contracts cannot be used by other parties by initializing them in the constructor:   https://github.com/keep-network/keep-tecdsa/issues/297,  https://github.com/keep-network/keep-core/issues/1424, and  https://github.com/keep-network/tbtc/issues/500.  Description  It should be avoided that the implementation for proxy contracts can be initialized by third parties. This can be the case if the initialize function is unprotected. Since the implementation contract is not meant to be used directly without a proxy delegate-calling it is recommended to protect the initialization method of the implementation by initializing on deployment.  Changing the proxies implementation (upgradeTo()) to a version that does not protect the initialization method may allow someone to front-run and initialize the contract if it is not done within the same transaction.  Examples  KeepVendor delegates to KeepVendorImplV1. The implementations initialization method is unprotected.  keep-tecdsa/solidity/contracts/BondedECDSAKeepVendorImplV1.sol:L22-L32  /// @notice Initializes Keep Vendor contract implementation.  /// @param registryAddress Keep registry contract linked to this contract.  function initialize(  address registryAddress  public  require(!initialized(), \"Contract is already initialized.\");  _initialized[\"BondedECDSAKeepVendorImplV1\"] = true;  registry = Registry(registryAddress);  KeepRandomBeaconServiceImplV1 and KeepRandomBeaconServiceUpgradeExample  keep-core/contracts/solidity/contracts/KeepRandomBeaconServiceImplV1.sol:L118-L137  function initialize(  uint256 priceFeedEstimate,  uint256 fluctuationMargin,  uint256 dkgContributionMargin,  uint256 withdrawalDelay,  address registry  public  require(!initialized(), \"Contract is already initialized.\");  _initialized[\"KeepRandomBeaconServiceImplV1\"] = true;  _priceFeedEstimate = priceFeedEstimate;  _fluctuationMargin = fluctuationMargin;  _dkgContributionMargin = dkgContributionMargin;  _withdrawalDelay = withdrawalDelay;  _pendingWithdrawal = 0;  _previousEntry = _beaconSeed;  _registry = registry;  _baseCallbackGas = 18845;  Deposit is deployed via cloneFactory delegating to a masterDepositAddress in DepositFactory. The masterDepositAddress (Deposit) might be left uninitialized.  tbtc/implementation/contracts/system/DepositFactoryAuthority.sol:L3-L14  contract DepositFactoryAuthority {  bool internal _initialized = false;  address internal _depositFactory;  /// @notice Set the address of the System contract on contract initialization  function initialize(address _factory) public {  require(! _initialized, \"Factory can only be initialized once.\");  _depositFactory = _factory;  _initialized = true;  Recommendation  Initialize unprotected implementation contracts in the implementation s constructor. Protect initialization methods from being called by unauthorized parties or ensure that deployment of the proxy and initialization is performed in the same transaction.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.22 keep-tecdsa - If caller sends more than is contained in the signer subsidy pool, the value is burned    Addressed",
        "body": "  Resolution   Issue addressed in   keep-network/keep-ecdsa#306. The  Description  The signer subsidy pool in BondedECDSAKeepFactory tracks funds sent to the contract. Each time a keep is opened, the subsidy pool is intended to be distributed to the members of the new keep:  keep-tecdsa/solidity/contracts/BondedECDSAKeepFactory.sol:L312-L320  // If subsidy pool is non-empty, distribute the value to signers but  // never distribute more than the payment for opening a keep.  uint256 signerSubsidy = subsidyPool < msg.value  ? subsidyPool  : msg.value;  if (signerSubsidy > 0) {  subsidyPool -= signerSubsidy;  keep.distributeETHToMembers.value(signerSubsidy)();  The tracking around subsidy pool increases is inconsistent, and can lead to sent value being burned. In the case that subsidyPool contains less Ether than is sent in msg.value, msg.value is unused and remains in the contract. It may or may not be added to subsidyPool, depending on the return status of the random beacon:  keep-tecdsa/solidity/contracts/BondedECDSAKeepFactory.sol:L347-L357  (bool success, ) = address(randomBeacon).call.gas(400000).value(msg.value)(  abi.encodeWithSignature(  \"requestRelayEntry(address,string,uint256)\",  address(this),  \"setGroupSelectionSeed(uint256)\",  callbackGas  );  if (!success) {  subsidyPool += msg.value; // beacon is busy  Recommendation  Rather than tracking the subsidyPool individually, simply distribute this.balance to each new keep s members.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.23 keep-core - TokenGrant and TokenStaking allow staking zero amount of tokens and front-running    Addressed",
        "body": "  Resolution   Addressed with   https://github.com/keep-network/keep-core/issues/1425 and  keep-network/keep-core#1461 by requiring a hardcoded minimum amount of tokens to be staked.  Description  Tokens are staked via the callback receiveApproval() which is normally invoked when calling approveAndCall(). The method is not restricting who can initiate the staking of tokens and relies on the fact that the token transfer to the TokenStaking contract is pre-approved by the owner, otherwise, the call would revert.  However, receiveApproval() allows the staking of a zero amount of tokens. The only check performed on the number of tokens transferred is, that the token holders balance covers the amount to be transferred. This check is both relatively weak - having enough balance does not imply that tokens are approved for transfer - and does not cover the fact that someone can call the method with a zero amount of tokens.  This way someone could create an arbitrary number of operators staking no tokens at all. This passes the token balance check, token.transferFrom() will succeed and an operator struct with a zero stake and arbitrary values for operator, from, magpie, authorizer can be set. Finally, an event is emitted for a zero stake.  An attacker could front-run calls to receiveApproval to block staking of a legitimate operator by creating a zero stake entry for the operator before she is able to. This vector might allow someone to permanently inconvenience an operator s address. To recover from this situation one could be forced to cancelStake terminating the zero stake struct in order to call the contract with the correct stake again.  The same issue exists for TokenGrant.  Examples  keep-core/contracts/solidity/contracts/TokenStaking.sol:L54-L81  /**  @notice Receives approval of token transfer and stakes the approved amount.  @dev Makes sure provided token contract is the same one linked to this contract.  @param _from The owner of the tokens who approved them to transfer.  @param _value Approved amount for the transfer and stake.  @param _token Token contract address.  @param _extraData Data for stake delegation. This byte array must have the  following values concatenated: Magpie address (20 bytes) where the rewards for participation  are sent, operator's (20 bytes) address, authorizer (20 bytes) address.  /  function receiveApproval(address _from, uint256 _value, address _token, bytes memory _extraData) public {  require(ERC20Burnable(_token) == token, \"Token contract must be the same one linked to this contract.\");  require(_value <= token.balanceOf(_from), \"Sender must have enough tokens.\");  require(_extraData.length == 60, \"Stake delegation data must be provided.\");  address payable magpie = address(uint160(_extraData.toAddress(0)));  address operator = _extraData.toAddress(20);  require(operators[operator].owner == address(0), \"Operator address is already in use.\");  address authorizer = _extraData.toAddress(40);  // Transfer tokens to this contract.  token.transferFrom(_from, address(this), _value);  operators[operator] = Operator(_value, block.number, 0, _from, magpie, authorizer);  ownerOperators[_from].push(operator);  emit Staked(operator, _value);  Recommendation  Require tokens to be staked and explicitly disallow the zero amount of tokens case. The balance check can be removed.  Note: Consider checking the calls return value or calling the contract via SafeERC20 to support potentially broken tokens that do not revert in error cases (token.transferFrom).  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.24 tbtc - Inconsistency between increaseRedemptionFee and provideRedemptionProof may create un-provable redemptions    Addressed",
        "body": "  Resolution   Issue addressed in   keep-network/tbtc#522  Description  DepositRedemption.increaseRedemptionFee is used by signers to approve a signable bitcoin transaction with a higher fee, in case the network is congested and miners are not approving the lower-fee transaction.  Fee increases can be performed every 4 hours:  tbtc/implementation/contracts/deposit/DepositRedemption.sol:L225  require(block.timestamp >= _d.withdrawalRequestTime + TBTCConstants.getIncreaseFeeTimer(), \"Fee increase not yet permitted\");  In addition, each increase must increment the fee by exactly the initial proposed fee:  tbtc/implementation/contracts/deposit/DepositRedemption.sol:L260-L263  // Check that we're incrementing the fee by exactly the redeemer's initial fee  uint256 _previousOutputValue = DepositUtils.bytes8LEToUint(_previousOutputValueBytes);  _newOutputValue = DepositUtils.bytes8LEToUint(_newOutputValueBytes);  require(_previousOutputValue.sub(_newOutputValue) == _d.initialRedemptionFee, \"Not an allowed fee step\");  Outside of these two restrictions, there is no limit to the number of times increaseRedemptionFee can be called. Over a 20-hour period, for example, increaseRedemptionFee could be called 5 times, increasing the fee to initialRedemptionFee * 5. Over a 24-hour period, increaseRedemptionFee could be called 6 times, increasing the fee to initialRedemptionFee * 6.  Eventually, it is expected that a transaction will be submitted and mined. At this point, anyone can call DepositRedemption.provideRedemptionProof, finalizing the redemption process and rewarding the signers. However, provideRedemptionProof will fail if the transaction fee is too high:  tbtc/implementation/contracts/deposit/DepositRedemption.sol:L308  require((_d.utxoSize().sub(_fundingOutputValue)) <= _d.initialRedemptionFee * 5, \"Fee unexpectedly very high\");  In the case that increaseRedemptionFee is called 6 times and the signers provide a signature for this transaction, the transaction can be submitted and mined but provideRedemptionProof for this will always fail. Eventually, a redemption proof timeout will trigger the deposit into liquidation and the signers will be punished.  Recommendation  Because it is difficult to say with certainty that a 5x fee increase will always ensure a transaction s redeemability, the upper bound on fee bumps should be removed from provideRedemptionProof.  This should be implemented in tandem with issue 5.37, so that signers cannot provide a proof that bypasses increaseRedemptionFee flow to spend the highest fee possible.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.25 keep-tecdsa - keep cannot be closed if a members bond was seized or fully reassigned    Addressed",
        "body": "  Description  A keep cannot be closed if the bonds have been completely reassigned or seized before, leaving at least one member with zero lockedBonds. In this case closeKeep() will throw in freeMembersBonds() because the requirement in keepBonding.freeBond is not satisfied anymore (lockedBonds[bondID] > 0). As a result of this, none of the potentially remaining bonds (reassign) are freed, the keep stays active even though it should be closed.  Examples  keep-tecdsa/solidity/contracts/BondedECDSAKeep.sol:L373-L396  /// @notice Closes keep when owner decides that they no longer need it.  /// Releases bonds to the keep members. Keep can be closed only when  /// there is no signing in progress or requested signing process has timed out.  /// @dev The function can be called by the owner of the keep and only is the  /// keep has not been closed already.  function closeKeep() external onlyOwner onlyWhenActive {  require(  !isSigningInProgress() || hasSigningTimedOut(),  \"Requested signing has not timed out yet\"  );  isActive = false;  freeMembersBonds();  emit KeepClosed();  /// @notice Returns bonds to the keep members.  function freeMembersBonds() internal {  for (uint256 i = 0; i < members.length; i++) {  keepBonding.freeBond(members[i], uint256(address(this)));  keep-tecdsa/solidity/contracts/KeepBonding.sol:L173-L190  /// @notice Releases the bond and moves the bond value to the operator's  /// unbounded value pool.  /// @dev Function requires that caller is the holder of the bond which is  /// being released.  /// @param operator Address of the bonded operator.  /// @param referenceID Reference ID of the bond.  function freeBond(address operator, uint256 referenceID) public {  address holder = msg.sender;  bytes32 bondID = keccak256(  abi.encodePacked(operator, holder, referenceID)  );  require(lockedBonds[bondID] > 0, \"Bond not found\");  uint256 amount = lockedBonds[bondID];  lockedBonds[bondID] = 0;  unbondedValue[operator] = amount;  Recommendation  Make sure the keep can be set to an end-state (closed/inactive) indicating its end-of-life even if the bond has been seized before. Avoid throwing an exception when freeing member bonds to avoid blocking the unlocking of bonds.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.26 tbtc - provideFundingECDSAFraudProof attempts to burn non-existent funds    Addressed",
        "body": "  Resolution   Addressed as   https://github.com/keep-network/tbtc/issues/502 and fixed with  keep-network/tbtc#523.  Description  The funding flow was recently changed from requiring the funder to provide a bond that stays in the Deposit contract to forwarding the funds to the keep, paying for the keep setup.  So at a high level, the funding bond was designed to ensure that funders had some minimum skin in the game, so that DoSing signers/the system was expensive. The upside was that we could refund it in happy paths. Now that we ve realized that opening the keep itself will cost enough to prevent DoS, the concept of refunding goes away entirely. We definitely missed cleaning up the funder handling in provideFundingECDSAFraudProof though.  Examples  tbtc/implementation/contracts/deposit/DepositFunding.sol:L170-L173  // If the funding timeout has elapsed, punish the funder too!  if (block.timestamp > _d.fundingProofTimerStart + TBTCConstants.getFundingTimeout()) {  address(0).transfer(address(this).balance);  // Burn it all down (fire emoji)  _d.setFailedSetup();  Recommendation  Remove the line that attempts to punish the funder by burning the Deposit contract balance which is zero due to recent changes in how the payment provided with createNewDepositis handled.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.27 bitcoin-spv - Bitcoin output script length is not checked in wpkhSpendSighash   ",
        "body": "  Resolution   Summa opted not to make this change. See   https://github.com/summa-tx/bitcoin-spv/issues/112 for details.  Description  CheckBitcoinSigs.wpkhSpendSighash calculates the sighash of a Bitcoin transaction. Among its parameters, it accepts bytes memory _outpoint, which is a 36-byte UTXO id consisting of a 32-byte transaction hash and a 4-byte output index.  The function in question should not accept an _outpoint that is not 36-bytes, but no length check is made:  bitcoin-spv/solidity/contracts/CheckBitcoinSigs.sol:L130-L159  function wpkhSpendSighash(  bytes memory _outpoint,  // 36 byte UTXO id  bytes20 _inputPKH,       // 20 byte hash160  bytes8 _inputValue,      // 8-byte LE  bytes8 _outputValue,     // 8-byte LE  bytes memory _outputScript    // lenght-prefixed output script  ) internal pure returns (bytes32) {  // Fixes elements to easily make a 1-in 1-out sighash digest  // Does not support timelocks  bytes memory _scriptCode = abi.encodePacked(  hex\"1976a914\",  // length, dup, hash160, pkh_length  _inputPKH,  hex\"88ac\");  // equal, checksig  bytes32 _hashOutputs = abi.encodePacked(  _outputValue,  // 8-byte LE  _outputScript).hash256();  bytes memory _sighashPreimage = abi.encodePacked(  hex\"01000000\",  // version  _outpoint.hash256(),  // hashPrevouts  hex\"8cb9012517c817fead650287d61bdd9c68803b6bf9c64133dcab3e65b5a50cb9\",  // hashSequence(00000000)  _outpoint,  // outpoint  _scriptCode,  // p2wpkh script code  _inputValue,  // value of the input in 8-byte LE  hex\"00000000\",  // input nSequence  _hashOutputs,  // hash of the single output  hex\"00000000\",  // nLockTime  hex\"01000000\"  // SIGHASH_ALL  );  return _sighashPreimage.hash256();  Recommendation  Check that _outpoint.length is 36.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.28 tbtc - liquidationInitiator can block purchaseSignerBondsAtAuction indefinitely    Addressed",
        "body": "  Resolution   Addressed with   https://github.com/keep-network/tbtc/issues/503 and commits from  keep-network/tbtc#524 switching from  Description  When reporting a fraudulent proof the deposits liquidationInitiator is set to the entity reporting and proofing the fraud. The deposit that is in a *_liquidation_in_progress state can be bought by anyone at an auction calling purchaseSignerBondsAtAuction.  Instead of receiving a share of the funds the liquidationInitiator can decide to intentionally reject the funds by raising an exception causing initiator.transfer(contractEthBalance) to throw, blocking the auction and forcing the liquidation to fail. The deposit will stay in one of the *_liquidation_in_progress states.  Examples  tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L224-L276  /// @notice     Closes an auction and purchases the signer bonds. Payout to buyer, funder, then signers if not fraud  /// @dev        For interface, reading auctionValue will give a past value. the current is better  /// @param  _d  deposit storage pointer  function purchaseSignerBondsAtAuction(DepositUtils.Deposit storage _d) public {  bool _wasFraud = _d.inFraudLiquidationInProgress();  require(_d.inSignerLiquidation(), \"No active auction\");  _d.setLiquidated();  _d.logLiquidated();  // send the TBTC to the TDT holder. If the TDT holder is the Vending Machine, burn it to maintain the peg.  address tdtHolder = _d.depositOwner();  TBTCToken _tbtcToken = TBTCToken(_d.TBTCToken);  uint256 lotSizeTbtc = _d.lotSizeTbtc();  require(_tbtcToken.balanceOf(msg.sender) >= lotSizeTbtc, \"Not enough TBTC to cover outstanding debt\");  if(tdtHolder == _d.VendingMachine){  _tbtcToken.burnFrom(msg.sender, lotSizeTbtc);  // burn minimal amount to cover size  else{  _tbtcToken.transferFrom(msg.sender, tdtHolder, lotSizeTbtc);  // Distribute funds to auction buyer  uint256 _valueToDistribute = _d.auctionValue();  msg.sender.transfer(_valueToDistribute);  // Send any TBTC left to the Fee Rebate Token holder  _d.distributeFeeRebate();  // For fraud, pay remainder to the liquidation initiator.  // For non-fraud, split 50-50 between initiator and signers. if the transfer amount is 1,  // division will yield a 0 value which causes a revert; instead,  // we simply ignore such a tiny amount and leave some wei dust in escrow  uint256 contractEthBalance = address(this).balance;  address payable initiator = _d.liquidationInitiator;  if (initiator == address(0)){  initiator = address(0xdead);  if (contractEthBalance > 1) {  if (_wasFraud) {  initiator.transfer(contractEthBalance);  } else {  // There will always be a liquidation initiator.  uint256 split = contractEthBalance.div(2);  _d.pushFundsToKeepGroup(split);  initiator.transfer(split);  Recommendation  Use a pull vs push funds pattern or use address.send instead of address.transfer which might leave some funds locked in the contract if it fails.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.29 bitcoin-spv - verifyHash256Merkle allows existence proofs for the same leaf in multiple locations in the tree   ",
        "body": "  Resolution   Summa opted not to make this change, citing inconsistencies in Bitcoin s merkle implementation. See   https://github.com/summa-tx/bitcoin-spv/issues/108 for details.  Description  BTCUtils.verifyHash256Merkle is used by ValidateSPV.prove to validate a transaction s existence in a Bitcoin block. The function accepts as input a _proof and an _index. The _proof consists of, in order: the transaction hash, a list of intermediate nodes, and the merkle root.  The proof is performed iteratively, and uses the _index to determine whether the next proof element represents a  left branch  or a  right branch:   bitcoin-spv/solidity/contracts/BTCUtils.sol:L574-L586  uint _idx = _index;  bytes32 _root = _proof.slice(_proof.length - 32, 32).toBytes32();  bytes32 _current = _proof.slice(0, 32).toBytes32();  for (uint i = 1; i < (_proof.length.div(32)) - 1; i++) {  if (_idx % 2 == 1) {  _current = _hash256MerkleStep(_proof.slice(i * 32, 32), abi.encodePacked(_current));  } else {  _current = _hash256MerkleStep(abi.encodePacked(_current), _proof.slice(i * 32, 32));  _idx = _idx >> 1;  return _current == _root;  If _idx is even, the computed hash is placed before the next proof element. If _idx is odd, the computed hash is placed after the next proof element. After each iteration, _idx is decremented by _idx /= 2.  Because verifyHash256Merkle makes no requirements on the size of _proof relative to _index, it is possible to pass in invalid values for _index that prove a transaction s existence in multiple locations in the tree.  Examples  By modifying existing tests, we showed that any transaction can be proven to exist at least one alternate index. This alternate index is calculated as (2 ** treeHeight) + prevIndex - though other alternate indices are possible. The modified test is below:  Recommendation  Use the length of _proof to determine the maximum allowed _index. _index should satisfy the following criterion: _index < 2 ** (_proof.length.div(32) - 2).  Note that subtraction by 2 accounts for the transaction hash and merkle root, which are assumed to be encoded in the proof along with the intermediate nodes.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.30 keep-core - stake operator should not be eligible if undelegatedAt is set    Addressed",
        "body": "  Resolution  Addressed with https://github.com/keep-network/keep-core/issues/1433 by enforcing that stake must be canceled in initialization period.  undelegatedAt is intended to support undelegation in advance at any given time. Whether we do < or <= is not actually significant, as transaction reordering also means ability to include/not include transactions arbitrarily, but changing the check to operator.UndelegatedAt == 0 would ruin e.g. the use-case where Alice wants to delegate to Bob for 12 months. If we don t currently need that use-case, the check can be simplified to == 0.  Description  An operator s stake should not be eligible if they stake an amount and immediately call undelegate in an attempt to indicate that they are going to recover their stake soon.  Examples  keep-core/contracts/solidity/contracts/TokenStaking.sol:L232-L236  bool notUndelegated = block.number <= operator.undelegatedAt || operator.undelegatedAt == 0;  if (isAuthorized && isActive && notUndelegated) {  balance = operator.amount;  Recommendation  A stake that is entering undelegation is indicated by operator.undelegatedAt being non-zero. Change the notUndelegated check block.number <= operator.undelegatedAt || operator.undelegatedAt == 0 to operator.undelegatedAT == 0 as any value being set indicates that undelegation is in progress.  Enforce that within the initialization period stake is canceled instead of being undelegated.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.31 keep-core - Specification inconsistency: TokenStaking amount to be slashed/seized    Addressed",
        "body": "  Resolution   Partially addressed with   https://github.com/keep-network/keep-core/issues/1428 by ensuring that at least some stack is slashed. As noted in the issue, the case where less than the minimum stake was slashed from an operator is left unhandled with this fix.  Description  The keep specification states that slash and seize affect at least the amount specified or the remaining stake of a member.  Slash each operator in the list misbehavers by the specified amount (or their remaining stake, whichever is lower).  Punish each operator in the list misbehavers by the specified amount or their remaining stake.  The implementation, however, bails if one of the accounts does not have enough stake to be slashed or seized because of the use of SafeMath.sub(). This behavior is inconsistent with the specification which states that min(amount, misbehaver.stake) stake should be affected. The call to slash/seize will revert and no stakes are affected. At max, the staked amount of the lowest staker can be slashed/seized from every staker.  Implementing this method as stated in the specification using min(amount, misbehaver.stake) will cover the fact that slashing/seizing was only partially successful. If misbehaver.stake is zero no error might be emitted even though no stake was slashed/seized.  Examples  keep-core/contracts/solidity/contracts/TokenStaking.sol:L151-L195  /**  @dev Slash provided token amount from every member in the misbehaved  operators array and burn 100% of all the tokens.  @param amount Token amount to slash from every misbehaved operator.  @param misbehavedOperators Array of addresses to seize the tokens from.  /  function slash(uint256 amount, address[] memory misbehavedOperators)  public  onlyApprovedOperatorContract(msg.sender) {  for (uint i = 0; i < misbehavedOperators.length; i++) {  address operator = misbehavedOperators[i];  require(authorizations[msg.sender][operator], \"Not authorized\");  operators[operator].amount = operators[operator].amount.sub(amount);  token.burn(misbehavedOperators.length.mul(amount));  /**  @dev Seize provided token amount from every member in the misbehaved  operators array. The tattletale is rewarded with 5% of the total seized  amount scaled by the reward adjustment parameter and the rest 95% is burned.  @param amount Token amount to seize from every misbehaved operator.  @param rewardMultiplier Reward adjustment in percentage. Min 1% and 100% max.  @param tattletale Address to receive the 5% reward.  @param misbehavedOperators Array of addresses to seize the tokens from.  /  function seize(  uint256 amount,  uint256 rewardMultiplier,  address tattletale,  address[] memory misbehavedOperators  ) public onlyApprovedOperatorContract(msg.sender) {  for (uint i = 0; i < misbehavedOperators.length; i++) {  address operator = misbehavedOperators[i];  require(authorizations[msg.sender][operator], \"Not authorized\");  operators[operator].amount = operators[operator].amount.sub(amount);  uint256 total = misbehavedOperators.length.mul(amount);  uint256 tattletaleReward = (total.mul(5).div(100)).mul(rewardMultiplier).div(100);  token.transfer(tattletale, tattletaleReward);  token.burn(total.sub(tattletaleReward));  Recommendation  Require that minimumStake has been provided and can be seized/slashed. Update the documentation to reflect the fact that the solution always seizes/slashes minimumStake. Ensure that stakers cannot cancel their stake while they are actively participating in the network.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.32 keep-tecdsa - Change state-mutability of checkSignatureFraud to view    Addressed",
        "body": "  Resolution   Addressed as part of   https://github.com/keep-network/keep-tecdsa/issues/254 with commits from  keep-network/keep-tecdsa#283 splitting the method into two parts:  Description  BondedECDSAKeep.sol.submitSignatureFraud is not state-changing and should, therefore, be declared with the function state-mutability view.  Examples  keep-tecdsa/solidity/contracts/BondedECDSAKeep.sol:L265-L290  function submitSignatureFraud(  uint8 _v,  bytes32 _r,  bytes32 _s,  bytes32 _signedDigest,  bytes calldata _preimage  ) external returns (bool _isFraud) {  require(publicKey.length != 0, \"Public key was not set yet\");  bytes32 calculatedDigest = sha256(_preimage);  require(  _signedDigest == calculatedDigest,  \"Signed digest does not match double sha256 hash of the preimage\"  );  bool isSignatureValid = publicKeyToAddress(publicKey) ==  ecrecover(_signedDigest, _v, _r, _s);  // Check if the signature is valid but was not requested.  require(  isSignatureValid && !digests[_signedDigest],  \"Signature is not fraudulent\"  );  return true;  Recommendation  Declare method as view. Consider renaming submitSignatureFraud to e.g. checkSignatureFraud to emphasize that it is only checking the signature and not actually changing state.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.33 keep-core - Specification inconsistency: TokenStaking.slash() is never called    Addressed",
        "body": "  Resolution   Addressed with   https://github.com/keep-network/keep-tecdsa/issues/254 and changesets from  keep-network/keep-tecdsa#283 by slashing the signer stakes when signature fraud is proven.  Description  According to the keep specification stake should be slashed if a staker violates the protocol:  Slashing If a staker violates the protocol of an operation in a way which can be proven on-chain, they will be penalized by having their stakes slashed.  While this functionality can only be called by the approved operator contract, it is not being used throughout the system. In contrast seize() is being called when reporting unauthorized signing or relay entry timeout.  Examples  keep-core/contracts/solidity/contracts/TokenStaking.sol:L151-L167  /**  @dev Slash provided token amount from every member in the misbehaved  operators array and burn 100% of all the tokens.  @param amount Token amount to slash from every misbehaved operator.  @param misbehavedOperators Array of addresses to seize the tokens from.  /  function slash(uint256 amount, address[] memory misbehavedOperators)  public  onlyApprovedOperatorContract(msg.sender) {  for (uint i = 0; i < misbehavedOperators.length; i++) {  address operator = misbehavedOperators[i];  require(authorizations[msg.sender][operator], \"Not authorized\");  operators[operator].amount = operators[operator].amount.sub(amount);  token.burn(misbehavedOperators.length.mul(amount));  Recommendation  Implement slashing according to the specification.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.34 tbtc - Remove notifyDepositExpiryCourtesyCall and allow exitCourtesyCall exiting the courtesy call at term    Addressed",
        "body": "  Resolution   Addressed with   keep-network/tbtc#476 following the recommendation.  Description  Following a deep dive into state transitions with the client it was agreed that notifyDepositExpiryCourtesyCall should be removed from the system as it is a left-over of a previous version of the deposit contract.  Additionally, exitCourtesyCall should be callable at any time.  Examples  tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L289-L298  /// @notice     Goes from courtesy call to active  /// @dev        Only callable if collateral is sufficient and the deposit is not expiring  /// @param  _d  deposit storage pointer  function exitCourtesyCall(DepositUtils.Deposit storage _d) public {  require(_d.inCourtesyCall(), \"Not currently in courtesy call\");  require(block.timestamp <= _d.fundedAt + TBTCConstants.getDepositTerm(), \"Deposit is expiring\");  require(getCollateralizationPercentage(_d) >= _d.undercollateralizedThresholdPercent, \"Deposit is still undercollateralized\");  _d.setActive();  _d.logExitedCourtesyCall();  Recommendation  Remove the notifyDepositExpiryCourtesyCall state transition and remove the requirement on exitCourtesyCall being callable only before the deposit expires.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.35 keep-tecdsa - withdraw should check for zero value transfer    Addressed",
        "body": "  Resolution   Addressed with   https://github.com/keep-network/keep-tecdsa/issues/280 by denying zero value withdrawals.  Description  Requesting the withdrawal of zero ETH in KeepBonding.withdraw should fail as this would allow the method to succeed, calling the user-provided destination even though the sender has no unbonded value.  Examples  keep-tecdsa/solidity/contracts/KeepBonding.sol:L78-L88  function withdraw(uint256 amount, address payable destination) public {  require(  unbondedValue[msg.sender] >= amount,  \"Insufficient unbonded value\"  );  unbondedValue[msg.sender] -= amount;  (bool success, ) = destination.call.value(amount)(\"\");  require(success, \"Transfer failed\");  And a similar instance in BondedECDSAKeep:  keep-tecdsa/solidity/contracts/BondedECDSAKeep.sol:L487-L498  /// @notice Withdraws amount of ether hold in the keep for the member.  /// The value is sent to the beneficiary of the specific member.  /// @param _member Keep member address.  function withdraw(address _member) external {  uint256 value = memberETHBalances[_member];  memberETHBalances[_member] = 0;  /* solium-disable-next-line security/no-call-value */  (bool success, ) = tokenStaking.magpieOf(_member).call.value(value)(\"\");  require(success, \"Transfer failed\");  Recommendation  Require that the amount to be withdrawn is greater than zero.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.36 keep-core - TokenStaking owner should be protected from slash() and seize() during initializationPeriod    Addressed",
        "body": "  Resolution   Addressed by   https://github.com/keep-network/keep-core/issues/1426 and fixed with  keep-network/keep-core#1453.  Description  From the specification:  Slashing If a staker violates the protocol of an operation in a way which can be proven on-chain, they will be penalized by having their stakes slashed.  The initialization period is a backoff time during which operator stakes are not active nor eligible to receive work. Since they cannot misbehave they should be protected from having their stake slashed or seized.  It should also be noted that slash() and seize() can be front-run during the initializationPeriod by having the operator owner cancel the deposit before it is being slashed or seized.  Recommendation  Require deposits to be in active state for being slashed or seized.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.37 tbtc - Signer collusion may bypass increaseRedemptionFee flow    Addressed",
        "body": "  Resolution   Issue addressed in   keep-network/tbtc#522  Description  DepositRedemption.increaseRedemptionFee is used by signers to approve a signable bitcoin transaction with a higher fee, in case the network is congested and miners are not approving the lower-fee transaction.  Fee increases can be performed every 4 hours:  tbtc/implementation/contracts/deposit/DepositRedemption.sol:L225  require(block.timestamp >= _d.withdrawalRequestTime + TBTCConstants.getIncreaseFeeTimer(), \"Fee increase not yet permitted\");  In addition, each increase must increment the fee by exactly the initial proposed fee:  tbtc/implementation/contracts/deposit/DepositRedemption.sol:L260-L263  // Check that we're incrementing the fee by exactly the redeemer's initial fee  uint256 _previousOutputValue = DepositUtils.bytes8LEToUint(_previousOutputValueBytes);  _newOutputValue = DepositUtils.bytes8LEToUint(_newOutputValueBytes);  require(_previousOutputValue.sub(_newOutputValue) == _d.initialRedemptionFee, \"Not an allowed fee step\");  Outside of these two restrictions, there is no limit to the number of times increaseRedemptionFee can be called. Over a 20-hour period, for example, increaseRedemptionFee could be called 5 times, increasing the fee to initialRedemptionFee * 5.  Rather than calling increaseRedemptionFee 5 times over 20 hours, colluding signers may immediately create and sign a transaction with a fee of initialRedemptionFee * 5, wait for it to be mined, then submit it to provideRedemptionProof. Because provideRedemptionProof does not check that a transaction signature signs an approved digest, interested parties would need to monitor the bitcoin blockchain, notice the spend, and provide an ECDSA fraud proof before provideRedemptionProof is called.  Recommendation  Track the latest approved fee, and ensure the transaction in provideRedemptionProof does not include a higher fee.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.38 tbtc - liquidating a deposit does not send the complete remainder of the contract balance to recipients    Addressed",
        "body": "  Resolution   Addressed with   https://github.com/keep-network/tbtc/issues/504 and commits from  keep-network/tbtc#524, transferring the remaining balance of the contract to the initiator and switching from  Description  purchaseSignerBondsAtAuction might leave a wei in the contract if:  there is only one wei remaining in the contract  there is more than one wei remaining but the contract balance is odd.  Examples  contract balances must be > 1 wei otherwise no transfer is attempted  the division at line 271 floors the result if dividing an odd balance. The contract is sending floor(contract.balance / 2) to the keep group and liquidationInitiator leaving one 1 in the contract.  tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L266-L275  if (contractEthBalance > 1) {  if (_wasFraud) {  initiator.transfer(contractEthBalance);  } else {  // There will always be a liquidation initiator.  uint256 split = contractEthBalance.div(2);  _d.pushFundsToKeepGroup(split);  initiator.transfer(split);  Recommendation  Define a reasonable minimum amount when awarding the fraud reporter or liquidation initiator. Alternatively, always transfer the contract balance. When splitting the amount use the contract balance after the first transfer as the value being sent to the second recipient. Use the presence of locked funds in a contract as an error indicator unless funds were sent forcefully to the contract.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.39 tbtc - approveAndCall unused return parameter    Addressed",
        "body": "  Resolution   Addressed with   https://github.com/keep-network/tbtc/issues/505 by returning  Description  approveAndCall always returns false because the return value bool success is never set.  Examples  tbtc/implementation/contracts/system/TBTCDepositToken.sol:L42-L54  /// @notice           Set allowance for other address and notify.  ///                   Allows `_spender` to transfer the specified TDT  ///                   on your behalf and then ping the contract about it.  /// @dev              The `_spender` should implement the `tokenRecipient` interface below  ///                   to receive approval notifications.  /// @param _spender   Address of contract authorized to spend.  /// @param _tdtId     The TDT they can spend.  /// @param _extraData Extra information to send to the approved contract.  function approveAndCall(address _spender, uint256 _tdtId, bytes memory _extraData) public returns (bool success) {  tokenRecipient spender = tokenRecipient(_spender);  approve(_spender, _tdtId);  spender.receiveApproval(msg.sender, _tdtId, address(this), _extraData);  Recommendation  Return the correct success state.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.40 bitcoin-spv - Unnecessary memory allocation in BTCUtils   Pending",
        "body": "  Resolution  The client provided feedback that this issue is not scheduled to be addressed.  Description  BTCUtils makes liberal use of BytesLib.slice, which returns a freshly-allocated slice of an existing bytes array. In many cases, the desired behavior is simply to read a 32-byte slice of a byte array. As a result, the typical pattern used is: bytesVar.slice(start, start + 32).toBytes32().  This pattern introduces unnecessary complexity and memory allocation in a critically important library: cloning a portion of the array, storing that clone in memory, and then reading it from memory. A simpler alternative would be to implement BytesLib.readBytes32(bytes _b, uint _idx) and other  memory-read  functions.  Rather than moving the free memory pointer and redundantly reading, storing, then re-reading memory, readBytes32 and similar functions would perform a simple length check and mload directly from the desired index in the array.  Examples  extractInputTxIdLE:  bitcoin-spv/solidity/contracts/BTCUtils.sol:L254-L260  /// @notice          Extracts the outpoint tx id from an input  /// @dev             32 byte tx id  /// @param _input    The input  /// @return          The tx id (little-endian bytes)  function extractInputTxIdLE(bytes memory _input) internal pure returns (bytes32) {  return _input.slice(0, 32).toBytes32();  verifyHash256Merkle:  bitcoin-spv/solidity/contracts/BTCUtils.sol:L574-L586  uint _idx = _index;  bytes32 _root = _proof.slice(_proof.length - 32, 32).toBytes32();  bytes32 _current = _proof.slice(0, 32).toBytes32();  for (uint i = 1; i < (_proof.length.div(32)) - 1; i++) {  if (_idx % 2 == 1) {  _current = _hash256MerkleStep(_proof.slice(i * 32, 32), abi.encodePacked(_current));  } else {  _current = _hash256MerkleStep(abi.encodePacked(_current), _proof.slice(i * 32, 32));  _idx = _idx >> 1;  return _current == _root;  Recommendation  Implement BytesLib.readBytes32 and favor its use over the bytesVar.slice(start, start + 32).toBytes32() pattern. Implement other memory-read functions where possible, and avoid the use of slice.  Note, too, that implementing this change in verifyHash256Merkle would allow _hash256MerkleStep to accept 2 bytes32 inputs (rather than bytes), removing additional unnecessary casting and memory allocation.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.41 bitcoin-spv - ValidateSPV.validateHeaderChain does not completely validate input   ",
        "body": "  Resolution   Summa opted not to make this change. See   https://github.com/summa-tx/bitcoin-spv/issues/111  Description  ValidateSPV.validateHeaderChain takes as input a sequence of Bitcoin headers and calculates the total accumulated difficulty across the entire sequence. The input headers are checked to ensure they are relatively well-formed:  bitcoin-spv/solidity/contracts/ValidateSPV.sol:L173-L174  // Check header chain length  if (_headers.length % 80 != 0) {return ERR_BAD_LENGTH;}  However, the function lacks a check for nonzero length of _headers. Although the total difficulty returned would be zero, an explicit check would make this more clear.  Recommendation  If headers.length is zero, return ERR_BAD_LENGTH  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.42 bitcoin-spv - unnecessary intermediate cast    Addressed",
        "body": "  Resolution   Issue addressed in   summa-tx/bitcoin-spv#123  Description  Examples  bitcoin-spv/solidity/contracts/CheckBitcoinSigs.sol:L15-L25  /// @notice          Derives an Ethereum Account address from a pubkey  /// @dev             The address is the last 20 bytes of the keccak256 of the address  /// @param _pubkey   The public key X & Y. Unprefixed, as a 64-byte array  /// @return          The account address  function accountFromPubkey(bytes memory _pubkey) internal pure returns (address) {  require(_pubkey.length == 64, \"Pubkey must be 64-byte raw, uncompressed key.\");  // keccak hash of uncompressed unprefixed pubkey  bytes32 _digest = keccak256(_pubkey);  return address(uint160(uint256(_digest)));  Recommendation  The intermediate cast from uint256 to uint160 can be omitted. Refactor to return address(uint256(_digest)) instead.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.43 bitcoin-spv - unnecessary logic in BytesLib.toBytes32()    Addressed",
        "body": "  Resolution   Issue addressed in   summa-tx/bitcoin-spv#125  Description  The heavily used library function BytesLib.toBytes32() unnecessarily casts _source to bytes (same type) and creates a copy of the dynamic byte array to check it s length, while this can be done directly on the user-provided bytes _source.  Examples  bitcoin-spv/solidity/contracts/BytesLib.sol:L399-L408  function toBytes32(bytes memory _source) pure internal returns (bytes32 result) {  bytes memory tempEmptyStringTest = bytes(_source);  if (tempEmptyStringTest.length == 0) {  return 0x0;  assembly {  result := mload(add(_source, 32))  Recommendation  function toBytes32(bytes memory _source) pure internal returns (bytes32 result) {  if (_source.length == 0) {  return 0x0;  assembly {  result := mload(add(_source, 32))  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.44 bitcoin-spv - redundant functionality   ",
        "body": "  Resolution   Summa opted not to make this change. See   https://github.com/summa-tx/bitcoin-spv/issues/116 for details.  Description  The library exposes redundant implementations of bitcoins double sha256.  Examples  solidity native implementation with an overzealous type correction issue 5.45  bitcoin-spv/solidity/contracts/BTCUtils.sol:L110-L116  /// @notice          Implements bitcoin's hash256 (double sha2)  /// @dev             abi.encodePacked changes the return to bytes instead of bytes32  /// @param _b        The pre-image  /// @return          The digest  function hash256(bytes memory _b) internal pure returns (bytes32) {  return abi.encodePacked(sha256(abi.encodePacked(sha256(_b)))).toBytes32();  assembly implementation  Note this implementation does not handle errors when staticcall ing the precompiled sha256 contract (private chains).  bitcoin-spv/solidity/contracts/BTCUtils.sol:L118-L129  /// @notice          Implements bitcoin's hash256 (double sha2)  /// @dev             sha2 is precompiled smart contract located at address(2)  /// @param _b        The pre-image  /// @return          The digest  function hash256View(bytes memory _b) internal view returns (bytes32 res) {  assembly {  let ptr := mload(0x40)  pop(staticcall(gas, 2, add(_b, 32), mload(_b), ptr, 32))  pop(staticcall(gas, 2, ptr, 32, ptr, 32))  res := mload(ptr)  Recommendation  We recommend providing only one implementation for calculating the double sha256 as maintaining two interfaces for the same functionality is not desirable. Furthermore, even though the assembly implementation is saving gas, we recommend keeping the language provided implementation.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.45 bitcoin-spv - unnecessary type correction    Addressed",
        "body": "  Resolution   Issue addressed in   summa-tx/bitcoin-spv#126  Description  The type correction encodePacked().toBytes32() is not needed as sha256 already returns bytes32.  Examples  bitcoin-spv/solidity/contracts/BTCUtils.sol:L114-L117  function hash256(bytes memory _b) internal pure returns (bytes32) {  return abi.encodePacked(sha256(abi.encodePacked(sha256(_b)))).toBytes32();  Recommendation  Refactor to return sha256(abi.encodePacked(sha256(_b))); to save gas.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.46 tbtc - Restrict access to fallback function in Deposit.sol    Addressed",
        "body": "  Resolution   Issue addressed in   keep-network/tbtc#526  Description  Deposit.sol has an empty, payable fallback function. It is unused except when seizing signer bonds from BondedECDSAKeep.  Recommendation  So that Ether is not accidentally sent to a Deposit, have the fallback revert if the sender is not the BondedECDSAKeep.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.47 tbtc - Where possible, a specific contract type should be used rather than address    Addressed",
        "body": "  Resolution   This issue has been addressed with   https://github.com/keep-network/tbtc/issues/507 and  keep-network/tbtc#542.  Description  Rather than storing addresses and then casting to the known contract type, it s better to use the best type available so the compiler can check for type safety.  Examples  tbtc/implementation/contracts/deposit/DepositUtils.sol:L25-L37  struct Deposit {  // SET DURING CONSTRUCTION  address TBTCSystem;  address TBTCToken;  address TBTCDepositToken;  address FeeRebateToken;  address VendingMachine;  uint256 lotSizeSatoshis;  uint8 currentState;  uint256 signerFeeDivisor;  uint128 undercollateralizedThresholdPercent;  uint128 severelyUndercollateralizedThresholdPercent;  tbtc/implementation/contracts/proxy/DepositFactory.sol:L16-L28  contract DepositFactory is CloneFactory, TBTCSystemAuthority{  // Holds the address of the deposit contract  // which will be used as a master contract for cloning.  address public masterDepositAddress;  address public tbtcSystem;  address public tbtcToken;  address public tbtcDepositToken;  address public feeRebateToken;  address public vendingMachine;  uint256 public keepThreshold;  uint256 public keepSize;  Remediation  Where possible, use more specific types instead of address. This goes for parameter types as well as state variable types.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.48 tbtc - Variable shadowing in DepositFactory    Addressed",
        "body": "  Resolution   Issue addressed in   keep-network/tbtc#512  Description  DepositFactory inherits from TBTCSystemAuthority. Both contracts declare a state variable with the same name, tbtcSystem.  tbtc/implementation/contracts/proxy/DepositFactory.sol:L21  address public tbtcSystem;  Recommendation  Remove the shadowed variable.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.49 tbtc - Values may contain dirty lower-order bits   Pending",
        "body": "  Resolution   This is being tracked as   https://github.com/keep-network/tbtc/issues/557.  Description  Examples  FundingScript.receiveApproval:  tbtc/implementation/contracts/scripts/FundingScript.sol:L38-L44  // Verify _extraData is a call to unqualifiedDepositToTbtc.  bytes4 functionSignature;  assembly { functionSignature := mload(add(_extraData, 0x20)) }  require(  functionSignature == vendingMachine.unqualifiedDepositToTbtc.selector,  \"Bad _extraData signature. Call must be to unqualifiedDepositToTbtc.\"  );  RedemptionScript.receiveApproval:  tbtc/implementation/contracts/scripts/RedemptionScript.sol:L39-L45  // Verify _extraData is a call to tbtcToBtc.  bytes4 functionSignature;  assembly { functionSignature := mload(add(_extraData, 0x20)) }  require(  functionSignature == vendingMachine.tbtcToBtc.selector,  \"Bad _extraData signature. Call must be to tbtcToBtc.\"  );  Recommendation  Solidity truncates these unneeded bytes in the subsequent comparison operations, so there is no action required. However, this is good to keep in mind if these values are ever used for anything outside of strict comparison.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.50 tbtc - Revert error string may be malformed   Pending",
        "body": "  Resolution   This issue is being tracked as   https://github.com/keep-network/tbtc/issues/509.  Description  FundingScript handles an error from a call to VendingMachine like so.  tbtc/implementation/contracts/scripts/FundingScript.sol:L46-L52  // Call the VendingMachine.  // We could explictly encode the call to vending machine, but this would  // involve manually parsing _extraData and allocating variables.  (bool success, bytes memory returnData) = address(vendingMachine).call(  _extraData  );  require(success, string(returnData));  On a high-level revert, returnData will already include the typical  error selector . As FundingScript propagates this error message, it will add another error selector, which may make it difficult to read the error message.  The same issue is present in RedemptionScript:  tbtc/implementation/contracts/scripts/RedemptionScript.sol:L47-L52  (bool success, bytes memory returnData) = address(vendingMachine).call(_extraData);  // By default, `address.call`  will catch any revert messages.  // Converting the `returnData` to a string will effectively forward any revert messages.  // https://ethereum.stackexchange.com/questions/69133/forward-revert-message-from-low-level-solidity-call  // TODO: there's some noisy couple bytes at the beginning of the converted string, maybe the ABI-coded length?  require(success, string(returnData));  Recommendation  Rather than adding an assembly-level revert to the affected contracts, ensure nested error selectors are handled in external libraries.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.51 tbtc - Where possible, use constant rather than state variables    Addressed",
        "body": "  Resolution   Issue addressed in   keep-network/tbtc#513  Description  TBTCSystem uses a state variable for pausedDuration, but this value is never changed.  tbtc/implementation/contracts/system/TBTCSystem.sol:L34  uint256 pausedDuration = 10 days;  Recommendation  Consider using the constant keyword.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.52 tbtc - Variable shadowing in TBTCDepositToken constructor    Addressed",
        "body": "  Resolution   Issue addressed in   keep-network/tbtc#512  Description  TBTCDepositToken inherits from DepositFactoryAuthority, which has a single state variable, _depositFactory. This variable is shadowed in the TBTCDepositToken constructor.  tbtc/implementation/contracts/system/TBTCDepositToken.sol:L21-L26  constructor(address _depositFactory)  ERC721Metadata(\"tBTC Deopsit Token\", \"TDT\")  DepositFactoryAuthority(_depositFactory)  public {  // solium-disable-previous-line no-empty-blocks  Recommendation  Rename the parameter or state variable.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"
    },
    {
        "title": "5.1 Eliminate assembly code by using ABI decode    ",
        "body": "  Resolution   All assembly code was replaced with proper use of   Description  There are several locations where assembly code is used to access and decode byte arrays (including uses inside loops). Even though assembly code was used for gas optimization, it reduces the readability (and future updatability) of the code.  Examples  code/amp-contracts/contracts/partitions/PartitionsBase.sol:L39-L44  assembly {  flag := mload(add(_data, 32))  if (flag == CHANGE_PARTITION_FLAG) {  assembly {  toPartition := mload(add(_data, 64))  code/amp-contracts/contracts/partitions/PartitionsBase.sol:L43-L44  assembly {  toPartition := mload(add(_data, 64))  Same code as above is also present here: /flexa-collateral-manager/contracts/FlexaCollateralManager.sol#L1403 flexa-collateral-manager/contracts/FlexaCollateralManager.sol#L1407  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L1463-L1470  for (uint256 i = 116; i <= _operatorData.length; i = i + 32) {  bytes32 temp;  assembly {  temp := mload(add(_operatorData, i))  proof[index] = temp;  index++;  Recommendation  As discussed in the mid-audit meeting, it is a good solution to use ABI decode since all uses of assembly simply access 32-byte chunks of data from user input. This should eliminate all assembly code and make the code significantly more clean. In addition, it might allow for more compact encoding in some cases (for instance, by eliminating or reducing the size of the flags).  This suggestion can be also applied to Merkle Root verifications/calculation code, which can reduce the for loops and complexity of these functions.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"
    },
    {
        "title": "5.2 Ignored return value for transferFrom call    ",
        "body": "  Resolution   Fixed by adding a   Description  When burning swap tokens the return value of the transferFrom call is ignored. Depending on the token s implementation this could allow an attacker to mint an arbitrary amount of Amp tokens.  Note that the severity of this issue could have been Critical if Flexa token was any arbitrarily tokens. We quickly verified that Flexa token implementation would revert if the amount exceeds the allowance, however it might not be the case for other token implementations.  code/amp-contracts/contracts/Amp.sol:L619-L620  swapToken.transferFrom(_from, swapTokenGraveyard, amount);  Recommendation  The code should be changed like this:  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"
    },
    {
        "title": "5.3 No integration tests for the two main components    ",
        "body": "  Resolution  amp-contracts added as a submodule to collateral-manager and full integration tests added  It is recommended to write test suites that achieve high code coverage to prevent missing obvious bugs that tests could cover.  Description  The existing tests cover each of the two main components and each set of tests mocks the other component. While this is good for unit testing some issues might be missed without proper system/integration tests that cover all components.  Recommendation  Consider adding system/integration tests for all components. As we ve seen in the recent issues in multi-contract smart contract systems, it s becoming more crucial to have a full test suits for future changes to the code base. Not having inter-component tests, could result in issues in the next development and deployment cycles.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"
    },
    {
        "title": "5.4 Potentially insufficient validation for operator transfers    ",
        "body": "  Resolution  removing operatorTransferByPartition and simplifying the interfaces to only tranferByPartition  This removes the existing tranferByPartition, converting operatorTransferByPartition to it. The reason for this is to make the client interface simpler, where there is one method to transfer by partition, and that method can be called by either a sender wanting to transfer from their own address, or an operator wanting to transfer from a different token holder address. We found that it was redundant to have multiple methods, and the client convenience wasn t worth the confusion.  Description  For operator transfers, the current validation does not require the sender to be an operator (as long as the transferred value does not exceed the allowance):  code/amp-contracts/contracts/Amp.sol:L755-L759  require(  _isOperatorForPartition(_partition, msg.sender, _from) ||  (_value <= _allowedByPartition[_partition][_from][msg.sender]),  EC_53_INSUFFICIENT_ALLOWANCE  );  It is unclear if this is the intention or whether the logical or should be a logical and.  Recommendation  Confirm that the code matches the intention. If so, consider documenting the behavior (for instance, by changing the name of function operatorTransferByPartition.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"
    },
    {
        "title": "5.5 Potentially missing nonce check   ",
        "body": "  Resolution  Nothing was done here, as Dave M writes:  The first two are working as intended, and the third does check that the value is monotonically increasing.  Description  When executing withdrawals in the collateral manager the per-address withdrawal nonce is simply updated without checking that the new nonce is one greater than the previous one (see Examples). It seems like without such a check it might be easy to make mistakes and causing issues with ordering of withdrawals.  Examples  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L663-L664  addressToWithdrawalNonce[_partition][supplier] = withdrawalRootNonce;  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L845-L846  addressToWithdrawalNonce[_partition][supplier] = maxWithdrawalRootNonce;  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L1155-L1156  maxWithdrawalRootNonce = _nonce;  Recommendation  Consider adding more validation and sanity checks for nonces on per-address withdrawals.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"
    },
    {
        "title": "5.6 Unbounded loop when validating Merkle proofs    ",
        "body": "  Resolution   The loop was removed by switching to   Description  It seems like the loop for validating Merkle proofs is unbounded. If possible it would be good to have an upper bound to prevent DoS-like attacks. It seems like the depth of the tree, and thus, the length of the proof could be bounded.  This could also simplify the decoding and make it more robust. For instance, in _decodeWithdrawalOperatorData it is unclear what happens if the data length is not a multiple of 32. It seems like it might result in out-of-bound reads.  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L1460-L1470  uint256 proofNb = (_operatorData.length - 84) / 32;  bytes32[] memory proof = new bytes32[](proofNb);  uint256 index = 0;  for (uint256 i = 116; i <= _operatorData.length; i = i + 32) {  bytes32 temp;  assembly {  temp := mload(add(_operatorData, i))  proof[index] = temp;  index++;  Recommendation  Consider enforcing a bound on the length of Merkle proofs.  Also note that if similar mitigation method as issue 5.1 is used, this method can be replaced by a simpler function using ABI Decode, which does not have any unbounded issues as the sizes of the hashes are fixed (or can be indicated in the passed objects)  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"
    },
    {
        "title": "5.7 Mitigation for possible reentrancy in token transfers    ",
        "body": "  Resolution  Fixed as recommended.  Description  ERC777 adds significant features to the token implementation, however there are some known risks associated with this token, such as possible reentrancy attack vector. Given that the Amp token uses hooks to communicate to Collateral manager, it seems that the environment is trusted and safe. However, a minor modification to the implementation can result in safer implementation of the token transfer.  Examples  In Amp.sol --> _transferByPartition()  code/amp-contracts/contracts/Amp.sol:L1152-L1177  require(  _balanceOfByPartition[_from][_fromPartition] >= _value,  EC_52_INSUFFICIENT_BALANCE  );  bytes32 toPartition = _fromPartition;  if (_data.length >= 64) {  toPartition = _getDestinationPartition(_fromPartition, _data);  _callPreTransferHooks(  _fromPartition,  _operator,  _from,  _to,  _value,  _data,  _operatorData  );  _removeTokenFromPartition(_from, _fromPartition, _value);  _transfer(_from, _to, _value);  _addTokenToPartition(_to, toPartition, _value);  _callPostTransferHooks(  toPartition,  Recommendation  It is suggested to move any condition check that is checking the balance to after the external call. However _callPostTransferHooks needs to be called after the state changes, so the suggested mitigation here is to move the require at line 1152 to after _callPreTransferHooks() function (e.g. line 1171).  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"
    },
    {
        "title": "5.8 Potentially inconsistent input validation    ",
        "body": "  Resolution  transferWithData was removed as a resolution of another filed issue, the rest are documented properly.  The msg.sender cannot be authorized or revoked from being an operator for itself. This should also be clear from the natspec comments now.  Description  There are some functions that might require additional input validation (similar to other functions):  Examples  Amp.transferWithData: require(_isOperator(msg.sender, _from), EC_58_INVALID_OPERATOR); like in  code/amp-contracts/contracts/Amp.sol:L699  require(_isOperator(msg.sender, _from), EC_58_INVALID_OPERATOR);  Amp.authorizeOperatorByPartition: require(_operator != msg.sender); like in  code/amp-contracts/contracts/Amp.sol:L789  require(_operator != msg.sender);  Amp.revokeOperatorByPartition: require(_operator != msg.sender); like in  code/amp-contracts/contracts/Amp.sol:L800  require(_operator != msg.sender);  Recommendation  Consider adding additional input validation.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"
    },
    {
        "title": "5.9 ERC20 compatibility of Amp token using defaultPartition    ",
        "body": "  Resolution  This fix resulted in significant changes to the token allowance work flow. The new implementation of balanceOf represents the total balance of tokens at that address (across any partition), instead of only default partition.  The approve + allowance based operations were using a distinct global allowance mapping, while the rest of the ERC20 compat operations were using the partition state mappings with the default partition. This makes the allowance operations behave the same as the balance based operations.  Description  It is somewhat unclear how the Amp token ensures ERC20 compatibility. While the default partition is used in some places (for instance, in function balanceOf) there are also separate fields for (aggregated) balances/allowances. This seems to introduce some redundancy and raises certain questions about when which fields are relevant.  Examples  _allowed is used in function allowance instead of _allowedByPartition with the default partition  An Approval event should be emitted when approving the default partition  code/amp-contracts/contracts/Amp.sol:L1494  emit ApprovalByPartition(_partition, _tokenHolder, _spender, _amount);  increaseAllowance() vs. increaseAllowanceByPartition()  Recommendation  After the mid-audit discussion, it was clear that the general balanceOf method (with no partition) is not needed and can be replaced with a balanceOf function that returns balance of the default partition, similarly for allowance, the general increaseAllowance function can simply call increaseAllowanceByPartition using default partition (same for decreaseAllowance).  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"
    },
    {
        "title": "5.10 Duplicate code better be moved to shared library    ",
        "body": "  Resolution   aforementioned functions were moved to a shared library   Description  There are some functionalities that the code is duplicated between different smart contracts.  Examples  _getDestinationPartition() is present in both PartitionBase.sol and FlexaCollateralManager.sol  Note that in PartitionBase the usage results in dead code in the contract.  code/amp-contracts/contracts/Amp.sol:L1158-L1160  if (_data.length >= 64) {  toPartition = _getDestinationPartition(_fromPartition, _data);  code/amp-contracts/contracts/partitions/PartitionsBase.sol:L33-L36  toPartition = _fromPartition;  if (_data.length < 64) {  return toPartition;  _splitPartition() is present in FlexaCollateralManager.sol, PartitionBase.sol with slightly different implementations. One has an extra return value for subPartition which is not used in the code under audit  Recommendation  Use a shared library for these functions, possibly ParitionBased.sol can be used in Collateral Manager.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"
    },
    {
        "title": "5.11 Additional validation for canReceive    ",
        "body": "  Resolution   Added proper checks and merged   Description  For FlexaCollateralManager.tokensReceived there is validation to ensure that only the Amp calls the function. In contrast, there is no such validation for canReceive and it is unclear if this is the intention.  Examples  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L492-L493  require(msg.sender == amp, \"Invalid sender\");  Recommendation  Consider adding a conjunct msg.sender == amp in function _canReceive.  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L468-L470  function _canReceive(address _to, bytes32 _destinationPartition) internal view returns (bool) {  return _to == address(this) && partitions[_destinationPartition];  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"
    },
    {
        "title": "5.12 Update to Solidity 0.6.10    ",
        "body": "  Resolution   Updated to   Description  Due to an issue found in 0.6.9, it is recommended to update the compiler version to latest version 0.6.10.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"
    },
    {
        "title": "5.13 Discrepancy between code and comments    ",
        "body": "  Description  There are some discrepancies between (uncommented) code and the documentations comment:  Examples  code/amp-contracts/contracts/Amp.sol:L459-L462  // Indicate token verifies Amp, ERC777 and ERC20 interfaces  ERC1820Implementer._setInterface(AMP_INTERFACE_NAME);  ERC1820Implementer._setInterface(ERC20_INTERFACE_NAME);  // ERC1820Implementer._setInterface(ERC777_INTERFACE_NAME);  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L268-L279  /**  @notice Indicates a supply refund was executed  @param supplier Address whose refund authorization was executed  @param partition Partition from which the tokens were transferred  @param amount Amount of tokens transferred  /  event SupplyRefund(  address indexed supplier,  bytes32 indexed partition,  uint256 amount,  uint256 indexed nonce  );  Recommendation  Consider updating either the code or the comment.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"
    },
    {
        "title": "5.14 Several fields could potentially be private   ",
        "body": "  Resolution  : Comment from Flexa team:  We audited the suggested fields, and determined that we would like them to be public for transparency and/or functionality reasons.  Description  Several fields in Amp could possibly be private:  Examples  swapToken:  code/amp-contracts/contracts/Amp.sol:L261  ISwapToken public swapToken;  swapTokenGraveyard:  code/amp-contracts/contracts/Amp.sol:L268  address public constant swapTokenGraveyard = 0x000000000000000000000000000000000000dEaD;  collateralManagers:  code/amp-contracts/contracts/Amp.sol:L236  address[] public collateralManagers;  partitionStrategies:  code/amp-contracts/contracts/Amp.sol:L248  bytes4[] public partitionStrategies;  The same hold for several fields in FlexaCollateralManager. For instance:  partitions:  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L78  mapping(bytes32 => bool) public partitions;  nonceToSupply:  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L144  mapping(uint256 => Supply) public nonceToSupply;  withdrawalRootToNonce:  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L163  mapping(bytes32 => uint256) public withdrawalRootToNonce;  Recommendation  Double-check that you really want to expose those fields.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"
    },
    {
        "title": "5.15 Several fields could be declared immutable   ",
        "body": "  Resolution  Comment from Flexa team:  We tried to add this, but found that it made validating the contract on Etherscan impossible. We have added comments to a reader of the contract indicating the fields are immutable after deployment, though.  Description  Several fields could be declared immutable to make clear that they never change after construction:  Examples  Amp._name:  code/amp-contracts/contracts/Amp.sol:L129  string internal _name;  Amp._symbol:  code/amp-contracts/contracts/Amp.sol:L134  string internal _symbol;  Amp.swapToken:  code/amp-contracts/contracts/Amp.sol:L261  ISwapToken public swapToken;  FlexaCollateralManager.amp:  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L73  address public amp;  Recommendation  Use the immutable annotation in Solidity (see Immutable).  ",
        "labels": [
            "Consensys",
            "Minor",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"
    },
    {
        "title": "5.1 Attacker can abuse swapLiquidity function to drain users  funds    ",
        "body": "  Resolution   Solved by removing   Description  The swapLiquidity function allows liquidity providers to atomically swap their collateral. The function takes a receiverAddressargument that normally points to an ISwapAdapter implementation trusted by the user.  code/contracts/lendingpool/LendingPoolCollateralManager.sol:L490-L517  vars.fromReserveAToken.burn(  msg.sender,  receiverAddress,  amountToSwap,  fromReserve.liquidityIndex  );  // Notifies the receiver to proceed, sending as param the underlying already transferred  ISwapAdapter(receiverAddress).executeOperation(  fromAsset,  toAsset,  amountToSwap,  address(this),  params  );  vars.amountToReceive = IERC20(toAsset).balanceOf(receiverAddress);  if (vars.amountToReceive != 0) {  IERC20(toAsset).transferFrom(  receiverAddress,  address(vars.toReserveAToken),  vars.amountToReceive  );  if (vars.toReserveAToken.balanceOf(msg.sender) == 0) {  _usersConfig[msg.sender].setUsingAsCollateral(toReserve.id, true);  vars.toReserveAToken.mint(msg.sender, vars.amountToReceive, toReserve.liquidityIndex);  However, since an attacker can pass any address as the receiverAddress, they can arbitrarily transfer funds from other contracts that have given allowances to the LendingPool contract (for example, another ISwapAdapter).  The amountToSwap is defined by the caller and can be very small. The attacker gets the difference between IERC20(toAsset).balanceOf(receiverAddress) value of toAsset and the amountToSwap of fromToken.  Remediation  Ensure that no funds can be stolen from contracts that have granted allowances to the LendingPool contract.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"
    },
    {
        "title": "5.2 Griefing attack by taking flash loan on behalf of user ",
        "body": "  Description  When taking a flash loan from the protocol, the arbitrary receiverAddress  address can be passed as the argument:  code/contracts/lendingpool/LendingPool.sol:L547-L554  function flashLoan(  address receiverAddress,  address asset,  uint256 amount,  uint256 mode,  bytes calldata params,  uint16 referralCode  ) external override {  That may allow anyone to execute a flash loan on behalf of other users. In order to make that attack, the receiverAddress should give the allowance to the LendingPool contract to make a transfer for the amount of currentAmountPlusPremium.  Example  If someone is giving the allowance to the LendingPool contract to make a deposit, the attacker can execute a flash loan on behalf of that user, forcing the user to pay fees from the flash loan. That will also prevent the victim from making a successful deposit transaction.  Remediation  Make sure that only the user can take a flash loan.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"
    },
    {
        "title": "5.3 Interest rates are updated incorrectly ",
        "body": "  Resolution  This issue was independently discovered by the Aave developers and had already been fixed by the end of the audit.  The function updateInterestRates() updates the borrow rates of a reserve. Since the rates depend on the available liquidity they must be recalculated each time liquidity changes. The function takes the amount of liquidity added or removed as the input and is called ahead of minting or burning ATokens. However, in LendingPoolCollateralManager an interest rate update is performed after aTokens have been burned, resulting in an incorrect interest rate.  code/contracts/lendingpool/LendingPoolCollateralManager.sol:L377-L382  vars.collateralAtoken.burn(  user,  receiver,  vars.maxCollateralToLiquidate,  collateralReserve.liquidityIndex  );  code/contracts/lendingpool/LendingPoolCollateralManager.sol:L427-L433  //updating collateral reserve  collateralReserve.updateInterestRates(  collateral,  address(vars.collateralAtoken),  0,  vars.maxCollateralToLiquidate  );  Recommendation  Update interest rates before calling collateralAtoken.burn().  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"
    },
    {
        "title": "5.4 Unhandled return values of transfer and transferFrom ",
        "body": "  Resolution  ERC20 implementations are not always consistent. Some implementations of transfer and transferFrom could return  false  on failure instead of reverting. It is safer to wrap such calls into require() statements to these failures. Unsafe transferFrom calls were found in the following locations:  code/contracts/lendingpool/LendingPool.sol:L578  IERC20(asset).transferFrom(receiverAddress, vars.aTokenAddress, vars.amountPlusPremium);  code/contracts/lendingpool/LendingPoolCollateralManager.sol:L407  IERC20(principal).transferFrom(receiver, vars.principalAToken, vars.actualAmountToLiquidate);  code/contracts/lendingpool/LendingPoolCollateralManager.sol:L507-L511  IERC20(toAsset).transferFrom(  receiverAddress,  address(vars.toReserveAToken),  vars.amountToReceive  );  Recommendation  Check the return value and revert on 0/false or use OpenZeppelin s SafeERC20 wrapper functions.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"
    },
    {
        "title": "5.5 Re-entrancy attacks with ERC-777 ",
        "body": "  Resolution   The issue was partially mitigated in   Description  Some tokens may allow users to perform re-entrancy while calling the transferFrom function. For example, it would be possible for an attacker to  borrow  a large amount of ERC-777 tokens from the lending pool by re-entering the deposit function from within transferFrom.  code/contracts/lendingpool/LendingPool.sol:L91-L118  function deposit(  address asset,  uint256 amount,  address onBehalfOf,  uint16 referralCode  ) external override {  _whenNotPaused();  ReserveLogic.ReserveData storage reserve = _reserves[asset];  ValidationLogic.validateDeposit(reserve, amount);  address aToken = reserve.aTokenAddress;  reserve.updateState();  reserve.updateInterestRates(asset, aToken, amount, 0);  bool isFirstDeposit = IAToken(aToken).balanceOf(onBehalfOf) == 0;  if (isFirstDeposit) {  _usersConfig[onBehalfOf].setUsingAsCollateral(reserve.id, true);  IAToken(aToken).mint(onBehalfOf, amount, reserve.liquidityIndex);  //transfer to the aToken contract  IERC20(asset).safeTransferFrom(msg.sender, aToken, amount);  emit Deposit(asset, msg.sender, onBehalfOf, amount, referralCode);  Because the safeTransferFrom call is happening at the end of the deposit function, the deposit will be fully processed before the tokens are actually transferred.  So at the beginning of the transfer, the attacker can re-enter the call to withdraw their deposit. The withdrawal will succeed even though the attacker s tokens have not yet been transferred to the lending pool. Essentially, the attacker is granted a flash-loan but without paying fees.  Additionally, after these calls, interest rates will be skewed because interest rate update relies on the actual current balance.  Remediation  Do not whitelist ERC-777 or other re-entrable tokens to prevent this kind of attack.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"
    },
    {
        "title": "5.6 Potential manipulation of stable interest rates using flash loans ",
        "body": "  Resolution  This type of manipulation is difficult to prevent completely especially when flash loans are available. In practice however, attacks are mitigated by the following factors:  Liquidity providers attempting to increase users  stable rates would have to pay a high flash loan premium. Users could also immediately swap to variable interest meaning that the attack could result in a net loss for the LP. In practice, it is likely that this makes the attack economically unfeasible.  Under normal conditions, users would only gain a relatively small advantage by lowering their stable rate due to the design of the stable rate curve. If a user attempted to manipulate their stable rate during a liquidity crisis, Aave could immediately rebalance them and bring the rate back to normal.  Flash loans allow users to borrow large amounts of liquidity from the protocol. It is possible to adjust the stable rate up or down by momentarily removing or adding large amounts of liquidity to reserves.  LPs increasing the interest rate of borrowers  The function rebalanceStableBorrowRate() increases the stable interest rate of a user if the current liquidity rate is higher than the user s stable rate. A liquidity provider could trigger an artificial  liquidity crisis  in a reserve and increase the stable interest rates of borrowers by atomically performing the following steps:  Take a flash loan to take a large number of tokens from a reserve  Re-balance the stable rate of the emptied reserves  borrowers  Repay the flash loan (plus premium)  Withdraw the collateral and repay the flash loan  Individual borrowers would then have to switch to the variable rate to return to a lower interest rate.  User borrowing at an artificially lowered interest rate  Users wanting to borrow funds could attempt to get a lower interest rate by temporarily adding liquidity to a reserve (which could e.g. be flash borrowed from a different protocol). While there s a check that prevents users from borrowing an asset while also adding a higher amount of the same asset as collateral, this can be bypassed rather easily by depositing the collateral from a different address (via smart contracts). Aave would then have to rebalance the user to restore an appropriate interest rate.  In practice, users would gain only a relatively small advantage here due to the design of the stable rate curve.  Recommendation  This type of manipulation is difficult to prevent especially when flash loans are available. The safest option to prevent the first variant would be to restrict access to rebalanceStableBorrowRate() to admins. In any case, Aave should monitor the protocol at all times to make sure that interest rates are being rebalanced to sane values.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"
    },
    {
        "title": "5.7 Code quality could be improved ",
        "body": "  Some minor code quality improvements are recommended to improve readability.  Explicitly set the visibility for of variables:  code/contracts/tokenization/StableDebtToken.sol:L23-L24  mapping(address => uint40) _timestamps;  uint40 _totalSupplyTimestamp;  code/contracts/configuration/LendingPoolAddressesProviderRegistry.sol:L17-L18  mapping(address => uint256) addressesProviders;  address[] addressesProvidersList;  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"
    },
    {
        "title": "5.8 Attacker can front-run delegator when changing allowance ",
        "body": "  Users can grant allowances to borrow debt assets to other users using the delegateAllowance function. Similar to the classical ERC20 approve attack, it is possible for a malicious user to front-run the delegator when they attempt to change the allowance and borrow the sum of the old and new values.  Example scenario:  Bob creates an allowance of 100 DAI for Malice: delegateBorrowAllowance(DAI, Malice, 100)  Later, Bob attempts to lower the allowance to 90: delegateBorrowAllowance(DAI, Malice, 90)  Malice borrows a total of 190 DAI by first frontrunning Bob s second transaction borrowing 100 DAI and then borrowing another 90 DAI after Bob s transaction was mined.  Recommentation  A commonly used way of preventing this attack is using increaseAllowance() and decreaseAllowance() functions specifically for increasing and decreasing allowances.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"
    },
    {
        "title": "5.9 Description of flash loan function is inconsistent with code ",
        "body": "  The function flashLoan in LendingPool.sol takes an argument mode that specifies the interest rate mode. If the mode is ReserveLogic.InterestRateMode.NONE the function call is treated as a flash loan, if not a normal borrow is executed.  However, inline comments in the function describe the behaviour as  If the transfer didn t succeed, the receiver either didn t return the funds, or didn t approve the transfer . It is unclear how this relates to the actual code or why it is possible to specify a mode in the first place.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"
    },
    {
        "title": "3.1 Winning pods can be frontrun with large deposits ",
        "body": "  Description  Pod.depositTo() grants users shares of the pod pool in exchange for tokenAmount of token.  code/pods-v3-contracts/contracts/Pod.sol:L266-L288  function depositTo(address to, uint256 tokenAmount)  external  override  returns (uint256)  require(tokenAmount > 0, \"Pod:invalid-amount\");  // Allocate Shares from Deposit To Amount  uint256 shares = _deposit(to, tokenAmount);  // Transfer Token Transfer Message Sender  IERC20Upgradeable(token).transferFrom(  msg.sender,  address(this),  tokenAmount  );  // Emit Deposited  emit Deposited(to, tokenAmount, shares);  // Return Shares Minted  return shares;  The winner of a prize pool is typically determined by an off-chain random number generator, which requires a request to first be made on-chain. The result of this RNG request can be seen in the mempool and frontrun. In this case, an attacker could identify a winning Pod contract and make a large deposit, diluting existing user shares and claiming the entire prize.  Recommendation  The modifier pauseDepositsDuringAwarding is included in the Pod contract but is unused.  code/pods-v3-contracts/contracts/Pod.sol:L142-L148  modifier pauseDepositsDuringAwarding() {  require(  !IPrizeStrategyMinimal(_prizePool.prizeStrategy()).isRngRequested(),  \"Cannot deposit while prize is being awarded\"  );  _;  Add this modifier to the depositTo() function along with corresponding test cases.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"
    },
    {
        "title": "3.2 Token transfers may return false ",
        "body": "  Description  There are a lot of token transfers in the code, and most of them are just calling transfer or transferFrom without checking the return value. Ideally, due to the ERC-20 token standard, these functions should always return True or False (or revert). If a token returns False, the code will process the transfer as if it succeeds.  Recommendation  Use the safeTransfer and the safeTransferFrom versions of transfers from OZ.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"
    },
    {
        "title": "3.3 TokenDrop: Unprotected initialize() function ",
        "body": "  Description  The TokenDrop.initialize() function is unprotected and can be called multiple times.  code/pods-v3-contracts/contracts/TokenDrop.sol:L81-L87  function initialize(address _measure, address _asset) external {  measure = IERC20Upgradeable(_measure);  asset = IERC20Upgradeable(_asset);  // Set Factory Deployer  factory = msg.sender;  Recommendation  Add the initializer modifier to the initialize() function and include an explicit test that every initialization function in the system can be called once and only once.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"
    },
    {
        "title": "3.4 Pod: Re-entrancy during deposit or withdrawal can lead to stealing funds ",
        "body": "  Description  During the deposit, the token transfer is made after the Pod shares are minted:  code/pods-v3-contracts/contracts/Pod.sol:L274-L281  uint256 shares = _deposit(to, tokenAmount);  // Transfer Token Transfer Message Sender  IERC20Upgradeable(token).transferFrom(  msg.sender,  address(this),  tokenAmount  );  That means that if the token allows re-entrancy, the attacker can deposit one more time inside the token transfer. If that happens, the second call will mint more tokens than it is supposed to, because the first token transfer will still not be finished. By doing so with big amounts, it s possible to drain the pod.  Recommendation  Add re-entrancy guard to the external functions.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"
    },
    {
        "title": "3.5 TokenDrop: Re-entrancy in the claim function can cause to draining funds ",
        "body": "  Description  code/pods-v3-contracts/contracts/TokenDrop.sol:L139-L153  function claim(address user) external returns (uint256) {  drop();  _captureNewTokensForUser(user);  uint256 balance = userStates[user].balance;  userStates[user].balance = 0;  totalUnclaimed = uint256(totalUnclaimed).sub(balance).toUint112();  // Transfer asset/reward token to user  asset.transfer(user, balance);  // Emit Claimed  emit Claimed(user, balance);  return balance;  Because the totalUnclaimed is already changed, but the current balance is not, the drop function will consider the funds from the unfinished transfer as the new tokens. These tokens will be virtually redistributed to everyone.  After that, the transfer will still happen, and further calls of the drop() function will fail because the following line will revert:  uint256 newTokens = assetTotalSupply.sub(totalUnclaimed);  That also means that any transfers of the Pod token will fail because they all are calling the drop function. The TokenDrop will  unfreeze  only if someone transfers enough tokens to the TokenDrop contract.  The severity of this issue is hard to evaluate because, at the moment, there s not a lot of tokens that allow this kind of re-entrancy.  Recommendation  Simply adding re-entrancy guard to the drop and the claim function won t help because the drop function is called from the claim. For that, the transfer can be moved to a separate function, and this function can have the re-entrancy guard as well as the drop function.  Also, it s better to make sure that _beforeTokenTransfer will not revert to prevent the token from being frozen.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"
    },
    {
        "title": "3.6 Pod: Having multiple token drops is inconsistent ",
        "body": "  Description  code/pods-v3-contracts/contracts/Pod.sol:L455-L477  function setTokenDrop(address _token, address _tokenDrop)  external  returns (bool)  require(  msg.sender == factory || msg.sender == owner(),  \"Pod:unauthorized-set-token-drop\"  );  // Check if target<>tokenDrop mapping exists  require(  drops[_token] == TokenDrop(0),  \"Pod:target-tokendrop-mapping-exists\"  );  // Set TokenDrop Referance  drop = TokenDrop(_tokenDrop);  // Set target<>tokenDrop mapping  drops[_token] = drop;  return true;  Recommendation  The mapping seems to be unused, and only one TokenDrop will normally be in the system. If that code is not used, it should be deleted.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"
    },
    {
        "title": "3.7 Pod: Fees are not limited by a user during the withdrawal ",
        "body": "  Description  When withdrawing from the Pod, the shares are burned, and the deposit is removed from the Pod. If there are not enough deposit tokens in the contract, the remaining tokens are withdrawn from the pool contract:  code/pods-v3-contracts/contracts/Pod.sol:L523-L532  if (amount > currentBalance) {  // Calculate Withdrawl Amount  uint256 _withdraw = amount.sub(currentBalance);  // Withdraw from Prize Pool  uint256 exitFee = _withdrawFromPool(_withdraw);  // Add Exit Fee to Withdrawl Amount  amount = amount.sub(exitFee);  These tokens are withdrawn with a fee from the pool, which is not controlled or limited by the user.  Recommendation  Allow users to pass a maxFee parameter to control fees.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"
    },
    {
        "title": "3.8 ProxyFactory.deployMinimal() does not check for contract creation failure ",
        "body": "  Description  The function ProxyFactory.deployMinimal() is used by both the PodFactory and the TokenDropFactory to deploy minimal proxy contracts. This function uses inline assembly to inline a target address into the minimal proxy and deploys the resulting bytecode. It then emits an event containing the resulting address and optionally makes a low-level call to the resulting address with user-provided data.  The result of a create() operation in assembly will be the zero address in the event that a revert or an exceptional halting state is encountered during contract creation. If execution of the contract initialization code succeeds but returns no runtime bytecode, it is also possible for the create() operation to return a nonzero address that contains no code.  code/pods-v3-contracts/contracts/external/ProxyFactory.sol:L9-L35  function deployMinimal(address _logic, bytes memory _data)  public  returns (address proxy)  // Adapted from https://github.com/optionality/clone-factory/blob/32782f82dfc5a00d103a7e61a17a5dedbd1e8e9d/contracts/CloneFactory.sol  bytes20 targetBytes = bytes20(_logic);  assembly {  let clone := mload(0x40)  mstore(  clone,  0x3d602d80600a3d3981f3363d3d373d3d3d363d73000000000000000000000000  mstore(add(clone, 0x14), targetBytes)  mstore(  add(clone, 0x28),  0x5af43d82803e903d91602b57fd5bf30000000000000000000000000000000000  proxy := create(0, clone, 0x37)  emit ProxyCreated(address(proxy));  if (_data.length > 0) {  (bool success, ) = proxy.call(_data);  require(success, \"ProxyFactory/constructor-call-failed\");  Recommendation  At a minimum, add a check that the resulting proxy address is nonzero before emitting the ProxyCreated event and performing the low-level call. Consider also checking the extcodesize of the proxy address is greater than zero.  Also note that the bytecode in the deployed  Clone  contract was not reviewed due to time constraints.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"
    },
    {
        "title": "3.9 Pod.setManager() checks validity of wrong address ",
        "body": "  Description  The current check will always pass once the contract is initialized with a nonzero manager. But, the contract can currently be initialized with a manager of IPodManager(address(0)). In this case, the check would prevent the manager from ever being updated.  code/pods-v3-contracts/contracts/Pod.sol:L233-L240  function setManager(IPodManager newManager)  public  virtual  onlyOwner  returns (bool)  // Require Valid Address  require(address(manager) != address(0), \"Pod:invalid-manager-address\");  Recommendation  Change the check to:  require(address(newManager) != address(0), \"Pod:invalid-manager-address\");  More generally, attempt to define validity criteria for all input values that are as strict as possible. Consider preventing zero inputs or inputs that might conflict with other addresses in the smart contract system altogether, including in contract initialization functions.  4 Recommendations  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"
    },
    {
        "title": "4.1 Rename Withdrawl event to Withdrawal",
        "body": "  Description  The Pod contract contains an event Withdrawl(address, uint256, uint256):  code/pods-v3-contracts/contracts/Pod.sol:L76-L79  /**  @dev Emitted when user withdraws  /  event Withdrawl(address user, uint256 amount, uint256 shares);  This appears to be a misspelling of the word Withdrawal. This is of course not a problem given it s consistent use, but could cause confusion for users or issues in future contract updates.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"
    },
    {
        "title": "5.1 Anyone is able to mint NFTs by calling mintNFTsForLM    ",
        "body": "  Resolution  Fixed. Not an issue, as the contract is meant to be used as a mock.  Description  The contract LiquidityMiningNFT has the method mintNFTsForLM.  code/contracts/LiquidityMiningNFT.sol:L12-L29  function mintNFTsForLM(address _liquidiyMiningAddr) external {  uint256[] memory _ids = new uint256[](NFT_TYPES_COUNT);  uint256[] memory _amounts = new uint256[](NFT_TYPES_COUNT);  _ids[0] = 1;  _amounts[0] = 5;  _ids[1] = 2;  _amounts[1] = 1 * LEADERBOARD_SIZE;  _ids[2] = 3;  _amounts[2] = 3 * LEADERBOARD_SIZE;  _ids[3] = 4;  _amounts[3] = 6 * LEADERBOARD_SIZE;  _mintBatch(_liquidiyMiningAddr, _ids, _amounts, \"\");  However, this contract does not have any kind of special permissions to limit who is able to mint tokens.  An attacker could call LiquidityMiningNFT.mintNFTsForLM(0xhackerAddress) to mint tokens for their address and sell them on the marketplace. They are also allowed to mint as many tokens as they want by calling the method multiple times.  Recommendation  Add some permissions to limit only some actors to mint tokens.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "5.2 Liquidity providers can create deficit of DAI tokens    ",
        "body": "  Resolution  Fixed by keeping all the DAI inside the PolicyBook.  Description  The current staking system is built in a way that a liquidity provider can stake DAIx tokens to the staking contract. By doing so, DAI tokens are getting withdrawn from the PolicyBook and there may be not enough funds to fulfill claims.  Recommendation  This issue requires major changes in the logic of the system.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "5.3 Profit and loss distribution mechanism is not working    ",
        "body": "  Resolution   Fixed by updating the   Description  That error may also lead to the deficit of funds during withdrawals or claims.  Recommendation  Properly keep track of the totalLiquidity.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "5.4 A liquidity provider can withdraw all his funds anytime    ",
        "body": "  Resolution  The funds are now locked when the withdrawal is requested, so funds cannot be transferred after the request, and this bug cannot be exploited anymore.  Description  Since some users provide liquidity to sell the insurance policies, it is important that these providers cannot withdraw their funds when the security breach happens and the policyholders are submitting claims. The liquidity providers can only request their funds first and withdraw them later (in a week).  code/contracts/PolicyBook.sol:L358-L382  function requestWithdrawal(uint256 _tokensToWithdraw) external override {  WithdrawalStatus _status = getWithdrawalStatus(msg.sender);  require(_status == WithdrawalStatus.NONE || _status == WithdrawalStatus.EXPIRED,  \"PB: Can't request withdrawal\");  uint256 _daiTokensToWithdraw = _tokensToWithdraw.mul(getDAIToDAIxRatio()).div(PERCENTAGE_100);  uint256 _availableDaiBalance = balanceOf(msg.sender).mul(getDAIToDAIxRatio()).div(PERCENTAGE_100);  if (block.timestamp < liquidityMining.getEndLMTime().add(neededTimeAfterLM)) {  _availableDaiBalance = _availableDaiBalance.sub(liquidityFromLM[msg.sender]);  require(totalLiquidity >= totalCoverTokens.add(_daiTokensToWithdraw),  \"PB: Not enough liquidity\");  require(_availableDaiBalance >= _daiTokensToWithdraw, \"PB: Wrong announced amount\");  WithdrawalInfo memory _newWithdrawalInfo;  _newWithdrawalInfo.amount = _tokensToWithdraw;  _newWithdrawalInfo.readyToWithdrawDate = block.timestamp.add(withdrawalPeriod);  withdrawalsInfo[msg.sender] = _newWithdrawalInfo;  emit RequestWithdraw(msg.sender, _tokensToWithdraw, _newWithdrawalInfo.readyToWithdrawDate);  code/contracts/PolicyBook.sol:L384-L396  function withdrawLiquidity() external override {  require(getWithdrawalStatus(msg.sender) == WithdrawalStatus.READY,  \"PB: Withdrawal is not ready\");  uint256 _tokensToWithdraw = withdrawalsInfo[msg.sender].amount;  uint256 _daiTokensToWithdraw = _tokensToWithdraw.mul(getDAIToDAIxRatio()).div(PERCENTAGE_100);  if (withdrawalQueue.length != 0 || totalLiquidity.sub(_daiTokensToWithdraw) < totalCoverTokens) {  withdrawalQueue.push(msg.sender);  } else {  _withdrawLiquidity(msg.sender, _tokensToWithdraw);  There is a restriction in requestWithdrawal that requires the liquidity provider to have enough funds at the moment of request:  code/contracts/PolicyBook.sol:L371-L374  require(totalLiquidity >= totalCoverTokens.add(_daiTokensToWithdraw),  \"PB: Not enough liquidity\");  require(_availableDaiBalance >= _daiTokensToWithdraw, \"PB: Wrong announced amount\");  But after the request is created, these funds can then be transferred to another address. When the request is created, the provider should wait for 7 days, and then there will be 2 days to withdraw the requested amount:  code/contracts/PolicyBook.sol:L113-L114  withdrawalPeriod = 1 weeks;  withdrawalExpirePeriod = 2 days;  The attacker would have 4 addresses that will send the pool tokens to each other and request withdrawal of the full amount one by one every 2 days. So at least one of the addresses can withdraw all of the funds at any point in time. If the liquidity provider needs to withdraw funds immediately, he should transfer all funds to that address and execute the withdrawal.  Recommendation  One of the solutions would be to block the DAIx tokens from being transferred after the withdrawal request.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "5.5 Re-entrancy issue for ERC1155    ",
        "body": "  Resolution   Addressed by moving   Description  ERC1155 tokens have callback functions on some of the transfers, like safeTransferFrom, safeBatchTransferFrom. During these transfers, the IERC1155ReceiverUpgradeable(to).onERC1155Received function is called in the to address.  For example, safeTransferFrom is used in the LiquidityMining contract:  code/contracts/LiquidityMining.sol:L204-L224  function distributeAllNFT() external {  require(block.timestamp > getEndLMTime(),  \"2 weeks after liquidity mining time has not expired\");  require(!isNFTDistributed, \"NFT is already distributed\");  for (uint256 i = 0; i < leaderboard.length; i++) {  address[] memory _groupLeaders = groupsLeaders[leaderboard[i]];  for (uint256 j = 0; j < _groupLeaders.length; j++) {  _sendNFT(j, _groupLeaders[j]);  for (uint256 i = 0; i < topUsers.length; i++) {  address _currentAddress = topUsers[i];  LMNFT.safeTransferFrom(address(this), _currentAddress, 1, 1, \"\");  emit NFTSent(_currentAddress, 1);  isNFTDistributed = true;  During that transfer, the distributeAllNFT  function can be called again and again. So multiple transfers will be done for each user.  In addition to that, any receiver of the tokens can revert the transfer. If that happens, nobody will be able to receive their tokens.  Recommendation  Add a reentrancy guard.  Avoid transferring tokens for different receivers in a single transaction.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "5.6 The buyPolicyFor/addLiquidityFor should transfer funds from msg.sender    ",
        "body": "  Resolution   Addressed by removing the   Description  When calling the buyPolicyFor/addLiquidityFor functions, are called with the parameter _policyHolderAddr/_liquidityHolderAddr who is going to be the beneficiary in buying policy/adding liquidity:  code/contracts/PolicyBook.sol:L183-L189  function buyPolicyFor(  address _policyHolderAddr,  uint256 _epochsNumber,  uint256 _coverTokens  ) external override {  _buyPolicyFor(_policyHolderAddr, _epochsNumber, _coverTokens);  code/contracts/PolicyBook.sol:L264-L266  function addLiquidityFor(address _liquidityHolderAddr, uint256 _liquidityAmount) external override {  _addLiquidityFor(_liquidityHolderAddr, _liquidityAmount, false);  During the execution, the funds for the policy/liquidity are transferred from the _policyHolderAddr/_liquidityHolderAddr, while it s usually expected that they should be transferred from msg.sender. Because of that, anyone can call a function on behalf of a user that gave the allowance to the PolicyBook.  For example, a user(victim) wants to add some DAI to the liquidity pool and gives allowance to the PolicyBook. After that, the user should call addLiquidity, but the attacker can front-run this transaction and buy a policy on behalf of the victim instead.  Also, there is a curious edge case that makes this issue Critical: _policyHolderAddr/_liquidityHolderAddr parameters can be equal to the address of the PolicyBook contract. That may lead to multiple different dangerous attack vectors.  Recommendation  Make sure that nobody can transfer funds on behalf of the users if it s not intended.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "5.7 LiquidityMining can t accept single ERC1155 tokens    ",
        "body": "  Resolution   Fixed by properly implementing the   Description  The contract LiquidityMining is also defined as an ERC1155Receiver  code/contracts/LiquidityMining.sol:L19  contract LiquidityMining is ILiquidityMining, ERC1155Receiver, Ownable {  The finalized EIP-1155 standard states that a contract which acts as an EIP-1155 Receiver must implement all the functions in the ERC1155TokenReceiver interface to be able to accept transfers.  These are indeed implemented here:  code/contracts/LiquidityMining.sol:L502  function onERC1155Received(  code/contracts/LiquidityMining.sol:L517  function onERC1155BatchReceived(  The standard states that they will be called and they MUST return a specific byte4 value, otherwise the transfer will fail.  However one of the methods returns an incorrect value. This seems to an error generated by a copy/paste action.  code/contracts/LiquidityMining.sol:L502-L515  function onERC1155Received(  address operator,  address from,  uint256 id,  uint256 value,  bytes memory data  external  pure  override  returns(bytes4)  return bytes4(keccak256(\"onERC1155BatchReceived(address,address,uint256[],uint256[],bytes)\"));  The value returned is equal to  bytes4(keccak256(\"onERC1155BatchReceived(address,address,uint256[],uint256[],bytes)\"));  But it should be  bytes4(keccak256(\"onERC1155Received(address,address,uint256,uint256,bytes)\")).  On top of this, the contract MUST implement the ERC-165 standard to correctly respond to supportsInterface.  Recommendation  Change the return value of onERC1155Received to be equal to 0xf23a6e61 which represents bytes4(keccak256(\"onERC1155Received(address,address,uint256,uint256,bytes)\")).  Also, make sure to implement supportsInterface to signify support of ERC1155TokenReceiver to accept transfers.  Add tests to check the functionality is correct and make sure these kinds of bugs do not exist in the future.  Make sure to read the EIP-1155 and EIP-165 standards in detail and implement them correctly.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "5.8 DAI is assumed to have the same price as DAIx in the staking contract    ",
        "body": "  Resolution  Fixed by not transferring DAI anymore.  Description  When a liquidity provider stakes tokens to the BMIDAIStaking contract, the equal amount of DAI and DAIx are transferred from the pool contract.  code/contracts/BMIDAIStaking.sol:L113-L124  function _stakeDAIx(address _user, uint256 _amount, address _policyBookAddr) internal {  require (_amount > 0, \"BMIDAIStaking: Can't stake zero tokens\");  PolicyBook _policyBook = PolicyBook(_policyBookAddr);  // transfer DAI from PolicyBook to yield generator  daiToken.transferFrom(_policyBookAddr, address(defiYieldGenerator), _amount);  // transfer bmiDAIx from user to staking  _policyBook.transferFrom(_user, address(this), _amount);  _mintNFT(_user, _amount, _policyBook);  Recommendation  Only the corresponding amount of DAI should be transferred to the pool.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "5.9 _updateWithdrawalQueue can run out of gas    ",
        "body": "  Resolution   The   Description  When there s not enough collateral to withdraw liquidity from a policy book, the withdrawal request is added to a queue. The queue is supposed to be processed and cleared once there are enough funds for that. The only way to do so is the _updateWithdrawalQueue function that is caller when new liquidity is added:  code/contracts/PolicyBook.sol:L315-L338  function _updateWithdrawalQueue() internal {  uint256 _availableLiquidity = totalLiquidity.sub(totalCoverTokens);  uint256 _countToRemoveFromQueue;  for (uint256 i = 0; i < withdrawalQueue.length; i++) {  uint256 _tokensToWithdraw = withdrawalsInfo[withdrawalQueue[i]].amount;  uint256 _amountInDai = _tokensToWithdraw.mul(getDAIToDAIxRatio()).div(PERCENTAGE_100);  if (balanceOf(withdrawalQueue[i]) < _tokensToWithdraw) {  _countToRemoveFromQueue++;  continue;  if (_availableLiquidity >= _amountInDai) {  _withdrawLiquidity(withdrawalQueue[i], _tokensToWithdraw);  _availableLiquidity = _availableLiquidity.sub(_amountInDai);  _countToRemoveFromQueue++;  } else {  break;  _removeFromQueue(_countToRemoveFromQueue);  The problem is that this function can only process all queue until the pool run out of available funds or the whole queue is going to be processed. If the queue is big enough, this process can be stuck.  Recommendation  Pass the parameter to the _updateWithdrawalQueue that defines how many requests to process in the queue per one call.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "5.10 The PolicyBook should make DAI transfers inside the contract    ",
        "body": "  Resolution   The   Description  The PolicyBook contract gives full allowance over DAI tokens to the other contracts:  code/contracts/PolicyBook.sol:L120-L125  function approveAllDaiTokensForStakingAndVotingAndTransferOwnership() internal {  daiToken.approve(address(bmiDaiStaking), MAX_INT);  daiToken.approve(address(claimVoting), MAX_INT);  transferOwnership(address(bmiDaiStaking));  That behavior is dangerous because it s hard to keep track of and control the contract s DAI balance. And it s also hard to track in the code where the balance of the PolicyBook can be changed from.  Recommendation  It s better to perform all the transfers inside the PolicyBook contract. So if the bmiDaiStaking and the claimVoting contracts need DAI tokens from the PolicyBook, they should call some function of the PolicyBook to perform transfers.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "5.11 Premium is payed instantly to the liquidity providers    ",
        "body": "  Resolution  The premium is now distributed on a daily basis.  Description  When the policy is bought, the premium is transferred to the PolicyBook instantly. Currently, these funds are not going to the liquidity providers as a reward due to the issue 5.3. But when the issue is fixed, it seems like the premium is paid and distributed as a reward instantly when the policy is purchased.  The problem is that if someone buys the policy for a long period of time, every liquidity provider instantly gets the premium from the full period. If there s enough liquidity, any provider can withdraw the funds after that without taking a risk for this period.  Recommendation  Distribute the premium over time. For example, increase the reward after each epoch.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "5.12 The totalCoverTokens is only updated when the policy is bought    ",
        "body": "  Resolution   The   Description  The totalCoverTokens value represents the amount of collateral that needs to be locked in the policy book. It should be changed either by buying a new policy or when an old policy expires. The problem is that when the old policy expires, this value is not updated; it is only updated when someone buys a policy by calling the _updateEpochsInfo  function:  code/contracts/PolicyBook.sol:L240-L251  function _updateEpochsInfo() internal {  uint256 _totalEpochTime = block.timestamp.sub(epochStartTime);  uint256 _countOfPassedEpoch = _totalEpochTime.div(epochDuration);  uint256 _lastEpochUpdate = currentEpochNumber;  currentEpochNumber = _countOfPassedEpoch.add(1);  for (uint256 i = _lastEpochUpdate; i < currentEpochNumber; i++) {  totalCoverTokens = totalCoverTokens.sub(epochAmounts[i]);  delete epochAmounts[i];  Users waiting to withdraw liquidity should wait for someone to buy the policy to update the totalCoverTokens.  Recommendation  Make sure it s possible to call the _updateEpochsInfo function without buying a new policy.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "5.13 Unbounded loops in LiquidityMining    ",
        "body": "  Resolution  Fixed by adding the limits.  Description  There are some methods that have unbounded loops and will fail when enough items exist in the arrays.  code/contracts/LiquidityMining.sol:L83  for (uint256 i = 0; i < _teamsNumber; i++) {  code/contracts/LiquidityMining.sol:L97  for (uint256 i = 0; i < _membersNumber; i++) {  code/contracts/LiquidityMining.sol:L110  for (uint256 i = 0; i < _usersNumber; i++) {  These methods will fail when lots of items will be added to them.  Recommendation  Consider adding limits (from, to) when requesting the items.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "5.14 The _removeFromQueue is very gas greedy    ",
        "body": "  Resolution  The queue structure has changed significantly and became more optimized. On the other hand, the new structure has some overhead and can be simplified to optimize more gas.  Description  The _removeFromQueue function is supposed to remove _countToRemove elements from the queue:  code/contracts/PolicyBook.sol:L296-L313  function _removeFromQueue(uint256 _countToRemove) internal {  for (uint256 i = 0; i < _countToRemove; i++) {  delete withdrawalsInfo[withdrawalQueue[i]];  if (_countToRemove == withdrawalQueue.length) {  delete withdrawalQueue;  } else {  uint256 _remainingArrLength = withdrawalQueue.length.sub(_countToRemove);  address[] memory _remainingArr = new address[](_remainingArrLength);  for (uint256 i = 0; i < _remainingArrLength; i++) {  _remainingArr[i] = withdrawalQueue[i.add(_countToRemove)];  withdrawalQueue = _remainingArr;  This function uses too much gas, which makes it easier to make attacks on the system. Even if only one request is removed and executed, this function rewrites all the requests to the storage.  Recommendation  The data structure should be changed so this function shouldn t rewrite the requests that did not change. For example, it can be a mapping (unit => address) with 2 indexes (start, end) that are only increasing.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "5.15 Withdrawal with zero amount is possible    ",
        "body": "  Resolution   The   Description  When creating a withdrawal request, the amount of tokens to withdraw is passed as a parameter:  code/contracts/PolicyBook.sol:L358  function requestWithdrawal(uint256 _tokensToWithdraw) external override {  The problem is that this parameter can be zero, and the function will be successfully executed. Moreover, this request can then be added to the queue, and the actual withdrawal will also be executed with zero value. Addresses that never added any liquidity could spam the system with these requests.  Recommendation  Do not allow withdrawals of zero tokens.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "5.16 The withdrawal queue is only updated when the liquidity is added    ",
        "body": "  Resolution   The queue is now updated via the   Description  Sometimes when the amount of liquidity is not much higher than the number of tokens locked for the collateral, it s impossible to withdraw liquidity. For a user that wants to withdraw liquidity, a withdrawal request is created. If the request can t be executed, it s added to the withdrawal queue, and the user needs to wait until there s enough collateral for withdrawal. There are potentially 2 ways to achieve that: either someone adds more liquidity or some existing policies expire.  Currently, the queue can only be cleared when the internal _updateWithdrawalQueue  function is called. And it is only called in one place while adding liquidity:  code/contracts/PolicyBook.sol:L276-L290  function _addLiquidityFor(address _liquidityHolderAddr, uint256 _liquidityAmount, bool _isLM) internal {  daiToken.transferFrom(_liquidityHolderAddr, address(this), _liquidityAmount);  uint256 _amountToMint = _liquidityAmount.mul(PERCENTAGE_100).div(getDAIToDAIxRatio());  totalLiquidity = totalLiquidity.add(_liquidityAmount);  _mintERC20(_liquidityHolderAddr, _amountToMint);  if (_isLM) {  liquidityFromLM[_liquidityHolderAddr] = liquidityFromLM[_liquidityHolderAddr].add(_liquidityAmount);  _updateWithdrawalQueue();  emit AddLiquidity(_liquidityHolderAddr, _liquidityAmount, totalLiquidity);  Recommendation  It would be better if the queue could be processed when some policies expire without adding new liquidity. For example, there may be an external function that allows users to process the queue.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "5.17 Optimize gas usage when checking max length of arrays    ",
        "body": "  Description  There are a few cases where some arrays have to be limited to a number of items.  And the max size is enforced by removing the last item if the array reached max size + 1.  code/contracts/LiquidityMining.sol:L386-L388  if (leaderboard.length == MAX_LEADERBOARD_SIZE.add(1)) {  leaderboard.pop();  code/contracts/LiquidityMining.sol:L439-L441  if (topUsers.length == MAX_TOP_USERS_SIZE.add(1)) {  topUsers.pop();  code/contracts/LiquidityMining.sol:L495-L497  if (_addresses.length == MAX_GROUP_LEADERS_SIZE.add(1)) {  groupsLeaders[_referralLink].pop();  A simpler and cheaper way to check if an item should be removed is to change the condition to  if (limitedSizedArray.length > MAX_DEFINED_SIZE_FOR_ARRAY) {  limitedSizedArray.pop();  This check does not need or do a SafeMath call (which is more expensive), and because of the limited number of items, as well as a practical impossibility to add enough items to overflow the limit, makes it a preferred way to check the maximum limit.  Recommendation  Rewrite the checks and remove SafeMath operations, as well as the addition by 1 and change the check to a  greater than  verification.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "5.18 Methods return values that are never used    ",
        "body": "  Description  When a user calls investDAI these 3 methods are called internally:  code/contracts/LiquidityMining.sol:L196-L198  _updateTopUsers();  _updateLeaderboard(_userTeamInfo.teamAddr);  _updateGroupLeaders(_userTeamInfo.teamAddr);  Each method returns a boolean, but the value is never used. It is also unclear what the value should represent.  Recommendation  Remove the returned variable or use it in method investDAI.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "5.19 Save some gas when looping over state arrays    ",
        "body": "  Resolution  Fixed by caching array state length in a local variable.  Description  There are a few loops over state arrays in LiquidutyMining.  code/contracts/LiquidityMining.sol:L209  for (uint256 i = 0; i < leaderboard.length; i++) {  code/contracts/LiquidityMining.sol:L217  for (uint256 i = 0; i < topUsers.length; i++) {  Consider caching the length in a local variable to reduce gas costs.  Examples  Similar to  code/contracts/LiquidityMining.sol:L107  uint256 _usersNumber = allUsers.length;  code/contracts/LiquidityMining.sol:L110  for (uint256 i = 0; i < _usersNumber; i++) {  Recommendation  Reduce gas cost by caching array state length in a local variable.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "5.20 Optimize gas costs when handling liquidity start and end times ",
        "body": "  Description  When the LiquidityMining contract is deployed, startLiquidityMiningTime saves the current block timestamp.  code/contracts/LiquidityMining.sol:L46  startLiquidityMiningTime = block.timestamp;  This value is never changed.  There also exists an end limit calculated by getEndLMTime.  code/contracts/LiquidityMining.sol:L271-L273  function getEndLMTime() public view override returns (uint256) {  return startLiquidityMiningTime.add(2 weeks);  This value is also fixed, once the start was defined.  None of the values change after the contract was deployed. This is why you can use the immutable feature provided by Solidity.  It will reduce costs significantly.  Examples  contract A {  uint public immutable start;  uint public immutable end;  constructor() {  start = block.timestamp;  end = block.timestamp + 2 weeks;  This contract defines 2 variables: start and end and their value is fixed on deploy and cannot be changed.  It does not need to use SafeMath because there s no risk of overflowing.  Setting public on both variables creates getters, and calling A.start() and A.end() returns the respective values.  Having set as immutable does not request EVM storage and makes them very cheap to access.  Recommendation  Use Solidity s immutable feature to reduce gas costs and rename variables for consistency.  Use the example for inspiration.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "5.21 Computing the quote should be done for a positive amount of tokens    ",
        "body": "  Description  When a policy is bought, a quote is requested from the PolicyQuote contract.  code/contracts/PolicyBook.sol:L191-L195  function _buyPolicyFor(  address _policyHolderAddr,  uint256 _epochsNumber,  uint256 _coverTokens  ) internal {  code/contracts/PolicyBook.sol:L213  uint256 _totalPrice = policyQuote.getQuote(_totalSeconds, _coverTokens, address(this));  The getQuote call is then forwarded to an internal function  code/contracts/PolicyQuote.sol:L39-L43  function getQuote(uint256 _durationSeconds, uint256 _tokens, address _policyBookAddr)  external view override returns (uint256 _daiTokens)  _daiTokens = _getQuote(_durationSeconds, _tokens, _policyBookAddr);  code/contracts/PolicyQuote.sol:L45-L47  function _getQuote(uint256 _durationSeconds, uint256 _tokens, address _policyBookAddr)  internal view returns (uint256)  There are some basic checks that make sure the total covered tokens with the requested quote do not exceed the total liquidity. On top of that check, it makes sure the total liquidity is positive.  code/contracts/PolicyQuote.sol:L52-L53  require(_totalCoverTokens.add(_tokens) <= _totalLiquidity, \"PolicyBook: Requiring more than there exists\");  require(_totalLiquidity > 0, \"PolicyBook: The pool is empty\");  But there is no check for the number of quoted tokens. It should also be positive.  Recommendation  Add an additional check for the number of quoted tokens to be positive. The check could fail or return 0, depending on your use case.  If you add a check for the number of quoted tokens to be positive, the check for _totalLiquidity to be positive becomes obsolete and can be removed.  6 Re-audit issues  This section lists the issues found in the re-audit phase. The audit team, reviewed the code fixes after the initial report was delivered.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "6.1 Anyone can win all the funds from the LiquidityMining without investing any DAI ",
        "body": "  Description  When a user decides to investDAI in the LiquidityMining contract, the policy book address is passed as a parameter:  code_new/contracts/LiquidityMining.sol:L198  function investDAI(uint256 _tokensAmount, address _policyBookAddr) external override {  But this parameter is never checked and only used at the end of the function:  code_new/contracts/LiquidityMining.sol:L223  IPolicyBook(_policyBookAddr).addLiquidityFromLM(msg.sender, _tokensAmount);  The attacker can pass the address of a simple multisig that will process this transaction successfully without doing anything. And pretend to invest a lot of DAI without actually doing that to win all the rewards in the LiquidityMining contract.  Recommendation  Check that the pool address is valid.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "6.2 Liquidity withdrawal can be blocked ",
        "body": "  Description  The main problem in that issue is that the liquidity provider may face many potential issues when withdrawing the liquidity. Under some circumstances, a normal user will never be able to withdraw the liquidity. This issue consists of multiple factors that are interconnected and share the same solution.  There are no partial withdrawals when in the queue. When the withdrawal request is added to the queue, it can only be processed fully: code_new/contracts/PolicyBook.sol:L444-L451 address _currentAddr = withdrawalQueue.head(); uint256 _tokensToWithdraw = withdrawalsInfo[_currentAddr].withdrawalAmount;  uint256 _amountInDAI = convertDAIXtoDAI(_tokensToWithdraw);  if (_availableLiquidity < _amountInDAI) {   break; } But when the request is not in the queue, it can still be processed partially, and the rest of the locked tokens will wait in the queue. code_new/contracts/PolicyBook.sol:L581-L590 } else if (_availableLiquidity < convertDAIXtoDAI(_tokensToWithdraw)) {   uint256 _availableDAIxTokens = convertDAIToDAIx(_availableLiquidity);   uint256 _currentWithdrawalAmount = _tokensToWithdraw.sub(_availableDAIxTokens);   withdrawalsInfo[_msgSender()].withdrawalAmount = _currentWithdrawalAmount;    aggregatedQueueAmount = aggregatedQueueAmount.add(_currentWithdrawalAmount);   withdrawalQueue.push(_msgSender());    _withdrawLiquidity(_msgSender(), _availableDAIxTokens); } else { If there s a huge request in the queue, it can become a bottleneck that does not allow others to withdraw even if there is enough free liquidity.  Withdrawals can be blocked forever by the bots. The withdrawal can only be requested if there are enough free funds in the contract. But once these funds appear, the bots can instantly buy a policy, and for the normal users, it will be impossible to request the withdrawal. Even when a withdrawal is requested and then in the queue, the same problem appears at that stage.  The policy can be bought even if there are pending withdrawals in the queue.  Recommendation  One of the solutions would be to implement the following changes, but the team should thoroughly consider them:  Allow people to request the withdrawal even if there is not enough liquidity at the moment.  Do not allow people to buy policies if there are pending withdrawals in the queue and cannot be executed.  (Optional) Even when the queue is empty, do not allow people to buy policies if there is not enough liquidity for the pending requests (that are not yet in the queue).  (Optional if the points above are implemented) Allow partial executions of the withdrawals in the queue.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "6.3 The totalCoverTokens can be decreased before the claim is committed ",
        "body": "  Description  The totalCoverTokens is decreased right after the policy duration ends (_endEpochNumber). When that happens, the liquidity providers can withdraw their funds:  code_new/contracts/PolicyBook.sol:L262-L265  policyHolders[_msgSender()] = PolicyHolder(_coverTokens, currentEpochNumber,  _endEpochNumber, _totalPrice, _reinsurancePrice);  epochAmounts[_endEpochNumber] = epochAmounts[_endEpochNumber].add(_coverTokens);  code_new/contracts/PolicyBook.sol:L343-L351  uint256 _countOfPassedEpoch = block.timestamp.sub(epochStartTime).div(EPOCH_DURATION);  newTotalCoverTokens = totalCoverTokens;  lastEpochUpdate = currentEpochNumber;  newEpochNumber = _countOfPassedEpoch.add(1);  for (uint256 i = lastEpochUpdate; i < newEpochNumber; i++) {  newTotalCoverTokens = newTotalCoverTokens.sub(epochAmounts[i]);  On the other hand, the claim can be created while the policy is still  active . And is considered active until one week after the policy expired:  code_new/contracts/PolicyRegistry.sol:L50-L58  function isPolicyActive(address _userAddr, address _policyBookAddr) public override view returns (bool) {  PolicyInfo storage _currentInfo = policyInfos[_userAddr][_policyBookAddr];  if (_currentInfo.endTime == 0) {  return false;  return _currentInfo.endTime.add(STILL_CLAIMABLE_FOR) > block.timestamp;  By the time when the claim is created + voted, the liquidity provider can potentially withdraw all of their funds already, and the claim will fail.  Recommendation  Make sure that there will always be enough funds for the claim.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "6.4 The totalCoverTokens is not decreased after the claim happened ",
        "body": "  Description  When the claim happens and the policy is removed, the totalCoverTokens should be decreased instantly, that s why the scheduled reduction value is removed:  code_new/contracts/PolicyBook.sol:L228-L236  PolicyHolder storage holder = policyHolders[claimer];  epochAmounts[holder.endEpochNumber] = epochAmounts[holder.endEpochNumber].sub(holder.coverTokens);  totalLiquidity = totalLiquidity.sub(claimAmount);  daiToken.transfer(claimer, claimAmount);  delete policyHolders[claimer];  policyRegistry.removePolicy(claimer);  But the totalCoverTokens is not changed and will have the coverage from the removed policy forever.  Recommendation  Decrease the totalCoverTokens inside the commitClaim function.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "6.5 The Queue remove function does not remove the item completely ",
        "body": "  Description  When removing an item in a queue, the following function is used:  code_new/contracts/helpers/Queue.sol:L78-L98  function remove(UniqueAddressQueue storage baseQueue, address addrToRemove) internal returns (bool) {  if (!contains(baseQueue, addrToRemove)) {  return false;  if (baseQueue.HEAD == addrToRemove) {  return removeFirst(baseQueue);  if (baseQueue.TAIL == addrToRemove) {  return removeLast(baseQueue);  address prevAddr = baseQueue.queue[addrToRemove].prev;  address nextAddr = baseQueue.queue[addrToRemove].next;  baseQueue.queue[prevAddr].next = nextAddr;  baseQueue.queue[nextAddr].prev = prevAddr;  baseQueue.queueLength--;  return true;  As the result, the baseQueue.queue[addrToRemove] is not deleted, so the contains function will still return True after the removal.  Recommendation  Remove the element from the queue completely.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "6.6 Optimization issue ",
        "body": "  Description  The codebase is huge, and there are still a lot of places where these complications and gas efficiency can be improved.  Examples  _updateTopUsers, _updateGroupLeaders, _updateLeaderboard are having a similar mechanism of adding users to a sorted set which makes more storage operations than needed: code_new/contracts/LiquidityMining.sol:L473-L486 uint256 _tmpIndex = _currentIndex - 1; uint256 _currentUserAmount = usersTeamInfo[msg.sender].stakedAmount;  while (_currentUserAmount > usersTeamInfo[topUsers[_tmpIndex]].stakedAmount) {     address _tmpAddr = topUsers[_tmpIndex];     topUsers[_tmpIndex] = msg.sender;     topUsers[_tmpIndex + 1] = _tmpAddr;      if (_tmpIndex == 0) {         break;     }      _tmpIndex--; } Instead of doing 2 operations per item that is lower than the new_item, same can be done with one operation: while topUsers[_tmpIndex] is lower than the new itemtopUsers[_tmpIndex + 1] = topUsers[_tmpIndex].  creating the Queue library looks like overkill for the intended task. It is only used for the withdrawal queue in the PolicyBook. The structure stores and processes extra data, which is unnecessary and more expensive. A larger codebase also has a higher chance of introducing a bug (and it happened here https://github.com/ConsenSys/bridge-mutual-audit-2021-03/issues/25). It s usually better to have a simpler and optimized version like described here issue 5.14.  There are a few for loops that are using uint8 iterators. It s unnecessary and can be even more expensive because, under the hood, it s additionally converted to uint256 all the time. In general, shrinking data to uint8 makes sense to optimize storage slots, but that s not the case here.  The value that is calculated in a loop can be obtained simpler by just having a 1-line formula: code_new/contracts/LiquidityMining.sol:L351-L367 function _getAvailableMonthForReward(address _userAddr) internal view returns (uint256) {     uint256 _oneMonth = 30 days;     uint256 _startRewardTime = getEndLMTime();      uint256 _countOfRewardedMonth = countsOfRewardedMonth[usersTeamInfo[_userAddr].teamAddr][_userAddr];     uint256 _numberOfMonthForReward;      for (uint256 i = _countOfRewardedMonth; i < MAX_MONTH_TO_GET_REWARD; i++) {         if (block.timestamp > _startRewardTime.add(_oneMonth.mul(i))) {         _numberOfMonthForReward++;         } else {             break;         }     }      return _numberOfMonthForReward; }  The mapping is using 2 keys, but the first key is strictly defined by the second one, so there s no need for it: code_new/contracts/LiquidityMining.sol:L60-L61 // Referral link => Address => count of rewarded month mapping (address => mapping (address => uint256)) public countsOfRewardedMonth;  There are a lot of structures in the code with duplicated and unnecessary data, for example: code_new/contracts/LiquidityMining.sol:L42-L48 struct UserTeamInfo {     string teamName;     address teamAddr;      uint256 stakedAmount;     bool isNFTDistributed; } Here the structure is created for every team member, duplicating the team name for each member.  Recommendation  Optimize and simplify the code.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "6.7 Proper usage of the transfer and the transferFrom functions ",
        "body": "  Description  Many ERC-20 transfers in the code are just called without checking the return values:  code_new/contracts/PolicyBook.sol:L269-L270  daiToken.transferFrom(_msgSender(), reinsurancePoolAddress, _reinsurancePrice);  daiToken.transferFrom(_msgSender(), address(this), _price);  code_new/contracts/PolicyBook.sol:L556-L559  function _unlockTokens(uint256 _amountToUnlock) internal {  this.transfer(_msgSender(), _amountToUnlock);  delete withdrawalsInfo[_msgSender()];  code_new/contracts/LiquidityMining.sol:L278  bmiToken.transfer(msg.sender, _userReward);  Even though the tokens in these calls are not arbitrary (DAI, BMI, DAIx, stkBMIToken) and probably always return True or call revert, it s still better to comply with the ERC-20 standard and make sure that the transfer went well.  Recommendation  The best solution would be better to always use the safe version of the transfers from openzeppelin/contracts/token/ERC20/SafeERC20.sol.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "6.8 The price and the duration of a policy may be unpredictable ",
        "body": "  Description  When the user is buying a policy, the price is calculated based on the current liquidity/coverage ratio, and the duration is calculated based on the current timestamp. A malicious actor can front-run the buyer (e.g., buy short-term insurance with a huge coverage) and increase the policy s price. Or the transaction can be executed much later for some reason, and the number of the totalSeconds may be larger, the coverage period can be between _epochsNumber - 1 and _epochsNumber.  Recommendation  Given the unpredictability of the price, it s better to pass the hard limit for the insurance price as a parameter. Also, as an opinion, you can add a deadline for the transaction as a parameter.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "6.9 The aggregatedQueueAmount  value is used inconsistently ",
        "body": "  Description  The aggregatedQueueAmount variable represents the cumulative DAIx amount in the queue that is waiting for the withdrawal. When requesting the withdrawal, this value is used as the amount of DAI that needs to be withdrawn, which may be significantly different:  code_new/contracts/PolicyBook.sol:L539-L540  require(totalLiquidity >= totalCoverTokens.add(aggregatedQueueAmount).add(_daiTokensToWithdraw),  \"PB: Not enough available liquidity\");  That may lead to allowing the withdrawal request even if it shouldn t be allowed and the opposite.  Recommendation  Convert aggregatedQueueAmount to DAI in the _requestWithdrawal.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "6.10 The claim can only be done once ",
        "body": "  Description  When the claim happens, the policy is removed afterward:  code_new/contracts/PolicyBook.sol:L222-L237  function commitClaim(address claimer, uint256 claimAmount)  external  override  onlyClaimVoting  updateBMIDAIXStakingReward  PolicyHolder storage holder = policyHolders[claimer];  epochAmounts[holder.endEpochNumber] = epochAmounts[holder.endEpochNumber].sub(holder.coverTokens);  totalLiquidity = totalLiquidity.sub(claimAmount);  daiToken.transfer(claimer, claimAmount);  delete policyHolders[claimer];  policyRegistry.removePolicy(claimer);  If the claim amount is much lower than the coverage, the users are incentivized not to submit it and wait until the end of the coverage period to accumulate all the claims into one.  Recommendation  Allow the policyholders to submit multiple claims until the coverTokens is not reached.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "6.11 Users are incentivised to invest right before the getEndLMTime to join the winning team ",
        "body": "  Description  When investing, there are 3 types of rewards in the LiquidityMining contracts: for the top users, for the top teams, for the group leaders in the top teams. EVERY member from the top teams is getting a reward proportional to the provided stake. Only the final snapshot of the stakes is used to determine the leaderboard which is right after the getEndLMTime.  Everyone can join any team, and everyone s goal is to go to the winning teams. The best way to do so is to wait right until the end of the period and join the most beneficial team.  Recommendation  It s better to avoid extra incentives that create race conditions.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"
    },
    {
        "title": "4.1 iETH.exchangeRateStored may not be accurate when invoked from external contracts ",
        "body": "  Resolution   This issue was addressed in commit   9876e3a by using a modifier to track the current  Description  iETH.exchangeRateStored returns the exchange rate of the contract as a function of the current cash of the contract. In the case of iETH, current cash is calculated as the contract s ETH balance minus msg.value:  code/contracts/iETH.sol:L54-L59  /**  @dev Gets balance of this contract in terms of the underlying  /  function _getCurrentCash() internal view override returns (uint256) {  return address(this).balance.sub(msg.value);  msg.value is subtracted because the majority of iETH methods are payable, and msg.value is implicitly added to a contract s balance before execution begins. If msg.value were not subtracted, the value sent with a call could be used to inflate the contract s exchange rate artificially.  Examples  This problem occurs in multiple locations in the Controller:  beforeMint uses the exchange rate to ensure the supply capacity of the market is not reached. In this case, inflation would prevent the entire supply capacity of the market from being utilized:  code/contracts/Controller.sol:L670-L678  // Check the iToken's supply capacity, -1 means no limit  uint256 _totalSupplyUnderlying =  IERC20Upgradeable(_iToken).totalSupply().rmul(  IiToken(_iToken).exchangeRateStored()  );  require(  _totalSupplyUnderlying.add(_mintAmount) <= _market.supplyCapacity,  \"Token supply capacity reached\"  );  beforeLiquidateBorrow uses the exchange rate via calcAccountEquity to calculate the value of the borrower s collateral. In this case, inflation would increase the account s equity, which could prevent the liquidator from liquidating:  code/contracts/Controller.sol:L917-L919  (, uint256 _shortfall, , ) = calcAccountEquity(_borrower);  require(_shortfall > 0, \"Account does not have shortfall\");  Recommendation  Rather than having the Controller query the iETH.exchangeRateStored, the exchange rate could be passed-in to Controller methods as a parameter.  Ensure no other components in the system rely on iETH.exchangeRateStored after being called from iETH.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"
    },
    {
        "title": "4.2 Unbounded loop in Controller.calcAccountEquity allows DoS on liquidation ",
        "body": "  Description  Controller.calcAccountEquity calculates the relative value of a user s supplied collateral and their active borrow positions. Users may mark an arbitrary number of assets as collateral, and may borrow from an arbitrary number of assets. In order to calculate the value of both of these positions, this method performs two loops.  First, to calculate the sum of the value of a user s collateral:  code/contracts/Controller.sol:L1227-L1233  // Calculate value of all collaterals  // collateralValuePerToken = underlyingPrice * exchangeRate * collateralFactor  // collateralValue = balance * collateralValuePerToken  // sumCollateral += collateralValue  uint256 _len = _accountData.collaterals.length();  for (uint256 i = 0; i < _len; i++) {  IiToken _token = IiToken(_accountData.collaterals.at(i));  Second, to calculate the sum of the value of a user s borrow positions:  code/contracts/Controller.sol:L1263-L1268  // Calculate all borrowed value  // borrowValue = underlyingPrice * underlyingBorrowed / borrowFactor  // sumBorrowed += borrowValue  _len = _accountData.borrowed.length();  for (uint256 i = 0; i < _len; i++) {  IiToken _token = IiToken(_accountData.borrowed.at(i));  From dForce, we learned that 200 or more assets would be supported by the Controller. This means that a user with active collateral and borrow positions on all 200 supported assets could force any calcAccountEquity action to perform some 400 iterations of these loops, each with several expensive external calls.  Examples  By modifying dForce s unit test suite, we showed that an attacker could force the cost of calcAccountEquity above the block gas limit. This would prevent all of the following actions, as each relies on calcAccountEquity:  iToken.transfer and iToken.transferFrom  iToken.redeem and iToken.redeemUnderlying  iToken.borrow  iToken.liquidateBorrow and iToken.seize  The following actions would still be possible:  iToken.mint  iToken.repayBorrow and iToken.repayBorrowBehalf  As a result, an attacker may abuse the unbounded looping in calcAccountEquity to prevent the liquidation of underwater positions. We provided dForce with a PoC here: gist.  Recommendation  There are many possible ways to address this issue. Some ideas have been outlined below, and it may be that a combination of these ideas is the best approach:  In general, cap the number of markets and borrowed assets a user may have: The primary cause of the DoS is that the number of collateral and borrow positions held by a user is only restricted by the number of supported assets. The PoC provided above showed that somewhere around 150 collateral positions and 150 borrow positions, the gas costs of calcAccountEquity use most of the gas in a block. Given that gas prices often spike along with turbulent market conditions and that liquidations are far more likely in turbulent market conditions, a cap on active markets / borrows should be much lower than 150 each so as to keep the cost of liquidations as low as possible.  dForce should perform their own gas cost estimates to determine a cap, and choose a safe, low value. Estimates should be performed on the high-level liquidateBorrow method, so as to simulate an actual liquidation event. Additionally, estimates should factor in a changing block gas limit, and the possibility of opcode gas costs changing in future forks. It may be wise to make this cap configurable, so that the limits may be adjusted for future conditions.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"
    },
    {
        "title": "4.3 Fix utilization rate computation and respect reserves when lending ",
        "body": "  Resolution   The dForce team has informed us that the only two interest rate models that are still in use are   2a0e974 and  c11fa9b.  Description  The utilization rate UR of an asset forms the basis for interest calculations and is defined as borrows / ( borrows + cash - reserves).  code/contracts/InterestRateModel/InterestRateModel.sol:L72-L88  /**  @notice Calculate the utilization rate: `_borrows / (_cash + _borrows - _reserves)`  @param _cash Asset balance  @param _borrows Asset borrows  @param _reserves Asset reserves  @return Asset utilization [0, 1e18]  /  function utilizationRate(  uint256 _cash,  uint256 _borrows,  uint256 _reserves  ) internal pure returns (uint256) {  // Utilization rate is 0 when there are no borrows  if (_borrows == 0) return 0;  return _borrows.mul(BASE).div(_cash.add(_borrows).sub(_reserves));  issue 4.4.  Recommendation  If reserves > cash \u2014 or, in other words, available cash is negative \u2014 this means part of the reserves have been borrowed, which ideally shouldn t happen in the first place. However, the reserves grow automatically over time, so it might be difficult to avoid this entirely. We recommend (1) avoiding this situation whenever it is possible and (2) fixing the UR computation such that it deals more gracefully with this scenario. More specifically:  Loan amounts should not be checked to be smaller than or equal to cash but cash - reserves (which might be negative). Note that the current check against cash happens more or less implicitly because the transfer just fails for insufficient cash.  Make the utilization rate computation return 1 if reserves > cash (unless borrows == 0, in which case return 0 as is already the case).  Remark  Internally, the utilization rate and other fractional values are scaled by 1e18. The discussion above has a more conceptual than technical perspective, so we used unscaled numbers. When making changes to the code, care must be taken to apply the scaling.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"
    },
    {
        "title": "4.4 If Base._updateInterest fails, the entire system will halt ",
        "body": "  Resolution   dForce removed   27f9a28.  Description  Before executing most methods, the iETH and iToken contracts update interest accumulated on borrows via the method Base._updateInterest. This method uses the contract s interest rate model to calculate the borrow interest rate. If the calculated value is above maxBorrowRate (0.001e18), the method will revert:  code/contracts/TokenBase/Base.sol:L92-L107  function _updateInterest() internal virtual override {  InterestLocalVars memory _vars;  _vars.currentCash = _getCurrentCash();  _vars.totalBorrows = totalBorrows;  _vars.totalReserves = totalReserves;  // Gets the current borrow interest rate.  _vars.borrowRate = interestRateModel.getBorrowRate(  _vars.currentCash,  _vars.totalBorrows,  _vars.totalReserves  );  require(  _vars.borrowRate <= maxBorrowRate,  \"_updateInterest: Borrow rate is too high!\"  );  If this method reverts, the entire contract may halt and be unrecoverable. The only ways to change the values used to calculate this interest rate lie in methods that must first call Base._updateInterest. In this case, those methods would fail.  One other potential avenue for recovery exists: the Owner role may update the interest rate calculation contract via TokenAdmin._setInterestRateModel:  code/contracts/TokenBase/TokenAdmin.sol:L46-L63  /**  @dev Sets a new interest rate model.  @param _newInterestRateModel The new interest rate model.  /  function _setInterestRateModel(  IInterestRateModelInterface _newInterestRateModel  ) external virtual onlyOwner settleInterest {  // Gets current interest rate model.  IInterestRateModelInterface _oldInterestRateModel = interestRateModel;  // Ensures the input address is the interest model contract.  require(  _newInterestRateModel.isInterestRateModel(),  \"_setInterestRateModel: This is not the rate model contract!\"  );  // Set to the new interest rate model.  interestRateModel = _newInterestRateModel;  However, this method also calls Base._updateInterest before completing the upgrade, so it would fail as well.  Examples  We used interest rate parameters taken from dForce s unit tests to determine whether any of the interest rate models could return a borrow rate that would cause this failure. The default InterestRateModel is deployed using these values:  Plugging these values in to their borrow rate calculations, we determined that the utilization rate of the contract would need to be 2103e18 in order to reach the max borrow rate and trigger a failure. Plugging this in to the formula for utilization rate, we derived the following ratio:  reserves >= (2102/2103)*borrows + cash  With the given interest rate parameters, if token reserves, total borrows, and underlying cash meet the above ratio, the interest rate model would return a borrow rate above the maximum, leading to the failure conditions described above.  Recommendation  Note that the examples above depend on the specific interest rate parameters configured by dForce. In general, with reasonable interest rate parameters and a reasonable reserve ratio, it seems unlikely that the maximum borrow rate will be reached. Consider implementing the following changes as a precaution:  As utilization rate should be between 0 and 1 (scaled by 1e18), prevent utilization rate calculations from returning anything above 1e18. See issue 4.3 for a more thorough discussion of this topic.  Remove the settleInterest modifier from TokenAdmin._setInterestRateModel: In a worst case scenario, this will allow the Owner role to update the interest rate model without triggering the failure in Base._updateInterest.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"
    },
    {
        "title": "4.5 RewardDistributor requirement prevents transition of Owner role to smart contract ",
        "body": "  Resolution   This issue was addressed in commit   4f1e31b by invoking  Description  From dForce, we learned that the eventual plan for the system Owner role is to use a smart contract (a multisig or DAO). However, a requirement in RewardDistributor would prevent the onlyOwner method _setDistributionFactors from working in this case.  _setDistributionFactors calls updateDistributionSpeed, which requires that the caller is an EOA:  code/contracts/RewardDistributor.sol:L179-L189  /**  @notice Update each iToken's distribution speed according to current global speed  @dev Only EOA can call this function  /  function updateDistributionSpeed() public override {  require(msg.sender == tx.origin, \"only EOA can update speeds\");  require(!paused, \"Can not update speeds when paused\");  // Do the actual update  _updateDistributionSpeed();  In the event the Owner role is a smart contract, this statement would necessitate a complicated upgrade to restore full functionality.  Recommendation  Rather than invoking updateDistributionSpeed, have _setDistributionFactors directly call the internal helper _updateDistributionSpeed, which does not require the caller is an EOA.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"
    },
    {
        "title": "4.6 MSDController._withdrawReserves does not update interest before withdrawal ",
        "body": "  Resolution  This issue was addressed in commit 2b5946e by changing calcEquity to update the interest of each MSDMinter assigned to an MSD asset.  Note that this method iterates over each MSDMinter, which may cause out-of-gas issues if the number of MSDMinters grows. dForce has informed us that the MSDMinter role will only be held by two contracts per asset (iMSD and MSDS).  Description  MSDController._withdrawReserves allows the Owner to mint the difference between an MSD asset s accumulated debt and earnings:  code/contracts/msd/MSDController.sol:L182-L195  function _withdrawReserves(address _token, uint256 _amount)  external  onlyOwner  onlyMSD(_token)  (uint256 _equity, ) = calcEquity(_token);  require(_equity >= _amount, \"Token do not have enough reserve\");  // Increase the token debt  msdTokenData[_token].debt = msdTokenData[_token].debt.add(_amount);  // Directly mint the token to owner  MSD(_token).mint(owner, _amount);  Debt and earnings are updated each time the asset s iMSD and MSDS contracts are used for the first time in a given block. Because _withdrawReserves does not force an update to these values, it is possible for the withdrawal amount to be calculated using stale values.  Recommendation  Ensure _withdrawReserves invokes iMSD.updateInterest() and MSDS.updateInterest().  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"
    },
    {
        "title": "4.7 permit functions use deployment-time instead of execution-time chain ID ",
        "body": "  Resolution   This has been addressed in commits   a7b8fb0 and  d659f2b. The approach taken by the dForce team is to include the chain ID separately in the digest to be signed and keep the deployment/initialization-time chain ID in the  Description  EIP-2612-style  EIP-712 signatures. We focus this discussion on the  code/contracts/TokenBase/Base.sol:L23-L56  function _initialize(  string memory _name,  string memory _symbol,  uint8 _decimals,  IControllerInterface _controller,  IInterestRateModelInterface _interestRateModel  ) internal virtual {  controller = _controller;  interestRateModel = _interestRateModel;  accrualBlockNumber = block.number;  borrowIndex = BASE;  flashloanFeeRatio = 0.0008e18;  protocolFeeRatio = 0.25e18;  __Ownable_init();  __ERC20_init(_name, _symbol, _decimals);  __ReentrancyGuard_init();  uint256 chainId;  assembly {  chainId := chainid()  DOMAIN_SEPARATOR = keccak256(  abi.encode(  keccak256(  \"EIP712Domain(string name,string version,uint256 chainId,address verifyingContract)\"  ),  keccak256(bytes(_name)),  keccak256(bytes(\"1\")),  chainId,  address(this)  );  The DOMAIN_SEPARATOR is supposed to prevent replay attacks by providing context for the signature; it is hashed into the digest to be signed.  code/contracts/TokenBase/Base.sol:L589-L610  bytes32 _digest =  keccak256(  abi.encodePacked(  \"\\x19\\x01\",  DOMAIN_SEPARATOR,  keccak256(  abi.encode(  PERMIT_TYPEHASH,  _owner,  _spender,  _value,  _currentNonce,  _deadline  );  address _recoveredAddress = ecrecover(_digest, _v, _r, _s);  require(  _recoveredAddress != address(0) && _recoveredAddress == _owner,  \"permit: INVALID_SIGNATURE!\"  );  The chain ID is not necessarily constant, though. In the event of a chain split, only one of the resulting chains gets to keep the original chain ID and the other will have to use a new one. With the current pattern, a signature will be valid on both chains; if the DOMAIN_SEPARATOR is recomputed for every verification, a signature will only be valid on the chain that keeps the original ID \u2014 which is probably the intended behavior.  Remark  The reason why the not necessarily constant chain ID is part of the supposedly constant DOMAIN_SEPARATOR is that EIP-712 predates the introduction of the CHAINID opcode. Originally, it was not possible to query the chain ID via opcode, so it had to be supplied to the constructor of a contract by the deployment script.  Recommendation  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"
    },
    {
        "title": "4.8 iETH.receive() does not support contracts executing during their constructor ",
        "body": "  Description  iETH.receive() requires that the caller is a contract:  code/contracts/iETH.sol:L187-L195  /**  @notice receive ETH, used for flashloan repay.  /  receive() external payable {  require(  msg.sender.isContract(),  \"receive: Only can call from a contract!\"  );  This method uses the extcodesize of an account to check that the account belongs to a contract. However, contracts currently executing their constructor will have an extcodesize of 0, and will not be able to use this method.  This is unlikely to cause significant issues, but dForce may want to consider supporting this edge case.  Recommendation  Use msg.sender != tx.origin as a more reliable method to detect use by a contract.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"
    },
    {
        "title": "4.1 Intentional secret reuse can block borrower and lender from accepting liquidation payment    ",
        "body": "  Resolution   This is fixed in   AtomicLoans/atomicloans-eth-contracts#65.  Description  For Dave (the liquidator) to claim the collateral he s purchasing, he must reveal secret D. Once that secret is revealed, Alice and Bob (the borrower and lender) can claim the payment.  Secrets must be provided via the Sales.provideSecret() function:  code/ethereum/contracts/Sales.sol:L193-L200  function provideSecret(bytes32 sale, bytes32 secret_) external {  require(sales[sale].set);  if      (sha256(abi.encodePacked(secret_)) == secretHashes[sale].secretHashA) { secretHashes[sale].secretA = secret_; }  else if (sha256(abi.encodePacked(secret_)) == secretHashes[sale].secretHashB) { secretHashes[sale].secretB = secret_; }  else if (sha256(abi.encodePacked(secret_)) == secretHashes[sale].secretHashC) { secretHashes[sale].secretC = secret_; }  else if (sha256(abi.encodePacked(secret_)) == secretHashes[sale].secretHashD) { secretHashes[sale].secretD = secret_; }  else                                                                          { revert(); }  Note that if Dave chooses the same secret hash as either Alice, Bob, or Charlie (arbiter), there is no way to set secretHashes[sale].secretD because one of the earlier conditionals will execute.  For Alice and Bob to later receive payment, they must be able to provide Dave s secret:  code/ethereum/contracts/Sales.sol:L218-L222  function accept(bytes32 sale) external {  require(!accepted(sale));  require(!off(sale));  require(hasSecrets(sale));  require(sha256(abi.encodePacked(secretHashes[sale].secretD)) == secretHashes[sale].secretHashD);  Dave can exploit this to obtain the collateral for free:  Dave looks at Alice s secret hashes to see which will be used in the sale.  Dave begins the liquidation process, using the same secret hash.  Alice and Bob reveal their secrets A and B through the process of moving the collateral.  Dave now knows the preimage for the secret hash he provided. It was revealed by Alice already.  Dave uses that secret to obtain the collateral.  Alice and Bob now want to receive payment, but they re unable to provide Dave s secret to the Sales smart contract due to the order of conditionals in provideSecret().  After an expiration, Dave can claim a refund.  Mitigating factors  Alice and Bob could notice that Dave chose a duplicate secret hash and refuse to proceed with the sale. This is not something they are likely to do.  Recommendation  Either change the way provideSecret() works to allow for duplicate secret hashes or reject duplicate hashes in create().  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/atomic-loans/"
    },
    {
        "title": "4.2 There is no way to convert between custom and non-custom funds   ",
        "body": "  Resolution  Users who want to switch between custom and non-custom funds can create a new address to do so. This is not actually a big burden because lenders need to use agent software to manage their funds anyway. That workflow typically involves generating a new address because the private key needs to be given to the agent software.  Description  Each fund is created using either Funds.create() or Funds.createCustom(). Both enforce a limitation that there can only be one fund per account:  code/ethereum/contracts/Funds.sol:L348-L355  function create(  uint256  maxLoanDur_,  uint256  maxFundDur_,  address  arbiter_,  bool     compoundEnabled_,  uint256  amount_  ) external returns (bytes32 fund) {  require(fundOwner[msg.sender].lender != msg.sender || msg.sender == deployer); // Only allow one loan fund per address  code/ethereum/contracts/Funds.sol:L383-L397  function createCustom(  uint256  minLoanAmt_,  uint256  maxLoanAmt_,  uint256  minLoanDur_,  uint256  maxLoanDur_,  uint256  maxFundDur_,  uint256  liquidationRatio_,  uint256  interest_,  uint256  penalty_,  uint256  fee_,  address  arbiter_,  bool     compoundEnabled_,  uint256  amount_  ) external returns (bytes32 fund) {  require(fundOwner[msg.sender].lender != msg.sender || msg.sender == deployer); // Only allow one loan fund per address  These functions are the only place where bools[fund].custom is set, and there s no way to delete a fund once it exists. This means there s no way for a given account to switch between a custom and non-custom fund.  This could be a problem if, for example, the default parameters change in a way that a user finds unappealing. They may want to switch to using a custom fund but find themselves unable to do so without moving to a new Ethereum account.  Recommendation  Either allow funds to be deleted or allow funds to be switched between custom and non-custom.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/atomic-loans/"
    },
    {
        "title": "4.3 Funds.maxFundDur has no effect if maxLoanDur is set    ",
        "body": "  Resolution   This is fixed in   AtomicLoans/atomicloans-eth-contracts#68.  Description  Funds.maxFundDur specifies the maximum amount of time a fund should be active. It s checked in request() to ensure the duration of the loan won t exceed that time, but the check is skipped if maxLoanDur is set:  code/ethereum/contracts/Funds.sol:L510-L514  if (maxLoanDur(fund) > 0) {  require(loanDur_       <= maxLoanDur(fund));  } else {  require(now + loanDur_ <= maxFundDur(fund));  Examples  If a user sets maxLoanDur (the maximum loan duration) to 1 week and sets the maxFundDur (timestamp when all loans should be complete) to December 1st, then there can actually be a loan that ends on December 7th.  Recommendation  Check against maxFundDur even when maxLoanDur is set.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/atomic-loans/"
    },
    {
        "title": "4.4 In Funds, maxFundDur is misnamed    ",
        "body": "  Resolution   This is fixed in   AtomicLoans/atomicloans-eth-contracts#66.  Description  This is a timestamp, not a duration.  Recommendation  Rename to something with  timestamp  or perhaps  expiration  in the name.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/atomic-loans/"
    },
    {
        "title": "4.5 Funds.update() lets users update fields that may not have any effect    ",
        "body": "  Resolution   This is fixed in   AtomicLoans/atomicloans-eth-contracts#67.  Description  Funds.update() allows users to update the following fields which are only used if bools[fund].custom is set:  minLoanamt  maxLoanAmt  minLoanDur  interest  penalty  fee  liquidationRatio  If bools[fund].custom is not set, then these changes have no effect. This may be misleading to users.  Examples  code/ethereum/contracts/Funds.sol:L454-L478  function update(  bytes32  fund,  uint256  minLoanAmt_,  uint256  maxLoanAmt_,  uint256  minLoanDur_,  uint256  maxLoanDur_,  uint256  maxFundDur_,  uint256  interest_,  uint256  penalty_,  uint256  fee_,  uint256  liquidationRatio_,  address  arbiter_  ) external {  require(msg.sender == lender(fund));  funds[fund].minLoanAmt       = minLoanAmt_;  funds[fund].maxLoanAmt       = maxLoanAmt_;  funds[fund].minLoanDur       = minLoanDur_;  funds[fund].maxLoanDur       = maxLoanDur_;  funds[fund].maxFundDur       = maxFundDur_;  funds[fund].interest         = interest_;  funds[fund].penalty          = penalty_;  funds[fund].fee              = fee_;  funds[fund].liquidationRatio = liquidationRatio_;  funds[fund].arbiter          = arbiter_;  Recommendation  This could be addressed by creating two update functions: one for custom funds and one for non-custom funds. Only the update for custom funds would allow setting these values.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/atomic-loans/"
    },
    {
        "title": "5.1 Reward rate changes are not taken into account in LP staking ",
        "body": "  Resolution  Comment from pSTAKE Finance team:  Have implemented two new Emission logic for pToken & Other Reward Tokens in StakeLP which mandatorily distributes rewards only after updating the Reward Pool, thereby fixing this potential issue.  Description  When users update their reward (e.g., by calling the calculateRewards function), the reward amount is calculated according to all reward rate changes after the last update. So it does not matter when and how frequently you update the reward; in the end, you re going to have the same amount.  On the other hand, we can t say the same about the lp staking provided in the StakeLPCoreV8 contract. The amount of these rewards depends on when you call the calculateRewardsAndLiquidity function, and the reward amount can even decrease over time.  Two main factors lead to this:  Changes in the reward rate. If the reward rate is decreased at some point, it s getting partially propagated to all the rewards there were not distributed yet. So the reward of the users that didn t call the calculateRewardsAndLiquidity function may decrease. On the other hand, if the reward rate is supposed to increase, it s better to wait and not call calculateRewardsAndLiquidity for as long as possible.  Not every liquidity provider will stake their LP tokens. When users provide liquidity but do not stake the LP tokens, the reward for these Stokens is still going to the Holder contract. These rewards getting proportionally distributed to the users that are staking their LP tokens. Basically, these rewards are added to the current reward rate but change more frequently. The same logic applies to that rewards; if you expect the unstaked LP tokens to increase, it s in your interest not to withdraw your rewards. But if they are decreasing, it s better to gather the rewards as early as possible.  Recommendation  The most preferred staking solution is to have an algorithm that is not giving people an incentive to gather the rewards earlier or later.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"
    },
    {
        "title": "5.2 The withdrawUnstakedTokens may run out of gas ",
        "body": "  Resolution  Comment from pSTAKE Finance team:  Have implemented a batchingLimit variable which enforces a definite number of iterations during withdrawal of unstaked tokens, instead of indefinite iterations.  Description  The withdrawUnstakedTokens is iterating over all batches of unstaked tokens. One user, if unstaked many times, could get their tokens stuck in the contract.  code/contracts/LiquidStakingV2.sol:L369-L403  function withdrawUnstakedTokens(address staker)  public  virtual  override  whenNotPaused  require(staker == _msgSender(), \"LQ20\");  uint256 _withdrawBalance;  uint256 _unstakingExpirationLength = _unstakingExpiration[staker]  .length;  uint256 _counter = _withdrawCounters[staker];  for (  uint256 i = _counter;  i < _unstakingExpirationLength;  i = i.add(1)  ) {  //get getUnstakeTime and compare it with current timestamp to check if 21 days + epoch difference has passed  (uint256 _getUnstakeTime, , ) = getUnstakeTime(  _unstakingExpiration[staker][i]  );  if (block.timestamp >= _getUnstakeTime) {  //if 21 days + epoch difference has passed, then add the balance and then mint uTokens  _withdrawBalance = _withdrawBalance.add(  _unstakingAmount[staker][i]  );  _unstakingExpiration[staker][i] = 0;  _unstakingAmount[staker][i] = 0;  _withdrawCounters[staker] = _withdrawCounters[staker].add(1);  require(_withdrawBalance > 0, \"LQ21\");  emit WithdrawUnstakeTokens(staker, _withdrawBalance, block.timestamp);  _uTokens.mint(staker, _withdrawBalance);  Recommendation  Limit the number of processed unstaked batches, and possibly add pagination.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"
    },
    {
        "title": "5.3 The _calculatePendingRewards can run out of gas ",
        "body": "  Resolution  Comment from pSTAKE Finance team:  A solution of maintaining cumulative timeshare values in an array and implementing binary search drastically lowers the iterations, has been implemented for calculating rewards for Other Reward Tokens. Also, for moving reward rate, strategically it will only be set max once a month making number of iterations very limited.  Description  The reward rate in STokens can be changed, and the history of these changes are stored in the contract:  code/contracts/STokensV2.sol:L124-L139  function setRewardRate(uint256 rewardRate)  public  virtual  override  returns (bool success)  // range checks for rewardRate. Since rewardRate cannot be more than 100%, the max cap  // is _valueDivisor * 100, which then brings the fees to 100 (percentage)  require(rewardRate <= _valueDivisor.mul(100), \"ST17\");  require(hasRole(DEFAULT_ADMIN_ROLE, _msgSender()), \"ST2\");  _rewardRate.push(rewardRate);  _lastMovingRewardTimestamp.push(block.timestamp);  emit SetRewardRate(rewardRate);  return true;  When the reward is calculated for each user, all changes of the _rewardRate are considered. So there is a for loop that iterates over all changes since the last reward update. If the reward rate was changed many times, the _calculatePendingRewards function could run out of gas.  Recommendation  Provide an option to partially update the reward, so the full update can be split in multiple transactions.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"
    },
    {
        "title": "5.4 Increase test coverage ",
        "body": "  Resolution  Comment from pSTAKE Finance team:  Created deep-dive test for each unique scenarios locally and tested before the code was deployed.  Description  Test coverage is fairly limited. LPStaking tests only cover the happy path. StakeLPCoreV8 has no tests. Many test descriptions are inaccurate.  Examples  Test description inaccuracy examples:  This tests that a Staker can mint new tokens, but does not check to make sure that Stakers are the ONLY group that can mint. https://github.com/ConsenSys/persistence-pstake-audit-2021-08/blob/3821182ca14e0e98ab9fccd47cbe0f1ce39ae54c/code/test/LiquidStakingTest.js#L82  This test only shows that an unauthorized address can t use the stake function to mint tokens. https://github.com/ConsenSys/persistence-pstake-audit-2021-08/blob/3821182ca14e0e98ab9fccd47cbe0f1ce39ae54c/code/test/LiquidStakingTest.js#L99  This test actually tests for the inverse case. https://github.com/ConsenSys/persistence-pstake-audit-2021-08/blob/3821182ca14e0e98ab9fccd47cbe0f1ce39ae54c/code/test/STokensTest.js#L82  Recommendation  Increase test coverage for entire codebase. Add tests for the inherited contracts from OpenZeppelin. Test for edge cases, and multiple expected cases. Ensure that the test description matches the functionality that is actually tested.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"
    },
    {
        "title": "5.5 The calculateRewards should not be callable by the whitelisted contract ",
        "body": "  Resolution  Comment from pSTAKE Finance team:  Have created a require condition in Smart Contract code to disallow whitelisted contracts from calling the function  Description  The calculateRewards function should only be called for non-whitelisted addresses:  code/contracts/STokensV2.sol:L348-L359  function calculateRewards(address to)  public  virtual  override  whenNotPaused  returns (bool success)  require(to == _msgSender(), \"ST5\");  uint256 reward = _calculateRewards(to);  emit TriggeredCalculateRewards(to, reward, block.timestamp);  return true;  For all the whitelisted addresses, the calculateHolderRewards function is called. But if the calculateRewards function is called by the whitelisted address directly, the function will execute, and the rewards will be distributed to the caller instead of the intended recipients.  Recommendation  While this scenario is unlikely to happen, adding the additional check in the calculateRewards is a good option.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"
    },
    {
        "title": "5.6 Presence of testnet code ",
        "body": "  Resolution  Comment from pSTAKE Finance team:  The testnet code has been re-considered as out of scope for audit  Description  Based on the discussions with pStake team and in-line comments, there are a few instances of code and commented code in the code base under audit that are not finalized for mainnet deployment.  Examples  code/contracts/PSTAKE.sol:L25-L37  function initialize(address pauserAddress) public virtual initializer {  __ERC20_init(\"pSTAKE Token\", \"PSTAKE\");  __AccessControl_init();  __Pausable_init();  _setupRole(DEFAULT_ADMIN_ROLE, _msgSender());  _setupRole(PAUSER_ROLE, pauserAddress);  // PSTAKE IS A SIMPLE ERC20 TOKEN HENCE 18 DECIMAL PLACES  _setupDecimals(18);  // pre-allocate some tokens to an admin address which will air drop PSTAKE tokens  // to each of holder contracts. This is only for testnet purpose. in Mainnet, we  // will use a vesting contract to allocate tokens to admin in a certain schedule  _mint(_msgSender(), 5000000000000000000000000);  The initialize function currently mints all the tokens to msg.sender, however the goal for mainnet is to use a vesting contract which is not present in the current code.  Recommendation  It is recommended to fully test the final code before deployment to the mainnet.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"
    },
    {
        "title": "5.7 Re-entrancy from LP token transfers ",
        "body": "  Resolution  Comment from pSTAKE Finance team:  Have implemented the nonReentrant modifier from the ReentrancyGuardUpgradeable OpenZeppelin contract in addition to strictly keeping to Checks-Effects-Interactions pattern throughout relevant areas  Description  The StakeLPCoreV8 contract is designed to stake LP tokens. These LP tokens are not directly controlled or developed by the protocol, so it can t be easily verified that no re-entrancy can happen during token transfers.  Recommendation  During the review, we did not find any specific ways to build the attack using the re-entrancy of LP tokens, but it is still better to have the re-entrancy protection modifiers in functions that use LP tokens transfers.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"
    },
    {
        "title": "5.8 Sanity check on all important variables ",
        "body": "  Resolution  Comment from pSTAKE Finance team:  Post the implementation of new emission logic there have been a rearrangement of some variables, but the rest have been sanity tested and corrected  Description  Most of the functionalities have proper sanity checks when it comes to setting system-wide variables, such as whitelist addresses. However there are a few key setters that lack such sanity checks.  Examples  Sanity check (!= address(0)) on all token contracts.  code/contracts/StakeLPCoreV8.sol:L303-L333  function setUTokensContract(address uAddress) public virtual override {  require(hasRole(DEFAULT_ADMIN_ROLE, _msgSender()), \"LP9\");  _uTokens = IUTokens(uAddress);  emit SetUTokensContract(uAddress);  /**  @dev Set 'contract address', called from constructor  @param sAddress: stoken contract address  Emits a {SetSTokensContract} event with '_contract' set to the stoken contract address.  /  function setSTokensContract(address sAddress) public virtual override {  require(hasRole(DEFAULT_ADMIN_ROLE, _msgSender()), \"LP10\");  _sTokens = ISTokens(sAddress);  emit SetSTokensContract(sAddress);  /**  @dev Set 'contract address', called from constructor  @param pstakeAddress: pStake contract address  Emits a {SetPSTAKEContract} event with '_contract' set to the stoken contract address.  /  function setPSTAKEContract(address pstakeAddress) public virtual override {  require(hasRole(DEFAULT_ADMIN_ROLE, _msgSender()), \"LP11\");  _pstakeTokens = IPSTAKE(pstakeAddress);  emit SetPSTAKEContract(pstakeAddress);  Sanity check on unstakingLockTime to be in the acceptable range (21 hours to 21 days)  code/contracts/LiquidStakingV2.sol:L105-L121  /**  @dev Set 'unstake props', called from admin  @param unstakingLockTime: varies from 21 hours to 21 days  Emits a {SetUnstakeProps} event with 'fee' set to the stake and unstake.  /  function setUnstakingLockTime(uint256 unstakingLockTime)  public  virtual  returns (bool success)  require(hasRole(DEFAULT_ADMIN_ROLE, _msgSender()), \"LQ3\");  _unstakingLockTime = unstakingLockTime;  emit SetUnstakingLockTime(unstakingLockTime);  return true;  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"
    },
    {
        "title": "5.9 Remove unused/commented code",
        "body": "  Description  There are a few snippets of commented code in the code base. It is suggested to remove and clean any unused and commented code in the final code.  Examples  code/contracts/PSTAKE.sol:L50-L73  /* function mint(address to, uint256 tokens) public virtual override returns (bool success) {  require(_msgSender() == _stakeLPCoreContract, \"PS1\");  // minted by STokens contract  _mint(to, tokens);  return true;  } */  /*  @dev Burn utokens for the provided 'address' and 'amount'  @param from: account address, tokens: number of tokens  Emits a {BurnTokens} event with 'from' set to address and 'tokens' set to amount of tokens.  Requirements:  - `amount` cannot be less than zero.  /  /* function burn(address from, uint256 tokens) public virtual override returns (bool success) {  require((tx.origin == from && _msgSender()==_liquidStakingContract) ||  // staking operation  (tx.origin == from && _msgSender() == _wrapperContract), \"UT2\"); // unwrap operation  _burn(from, tokens);  return true;  } */  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"
    },
    {
        "title": "4.1 Initialization flaws    ",
        "body": "  Resolution   This has been fixed in   Description  OpenZeppelin Contracts Upgradeable is to have a  Examples  The __ERC20WrapperGluwacoin_init function is implemented as follows:  code/contracts/ERC20WrapperGluwacoin.sol:L36-L48  function __ERC20WrapperGluwacoin_init(  string memory name,  string memory symbol,  IERC20 token  ) internal initializer {  __Context_init_unchained();  __ERC20_init_unchained(name, symbol);  __ERC20ETHless_init_unchained();  __ERC20Reservable_init_unchained();  __AccessControlEnumerable_init_unchained();  __ERC20Wrapper_init_unchained(token);  __ERC20WrapperGluwacoin_init_unchained();  And the C3 linearization is:  The calls __ERC165_init_unchained(); and __AccessControl_init_unchained(); are missing, and __ERC20Wrapper_init_unchained(token); should move between __ERC20_init_unchained(name, symbol); and __ERC20ETHless_init_unchained();.  Recommendation  Review all *_init functions, add the missing *_init_unchained calls, and fix the order of these calls.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/10/gluwacoin-erc-20-wrapper/"
    },
    {
        "title": "4.2 Flaw in _beforeTokenTransfer call chain and missing tests    ",
        "body": "  Resolution   This has been fixed in   Description  In OpenZeppelin s ERC-20 implementation, the virtual _beforeTokenTransfer function provides a hook that is called before tokens are transferred, minted, or burned. In the Gluwacoin codebase, it is used to check whether the unreserved balance (as opposed to the regular balance, which is checked by the ERC-20 implementation) of the sender is sufficient to allow this transfer or burning.  In ERC20WrapperGluwacoin, ERC20Reservable, and ERC20Wrapper, the _beforeTokenTransfer function is implemented in the following way:  code/contracts/ERC20WrapperGluwacoin.sol:L54-L61  function _beforeTokenTransfer(  address from,  address to,  uint256 amount  ) internal override(ERC20Upgradeable, ERC20Wrapper, ERC20Reservable) {  ERC20Wrapper._beforeTokenTransfer(from, to, amount);  ERC20Reservable._beforeTokenTransfer(from, to, amount);  code/contracts/abstracts/ERC20Reservable.sol:L156-L162  function _beforeTokenTransfer(address from, address to, uint256 amount) internal virtual override (ERC20Upgradeable) {  if (from != address(0)) {  require(_unreservedBalance(from) >= amount, \"ERC20Reservable: transfer amount exceeds unreserved balance\");  super._beforeTokenTransfer(from, to, amount);  code/contracts/abstracts/ERC20Wrapper.sol:L176-L178  function _beforeTokenTransfer(address from, address to, uint256 amount) internal virtual override (ERC20Upgradeable) {  super._beforeTokenTransfer(from, to, amount);  Finally, the C3-linearization of the contracts is:  Moreover, while reviewing the correctness and coverage of the tests is not in scope for this engagement, we happened to notice that there are no tests that check whether the unreserved balance is sufficient for transferring or burning tokens.  Recommendation  ERC20WrapperGluwacoin._beforeTokenTransfer should just call super._beforeTokenTransfer. Moreover, the _beforeTokenTransfer implementation can be removed from ERC20Wrapper.  We would like to stress the importance of careful and comprehensive testing in general and of this functionality in particular, as it is crucial for the system s integrity. We also encourage investigating whether there are more such omissions and an evaluation of the test quality and coverage in general.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/10/gluwacoin-erc-20-wrapper/"
    },
    {
        "title": "4.3 Hard-coded decimals    ",
        "body": "  Resolution   In   Description  The Gluwacoin wrapper token should have the same number of decimals as the wrapped ERC-20. Currently, the number of decimals is hard-coded to 6. This limits flexibility or requires source code changes and recompilation if a token with a different number of decimals is to be wrapped.  code/contracts/ERC20WrapperGluwacoin.sol:L32-L34  function decimals() public pure override returns (uint8) {  return 6;  Recommendation  We recommend supplying the number of decimals as an initialization parameter and storing it in a state variable. That increases gas consumption of the decimals function, but we doubt this view function will be frequently called from a contract, and even if it was, we think the benefits far outweigh the costs. Moreover, we believe the decimals logic (i.e., function decimals and the new state variable) should be implemented in the ERC20Wrapper contract   which holds the basic ERC-20 functionality of the wrapper token   and not in ERC20WrapperGluwacoin, which is the base contract of the entire system.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/10/gluwacoin-erc-20-wrapper/"
    },
    {
        "title": "5.1 Delegated transactions can be executed for multiple accounts    ",
        "body": "  Resolution  Comment from the client: The issue has been solved  Description  The Gateway contract allows users to create meta transactions triggered by the system s backend. To do so, one of the owners of the account should sign the message in the following format:  code/src/gateway/Gateway.sol:L125-L131  address sender = _hashPrimaryTypedData(  _hashTypedData(  nonce,  to,  data  ).recoverAddress(senderSignature);  The message includes a nonce, destination address, and call data. The problem is that this message does not include the account address. So if the sender is the owner of multiple accounts, this meta transaction can be called for multiple accounts.  Recommendation  Add the account field in the signed message or make sure that any address can be the owner of only one account.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"
    },
    {
        "title": "5.2 Removing an owner does not work in PersonalAccountRegistry    ",
        "body": "  Resolution  Comment from the client: The issue has been solved  Description  An owner of a personal account can be added/removed by other owners. When removing the owner, only removedAtBlockNumber value is updated. accounts[account].owners[owner].added remains true:  code/src/personal/PersonalAccountRegistry.sol:L116-L121  accounts[account].owners[owner].removedAtBlockNumber = block.number;  emit AccountOwnerRemoved(  account,  owner  );  But when the account is checked whether this account is the owner, only accounts[account].owners[owner].added is actually checked:  code/src/personal/PersonalAccountRegistry.sol:L255-L286  function _verifySender(  address account  private  returns (address)  address sender = _getContextSender();  if (!accounts[account].owners[sender].added) {  require(  accounts[account].salt == 0  );  bytes32 salt = keccak256(  abi.encodePacked(sender)  );  require(  account == _computeAccountAddress(salt)  );  accounts[account].salt = salt;  accounts[account].owners[sender].added = true;  emit AccountOwnerAdded(  account,  sender  );  return sender;  So the owner will never be removed, because accounts[account].owners[owner].added will always be `true.  Recommendation  Properly check if the account is still the owner in the _verifySender  function.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"
    },
    {
        "title": "5.3 The withdrawal mechanism is overcomplicated    ",
        "body": "  Resolution  Comment from the client:  The withdrawal mechanism has been refactored. In current version user can withdraw funds from the deposit account in two ways:  with guardian signature - withdrawDeposit  using  deposit exit  process  Description  To withdraw the funds, anyone who has the account in PaymentRegistry should call the withdrawDeposit function and go through the withdrawal process. After the lockdown period (30 days), the user will withdraw all the funds from the account.  code/src/payment/PaymentRegistry.sol:L160-L210  function withdrawDeposit(  address token  external  address owner = _getContextAccount();  uint256 lockedUntil = deposits[owner].withdrawalLockedUntil[token];  /* solhint-disable not-rely-on-time */  if (lockedUntil != 0 && lockedUntil <= now) {  deposits[owner].withdrawalLockedUntil[token] = 0;  address depositAccount = deposits[owner].account;  uint256 depositValue;  if (token == address(0)) {  depositValue = depositAccount.balance;  } else {  depositValue = ERC20Token(token).balanceOf(depositAccount);  _transferFromDeposit(  depositAccount,  owner,  token,  depositValue  );  emit DepositWithdrawn(  depositAccount,  owner,  token,  depositValue  );  } else {  _deployDepositAccount(owner);  lockedUntil = now.add(depositWithdrawalLockPeriod);  deposits[owner].withdrawalLockedUntil[token] = lockedUntil;  emit DepositWithdrawalRequested(  deposits[owner].account,  owner,  token,  lockedUntil  );  /* solhint-enable not-rely-on-time */  During that period, everyone who has a channel with the user is forced to commit their channels or lose money from that channel. When doing so, every user will reset the initial lockdown period and the withdrawer should start the process again.  code/src/payment/PaymentRegistry.sol:L479-L480  if (deposits[sender].withdrawalLockedUntil[token] > 0) {  deposits[sender].withdrawalLockedUntil[token] = 0;  There is no way for the withdrawer to close the channel by himself. If the withdrawer has N channels, it s theoretically possible to wait for up to N*(30 days) period and make N+2 transactions.  Recommendation  There may be some minor recommendations on how to improve that without major changes:  When committing a payment channel, do not reset the lockdown period to zero. Two better option would be either not change it at all or extend to now + depositWithdrawalLockPeriod  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"
    },
    {
        "title": "5.4 A malicious guardian can steal funds   ",
        "body": "  Resolution  Comment from the client: The etherspot payment system is semi-trusted by design.  Description  A guardian is signing every message that should be submitted as a payment channel update. A guardian s two main things to verify are: blockNumber and the fact that the sender has enough funds.  There are two main attack vectors for the malicious guardian:  It s possible to conspire with the previous owner of the account and submit the old blockNumber. This allows them to drain the account.  A guardian can also conspire with the sender and send more funds to multiple channels than funds in the account.  Recommendation  Reduce the system s reliance on single points of failure like the guardians.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"
    },
    {
        "title": "5.5 Upgrade solidity version    ",
        "body": "  Resolution  Solidity version has been upgraded to 0.6.12  Description  The current minimal solidity version is 0.6.0. But some parts of the code use features from the later versions of solidity, like the high-level version of CREATE2 to create accounts.  Recommendation  Upgrade solidity version to the latest stable (0.6.12).  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"
    },
    {
        "title": "5.6 The lockdown period shouldn t be extended when called multiple times    ",
        "body": "  Resolution  Comment from the client: The issue has been solved  Description  In order to withdraw a deposit from the PaymentRegistry, the account owner should call the withdrawDeposit function and wait for depositWithdrawalLockPeriod (30 days) before actually transferring all the tokens from the account.  The issue is that if the withdrawer accidentally calls it for the second time before these 30 days pass, the waiting period gets extended for 30 days again.  code/src/payment/PaymentRegistry.sol:L170-L199  if (lockedUntil != 0 && lockedUntil <= now) {  deposits[owner].withdrawalLockedUntil[token] = 0;  address depositAccount = deposits[owner].account;  uint256 depositValue;  if (token == address(0)) {  depositValue = depositAccount.balance;  } else {  depositValue = ERC20Token(token).balanceOf(depositAccount);  _transferFromDeposit(  depositAccount,  owner,  token,  depositValue  );  emit DepositWithdrawn(  depositAccount,  owner,  token,  depositValue  );  } else {  _deployDepositAccount(owner);  lockedUntil = now.add(depositWithdrawalLockPeriod);  Recommendation  Only extend the waiting period when a withdrawal is requested for the first time.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"
    },
    {
        "title": "5.7 Missing documentation    ",
        "body": "  Resolution  Comment from the client: Code has been documented - We will work on white paper, graphs later  Description  The code base as is, is missing proper documentations to understand the code work flow and logic. The most important pieces are high-level diagrams, user work flows, and updated white paper.  It is important for readability and maintainability of the codebase to add in-line documentations. The Pillar code base under the audit lacks any type of inline documentation and it makes the code reviewer s job much harder. We highly recommend to provide inline documentation using Solidity s natspec format, as this will be easier to maintain.  As an example PaymentRegistry.sol without the documentation is really hard to read and understand. There are many assumptions or off-chain dependencies and it s impossible to understand the flows simply by reading the solidity code.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"
    },
    {
        "title": "5.8 Gateway can call any contract   ",
        "body": "  Resolution  Comment from the client: That s right Gateway can call any contract, we want to keep it open for any external contract.  Description  The Gateway contract is used as a gateway for meta transactions and batched transactions. It can currently call any contract, while is only intended to call specific contracts in the system that implemented GatewayRecipient interface:  code/src/gateway/Gateway.sol:L280-L292  for (uint256 i = 0; i < data.length; i++) {  require(  to[i] != address(0)  );  // solhint-disable-next-line avoid-low-level-calls  (succeeded,) = to[i].call(abi.encodePacked(data[i], account, sender));  require(  succeeded  );  There are currently no restrictions for to value.  Recommendation  Make sure, only intended contracts can be called by the Gateway : PersonalAccountRegistry, PaymentRegistry, ENSController.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"
    },
    {
        "title": "5.9 Remove unused code    ",
        "body": "  Resolution  Comment from the client: Unused code has been removed  Description  In account/AccountController.sol when deploying an account, the function _deployAccount() gets an extra input value which is always 0 and not set in any other method.  Examples  code/src/common/account/AccountController.sol:L24-L38  return _deployAccount(  salt,  );  function _deployAccount(  bytes32 salt,  uint256 value  internal  returns (address)  return address(new Account{salt: salt, value: value}());  Recommendation  It is recommended to remove this value as there are no use cases for it at the moment, however if it is planned to be used in the future, it should be well documented in the code to prevent confusion.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"
    },
    {
        "title": "5.10 Using ENS subdomains introduces possible privacy issues   ",
        "body": "  Resolution   Comment from the client: This is a known issue - we also added   Description  Using ENS names by default introduces a privacy issue for users. The current implementation leaks all user addresses and their associated username. This is possibly known issue, however it is worth to mention as part of this audit.  Examples  Here s a sample of already registered addresses on mainnet fetched using Legions:  sbeta> ens listSubdomains name=\"pillar.eth\"  > Subdomains for 'pillar.eth'  > NameHash: '0x5bb02333b1f96385ba28fd63408843cfeee095b32196b718786a56e491e33387'  mrsirio  0x6d2ce500f82e20cdeb733ec0530360d2e761f44d  coinstacker  0x60cc065f860682fb899a385b9af66fe82b412b29  dadang  0x904e88eb2602d947ded5c0c5b84c32109255a5f2  ramaido  0x1ee590464e00780ab1c620de41545e74c0731521  tongkol  0x3cbbf43f7a449d54a71bf97c779186f183d1e9eb  kell  0x3d48c65ddfb5bed5980b40974416b55eceed6fab  sipa  0x944972562ea6a07ee0f77bf6ce89559214347774  joyboy  0x4660b09e45930d5ffaedf36bad4a37705303970b  ryanc  0x0c58b9d8b6bdfcd7fb33ab1ecc6b0db4fa94a7b8  hammad  0xe94bb8ea91bfa791cf632e2353cabb87a93713d6  nicolas  0x12ce0a744ccf8958b6859aff1e85bca797e4f742  timmy2shoes  0xafad99c454d97b0130da64179e1a5a7b516ae225  sergvind  0xd5164fe7b9b1d44dd4eb35ef312ada6bce2878ff  0x7384e49fdf540de561f0dc810cc9ad87e909afbe  0x2e496c59c5a0f525d82cf0402851f361ac879c63  ",
        "labels": [
            "Consensys",
            "Minor",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"
    },
    {
        "title": "5.1 Random task execution    ",
        "body": "  Resolution   Fixed in   DecenterApps/defisaver-v3-contracts@478e9cd by adding  Description  In a scenario where user takes a flash loan, _parseFLAndExecute() gives the flash loan wrapper contract (FLAaveV2, FLDyDx) the permission to execute functions on behalf of the user s DSProxy. This execution permission is revoked only after the entire recipe execution is finished, which means that in case that any of the external calls along the recipe execution is malicious, it might call executeAction() back and inject any task it wishes (e.g. take user s funds out, drain approved tokens, etc)  Examples  code/contracts/actions/flashloan/FLAaveV2.sol:L105-L136  function executeOperation(  address[] memory _assets,  uint256[] memory _amounts,  uint256[] memory _fees,  address _initiator,  bytes memory _params  ) public returns (bool) {  require(msg.sender == AAVE_LENDING_POOL, ERR_ONLY_AAVE_CALLER);  require(_initiator == address(this), ERR_SAME_CALLER);  (Task memory currTask, address proxy) = abi.decode(_params, (Task, address));  // Send FL amounts to user proxy  for (uint256 i = 0; i < _assets.length; ++i) {  _assets[i].withdrawTokens(proxy, _amounts[i]);  address payable taskExecutor = payable(registry.getAddr(TASK_EXECUTOR_ID));  // call Action execution  IDSProxy(proxy).execute{value: address(this).balance}(  taskExecutor,  abi.encodeWithSelector(CALLBACK_SELECTOR, currTask, bytes32(_amounts[0] + _fees[0]))  );  // return FL  for (uint256 i = 0; i < _assets.length; i++) {  _assets[i].approveToken(address(AAVE_LENDING_POOL), _amounts[i] + _fees[i]);  return true;  Recommendation  A reentrancy guard (mutex) that covers the entire content of FLAaveV2.executeOperation/FLDyDx.callFunction should be used to prevent such attack.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"
    },
    {
        "title": "5.2 Tokens with more than 18 decimal points will cause issues    ",
        "body": "  Resolution   Fixed in   DecenterApps/defisaver-v3-contracts@de22007 by using  Description  It is assumed that the maximum number of decimals for each token is 18. However uncommon, but it is possible to have tokens with more than 18 decimals, as an Example YAMv2 has 24 decimals. This can result in broken code flow and unpredictable outcomes (e.g. an underflow will result with really high rates).  Examples  contracts/exchangeV3/wrappersV3/KyberWrapperV3.sol  function getSellRate(address _srcAddr, address _destAddr, uint _srcAmount, bytes memory) public override view returns (uint rate) {  (rate, ) = KyberNetworkProxyInterface(KYBER_INTERFACE)  .getExpectedRate(IERC20(_srcAddr), IERC20(_destAddr), _srcAmount);  // multiply with decimal difference in src token  rate = rate * (10**(18 - getDecimals(_srcAddr)));  // divide with decimal difference in dest token  rate = rate / (10**(18 - getDecimals(_destAddr)));  code/contracts/views/AaveView.sol : also used in getLoanData()  Recommendation  Make sure the code won t fail in case the token s decimals is more than 18.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"
    },
    {
        "title": "5.3 Error codes of Compound s Comptroller.enterMarket, Comptroller.exitMarket are not checked    ",
        "body": "  Resolution   Fixed in   DecenterApps/defisaver-v3-contracts@7075e49 by reverting in the case the return value is non zero.  Description  Compound s enterMarket/exitMarket functions return an error code instead of reverting in case of failure. DeFi Saver smart contracts never check for the error codes returned from Compound smart contracts, although the code flow might revert due to unavailability of the CTokens, however early on checks for Compound errors are suggested.  Examples  code/contracts/actions/compound/helpers/CompHelper.sol:L26-L37  function enterMarket(address _cTokenAddr) public {  address[] memory markets = new address[](1);  markets[0] = _cTokenAddr;  IComptroller(COMPTROLLER_ADDR).enterMarkets(markets);  /// @notice Exits the Compound market  /// @param _cTokenAddr CToken address of the token  function exitMarket(address _cTokenAddr) public {  IComptroller(COMPTROLLER_ADDR).exitMarket(_cTokenAddr);  Recommendation  Caller contract should revert in case the error code is not 0.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"
    },
    {
        "title": "5.4 Reversed order of parameters in allowance function call    ",
        "body": "  Resolution   Fixed in   DecenterApps/defisaver-v3-contracts@8b5657b by swapping the order of function call parameters.  Description  When trying to pull the maximum amount of tokens from an approver to the allowed spender, the parameters that are used for the allowance function call are not in the same order that is used later in the call to safeTransferFrom.  Examples  code/contracts/utils/TokenUtils.sol:L26-L44  function pullTokens(  address _token,  address _from,  uint256 _amount  ) internal returns (uint256) {  // handle max uint amount  if (_amount == type(uint256).max) {  uint256 allowance = IERC20(_token).allowance(address(this), _from);  uint256 balance = getBalance(_token, _from);  _amount = (balance > allowance) ? allowance : balance;  if (_from != address(0) && _from != address(this) && _token != ETH_ADDR && _amount != 0) {  IERC20(_token).safeTransferFrom(_from, address(this), _amount);  return _amount;  Recommendation  Reverse the order of parameters in allowance function call to fit the order that is in the safeTransferFrom function call.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"
    },
    {
        "title": "5.5 Full test suite is recommended   Pending",
        "body": "  Description  The test suite at this stage is not complete and many of the tests fail to execute. For complicated systems such as DeFi Saver, which uses many different modules and interacts with different DeFi protocols, it is crucial to have a full test coverage that includes the edge cases and failed scenarios. Especially this helps with safer future development and upgrading each modules.  As we ve seen in some smart contract incidents, a complete test suite can prevent issues that might be hard to find with manual reviews.  Some issues such as issue 5.4 could be caught by a full coverage test suite.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"
    },
    {
        "title": "5.6 Kyber getRates code is unclear ",
        "body": "  Description  In contracts/exchangeV3/wrappersV3/KyberWrapperV3.sol the function names don t reflect their true functionalities, and the code uses some undocumented assumptions.  Examples  getSellRate can be converted into one function to get the rates, which then for buy or sell can swap input and output tokens  getBuyRate uses a 3% slippage that is not documented.  function getSellRate(address _srcAddr, address _destAddr, uint _srcAmount, bytes memory) public override view returns (uint rate) {  (rate, ) = KyberNetworkProxyInterface(KYBER_INTERFACE)  .getExpectedRate(IERC20(_srcAddr), IERC20(_destAddr), _srcAmount);  // multiply with decimal difference in src token  rate = rate * (10**(18 - getDecimals(_srcAddr)));  // divide with decimal difference in dest token  rate = rate / (10**(18 - getDecimals(_destAddr)));  /// @notice Return a rate for which we can buy an amount of tokens  /// @param _srcAddr From token  /// @param _destAddr To token  /// @param _destAmount To amount  /// @return rate Rate  function getBuyRate(address _srcAddr, address _destAddr, uint _destAmount, bytes memory _additionalData) public override view returns (uint rate) {  uint256 srcRate = getSellRate(_destAddr, _srcAddr, _destAmount, _additionalData);  uint256 srcAmount = wmul(srcRate, _destAmount);  rate = getSellRate(_srcAddr, _destAddr, srcAmount, _additionalData);  // increase rate by 3% too account for inaccuracy between sell/buy conversion  rate = rate + (rate / 30);  Recommendation  Refactoring the code to separate getting rate functionality with getSellRate and getBuyRate. Explicitly document any assumptions in the code ( slippage, etc)  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"
    },
    {
        "title": "5.7 Missing check in IOffchainWrapper.takeOrder implementation ",
        "body": "  Description  IOffchainWrapper.takeOrder wraps an external call that is supposed to perform a token swap. As for the two different implementations ZeroxWrapper and ScpWrapper this function validates that the destination token balance after the swap is greater than the value before. However, it is not sufficient, and the user-provided minimum amount for swap should be taken in consideration as well. Besides, the external contract should not be trusted upon, and SafeMath should be used for the subtraction operation.  Examples  code/contracts/exchangeV3/offchainWrappersV3/ZeroxWrapper.sol:L42-L50  uint256 tokensBefore = _exData.destAddr.getBalance(address(this));  (success, ) = _exData.offchainData.exchangeAddr.call{value: _exData.offchainData.protocolFee}(_exData.offchainData.callData);  uint256 tokensSwaped = 0;  if (success) {  // get the current balance of the swaped tokens  tokensSwaped = _exData.destAddr.getBalance(address(this)) - tokensBefore;  require(tokensSwaped > 0, ERR_TOKENS_SWAPED_ZERO);  code/contracts/exchangeV3/offchainWrappersV3/ScpWrapper.sol:L43-L51  uint256 tokensBefore = _exData.destAddr.getBalance(address(this));  (success, ) = _exData.offchainData.exchangeAddr.call{value: _exData.offchainData.protocolFee}(_exData.offchainData.callData);  uint256 tokensSwaped = 0;  if (success) {  // get the current balance of the swaped tokens  tokensSwaped = _exData.destAddr.getBalance(address(this)) - tokensBefore;  require(tokensSwaped > 0, ERR_TOKENS_SWAPED_ZERO);  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"
    },
    {
        "title": "5.8 Unused code present in the codebase ",
        "body": "  Resolution   Some of the unused code were removed in   DecenterApps/defisaver-v3-contracts@61b0c09.  Description  There are a few instances of unused code (dead code) in the code base, that is suggested to be removed .  Examples  DFSExchange.sol contract is not used  /contracts/utils/ZrxAllowlist.sol these functions are not used in the codebase:  nonPayableAddrs mapping addNonPayableAddr() removeNonPayableAddr() isNonPayableAddr()  DSProxy.execute(bytes memory _code, bytes memory _data) is not intended to used.  There might be more instances of unused code in the codebase.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"
    },
    {
        "title": "5.9 Return values not used for DFSExchangeCore.onChainSwap ",
        "body": "  Description  Return values from DFSExchangeCore.onChainSwap are not used.  Examples  code/contracts/exchangeV3/DFSExchangeCore.sol:L37-L73  function _sell(ExchangeData memory exData) internal returns (address, uint256) {  uint256 amountWithoutFee = exData.srcAmount;  address wrapper = exData.offchainData.wrapper;  bool offChainSwapSuccess;  uint256 destBalanceBefore = exData.destAddr.getBalance(address(this));  // Takes DFS exchange fee  exData.srcAmount -= getFee(  exData.srcAmount,  exData.user,  exData.srcAddr,  exData.dfsFeeDivider  );  // Try 0x first and then fallback on specific wrapper  if (exData.offchainData.price > 0) {  (offChainSwapSuccess, ) = offChainSwap(exData, ExchangeActionType.SELL);  // fallback to desired wrapper if 0x failed  if (!offChainSwapSuccess) {  onChainSwap(exData, ExchangeActionType.SELL);  wrapper = exData.wrapper;  uint256 destBalanceAfter = exData.destAddr.getBalance(address(this));  uint256 amountBought = sub(destBalanceAfter, destBalanceBefore);  // check slippage  require(amountBought >= wmul(exData.minPrice, exData.srcAmount), ERR_SLIPPAGE_HIT);  // revert back exData changes to keep it consistent  exData.srcAmount = amountWithoutFee;  return (wrapper, amountBought);  code/contracts/exchangeV3/DFSExchangeCore.sol:L79-L117  function _buy(ExchangeData memory exData) internal returns (address, uint256) {  require(exData.destAmount != 0, ERR_DEST_AMOUNT_MISSING);  uint256 amountWithoutFee = exData.srcAmount;  address wrapper = exData.offchainData.wrapper;  bool offChainSwapSuccess;  uint256 destBalanceBefore = exData.destAddr.getBalance(address(this));  // Takes DFS exchange fee  exData.srcAmount -= getFee(  exData.srcAmount,  exData.user,  exData.srcAddr,  exData.dfsFeeDivider  );  // Try 0x first and then fallback on specific wrapper  if (exData.offchainData.price > 0) {  (offChainSwapSuccess, ) = offChainSwap(exData, ExchangeActionType.BUY);  // fallback to desired wrapper if 0x failed  if (!offChainSwapSuccess) {  onChainSwap(exData, ExchangeActionType.BUY);  wrapper = exData.wrapper;  uint256 destBalanceAfter = exData.destAddr.getBalance(address(this));  uint256 amountBought = sub(destBalanceAfter, destBalanceBefore);  // check slippage  require(amountBought >= exData.destAmount, ERR_SLIPPAGE_HIT);  // revert back exData changes to keep it consistent  exData.srcAmount = amountWithoutFee;  return (wrapper, amountBought);  Recommendation  The return value can be used for verification of the swap or used in the event data.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"
    },
    {
        "title": "5.10 Return value is not used for TokenUtils.withdrawTokens    ",
        "body": "  Resolution   Fixed in   DecenterApps/defisaver-v3-contracts@37dabff by storing the return value locally and use its value throughout the execution.  Description  The return value of TokenUtils.withdrawTokens which represents the actual amount of tokens that were transferred is never used throughout the repository. This might cause discrepancy in the case where the original value of _amount was type(uint256).max.  Examples  code/contracts/actions/aave/AaveBorrow.sol:L70-L97  function _borrow(  address _market,  address _tokenAddr,  uint256 _amount,  uint256 _rateMode,  address _to,  address _onBehalf  ) internal returns (uint256) {  ILendingPoolV2 lendingPool = getLendingPool(_market);  // defaults to onBehalf of proxy  if (_onBehalf == address(0)) {  _onBehalf = address(this);  lendingPool.borrow(_tokenAddr, _amount, _rateMode, AAVE_REFERRAL_CODE, _onBehalf);  _tokenAddr.withdrawTokens(_to, _amount);  logger.Log(  address(this),  msg.sender,  \"AaveBorrow\",  abi.encode(_market, _tokenAddr, _amount, _rateMode, _to, _onBehalf)  );  return _amount;  code/contracts/utils/TokenUtils.sol:L46-L53  function withdrawTokens(  address _token,  address _to,  uint256 _amount  ) internal returns (uint256) {  if (_amount == type(uint256).max) {  _amount = getBalance(_token, address(this));  Recommendation  The return value can be used to validate the withdrawal or used in the event emitted.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"
    },
    {
        "title": "5.11 Missing access control for DefiSaverLogger.Log",
        "body": "  Description  DefiSaverLogger is used as a logging aggregator within the entire dapp, but anyone can create logs.  Examples  code/contracts/utils/DefisaverLogger.sol:L14-L21  function Log(  address _contract,  address _caller,  string memory _logName,  bytes memory _data  ) public {  emit LogEvent(_contract, _caller, _logName, _data);  6 Recommendations  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"
    },
    {
        "title": "6.1 Use a single file for all system-wide constants",
        "body": "  Description  There are many addresses and constants using in the system. It is suggested to put the most used ones in one file (e.g. constants.sol and use inheritance to access these values. This will help with the readability and easier maintenance for future changes. As some of these hardcoded values are admin addresses, this also helps with any possible incident response.  Examples  Logger:  DFSRegistry  TaskExecutor  ActionBase  DefisaverLogger public constant logger = DefisaverLogger(  0x5c55B921f590a89C1Ebe84dF170E655a82b62126  );  Admin Vault:  AdminAuth  AdminVault public constant adminVault = AdminVault(0xCCf3d848e08b94478Ed8f46fFead3008faF581fD);  REGISTRY_ADDR  SubscriptionProxy  StrategyExecutor  TaskExecutor  ActionBase  address public constant REGISTRY_ADDR = 0xB0e1682D17A96E8551191c089673346dF7e1D467;  Any other constant in the system also can be moved to this contract.  Recommendation  Use constants.sol and import this file in the contracts that require access to these values. This is just a recommendation, as discussed with the team, on some use cases this might result in higher gas usage on deployment.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"
    },
    {
        "title": "6.2 Code quality & Styling",
        "body": "  Description  Here are some examples that the code style does not follow the best practices:  Examples  Public/external function names should not be prefixed with _  code/contracts/core/TaskExecutor.sol:L56  function _executeActionsFromFL(Task memory _currTask, bytes32 _flAmount) public payable {  Function parameters are being overriden  code/contracts/exchangeV3/DFSExchange.sol:L24-L37  function sell(ExchangeData memory exData, address payable _user) public payable   {  exData.dfsFeeDivider = SERVICE_FEE;  exData.user = _user;  // Perform the exchange  (address wrapper, uint destAmount) = _sell(exData);  // send back any leftover ether or tokens  sendLeftover(exData.srcAddr, exData.destAddr, _user);  // log the event  logger.Log(address(this), msg.sender, \"ExchangeSell\", abi.encode(wrapper, exData.srcAddr, exData.destAddr, exData.srcAmount, destAmount));  MAX_SERVICE_FEE should be MIN_SERVICE_FEE  code/contracts/utils/Discount.sol:L28-L33  function setServiceFee(address _user, uint256 _fee) public {  require(msg.sender == owner, \"Only owner\");  require(_fee >= MAX_SERVICE_FEE || _fee == 0, \"Wrong fee value\");  serviceFees[_user] = CustomServiceFee({active: true, amount: _fee});  Functions with a get prefix should not modify state  code/contracts/exchangeV3/DFSExchangeCore.sol:L182-L206  function getFee(  uint256 _amount,  address _user,  address _token,  uint256 _dfsFeeDivider  ) internal returns (uint256 feeAmount) {  if (_dfsFeeDivider != 0 && Discount(DISCOUNT_ADDRESS).isCustomFeeSet(_user)) {  _dfsFeeDivider = Discount(DISCOUNT_ADDRESS).getCustomServiceFee(_user);  if (_dfsFeeDivider == 0) {  feeAmount = 0;  } else {  feeAmount = _amount / _dfsFeeDivider;  // fee can't go over 10% of the whole amount  if (feeAmount > (_amount / 10)) {  feeAmount = _amount / 10;  address walletAddr = feeRecipient.getFeeAddr();  _token.withdrawTokens(walletAddr, feeAmount);  Protocol fee value should be validated against msg.value and not against contract s balance  code/contracts/exchangeV3/offchainWrappersV3/ZeroxWrapper.sol:L25-L31  function takeOrder(  ExchangeData memory _exData,  ExchangeActionType _type  ) override public payable returns (bool success, uint256) {  // check that contract have enough balance for exchange and protocol fee  require(_exData.srcAddr.getBalance(address(this)) >= _exData.srcAmount, ERR_SRC_AMOUNT);  require(TokenUtils.ETH_ADDR.getBalance(address(this)) >= _exData.offchainData.protocolFee, ERR_PROTOCOL_FEE);  Remove deprecation warning (originated in OpenZeppelin s implementation) in comment, as the issue has been solved  code/contracts/utils/SafeERC20.sol:L33-L44  /**  @dev Deprecated. This function has issues similar to the ones found in  {ERC20-approve}, and its usage is discouraged.  /  function safeApprove(  IERC20 token,  address spender,  uint256 value  ) internal {  _callOptionalReturn(token, abi.encodeWithSelector(token.approve.selector, spender, 0));  _callOptionalReturn(token, abi.encodeWithSelector(token.approve.selector, spender, value));  Typo RECIPIE_FEE instead of RECIPE_FEE  code/contracts/actions/exchange/DfsSell.sol:L15  uint internal constant RECIPIE_FEE = 400;  Code duplication : sendLeftOver is identical both in UniswapWrapperV3 and in KyberWrapperV3, and thus can be shared in a base class.  code/contracts/exchangeV3/wrappersV3/KyberWrapperV3.sol:L127-L133  function sendLeftOver(address _srcAddr) internal {  msg.sender.transfer(address(this).balance);  if (_srcAddr != KYBER_ETH_ADDRESS) {  IERC20(_srcAddr).safeTransfer(msg.sender, IERC20(_srcAddr).balanceOf(address(this)));  Code duplication : sliceUint function is identical both in DFSExchangeHelper and in DFSPrices  DFSPricesV3.getBestPrice, DFSPricesV3.getExpectedRate should be view functions  Fix the code comments from User borrows tokens to to User borrows tokens from  code/contracts/actions/aave/AaveBorrow.sol:L63-L77  /// @notice User borrows tokens to the Aave protocol  /// @param _market Address provider for specific market  /// @param _tokenAddr The address of the token to be borrowed  /// @param _amount Amount of tokens to be borrowed  /// @param _rateMode Send 1 for stable rate and 2 for variable  /// @param _to The address we are sending the borrowed tokens to  /// @param _onBehalf From what user we are borrow the tokens, defaults to proxy  function _borrow(  address _market,  address _tokenAddr,  uint256 _amount,  uint256 _rateMode,  address _to,  address _onBehalf  ) internal returns (uint256) {  code/contracts/actions/compound/CompBorrow.sol:L51-L59  /// @notice User borrows tokens to the Compound protocol  /// @param _cTokenAddr Address of the cToken we are borrowing  /// @param _amount Amount of tokens to be borrowed  /// @param _to The address we are sending the borrowed tokens to  function _borrow(  address _cTokenAddr,  uint256 _amount,  address _to  ) internal returns (uint256) {  IExchangeV3.sell, IExchangeV3.buy should not be payable  TaskExecutor._executeAction should not forward contract s balance within the IDSProxy.execute call, as the funds are being sent to the same contract.  code/contracts/core/TaskExecutor.sol:L90-L105  function _executeAction(  Task memory _currTask,  uint256 _index,  bytes32[] memory _returnValues  ) internal returns (bytes32 response) {  response = IDSProxy(address(this)).execute{value: address(this).balance}(  registry.getAddr(_currTask.actionIds[_index]),  abi.encodeWithSignature(  \"executeAction(bytes[],bytes[],uint8[],bytes32[])\",  _currTask.callData[_index],  _currTask.subData[_index],  _currTask.paramMapping[_index],  _returnValues  );  Unsafe arithmetic operations  code/contracts/actions/compound/CompClaim.sol:L73  uint256 compClaimed = compBalanceAfter - compBalanceBefore;  code/contracts/actions/compound/CompWithdraw.sol:L84  _amount = tokenBalanceAfter - tokenBalanceBefore;  code/contracts/actions/uniswap/UniSupply.sol:L82-L83  _uniData.tokenA.withdrawTokens(_uniData.to, (_uniData.amountADesired - amountA));  _uniData.tokenB.withdrawTokens(_uniData.to, (_uniData.amountBDesired - amountB));  code/contracts/actions/flashloan/FLAaveV2.sol:L125-L133  IDSProxy(proxy).execute{value: address(this).balance}(  taskExecutor,  abi.encodeWithSelector(CALLBACK_SELECTOR, currTask, bytes32(_amounts[0] + _fees[0]))  );  // return FL  for (uint256 i = 0; i < _assets.length; i++) {  _assets[i].approveToken(address(AAVE_LENDING_POOL), _amounts[i] + _fees[i]);  code/contracts/exchangeV3/DFSExchangeCore.sol:L45  exData.srcAmount -= getFee(  code/contracts/exchangeV3/offchainWrappersV3/ZeroxWrapper.sol:L48  tokensSwaped = _exData.destAddr.getBalance(address(this)) - tokensBefore;  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"
    },
    {
        "title": "6.3 Gas optimization",
        "body": "  Description  Use address(this) instead of external call for registry when possible.  Examples  code/contracts/actions/flashloan/FLAaveV2.sol:L82-L102  function _flAaveV2(FLAaveV2Data memory _flData, bytes memory _params) internal returns (uint) {  ILendingPoolV2(AAVE_LENDING_POOL).flashLoan(  payable(registry.getAddr(FL_AAVE_V2_ID)),  _flData.tokens,  _flData.amounts,  _flData.modes,  _flData.onBehalfOf,  _params,  AAVE_REFERRAL_CODE  );  logger.Log(  address(this),  msg.sender,  \"FLAaveV2\",  abi.encode(_flData.tokens, _flData.amounts, _flData.modes, _flData.onBehalfOf)  );  return _flData.amounts[0];  code/contracts/actions/flashloan/dydx/FLDyDx.sol:L76-L107  function _flDyDx(  uint256 _amount,  address _token,  bytes memory _data  ) internal returns (uint256) {  address payable receiver = payable(registry.getAddr(FL_DYDX_ID));  ISoloMargin solo = ISoloMargin(SOLO_MARGIN_ADDRESS);  // Get marketId from token address  uint256 marketId = _getMarketIdFromTokenAddress(SOLO_MARGIN_ADDRESS, _token);  uint256 repayAmount = _getRepaymentAmountInternal(_amount);  IERC20(_token).safeApprove(SOLO_MARGIN_ADDRESS, repayAmount);  Actions.ActionArgs[] memory operations = new Actions.ActionArgs[](3);  operations[0] = _getWithdrawAction(marketId, _amount, receiver);  operations[1] = _getCallAction(_data, receiver);  operations[2] = _getDepositAction(marketId, repayAmount, address(this));  Account.Info[] memory accountInfos = new Account.Info[](1);  accountInfos[0] = _getAccountInfo();  solo.operate(accountInfos, operations);  logger.Log(address(this), msg.sender, \"FLDyDx\", abi.encode(_amount, _token));  return _amount;  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"
    },
    {
        "title": "5.1 Every node gets a full validator s bounty    ",
        "body": "  Resolution  This issue is addressed in Bug/skale 3273 formula fix 435 and SKALE-3273 Fix BountyV2 populating error 438.  The main change is related to how bounties are calculated for each validator. Below are a few notes on these pull requests:  nodesByValidator mapping is no longer used in the codebase and the non-zero values are deleted when calculateBounty() is called for a specific validator. The mapping is kept in the code for compatible storage layout in upgradable proxies.  Some functions such as populate() was developed for the transition to the upgraded contracts (rewrite _effectiveDelegatedSum values based on the new calculation formula). This function is not part of this review and will be removed in the future updates.  Unlike the old architecture, nodesByValidator[validatorId] is no longer used within the system to calculate _effectiveDelegatedSum and bounties. This is replaced by using overall staked amount and duration.  If a validator does not claim their bounty during a month, it is considered as a misbehave and her bounty goes to the bounty pool for the next month.  Description  To get the bounty, every node calls the getBounty function of the SkaleManager contract. This function can be called once per month. The size of the bounty is defined in the BountyV2 contract in the _calculateMaximumBountyAmount function:  code/contracts/BountyV2.sol:L213-L221  return epochPoolSize  .add(_bountyWasPaidInCurrentEpoch)  .mul(  delegationController.getAndUpdateEffectiveDelegatedToValidator(  nodes.getValidatorId(nodeIndex),  currentMonth  .div(effectiveDelegatedSum);  The problem is that this amount actually represents the amount that should be paid to the validator of that node. But each node will get this amount. Additionally, the amount of validator s bounty should also correspond to the number of active nodes, while this formula only uses the amount of delegated funds.  Recommendation  Every node should get only their parts of the bounty.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"
    },
    {
        "title": "5.2 A node exit prevents some other nodes from exiting for some period   Pending",
        "body": "  Resolution  Skale team s comment:  Description  When a node wants to exit, the nodeExit function should be called as many times, as there are schains in the node. Each time one schain is getting removed from the node. During every call, all the active schains are getting frozen for 12 hours.  code/contracts/NodeRotation.sol:L84-L105  function freezeSchains(uint nodeIndex) external allow(\"SkaleManager\") {  SchainsInternal schainsInternal = SchainsInternal(contractManager.getContract(\"SchainsInternal\"));  bytes32[] memory schains = schainsInternal.getActiveSchains(nodeIndex);  for (uint i = 0; i < schains.length; i++) {  Rotation memory rotation = rotations[schains[i]];  if (rotation.nodeIndex == nodeIndex && now < rotation.freezeUntil) {  continue;  string memory schainName = schainsInternal.getSchainName(schains[i]);  string memory revertMessage = \"Node cannot rotate on Schain \";  revertMessage = revertMessage.strConcat(schainName);  revertMessage = revertMessage.strConcat(\", occupied by Node \");  revertMessage = revertMessage.strConcat(rotation.nodeIndex.uint2str());  string memory dkgRevert = \"DKG process did not finish on schain \";  ISkaleDKG skaleDKG = ISkaleDKG(contractManager.getContract(\"SkaleDKG\"));  require(  skaleDKG.isLastDKGSuccessful(keccak256(abi.encodePacked(schainName))),  dkgRevert.strConcat(schainName));  require(rotation.freezeUntil < now, revertMessage);  _startRotation(schains[i], nodeIndex);  Because of that, no other node that is running one of these schains can exit during that period. In the worst-case scenario, one malicious node has 128 Schains and calls nodeExit every 12 hours. That means that some nodes will not be able to exit for 64 days.  Recommendation  Make node exiting process less synchronous.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"
    },
    {
        "title": "5.3 Removing a node require multiple transactions and may be very expensive   Pending",
        "body": "  Resolution  Skale team s comment:  Description  When removing a node from the network, the owner should redistribute all the schains that are currently on that node to the other nodes. To do so, the validator should call the nodeExit function of the SkaleManager contract. In this function, only one schain is going to be removed from the node. So the node would have to call the nodeExit function as many times as there are schains in the node. Every call iterates over every potential node that can be used as a replacement (like in https://github.com/ConsenSys/skale-network-audit-2020-10/issues/3).  In addition to that, the first call will iterate over all schains in the node, make 4 SSTORE operations and external calls for each schain:  code/contracts/NodeRotation.sol:L204-L210  function _startRotation(bytes32 schainIndex, uint nodeIndex) private {  ConstantsHolder constants = ConstantsHolder(contractManager.getContract(\"ConstantsHolder\"));  rotations[schainIndex].nodeIndex = nodeIndex;  rotations[schainIndex].newNodeIndex = nodeIndex;  rotations[schainIndex].freezeUntil = now.add(constants.rotationDelay());  waitForNewNode[schainIndex] = true;  This may hit the block gas limit even easier than issue 5.4.  If the first transaction does not hit the block s gas limit, the maximum price of deleting a node would be BLOCK_GAS_COST * 128. At the moment, it s around $50,000.  Recommendation  Optimize the process of deleting a node, so it can t hit the gas limit in one transaction, and the overall price should be cheaper.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"
    },
    {
        "title": "5.4 Adding a new schain may potentially hit the gas limit   Pending",
        "body": "  Resolution  Skale team s comment:  Description  When adding a new schain, a group of random 16 nodes is randomly selected to run that schain. In order to do so, the _generateGroup function iterates over all the nodes that can be used for that purpose:  code/contracts/SchainsInternal.sol:L522-L541  function _generateGroup(bytes32 schainId, uint numberOfNodes) private returns (uint[] memory nodesInGroup) {  Nodes nodes = Nodes(contractManager.getContract(\"Nodes\"));  uint8 space = schains[schainId].partOfNode;  nodesInGroup = new uint[](numberOfNodes);  uint[] memory possibleNodes = isEnoughNodes(schainId);  require(possibleNodes.length >= nodesInGroup.length, \"Not enough nodes to create Schain\");  uint ignoringTail = 0;  uint random = uint(keccak256(abi.encodePacked(uint(blockhash(block.number.sub(1))), schainId)));  for (uint i = 0; i < nodesInGroup.length; ++i) {  uint index = random % (possibleNodes.length.sub(ignoringTail));  uint node = possibleNodes[index];  nodesInGroup[i] = node;  _swap(possibleNodes, index, possibleNodes.length.sub(ignoringTail).sub(1));  ++ignoringTail;  _exceptionsForGroups[schainId][node] = true;  addSchainForNode(node, schainId);  require(nodes.removeSpaceFromNode(node, space), \"Could not remove space from Node\");  If the total number of nodes exceeds around a few thousands, adding a schain may hit the block gas limit.  Recommendation  Avoid iterating over all nodes when selecting a random node for a schain.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"
    },
    {
        "title": "5.5 Typos ",
        "body": "  Description  There are a few typos in the contract source code. This could result in unforeseeable issues in the future development cycles.  Examples  succesful instead of successful:  code/contracts/SkaleDKG.sol:L77-L78  mapping(bytes32 => uint) public lastSuccesfulDKG;  code/contracts/SkaleDKG.sol:L372-L373  _setSuccesfulDKG(schainId);  and many other instances of succesful through out the code.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"
    },
    {
        "title": "5.6 Redundant Checks in some flows",
        "body": "  Description  The workflows in the Skale network are complicated and multi layers (multiple calls to different modules). Some checks are done in this process that are redundant and can be removed, based on the current code and the workflow.  Examples  An example of this redundancy, is when completing a node exit procedure. completeExit() checks if the node status is leaving and if so continues:  code/contracts/Nodes.sol:L309-L311  require(isNodeLeaving(nodeIndex), \"Node is not Leaving\");  _setNodeLeft(nodeIndex);  However, in _setNodeLeft() it has an if clause for the status being Active, which will never be true.  code/contracts/Nodes.sol:L795-L803  function _setNodeLeft(uint nodeIndex) private {  nodesIPCheck[nodes[nodeIndex].ip] = false;  nodesNameCheck[keccak256(abi.encodePacked(nodes[nodeIndex].name))] = false;  delete nodesNameToIndex[keccak256(abi.encodePacked(nodes[nodeIndex].name))];  if (nodes[nodeIndex].status == NodeStatus.Active) {  numberOfActiveNodes--;  } else {  numberOfLeavingNodes--;  Recommendation  To properly check the code flows for unreachable code and remove redundant checks.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"
    },
    {
        "title": "5.7 Presence of empty function   ",
        "body": "  Resolution   Implemented in   Bug/skale 3273 formula fix 435.  Description  estimateBounty() is declared but neither implemented nor used in any part of the current code base.  Examples  code/contracts/BountyV2.sol:L142-L159  function estimateBounty(uint /* nodeIndex */) external pure returns (uint) {  revert(\"Not implemented\");  // ConstantsHolder constantsHolder = ConstantsHolder(contractManager.getContract(\"ConstantsHolder\"));  // Nodes nodes = Nodes(contractManager.getContract(\"Nodes\"));  // TimeHelpers timeHelpers = TimeHelpers(contractManager.getContract(\"TimeHelpers\"));  // uint stagePoolSize;  // uint nextStage;  // (stagePoolSize, nextStage) = _getEpochPool(timeHelpers.getCurrentMonth(), timeHelpers, constantsHolder);  // return _calculateMaximumBountyAmount(  //     stagePoolSize,  //     nextStage.sub(1),  //     nodeIndex,  //     constantsHolder,  //     nodes  // );  Recommendation  It is suggested to remove dead code from the code base, or fully implement it before the next step.  ",
        "labels": [
            "Consensys",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"
    },
    {
        "title": "5.8 Presence of TODO tags in the codebase",
        "body": "  Description  A few TODO tags are present in the codebase.  Examples  code/contracts/SchainsInternal.sol:L153-L160  // TODO:  // optimize  for (uint i = 0; i + 1 < schainsAtSystem.length; i++) {  if (schainsAtSystem[i] == schainId) {  schainsAtSystem[i] = schainsAtSystem[schainsAtSystem.length.sub(1)];  break;  code/contracts/SchainsInternal.sol:L294-L301  /**  @dev Checks whether schain name is available.  TODO Need to delete - copy of web3.utils.soliditySha3  /  function isSchainNameAvailable(string calldata name) external view returns (bool) {  bytes32 schainId = keccak256(abi.encodePacked(name));  return schains[schainId].owner == address(0) && !usedSchainNames[schainId];  code/contracts/Nodes.sol:L81-L89  // TODO: move outside the contract  struct NodeCreationParams {  string name;  bytes4 ip;  bytes4 publicIp;  uint16 port;  bytes32[2] publicKey;  uint16 nonce;  And a few others in test scripts.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"
    },
    {
        "title": "6.1 Collaterals are not guaranteed to be returned after a batch is cancelled    ",
        "body": "  Resolution   Fixed with   AragonBlack/fundraising#162  Description  When traders open buy orders, they also transfer collateral tokens to the market maker contract. If the current batch is going to be cancelled, there is a chance that these collateral tokens will not be returned to the traders.  Examples  If a current collateralsToBeClaimed value is zero on a batch initialization and in this new batch only buy orders are submitted, collateralsToBeClaimed value will still stay zero.  At the same time if in Tap contract tapped amount was bigger than _maximumWithdrawal() on batch initialisation, _maximumWithdrawal() will most likely increase when the traders transfer new collateral tokens with the buy orders. And a beneficiary will be able to withdraw part of these tokens. Because of that, there might be not enough tokens to withdraw by the traders if the batch is cancelled.  It s partially mitigated by having floor value in Tap contract, but if there are more collateral tokens in the batch than floor, the issue is still valid.  Recommendation  Ensure that tapped is not bigger than _maximumWithdrawal()  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"
    },
    {
        "title": "6.2 Fees can be changed during the batch    ",
        "body": "  Resolution   Fixed with   AragonBlack/fundraising@0941f53 by storing current fee in meta batch.  Description  Shareholders can vote to change the fees. For buy orders, fees are withdrawn immediately when order is submitted and the only risk is frontrunning by the shareholder s voting contract.  For sell orders, fees are withdrawn when a trader claims an order and withdraws funds in _claimSellOrder  function:  code/apps/batched-bancor-market-maker/contracts/BatchedBancorMarketMaker.sol:L790-L792  if (fee > 0) {  reserve.transfer(_collateral, beneficiary, fee);  Fees can be changed between opening order and claiming this order which makes the fees unpredictable.  Recommendation  Fees for an order should not be updated during its lifetime.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"
    },
    {
        "title": "6.3 Bancor formula should not be updated during the batch    ",
        "body": "  Resolution   Fixed with   AragonBlack/fundraising@a8c2e21 by storing a ref to the Formula with the meta batch.  Description  Shareholders can vote to change the bancor formula contract. That can make a price in the current batch unpredictable.  code/apps/batched-bancor-market-maker/contracts/BatchedBancorMarketMaker.sol:L212-L216  function updateFormula(IBancorFormula _formula) external auth(UPDATE_FORMULA_ROLE) {  require(isContract(_formula), ERROR_CONTRACT_IS_EOA);  _updateFormula(_formula);  Recommendation  Bancor formula update should be executed in the next batch or with a timelock that is greater than batch duration.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"
    },
    {
        "title": "6.4 Maximum slippage shouldn t be updated for the current batch    ",
        "body": "  Resolution   Fixed with   AragonBlack/fundraising@aa4f03e by storing slippage with the batch.  Description  When anyone submits a new order, the batch price is updated and it s checked whether the price slippage is acceptable. The problem is that the maximum slippage can be updated during the batch and traders cannot be sure that price is limited as they initially expected.  code/apps/batched-bancor-market-maker/contracts/BatchedBancorMarketMaker.sol:L487-L489  function _slippageIsValid(Batch storage _batch, address _collateral) internal view returns (bool) {  uint256 staticPricePPM = _staticPricePPM(_batch.supply, _batch.balance, _batch.reserveRatio);  uint256 maximumSlippage = collaterals[_collateral].slippage;  Additionally, if a maximum slippage is updated to a lower value, some of the orders that should lower the current slippage will also revert.  Recommendation  Save a slippage value on batch initialization and use it during the current batch.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"
    },
    {
        "title": "6.5 AragonFundraisingController - an untapped address in toReset can block attempts of opening Trading after presale    ",
        "body": "  Resolution   Fixed with   AragonBlack/fundraising@9451147 by checking if token is tapped. Gas consumption is increased due to external call to Tap to check if token is actually tapped. The number of tokens to be reset is capped.  Description  AragonFundraisingController can be initialized with a list of token addresses _toReset that are to be reset when trading opens after the presale. These addresses are supposed to be addresses of tapped tokens. However, the list needs to be known when initializing the contract but the tapped tokens are added after initialization when calling addCollateralToken (and tapped with _rate>0). This can lead to an inconsistency that blocks openTrading.  code/apps/aragon-fundraising/contracts/AragonFundraisingController.sol:L99-L102  for (uint256 i = 0; i < _toReset.length; i++) {  require(_tokenIsContractOrETH(_toReset[i]), ERROR_INVALID_TOKENS);  toReset.push(_toReset[i]);  In case a token address makes it into the list of toReset tokens that is not tapped it will be impossible to openTrading as tap.resetTappedToken(toReset[i]); throws for untapped tokens. According to the permission setup in FundraisingMultisigTemplate only Controller can call Marketmaker.open  code/apps/aragon-fundraising/contracts/AragonFundraisingController.sol:L163-L169  function openTrading() external auth(OPEN_TRADING_ROLE) {  for (uint256 i = 0; i < toReset.length; i++) {  tap.resetTappedToken(toReset[i]);  marketMaker.open();  Recommendation  Instead of initializing the Controller with a list of tapped tokens to be reset when trading opens, add a flag to addCollateralToken to indicate that the token should be reset when calling openTrading, making sure only tapped tokens are added to this list. This also allows adding tapped tokens that are to be reset at a later point in time.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"
    },
    {
        "title": "6.6 Tap payments inconsistency    ",
        "body": "  Resolution   Fixed with   AragonBlack/fundraising#162  Description  Every time project managers want to withdraw tapped funds, the maximum amount of withdrawable funds is calculated in tap._maximumWithdrawal function. The method ensures that project managers can only withdraw unlocked funds (balance exceeding the collaterals minimum comprised of the collaterals configured floor including the minimum tokens to hold) even though their allowance might be higher.  if there are no unlocked funds available, the maximum withdrawal is zero (balance <= minimum).  if there are unlocked funds available (balance > minimum) and the allowance (tapped) would result in a balance >= minimum, the maximum withdrawal amount is the calculated allowance tapped.  if there are unlocked funds available (balance > minimum) and the allowance (tapped) would result in a balance < minimum, the maximum withdrawal amount tapped is capped to balance - minimum to ensure that the remaining collateral balance is at least at the minimum and not below.  This means that in the case of (3) if there are not enough funds to withdraw tapped(time*tap_rate) amount of tokens, it gets truncated and only a part of tapped tokens gets withdrawn.  code/apps/tap/contracts/Tap.sol:L239-L255  function _maximumWithdrawal(address _token) internal view returns (uint256) {  uint256 toBeClaimed = controller.collateralsToBeClaimed(_token);  uint256 floor = floors[_token];  uint256 minimum = toBeClaimed.add(floor);  uint256 balance = _token == ETH ? address(reserve).balance : ERC20(_token).staticBalanceOf(reserve);  uint256 tapped = (_currentBatchId().sub(lastWithdrawals[_token])).mul(rates[_token]);  if (minimum >= balance) {  return 0;  if (balance >= tapped.add(minimum)) {  return tapped;  return balance.sub(minimum);  The problem is that the remaining tokens (tapped - capped_tapped) cannot be claimed afterward and tapped value is reset to zero.  Remediation  In case the maximum withdrawal amount gets capped, the information about the remaining tokens that the project team should have been able to withdraw should be kept to allow them to withdraw the tokens at a later point in time when there are enough funds for it.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"
    },
    {
        "title": "6.7 [New] Tapped collaterals can be bought by traders   ",
        "body": "  Resolution  This behaviour is intentional and if there is not a lot of funds in the pool, shareholders have a priority to buy tokens even if these tokens can already be withdrawn by the beneficiary. It is done in order to protect shareholders in case if the project is dying and running out of funds. The downside of this behaviour is that it creates an additional incentive for the beneficiary to withdraw tapped tokens as soon and as often as possible which creates a race condition.  Description  When a trader submits a sell order, _openSellOrder() function checks that there are enough tokens in reserve by calling _poolBalanceIsSufficient function  code/apps/batched-bancor-market-maker/contracts/BatchedBancorMarketMaker.sol:L483-L485  function _poolBalanceIsSufficient(address _collateral) internal view returns (bool) {  return controller.balanceOf(address(reserve), _collateral) >= collateralsToBeClaimed[_collateral];  the problem is that because collateralsToBeClaimed[_collateral] has increased, controller.balanceOf(address(reserve), _collateral) could also increase. It happens so because controller.balanceOf() function subtracts tapped amount from the reserve s balance.  code/apps/aragon-fundraising/contracts/AragonFundraisingController.sol:L358-L366  function balanceOf(address _who, address _token) public view isInitialized returns (uint256) {  uint256 balance = _token == ETH ? _who.balance : ERC20(_token).staticBalanceOf(_who);  if (_who == address(reserve)) {  return balance.sub(tap.getMaximumWithdrawal(_token));  } else {  return balance;  And tap.getMaximumWithdrawal(_token) could decrease because it depends on collateralsToBeClaimed[_collateral]  apps/tap/contracts/Tap.sol:L231-L264  function _tappedAmount(address _token) internal view returns (uint256) {  uint256 toBeKept = controller.collateralsToBeClaimed(_token).add(floors[_token]);  uint256 balance = _token == ETH ? address(reserve).balance : ERC20(_token).staticBalanceOf(reserve);  uint256 flow = (_currentBatchId().sub(lastTappedAmountUpdates[_token])).mul(rates[_token]);  uint256 tappedAmount = tappedAmounts[_token].add(flow);  /**  whatever happens enough collateral should be  kept in the reserve pool to guarantee that  its balance is kept above the floor once  all pending sell orders are claimed  /  /**  the reserve's balance is already below the balance to be kept  the tapped amount should be reset to zero  /  if (balance <= toBeKept) {  return 0;  /**  the reserve's balance minus the upcoming tap flow would be below the balance to be kept  the flow should be reduced to balance - toBeKept  /  if (balance <= toBeKept.add(tappedAmount)) {  return balance.sub(toBeKept);  /**  the reserve's balance minus the upcoming flow is above the balance to be kept  the flow can be added to the tapped amount  /  return tappedAmount;  That means that the amount that beneficiary can withdraw has just decreased, which should not be possible.  Recommendation  Ensure that tappedAmount cannot be decreased once updated.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"
    },
    {
        "title": "6.8 Presale - contributionToken double cast and invalid comparison    ",
        "body": "  Resolution   Fixed with   AragonBlack/fundraising@61f5803.  Description  Examples  contribute - invalid comparison of contract type against address(0x00). Even though this is accepted in solidity <0.5.0 it is going to raise a compiler error with newer versions (>=0.5.0).  code/apps/presale/contracts/Presale.sol:L163-L170  function contribute(address _contributor, uint256 _value) external payable nonReentrant auth(CONTRIBUTE_ROLE) {  require(state() == State.Funding, ERROR_INVALID_STATE);  if (contributionToken == ETH) {  require(msg.value == _value, ERROR_INVALID_CONTRIBUTE_VALUE);  } else {  require(msg.value == 0,      ERROR_INVALID_CONTRIBUTE_VALUE);  _transfer - double cast token to ERC20 if it is the contribution token.  code/apps/presale/contracts/Presale.sol:L344-L344  require(ERC20(_token).safeTransfer(_to, _amount), ERROR_TOKEN_TRANSFER_REVERTED);  Recommendation  contributionToken can either be ETH or a valid ERC20 contract address. It is therefore recommended to store the token as an address type instead of the more precise contract type to resolve the double cast and the invalid contract type to address comparison or cast the ERC20 type to address() before comparison.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"
    },
    {
        "title": "6.9 Fees are not returned for buy orders if a batch is canceled   ",
        "body": "  Resolution  This issue has been addressed with the following statement:  The only situation where a batch can be cancelled is when a collateral is un-whitelisted. This is obviously a very critical operation that we introduced just in case the collateral happened to be malicious token. Handling the ability to return fees in case a batch order is cancelled would thus add a lot of computation overhead for: a. a very unlikely situation b. where the fees would anyhow be returned in a malicious token. c. given a small amount [it s a fee and not the main amount]. We figured out that it was a bad decision to add gas overhead to all orders just to prevent this situation.  Description  Every trader pays fees on each buy order and transfers it directly to the beneficiary.  code/apps/batched-bancor-market-maker/contracts/BatchedBancorMarketMaker.sol:L706-L713  uint256 fee = _value.mul(buyFeePct).div(PCT_BASE);  uint256 value = _value.sub(fee);  // collect fee and collateral  if (fee > 0) {  _transfer(_buyer, beneficiary, _collateral, fee);  _transfer(_buyer, address(reserve), _collateral, value);  If the batch is canceled, fees are not returned to the traders because there is no access to the beneficiary account.  Additionally, fees are returned to traders for all the sell orders if the batch is canceled.  Recommendation  Consider transferring fees to a beneficiary only after the batch is over.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"
    },
    {
        "title": "6.10 Tap - Controller should not be updateable    ",
        "body": "  Resolution   Fixed with   AragonBlack/fundraising@f6054443 by removing update functionality.  Description  Similar to the issue 6.11, Tap allows updating the Controller contract it is using. The permission is currently not assigned in the FundraisingMultisigTemplate but might be used in custom deployments.  code/apps/tap/contracts/Tap.sol:L117-L125  /**  @notice Update controller to `_controller`  @param _controller The address of the new controller contract  /  function updateController(IAragonFundraisingController _controller) external auth(UPDATE_CONTROLLER_ROLE) {  require(isContract(_controller), ERROR_CONTRACT_IS_EOA);  _updateController(_controller);  Recommendation  To avoid inconsistencies, we suggest to remove this functionality and provide a guideline on how to safely upgrade components of the system.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"
    },
    {
        "title": "6.11 Tap - reserve can be updated in Tap but not in MarketMaker or Controller    ",
        "body": "  Resolution   Fixed with   AragonBlack/fundraising@987720b1 by removing update functionality.  Description  The address of the pool/reserve contract can be updated in Tap if someone owns the UPDATE_RESERVE_ROLE permission. The permission is currently not assigned in the template.  The reserve is being referenced by multiple Contracts. Tap interacts with it to transfer funds to the beneficiary, Controller adds new protected tokens, and MarketMaker transfers funds when someone sells their Shareholder token.  Updating reserve only in Tap is inconsistent with the system as the other contracts are still referencing the old reserve unless they are updated via the Aragon Application update mechanisms.  code/apps/tap/contracts/Tap.sol:L127-L135  /**  @notice Update reserve to `_reserve`  @param _reserve The address of the new reserve [pool] contract  /  function updateReserve(Vault _reserve) external auth(UPDATE_RESERVE_ROLE) {  require(isContract(_reserve), ERROR_CONTRACT_IS_EOA);  _updateReserve(_reserve);  Recommendation  Remove the possibility to update reserve in Tap to keep the system consistent. Provide information about update mechanisms in case the reserve needs to be updated for all components.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"
    },
    {
        "title": "6.12 Presale can be opened earlier than initially assigned date    ",
        "body": "  Resolution   Fixed with   AragonBlack/fundraising@0726e29.  Description  There are 2 ways how presale opening date can be assigned. Either it s defined on initialization or the presale will start when open() function is executed.  code/apps/presale/contracts/Presale.sol:L144-L146  if (_openDate != 0) {  _setOpenDate(_openDate);  The problem is that even if openDate is assigned to some non-zero date, it can still be opened earlier by calling open() function.  code/apps/presale/contracts/Presale.sol:L152-L156  function open() external auth(OPEN_ROLE) {  require(state() == State.Pending, ERROR_INVALID_STATE);  _open();  Recommendation  Require that openDate is not set (0) when someone manually calls the open() function.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"
    },
    {
        "title": "6.13 Presale - should not allow zero value contributions    ",
        "body": "  Resolution   Fixed with   AragonBlack/fundraising@6a6e222.  Description  The Presale accepts zero value contributions emitting a contribution event if none of the Aragon components (TokenManager, MinimeToken) raises an exception.  code/apps/presale/contracts/Presale.sol:L163-L173  function contribute(address _contributor, uint256 _value) external payable nonReentrant auth(CONTRIBUTE_ROLE) {  require(state() == State.Funding, ERROR_INVALID_STATE);  if (contributionToken == ETH) {  require(msg.value == _value, ERROR_INVALID_CONTRIBUTE_VALUE);  } else {  require(msg.value == 0,      ERROR_INVALID_CONTRIBUTE_VALUE);  _contribute(_contributor, _value);  Recommendation  Reject zero value ETH or ERC20 contributions.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"
    },
    {
        "title": "6.14 Compiler Warnings - Function state mutability can be restricted to view    ",
        "body": "  Resolution   Fixed with   AragonBlack/fundraising@cfd677a.  Description  The following methods are not state-changing and can, therefore, be restricted to view.  Recommendation  Restrict function state mutability of the listed methods to view.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"
    },
    {
        "title": "6.15 FundraisingMultisigTemplate - should use BaseTemplate._createPermissionForTemplate() to assign permissions to itself    ",
        "body": "  Resolution   Fixed with   AragonBlack/fundraising@dd153e0.  Description  The template temporarily assigns permissions to itself to be able to configure parts of the system. This can either be done by calling acl.createPermission(address(this), app, role, manager) or by using a distinct method provided with the DAO-Templates BaseTemplate _createPermissionForTemplate.  We suggest that in order to make it clear that permissions are assigned to the template and make it easier to audit that permissions are either revoked or transferred before the DAO is transferred to the new user, the method provided and used with the default Aragon DAO-Templates should be used.  use createPermission if permissions are assigned to an entity other than the template contract.  use _createPermissionForTemplate when creating permissions for the template contract.  code/templates/multisig/contracts/FundraisingMultisigTemplate.sol:L333-L334  // create and grant ADD_PROTECTED_TOKEN_ROLE to this template  acl.createPermission(this, controller, controller.ADD_COLLATERAL_TOKEN_ROLE(), this);  Sidenote: pass address(this) instead of the contract instance to createPermission.  Recommendation  Use BaseTemplate._createPermissionForTemplate to assign permissions to the template.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"
    },
    {
        "title": "6.16 FundraisingMultisigTemplate - misleading comments    ",
        "body": "  Resolution   Fixed with   AragonBlack/fundraising@3b4700a and  AragonBlack/fundraising@40c465fc.  Description  The comment mentionsADD_PROTECTED_TOKEN_ROLE but permissions for ADD_COLLATERAL_TOKEN_ROLE are created.  code/templates/multisig/contracts/FundraisingMultisigTemplate.sol:L333-L334  // create and grant ADD_PROTECTED_TOKEN_ROLE to this template  acl.createPermission(this, controller, controller.ADD_COLLATERAL_TOKEN_ROLE(), this);  code/templates/multisig/contracts/FundraisingMultisigTemplate.sol:L355-L356  // transfer ADD_PROTECTED_TOKEN_ROLE  _transferPermissionFromTemplate(acl, controller, shareVoting, controller.ADD_COLLATERAL_TOKEN_ROLE(), shareVoting);  Recommendation  ADD_PROTECTED_TOKEN_ROLE in the comment should be ADD_COLLATERAL_TOKEN_ROLE.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"
    },
    {
        "title": "6.17 FundraisingMultisigTemplate - unnecessary cast to address    ",
        "body": "  Resolution   Fixed with   AragonBlack/fundraising@0e00269.  Description  The addresses of DAI (argument address _dai) and AND (argument address _ant) are unnecessarily cast to address.  code/templates/multisig/contracts/FundraisingMultisigTemplate.sol:L58-L76  constructor(  DAOFactory              _daoFactory,  ENS                     _ens,  MiniMeTokenFactory      _miniMeFactory,  IFIFSResolvingRegistrar _aragonID,  address                 _dai,  address                 _ant  BaseTemplate(_daoFactory, _ens, _miniMeFactory, _aragonID)  public  _ensureAragonIdIsValid(_aragonID);  _ensureMiniMeFactoryIsValid(_miniMeFactory);  _ensureTokenIsContractOrETH(_dai);  _ensureTokenIsContractOrETH(_ant);  collaterals.push(address(_dai));  collaterals.push(address(_ant));  Recommendation  Both arguments are already of type address, therefore remove the explicit cast to address() when pushing to the collaterals array.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"
    },
    {
        "title": "6.18 FundraisingMultisigTemplate - unused import ERC20    ",
        "body": "  Resolution   Fixed with   AragonBlack/fundraising@73481d1.  Description  The interface ERC20 is imported but never used.  code/templates/multisig/contracts/FundraisingMultisigTemplate.sol:L4-L4  import \"@aragon/os/contracts/lib/token/ERC20.sol\";  Recommendation  Remove the unused import.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"
    },
    {
        "title": "6.19 FundraisingMultisigTemplate - DAI/ANT token address cannot be zero    ",
        "body": "  Resolution   Fixed with   AragonBlack/fundraising@da561ce.  Description  code/templates/multisig/contracts/FundraisingMultisigTemplate.sol:L71-L72  _ensureTokenIsContractOrETH(_dai);  _ensureTokenIsContractOrETH(_ant);  code/templates/multisig/contracts/FundraisingMultisigTemplate.sol:L572-L575  function _ensureTokenIsContractOrETH(address _token) internal view returns (bool) {  require(isContract(_token) || _token == ETH, ERROR_BAD_SETTINGS);  Recommendation  Use isContract() instead of _ensureTokenIsContractOrETH() and optionally require that collateral[0] != collateral[1] as an additional check to prevent that the fundraising template is being deployed with an invalid configuration.  7 Tool-Based Analysis  Several tools were used to perform an automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"
    },
    {
        "title": "7.1 MythX",
        "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"
    },
    {
        "title": "7.2 Ethlint",
        "body": "  Ethlint is an open-source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  $ solium --version  Solium version 1.2.5  $ solium -d  apps/aragon-fundraising/contracts/AragonFundraisingController.sol  370:5    error    Only use indent of 4 spaces.    indentation  templates/multisig/contracts/FundraisingMultisigTemplate.sol  573:5    error    Only use indent of 4 spaces.    indentation  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"
    },
    {
        "title": "7.3 Surya",
        "body": "  Surya is a utility tool for smart contract systems. It provides a number of visual outputs and information about the structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers (please use horizontal scroll to view all columns):  Contracts Description Table  Function Name  Visibility  Mutability  Modifiers  AragonFundraisingController  Implementation  EtherTokenConstant, IsContract, IAragonFundraisingController, AragonApp  initialize  External    onlyInit  updateBeneficiary  External    auth  updateFees  External    auth  openPresale  External    auth  closePresale  External    isInitialized  contribute  External    auth  refund  External    isInitialized  openTrading  External    auth  openBuyOrder  External    auth  openSellOrder  External    auth  claimBuyOrder  External    isInitialized  claimSellOrder  External    isInitialized  addCollateralToken  External    auth  reAddCollateralToken  External    auth  removeCollateralToken  External    auth  updateCollateralToken  External    auth  updateMaximumTapRateIncreasePct  External    auth  updateMaximumTapFloorDecreasePct  External    auth  addTokenTap  External    auth  updateTokenTap  External    auth  withdraw  External    auth  token  Public    isInitialized  contributionToken  Public    isInitialized  getMaximumWithdrawal  Public    isInitialized  collateralsToBeClaimed  Public    isInitialized  balanceOf  Public    isInitialized  _tokenIsContractOrETH  Internal \ud83d\udd12  BancorFormula  Implementation  IBancorFormula, Utils  <Constructor>  Public    calculatePurchaseReturn  Public    NO   calculateSaleReturn  Public    NO   calculateCrossConnectorReturn  Public    NO   power  Internal \ud83d\udd12  generalLog  Internal \ud83d\udd12  floorLog2  Internal \ud83d\udd12  findPositionInMaxExpArray  Internal \ud83d\udd12  generalExp  Internal \ud83d\udd12  optimalLog  Internal \ud83d\udd12  optimalExp  Internal \ud83d\udd12  BatchedBancorMarketMaker  Implementation  EtherTokenConstant, IsContract, AragonApp  initialize  External    onlyInit  open  External    auth  updateFormula  External    auth  updateBeneficiary  External    auth  updateFees  External    auth  addCollateralToken  External    auth  removeCollateralToken  External    auth  updateCollateralToken  External    auth  openBuyOrder  External    auth  openSellOrder  External    auth  claimBuyOrder  External    nonReentrant isInitialized  claimSellOrder  External    nonReentrant isInitialized  claimCancelledBuyOrder  External    nonReentrant isInitialized  claimCancelledSellOrder  External    nonReentrant isInitialized  getCurrentBatchId  Public    isInitialized  getCollateralToken  Public    isInitialized  getBatch  Public    isInitialized  getStaticPricePPM  Public    isInitialized  _staticPricePPM  Internal \ud83d\udd12  _currentBatchId  Internal \ud83d\udd12  _beneficiaryIsValid  Internal \ud83d\udd12  _feeIsValid  Internal \ud83d\udd12  _reserveRatioIsValid  Internal \ud83d\udd12  _tokenManagerSettingIsValid  Internal \ud83d\udd12  _collateralValueIsValid  Internal \ud83d\udd12  _bondAmountIsValid  Internal \ud83d\udd12  _collateralIsWhitelisted  Internal \ud83d\udd12  _batchIsOver  Internal \ud83d\udd12  _batchIsCancelled  Internal \ud83d\udd12  _userIsBuyer  Internal \ud83d\udd12  _userIsSeller  Internal \ud83d\udd12  _poolBalanceIsSufficient  Internal \ud83d\udd12  _slippageIsValid  Internal \ud83d\udd12  _buySlippageIsValid  Internal \ud83d\udd12  _sellSlippageIsValid  Internal \ud83d\udd12  _currentBatch  Internal \ud83d\udd12  _open  Internal \ud83d\udd12  _updateBeneficiary  Internal \ud83d\udd12  _updateFormula  Internal \ud83d\udd12  _updateFees  Internal \ud83d\udd12  _cancelCurrentBatch  Internal \ud83d\udd12  _addCollateralToken  Internal \ud83d\udd12  _removeCollateralToken  Internal \ud83d\udd12  _updateCollateralToken  Internal \ud83d\udd12  _openBuyOrder  Internal \ud83d\udd12  _openSellOrder  Internal \ud83d\udd12  _claimBuyOrder  Internal \ud83d\udd12  _claimSellOrder  Internal \ud83d\udd12  _claimCancelledBuyOrder  Internal \ud83d\udd12  _claimCancelledSellOrder  Internal \ud83d\udd12  _updatePricing  Internal \ud83d\udd12  _transfer  Internal \ud83d\udd12  Presale  Implementation  EtherTokenConstant, IsContract, AragonApp  initialize  External    onlyInit  open  External    auth  contribute  External    nonReentrant auth  refund  External    nonReentrant isInitialized  close  External    nonReentrant isInitialized  contributionToTokens  Public    isInitialized  state  Public    isInitialized  _timeSinceOpen  Internal \ud83d\udd12  _setOpenDate  Internal \ud83d\udd12  _setVestingDatesWhenOpenDateIsKnown  Internal \ud83d\udd12  _open  Internal \ud83d\udd12  _contribute  Internal \ud83d\udd12  _refund  Internal \ud83d\udd12  _close  Internal \ud83d\udd12  _transfer  Internal \ud83d\udd12  Tap  Implementation  TimeHelpers, EtherTokenConstant, IsContract, AragonApp  initialize  External    onlyInit  updateController  External    auth  updateReserve  External    auth  updateBeneficiary  External    auth  updateMaximumTapRateIncreasePct  External    auth  updateMaximumTapFloorDecreasePct  External    auth  addTappedToken  External    auth  removeTappedToken  External    auth  updateTappedToken  External    auth  resetTappedToken  External    auth  withdraw  External    auth  getMaximumWithdrawal  Public    isInitialized  _currentBatchId  Internal \ud83d\udd12  _maximumWithdrawal  Internal \ud83d\udd12  _beneficiaryIsValid  Internal \ud83d\udd12  _maximumTapFloorDecreasePctIsValid  Internal \ud83d\udd12  _tokenIsContractOrETH  Internal \ud83d\udd12  _tokenIsTapped  Internal \ud83d\udd12  _tapRateIsValid  Internal \ud83d\udd12  _tapUpdateIsValid  Internal \ud83d\udd12  _tapRateUpdateIsValid  Internal \ud83d\udd12  _tapFloorUpdateIsValid  Internal \ud83d\udd12  _updateController  Internal \ud83d\udd12  _updateReserve  Internal \ud83d\udd12  _updateBeneficiary  Internal \ud83d\udd12  _updateMaximumTapRateIncreasePct  Internal \ud83d\udd12  _updateMaximumTapFloorDecreasePct  Internal \ud83d\udd12  _addTappedToken  Internal \ud83d\udd12  _removeTappedToken  Internal \ud83d\udd12  _updateTappedToken  Internal \ud83d\udd12  _resetTappedToken  Internal \ud83d\udd12  _withdraw  Internal \ud83d\udd12  FundraisingMultisigTemplate  Implementation  EtherTokenConstant, BaseTemplate  <Constructor>  Public    BaseTemplate  prepareInstance  External    NO   installShareApps  External    NO   installFundraisingApps  External    NO   finalizeInstance  External    NO   _installBoardApps  Internal \ud83d\udd12  _installShareApps  Internal \ud83d\udd12  _installFundraisingApps  Internal \ud83d\udd12  _proxifyFundraisingApps  Internal \ud83d\udd12  _initializePresale  Internal \ud83d\udd12  _initializeMarketMaker  Internal \ud83d\udd12  _initializeTap  Internal \ud83d\udd12  _initializeController  Internal \ud83d\udd12  _setupCollaterals  Internal \ud83d\udd12  _setupBoardPermissions  Internal \ud83d\udd12  _setupSharePermissions  Internal \ud83d\udd12  _setupFundraisingPermissions  Internal \ud83d\udd12  _cacheDao  Internal \ud83d\udd12  _cacheBoardApps  Internal \ud83d\udd12  _cacheShareApps  Internal \ud83d\udd12  _cacheFundraisingApps  Internal \ud83d\udd12  _daoCache  Internal \ud83d\udd12  _boardAppsCache  Internal \ud83d\udd12  _shareAppsCache  Internal \ud83d\udd12  _fundraisingAppsCache  Internal \ud83d\udd12  _clearCache  Internal \ud83d\udd12  _vaultCache  Internal \ud83d\udd12  _shareTMCache  Internal \ud83d\udd12  _reserveCache  Internal \ud83d\udd12  _presaleCache  Internal \ud83d\udd12  _controllerCache  Internal \ud83d\udd12  _ensureTokenIsContractOrETH  Internal \ud83d\udd12  _ensureBoardAppsCache  Internal \ud83d\udd12  _ensureShareAppsCache  Internal \ud83d\udd12  _ensureFundraisingAppsCache  Internal \ud83d\udd12  _registerApp  Internal \ud83d\udd12  Legend  Function can modify state  Function is payable  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"
    },
    {
        "title": "7.4 Test Coverage Measurement",
        "body": "  Testing is implemented using Truffle an all provided test cases pass. However, the Presale contract fails to generate coverage statistics.  MarketMaker  Controller  Tap  Presale  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"
    },
    {
        "title": "4.1 StableSwapOperatorV1 - resistantFei value is not correct in the resistantBalanceAndFei function ",
        "body": "  Description  The resistantBalanceAndFei function of a PCVDeposit contract is supposed to return the amount of funds that the contract controls; it is then used to evaluate the total value of PCV (collateral in the protocol). Additionally, this function returns the number of FEI tokens that are protocol-controlled. These FEI tokens are  temporarily minted ; they are not backed up by the collateral and shouldn t be used in calculations that determine the collateralization of the protocol.  Ideally, the amount of these FEI tokens should be the same during the deposit, withdrawal, and the resistantBalanceAndFei function call. In the StableSwapOperatorV1  contract, all these values are totally different:  during the deposit, the amount of required FEI tokens is calculated. It s done in a way so the values of FEI and 3pool tokens in the metapool should be equal after the deposit. So if there is the initial imbalance of FEI and 3pool tokens, the deposit value of these tokens will be different: code/contracts/pcv/curve/StableSwapOperatorV1.sol:L156-L171 // get the amount of tokens in the pool (uint256 _3crvAmount, uint256 _feiAmount) = (     IStableSwap2(pool).balances(_3crvIndex),     IStableSwap2(pool).balances(_feiIndex) ); // ... and the expected amount of 3crv in it after deposit uint256 _3crvAmountAfter = _3crvAmount + _3crvBalanceAfter;  // get the usd value of 3crv in the pool uint256 _3crvUsdValue = _3crvAmountAfter * IStableSwap3(_3pool).get_virtual_price() / 1e18;  // compute the number of FEI to deposit uint256 _feiToDeposit = 0; if (_3crvUsdValue > _feiAmount) {     _feiToDeposit = _3crvUsdValue - _feiAmount; }  during the withdrawal, the FEI and 3pool tokens are withdrawn in the same proportion as they are present in the metapool: code/contracts/pcv/curve/StableSwapOperatorV1.sol:L255-L258 uint256[2] memory _minAmounts; // [0, 0] IERC20(pool).approve(pool, _lpToWithdraw); uint256 _3crvBalanceBefore = IERC20(_3crv).balanceOf(address(this)); IStableSwap2(pool).remove_liquidity(_lpToWithdraw, _minAmounts);  in the resistantBalanceAndFei function, the value of protocol-controlled FEI tokens and the value of 3pool tokens deposited are considered equal: code/contracts/pcv/curve/StableSwapOperatorV1.sol:L348-L349 resistantBalance = _lpPriceUSD / 2; resistantFei = resistantBalance;  Some of these values may be equal under some circumstances, but that is not enforced. After one of the steps (deposit or withdrawal), the total PCV value and collateralization may be changed significantly.  Recommendation  Make sure that deposit, withdrawal, and the resistantBalanceAndFei are consistent and won t instantly change the PCV value significantly.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"
    },
    {
        "title": "4.2 CollateralizationOracle - Fei in excluded deposits contributes to userCirculatingFei ",
        "body": "  Description  CollateralizationOracle.pcvStats iterates over all deposits, queries the resistant balance and FEI for each deposit, and accumulates the total value of the resistant balances and the total resistant FEI. Any Guardian or Governor can exclude (and re-include) a deposit that has become problematic in some way, for example, because it is reporting wrong numbers. Finally, the pcvStats function computes the userCirculatingFei as the total FEI supply minus the accumulated resistant FEI balances; the idea here is to determine the amount of  free  FEI, or FEI that is not PCV. However, the FEI balances from excluded deposits contribute to the userCirculatingFei, although they are clearly not  free  FEI. That leads to a wrong protocolEquity and a skewed collateralization ratio and might therefore have a significant impact on the economics of the system.  It should be noted that even the exclusion from the total PCV leads to a protocolEquity and a collateralization ratio that could be considered skewed (again, it might depend on the exact reasons for exclusion), but  adding  the missing FEI to the userCirculatingFei distorts these numbers even more.  In the extreme scenario that all deposits have been excluded, the entire Fei supply is currently reported as userCirculatingFei.  code/contracts/oracle/CollateralizationOracle.sol:L278-L328  /// @notice returns the Protocol-Controlled Value, User-circulating FEI, and  ///         Protocol Equity.  /// @return protocolControlledValue : the total USD value of all assets held  ///         by the protocol.  /// @return userCirculatingFei : the number of FEI not owned by the protocol.  /// @return protocolEquity : the difference between PCV and user circulating FEI.  ///         If there are more circulating FEI than $ in the PCV, equity is 0.  /// @return validityStatus : the current oracle validity status (false if any  ///         of the oracles for tokens held in the PCV are invalid, or if  ///         this contract is paused).  function pcvStats() public override view returns (  uint256 protocolControlledValue,  uint256 userCirculatingFei,  int256 protocolEquity,  bool validityStatus  ) {  uint256 _protocolControlledFei = 0;  validityStatus = !paused();  // For each token...  for (uint256 i = 0; i < tokensInPcv.length(); i++) {  address _token = tokensInPcv.at(i);  uint256 _totalTokenBalance  = 0;  // For each deposit...  for (uint256 j = 0; j < tokenToDeposits[_token].length(); j++) {  address _deposit = tokenToDeposits[_token].at(j);  // ignore deposits that are excluded by the Guardian  if (!excludedDeposits[_deposit]) {  // read the deposit, and increment token balance/protocol fei  (uint256 _depositBalance, uint256 _depositFei) = IPCVDepositBalances(_deposit).resistantBalanceAndFei();  _totalTokenBalance += _depositBalance;  _protocolControlledFei += _depositFei;  // If the protocol holds non-zero balance of tokens, fetch the oracle price to  // increment PCV by _totalTokenBalance * oracle price USD.  if (_totalTokenBalance != 0) {  (Decimal.D256 memory _oraclePrice, bool _oracleValid) = IOracle(tokenToOracle[_token]).read();  if (!_oracleValid) {  validityStatus = false;  protocolControlledValue += _oraclePrice.mul(_totalTokenBalance).asUint256();  userCirculatingFei = fei().totalSupply() - _protocolControlledFei;  protocolEquity = int256(protocolControlledValue) - int256(userCirculatingFei);  Recommendation  It is unclear how to fix this. One might want to exclude the FEI in excluded deposits entirely from the calculation, but not knowing the amount was the reason to exclude the deposit in the first place. One option could be to let the entity that excludes a deposit specify substitute values that should be used instead of querying the numbers from the deposit. However, it is questionable whether this approach is practical if the numbers we d like to see as substitute values change quickly or repeatedly over time. Ultimately, the querying function itself should be fixed. Moreover, as the substitute values can dramatically impact the system economics, we d only like to trust the Governor with this and not give this permission to a Guardian. However, the original intention was to give a role with less trust than the Governor the possibility to react quickly to a deposit that reports wrong numbers; if the exclusion of deposits becomes the Governor s privilege, such a quick and lightweight intervention isn t possible anymore.  Independently, we recommend taking proper care of the situation that all deposits   or just too many   have been excluded, for example, by setting the returned validityStatus to false, as in this case, there is not enough information to compute the collateralization ratio even as a crude approximation.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"
    },
    {
        "title": "4.3 StableSwapOperatorV1 - the _minLpOut value is not accurate ",
        "body": "  Description  When depositing, the expected minimum amount of the output LP tokens is calculated:  code/contracts/pcv/curve/StableSwapOperatorV1.sol:L194-L200  // slippage check on metapool deposit  uint256 _balanceDeposited = IERC20(pool).balanceOf(address(this)) - _balanceBefore;  uint256 _metapoolVirtualPrice = IStableSwap2(pool).get_virtual_price();  uint256 _minLpOut = (_feiToDeposit + _3crvBalanceAfter) * 1e18 / _metapoolVirtualPrice * (Constants.BASIS_POINTS_GRANULARITY - depositMaxSlippageBasisPoints) / Constants.BASIS_POINTS_GRANULARITY;  require(_balanceDeposited >= _minLpOut, \"StableSwapOperatorV1: metapool deposit slippage too high\");  The problem is that the get_virtual_price function returns a valid price only if the tokens in the pool are expected to have a price equal to $1 which is not the case. Also, the balances of deposited FEI and 3pool lp tokens are just added to each other while they have a different price: _feiToDeposit + _3crvBalanceAfter.  The price of the 3pool lp tokens is currently very close to 1$ so this difference is not that visible at the moment, but this can slowly change over time.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"
    },
    {
        "title": "4.4 StableSwapOperatorV1 - FEI tokens in the contract are not considerred as protocol-owned ",
        "body": "  Description  Every PCVDeposit contract should return the amount of PCV controlled by this contract in the resistantBalanceAndFei. In addition to that, this function returns the amount of protocol-controlled FEI, which is not supposed to be collateralized. These values are crucial for evaluating the collateralization of the protocol.  Unlike some other PCVDeposit contracts, protocol-controlled FEI is not minted during the deposit and not burnt during the withdrawal. These FEI tokens are transferred beforehand, so when depositing, all the FEI that are instantly becoming protocol-controlled and heavily impact the collateralization rate. The opposite impact, but as much significant, happens during the withdrawal.  The amount of FEI needed for the deposited is calculated dynamically, it is hard to predict the exact amount beforehand. There may be too many FEI tokens in the contract and the leftovers will be considered as the user-controlled FEI.  Recommendation  There may be different approaches to solve this issue. One of them would be to make sure that the Fei transfers to/from the contract and the deposit/withdraw calls are happening in a single transaction. These FEI should be minted, burnt, or re-used as the protocol-controlled FEI in the same transaction. Another option would be to consider all the FEI balance in the contract as the protocol-controlled FEI.  If the intention is to have all these FEI collateralized, the other solution is needed: make sure that resistantBalanceAndFei always returns resistantFei equals zero.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"
    },
    {
        "title": "4.5 BalancerLBPSwapper - init() can be front-run to potentially steal tokens ",
        "body": "  Description  The deployment process for BalancerLBPSwapper appears to be the following:  deploy BalancerLBPSwapper.  run ILiquidityBootstrappingPoolFactory.create() proving the newly deployed swapper address as the owner of the pool.  initialize BalancerLBPSwapper.init() with the address of the newly created pool.  This process may be split across multiple transactions as in the v2Phase1.js deployment scenario.  Between step (1) and (3) there is a window of opportunity for someone to maliciously initialize contract. This should be easily detectable because calling init() twice should revert the second transaction. If this is not caught in the deployment script this may have more severe security implications. Otherwise, this window can be used to grief the deployment initializing it before the original initializer does forcing them to redeploy the contract or to steal any tokenSpent/tokenReceived that are owned by the contract at this time.  Note: It is assumed that the contract will not own a lot of tokens right after deployment rendering the scenario of stealing tokens more unlikely. However, that highly depends on the deployment script for the contract system.  Examples  code/contracts/pcv/balancer/BalancerLBPSwapper.sol:L107-L117  function init(IWeightedPool _pool) external {  require(address(pool) == address(0), \"BalancerLBPSwapper: initialized\");  pool = _pool;  IVault _vault = _pool.getVault();  vault = _vault;  // Check ownership  require(_pool.getOwner() == address(this), \"BalancerLBPSwapper: contract not pool owner\");  code/contracts/pcv/balancer/BalancerLBPSwapper.sol:L159-L160  IERC20(tokenSpent).approve(address(_vault), type(uint256).max);  IERC20(tokenReceived).approve(address(_vault), type(uint256).max);  Recommendation  protect BalancerLBPSwapper.init() and only allow a trusted entity (e.g. the initial deployer) to call this method.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"
    },
    {
        "title": "4.6 PCVEquityMinter and BalancerLBPSwapper - desynchronisation race ",
        "body": "  Description  There is nothing that prevents other actors from calling BalancerLBPSwapper.swap() afterTime but right before PCVEquityMinter.mint() would as long as the minAmount required for the call to pass is deposited to BalancerLBPSwapper.  instead of taking the newly minted FEI from PCVEquityMinter, existing FEI from the malicious user will be used with the pool. (instead of inflating the token the malicious actor basically pays for it)  the Timed modifiers of both contracts will be out of sync with BalancerLBPSwapper.swap() being reset (and failing until it becomes available again) and PCVEquityMinter.mint() still being available. Furthermore, keeper-scripts (or actors that want to get the incentive) might continue to attempt to mint() while the call will ultimately fail in .swap() due to the resynchronization of timed (unless they simulate the calls first).  Note: There are not a lot of incentives to actually exploit this other than preventing protocol inflation (mint) and potentially griefing users. A malicious user will lose out on the incentivized call and has to ensure that the minAmount required for .swap() to work is available. It is, however, in the best interest of security to defuse the unpredictable racy character of the contract interaction.  Examples  code/contracts/token/PCVEquityMinter.sol:L91-L93  function _afterMint() internal override {  IPCVSwapper(target).swap();  code/contracts/pcv/balancer/BalancerLBPSwapper.sol:L172-L181  function swap() external override afterTime whenNotPaused {  uint256 spentReserves,  uint256 receivedReserves,  uint256 lastChangeBlock  ) = getReserves();  // Ensures no actor can change the pool contents earlier in the block  require(lastChangeBlock < block.number, \"BalancerLBPSwapper: pool changed this block\");  Recommendation  If BalancerLBPSwapper.swap() is only to be called within the flows of action from a PCVEquityMinter.mint() it is suggested to authenticate the call and only let PCVEquityMinter call .swap()  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"
    },
    {
        "title": "4.7 CollateralizationOracleWrapper - the deviation threshold check in update() always returns false ",
        "body": "  Description  A call to update() returns a boolean flag indicating whether the update was performed on outdated data. This flag is being checked in updateIfOutdated() which is typically called by an incentivized keeper function.  There may currently be no incentive (e.g. from the keeper side) to call update() if the values are not outdated but they deviated too much from the target. However, anyone can force an update by calling the non-incentivized public update() method instead.  Examples  code/contracts/oracle/CollateralizationOracleWrapper.sol:L156-L177  require(_validityStatus, \"CollateralizationOracleWrapper: CollateralizationOracle is invalid\");  // set cache variables  cachedProtocolControlledValue = _protocolControlledValue;  cachedUserCirculatingFei = _userCirculatingFei;  cachedProtocolEquity = _protocolEquity;  // reset time  _initTimed();  // emit event  emit CachedValueUpdate(  msg.sender,  cachedProtocolControlledValue,  cachedUserCirculatingFei,  cachedProtocolEquity  );  return outdated  || _isExceededDeviationThreshold(cachedProtocolControlledValue, _protocolControlledValue)  || _isExceededDeviationThreshold(cachedUserCirculatingFei, _userCirculatingFei);  Recommendation  Add unit tests to check for all three return conditions (timed, deviationA, deviationB)  Make sure to compare the current to the stored value before updating the cached values when calling _isExceededDeviationThreshold.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"
    },
    {
        "title": "4.8 ChainlinkOracleWrapper - latestRoundData might return stale results ",
        "body": "  Description  The oracle wrapper calls out to a chainlink oracle receiving the latestRoundData(). It then checks freshness by verifying that the answer is indeed for the last known round. The returned updatedAt timestamp is not checked.  If there is a problem with chainlink starting a new round and finding consensus on the new value for the oracle (e.g. chainlink nodes abandon the oracle, chain congestion, vulnerability/attacks on the chainlink system) consumers of this contract may continue using outdated stale data (if oracles are unable to submit no new round is started)  Examples  code/contracts/oracle/ChainlinkOracleWrapper.sol:L49-L58  /// @notice read the oracle price  /// @return oracle price  /// @return true if price is valid  function read() external view override returns (Decimal.D256 memory, bool) {  (uint80 roundId, int256 price,,, uint80 answeredInRound) = chainlinkOracle.latestRoundData();  bool valid = !paused() && price > 0 && answeredInRound == roundId;  Decimal.D256 memory value = Decimal.from(uint256(price)).div(oracleDecimalsNormalizer);  return (value, valid);  code/contracts/oracle/ChainlinkOracleWrapper.sol:L42-L47  /// @notice determine if read value is stale  /// @return true if read value is stale  function isOutdated() external view override returns (bool) {  (uint80 roundId,,,, uint80 answeredInRound) = chainlinkOracle.latestRoundData();  return answeredInRound != roundId;  Recommendation  Consider checking the oracle responses updatedAt value after calling out to chainlinkOracle.latestRoundData() verifying that the result is within an allowed margin of freshness.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"
    },
    {
        "title": "4.9 CollateralizationOracle - missing events and incomplete event information ",
        "body": "  Description  The CollateralizationOracle.setDepositExclusion function is used to exclude and re-include deposits from collateralization calculations. Unlike the other state-changing functions in this contract, it doesn t emit an event to inform about the exclusion or re-inclusion.  code/contracts/oracle/CollateralizationOracle.sol:L111-L113  function setDepositExclusion(address _deposit, bool _excluded) external onlyGuardianOrGovernor {  excludedDeposits[_deposit] = _excluded;  The DepositAdd event emits not only the deposit address but also the deposit s token. Despite the symmetry, the DepositRemove event does not emit the token.  code/contracts/oracle/CollateralizationOracle.sol:L25-L26  event DepositAdd(address from, address indexed deposit, address indexed token);  event DepositRemove(address from, address indexed deposit);  Recommendation  setDepositInclusion should emit an event that informs about the deposit and whether it was included or excluded.  For symmetry reasons and because it is indeed useful information, the DepositRemove event could include the deposit s token.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"
    },
    {
        "title": "4.10 RateLimited - Contract starts with a full buffer at deployment ",
        "body": "  Description  A contract that inherits from RateLimited starts out with a full buffer when it is deployed.  code/contracts/utils/RateLimited.sol:L35  _bufferStored = _bufferCap;  That means the full bufferCap is immediately available after deployment; it doesn t have to be built up over time. This behavior might be unexpected.  Recommendation  We recommend starting with an empty buffer, or   if there are valid reasons for the current implementation   at least document it clearly.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"
    },
    {
        "title": "4.11 StableSwapOperatorV1 - the contract relies on the 1$ price of every token in 3pool ",
        "body": "  Description  To evaluate the price of the 3pool lp token, the built-in get_virtual_price function is used. This function is supposed to be a manipulation-resistant pricing function that works under the assumption that all the tokens in the pool are worth 1$. If one of the tokens is broken and is priced less, the price is harder to calculate. For example, Chainlink uses the following function to calculate at least the lower boundary of the lp price: https://blog.chain.link/using-chainlink-oracles-to-securely-utilize-curve-lp-pools/  The withdrawal and the controlled value calculation are always made in DAI instead of other stablecoins of the 3pool. So if DAI gets compromised but other tokens aren t, there is no way to switch to them.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"
    },
    {
        "title": "4.12 BalancerLBPSwapper - tokenSpent and tokenReceived should be immutable ",
        "body": "  Description  Acc. to the inline comment both tokenSpent and tokenReceived should be immutable but they are not declared as such.  Examples  code/contracts/pcv/balancer/BalancerLBPSwapper.sol:L92-L94  // tokenSpent and tokenReceived are immutable  tokenSpent = _tokenSpent;  tokenReceived = _tokenReceived;  code/contracts/pcv/balancer/BalancerLBPSwapper.sol:L40-L44  /// @notice the token to be auctioned  address public override tokenSpent;  /// @notice the token to buy  address public override tokenReceived;  Recommendation  Declare both variable immutable.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"
    },
    {
        "title": "4.13 CollateralizationOracle - potentially unsafe casts ",
        "body": "  Description  protocolControlledValue is the cumulative USD token value of all tokens in the PCV. The USD value is determined using external chainlink oracles. To mitigate some effects of attacks on chainlink to propagate to this protocol it is recommended to implement a defensive approach to handling values derived from the external source. Arithm. overflows are checked by the compiler (0.8.4), however, it does not guarantee safe casting from unsigned to signed integer. The scenario of this happening might be rather unlikely, however, there is no guarantee that the external price-feed is not taken over by malicious actors and this is when every line of defense counts.  //solidity 0.8.7  \u00bb  int(uint(2**255))  57896044618658097711785492504343953926634992332820282019728792003956564819968  \u00bb  int(uint(2**255-2))  57896044618658097711785492504343953926634992332820282019728792003956564819966  Examples  code/contracts/oracle/CollateralizationOracle.sol:L327-L327  protocolEquity = int256(protocolControlledValue) - int256(userCirculatingFei);  code/contracts/oracle/CollateralizationOracle.sol:L322-L322  protocolControlledValue += _oraclePrice.mul(_totalTokenBalance).asUint256();  Recommendation  Perform overflow checked SafeCast as another line of defense against oracle manipulation.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"
    },
    {
        "title": "4.14 FeiTimedMinter - constructor does not enforce the same boundaries as setter for frequency ",
        "body": "  Description  The setter method for frequency enforced upper and lower bounds while the constructor does not. Users cannot trust that the frequency is actually set to be within bounds on deployment.  Examples  code/contracts/token/FeiTimedMinter.sol:L32-L48  constructor(  address _core,  address _target,  uint256 _incentive,  uint256 _frequency,  uint256 _initialMintAmount  CoreRef(_core)  Timed(_frequency)  Incentivized(_incentive)  RateLimitedMinter((_initialMintAmount + _incentive) / _frequency, (_initialMintAmount + _incentive), true)  _initTimed();  _setTarget(_target);  _setMintAmount(_initialMintAmount);  code/contracts/token/FeiTimedMinter.sol:L82-L87  function setFrequency(uint256 newFrequency) external override onlyGovernorOrAdmin {  require(newFrequency >= MIN_MINT_FREQUENCY, \"FeiTimedMinter: frequency low\");  require(newFrequency <= MAX_MINT_FREQUENCY, \"FeiTimedMinter: frequency high\");  _setDuration(newFrequency);  Recommendation  Perform the same checks on frequency in the constructor as in the setFrequency method.  This contract is also inherited by a range of contracts that might specify different boundaries to what is hardcoded in the FeiTimedMinter. A way to enforce bounds-checks could be to allow overriding the setter method and using the setter in the constructor as well ensuring that bounds are also checked on deployment.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"
    },
    {
        "title": "4.15 CollateralizationOracle - swapDeposit should call internal functions to remove/add deposits ",
        "body": "  Description  Examples  code/contracts/oracle/CollateralizationOracle.sol:L191-L198  /// @notice Swap a PCVDeposit with a new one, for instance when a new version  ///         of a deposit (holding the same token) is deployed.  /// @param _oldDeposit : the PCVDeposit to remove from the list.  /// @param _newDeposit : the PCVDeposit to add to the list.  function swapDeposit(address _oldDeposit, address _newDeposit) external onlyGovernor {  removeDeposit(_oldDeposit);  addDeposit(_newDeposit);  Recommendation  Call the internal functions instead. addDeposit s and removeDeposit s visibility can then be changed from public to external.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"
    },
    {
        "title": "4.16 CollateralizationOracle - misleading comments ",
        "body": "  Description  According to an inline comment in isOvercollateralized, the validity status of pcvStats is ignored, while it is actually being checked.  Similarly, a comment in pcvStats mentions that the returned protocolEquity is 0 if there is less PCV than circulating FEI, while in reality, pcvStats always returns the difference between the former and the latter, even if it is negative.  Examples  code/contracts/oracle/CollateralizationOracle.sol:L332-L339  ///         Controlled Value) than the circulating (user-owned) FEI, i.e.  ///         a positive Protocol Equity.  ///         Note: the validity status is ignored in this function.  function isOvercollateralized() external override view whenNotPaused returns (bool) {  (,, int256 _protocolEquity, bool _valid) = pcvStats();  require(_valid, \"CollateralizationOracle: reading is invalid\");  return _protocolEquity > 0;  code/contracts/oracle/CollateralizationOracle.sol:L283-L284  /// @return protocolEquity : the difference between PCV and user circulating FEI.  ///         If there are more circulating FEI than $ in the PCV, equity is 0.  code/contracts/oracle/CollateralizationOracle.sol:L327  protocolEquity = int256(protocolControlledValue) - int256(userCirculatingFei);  Recommendation  Revise the comments.  5 Recommendations  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"
    },
    {
        "title": "5.1 Update Natspec",
        "body": "  Examples  token is not in natspec  code/contracts/pcv/utils/ERC20Splitter.sol:L6-L28  /// @notice a contract to split token held to multiple locations  contract ERC20Splitter is PCVSplitter {  /// @notice token to split  IERC20 public token;  /**  @notice constructor for ERC20Splitter  @param _core the Core address to reference  @param _pcvDeposits the locations to send tokens  @param _ratios the relative ratios of how much tokens to send each location, in basis points  /  constructor(  address _core,  IERC20 _token,  address[] memory _pcvDeposits,  uint256[] memory _ratios  CoreRef(_core)  PCVSplitter(_pcvDeposits, _ratios)  token = _token;  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"
    },
    {
        "title": "5.2 TribeReserveStabilizer - different minting procedures",
        "body": "  Description  The TRIBE token doesn t have a burn functionality. TRIBE that is supposed to be taken out of circulation is sent to the TribeReserveStabilizer contract, and when that contract has to mint new TRIBE in exchange for FEI, it will first use up the currently held TRIBE balance before actually minting new tokens.  code/contracts/stabilizer/TribeReserveStabilizer.sol:L117-L133  // Transfer held TRIBE first, then mint to cover remainder  function _transfer(address to, uint256 amount) internal override {  _depleteBuffer(amount);  uint256 _tribeBalance = balance();  uint256 mintAmount = amount;  if(_tribeBalance != 0) {  uint256 transferAmount = Math.min(_tribeBalance, amount);  _withdrawERC20(address(token), to, transferAmount);  mintAmount = mintAmount - transferAmount;  assert(mintAmount + transferAmount == amount);  if (mintAmount != 0) {  _mint(to, mintAmount);  The contract also has a mint function that allows the Governor to mint new TRIBE. Unlike the exchangeFei function described above, this function does not first utilize TRIBE held in the contract but directly instructs the token contract to mint the entire amount.  code/contracts/stabilizer/TribeReserveStabilizer.sol:L102-L107  /// @notice mints TRIBE to the target address  /// @param to the address to send TRIBE to  /// @param amount the amount of TRIBE to send  function mint(address to, uint256 amount) external override onlyGovernor {  _mint(to, amount);  code/contracts/stabilizer/TribeReserveStabilizer.sol:L135-L138  function _mint(address to, uint256 amount) internal {  ITribe _tribe = ITribe(address(token));  _tribe.mint(to, amount);  Recommendation  It would make sense and be more consistent with exchangeFei if the mint function first used TRIBE held in the contract before actually minting new tokens.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"
    },
    {
        "title": "4.1 Potential Reentrancy Into Strategies ",
        "body": "  Resolution  EigenLabs Quick Summary: The StrategyBase contract may be vulnerable to a token contract that employs some sort of callback to a function like sharesToUnderlyingView, before the balance change is reflected in the contract. The shares have been decremented, which would lead to an incorrect return value from sharesToUnderlyingView.  EigenLabs Response: As noted in the report, this is not an issue if the token contract being used does not allow for reentrancy. For now, we will make it clear both in the contracts as well as the docs that our implementation of StrategyBase.sol does not support tokens with reentrancy. Because of the way our system is designed, anyone can choose to design a strategy with this in mind!  Description  Nevertheless, other functions could be reentered, for example, sharesToUnderlyingView and underlyingToSharesView, as well as their (supposedly) non-view counterparts.  Let s look at the withdraw function in StrategyBase. First, the amountShares shares are burnt, and at the end of the function, the equivalent amount of token is transferred to the depositor:  src/contracts/strategies/StrategyBase.sol:L108-L143  function withdraw(address depositor, IERC20 token, uint256 amountShares)  external  virtual  override  onlyWhenNotPaused(PAUSED_WITHDRAWALS)  onlyStrategyManager  require(token == underlyingToken, \"StrategyBase.withdraw: Can only withdraw the strategy token\");  // copy `totalShares` value to memory, prior to any decrease  uint256 priorTotalShares = totalShares;  require(  amountShares <= priorTotalShares,  \"StrategyBase.withdraw: amountShares must be less than or equal to totalShares\"  );  // Calculate the value that `totalShares` will decrease to as a result of the withdrawal  uint256 updatedTotalShares = priorTotalShares - amountShares;  // check to avoid edge case where share rate can be massively inflated as a 'griefing' sort of attack  require(updatedTotalShares >= MIN_NONZERO_TOTAL_SHARES || updatedTotalShares == 0,  \"StrategyBase.withdraw: updated totalShares amount would be nonzero but below MIN_NONZERO_TOTAL_SHARES\");  // Actually decrease the `totalShares` value  totalShares = updatedTotalShares;  /**  @notice calculation of amountToSend *mirrors* `sharesToUnderlying(amountShares)`, but is different since the `totalShares` has already  been decremented. Specifically, notice how we use `priorTotalShares` here instead of `totalShares`.  /  uint256 amountToSend;  if (priorTotalShares == amountShares) {  amountToSend = _tokenBalance();  } else {  amountToSend = (_tokenBalance() * amountShares) / priorTotalShares;  underlyingToken.safeTransfer(depositor, amountToSend);  If we assume that the token contract has a callback to the recipient of the transfer before the actual balance changes take place, then the recipient could reenter the strategy contract, for example, in sharesToUnderlyingView:  src/contracts/strategies/StrategyBase.sol:L159-L165  function sharesToUnderlyingView(uint256 amountShares) public view virtual override returns (uint256) {  if (totalShares == 0) {  return amountShares;  } else {  return (_tokenBalance() * amountShares) / totalShares;  The crucial point is: If the callback is executed before the actual balance change, then sharesToUnderlyingView will report a bad result because the shares have already been burnt but the token balance has not been updated yet.  For deposits, the token transfer to the strategy happens first, and the shares are minted after that:  src/contracts/core/StrategyManager.sol:L643-L652  function _depositIntoStrategy(address depositor, IStrategy strategy, IERC20 token, uint256 amount)  internal  onlyStrategiesWhitelistedForDeposit(strategy)  returns (uint256 shares)  // transfer tokens from the sender to the strategy  token.safeTransferFrom(msg.sender, address(strategy), amount);  // deposit the assets into the specified strategy and get the equivalent amount of shares in that strategy  shares = strategy.deposit(token, amount);  src/contracts/strategies/StrategyBase.sol:L69-L99  function deposit(IERC20 token, uint256 amount)  external  virtual  override  onlyWhenNotPaused(PAUSED_DEPOSITS)  onlyStrategyManager  returns (uint256 newShares)  require(token == underlyingToken, \"StrategyBase.deposit: Can only deposit underlyingToken\");  /**  @notice calculation of newShares *mirrors* `underlyingToShares(amount)`, but is different since the balance of `underlyingToken`  has already been increased due to the `strategyManager` transferring tokens to this strategy prior to calling this function  /  uint256 priorTokenBalance = _tokenBalance() - amount;  if (priorTokenBalance == 0 || totalShares == 0) {  newShares = amount;  } else {  newShares = (amount * totalShares) / priorTokenBalance;  // checks to ensure correctness / avoid edge case where share rate can be massively inflated as a 'griefing' sort of attack  require(newShares != 0, \"StrategyBase.deposit: newShares cannot be zero\");  uint256 updatedTotalShares = totalShares + newShares;  require(updatedTotalShares >= MIN_NONZERO_TOTAL_SHARES,  \"StrategyBase.deposit: updated totalShares amount would be nonzero but below MIN_NONZERO_TOTAL_SHARES\");  // update total share amount  totalShares = updatedTotalShares;  return newShares;  That means if there is a callback in the token s transferFrom function and it is executed after the balance change, a reentering call to sharesToUnderlyingView (for example) will again return a wrong result because shares and token balances are not  in sync.   In addition to the reversed order of token transfer and shares update, there s another vital difference between withdraw and deposit: For withdrawals, the call to the token contract originates in the strategy, while for deposits, it is the strategy manager that initiates the call to the token contract (before calling into the strategy). That s a technicality that has consequences for reentrancy protection: Note that for withdrawals, it is the strategy contract that is reentered, while for deposits, there is not a single contract that is reentered; instead, it is the contract system that is in an inconsistent state when the reentrancy happens. Hence, reentrancy protection on the level of individual contracts is not sufficient.  src/contracts/core/StrategyManager.sol:L244-L286  function depositIntoStrategyWithSignature(  IStrategy strategy,  IERC20 token,  uint256 amount,  address staker,  uint256 expiry,  bytes memory signature  external  onlyWhenNotPaused(PAUSED_DEPOSITS)  onlyNotFrozen(staker)  nonReentrant  returns (uint256 shares)  require(  expiry >= block.timestamp,  \"StrategyManager.depositIntoStrategyWithSignature: signature expired\"  );  // calculate struct hash, then increment `staker`'s nonce  uint256 nonce = nonces[staker];  bytes32 structHash = keccak256(abi.encode(DEPOSIT_TYPEHASH, strategy, token, amount, nonce, expiry));  unchecked {  nonces[staker] = nonce + 1;  bytes32 digestHash = keccak256(abi.encodePacked(\"\\x19\\x01\", DOMAIN_SEPARATOR, structHash));  /**  check validity of signature:  1) if `staker` is an EOA, then `signature` must be a valid ECSDA signature from `staker`,  indicating their intention for this action  2) if `staker` is a contract, then `signature` must will be checked according to EIP-1271  /  if (Address.isContract(staker)) {  require(IERC1271(staker).isValidSignature(digestHash, signature) == ERC1271_MAGICVALUE,  \"StrategyManager.depositIntoStrategyWithSignature: ERC1271 signature verification failed\");  } else {  require(ECDSA.recover(digestHash, signature) == staker,  \"StrategyManager.depositIntoStrategyWithSignature: signature not from staker\");  shares = _depositIntoStrategy(staker, strategy, token, amount);  Hence, querying the staker s nonce in reentrancy would still give a result based on an  incomplete state change.  It is, for example, conceivable that the staker still has zero shares, and yet their nonce is already 1. This particular situation is most likely not an issue, but the example shows that reentrancy can be subtle.  Recommendation  This is fine if the token doesn t allow reentrancy in the first place. As discussed above, among the tokens that do allow reentrancy, some variants of when reentrancy can happen in relation to state changes in the token seem more dangerous than others, but we have also argued that this kind of reasoning can be dangerous and error-prone. Hence, we recommend employing comprehensive and defensive reentrancy protection based on reentrancy guards such as OpenZeppelin s ReentrancyGuardUpgradeable, which is already used in the StrategyManager.  Unfortunately, securing a multi-contract system against reentrancy can be challenging, but we hope the preceding discussion and the following pointers will prove helpful:  External functions in strategies that should only be callable by the strategy manager (such as deposit and withdraw) should have the onlyStrategyManager modifier. This is already the case in the current codebase and is listed here only for completeness.  External functions in strategies for which item 1 doesn t apply (such as sharesToUnderlying and underlyingToShares) should query the strategy manager s reentrancy lock and revert if it is set.  In principle, the restrictions above also apply to public functions, but if a public function is also used internally, checks against reentrancy can cause problems (if used in an internal context) or at least be redundant. In the context of reentrancy protection, it is often easier to split public functions into an internal and an external one.  If view functions are supposed to give reliable results (either internally   which is typically the case   or for other contracts), they have to be protected too.  The previous item also applies to the StrategyManager: view functions that have to provide correct results should query the reentrancy lock and revert if it is set.  Solidity automatically generates getters for public state variables. Again, if these (external view) functions must deliver correct results, the same measures must be taken as for explicit view functions. In practice, the state variable has to become internal or private, and the getter function must be hand-written.  The StrategyBase contract provides some basic functionality. Concrete strategy implementations can inherit from this contract, meaning that some functions may be overridden (and might or might not call the overridden version via super), and new functions might be added. While the guidelines above should be helpful, derived contracts must be reviewed and assessed separately on a case-by-case basis. As mentioned before, reentrancy protection can be challenging, especially in a multi-contract system.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"
    },
    {
        "title": "4.2 StrategyBase   Inflation Attack Prevention Can Lead to Stuck Funds ",
        "body": "  Resolution  EigenLabs Quick Summary: The StrategyBase contract sets a minimum initial deposit amount of 1e9. This is to mitigate ERC-4626 related inflation attacks, where an attacker can front-run a deposit, inflating the exchange rate between tokens and shares. A consequence of that protection is that any amount less than 1e9 is not withdrawable.  EigenLabs Response: We recognize that this may be notable for tokens such as USDC, where the smallest unit of which is 1e-6. For now, we will make it clear both in the contracts as well as the docs that our implementation of StrategyBase.sol makes this assumption about the tokens being used in the strategy.  Description  As a defense against what has come to be known as inflation or donation attack in the context of ERC-4626, the StrategyBase contract   from which concrete strategy implementations are supposed to inherit   enforces that the amount of shares in existence for a particular strategy is always either 0 or at least a certain minimum amount that is set to 10^9. This mitigates inflation attacks, which require a small total supply of shares to be effective.  src/contracts/strategies/StrategyBase.sol:L92-L95  uint256 updatedTotalShares = totalShares + newShares;  require(updatedTotalShares >= MIN_NONZERO_TOTAL_SHARES,  \"StrategyBase.deposit: updated totalShares amount would be nonzero but below MIN_NONZERO_TOTAL_SHARES\");  src/contracts/strategies/StrategyBase.sol:L123-L127  // Calculate the value that `totalShares` will decrease to as a result of the withdrawal  uint256 updatedTotalShares = priorTotalShares - amountShares;  // check to avoid edge case where share rate can be massively inflated as a 'griefing' sort of attack  require(updatedTotalShares >= MIN_NONZERO_TOTAL_SHARES || updatedTotalShares == 0,  \"StrategyBase.withdraw: updated totalShares amount would be nonzero but below MIN_NONZERO_TOTAL_SHARES\");  This particular approach has the downside that, in the worst case, a user may be unable to withdraw the underlying asset for up to 10^9 - 1 shares. While the extreme circumstances under which this can happen might be unlikely to occur in a realistic setting and, in many cases, the value of 10^9 - 1 shares may be negligible, this is not ideal.  Recommendation  It isn t easy to give a good general recommendation. None of the suggested mitigations are without a downside, and what s the best choice may also depend on the specific situation. We do, however, feel that alternative approaches that can t lead to stuck funds might be worth considering, especially for a default implementation.  One option is internal accounting, i.e., the strategy keeps track of the number of underlying tokens it owns. It uses this number for conversion rate calculation instead of its balance in the token contract. This avoids the donation attack because sending tokens directly to the strategy will not affect the conversion rate. Moreover, this technique helps prevent reentrancy issues when the EigenLayer state is out of sync with the token contract s state. The downside is higher gas costs and that donating by just sending tokens to the contract is impossible; more specifically, if it happens accidentally, the funds are lost unless there s some special mechanism to recover them.  An alternative approach with virtual shares and assets is presented here, and the document lists pointers to more discussions and proposed solutions.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"
    },
    {
        "title": "4.3 StrategyWrapper   Functions Shouldn t Be virtual (Out of Scope) ",
        "body": "  Resolution  EigenLabs Quick Summary: The documentation assumes that StrategyWrapper.sol shouldn t be inheritable, and yet its functions are marked  virtual . It should be noted that this contract was not audited beyond this issue which was noticed accidentally.  EigenLabs Response: This fix was acknowledged and fixed in the following commit: 053ee9d.  Description  The StrategyWrapper contract is a straightforward strategy implementation and   as its NatSpec documentation explicitly states   is not designed to be inherited from:  src/contracts/strategies/StrategyWrapper.sol:L8-L17  /**  @title Extremely simple implementation of `IStrategy` interface.  @author Layr Labs, Inc.  @notice Simple, basic, \"do-nothing\" Strategy that holds a single underlying token and returns it on withdrawals.  Assumes shares are always 1-to-1 with the underlyingToken.  @dev Unlike `StrategyBase`, this contract is *not* designed to be inherited from.  @dev This contract is expressly *not* intended for use with 'fee-on-transfer'-type tokens.  Setting the `underlyingToken` to be a fee-on-transfer token may result in improper accounting.  /  contract StrategyWrapper is IStrategy {  However, all functions in this contract are virtual, which only makes sense if inheriting from StrategyWrapper is possible.  Recommendation  Assuming the NatSpec documentation is correct, and no contract should inherit from StrategyWrapper, remove the virtual keyword from all function definitions. Otherwise, fix the documentation.  Remark  This contract is out of scope, and this finding is only included because we noticed it accidentally. This does not mean we have reviewed the contract or other out-of-scope files.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"
    },
    {
        "title": "4.4 StrategyBase   Inheritance-Related Issues ",
        "body": "  Resolution  60141d8.  Description  src/contracts/interfaces/IStrategy.sol:L39-L45  /**  @notice Used to convert an amount of underlying tokens to the equivalent amount of shares in this strategy.  @notice In contrast to `underlyingToSharesView`, this function **may** make state modifications  @param amountUnderlying is the amount of `underlyingToken` to calculate its conversion into strategy shares  @dev Implementation for these functions in particular may vary signifcantly for different strategies  /  function underlyingToShares(uint256 amountUnderlying) external view returns (uint256);  src/contracts/strategies/StrategyBase.sol:L192-L200  /**  @notice Used to convert an amount of underlying tokens to the equivalent amount of shares in this strategy.  @notice In contrast to `underlyingToSharesView`, this function **may** make state modifications  @param amountUnderlying is the amount of `underlyingToken` to calculate its conversion into strategy shares  @dev Implementation for these functions in particular may vary signifcantly for different strategies  /  function underlyingToShares(uint256 amountUnderlying) external view virtual returns (uint256) {  return underlyingToSharesView(amountUnderlying);  src/contracts/strategies/StrategyBase.sol:L167-L175  /**  @notice Used to convert a number of shares to the equivalent amount of underlying tokens for this strategy.  @notice In contrast to `sharesToUnderlyingView`, this function **may** make state modifications  @param amountShares is the amount of shares to calculate its conversion into the underlying token  @dev Implementation for these functions in particular may vary signifcantly for different strategies  /  function sharesToUnderlying(uint256 amountShares) public view virtual override returns (uint256) {  return sharesToUnderlyingView(amountShares);  B. The initialize function in the StrategyBase contract is not virtual, which means the name will not be available in derived contracts (unless with different parameter types). It also has the initializer modifier, which is unavailable in concrete strategies inherited from StrategyBase.  Recommendation  A. If state-changing versions of the conversion functions are needed, the view modifier has to be removed from IStrategy.underlyingToShares, StrategyBase.underlyingToShares, and StrategyBase.sharesToUnderlying. They should be removed entirely from the interface and base contract if they re not needed.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"
    },
    {
        "title": "4.5 StrategyManager - Cross-Chain Replay Attacks After Chain Split Due to Hard-Coded DOMAIN_SEPARATOR ",
        "body": "  Resolution  EigenLabs Quick Summary: A. For the implementation of EIP-712 signatures, the domain separator is set in the initialization of the contract, which includes the chain ID. In the case of a chain split, this ID is subject to change. Thus the domain separator must be recomputed. B. The domain separator is calculated using bytes(\"EigenLayer\")   the EIP-712 spec requires a keccak256 hash, i.e. keccak256(bytes(\"EigenLayer\")). C. The EIP712Domain does not include a version string.  EigenLabs Response: A. We have modified our implementation to dynamically check for the chain ID. If we detect a change since initialization, we recompute the domain separator. If not, we use the precomputed value. B. We changed our computation to use keccak256(bytes(\"EigenLayer\")). C. We decided that we would forgo this change for the time being. Changes in A. and B. implemented in this commit: 714dbb6.  Description  A. The StrategyManager contract allows stakers to deposit into and withdraw from strategies. A staker can either deposit themself or have someone else do it on their behalf, where the latter requires an EIP-712-compliant signature. The EIP-712 domain separator is computed in the initialize function and stored in a state variable for later retrieval:  src/contracts/core/StrategyManagerStorage.sol:L23-L24  /// @notice EIP-712 Domain separator  bytes32 public DOMAIN_SEPARATOR;  src/contracts/core/StrategyManager.sol:L149-L153  function initialize(address initialOwner, address initialStrategyWhitelister, IPauserRegistry _pauserRegistry, uint256 initialPausedStatus, uint256 _withdrawalDelayBlocks)  external  initializer  DOMAIN_SEPARATOR = keccak256(abi.encode(DOMAIN_TYPEHASH, bytes(\"EigenLayer\"), block.chainid, address(this)));  Once set in the initialize function, the value can t be changed anymore. In particular, the chain ID is  baked into  the DOMAIN_SEPARATOR during initialization. However, it is not necessarily constant: In the event of a chain split, only one of the resulting chains gets to keep the original chain ID, and the other should use a new one. With the current approach to compute the DOMAIN_SEPARATOR during initialization, store it, and then use the stored value for signature verification, a signature will be valid on both chains after a split   but it should not be valid on the chain with the new ID. Hence, the domain separator should be computed dynamically.  B. The name in the EIP712Domain is of type string:  src/contracts/core/StrategyManagerStorage.sol:L18-L19  bytes32 public constant DOMAIN_TYPEHASH =  keccak256(\"EIP712Domain(string name,uint256 chainId,address verifyingContract)\");  What s encoded when the domain separator is computed is bytes(\"EigenLayer\"):  src/contracts/core/StrategyManager.sol:L153  DOMAIN_SEPARATOR = keccak256(abi.encode(DOMAIN_TYPEHASH, bytes(\"EigenLayer\"), block.chainid, address(this)));  According to EIP-712,  The dynamic values bytes and string are encoded as a keccak256 hash of their contents.  Hence, bytes(\"EigenLayer\") should be replaced with keccak256(bytes(\"EigenLayer\")).  C. The EIP712Domain does not include a version string:  src/contracts/core/StrategyManagerStorage.sol:L18-L19  bytes32 public constant DOMAIN_TYPEHASH =  keccak256(\"EIP712Domain(string name,uint256 chainId,address verifyingContract)\");  That is allowed according to the specification. However, given that most, if not all, projects, as well as OpenZeppelin s EIP-712 implementation, do include a version string in their EIP712Domain, it might be a pragmatic choice to do the same, perhaps to avoid potential incompatibilities.  Recommendation  Individual recommendations have been given above. Alternatively, you might want to utilize OpenZeppelin s EIP712Upgradeable library, which will take care of these issues. Note that some of these changes will break existing signatures.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"
    },
    {
        "title": "4.6 StrategyManagerStorage   Miscalculated Gap Size ",
        "body": "  Resolution  EigenLabs Quick Summary: Gap size in StrategyManagerStorage is set to 41 even though 10 slots are utilized. General convention is to have 50 total slots available for storage.  EigenLabs Response: Storage gap size fixed, changed from 41 to 40. This is possible as we aren t storing anything after the gap. Commit hash: d249641.  Description  Upgradeable contracts should have a  gap  of unused storage slots at the end to allow for adding state variables when the contract is upgraded. The convention is to have a gap whose size adds up to 50 with the used slots at the beginning of the contract s storage.  In StrategyManagerStorage, the number of consecutively used storage slots is 10:  DOMAIN_SEPARATOR  nonces  strategyWhitelister  withdrawalDelayBlocks  stakerStrategyShares  stakerStrategyList  withdrawalRootPending  numWithdrawalsQueued  strategyIsWhitelistedForDeposit  beaconChainETHSharesToDecrementOnWithdrawal  However, the gap size in the storage contract is 41:  src/contracts/core/StrategyManagerStorage.sol:L84  uint256[41] private __gap;  Recommendation  If you don t have to maintain compatibility with an existing deployment, we recommend reducing the storage gap size to 40. Otherwise, we recommend adding a comment explaining that, in this particular case, the gap size and the used storage slots should add up to 51 instead of 50 and that this invariant has to be maintained in future versions of this contract.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"
    },
    {
        "title": "4.7 Unnecessary Usage of BytesLib ",
        "body": "  Resolution  EigenLabs Quick Summary: Several contracts import BytesLib.sol and yet do not use it.  6164a12.  Description  Various contracts throughout the codebase import BytesLib.sol without actually using it. It is only utilized once in EigenPod to convert a bytes array of length 32 to a bytes32.  src/contracts/pods/EigenPod.sol:L189-L190  require(validatorFields[BeaconChainProofs.VALIDATOR_WITHDRAWAL_CREDENTIALS_INDEX] == _podWithdrawalCredentials().toBytes32(0),  \"EigenPod.verifyCorrectWithdrawalCredentials: Proof is not for this EigenPod\");  However, this can also be achieved with an explicit conversion to bytes32, and means provided by the language itself should usually be preferred over external libraries.  Recommendation  Remove the import of BytesLib.sol and the accompanying using BytesLib for bytes; when the library is unused. Consider replacing the single usage in EigenPod with an explicit conversion to bytes32, and consequentially removing the import and using statements.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"
    },
    {
        "title": "4.8 DelayedWithdrawalRouter   Anyone Can Claim Earlier Than Intended on Behalf of Someone Else",
        "body": "  Resolution  EigenLabs Quick Summary: DelayedWithdrawalRouter implements a claiming function that allows for a provided recipient address. This function is not permissioned and thus can be called at any time (potentially a griefing attack vector with something like taxes).  EigenLabs Response: For now we have decided to leave this function as is and have added a note/warning about the noted behavior.  Description  The DelayedWithdrawalRouter has two functions to claim delayed withdrawals: one the recipient can call themself (i.e., the recipient is msg.sender) and one to claim on behalf of someone else (i.e., the recipient is given as a parameter):  src/contracts/pods/DelayedWithdrawalRouter.sol:L84-L94  /**  @notice Called in order to withdraw delayed withdrawals made to the caller that have passed the `withdrawalDelayBlocks` period.  @param maxNumberOfDelayedWithdrawalsToClaim Used to limit the maximum number of delayedWithdrawals to loop through claiming.  /  function claimDelayedWithdrawals(uint256 maxNumberOfDelayedWithdrawalsToClaim)  external  nonReentrant  onlyWhenNotPaused(PAUSED_DELAYED_WITHDRAWAL_CLAIMS)  _claimDelayedWithdrawals(msg.sender, maxNumberOfDelayedWithdrawalsToClaim);  src/contracts/pods/DelayedWithdrawalRouter.sol:L71-L82  /**  @notice Called in order to withdraw delayed withdrawals made to the `recipient` that have passed the `withdrawalDelayBlocks` period.  @param recipient The address to claim delayedWithdrawals for.  @param maxNumberOfDelayedWithdrawalsToClaim Used to limit the maximum number of delayedWithdrawals to loop through claiming.  /  function claimDelayedWithdrawals(address recipient, uint256 maxNumberOfDelayedWithdrawalsToClaim)  external  nonReentrant  onlyWhenNotPaused(PAUSED_DELAYED_WITHDRAWAL_CLAIMS)  _claimDelayedWithdrawals(recipient, maxNumberOfDelayedWithdrawalsToClaim);  An attacker can not control where the funds are sent, but they can control when the funds are sent once the withdrawal becomes claimable. It is unclear whether this can become a problem. It is, for example, conceivable that there are negative tax implications if funds arrive earlier than intended at a particular address: For instance, disadvantages for the recipient can arise if funds arrive before the new year starts or before some other funds were sent to the same address (e.g., in case the FIFO principle is applied for taxes).  The downside of allowing only the recipient to claim their withdrawals is that contract recipients must be equipped to make the claim.  Recommendation  If these points haven t been considered yet, we recommend doing so.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"
    },
    {
        "title": "4.9 Consider Using Custom Errors",
        "body": "  Resolution  EigenLabs Quick Summary: Suggestion to use custom errors in Solidity.  EigenLabs Response: For now we will continue to use standard error messages for the upcoming deployment.  Description and Recommendation  Custom errors were introduced in Solidity version 0.8.4 and have some advantages over the traditional string-based errors: They are usually more gas-efficient, especially regarding deployment costs, and it is easier to include dynamic information in error messages.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"
    },
    {
        "title": "4.10 StrategyManager - Immediate Settings Changes Can Have Unintended Side Effects",
        "body": "  Resolution  EigenLabs Quick Summary: StrategyManager contract allows for setWithdrawalDelayBlocks() to be called such that a user attempting to call completeQueuedWithdrawal() may be prevented from withdrawing.  EigenLabs Response: We have decided to leave this concern unaddressed.  Description  Withdrawal delay blocks can be changed immediately:  src/contracts/core/StrategyManager.sol:L570-L572  function setWithdrawalDelayBlocks(uint256 _withdrawalDelayBlocks) external onlyOwner {  _setWithdrawalDelayBlocks(_withdrawalDelayBlocks);  Allows owner to sandwich e.g. completeQueuedWithdrawal function calls to prevent users from withdrawing their stake due to the following check:  src/contracts/core/StrategyManager.sol:L749-L753  require(queuedWithdrawal.withdrawalStartBlock + withdrawalDelayBlocks <= block.number  || queuedWithdrawal.strategies[0] == beaconChainETHStrategy,  \"StrategyManager.completeQueuedWithdrawal: withdrawalDelayBlocks period has not yet passed\"  );  Recommendation  We recommend introducing a simple delay for settings changes to prevent sandwiching attack vectors. It is worth noting that this finding has been explicitly raised because it allows authorized personnel to target individual transactions and users by extension.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"
    },
    {
        "title": "4.11 StrategyManager - Unused Modifier",
        "body": "  Resolution  EigenLabs Quick Summary: StrategyManager contract defines an onlyEigenPod modifier which is not used.  EigenLabs Response: We have removed that modifier. Commit hash: 50bb6a8.  Description  The StrategyManager contract defines an onlyEigenPod modifier, but it is never used.  src/contracts/core/StrategyManager.sol:L113-L116  modifier onlyEigenPod(address podOwner, address pod) {  require(address(eigenPodManager.getPod(podOwner)) == pod, \"StrategyManager.onlyEigenPod: not a pod\");  _;  Recommendation  The modifier can be removed.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"
    },
    {
        "title": "4.12 Inconsistent Data Types for Block Numbers",
        "body": "  Resolution  EigenLabs Quick Summary: Across our contracts we use both uint32 and uint64 to represent blockNumber values.  Description  The block number attribute is declared using uint32 and uint64. This usage should be more consistent.  Examples  uint32  src/contracts/interfaces/IEigenPod.sol:L30-L35  struct PartialWithdrawalClaim {  PARTIAL_WITHDRAWAL_CLAIM_STATUS status;  // block at which the PartialWithdrawalClaim was created  uint32 creationBlockNumber;  // last block (inclusive) in which the PartialWithdrawalClaim can be fraudproofed  uint32 fraudproofPeriodEndBlockNumber;  src/contracts/core/StrategyManager.sol:L393  withdrawalStartBlock: uint32(block.number),  src/contracts/pods/DelayedWithdrawalRouter.sol:L62-L65  DelayedWithdrawal memory delayedWithdrawal = DelayedWithdrawal({  amount: withdrawalAmount,  blockCreated: uint32(block.number)  });  uint64  src/contracts/pods/EigenPod.sol:L175-L176  function verifyWithdrawalCredentialsAndBalance(  uint64 oracleBlockNumber,  src/contracts/pods/EigenPodManager.sol:L216  function getBeaconChainStateRoot(uint64 blockNumber) external view returns(bytes32) {  Recommendation  Use one data type consistently to minimize the risk of conversion errors or truncation during casts. This is a measure aimed at future-proofing the code base.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"
    },
    {
        "title": "4.13 EigenPod   Stray nonReentrant Modifier",
        "body": "  Resolution  EigenLabs Quick Summary: There is a stray nonReentrant modifier in EigenPod.sol.  EigenLabs Response: Modifier has been removed. Commit hash: 9f45837.  Description  There is a stray nonReentrant modifier in EigenPod:  src/contracts/pods/EigenPod.sol:L432-L453  /**  @notice Transfers `amountWei` in ether from this contract to the specified `recipient` address  @notice Called by EigenPodManager to withdrawBeaconChainETH that has been added to the EigenPod's balance due to a withdrawal from the beacon chain.  @dev Called during withdrawal or slashing.  @dev Note that this function is marked as non-reentrant to prevent the recipient calling back into it  /  function withdrawRestakedBeaconChainETH(  address recipient,  uint256 amountWei  external  onlyEigenPodManager  nonReentrant  // reduce the restakedExecutionLayerGwei  restakedExecutionLayerGwei -= uint64(amountWei / GWEI_TO_WEI);  emit RestakedBeaconChainETHWithdrawn(recipient, amountWei);  // transfer ETH from pod to `recipient`  _sendETH(recipient, amountWei);  src/contracts/pods/EigenPod.sol:L466-L468  function _sendETH(address recipient, uint256 amountWei) internal {  delayedWithdrawalRouter.createDelayedWithdrawal{value: amountWei}(podOwner, recipient);  This modifier is likely a leftover from an earlier version of the contract in which Ether was sent directly.  Recommendation  Remove the ineffective modifier for more readable code and reduced gas usage. The import of ReentrancyGuardUpgradeable.sol and the inheritance from ReentrancyGuardUpgradeable can also be removed. Also, consider giving the _sendEth function a more appropriate name.  Remark  The two nonReentrant modifiers in DelayedWithdrawalRouter are neither strictly needed. Still, they look considerably less  random  than the one in EigenPod, and could be warranted by a defense-in-depth approach.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"
    },
    {
        "title": "4.1 RPC starkNet_sendTransaction - The User Displayed Message Generated With getSigningTxnText() Is Prone to Markdown/Control Chars Injection From contractCallData    ",
        "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by rendering untrusted user input with the copyable UI component, preventing markdown injection. Additionally, the client provided the following statement:  restructure dialog ui by using MM copyable field, it can ignore any markdown or tag block  validate send transaction calldata has to be able to convert to bigInt  Description  In the code snippet below, contractCallData is potentially untrusted and may contain Markdown renderable strings or strings containing Control Characters that break the context of the message displayed to the user. This can lead to misrepresenting the transaction data to be signed, which should be avoided.  packages/starknet-snap/src/utils/snapUtils.ts:L163-L195  export function getSigningTxnText(  state: SnapState,  contractAddress: string,  contractFuncName: string,  contractCallData: string[],  senderAddress: string,  maxFee: number.BigNumberish,  network: Network,  ): string {  // Retrieve the ERC-20 token from snap state for confirmation display purpose  const token = getErc20Token(state, contractAddress, network.chainId);  let tokenTransferStr = '';  if (token && contractFuncName === 'transfer') {  try {  let amount = '';  if ([3, 6, 9, 12, 15, 18].includes(token.decimals)) {  amount = convert(contractCallData[1], -1 * token.decimals, 'ether');  } else {  amount = (Number(contractCallData[1]) * Math.pow(10, -1 * token.decimals)).toFixed(token.decimals);  tokenTransferStr = `\\n\\nSender Address: ${senderAddress}\\n\\nRecipient Address: ${contractCallData[0]}\\n\\nAmount(${token.symbol}): ${amount}`;  } catch (err) {  console.error(`getSigningTxnText: error found in amount conversion: ${err}`);  return (  `Contract: ${contractAddress}\\n\\nCall Data: [${contractCallData.join(', ')}]\\n\\nEstimated Gas Fee(ETH): ${convert(  maxFee,  'wei',  'ether',  )}\\n\\nNetwork: ${network.name}` + tokenTransferStr  );  packages/starknet-snap/src/sendTransaction.ts:L60-L80  const signingTxnText = getSigningTxnText(  state,  contractAddress,  contractFuncName,  contractCallData,  senderAddress,  maxFee,  network,  );  const response = await wallet.request({  method: 'snap_dialog',  params: {  type: DialogType.Confirmation,  content: panel([  heading('Do you want to sign this transaction ?'),  text(`It will be signed with address: ${senderAddress}`),  text(signingTxnText),  ]),  },  });  Please note that we have also reported to the MM Snaps team, that dialogues do not by default hint the origin of the action. We hope this will be addressed in a common way for all snaps in the future,  Recommendation  Validate inputs. Encode data in a safe way to be displayed to the user. Show the original data provided within a pre-text or code-block. Show derived or decoded information (token recipient) as additional information to the user.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"
    },
    {
        "title": "4.2 Lax Validation Using@starknet::validateAndParseAddress Allows Short Addresses and Does Not Verify Checksums    ",
        "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by wrapping validateAndParseAddress() with an implicit length check. Additionally, the client provided the following statement:  Add validation on the snap side for address length  Checksum will not implement as some users are going to call the Snap directly without going through the dApp  As per the client s decision, checksummed addresses are not enforced.  Description  Address inputs in RPC calls are validated using @starknet::validateAndParseAddress().  packages/starknet-snap/src/getErc20TokenBalance.ts:L19-L28  try {  validateAndParseAddress(requestParamsObj.tokenAddress);  } catch (err) {  throw new Error(`The given token address is invalid: ${requestParamsObj.tokenAddress}`);  try {  validateAndParseAddress(requestParamsObj.userAddress);  } catch (err) {  throw new Error(`The given user address is invalid: ${requestParamsObj.userAddress}`);  While the message validates the general structure for valid addresses, it does not strictly enforce address length and may silently add padding to the inputs before validation. This can be problematic as it may hide user input errors when a user provides an address that is too short and silently gets left-padded with zeroes. This may unintentionally cause a user to request action on the wrong address without them recognizing it.  ../src/utils/address.ts:L14-L24  export function validateAndParseAddress(address: BigNumberish): string {  assertInRange(address, ZERO, MASK_251, 'Starknet Address');  const result = addAddressPadding(address);  if (!result.match(/^(0x)?[0-9a-fA-F]{64}$/)) {  throw new Error('Invalid Address Format');  return result;  export function validateAndParseAddress(address: BigNumberish): string {  assertInRange(address, ZERO, MASK_251, 'Starknet Address');  const result = addAddressPadding(address);  if (!result.match(/^(0x)?[0-9a-fA-F]{64}$/)) {  throw new Error('Invalid Address Format');  return result;  Recommendation  The exposed Snap API should strictly validate inputs. User input must be provided in a safe canonical form (exact address length, checksum) by the dapp.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"
    },
    {
        "title": "4.3 RPC starkNet_signMessage - Fails to Display the User Account That Is Used for Signing the Message    ",
        "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by displaying the signing accounts address with the dialog. All user-provided fields are copyable, preventing any markdown injection. Additionally, the client provided the following statement:  add signer address add bottom of the dialog  We want to note that the origin of the RPC call is not visible in the dialog. However, we recommend addressing this with the MM Snap SDK by generically showing the origin of MM popups with the dialog.  Description  The signing request dialogue does not display the user account that is being used to sign the message. A malicious dapp may pretend to sign a message with one account while issuing an RPC call for a different account.  Note that StarkNet signing requests should implement similar security measures to how MetaMask signing requests work. Being fully transparent on  who signs what , also displaying the origin of the request. This is especially important on multi-dapp snaps to avoid users being tricked into signing transactions they did not intend to sign (wrong signer).  packages/starknet-snap/src/signMessage.ts:L34-L42  const response = await wallet.request({  method: 'snap_dialog',  params: {  type: DialogType.Confirmation,  content: panel([heading('Do you want to sign this message ?'), text(JSON.stringify(typedDataMessage))]),  },  });  if (!response) return false;  Examples  UI does not show the signing accounts address. Hence, the user cannot be sure what account is used to sign the message.  Recommendation  Show what account is requested to sign a message. Display the origin of the RPC call.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"
    },
    {
        "title": "4.4 RPC starkNet_signMessage - Inconsistency When Previewing the Signed Message (Markdown Injection)    ",
        "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by rendering user-provided information with the copyable UI component. Additionally, the client provided the following statement:  restructure dialog ui by using MM copyable field, it can ignore any markdown or tag block  Description  The snap displays an dialogue to the user requesting them to confirm that they want to sign a message when a dapp performs a request to starkNet_signMessage. However, the MetaMask Snaps UI text() component will render Markdown. This means that the message-to-be-signed displayed to the user for approval will be inaccurate if it contains Markdown renderable text.  packages/starknet-snap/src/signMessage.ts:L35-L41  const response = await wallet.request({  method: 'snap_dialog',  params: {  type: DialogType.Confirmation,  content: panel([heading('Do you want to sign this message ?'), text(JSON.stringify(typedDataMessage))]),  },  });  Examples  {\"a **mykey**\":\"this should not render **markdown** <pre>test</pre><b>bbb</b><strong>strongstrong</strong>[visit oststrom](https://oststrom.com) _ital_\"}  Recommendation  Render signed message contents in a code block or preformatted text blocks.  Note: we ve also reported this to the MetaMask Snaps team to provide further guidance.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"
    },
    {
        "title": "4.5 UI/AlertView - Unnecessary Use of dangerouslySetInnerHTML    ",
        "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by not using dangerouslySetInnerHTML. Additionally, the client provided the following statement:  Remove dangerouslySetInnerHTML from UI  Description  AlertView is populated by setting innerHTML instead of the component s value, which would be auto-escaped. This only makes sense if the component is supposed to render HTML. However, the component is never used with HTML as input, and the attribute name text is misleading.  packages/wallet-ui/src/components/ui/atom/Alert/Alert.view.tsx:L11-L36  export function AlertView({ text, variant, ...otherProps }: Props) {  const paragraph = useRef<HTMLParagraphElement | null>(null);  const [isMultiline, setIsMultiline] = useState(false);  useEffect(() => {  if (paragraph.current) {  const height = paragraph.current.offsetHeight;  setIsMultiline(height > 20);  }, []);  return (  <Wrapper isMultiline={isMultiline} variant={variant} {...otherProps}>  <>  {variant === VariantOptions.SUCCESS && <LeftIcon icon={['fas', 'check-circle']} />}  {variant === VariantOptions.INFO && <LeftIcon icon={['fas', 'info-circle']} color={theme.palette.info.dark} />}  {variant === VariantOptions.ERROR && (  <LeftIcon icon={['fas', 'exclamation-circle']} color={theme.palette.error.main} />  )}  {variant === VariantOptions.WARNING && (  <LeftIcon icon={['fas', 'exclamation-triangle']} color={theme.palette.warning.main} />  )}  <Parag ref={paragraph} color={variant} dangerouslySetInnerHTML={{ __html: text }} />  </>  </Wrapper>  );  packages/wallet-ui/src/components/ui/organism/NoFlaskModal/NoFlaskModal.view.tsx:L4-L25  export const NoFlaskModalView = () => {  return (  <Wrapper>  <StarknetLogo />  <Title>You don't have the MetaMask Flask extension</Title>  <DescriptionCentered>  You need to install MetaMask Flask extension in order to use the StarkNet Snap.  <br />  <br />  <AlertView  text=\"Please make sure that the regular MetaMask extension is disabled or use a different browser profile\"  variant=\"warning\"  />  </DescriptionCentered>  <a href=\"https://metamask.io/flask\" target=\"_blank\" rel=\"noreferrer noopener\">  <ConnectButton customIconLeft={<FlaskIcon />} onClick={() => {}}>  Download MetaMask Flask  </ConnectButton>  </a>  </Wrapper>  );  };  Setting HTML from code is risky because it s easy to inadvertently expose users to a cross-site scripting (XSS) attack.  Recommendation  Do not use dangerouslySetInnerHTML unless there is a specific requirement that passed in HTML be rendered. If so, rename the attribute name to html instead of text to set clear expectations regarding how the input is treated. Nevertheless, since the component is not used with HTML input, we recommend removing dangerouslySetInnerHTML altogether.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"
    },
    {
        "title": "4.6 RPC starkNet_addErc20Token - Should Ask for User Confirmation    ",
        "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by requesting user confirmation for adding new ERC20 Tokens. Additionally, the client provided the following statement:  Adding confirm dialog with MM copyable field, it can ignore any markdown or tag block  Disable loading frame when user reject the add ec220 token request on UI  Description  The RPC method upserts ERC20 tokens received via RPC without asking the user for confirmation. This would allow a connected dapp to insert/change ERC20 token information anytime. This can even be more problematic when multiple dapps are connected to the StarkNet-Snap (race conditions).  packages/starknet-snap/src/addErc20Token.ts:L30-L47  validateAddErc20TokenParams(requestParamsObj, network);  const erc20Token: Erc20Token = {  address: tokenAddress,  name: tokenName,  symbol: tokenSymbol,  decimals: tokenDecimals,  chainId: network.chainId,  };  await upsertErc20Token(erc20Token, wallet, saveMutex);  console.log(`addErc20Token:\\nerc20Token: ${JSON.stringify(erc20Token)}`);  return erc20Token;  } catch (err) {  console.error(`Problem found: ${err}`);  throw err;  Recommendation  Ask the user for confirmation when changing the snaps state.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"
    },
    {
        "title": "4.7 getKeysFromAddress - Possible Unchecked Null Dereference When Looking Up Private Key    ",
        "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by throwing an exception on error. Additionally, the client provided the following statement:  instead of return null, raise err in getKeysFromAddress, caller will catch the exception  Description  getKeysFromAddress() may return null if an invalid address was provided but most callers of the function do not check for the null condition and blindly dereference or unpack the return value causing an exception.  packages/starknet-snap/src/utils/starknetUtils.ts:L453-L455  return null;  };  Examples  packages/starknet-snap/src/signMessage.ts:L44-L46  const { privateKey: signerPrivateKey } = await getKeysFromAddress(keyDeriver, network, state, signerAddress);  const signerKeyPair = getKeyPairFromPrivateKey(signerPrivateKey);  const typedDataSignature = getTypedDataMessageSignature(signerKeyPair, typedDataMessage, signerAddress);  packages/starknet-snap/src/extractPrivateKey.ts:L37  const { privateKey: userPrivateKey } = await getKeysFromAddress(keyDeriver, network, state, userAddress);  packages/starknet-snap/src/extractPublicKey.ts:L31-L32  const { publicKey } = await getKeysFromAddress(keyDeriver, network, state, userAddress);  userPublicKey = publicKey;  packages/starknet-snap/src/sendTransaction.ts:L48-L52  const {  privateKey: senderPrivateKey,  publicKey,  addressIndex,  } = await getKeysFromAddress(keyDeriver, network, state, senderAddress);  packages/starknet-snap/src/signMessage.ts:L44-L45  const { privateKey: signerPrivateKey } = await getKeysFromAddress(keyDeriver, network, state, signerAddress);  const signerKeyPair = getKeyPairFromPrivateKey(signerPrivateKey);  packages/starknet-snap/src/verifySignedMessage.ts:L38  const { privateKey: signerPrivateKey } = await getKeysFromAddress(keyDeriver, network, state, verifySignerAddress);  packages/starknet-snap/src/estimateFee.ts:L48-L53  const { privateKey: senderPrivateKey, publicKey } = await getKeysFromAddress(  keyDeriver,  network,  state,  senderAddress,  );  Recommendation  Explicitly check for the null or {} case. Consider returning {} to not allow unpacking followed by an explicit null check.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"
    },
    {
        "title": "4.8 RPC starkNet_getStoredTransactions - Lax or Missing Input Validation   ",
        "body": "  Resolution  Won t fix. The client provided the following statement:  not fix, minor impact  We want to note that strict input validation should be performed on all untrusted inputs for read/write and read-only methods. Just because the method is read-only now does not necessarily mean it will stay that way. Leaving untrusted inputs unchecked may lead to more severe security vulnerabilities with a growing codebase in the future.  Description  Potentially untrusted inputs, e.g. addresses received via RPC calls, are not always checked to conform to the StarkNet address format. For example, requestParamsObj.senderAddress is never checked to be a valid StarkNet address.  packages/starknet-snap/src/getStoredTransactions.ts:L18-L26  const transactions = getTransactions(  state,  network.chainId,  requestParamsObj.senderAddress,  requestParamsObj.contractAddress,  requestParamsObj.txnType,  undefined,  minTimeStamp,  );  Recommendation  This method is read-only, and therefore, severity is estimated as Minor. However, it is always suggested to perform strict input validation on all user-provided inputs for read-only and read-write methods.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"
    },
    {
        "title": "4.9 Disable Debug Log for Production Build    ",
        "body": "  Resolution  Addressed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by introducing a configurable logger. Additionally, the client provided the following statement:  add custom logger to replace console.log, and log message base on debug level, when debug level is off, it will not log anything  update production CICD pipeline to build project with debug level = off/disabled  There re still some instances of console.log(). However, internal state or full requests are not logged anymore. We would still recommend replacing the remaining console.log calls (e.g. the one in addERC20Token).  Description  Throughout the codebase, there are various places where debug log output is being printed to the console. This should be avoided for production builds.  Examples  packages/starknet-snap/src/index.ts:L45-L46  // Switch statement for methods not requiring state to speed things up a bit  console.log(origin, request);  packages/starknet-snap/src/index.ts:L91-L92  console.log(`${request.method}:\\nrequestParams: ${JSON.stringify(requestParams)}`);  packages/starknet-snap/src/index.ts:L103  console.log(`Snap State:\\n${JSON.stringify(state, null, 2)}`);  Recommendation  Remove the debug output or create a custom log method that allows to enable/disable logging to console.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"
    },
    {
        "title": "4.10 package.json - Dependecy Mixup    ",
        "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a as per recommendation. Additionally, the client provided the following statement:  Move development dependencies to package.json::devDependencies  Description  The following dependencies are only used for testing or development purposes and should therefore be listed as devDependencies in package.json, otherwise they may be installed for production builds, too.  https://sinonjs.org/  https://www.chaijs.com/  packages/starknet-snap/package.json:L50  \"chai\": \"^4.3.6\",  packages/starknet-snap/package.json:L53-L54  \"sinon\": \"^13.0.2\",  \"sinon-chai\": \"^3.7.0\",  Recommendation  Move development dependencies to package.json::devDependencies.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"
    },
    {
        "title": "4.11 package.json - Invalid License    Invalid",
        "body": "  Resolution  Invalid. Legal clarified that it is perfectly fine to allow MIT+Apache2. Additionally, client provided the following statement:  not fix, choose to stick with dual license  Description  The license field in package.json is invalid.  packages/starknet-snap/package.json:L4  \"license\": \"(Apache-2.0 OR MIT)\",  Recommendation  Update the license field.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"
    },
    {
        "title": "4.12 RPC starkNet_extractPrivateKey - Should Be Renamed to starkNet_displayPrivateKey  ",
        "body": "  Resolution  Won t Fix. The client provided the following statement:  not fix, the extractPrivateKey is not for display purpose  We want to note that we still encourage changing the method name and return value to explicitly return null in the RPC handler for the sake of good secure coding practices discouraging future devs to return implementing key extraction RPC endpoints that may expose wallet credentials to a linked dapp.  Description  It is recommended to rename starkNet_extractPrivateKey  to starkNet_displayPrivateKey as this more accurately describes what the RPC method is doing.  Also, the way the method handler is implemented makes it appear as if it returns the private key to the RPC origin while the submethod returns null. Consider changing this to an explicit empty return to clearly mark in the outer call that no private key is exposed to the caller. Not to confuse this with how starkNet_extractPublicKey  works which actually returns the pubkey to the RPC caller.  packages/starknet-snap/src/index.ts:L123-L127  case 'starkNet_extractPrivateKey':  apiParams.keyDeriver = await getAddressKeyDeriver(snap);  return extractPrivateKey(apiParams);  ",
        "labels": [
            "Consensys",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"
    },
    {
        "title": "4.13 UI/hooks - detectEthereumProvider() Should Require mustBeMetaMask  ",
        "body": "  Resolution  Won t Fix. The client provided the following statement:  not fix, minor impact  Description  MetaMask Snaps require a Metamask provider. However, detectEthereumProvider() does not explicitly require a MetaMask provider and would continue if the alternative provider contains the substring flask in their signature.  packages/wallet-ui/src/hooks/useHasMetamaskFlask.ts:L7-L16  const detectMetamaskFlask = async () => {  try {  const provider = (await detectEthereumProvider({  mustBeMetaMask: false,  silent: true,  })) as any | undefined;  const isFlask = (await provider?.request({ method: 'web3_clientVersion' }))?.includes('flask');  if (provider && isFlask) {  return true;  Consider requiring mustBeMetaMask = true to enforce that the injected provider is indeed MetaMask. This will also work with MetaMask Flask as shown here:  \u21d2 window.ethereum.isMetaMask  true  \u21d2 await window.ethereum.request({ method: 'web3_clientVersion' })  'MetaMask/v10.32.0-flask.0'  ",
        "labels": [
            "Consensys",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"
    },
    {
        "title": "4.14 RPC starkNet_addNetwork - Not Implemented, No User Confirmation  ",
        "body": "  Resolution  Won t Fix. The client provided the following statement:  not fix, minor impact  Description  It was observed that the RPC method starkNet_addNetwork is not implemented.  In case this method is to be exposed to dapps, we recommended to follow the advise given in issue 4.6 to ask for user confirmation when adjusting the snaps configuration state.  ",
        "labels": [
            "Consensys",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"
    },
    {
        "title": "4.1 Insufficient tests ",
        "body": "  Resolution  Comment from NUTS Finance team:  We have added more mainnet fork test coverage for single plus assets. We will continue to add more test cases on edge cases. For the test coverage command, it s strange that if we run all test cases with  truffle test , the LiquidityGauge test case fails. However, it passes if we run it by ourselves. We have done some debugging but cannot figure it out yet. We believe that our test cases are all valid.  Description  It is crucial to write tests with possibly 100% coverage for smart contract systems. Given that BTCPlus has inner complexity and also integrates many DeFi projects, using unit testing and fuzzing in all code paths is essential to a secure system.  Currently there are only 63 unit tests (with 1 failing) for the main components (Plus/Composite token, Governance, Liquidity Gauge, etc) which are only testing the predetermined code execution paths. There are also DeFi protocol specific tests that are not well organized to be able to find the coverage on the system.  Recommendation  Write proper tests for all possible code flows and specially edge cases (Price volatility, token transfer failure, 0 amounts, etc). It is useful to have one command to run all tests and have a code coverage report at the end. Also using libraries like eth-gas-reporter it s possible to know the gas usage of different functionalities in order to optimize and prevent lock ups in the future.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/nuts-finance-btcplus/"
    },
    {
        "title": "4.2 Simplify the harvest method in each SinglePlus    ",
        "body": "  Resolution  Comment from NUTS Finance team:  We have replaced all safeApprove() usage with approve() and used block.timestamp as the expiration date.  Description  The BadgerSBTCCrvPlus single plus contract implements a custom harvest method.  code/BTC-Plus/contracts/single/eth/BadgerSBTCCrv%2B.sol:L52-L56  /**  @dev Harvest additional yield from the investment.  Only governance or strategist can call this function.  /  function harvest(address[] calldata _tokens, uint256[] calldata _cumulativeAmounts, uint256 _index, uint256 _cycle,  This method can only be called by the strategist because of the onlyStrategist modifier.  This method has a few steps which take one asset and transform it into another asset a few times.  It first claims the Badger tokens:  code/BTC-Plus/contracts/single/eth/BadgerSBTCCrv%2B.sol:L58-L59  // 1. Harvest from Badger Tree  IBadgerTree(BADGER_TREE).claim(_tokens, _cumulativeAmounts, _index, _cycle, _merkleProof, _amountsToClaim);  Then it transforms the Badger tokens into WBTC using Uniswap.  code/BTC-Plus/contracts/single/eth/BadgerSBTCCrv%2B.sol:L61-L72  // 2. Sushi: Badger --> WBTC  uint256 _badger = IERC20Upgradeable(BADGER).balanceOf(address(this));  if (_badger > 0) {  IERC20Upgradeable(BADGER).safeApprove(SUSHISWAP, 0);  IERC20Upgradeable(BADGER).safeApprove(SUSHISWAP, _badger);  address[] memory _path = new address[](2);  _path[0] = BADGER;  _path[1] = WBTC;  IUniswapRouter(SUSHISWAP).swapExactTokensForTokens(_badger, uint256(0), _path, address(this), block.timestamp.add(1800));  This step can be simplified in two ways.  First, the safeApprove method isn t useful because its usage is not recommended anymore.  The OpenZeppelin version 4 implementation states the method is deprecated and its usage is discouraged.  contracts/token/ERC20/utils/SafeERC20Upgradeable.sol:L29-L30  * @dev Deprecated. This function has issues similar to the ones found in      * {IERC20-approve}, and its usage is discouraged.  @dev Deprecated. This function has issues similar to the ones found in  {IERC20-approve}, and its usage is discouraged.  Thus, the SafeERC20Upgradeable.sol is not needed anymore and the import can be removed.  @dev Deprecated. This function has issues similar to the ones found in  {IERC20-approve}, and its usage is discouraged.  Thus, the SafeERC20Upgradeable.sol is not needed anymore and the import can be removed.  Another step is swapping the tokens on Uniswap.  code/BTC-Plus/contracts/single/eth/BadgerSBTCCrv%2B.sol:L71  IUniswapRouter(SUSHISWAP).swapExactTokensForTokens(_badger, uint256(0), _path, address(this), block.timestamp.add(1800));  In this case, the last argument block.timestamp.add(1800) is the deadline. This is useful when the transaction is sent to the network and a deadline is needed to expire the transaction. However, the execution is right now and there s no need for a future expiration date.  Removing the safe math addition will have the same end effect, the tokens will be swapped and the call is not at risk to expire.  Recommendation  Remove safeApprove and favor using approve. This also removes the need of having SafeERC20Upgradeable.sol included.  Do not use safe math when sending the expiration date. Use block.timestamp for the same effect and a reduced gas cost.  Apply the same principles for other Single Plus Tokens.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/nuts-finance-btcplus/"
    },
    {
        "title": "4.3 Reduce complexity in modifiers related to governance and strategist    ",
        "body": "  Resolution  Comment from NUTS Finance team:  The code size seems to be an issue for us. For example, the code size of the CompositePlus contract is more than 21k. If you could provide more suggestions on how to reduce the contract code size, we d appreciate it.  Description  The modifier onlyGovernance:  code/BTC-Plus/contracts/Plus.sol:L101-L104  modifier onlyGovernance() {  _checkGovernance();  _;  Calls the internal function _checkGovernance:  code/BTC-Plus/contracts/Plus.sol:L97-L99  function _checkGovernance() internal view {  require(msg.sender == governance, \"not governance\");  There is no other case where the internal method _checkGovernance is called directly.  One can reduce complexity by removing the internal function and moving its code directly in the modifier. This will increase code size but reduce gas used and code complexity.  There are multiple similar instances:  code/BTC-Plus/contracts/Plus.sol:L106-L113  function _checkStrategist() internal view {  require(msg.sender == governance || strategists[msg.sender], \"not strategist\");  modifier onlyStrategist {  _checkStrategist();  _;  code/BTC-Plus/contracts/governance/GaugeController.sol:L298-L305  function _checkGovernance() internal view {  require(msg.sender == governance, \"not governance\");  modifier onlyGovernance() {  _checkGovernance();  _;  code/BTC-Plus/contracts/governance/LiquidityGauge.sol:L450-L457  function _checkGovernance() internal view {  require(msg.sender == IGaugeController(controller).governance(), \"not governance\");  modifier onlyGovernance() {  _checkGovernance();  _;  Recommendation  Consider removing the internal function and including its body in the modifier directly if the code size is not an issue.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/nuts-finance-btcplus/"
    },
    {
        "title": "4.4 safeMath is integrated in Solidity 0.8.0^    ",
        "body": "  Resolution  Comment from NUTS Finance team:  We ve replaced all SafeMath usage with native math.  Description  The code base is using Solidity 0.8.0 which has safeMath integrated in the compiler. In addition, the codebase also utilizes OpenZepplin SafeMath library for arithmetic operations.  Recommendation  Removing safeMath from the code base results in gas usage optimization and also clearer code.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/nuts-finance-btcplus/"
    },
    {
        "title": "4.5 Lack of up to date documentation",
        "body": "  Resolution  Comment from NUTS Finance team:  We are continuing to enhance our docs about the latest design.  Description  This is a complicated system with many design decisions that are resulted from integration with other DeFi projects. In the code base there are many hard coded values or anti-patterns that are not documented and creates an unhealthy and hard to maintain code base.  Examples  TOKENLESS_PRODUCTION = 40; is not documented in the LiquidityGauge.sol.  uint256 _balance = balanceOf(_account);  uint256 _supply = totalSupply();  uint256 _limit = _balance.mul(TOKENLESS_PRODUCTION).div(100);  if (_votingTotal > 0) {  uint256 _boosting = _supply.mul(_votingBalance).mul(100 - TOKENLESS_PRODUCTION).div(_votingTotal).div(100);  _limit = _limit.add(_boosting);  Based on the conversation with the NUTS finance developer, this is due to fact that this is a fork of the way Curve s DAO contract works.  Recommendation  We recommend to have dedicated up to date documents on the system overview of the system and each module. In addition, in-line documentation in the code base helps to understand the code base and increases the readability of the code.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/nuts-finance-btcplus/"
    },
    {
        "title": "6.1 ERC20Lockable - inconsistent locking status    ",
        "body": "  Resolution  Issue was fixed by completely removing the unlock date mechanism.  Description  Vega_Token.is_tradable() will incorrectly return false if the token is never manually unlocked by the owner but unlock_time has passed, which will automatically unlock trading.  Examples  code/ERC20Lockable.sol:L48-L67  /**  @dev locked status, only applicable before unlock_date  /  bool public _is_locked = true;  /**  @dev Modifier that only allows function to run if either token is unlocked or time has expired.  Throws if called while token is locked.  /  modifier onlyUnlocked() {  require(!_is_locked || now > unlock_date);  _;  /**  @dev Internal function that unlocks token. Can only be ran before expiration (give that it's irrelevant after)  /  function _unlock() internal {  require(now <= unlock_date);  _is_locked = false;  Recommendation  declare _is_locked as private instead of public  create a getter method that correctly returns the locking status  function _isLocked() internal view {  return !_is_locked || now > unlock_date;  make modifier onlyUnlocked() use the newly created getter (_isLocked())  make Vega_Token.is_tradeable() use the newly created getter (_isLocked())  _unlock() should raise an errorcondition when called on an already unlocked contract  it could make sense to emit a  contract hast been unlocked  event for auditing purposes  7 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/vega-vegatoken/"
    },
    {
        "title": "7.1 MythX",
        "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  The output of a MythX Full Mode analysis was reviewed by the audit team and no relevant issues were raised as part of the process.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/vega-vegatoken/"
    },
    {
        "title": "7.2 Ethlint",
        "body": "  Ethlint is an open source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  Solium version 1.2.5  contracts/Address.sol  22:8    warning    Line contains trailing whitespace    no-trailing-whitespace  29:8    error      Avoid using Inline Assembly.         security/no-inline-assembly  contracts/ERC20Lockable.sol  58:8     warning    Provide an error message for require()             error-reason  58:31    warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  66:8     warning    Provide an error message for require()             error-reason  66:16    warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  contracts/ERC20StaticSupply.sol  15:4    warning    Line exceeds the limit of 145 characters    max-len  contracts/SafeERC20.sol  33:16    error      Only use indent of 12 spaces.             indentation  67:65    warning    Avoid using low-level function 'call'.    security/no-low-level-calls  contracts/Vega_Token.sol  9:1    warning    Line contains trailing whitespace    no-trailing-whitespace  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/vega-vegatoken/"
    },
    {
        "title": "7.3 Surya",
        "body": "  Surya is a utility tool for smart contract systems. It provides a number of visual outputs and information about the structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  S\u016brya s Description Report  Files Description Table  contracts/Vega_Token.sol  b92b3c54b2f47a88fa9e84534046b462dbaee9aa  contracts/Address.sol  1213b0f150dd5e3f694c3721c44cb5cc3202b743  contracts/ERC20Detailed.sol  7e4d00c462120565201f28361b29201d1bfe0a34  contracts/IERC20.sol  72c15b6a16b7dc92e69ff97ccfe1958d9948e200  contracts/ERC20Lockable.sol  377447995444beee2b3c5342bfa9b1bbc1d08356  contracts/SafeMath.sol  c8bda5eb19c16d34bc48bf115229a9b967feb6ef  contracts/ERC20StaticSupply.sol  bf3e66af74470eed08d0e0f82b9a98705a745c7c  contracts/Ownable.sol  12ec51ec8a3b4eed6326434fd0f5926b40602778  contracts/Roles.sol  2c85acf184ae36f96ebafd8f6e26232ea459a711  contracts/SafeERC20.sol  ebd65ea9a0cdcb29bbbbf651a1076d51be031443  Contracts Description Table  Function Name  Visibility  Mutability  Modifiers  Vega_Token  Implementation  Ownable, ERC20StaticSupply  Public    ERC20StaticSupply  unlock_token  Public    onlyOwner  is_tradable  Public    NO   Address  Library  isContract  Internal \ud83d\udd12  toPayable  Internal \ud83d\udd12  ERC20Detailed  Implementation  IERC20  Public    NO   name  Public    NO   symbol  Public    NO   decimals  Public    NO   IERC20  Interface  totalSupply  External    NO   balanceOf  External    NO   transfer  External    NO   allowance  External    NO   approve  External    NO   transferFrom  External    NO   ERC20Lockable  Implementation  IERC20  _unlock  Internal \ud83d\udd12  totalSupply  Public    NO   balanceOf  Public    NO   transfer  Public    onlyUnlocked  allowance  Public    NO   approve  Public    NO   transferFrom  Public    onlyUnlocked  increaseAllowance  Public    NO   decreaseAllowance  Public    NO   _transfer  Internal \ud83d\udd12  _mint  Internal \ud83d\udd12  _burn  Internal \ud83d\udd12  _approve  Internal \ud83d\udd12  _burnFrom  Internal \ud83d\udd12  SafeMath  Library  add  Internal \ud83d\udd12  sub  Internal \ud83d\udd12  sub  Internal \ud83d\udd12  mul  Internal \ud83d\udd12  div  Internal \ud83d\udd12  div  Internal \ud83d\udd12  mod  Internal \ud83d\udd12  mod  Internal \ud83d\udd12  ERC20StaticSupply  Implementation  ERC20Detailed, Ownable, ERC20Lockable  Public    ERC20Detailed  issue  Public    onlyOwner  Ownable  Implementation  Internal \ud83d\udd12  owner  Public    NO   isOwner  Public    NO   renounceOwnership  Public    onlyOwner  transferOwnership  Public    onlyOwner  _transferOwnership  Internal \ud83d\udd12  Roles  Library  add  Internal \ud83d\udd12  remove  Internal \ud83d\udd12  has  Internal \ud83d\udd12  SafeERC20  Library  safeTransfer  Internal \ud83d\udd12  safeTransferFrom  Internal \ud83d\udd12  safeApprove  Internal \ud83d\udd12  safeIncreaseAllowance  Internal \ud83d\udd12  safeDecreaseAllowance  Internal \ud83d\udd12  callOptionalReturn  Private \ud83d\udd10  Legend  Function can modify state  Function is payable  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/vega-vegatoken/"
    },
    {
        "title": "6.1 Re-Entrancy Risks Associated With External Calls With Other Liquid Staking Systems.    ",
        "body": "  Resolution   Fixed in   commit f43b7cd5135872143cc35f40cae95870446d0413 by introducing reentrancy guards.  Description  As part of the strategy to integrate with Liquid Staking tokens for Ethereum staking, the Lybra Protocol vaults are required to make external calls to Liquid Staking systems.  For example, the depositEtherToMint function in the vaults makes external calls to deposit Ether and receive the LSD tokens back. While external calls to untrusted third-party contracts may be dangerous, in this case, the Lybra Protocol already extends trust assumptions to these third parties simply through the act of accepting their tokens as collateral. Indeed, in some cases the contract addresses are even hardcoded into the contract and called directly instead of relying on some registry:  contracts/lybra/pools/LybraWstETHVault.sol:L21-L40  contract LybraWstETHVault is LybraPeUSDVaultBase {  Ilido immutable lido;  //WstETH = 0x7f39C581F595B53c5cb19bD0b3f8dA6c935E2Ca0;  //Lido = 0xae7ab96520DE3A18E5e111B5EaAb095312D7fE84;  constructor(address _lido, address _asset, address _oracle, address _config) LybraPeUSDVaultBase(_asset, _oracle, _config) {  lido = Ilido(_lido);  function depositEtherToMint(uint256 mintAmount) external payable override {  require(msg.value >= 1 ether, \"DNL\");  uint256 sharesAmount = lido.submit{value: msg.value}(address(configurator));  require(sharesAmount != 0, \"ZERO_DEPOSIT\");  lido.approve(address(collateralAsset), msg.value);  uint256 wstETHAmount = IWstETH(address(collateralAsset)).wrap(msg.value);  depositedAsset[msg.sender] += wstETHAmount;  if (mintAmount > 0) {  _mintPeUSD(msg.sender, msg.sender, mintAmount, getAssetPrice());  emit DepositEther(msg.sender, address(collateralAsset), msg.value,wstETHAmount, block.timestamp);  In that case, depending on the contract, it may be known what contract is being called, and the risk may be assessed as far as what logic may be executed.  However, in the cases of BETH and rETH, the calls are being made into a proxy and a contract registry of a DAO (RocketPool s DAO) respectively.  contracts/lybra/pools/LybraWbETHVault.sol:L15-L32  contract LybraWBETHVault is LybraPeUSDVaultBase {  //WBETH = 0xa2e3356610840701bdf5611a53974510ae27e2e1  constructor(address _asset, address _oracle, address _config)  LybraPeUSDVaultBase(_asset, _oracle, _config) {}  function depositEtherToMint(uint256 mintAmount) external payable override {  require(msg.value >= 1 ether, \"DNL\");  uint256 preBalance = collateralAsset.balanceOf(address(this));  IWBETH(address(collateralAsset)).deposit{value: msg.value}(address(configurator));  uint256 balance = collateralAsset.balanceOf(address(this));  depositedAsset[msg.sender] += balance - preBalance;  if (mintAmount > 0) {  _mintPeUSD(msg.sender, msg.sender, mintAmount, getAssetPrice());  emit DepositEther(msg.sender, address(collateralAsset), msg.value,balance - preBalance, block.timestamp);  contracts/lybra/pools/LybraRETHVault.sol:L25-L42  constructor(address _rocketStorageAddress, address _rETH, address _oracle, address _config)  LybraPeUSDVaultBase(_rETH, _oracle, _config) {  rocketStorage = IRocketStorageInterface(_rocketStorageAddress);  function depositEtherToMint(uint256 mintAmount) external payable override {  require(msg.value >= 1 ether, \"DNL\");  uint256 preBalance = collateralAsset.balanceOf(address(this));  IRocketDepositPool(rocketStorage.getAddress(keccak256(abi.encodePacked(\"contract.address\", \"rocketDepositPool\")))).deposit{value: msg.value}();  uint256 balance = collateralAsset.balanceOf(address(this));  depositedAsset[msg.sender] += balance - preBalance;  if (mintAmount > 0) {  _mintPeUSD(msg.sender, msg.sender, mintAmount, getAssetPrice());  emit DepositEther(msg.sender, address(collateralAsset), msg.value,balance - preBalance, block.timestamp);  As a result, it is impossible to make any guarantees for what logic will be executed during the external calls. Namely, reentrancy risks can t be ruled out, and the damage could be critical to the system. While the trust in these parties isn t in question, it would be best practice to avoid any additional reentrancy risks by placing reentrancy guards. Indeed, in the LybraRETHVault and LybraWbETHVault contracts, one can see the possible damage as the calls are surrounded in a preBalance <-> balance pattern.  The whole of third party Liquid Staking systems  operations need not be compromised, only these particular parts would be enough to cause critical damage to the Lybra Protocol.  Recommendation  After conversations with the Lybra Finance team, it has been assessed that reentrancy guards are appropriate in this scenario to avoid any potential reentrancy risk, which is exactly the recommendation this audit team would provide.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"
    },
    {
        "title": "6.2 The Deployer of GovernanceTimelock Gets Privileged Access to the System.    ",
        "body": "  Resolution   As per discussions with the Lybra Finance team, this has been acknowledged as a temporary measure to configure anything before the launch of V2. Following the discussions, the Lybra Finance team has revoked the deployer s permissions in   transaction 0x12c95eec095f7e24abc6a127f378f9f0fb3a0021aeac82b487c11afa01b793af and updated the  commit 77e8bc3664fb1b195fd718c2ce1d49af8530f981 to instead introduce a multisig address that will have the  Description  The GovernanceTimelock contract is responsible for Roles Based Access Control management and checks in the Lybra Protocol. It offers two functions specifically that check if an address has the required role - checkRole and checkOnlyRole:  contracts/lybra/governance/GovernanceTimelock.sol:L24-L30  function checkRole(bytes32 role, address _sender) public view  returns(bool){  return hasRole(role, _sender) || hasRole(DAO, _sender);  function checkOnlyRole(bytes32 role, address _sender) public view  returns(bool){  return hasRole(role, _sender);  In checkRole, the contract also lets an address with the role DAO bypass the check altogether, making it a powerful role.  For initial role management, when the GovernanceTimelock contract gets deployed, its constructor logic initializes a few roles, assigns relevant admin roles, and, notably, assigns the DAO role to the contract, and the DAO and the GOV role to the deployer.  contracts/lybra/governance/GovernanceTimelock.sol:L14-L23  constructor(uint256 minDelay, address[] memory proposers, address[] memory executors, address admin) TimelockController(minDelay, proposers, executors, admin) {  _setRoleAdmin(DAO, GOV);  _setRoleAdmin(TIMELOCK, GOV);  _setRoleAdmin(ADMIN, GOV);  _grantRole(DAO, address(this));  _grantRole(DAO, msg.sender);  _grantRole(GOV, msg.sender);  The assignment of such powerful roles to a single private key with the deployer has inherent risks. Specifically in our case, the DAO role alone as we saw may bypass many checks within the Lybra Protocol, and the GOV role even has role management privileges.  However, it does make sense to assign such roles at the beginning of the deployment to finish initialization and assign the rest of the roles. One could argue that having access to the DAO role in the early stages of the system s life could allow for quick disaster recovery in the event of incidents as well. Though, it is still dangerous to hold privileges for such a system in a single address as we have seen over the last years in security incidents that have to do with compromised keys.  Recommendation  While redesigning the deployment process to account for a lesser-privileged deployer would be ideal, the Lybra Finance team should at least transfer ownership as soon as the deployment is complete to minimize compromised private key risk.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"
    },
    {
        "title": "6.3 The configurator.getEUSDMaxLocked() Condition Can Be Bypassed During a Flashloan    ",
        "body": "  Resolution   Fixed in   f6c3afb5e48355c180417b192bd24ba294f77797 by checking  Description  When converting EUSD tokens to peUSD, there is a check that limits the total amount of EUSD that can be converted:  contracts/lybra/token/PeUSDMainnet.sol:L74-L77  function convertToPeUSD(address user, uint256 eusdAmount) public {  require(_msgSender() == user || _msgSender() == address(this), \"MDM\");  require(eusdAmount != 0, \"ZA\");  require(EUSD.balanceOf(address(this)) + eusdAmount <= configurator.getEUSDMaxLocked(),\"ESL\");  The issue is that there is a way to bypass this restriction. An attacker can get a flash loan (in EUSD) from this contract, essentially reducing the visible amount of locked tokens (EUSD.balanceOf(address(this))).  Recommendation  Multiple approaches can solve this issue. One would be adding reentrancy protection. Another one could be keeping track of the borrowed amount for a flashloan.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"
    },
    {
        "title": "6.4 Liquidation Keepers Automatically Become eUSD Debt Providers for Other Liquidations.    ",
        "body": "  Resolution   Fixed in   commit bbcf1867ef66cfdcd4b4fd26df39518048fbde1f by adding an alternative check to the allowance flag to see if  Description  One of the most important mechanisms in the Lybra Protocol is the liquidation of poorly collateralized vaults. For example, if a vault is found to have a collateralization ratio that is too small, a liquidator may provide debt tokens to the protocol and retrieve the vault collateral at a discount:  contracts/lybra/pools/base/LybraEUSDVaultBase.sol:L148-L170  function liquidation(address provider, address onBehalfOf, uint256 assetAmount) external virtual {  uint256 assetPrice = getAssetPrice();  uint256 onBehalfOfCollateralRatio = (depositedAsset[onBehalfOf] * assetPrice * 100) / borrowed[onBehalfOf];  require(onBehalfOfCollateralRatio < badCollateralRatio, \"Borrowers collateral ratio should below badCollateralRatio\");  require(assetAmount * 2 <= depositedAsset[onBehalfOf], \"a max of 50% collateral can be liquidated\");  require(EUSD.allowance(provider, address(this)) != 0, \"provider should authorize to provide liquidation EUSD\");  uint256 eusdAmount = (assetAmount * assetPrice) / 1e18;  _repay(provider, onBehalfOf, eusdAmount);  uint256 reducedAsset = assetAmount * 11 / 10;  totalDepositedAsset -= reducedAsset;  depositedAsset[onBehalfOf] -= reducedAsset;  uint256 reward2keeper;  if (provider == msg.sender) {  collateralAsset.safeTransfer(msg.sender, reducedAsset);  } else {  reward2keeper = (reducedAsset * configurator.vaultKeeperRatio(address(this))) / 110;  collateralAsset.safeTransfer(provider, reducedAsset - reward2keeper);  collateralAsset.safeTransfer(msg.sender, reward2keeper);  emit LiquidationRecord(provider, msg.sender, onBehalfOf, eusdAmount, reducedAsset, reward2keeper, false, block.timestamp);  To liquidate the vault, the liquidator needs to transfer debt tokens from the provider address, which in turn needs to have had approved allowance of the token for the vault:  contracts/lybra/pools/base/LybraEUSDVaultBase.sol:L154  require(EUSD.allowance(provider, address(this)) != 0, \"provider should authorize to provide liquidation EUSD\");  The allowance doesn t need to be large, it only needs to be non-zero. While it is true that in the superLiquidation function the allowance check is for eusdAmount, which is the amount associated with assetAmount (the requested amount of collateral to be liquidated), the liquidator could simply call the maximum of the allowance the provider has given to the vault and then repeat the liquidation process. The allowance does not actually decrease throughout the liquidation process.  contracts/lybra/pools/base/LybraEUSDVaultBase.sol:L191  require(EUSD.allowance(provider, address(this)) >= eusdAmount, \"provider should authorize to provide liquidation EUSD\");  Notably, this address doesn t have to be the same one as the liquidator. In fact, there are no checks on whether the liquidator has an agreement or allowance from the provider to use their tokens in this particular vault s liquidation. The contract only checks to see if the provider has EUSD allowance for the vault, and how to split the rewards if the provider is different from the liquidator:  contracts/lybra/pools/base/LybraEUSDVaultBase.sol:L162-L168  if (provider == msg.sender) {  collateralAsset.safeTransfer(msg.sender, reducedAsset);  } else {  reward2keeper = (reducedAsset * configurator.vaultKeeperRatio(address(this))) / 110;  collateralAsset.safeTransfer(provider, reducedAsset - reward2keeper);  collateralAsset.safeTransfer(msg.sender, reward2keeper);  In fact, this is a design choice of the system to treat the allowance to the vault as an agreement to become a public provider of debt tokens for the liquidation process. It is important to note that there are incentives associated with being a provider as they get the collateral asset at a discount.  However, it is not obvious from documentation at the time of the audit nor the code that an address having a non-zero EUSD allowance for the vault automatically allows other users to use that address as a provider. Indeed, many general-purpose liquidator bots use their tokens during liquidations, using the same address for both the liquidator and the provider. As a result, this would put that address at the behest of any other user who would want to utilize these tokens in liquidations. The user might not be comfortable doing this trade in any case, even at a discount.  In fact, due to this mechanism, even during consciously initiated liquidations MEV bots could spot this opportunity and front-run the liquidator s transaction. A frontrunner could put themselves as the keeper and the original user as the provider, grabbing the reward2keeper fee and leaving the original address with fewer rewards and failed gas after the liquidation.  Recommendation  While the mechanism is understood to be done for convenience and access to liquidity as a design decision, this could put unaware users in unfortunate situations of having performed a trade without explicit consent. Specifically, the MEV attack vector could be executed and repeated without fail by a capable actor monitoring the mempool. Consider having a separate, explicit flag for allowing others to use a user s tokens during liquidation, thus also accommodating solo liquidators by removing the MEV attack vector. Consider explicitly mentioning these mechanisms in the documentation as well.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"
    },
    {
        "title": "6.5 Use the Same Solidity Version Across Contracts.    ",
        "body": "  Resolution   Fixed in   commit 33af5c92044cd84c7f69eb8a55316d1e8535ea84 and  commit b1c6ac26b262ec6011c14297583d67d9e3e94326.  Description  Most contracts use the same Solidity version with pragma solidity ^0.8.17. The only exception is the StakingRewardsV2 contract which has pragma solidity ^0.8.  contracts/lybra/miner/stakerewardV2pool.sol:L2  pragma solidity ^0.8;  Recommendation  If all contracts will be tested and utilized together, it would be best to utilize and document the same version within all contract code to avoid any issues and inconsistencies that may arise across Solidity versions.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"
    },
    {
        "title": "6.6 Duplication of Bad Collateral Ratio   ",
        "body": "  Resolution  The Lybra Finance team has acknowledged this as a choice by design and provided the following note:  The liquidation ratio for each eUSD vault is fixed, and this has been stated in our docs. Therefore, we will keep it unchanged.  Description  It is possible to set a bad collateral ratio in the LybraConfigurator contract for any vault:  contracts/lybra/configuration/LybraConfigurator.sol:L137-L141  function setBadCollateralRatio(address pool, uint256 newRatio) external onlyRole(DAO) {  require(newRatio >= 130 * 1e18 && newRatio <= 150 * 1e18 && newRatio <= vaultSafeCollateralRatio[pool] + 1e19, \"LNA\");  vaultBadCollateralRatio[pool] = newRatio;  emit SafeCollateralRatioChanged(pool, newRatio);  But in the LybraEUSDVaultBase contract, this value is fixed and cannot be changed:  contracts/lybra/pools/base/LybraEUSDVaultBase.sol:L19  uint256 public immutable badCollateralRatio = 150 * 1e18;  This duplication of values can be misleading at some point. It s better to make sure you cannot change the bad collateral ratio in the LybraConfigurator contract for some types of vaults.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"
    },
    {
        "title": "6.7 Missing Events.    ",
        "body": "  Resolution   Fixed in   commit 518ef434c6f89c7747373b6ae178d9665d3637f2.  Description  In a few cases in the Lybra Protocol system, there are contracts that are missing events in significant scenarios, such as important configuration changes like a price oracle change. Consider implementing more events in the below examples.  Examples  No events in the contract:  contracts/lybra/miner/esLBRBoost.sol:L10-L30  contract esLBRBoost is Ownable {  esLBRLockSetting[] public esLBRLockSettings;  mapping(address => LockStatus) public userLockStatus;  IMiningIncentives public miningIncentives;  // Define a struct for the lock settings  struct esLBRLockSetting {  uint256 duration;  uint256 miningBoost;  // Define a struct for the user's lock status  struct LockStatus {  uint256 lockAmount;  uint256 unlockTime;  uint256 duration;  uint256 miningBoost;  // Constructor to initialize the default lock settings  constructor(address _miningIncentives) {  Missing an event during a premature unlock:  contracts/lybra/miner/ProtocolRewardsPool.sol:L125-L135  function unlockPrematurely() external {  require(block.timestamp + exitCycle - 3 days > time2fullRedemption[msg.sender], \"ENW\");  uint256 burnAmount = getReservedLBRForVesting(msg.sender) - getPreUnlockableAmount(msg.sender);  uint256 amount = getPreUnlockableAmount(msg.sender) + getClaimAbleLBR(msg.sender);  if (amount > 0) {  LBR.mint(msg.sender, amount);  unstakeRatio[msg.sender] = 0;  time2fullRedemption[msg.sender] = 0;  grabableAmount += burnAmount;  Missing events for setting important configurations such as setToken, setLBROracle, and setPools:  contracts/lybra/miner/EUSDMiningIncentives.sol:L87-L102  function setToken(address _lbr, address _eslbr) external onlyOwner {  LBR = _lbr;  esLBR = _eslbr;  function setLBROracle(address _lbrOracle) external onlyOwner {  lbrPriceFeed = AggregatorV3Interface(_lbrOracle);  function setPools(address[] memory _vaults) external onlyOwner {  require(_vaults.length <= 10, \"EL\");  for (uint i = 0; i < _vaults.length; i++) {  require(configurator.mintVault(_vaults[i]), \"NOT_VAULT\");  vaults = _vaults;  Missing events for setting important configurations such as setRewardsDuration and setBoost:  contracts/lybra/miner/stakerewardV2pool.sol:L121-L130  // Allows the owner to set the rewards duration  function setRewardsDuration(uint256 _duration) external onlyOwner {  require(finishAt < block.timestamp, \"reward duration not finished\");  duration = _duration;  // Allows the owner to set the boost contract address  function setBoost(address _boost) external onlyOwner {  esLBRBoost = IesLBRBoost(_boost);  Missing event during what is essentially staking LBR into esLBR (such as in ProtocolRewardsPool.stake()). Consider an appropriate event here such as StakeLBR:  contracts/lybra/miner/esLBRBoost.sol:L55-L58  if(useLBR) {  IesLBR(miningIncentives.LBR()).burn(msg.sender, lbrAmount);  IesLBR(miningIncentives.esLBR()).mint(msg.sender, lbrAmount);  Recommendation  Implement additional events as appropriate.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"
    },
    {
        "title": "6.8 Incorrect Interfaces    ",
        "body": "  Resolution   Fixed in   commit 90285107de8a6754954c303cd69d97b5fdb4e248,  commit 0ac9cd732b601d0baef2690ef9f9f02cda989331, and  commit 518ef434c6f89c7747373b6ae178d9665d3637f2.  Description  In a few cases, incorrect interfaces are used on top of contracts. Though the effect is the same as the contracts are just tokens and follow the same interfaces, it is best practice to implement correct interfaces.  IPeUSD is used instead of IEUSD  contracts/lybra/configuration/LybraConfigurator.sol:L60  IPeUSD public EUSD;  IPeUSD is used instead of IEUSD  contracts/lybra/configuration/LybraConfigurator.sol:L109  if (address(EUSD) == address(0)) EUSD = IPeUSD(_eusd);  IesLBR instead of ILBR  contracts/lybra/miner/ProtocolRewardsPool.sol:L29  IesLBR public LBR;  IesLBR instead of ILBR  contracts/lybra/miner/ProtocolRewardsPool.sol:L57  LBR = IesLBR(_lbr);  Recommendation  Implement correct interfaces for consistency.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"
    },
    {
        "title": "6.9 The ETH Staking Rewards Distribution Tradeoff",
        "body": "  Description  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"
    },
    {
        "title": "6.1 Potentially dangerous use of a cached exchange rate from Compound ",
        "body": "  Description  GPortfolioReserveManager.adjustReserve performs reserve adjustment calculations based on Compound s cached exchange rate values (using CompoundLendingMarketAbstraction.getExchangeRate()) then triggers operations on managed tokens based on up-to-date values (using CompoundLendingMarketAbstraction.fetchExchangeRate()) . Significant deviation between the cached and up-to-date values may make it difficult to predict the outcome of reserve adjustments.  Recommendation  Use getExchangeRate() consistently, or ensure fetchExchangeRate() is used first, and getExchangeRate() afterward.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/12/growth-defi-v1/"
    },
    {
        "title": "6.2 Potential resource exhaustion by external calls performed within an unbounded loop ",
        "body": "  Description  DydxFlashLoanAbstraction._requestFlashLoan performs external calls in a potentially-unbounded loop. Depending on changes made to DyDx s SoloMargin, this may render this flash loan provider prohibitively expensive. In the worst case, changes to SoloMargin could make it impossible to execute this code due to the block gas limit.  code/contracts/modules/DydxFlashLoanAbstraction.sol:L62-L69  uint256 _numMarkets = SoloMargin(_solo).getNumMarkets();  for (uint256 _i = 0; _i < _numMarkets; _i++) {  address _address = SoloMargin(_solo).getMarketTokenAddress(_i);  if (_address == _token) {  _marketId = _i;  break;  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/12/growth-defi-v1/"
    },
    {
        "title": "6.1 Funds Refunded From Celer Bridge Might Be Stolen ",
        "body": "  Resolution   Remediated as per the client team in   SocketDotTech/socket-ll-contracts#144 by adding checks to see if the refund is received and equal to the expected amount.  Description  The function refundCelerUser from CelerImpl.sol allows a user that deposited into the Celer pool on the source chain, to be refunded for tokens that were not bridged to the destination chain. The tokens are reimbursed to the user by calling the withdraw method on the Celer pool. This is what the refundCelerUser function is doing.  src/bridges/cbridge/CelerImpl.sol:L413-L415  if (!router.withdraws(transferId)) {  router.withdraw(_request, _sigs, _signers, _powers);  From the point of view of the Celer bridge, the initial depositor of the tokens is the SocketGateway. As a consequence, the Celer contract transfers the tokens to be refunded to the gateway. The gateway is then in charge of forwarding the tokens to the initial depositor. To achieve this, it keeps a mapping of unique transfer IDs to depositor addresses. Once a refund is processed, the corresponding address in the mapping is reset to the zero address.  Looking at the withdraw function of the Celer pool, we see that for some tokens, it is possible that the reimbursement will not be processed directly, but only after some delay. From the gateway point of view, the reimbursement will be marked as successful, and the address of the original sender corresponding to this transfer ID will be reset to address(0).  It is then the responsibility of the user, once the locking delay has passed, to call another function to claim the tokens. Unfortunately, in our case, this means that the funds will be sent back to the gateway contract and not to the original sender. Because the gateway implements rescueEther, and rescueFunds functions, the admin might be able to send the funds back to the user. However, this requires manual intervention and breaks the trustlessness assumptions of the system. Also, in that case, there is no easy way to trace back the original address of the sender, that corresponds to this refund.  src/bridges/cbridge/CelerImpl.sol:L120-L127  function bridgeAfterSwap(  uint256 amount,  bytes calldata bridgeData  ) external payable override {  CelerBridgeData memory celerBridgeData = abi.decode(  bridgeData,  (CelerBridgeData)  );  src/bridges/stargate/l2/Stargate.sol:L183-L186  function swapAndBridge(  uint32 swapId,  bytes calldata swapData,  StargateBridgeDataNoToken calldata stargateBridgeData  Note that this violates the security assumption:  The contracts are not supposed to hold any funds post-tx execution.   Recommendation  Make sure that CelerImpl supports also the delayed withdrawals functionality and that withdrawal requests are deleted only if the receiver has received the withdrawal in a single transaction.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"
    },
    {
        "title": "6.2 Calls Made to Non-Existent/Removed Routes or Controllers Will Not Result in Failure ",
        "body": "  Resolution   Remediated as per the client team in   SocketDotTech/socket-ll-contracts#145 by adding a  Description  This issue was found in commit hash a8d0ad1c280a699d88dc280d9648eacaf215fb41.  In the Ethereum Virtual Machine (EVM), delegatecall will succeed for calls to externally owned accounts and more specifically to the zero address, which presents a potential security risk. We have identified multiple instances of delegatecall being used to invoke smart contract functions.  This, combined with the fact that routes can be removed from the system by the owner of the SocketGateway contract using the disableRoute function, makes it possible for the user s funds to be lost in case of an executeRoute transaction (for instance) that s waiting in the mempool is eventually being front-ran by a call to disableRoute.  Examples  src/SocketGateway.sol:L95  (bool success, bytes memory result) = addressAt(routeId).delegatecall(  src/bridges/cbridge/CelerImpl.sol:L208  .delegatecall(swapData);  src/bridges/stargate/l1/Stargate.sol:L187  .delegatecall(swapData);  src/bridges/stargate/l2/Stargate.sol:L190  .delegatecall(swapData);  src/controllers/BaseController.sol:L50  .delegatecall(data);  Even after the upgrade to commit hash d0841a3e96b54a9d837d2dba471aa0946c3c8e7b, the following bug is still present:  src/SocketGateway.sol:L411-L428  function addressAt(uint32 routeId) public view returns (address) {  if (routeId < 513) {  if (routeId < 257) {  if (routeId < 129) {  if (routeId < 65) {  if (routeId < 33) {  if (routeId < 17) {  if (routeId < 9) {  if (routeId < 5) {  if (routeId < 3) {  if (routeId == 1) {  return  0x822D4B4e63499a576Ab1cc152B86D1CFFf794F4f;  } else {  return  0x822D4B4e63499a576Ab1cc152B86D1CFFf794F4f;  } else {  src/SocketGateway.sol:L2971-L2972  if (routes[routeId] == address(0)) revert ZeroAddressNotAllowed();  return routes[routeId];  Recommendation  Consider adding a check to validate that the callee of a delegatecall is indeed a contract, you may refer to the Address library by OZ.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"
    },
    {
        "title": "6.3 Owner Can Add Arbitrary Code to Be Executed From the SocketGateway Contract ",
        "body": "  Resolution  The client team has responded with the following note:  Noted, we will setup tests and rigorous processes around adding new routes.  Description  Since these routes are called via delegatecall(), they don t hold any storage variables that would be used in the Socket systems. However, as Socket aggregates more solutions, unexpected complexities may arise that could require storing and accessing variables through additional contracts. Those contracts would be access control protected to only have the SocketGateway contract have the privileges to modify its variables.  This together with the Owner of the SocketGateway being able to add routes with arbitrary code creates an attack vector where a compromised address with Owner privileges may add a route that would contain code that exploits the special privileges assigned to the SocketGateway contract for their benefit.  For example, the Celer bridge needs extra logic to account for its refund mechanism, so there is an additional CelerStorageWrapper contract that maintains a mapping between individual bridge transfer transactions and their associated msg.sender:  src/bridges/cbridge/CelerImpl.sol:L145  celerStorageWrapper.setAddressForTransferId(transferId, msg.sender);  src/bridges/cbridge/CelerStorageWrapper.sol:L6-L12  /**  @title CelerStorageWrapper  @notice handle storageMappings used while bridging ERC20 and native on CelerBridge  @dev all functions ehich mutate the storage are restricted to Owner of SocketGateway  @author Socket dot tech.  /  contract CelerStorageWrapper {  Consequently, this contract has access-protected functions that may only be called by the SocketGateway to set and delete the transfer IDs:  src/bridges/cbridge/CelerStorageWrapper.sol:L32  function setAddressForTransferId(  src/bridges/cbridge/CelerStorageWrapper.sol:L52  function deleteTransferId(bytes32 transferId) external {  A compromised Owner of SocketGateway could then create a route that calls into the CelerStorageWrapper contract and updates the transfer IDs associated addresses to be under their control via deleteTransferId() and setAddressForTransferId() functions. This could create a significant drain of user funds, though, it depends on a compromised privileged Owner address.  Recommendation  Although it may indeed be unlikely, for aggregating solutions it is especially important to try and minimize compromised access issues. As future solutions require more complexity, consider architecting their integrations in such a way that they require as few administrative and SocketGateway-initiated transactions as possible. Through conversations with the Socket team, it appears that solutions such as timelocks on adding new routes are being considered as well, which would help catch the problem before it appears as well.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"
    },
    {
        "title": "6.4 Dependency on Third-Party APIs to Create the Right Payload ",
        "body": "  Resolution  The client team has responded with the following note:  We offset this risk by following 2 approaches - verifying oneinch calldata on our api before making full calldata for SocketGateway and making verifier contracts/libs that integrators can use to verify our calldata on their side before making actual transaction.  Description  The Socket system of routes and controllers integrates swaps, bridges, and potentially other solutions that are vastly different from each other. The function arguments that are required to execute them may often seem like a black box of a payload for a typical end user. In fact, even when users explicitly provide a destination token with an associated amount for a swap, these arguments themselves might not even be fully (or at all) used in the route itself. Instead, often the routes and controllers accept a bytes payload that contains all the necessary data for its action. These data payloads are generated off-chain, often via centralized APIs provided by the integrated systems themselves, which is understandable in isolation as they have to be generated somewhere at some point. However, the provided bytes do not get checked for their correctness or matching with the other arguments that the user explicitly provided. Even the events that get emitted refer to the individual arguments of functions as opposed to what actually was being used to execute the logic.  src/swap/oneinch/OneInchImpl.sol:L59-L63  // additional data is generated in off-chain using the OneInch API which takes in  // fromTokenAddress, toTokenAddress, amount, fromAddress, slippage, destReceiver, disableEstimate  (bool success, bytes memory result) = ONEINCH_AGGREGATOR.call(  swapExtraData  );  Even the event at the end of the transaction partially refers to the explicitly provided arguments instead of those that actually facilitated the execution of logic  src/swap/oneinch/OneInchImpl.sol:L84-L91  emit SocketSwapTokens(  fromToken,  toToken,  returnAmount,  amount,  OneInchIdentifier,  receiverAddress  );  As Socket aggregates other solutions, it naturally incurs the trust assumptions and risks associated with its integrations. In some ways, they even stack on top of each other, especially in those Socket functions that batch several routes together   all of them and their associated API calls need to return the correct payloads. So, there is an opportunity to minimize these risks by introducing additional checks into the contracts that would verify the correctness of the payloads that are passed over to the routes and controllers. In fact, creating these payloads within the contracts would allow other systems to integrate Socket more simpler as they could just call the functions with primary logical arguments such as the source token, destination token, and amount.  Recommendation  Consider allocating additional checks within the route implementations that ensure that the explicitly passed arguments match what is being sent for execution to the integrated solutions, like in the above example with the 1inch implementation.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"
    },
    {
        "title": "6.5 NativeOptimismImpl - Events Will Not Be Emitted in Case of Non-Native Tokens Bridging ",
        "body": "  Resolution   Remediated as per the client team in   SocketDotTech/socket-ll-contracts#146 by moving the event above the bridging code, making sure events are emitted for all cases, and adding the fix to other functions that had a similar issue.  Description  In the case of the usage of non-native tokens by users, the SocketBridge event will not be emitted since the code will return early.  Examples  src/bridges/optimism/l1/NativeOptimism.sol:L110  function bridgeAfterSwap(  src/bridges/optimism/l1/NativeOptimism.sol:L187  function swapAndBridge(  src/bridges/optimism/l1/NativeOptimism.sol:L283  function bridgeERC20To(  Recommendation  Make sure that the SocketBridge event is emitted for non-native tokens as well.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"
    },
    {
        "title": "6.6 Inconsistent Comments ",
        "body": "  Resolution   Remediated as per the client team in   SocketDotTech/socket-ll-contracts#147.  Description  Some of the contracts in the code have incorrect developer comments annotated for them. This could create confusion for future readers of this code that may be trying to maintain, audit, update, fork, integrate it, and so on.  Examples  src/bridges/stargate/l2/Stargate.sol:L174-L183  /**  @notice function to bridge tokens after swap. This is used after swap function call  @notice This method is payable because the caller is doing token transfer and briding operation  @dev for usage, refer to controller implementations  encodedData for bridge should follow the sequence of properties in Stargate-BridgeData struct  @param swapId routeId for the swapImpl  @param swapData encoded data for swap  @param stargateBridgeData encoded data for StargateBridgeData  /  function swapAndBridge(  This is the same comment as bridgeAfterSwap, whereas it instead does swapping and bridging together  src/bridges/cbridge/CelerStorageWrapper.sol:L24-L32  /**  @notice function to store the transferId and message-sender of a bridging activity  @notice This method is payable because the caller is doing token transfer and briding operation  @dev for usage, refer to controller implementations  encodedData for bridge should follow the sequence of properties in CelerBridgeData struct  @param transferId transferId generated during the bridging of ERC20 or native on CelerBridge  @param transferIdAddress message sender who is making the bridging on CelerBridge  /  function setAddressForTransferId(  This comment refers to a payable property of this function when it isn t.  src/bridges/cbridge/CelerStorageWrapper.sol:L45-L52  /**  @notice function to store the transferId and message-sender of a bridging activity  @notice This method is payable because the caller is doing token transfer and briding operation  @dev for usage, refer to controller implementations  encodedData for bridge should follow the sequence of properties in CelerBridgeData struct  @param transferId transferId generated during the bridging of ERC20 or native on CelerBridge  /  function deleteTransferId(bytes32 transferId) external {  This comment is copied from the above function when it does the opposite of storing - it deletes the transferId  Recommendation  Adjust comments so they reflect what the functions are actually doing.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"
    },
    {
        "title": "6.7 Ether Might Be Sent to Routes by Mistake, and Can Be Stolen ",
        "body": "  Resolution  The client team has responded with the following note:  This can happen only if there is an error in API or integration. There are test cases to verify value on API side and we also run an automated testing suite using small amounts after each upgrade to the API before releasing to public. We also work with integrators to test out the flow covering all edge cases before they release. Overall we are fine with taking this risk and relying on rescue function to recover funds while testing.  Description  Most functions of SocketGateway are payable, and can receive ether, which is processed in different ways, depending on the routes. A user might send ether to a payable function of SocketGateway with a wrong payload, either by mistake or because of an API bug. Let s illustrate the issue with the performAction of the 1inch route. However, this can be generalized to other routes.  src/SocketGateway.sol:L90-L97  function executeRoute(  uint32 routeId,  bytes calldata routeData,  bytes calldata eventData  ) external payable returns (bytes memory) {  (bool success, bytes memory result) = addressAt(routeId).delegatecall(  routeData  );  function performAction(  address fromToken,  address toToken,  uint256 amount,  address receiverAddress,  bytes calldata swapExtraData  ) external payable override returns (uint256) {  uint256 returnAmount;  if (fromToken != NATIVE_TOKEN_ADDRESS) {  ...  ...  (bool success, bytes memory result) = ONEINCH_AGGREGATOR.call(  swapExtraData //<-- here we do not use the value  );  ...  } else {  ....  (bool success, bytes memory result) = ONEINCH_AGGREGATOR.call{  value: amount  //<-- here we use the value  }(swapExtraData);  ...  ...  Assume the user sent some ETH, but sent a payload with fromToken != NATIVE_TOKEN_ADDRESS (and the user has already approved the gateway for fromToken). Then, the ether is not used in the transaction and remains stuck in the SocketGateway contract. This is because the function only executes the part of the code that transfers and swaps ERC-20 tokens, but not the part that handles ether.  Now, suppose another user calls the performAction function with fromToken == NATIVE_TOKEN_ADDRESS and provides enough gas to execute the function. Since there is ether stuck in the contract, this user can force the contract to use the stuck ether to execute the swap by sending the exact amount of ether stuck in the contract as the value of the transaction, effectively stealing the funds.  This is why it s important to ensure that ether is only accepted when it is needed and not left stuck in the contract, as it can be vulnerable to theft in future transactions.  One could be tempted to fix the issue by requiring that the gateway balance always equals 0 at the end of the transaction. However, this is not a good idea, as anyone could cause a Denial of Service in the gateway by sending a tiny amount of ETH.  One might also be tempted to fix this issue by requiring that msg.value == 0 iff fromToken != NATIVE_TOKEN_ADDRESS. However, this also poses a problem, as the gateway might execute multiple routes in a  for  loop. This could lead to reverting valid transactions (when both native and non-native tokens are involved).  The best way to solve this issue might be to compare the balance of the gateway before and after the transaction in all relevant functions. The balance should stay the same otherwise, something wrong happened, and we should revert the transaction. This could be implemented by adding a modifier in SocketGateway, that compares the balance of the gateway before and after the function call. Below is an example to illustrate the idea.  modifier checkGatewayBalance() {  uint256 initialBalance = address(this).balance;  _;  uint256 finalBalance = address(this).balance;  require(initialBalance == finalBalance, \"Gateway balance changed during execution\");  issue 6.1)  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"
    },
    {
        "title": "6.8 No Event Is Emitted When Invoking a Route Through the socketGateway Fallback Function ",
        "body": "  Resolution   Remediated as per the client team in   SocketDotTech/socket-ll-contracts#152. Further discussion about the scope of events in these cases is still ongoing.  Description  When a route is invoked through executeRoute, or executeRoutes functions, a SocketRouteExecuted event is emitted. However, a route can also be executed by invoking the fallback function of the socketGateway. And in that case, no event is emitted. This might impact off-chain systems that rely on those events.  Recommendation  Consider also emitting a SocketRouteExecuted event in case the route is invoked through the fallback function  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"
    },
    {
        "title": "6.9 Unused Error Codes. ",
        "body": "  Resolution   Remediated as per the client team in   SocketDotTech/socket-ll-contracts#148.  Description  SocketErrors.sol has errors that are defined but are not used:  error RouteAlreadyExist();  error ContractContainsNoCode();  error ControllerAlreadyExist();  error ControllerAddressIsZero();  It seems that they were created as errors that may have been expected to occur during the early stages of development, but the resulting architecture doesn t seem to have a place for them currently.  Examples  src/errors/SocketErrors.sol:L12-L19  error RouteAlreadyExist();  error SwapFailed();  error UnsupportedInterfaceId();  error ContractContainsNoCode();  error InvalidCelerRefund();  error CelerAlreadyRefunded();  error ControllerAlreadyExist();  error ControllerAddressIsZero();  Recommendation  Consider revisiting these errors and identifying whether they need to remain or can be removed.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"
    },
    {
        "title": "6.10 Inaccurate Interface. ",
        "body": "  Resolution   Remediated as per the client team in   SocketDotTech/socket-ll-contracts#149.  Description  ISocketGateway implies a bridge(uint32 routeId, bytes memory data) function, but there is no socket contract with a function like that, including the SocketGateway contract.  Examples  src/interfaces/ISocketGateway.sol:L32-L35  function bridge(  uint32 routeId,  bytes memory data  ) external payable returns (bytes memory);  Recommendation  Adjust the interface.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"
    },
    {
        "title": "6.11 Validate Array Length Matching Before Execution to Avoid Reverts ",
        "body": "  Resolution   Remediated as per the client team in   SocketDotTech/socket-ll-contracts#150 by adding the necessary array length checks.  Description  The Socket system not only aggregates different solutions via its routes and controllers but also allows to batch calls between them into one transaction. For example, a user may call swaps between several DEXs and then perform a bridge transfer.  As a result, the SocketGateway contract has many functions that accept multiple arrays that contain the necessary data for execution in their respective routes. However, these arrays need to be of the same length because individual elements in the arrays are intended to be matched at the same indices:  src/SocketGateway.sol:L196-L218  function executeRoutes(  uint32[] calldata routeIds,  bytes[] calldata dataItems,  bytes[] calldata eventDataItems  ) external payable {  uint256 routeIdslength = routeIds.length;  for (uint256 index = 0; index < routeIdslength; ) {  (bool success, bytes memory result) = addressAt(routeIds[index])  .delegatecall(dataItems[index]);  if (!success) {  assembly {  revert(add(result, 32), mload(result))  emit SocketRouteExecuted(routeIds[index], eventDataItems[index]);  unchecked {  ++index;  Note that in the above example function, all 3 different calldata arrays routeIds, dataItems, and eventDataItems were utilizing the same index to retrieve the correct element. A common practice in such cases is to confirm that the sizes of the arrays match before continuing with the execution of the rest of the transaction to avoid costly reverts that could happen due to  Index out of bounds  error.  Due to the aggregating and batching nature of the Socket system that may have its users rely on 3rd party offchain APIs to construct these array payloads, such as from APIs of the systems that Socket is integrating, a mishap in just any one of them could cause this issue.  Recommendation  Implement a check on the array lengths so they match.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"
    },
    {
        "title": "6.12 Destroyed Routes Eth Balances Will Be Left Locked in SocketDeployFactory ",
        "body": "  Resolution   Remediated as per the client team in   SocketDotTech/socket-ll-contracts#151 by adding rescue functions.  Description  SocketDeployFactory.destroy calls the killme function which in turn self-destructs the route and sends back any eth to the factory contract. However, these funds can not be claimed from the SocketDeployFactory contract.  Examples  src/deployFactory/SocketDeployFactory.sol:L170  function destroy(uint256 routeId) external onlyDisabler {  Recommendation  Make sure that these funds can be claimed.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"
    },
    {
        "title": "6.13 Possible Double Spends of msg.value in Code Paths That Include More Than One Delegatecall ",
        "body": "  Resolution  The client team has responded with the following note:  Adding the recommended CI/CD task to verify that future routes are delegate safe.  Description  The usage of msg.value multiple times in the context of a single transaction is dangerous and may lead to loss of funds as previously seen (in a different variation) in the Opyn hack. We were not able to find any concrete instance of the described issue, however, we do see how this pitfall may become an issue in future delegatee contracts.  Examples  Every code path that includes multiple delegatecalls, including:  SocketGateway.swapAndMultiBridge  the swapAndBridge function in all the different route contracts.  Recommendation  Consider implementing this recommendation.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"
    },
    {
        "title": "5.1 VotingMachine - tryToMoveToValidating can lock up proposals    ",
        "body": "  Resolution  Fixed per our recommendation.  Description  After a vote was received, the proposal can move to a validating state if any of the votes pass the proposal s precReq value, referred to as the minimum threshold.  code/contracts/governance/VotingMachine.sol:L391  tryToMoveToValidating(_proposalId);  Inside the method tryToMoveToValidating each of the vote options are checked to see if they pass precReq. In case that happens, the proposal goes into the next stage, specifically Validating.  code/contracts/governance/VotingMachine.sol:L394-L407  /// @notice Function to move to Validating the proposal in the case the last vote action  ///  was done before the required votingBlocksDuration passed  /// @param _proposalId The id of the proposal  function tryToMoveToValidating(uint256 _proposalId) public {  Proposal storage _proposal = proposals[_proposalId];  require(_proposal.proposalStatus == ProposalStatus.Voting, \"VOTING_STATUS_REQUIRED\");  if (_proposal.currentStatusInitBlock.add(_proposal.votingBlocksDuration) <= block.number) {  for (uint256 i = 0; i <= COUNT_CHOICES; i++) {  if (_proposal.votes[i] > _proposal.precReq) {  internalMoveToValidating(_proposalId);  The method internalMoveToValidating checks the proposal s status to be Voting and proceeds to moving the proposal into Validating state.  code/contracts/governance/VotingMachine.sol:L270-L278  /// @notice Internal function to change proposalStatus from Voting to Validating  /// @param _proposalId The id of the proposal  function internalMoveToValidating(uint256 _proposalId) internal {  Proposal storage _proposal = proposals[_proposalId];  require(_proposal.proposalStatus == ProposalStatus.Voting, \"ONLY_ON_VOTING_STATUS\");  _proposal.proposalStatus = ProposalStatus.Validating;  _proposal.currentStatusInitBlock = block.number;  emit StatusChangeToValidating(_proposalId);  The problem appears if multiple vote options go past the minimum threshold. This is because the loop does not stop after the first found option and the loop will fail when the method internalMoveToValidating is called a second time.  code/contracts/governance/VotingMachine.sol:L401-L405  for (uint256 i = 0; i <= COUNT_CHOICES; i++) {  if (_proposal.votes[i] > _proposal.precReq) {  internalMoveToValidating(_proposalId);  The method internalMoveToValidating fails the second time because the first time it is called, the proposal goes into the Validating state and the second time it is called, the require check fails.  code/contracts/governance/VotingMachine.sol:L274-L275  require(_proposal.proposalStatus == ProposalStatus.Voting, \"ONLY_ON_VOTING_STATUS\");  _proposal.proposalStatus = ProposalStatus.Validating;  This can lead to proposal lock-ups if there are enough votes to at least one option that pass the minimum threshold.  Recommendation  After moving to the Validating state return successfully.  function tryToMoveToValidating(uint256 _proposalId) public {  Proposal storage _proposal = proposals[_proposalId];  require(_proposal.proposalStatus == ProposalStatus.Voting, \"VOTING_STATUS_REQUIRED\");  if (_proposal.currentStatusInitBlock.add(_proposal.votingBlocksDuration) <= block.number) {  for (uint256 i = 0; i <= COUNT_CHOICES; i++) {  if (_proposal.votes[i] > _proposal.precReq) {  internalMoveToValidating(_proposalId);  return; // <- this was added  An additional change can be done to internalMoveToValidating because it is called only in tryToMoveToValidating and the parent method already does the check.  /// @notice Internal function to change proposalStatus from Voting to Validating  /// @param _proposalId The id of the proposal  function internalMoveToValidating(uint256 _proposalId) internal {  Proposal storage _proposal = proposals[_proposalId];  // The line below can be removed  // require(_proposal.proposalStatus == ProposalStatus.Voting, \"ONLY_ON_VOTING_STATUS\");  _proposal.proposalStatus = ProposalStatus.Validating;  _proposal.currentStatusInitBlock = block.number;  emit StatusChangeToValidating(_proposalId);  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/08/aave-governance-dao/"
    },
    {
        "title": "5.2 VotingMachine - verifyNonce should only allow the next nonce    ",
        "body": "  Resolution  Fixed per our recommendation.  Description  When a relayer calls submitVoteByRelayer they also need to provide a nonce. This nonce is cryptographicly checked against the provided signature. It is also checked again to be higher than the previous nonce saved for that voter.  code/contracts/governance/VotingMachine.sol:L232-L239  /// @notice Verifies the nonce of a voter on a proposal  /// @param _proposalId The id of the proposal  /// @param _voter The address of the voter  /// @param _relayerNonce The nonce submitted by the relayer  function verifyNonce(uint256 _proposalId, address _voter, uint256 _relayerNonce) public view {  Proposal storage _proposal = proposals[_proposalId];  require(_proposal.voters[_voter].nonce < _relayerNonce, \"INVALID_NONCE\");  When the vote is saved, the previous nonce is incremented.  code/contracts/governance/VotingMachine.sol:L387  voter.nonce = voter.nonce.add(1);  This leaves the opportunity to use the same signature to vote multiple times, as long as the provided nonce is higher than the incremented nonce.  Recommendation  The check should be more restrictive and make sure the consecutive nonce was provided.  require(_proposal.voters[_voter].nonce + 1 == _relayerNonce, \"INVALID_NONCE\");  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/08/aave-governance-dao/"
    },
    {
        "title": "5.3 VoteMachine - Cancelling vote does not increase the nonce    ",
        "body": "  Resolution  Fixed per our recommendation.  Description  A vote can be cancelled by calling cancelVoteByRelayer with the proposal ID, nonce, voter s address, signature and a hash of the sent params.  The parameters are hashed and checked against the signature correctly.  The nonce is part of these parameters and it is checked to be valid.  code/contracts/governance/VotingMachine.sol:L238  require(_proposal.voters[_voter].nonce < _relayerNonce, \"INVALID_NONCE\");  Once the vote is cancelled, the data is cleared but the nonce is not increased.  code/contracts/governance/VotingMachine.sol:L418-L434  if (_cachedVoter.balance > 0) {  _proposal.votes[_cachedVoter.vote] = _proposal.votes[_cachedVoter.vote].sub(_cachedVoter.balance.mul(_cachedVoter.weight));  _proposal.totalVotes = _proposal.totalVotes.sub(1);  voter.weight = 0;  voter.balance = 0;  voter.vote = 0;  voter.asset = address(0);  emit VoteCancelled(  _proposalId,  _voter,  _cachedVoter.vote,  _cachedVoter.asset,  _cachedVoter.weight,  _cachedVoter.balance,  uint256(_proposal.proposalStatus)  );  This means that in the future, the same signature can be used as long as the nonce is still higher than the current one.  Recommendation  Considering the recommendation from issue https://github.com/ConsenSys/aave-governance-dao-audit-2020-01/issues/4 is implemented, the nonce should also increase when the vote is cancelled. Otherwise the same signature can be replayed again.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/08/aave-governance-dao/"
    },
    {
        "title": "5.4 Possible lock ups with SafeMath multiplication   ",
        "body": "  Resolution  The situation described is unlikely to occur, and does not justify mitigations which might introduce other risks.  Description  In some cases using SafeMath can lead to a situation where a contract is locked up due to an unavoidable overflow.  It is theoretically possible that both the internalSubmitVote() and internalCancelVote() functions could become unusable by voters with a high enough balance, if the asset weighting is set extremely high.  Examples  This line in internalSubmitVote() could overflow if the voter s balance and the asset weight were sufficiently high:  code/contracts/governance/VotingMachine.sol:L379  uint256 _votingPower = _voterAssetBalance.mul(_assetWeight);  A similar situation occurs in internalCancelVote():  code/contracts/governance/VotingMachine.sol:L419-L420  _proposal.votes[_cachedVoter.vote] = _proposal.votes[_cachedVoter.vote].sub(_cachedVoter.balance.mul(_cachedVoter.weight));  _proposal.totalVotes = _proposal.totalVotes.sub(1);  Recommendation  This could be protected against by setting a maximum value for asset weights. In practice it is very unlikely to occur in this situation, but it could be introduced at some point in the future.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/08/aave-governance-dao/"
    },
    {
        "title": "6.1 Frontrunning attacks by the owner ",
        "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  There are few possible attack vectors by the owner:  All strategies have fees from rewards. In addition to that, the PancakeSwap strategy has deposit fees. The default deposit fees equal zero; the maximum is limited to 5%: wheat-v1-core-audit/contracts/PancakeSwapCompoundingStrategyToken.sol:L29-L33 uint256 constant MAXIMUM_DEPOSIT_FEE = 5e16; // 5% uint256 constant DEFAULT_DEPOSIT_FEE = 0e16; // 0%  uint256 constant MAXIMUM_PERFORMANCE_FEE = 50e16; // 50% uint256 constant DEFAULT_PERFORMANCE_FEE = 10e16; // 10% When a user deposits tokens, expecting to have zero deposit fees, the owner can frontrun the deposit and increase fees to 5%. If the deposit size is big enough, that may be a significant amount of money.  In the gulp function, the reward tokens are exchanged for the reserve tokens on the exchange: wheat-v1-core-audit/contracts/PancakeSwapCompoundingStrategyToken.sol:L218-L244 function gulp(uint256 _minRewardAmount) external onlyEOAorWhitelist nonReentrant { \tuint256 _pendingReward = _getPendingReward(); \tif (_pendingReward > 0) { \t\t_withdraw(0); \t} \t{ \t\tuint256 _totalReward = Transfers._getBalance(rewardToken); \t\tuint256 _feeReward = _totalReward.mul(performanceFee) / 1e18; \t\tTransfers._pushFunds(rewardToken, collector, _feeReward); \t} \tif (rewardToken != routingToken) { \t\trequire(exchange != address(0), \"exchange not set\"); \t\tuint256 _totalReward = Transfers._getBalance(rewardToken); \t\tTransfers._approveFunds(rewardToken, exchange, _totalReward); \t\tIExchange(exchange).convertFundsFromInput(rewardToken, routingToken, _totalReward, 1); \t} \tif (routingToken != reserveToken) { \t\trequire(exchange != address(0), \"exchange not set\"); \t\tuint256 _totalRouting = Transfers._getBalance(routingToken); \t\tTransfers._approveFunds(routingToken, exchange, _totalRouting); \t\tIExchange(exchange).joinPoolFromInput(reserveToken, routingToken, _totalRouting, 1); \t} \tuint256 _totalBalance = Transfers._getBalance(reserveToken); \trequire(_totalBalance >= _minRewardAmount, \"high slippage\"); \t_deposit(_totalBalance); } The owner can change the exchange parameter to the malicious address that steals tokens. The owner then calls gulp with _minRewardAmount==0, and all the rewards will be stolen. The same attack can be implemented in fee collectors and the buyback contract.  Recommendation  Use a timelock to avoid instant changes of the parameters.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"
    },
    {
        "title": "6.2 New deposits are instantly getting a share of undistributed rewards ",
        "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  When a new deposit is happening, the current pending rewards are not withdrawn and re-invested yet. And they are not taken into account when calculating the number of shares that the depositor receives. The number of shares is calculated as if there were no pending rewards. The other side of this issue is that all the withdrawals are also happening without considering the pending rewards. So currently, it makes more sense to withdraw right after gulp to gather the rewards. In addition to the general  unfairness  of the reward distribution during the deposit/withdrawal, there is also an attack vector created by this issue.  The Attack  If the deposit is made right before the gulp function is called, the rewards from the gulp are distributed evenly across all the current deposits, including the ones that were just recently made. So if the deposit-gulp-withdraw sequence is executed, the caller receives guaranteed profit. If the attacker also can execute these functions briefly (in one block or transaction) and take a huge loan to deposit a lot of tokens, almost all the rewards from the gulp will be stolen by the attacker. The easy 1-transaction attack with a flashloan can be done by the owner, miner, whitelisted contracts, or any contract if the onlyEOAorWhitelist modifier is disabled or stops working (https://github.com/ConsenSys/growthdefi-audit-2021-06/issues/3). Even if onlyEOAorWhitelist is working properly, anyone can take a regular loan to make the attack. The risk is not that big because no price manipulation is required. The price will likely remain the same during the attack (few blocks maximum).  Recommendation  If issue issue 6.3 is fixed while allowing anyone call the gulp contract, the best solution would be to include the gulp call at the beginning of the deposit and withdraw. In case of withdrawing, there should also be an option to avoid calling gulp as the emergency case.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"
    },
    {
        "title": "6.3 Proactive sandwiching of the gulp calls ",
        "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  Each strategy token contract provides a gulp method to fetch pending rewards, convert them into the reserve token and split up the balances. One share is sent to the fee collector as a performance fee, while the rest is deposited into the respective MasterChef contract to accumulate more rewards. Suboptimal trades are prevented by passing a minimum slippage value with the function call, which results in revert if the expected reserve token amount cannot be provided by the trade(s).  The slippage parameter and the trades performed in gulp open the function up to proactive sandwich attacks. The slippage parameter can be freely set by the attacker, resulting in the system performing arbitrarily bad trades based on how much the attacker can manipulate the liquidity of involved assets around the gulp function call.  This attack vector is significant under the following assumptions:  The exchange the trade is performed on allows significant changes in liquidity pools in a single transaction (e.g., not limiting transactions to X% of the pool amount),  The attacker can frontrun legitimate gulp calls with reasonable slippage values,  Trades are performed, i.e. when rewardToken != routingToken and/or routingToken != reserveToken hold true.  Examples  This affects the gulp functions in all the strategies:  PancakeSwapCompoundingStrategyToken  AutoFarmCompoundingStrategyToken  PantherSwapCompoundingStrategyToken  and also fees collectors and the buyback adapters:  PantherSwapBuybackAdapter  AutoFarmFeeCollectorAdapter  PancakeSwapFeeCollector  UniversalBuyback  Recommendation  There are different possible solutions to this issue and all have some tradeoffs. Initially, we came up with the following suggestion:  The onlyOwner modifier should be added to the gulp function to ensure only authorized parties with reasonable slippages can execute trades on behalf of the strategy contracts. Furthermore, additional slippage checks can be added to avoid unwanted behavior of authorized addresses, e.g., to avoid a bot setting unreasonable slippage values due to a software bug.  But in order to fix another issue (https://github.com/ConsenSys/growthdefi-audit-2021-06/issues/8), we came up with the alternative solution:  Use oracles to restrict users from calling the gulp function with unreasonable slippage (more than 5% from the oracle s moving average price). The side effect of that solution is that sometimes the outdated price will be used. That means that when the price crashes, nobody will be able to call the gulp.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"
    },
    {
        "title": "6.4 Expected amounts of tokens in the withdraw function ",
        "body": "  Resolution  Client s statement :  This issue did not really need fixing. The mitigation was already in place by depositing a tiny amount of the reserve into the contract, if necessary   Description  Every withdraw function in the strategy contracts is calculating the expected amount of the returned tokens before withdrawing them:  wheat-v1-core-audit/contracts/PantherSwapCompoundingStrategyToken.sol:L200-L208  function withdraw(uint256 _shares, uint256 _minAmount) external onlyEOAorWhitelist nonReentrant  address _from = msg.sender;  (uint256 _amount, uint256 _withdrawalAmount, uint256 _netAmount) = _calcAmountFromShares(_shares);  require(_netAmount >= _minAmount, \"high slippage\");  _burn(_from, _shares);  _withdraw(_amount);  Transfers._pushFunds(reserveToken, _from, _withdrawalAmount);  After that, the contract is trying to transfer this pre-calculated amount to the msg.sender. It is never checked whether the intended amount was actually transferred to the strategy contract. If the amount is lower, that may result in reverting the withdraw function all the time and locking up tokens.  Even though we did not find any specific case of returning a different amount of tokens, it is still a good idea to handle this situation to minimize relying on the security of the external contracts.  Recommendation  There are a few options how to mitigate the issue:  Double-check the balance difference before and after the MasterChef s withdraw function is called.  Handle this situation in the emergency mode (https://github.com/ConsenSys/growthdefi-audit-2021-06/issues/11).  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"
    },
    {
        "title": "6.5 Emergency mode of the MasterChef contracts is not supported ",
        "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  All the underlying MasterChef contracts have the emergency withdrawal mode, which allows simpler withdrawal (excluding the rewards):  // Withdraw without caring about rewards. EMERGENCY ONLY.  function emergencyWithdraw(uint256 _pid) public nonReentrant {  PoolInfo storage pool = poolInfo[_pid];  UserInfo storage user = userInfo[_pid][msg.sender];  uint256 amount = user.amount;  user.amount = 0;  user.rewardDebt = 0;  user.rewardLockedUp = 0;  user.nextHarvestUntil = 0;  pool.lpToken.safeTransfer(address(msg.sender), amount);  emit EmergencyWithdraw(msg.sender, _pid, amount);  // Withdraw without caring about rewards. EMERGENCY ONLY.  function emergencyWithdraw(uint256 _pid) public {  PoolInfo storage pool = poolInfo[_pid];  UserInfo storage user = userInfo[_pid][msg.sender];  pool.lpToken.safeTransfer(address(msg.sender), user.amount);  emit EmergencyWithdraw(msg.sender, _pid, user.amount);  user.amount = 0;  user.rewardDebt = 0;  // Withdraw without caring about rewards. EMERGENCY ONLY.  function emergencyWithdraw(uint256 _pid) public nonReentrant {  PoolInfo storage pool = poolInfo[_pid];  UserInfo storage user = userInfo[_pid][msg.sender];  uint256 wantLockedTotal =  IStrategy(poolInfo[_pid].strat).wantLockedTotal();  uint256 sharesTotal = IStrategy(poolInfo[_pid].strat).sharesTotal();  uint256 amount = user.shares.mul(wantLockedTotal).div(sharesTotal);  IStrategy(poolInfo[_pid].strat).withdraw(msg.sender, amount);  pool.want.safeTransfer(address(msg.sender), amount);  emit EmergencyWithdraw(msg.sender, _pid, amount);  user.shares = 0;  user.rewardDebt = 0;  While it s hard to predict how and why the emergency mode can be enabled in the underlying MasterChef contracts, these functions are there for a reason, and it s safer to be able to use them. If some emergency happens and this is the only way to withdraw funds, the funds in the strategy contracts will be locked forever.  Recommendation  Add the emergency mode implementation.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"
    },
    {
        "title": "6.6 The capping mechanism for Panther token leads to increased fees ",
        "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  Panther token has a cap in transfer sizes, so any transfer in the contract is limited beforehand:  wheat-v1-core-audit/contracts/PantherSwapCompoundingStrategyToken.sol:L218-L245  function gulp(uint256 _minRewardAmount) external onlyEOAorWhitelist nonReentrant  uint256 _pendingReward = _getPendingReward();  if (_pendingReward > 0) {  _withdraw(0);  uint256 __totalReward = Transfers._getBalance(rewardToken);  (uint256 _feeReward, uint256 _retainedReward) = _capFeeAmount(__totalReward.mul(performanceFee) / 1e18);  Transfers._pushFunds(rewardToken, buyback, _feeReward);  if (rewardToken != routingToken) {  require(exchange != address(0), \"exchange not set\");  uint256 _totalReward = Transfers._getBalance(rewardToken);  _totalReward = _capTransferAmount(rewardToken, _totalReward, _retainedReward);  Transfers._approveFunds(rewardToken, exchange, _totalReward);  IExchange(exchange).convertFundsFromInput(rewardToken, routingToken, _totalReward, 1);  if (routingToken != reserveToken) {  require(exchange != address(0), \"exchange not set\");  uint256 _totalRouting = Transfers._getBalance(routingToken);  _totalRouting = _capTransferAmount(routingToken, _totalRouting, _retainedReward);  Transfers._approveFunds(routingToken, exchange, _totalRouting);  IExchange(exchange).joinPoolFromInput(reserveToken, routingToken, _totalRouting, 1);  uint256 _totalBalance = Transfers._getBalance(reserveToken);  _totalBalance = _capTransferAmount(reserveToken, _totalBalance, _retainedReward);  require(_totalBalance >= _minRewardAmount, \"high slippage\");  _deposit(_totalBalance);  Fees here are calculated from the full amount of rewards (__totalReward ):  wheat-v1-core-audit/contracts/PantherSwapCompoundingStrategyToken.sol:L225  (uint256 _feeReward, uint256 _retainedReward) = _capFeeAmount(__totalReward.mul(performanceFee) / 1e18);  But in fact, if the amount of the rewards is too big, it will be capped, and the residuals will be  taxed  again during the next call of the gulp function. That behavior leads to multiple taxations of the same tokens, which means increased fees.  Recommendation  The best solution would be to cap __totalReward  first and then calculate fees from the capped value.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"
    },
    {
        "title": "6.7 The _capFeeAmount function is not working as intended ",
        "body": "  Resolution  Client s statement :  With the fix of 6.6 this code was removed and therefore no changes were required. \"  Description  Panther token has a limit on the transfer size. Because of that, all the Panther transfer values in the PantherSwapCompoundingStrategyToken are also capped beforehand. The following function is called to cap the size of fees:  wheat-v1-core-audit/contracts/PantherSwapCompoundingStrategyToken.sol:L357-L366  function _capFeeAmount(uint256 _amount) internal view returns (uint256 _capped, uint256 _retained)  _retained = 0;  uint256 _limit = _calcMaxRewardTransferAmount();  if (_amount > _limit) {  _amount = _limit;  _retained = _amount.sub(_limit);  return (_amount, _retained);  This function should return the capped amount and the amount of retained tokens. But because the _amount is changed before calculating the _retained, the retained amount will always be 0.  Recommendation  Calculate the retained value before changing the amount.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"
    },
    {
        "title": "6.8 Stale split ratios in UniversalBuyback ",
        "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  The gulp and pendingBurning functions of the UniversalBuyback contract use the hardcoded, constant values of DEFAULT_REWARD_BUYBACK1_SHARE and DEFAULT_REWARD_BUYBACK2_SHARE to determine the ratio the trade value is split with.  Consequently, any call to setRewardSplit to set a new ratio will be ineffective but still result in a ChangeRewardSplit event being emitted. This event can deceive system operators and users as it does not reflect the correct values of the contract.  Examples  wheat-v1-core-audit/contracts/UniversalBuyback.sol:L80-L81  uint256 _amount1 = _balance.mul(DEFAULT_REWARD_BUYBACK1_SHARE) / 1e18;  uint256 _amount2 = _balance.mul(DEFAULT_REWARD_BUYBACK2_SHARE) / 1e18;  wheat-v1-core-audit/contracts/UniversalBuyback.sol:L97-L98  uint256 _amount1 = _balance.mul(DEFAULT_REWARD_BUYBACK1_SHARE) / 1e18;  uint256 _amount2 = _balance.mul(DEFAULT_REWARD_BUYBACK2_SHARE) / 1e18;  Recommendation  Instead of the default values, rewardBuyback1Share and rewardBuyback2Share should be used.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"
    },
    {
        "title": "6.9 Future-proofness of the onlyEOAorWhitelist modifier ",
        "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  The onlyEOAorWhitelist modifier is used in various locations throughout the code. It performs a check that asserts the message sender being equal to the transaction origin to assert the calling party is not a smart contract.  This approach may stop working if EIP-3074 and its AUTH and AUTHCALL opcodes get deployed.  While the OpenZeppelin reentrancy guard does not depend on tx.origin, the EOA check does. Its evasion can result in additional attack vectors such as flash loans opening up. It is noteworthy that preventing smart contract interaction with the protocol may limit its opportunities as smart contracts cannot integrate with it in the same way that GrowthDeFi integrates with its third-party service providers.  The onlyEOAorWhitelist modifier may give a false sense of security because it won t allow making a flash loan attack by most of the users. But the same attack can still be made by some people or with more risk:  The owner and the whitelisted contracts are not affected by the modifier.  The modifier can be disabled: **wheat-v1-core-audit/contracts/WhitelistGuard.sol:L21-L28** ```solidity modifier onlyEOAorWhitelist() { \tif (enabled) { \t\taddress _from = _msgSender(); \t\trequire(tx.origin == _from || whitelist.contains(_from), \"access denied\"); \t} \t_; } ```  And in the deployment script, this modifier is disabled for testing purposes, and it s important not to forget to turn it in on the production: wheat-v1-core-audit/migrations/02_deploy_contracts.js:L50 await pancakeSwapFeeCollector.setWhitelistEnabled(false); // allows testing  The attack can usually be split into multiple transactions. Miners can put these transactions closely together and don t take any additional risk. Regular users can take a risk, take the loan, and execute the attack in multiple transactions or even blocks.  Recommendation  It is strongly recommended to monitor the progress of this EIP and its potential implementation on the Binance Smart Chain. If this functionality gets enabled, the development team should update the contract system to use the new opcodes. We also strongly recommend relying less on the fact that only EOA will call the functions. It is better to write the code that can be called by the external smart contracts without compromising its security.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"
    },
    {
        "title": "6.10 Exchange owner might steal users  funds using reentrancy ",
        "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  The practice of pulling funds from a user (by using safeTransferFrom) and then later pushing (some) of the funds back to the user occurs in various places in the Exchange contract. In case one of the used token contracts (or one of its dependent calls) externally calls the Exchange owner, the owner may utilize that to call back Exchange.recoverLostFunds and drain (some) user funds.  Examples  wheat-v1-core-audit/contracts/Exchange.sol:L80-L89  function convertFundsFromInput(address _from, address _to, uint256 _inputAmount, uint256 _minOutputAmount) external override returns (uint256 _outputAmount)  address _sender = msg.sender;  Transfers._pullFunds(_from, _sender, _inputAmount);  _inputAmount = Math._min(_inputAmount, Transfers._getBalance(_from)); // deals with potential transfer tax  _outputAmount = UniswapV2ExchangeAbstraction._convertFundsFromInput(router, _from, _to, _inputAmount, _minOutputAmount);  _outputAmount = Math._min(_outputAmount, Transfers._getBalance(_to)); // deals with potential transfer tax  Transfers._pushFunds(_to, _sender, _outputAmount);  return _outputAmount;  wheat-v1-core-audit/contracts/Exchange.sol:L121-L130  function joinPoolFromInput(address _pool, address _token, uint256 _inputAmount, uint256 _minOutputShares) external override returns (uint256 _outputShares)  address _sender = msg.sender;  Transfers._pullFunds(_token, _sender, _inputAmount);  _inputAmount = Math._min(_inputAmount, Transfers._getBalance(_token)); // deals with potential transfer tax  _outputShares = UniswapV2LiquidityPoolAbstraction._joinPoolFromInput(router, _pool, _token, _inputAmount, _minOutputShares);  _outputShares = Math._min(_outputShares, Transfers._getBalance(_pool)); // deals with potential transfer tax  Transfers._pushFunds(_pool, _sender, _outputShares);  return _outputShares;  wheat-v1-core-audit/contracts/Exchange.sol:L99-L111  function convertFundsFromOutput(address _from, address _to, uint256 _outputAmount, uint256 _maxInputAmount) external override returns (uint256 _inputAmount)  address _sender = msg.sender;  Transfers._pullFunds(_from, _sender, _maxInputAmount);  _maxInputAmount = Math._min(_maxInputAmount, Transfers._getBalance(_from)); // deals with potential transfer tax  _inputAmount = UniswapV2ExchangeAbstraction._convertFundsFromOutput(router, _from, _to, _outputAmount, _maxInputAmount);  uint256 _refundAmount = _maxInputAmount - _inputAmount;  _refundAmount = Math._min(_refundAmount, Transfers._getBalance(_from)); // deals with potential transfer tax  Transfers._pushFunds(_from, _sender, _refundAmount);  _outputAmount = Math._min(_outputAmount, Transfers._getBalance(_to)); // deals with potential transfer tax  Transfers._pushFunds(_to, _sender, _outputAmount);  return _inputAmount;  wheat-v1-core-audit/contracts/Exchange.sol:L139-L143  function recoverLostFunds(address _token) external onlyOwner  uint256 _balance = Transfers._getBalance(_token);  Transfers._pushFunds(_token, treasury, _balance);  Recommendation  Reentrancy guard protection should be added to Exchange.convertFundsFromInput, Exchange.convertFundsFromOutput, Exchange.joinPoolFromInput, Exchange.recoverLostFunds at least, and in general to all public/external functions since gas price considerations are less relevant for contracts deployed on BSC.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"
    },
    {
        "title": "5.1 VaultConfig.setVaultConfig doesn t check all critical arguments ",
        "body": "  Resolution  Remediated per Notional s team notes in commit by adding the following checks:  Checks to ensure borrow currency and secondary currencies cannot change once set  Check to ensure liquidationRate does not exceed minCollateralRatioBPS  Check for maxBorrowMarketIndex was not added. The Notional team will review this parameter on a case-by-case basis as for some vaults borrowing idiosyncratic fCash may not be an issue  Description  The Notional Strategy Vaults need to get whitelisted and have specific Notional parameters set in order to interact with the rest of the Notional system. This is done through VaultAction.updateVault() where the owner address can provide a VaultConfigStorage calldata vaultConfig argument to either whitelist a new vault or change an existing one. While this is to be performed by a trusted privileged actor (the owner), and it could be assumed they are careful with their updates, the contracts themselves don t perform enough checks on the validity of the parameters, either in isolation or when compared against the existing vault state. Below are examples of arguments that should be better checked.  borrowCurrencyId  The borrowCurrencyId parameter gets provided to TokenHandler.getAssetToken() and TokenHandler.getUnderlyingToken() to retrieve its associated TokenStorage object and verify that the currency doesn t have transfer fees.  contracts-v2/contracts/internal/vaults/VaultConfiguration.sol:L162-L164  Token memory assetToken = TokenHandler.getAssetToken(vaultConfig.borrowCurrencyId);  Token memory underlyingToken = TokenHandler.getUnderlyingToken(vaultConfig.borrowCurrencyId);  require(!assetToken.hasTransferFee && !underlyingToken.hasTransferFee);  However, these calls retrieve data from the mapping from storage which returns an empty struct for an unassigned currency ID. This would pass the check in the last require statement regarding the transfer fees and would successfully allow to set the currency even if isn t actually registered in Notional. The recommendation would be to check that the returned TokenStorage object has data inside of it, perhaps by checking the decimals on the token.  In the event that this is a call to update the configuration on a vault instead of whitelisting a whole new vault, this would also allow to switch the borrow currency without checking that the existing borrow and lending accounting has been cleared. This could cause accounting issues. A check for existing debt before swapping the borrow currency IDs is recommended.  liquidationRate and minCollateralRatioBPS  contracts-v2/contracts/external/actions/VaultAccountAction.sol:L274-L283  uint256 vaultSharesToLiquidator;  vaultSharesToLiquidator = vaultAccount.tempCashBalance.toUint()  .mul(vaultConfig.liquidationRate.toUint())  .mul(vaultAccount.vaultShares)  .div(vaultShareValue.toUint())  .div(uint256(Constants.RATE_PRECISION));  vaultAccount.vaultShares = vaultAccount.vaultShares.sub(vaultSharesToLiquidator);  maxBorrowMarketIndex  The current Strategy Vault implementation does not allow for idiosyncratic cash because it causes issues during exits as there are no active markets for the account s maturity. Therefore, the configuration shouldn t be set with maxBorrowMarketIndex >=3 as that would open up the 1 Year maturity for vault accounts that could cause idiosyncratic fCash. The recommendation would be to add that check.  secondaryBorrowCurrencies  Similarly to the borrowCurrencyId, there are few checks that actually determine that the secondaryBorrowCurrencies[] given are actually registered in Notional. This is, however, more inline with how some vaults are supposed to work as they may have no secondary currencies at all, such as when the secondaryBorrowCurrencies[] id is given as 0. In the event that this is a call to update the configuration on a vault instead of whitelisting a whole new vault, this would also allow to switch the secondary borrow currency without checking that the existing borrow and lending accounting has been cleared. For example, the VaultAction.updateSecondaryBorrowCapacity() function could be invoked on the new set of secondary currencies and simply increase the borrow there. This could cause accounting issues. A check for existing debt before swapping the borrow currency IDs is recommended.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"
    },
    {
        "title": "5.2 Handle division by 0 ",
        "body": "  Resolution  Remediated per Notional s team notes in commit by adding the following checks:  Check to account for div by zero in settle vault account  Short circuit to ensure debtSharesToRepay is never zero. Divide by zero may still occur but this would signal a critical accounting issue  The Notional team also acknowledged that the contract will revert when vaultShareValue = 0. The team decided to not make any changes related to that since liquidation will not accomplish anything for an account with no vault share value.  Description  There are a few places in the code where division by zero may occur but isn t handled.  Examples  If the vault settles at exactly 0 value with 0 remaining strategy token value, there may be an unhandled division by zero trying to divide claims on the settled assets:  contracts-v2/contracts/internal/vaults/VaultAccount.sol:L424-L436  int256 settledVaultValue = settlementRate.convertToUnderlying(residualAssetCashBalance)  .add(totalStrategyTokenValueAtSettlement);  // If the vault is insolvent (meaning residualAssetCashBalance < 0), it is necessarily  // true that totalStrategyTokens == 0 (meaning all tokens were sold in an attempt to  // repay the debt). That means settledVaultValue == residualAssetCashBalance, strategyTokenClaim == 0  // and assetCashClaim == totalAccountValue. Accounts that are still solvent will be paid from the  // reserve, accounts that are insolvent will have a totalAccountValue == 0.  strategyTokenClaim = totalAccountValue.mul(vaultState.totalStrategyTokens.toInt())  .div(settledVaultValue).toUint();  assetCashClaim = totalAccountValue.mul(residualAssetCashBalance)  .div(settledVaultValue);  If a vault account is entirely insolvent and its vaultShareValue is zero, there will be an unhandled division by zero during liquidation:  contracts-v2/contracts/external/actions/VaultAccountAction.sol:L274-L281  uint256 vaultSharesToLiquidator;  vaultSharesToLiquidator = vaultAccount.tempCashBalance.toUint()  .mul(vaultConfig.liquidationRate.toUint())  .mul(vaultAccount.vaultShares)  .div(vaultShareValue.toUint())  .div(uint256(Constants.RATE_PRECISION));  If a vault account s secondary debt is being repaid when there is none, there will be an unhandled division by zero:  contracts-v2/contracts/internal/vaults/VaultConfiguration.sol:L661-L666  VaultSecondaryBorrowStorage storage balance =  LibStorage.getVaultSecondaryBorrow()[vaultConfig.vault][maturity][currencyId];  uint256 totalfCashBorrowed = balance.totalfCashBorrowed;  uint256 totalAccountDebtShares = balance.totalAccountDebtShares;  fCashToLend = debtSharesToRepay.mul(totalfCashBorrowed).div(totalAccountDebtShares).toInt();  While these cases may be unlikely today, this code could be reutilized in other circumstances later that could cause reverts and even disrupt operations more frequently.  Recommendation  Handle the cases where the denominator could be zero appropriately.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"
    },
    {
        "title": "5.3 Increasing a leveraged position in a vault with secondary borrow currency will revert ",
        "body": "  Resolution   Per Notional team s notes, they have rearranged if statement to ensure that increasing an existing position will work. The proposed solution was skipped as it creates issues with the   Commit  Description  From the client s specifications for the strategy vaults, we know that accounts should be able to increase their leveraged positions before maturity. This property will not hold for the vaults that require borrowing a secondary currency to enter a position. When an account opens its position in such vault for the first time, the VaultAccountSecondaryDebtShareStorage.maturity is set to the maturity an account has entered. When the account is trying to increase the debt position, an accounts current maturity will be checked, and since it is not set to 0, as in the case where an account enters the vault for the first time, nor it is smaller than the new maturity passed by an account as in case of a rollover, the code will revert.  Examples  contracts-v2/contracts/external/actions/VaultAction.sol:L226-L228  if (accountMaturity != 0) {  // Cannot roll to a shorter term maturity  require(accountMaturity < maturity);  Recommendation  In order to fix this issue, we recommend that < is replaced with <= so that account can enter the vault maturity the account is already in as well as the future once.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"
    },
    {
        "title": "5.4 Secondary Currency debt is not managed by the Notional Controller ",
        "body": "  Resolution   Remediated per Notional s team notes in   commit by adding valuation for secondary borrow within the vault.  Description  Some of the Notional Strategy Vaults may allow for secondary currencies to be borrowed as part of the same strategy. For example, a strategy may allow for USDC to be its primary borrow currency as well as have ETH as its secondary borrow currency.  In order to enter the vault, a user would have to deposit depositAmountExternal of the primary borrow currency when calling VaultAccountAction.enterVault(). This would allow the user to borrow with leverage, as long as the vaultConfig.checkCollateralRatio() check on that account succeeds, which is based on the initial deposit and borrow currency amounts. This collateral ratio check is then performed throughout that user account s lifecycle in that vault, such as when they try to roll their maturity, or when liquidators try to perform collateral checks to ensure there is no bad debt.  However, in the event that the vault has a secondary borrow currency as well, that additional secondary debt is not calculated as part of the checkCollateralRatio() check. The only debt that is being considered is the vaultAccount.fCash that corresponds to the primary borrow currency debt:  contracts-v2/contracts/internal/vaults/VaultConfiguration.sol:L313-L319  function checkCollateralRatio(  VaultConfig memory vaultConfig,  VaultState memory vaultState,  VaultAccount memory vaultAccount  ) internal view {  (int256 collateralRatio, /* */) = calculateCollateralRatio(  vaultConfig, vaultState, vaultAccount.account, vaultAccount.vaultShares, vaultAccount.fCash  contracts-v2/contracts/internal/vaults/VaultConfiguration.sol:L278-L292  function calculateCollateralRatio(  VaultConfig memory vaultConfig,  VaultState memory vaultState,  address account,  uint256 vaultShares,  int256 fCash  ) internal view returns (int256 collateralRatio, int256 vaultShareValue) {  vaultShareValue = vaultState.getCashValueOfShare(vaultConfig, account, vaultShares);  // We do not discount fCash to present value so that we do not introduce interest  // rate risk in this calculation. The economic benefit of discounting will be very  // minor relative to the added complexity of accounting for interest rate risk.  // Convert fCash to a positive amount of asset cash  int256 debtOutstanding = vaultConfig.assetRate.convertFromUnderlying(fCash.neg());  Whereas the value of strategy tokens that belong to that user account are being calculated by calling IStrategyVault(vault).convertStrategyToUnderlying() on the associated strategy vault:  contracts-v2/contracts/internal/vaults/VaultState.sol:L314-L324  function getCashValueOfShare(  VaultState memory vaultState,  VaultConfig memory vaultConfig,  address account,  uint256 vaultShares  ) internal view returns (int256 assetCashValue) {  if (vaultShares == 0) return 0;  (uint256 assetCash, uint256 strategyTokens) = getPoolShare(vaultState, vaultShares);  int256 underlyingInternalStrategyTokenValue = _getStrategyTokenValueUnderlyingInternal(  vaultConfig.borrowCurrencyId, vaultConfig.vault, account, strategyTokens, vaultState.maturity  );  contracts-v2/contracts/internal/vaults/VaultState.sol:L296-L311  function _getStrategyTokenValueUnderlyingInternal(  uint16 currencyId,  address vault,  address account,  uint256 strategyTokens,  uint256 maturity  ) private view returns (int256) {  Token memory token = TokenHandler.getUnderlyingToken(currencyId);  // This will be true if the the token is \"NonMintable\" meaning that it does not have  // an underlying token, only an asset token  if (token.decimals == 0) token = TokenHandler.getAssetToken(currencyId);  return token.convertToInternal(  IStrategyVault(vault).convertStrategyToUnderlying(account, strategyTokens, maturity)  );  From conversations with the Notional team, it is assumed that this call returns the strategy token value subtracted against the secondary currencies debt, as is the case in the Balancer2TokenVault for example. In other words, when collateral ratio checks are performed, those strategy vaults that utilize secondary currency borrows would need to calculate the value of strategy tokens already accounting for any secondary debt. However, this is a dependency for a critical piece of the Notional controller s strategy vaults collateral checks.  Therefore, even though the strategy vaults  code and logic would be vetted before their whitelisting into the Notional system, they would still remain an external dependency with relatively arbitrary code responsible for the liquidation infrastructure that could lead to bad debt or incorrect liquidations if the vaults give inaccurate information, and thus potential loss of funds.  Recommendation  Specific strategy vault implementations using secondary borrows were not in scope of this audit. However, since the core Notional Vault system was, and it includes secondary borrow currency functionality, from the point of view of the larger Notional system it is recommended to include secondary debt checks within the Notional controller contract to reduce external dependency on the strategy vaults  logic.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"
    },
    {
        "title": "5.5 Vaults are unable to borrow single secondary currency ",
        "body": "  Resolution  Remediated per Notional s team notes.  Description  As was previously mentioned some strategies require borrowing one or two secondary currencies. All secondary currencies have to be whitelisted in the VaultConfig.secondaryBorrowCurrencies. Borrow operation on secondary currencies is performed in the borrowSecondaryCurrencyToVault(...) function. Due to a require statement in that function, vaults will only be able to borrow secondary currencies if both of the currencies are whitelisted in VaultConfig.secondaryBorrowCurrencies. Considering that many strategies will have just one secondary currency, this will prevent those strategies from borrowing any secondary assets.  Examples  contracts-v2/contracts/external/actions/VaultAction.sol:L214  require(currencies[0] != 0 && currencies[1] != 0);  Recommendation  contracts-v2/contracts/external/actions/VaultAction.sol:L202-L208  function borrowSecondaryCurrencyToVault(  address account,  uint256 maturity,  uint256[2] calldata fCashToBorrow,  uint32[2] calldata maxBorrowRate,  uint32[2] calldata minRollLendRate  ) external override returns (uint256[2] memory underlyingTokensTransferred) {  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"
    },
    {
        "title": "5.6 An account roll may be impossible if the vault is already at the maximum borrow capacity. ",
        "body": "  Resolution   Remediated per Notional s team notes in   commit by adding the ability for accounts to deposit during a roll vault position call to offset any additional cost that would put them over the maximum borrow capacity.  Description  One of the actions allowed in Notional Strategy Vaults is to roll an account s maturity to a later one by borrowing from a later maturity and repaying that into the debt of the earlier maturity.  However, this could cause an issue if the vault is at maximum capacity at the time of the roll. When an account performs this type of roll, the new borrow would have to be more than the existing debt simply because it has to at least cover the existing debt and pay for the borrow fees that get added on every new borrow. Since the whole vault was already at max borrow capacity before with the old, smaller borrow, this process would revert at the end after the new borrow as well once the process gets to VaultAccount.updateAccountfCash and VaultConfiguration.updateUsedBorrowCapacity:  contracts-v2/contracts/internal/vaults/VaultConfiguration.sol:L243-L257  function updateUsedBorrowCapacity(  address vault,  uint16 currencyId,  int256 netfCash  ) internal returns (int256 totalUsedBorrowCapacity) {  VaultBorrowCapacityStorage storage cap = LibStorage.getVaultBorrowCapacity()[vault][currencyId];  // Update the total used borrow capacity, when borrowing this number will increase (netfCash < 0),  // when lending this number will decrease (netfCash > 0).  totalUsedBorrowCapacity = int256(uint256(cap.totalUsedBorrowCapacity)).sub(netfCash);  if (netfCash < 0) {  // Always allow lending to reduce the total used borrow capacity to satisfy the case when the max borrow  // capacity has been reduced by governance below the totalUsedBorrowCapacity. When borrowing, it cannot  // go past the limit.  require(totalUsedBorrowCapacity <= int256(uint256(cap.maxBorrowCapacity)), \"Max Capacity\");  The result is that users won t able to roll while the vault is at max capacity. However, users may exit some part of their position to reduce their borrow, thereby reducing the overall vault borrow capacity, and then could execute the roll. A bigger problem would occur if the vault configuration got updated to massively reduce the borrow capacity, which would force users to exit their position more significantly with likely a much smaller chance at being able to roll.  Recommendation  Document this case so that users can realise that rolling may not always be an option. Perhaps consider adding ways where users can pay a small deposit, like on enterVault, to offset the additional difference in borrows and pay for fees so they can remain with essentially the same size position within Notional.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"
    },
    {
        "title": "5.7 Rollover might introduce economically impractical deposits of dust into a strategy ",
        "body": "  Resolution  Acknowledged with a note from the Notional s team:   This is true, however, vaults with secondary borrows may need to execute logic in order to roll positions forward. We will opt to not do any handling for dust amounts on the vault controller side and allow each vault to set its own dust thresholds.   Description  During the rollover of the strategy position into a longer maturity, several things happen:  Funds are borrowed from the longer maturity to pay off the debt and fees of the current maturity.  Strategy tokens that are associated with the current maturity are moved to the new maturity.  Any additional funds provided by the account are deposited into the strategy into a new longer maturity.  In reality, due to the AMM nature of the protocol, the funds borrowed from the new maturity could exceed the debt the account has in the current maturity, resulting in a non-zero vaultAccount.tempCashBalance. In that case, those funds will be deposited into the strategy. That would happen even if there are no external funds supplied by the account for the deposit.  It is possible that the dust in the temporary account balance will not cover the gas cost of triggering a full deposit call of the strategy.  Examples  contracts-v2/contracts/internal/vaults/VaultState.sol:L244-L246  uint256 strategyTokensMinted = vaultConfig.deposit(  vaultAccount.account, vaultAccount.tempCashBalance, vaultState.maturity, additionalUnderlyingExternal, vaultData  );  Recommendation  We suggest that additional checks are introduced that would check that on rollover vaultAccount.tempCashBalance + additionalUnderlyingExternal > 0 or larger than a certain threshold like minAccountBorrowSize for example.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"
    },
    {
        "title": "5.8 Significantly undercollateralized accounts will revert on liquidation ",
        "body": "  Resolution   Remediated per Notional s team notes in   commit by updating the calculations within  Description  The Notional Strategy Vaults utilise collateral to allow leveraged borrowing as long as the account passes the checkCollateralRatio check that ensures the overall account value is at least minCollateralRatio greater than its debts. If the account doesn t have sufficient collateral, it goes through a liquidation process where some of the collateral is sold to liquidators for the account s borrowed currency in attempt to improve the collateral ratio. However, if the account is severely undercollateralised, the entire account position is liquidated and given over to the liquidator:  contracts-v2/contracts/internal/vaults/VaultAccount.sol:L282-L289  int256 depositRatio = maxLiquidatorDepositAssetCash.mul(vaultConfig.liquidationRate).div(vaultShareValue);  // Use equal to so we catch potential off by one issues, the deposit amount calculated inside the if statement  // below will round the maxLiquidatorDepositAssetCash down  if (depositRatio >= Constants.RATE_PRECISION) {  maxLiquidatorDepositAssetCash = vaultShareValue.divInRatePrecision(vaultConfig.liquidationRate);  // Set this to true to ensure that the account gets fully liquidated  mustLiquidateFullAmount = true;  Here, the liquidator will need to deposit exactly maxLiquidatorDepositAssetCash=vaultShareValue/liquidationRate in order to get all of account s assets, i.e. all of vaultShareValue in the form of vaultAccount.vaultShares. In fact, later this deposit will be set in vaultAccount.tempCashBalance:  contracts-v2/contracts/external/actions/VaultAccountAction.sol:L361-L380  int256 maxLiquidatorDepositExternal = assetToken.convertToExternal(maxLiquidatorDepositAssetCash);  // NOTE: deposit amount external is always positive in this method  if (depositAmountExternal < maxLiquidatorDepositExternal) {  // If this flag is set, the liquidator must deposit more cash in order to liquidate the account  // down to a zero fCash balance because it will fall under the minimum borrowing limit.  require(!mustLiquidateFull, \"Must Liquidate All Debt\");  } else {  // In the other case, limit the deposited amount to the maximum  depositAmountExternal = maxLiquidatorDepositExternal;  // Transfers the amount of asset tokens into Notional and credit it to the account's temp cash balance  int256 assetAmountExternalTransferred = assetToken.transfer(  liquidator, vaultConfig.borrowCurrencyId, depositAmountExternal  );  vaultAccount.tempCashBalance = vaultAccount.tempCashBalance.add(  assetToken.convertToInternal(assetAmountExternalTransferred)  );  Then the liquidator will get:  contracts-v2/contracts/external/actions/VaultAccountAction.sol:L274-L281  uint256 vaultSharesToLiquidator;  vaultSharesToLiquidator = vaultAccount.tempCashBalance.toUint()  .mul(vaultConfig.liquidationRate.toUint())  .mul(vaultAccount.vaultShares)  .div(vaultShareValue.toUint())  .div(uint256(Constants.RATE_PRECISION));  And if (except for precision and conversions) vaultAccount.tempCashBalance=maxLiquidatorDepositAssetCash=vaultShareValue/liquidationRate, then vaultSharesToLiquidator = (vaultAccount.tempCashBalance * liquidationRate * vaultAccount.vaultShares) / (vaultShareValue) becomes vaultSharesToLiquidator = ((vaultShareValue/liquidationRate)* liquidationRate * vaultAccount.vaultShares) / (vaultShareValue) = vaultAccount.vaultShares  In other words, the liquidator needed to deposit exactly vaultShareValue/liquidationRate to get all vaultAccount.vaultShares. However, the liquidator deposit (what would be in vaultAccount.tempCashBalance) needs to cover all of that account s debt, i.e. vaultAccount.fCash. At the end of the liquidation process, the vault account has its fCash and tempCash balances updated:  contracts-v2/contracts/external/actions/VaultAccountAction.sol:L289-L290  int256 fCashToReduce = vaultConfig.assetRate.convertToUnderlying(vaultAccount.tempCashBalance);  vaultAccount.updateAccountfCash(vaultConfig, vaultState, fCashToReduce, vaultAccount.tempCashBalance.neg());  contracts-v2/contracts/internal/vaults/VaultAccount.sol:L77-L88  function updateAccountfCash(  VaultAccount memory vaultAccount,  VaultConfig memory vaultConfig,  VaultState memory vaultState,  int256 netfCash,  int256 netAssetCash  ) internal {  vaultAccount.tempCashBalance = vaultAccount.tempCashBalance.add(netAssetCash);  // Update fCash state on the account and the vault  vaultAccount.fCash = vaultAccount.fCash.add(netfCash);  require(vaultAccount.fCash <= 0);  While the vaultAccount.tempCashBalance gets cleared to 0, the vaultAccount.fCash amount only gets to vaultAccount.fCash = vaultAccount.fCash.add(netfCash), and netfCash=fCashToReduce = vaultConfig.assetRate.convertToUnderlying(vaultAccount.tempCashBalance), which, based on the constraints above essentially becomes:  vaultAccount.fCash=vaultAccount.fCash+vaultConfig.assetRate.convertToUnderlying(assetToken.convertToExternal(vaultShareValue/vaultConfig.liquidationRate))  However, later this account is set on storage, and, considering it is going through 100% liquidation, the account will necessarily be below minimum borrow size and will need to be at vaultAccount.fCash==0.  contracts-v2/contracts/internal/vaults/VaultAccount.sol:L52-L62  function setVaultAccount(VaultAccount memory vaultAccount, VaultConfig memory vaultConfig) internal {  mapping(address => mapping(address => VaultAccountStorage)) storage store = LibStorage  .getVaultAccount();  VaultAccountStorage storage s = store[vaultAccount.account][vaultConfig.vault];  // The temporary cash balance must be cleared to zero by the end of the transaction  require(vaultAccount.tempCashBalance == 0); // dev: cash balance not cleared  // An account must maintain a minimum borrow size in order to enter the vault. If the account  // wants to exit under the minimum borrow size it must fully exit so that we do not have dust  // accounts that become insolvent.  require(vaultAccount.fCash == 0 || vaultConfig.minAccountBorrowSize <= vaultAccount.fCash.neg(), \"Min Borrow\");  The case where vaultAccount.fCash>0 is taken care of by taking any extra repaid value and assigning it to the protocol, zeroing out the account s balances:  contracts-v2/contracts/external/actions/VaultAccountAction.sol:L293  if (vaultAccount.fCash > 0) vaultAccount.fCash = 0;  The case where vaultAccount.fCash < 0 is however not addressed, and instead the process will revert. This will occur whenever the vaultShareValue discounted with the liquidation rate is less than the fCash debt after all the conversions between external and underlying accounting. So, whenever the below is true, the account will not be liquidate-able. fCash>vaultShareValue/liquidationRate  This is an issue because the account is still technically solvent even though it is undercollateralized, but the current implementation would simply revert until the account is entirely insolvent (still without liquidation options) or its balances are restored enough to be liquidated fully.  Consider implementing a dynamic liquidation rate that becomes smaller the closer the account is to insolvency, thereby encouraging liquidators to promptly liquidate the accounts.  6 Strategy Vaults  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"
    },
    {
        "title": "6.1 Strategy vault swaps can be frontrun ",
        "body": "  Resolution  Acknowledged with a note from the Notional s team:  This is a large part of the diligence process for writing strategies   Description  Some strategy vaults utilize borrowing one currency, swapping it for another, and then using the new currency somewhere to generate yield. For example, the CrossCurrencyfCash strategy vault could borrow USDC, swap it for DAI, and then deposit that DAI back into Notional if the DAI lending interest rates are greater than USDC borrowing interest rates. However, during vault settlement the assets would need to be swapped back into the original borrow currency.  Since these vaults control the borrowed assets that go only into white-listed strategies, the Notional system allows users to borrow multiples of their posted collateral and claim the yield from a much larger position. As a result, these strategy vaults would likely have significant funds being borrowed and managed into these strategies.  However, as mentioned above, these strategies usually utilize a trading mechanism to swap borrowed currencies into whatever is required by the strategy, and these trades may be quite large. In fact, the BaseStrategyVault implementation contains functions that interact with Notional s trading module to assist with those swaps:  strategy-vaults/contracts/vaults/BaseStrategyVault.sol:L100-L127  /// @notice Can be used to delegate call to the TradingModule's implementation in order to execute  /// a trade.  function _executeTrade(  uint16 dexId,  Trade memory trade  ) internal returns (uint256 amountSold, uint256 amountBought) {  (bool success, bytes memory result) = nProxy(payable(address(TRADING_MODULE))).getImplementation()  .delegatecall(abi.encodeWithSelector(ITradingModule.executeTrade.selector, dexId, trade));  require(success);  (amountSold, amountBought) = abi.decode(result, (uint256, uint256));  /// @notice Can be used to delegate call to the TradingModule's implementation in order to execute  /// a trade.  function _executeTradeWithDynamicSlippage(  uint16 dexId,  Trade memory trade,  uint32 dynamicSlippageLimit  ) internal returns (uint256 amountSold, uint256 amountBought) {  (bool success, bytes memory result) = nProxy(payable(address(TRADING_MODULE))).getImplementation()  .delegatecall(abi.encodeWithSelector(  ITradingModule.executeTradeWithDynamicSlippage.selector,  dexId, trade, dynamicSlippageLimit  );  require(success);  (amountSold, amountBought) = abi.decode(result, (uint256, uint256));  Although some strategies may manage stablecoin <-> stablecoin swaps that typically would incur low slippage, large size trades could still suffer from low on-chain liquidity and end up getting frontrun and  sandwiched  by MEV bots or other actors, thereby extracting maximum amount from the strategy vault swaps as slippage permits. This could be especially significant during vaults  settlements, that can be initiated by anyone, as lending currencies may be swapped in large batches and not do it on a per-account basis. For example with the CrossCurrencyfCash vault, it can only enter settlement if all strategy tokens (lending currency in this case) are gone and swapped back into the borrow currency:  strategy-vaults/contracts/vaults/CrossCurrencyfCashVault.sol:L141-L143  if (vaultState.totalStrategyTokens == 0) {  NOTIONAL.settleVault(address(this), maturity);  As a result, in addition to the risk of stablecoins  getting off-peg, unfavorable market liquidity conditions and arbitrage-seeking actors could eat into the profits generated by this strategy as per the maximum allowed slippage. However, during settlement the strategy vaults don t have the luxury of waiting for the right conditions to perform the trade as the borrows need to repaid at their maturities.  So, the profitability of the vaults, and therefore users, could suffer due to potential low market liquidity allowing high slippage and risks of being frontrun with the chosen strategy vaults  currencies.  Recommendation  Ensure that the currencies chosen to generate yield in the strategy vaults have sufficient market liquidity on exchanges allowing for low slippage swaps.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"
    },
    {
        "title": "6.2 Cross currency strategy should not have same lend and borrow currencies ",
        "body": "  Description  Cross currency strategy currently takes lend and borrow currencies as the initialization arguments. Due to the way strategy and TradingModule are implemented, the strategy will not operate correctly if lend and borrow currencies are the same. Despite those arguments being passed exclusively by the Notional team, there is still a possibility of incorrect arguments being used.  Examples  strategy-vaults/contracts/vaults/CrossCurrencyfCashVault.sol:L77-L82  function initialize(  string memory name_,  uint16 borrowCurrencyId_,  uint16 lendCurrencyId_,  uint64 settlementSlippageLimit_  ) external initializer {  Recommendation  We suggest adding a require check in the initialization function of the CrossCurrencyfCashVault.sol that will ensure that lend and borrow currencies are different.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"
    },
    {
        "title": "3.1 TribalChief - A wrong user.rewardDebt value is calculated during the withdrawFromDeposit function call ",
        "body": "  Description  When withdrawing a single deposit, the reward debt is updated:  contracts/staking/TribalChief.sol:L468-L474  uint128 virtualAmountDelta = uint128( ( amount * poolDeposit.multiplier ) / SCALE_FACTOR );  // Effects  poolDeposit.amount -= amount;  user.rewardDebt = user.rewardDebt - toSigned128(user.virtualAmount * pool.accTribePerShare) / toSigned128(ACC_TRIBE_PRECISION);  user.virtualAmount -= virtualAmountDelta;  pool.virtualTotalSupply -= virtualAmountDelta;  Instead of the user.virtualAmount in reward debt calculation, the virtualAmountDelta  should be used. Because of that bug, the reward debt is much lower than it would be, which means that the reward itself will be much larger during the harvest. By making multiple deposit-withdraw actions, any user can steal all the Tribe tokens from the contract.  Recommendation  Use the virtualAmountDelta instead of the user.virtualAmount.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"
    },
    {
        "title": "3.2 TribalChief - Setting the totalAllocPoint to zero shouldn t be allowed ",
        "body": "  Description  TribalChief.updatePool will revert in the case totalAllocPoint = 0, which will essentially cause users  funds and rewards to be locked.  Recommendation  TribalChief.add and TribalChief.set should assert that totalAllocPoint > 0. A similar validation check should be added to TribalChief.updatePool as well.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"
    },
    {
        "title": "3.3 TribalChief - Unlocking users  funds in a pool where a multiplier has been increased is missing ",
        "body": "  Description  When a user deposits funds to a pool, the current multiplier in use for this pool is being stored locally for this deposit. The value that is used later in a withdrawal operation is the local one, and not the one that is changing when a governor calls governorAddPoolMultiplier. It means that a decrease in the multiplier value for a given pool does not affect users that already deposited, but an increase does. Users that had already deposited should have the right to withdraw their funds when the multiplier for their pool increases by the governor.  Examples  code/contracts/staking/TribalChief.sol:L143-L158  function governorAddPoolMultiplier(  uint256 _pid,  uint64 lockLength,  uint64 newRewardsMultiplier  ) external onlyGovernor {  PoolInfo storage pool = poolInfo[_pid];  uint256 currentMultiplier = rewardMultipliers[_pid][lockLength];  // if the new multplier is less than the current multiplier,  // then, you need to unlock the pool to allow users to withdraw  if (newRewardsMultiplier < currentMultiplier) {  pool.unlocked = true;  rewardMultipliers[_pid][lockLength] = newRewardsMultiplier;  emit LogPoolMultiplier(_pid, lockLength, newRewardsMultiplier);  Recommendation  Replace the < operator with > in TribalChief line 152.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"
    },
    {
        "title": "3.4 TribalChief - Unsafe down-castings ",
        "body": "  Description  TribalChief consists of multiple unsafe down-casting operations. While the usage of types that can be packed into a single storage slot is more gas efficient, it may introduce hidden risks in some cases that can lead to loss of funds.  Examples  Various instances in TribalChief, including (but not necessarily only) :  code/contracts/staking/TribalChief.sol:L429  user.rewardDebt = int128(user.virtualAmount * pool.accTribePerShare) / toSigned128(ACC_TRIBE_PRECISION);  code/contracts/staking/TribalChief.sol:L326  pool.accTribePerShare = uint128(pool.accTribePerShare + ((tribeReward * ACC_TRIBE_PRECISION) / virtualSupply));  code/contracts/staking/TribalChief.sol:L358  userPoolData.rewardDebt += int128(virtualAmountDelta * pool.accTribePerShare) / toSigned128(ACC_TRIBE_PRECISION);  Recommendation  Given the time constraints of this audit engagement, we could not verify the implications and provide mitigation actions for each of the unsafe down-castings operations. However, we do recommend to either use numeric types that use 256 bits, or to add proper validation checks and handle these scenarios to avoid silent over/under-flow errors. Keep in mind that reverting these scenarios can sometimes lead to a denial of service, which might be harmful in some cases.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"
    },
    {
        "title": "3.5 EthCompoundPCVDeposit - should provide means to recover ETH ",
        "body": "  Description  The CToken to be used is configured on EthCompoundPCVDeposit deployment. It is not checked, whether the provided CToken address is actually a valid CToken.  If the configured CToken ceases to work correctly (e.g. CToken.mint|redeem* disabled or the configured CToken address is invalid), ETH held by the contract may be locked up.  Recommendation  In CompoundPCVDepositBase consider verifying, that the CToken constructor argument is actually a valid CToken by checking require(ctoken.isCToken(), \"not a valid CToken\").  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"
    },
    {
        "title": "3.6 TribalChief - Governor decrease of pool s allocation point should unlock depositors  funds ",
        "body": "  Description  When the TribalChief governor decreases the ratio between the allocation point (PoolInfo.allocPoint) and the total allocation point (totalAllocPoint) for a specific pool (either be directly decreasing PoolInfo.allocPoint of a given pool, or by increasing this value for other pools), the total reward for this pool is decreased as well. Depositors should be able to withdraw their funds immediately after this kind of change.  Examples  code/contracts/staking/TribalChief.sol:L252-L261  function set(uint256 _pid, uint128 _allocPoint, IRewarder _rewarder, bool overwrite) public onlyGovernor {  totalAllocPoint = (totalAllocPoint - poolInfo[_pid].allocPoint) + _allocPoint;  poolInfo[_pid].allocPoint = _allocPoint.toUint64();  if (overwrite) {  rewarder[_pid] = _rewarder;  emit LogSetPool(_pid, _allocPoint, overwrite ? _rewarder : rewarder[_pid], overwrite);  Recommendation  Make sure that depositors  funds are unlocked for pools that affected negatively by calling TribalChief.set.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"
    },
    {
        "title": "3.7 TribalChief - new block reward retrospectively takes effect on pools that have not been updated recently ",
        "body": "  Description  When the governor updates the block reward tribalChiefTribePerBlock the new reward is applied for the outstanding duration of blocks in updatePool. This means, if a pool hasn t updated in a while (unlikely) the new block reward is retrospectively applied to the pending duration instead of starting from when the block reward changed.  Examples  rewards calculation  code/contracts/staking/TribalChief.sol:L323-L327  if (virtualSupply > 0) {  uint256 blocks = block.number - pool.lastRewardBlock;  uint256 tribeReward = (blocks * tribePerBlock() * pool.allocPoint) / totalAllocPoint;  pool.accTribePerShare = uint128(pool.accTribePerShare + ((tribeReward * ACC_TRIBE_PRECISION) / virtualSupply));  updating the block reward  code/contracts/staking/TribalChief.sol:L111-L116  /// @notice Allows governor to change the amount of tribe per block  /// @param newBlockReward The new amount of tribe per block to distribute  function updateBlockReward(uint256 newBlockReward) external onlyGovernor {  tribalChiefTribePerBlock = newBlockReward;  emit NewTribePerBlock(newBlockReward);  Recommendation  It is recommended to update pools before changing the block reward. Document and make users aware that the new reward is applied to the outstanding duration when calling updatePool.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"
    },
    {
        "title": "3.8 TribalChief - duplicate import SafeERC20 ",
        "body": "  Description  Duplicate import for SafeERC20.  Examples  code/contracts/staking/TribalChief.sol:L7-L8  import \"@openzeppelin/contracts/token/ERC20/utils/SafeERC20.sol\";  import \"@openzeppelin/contracts/token/ERC20/utils/SafeERC20.sol\";  Recommendation  Remove duplicate import line.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"
    },
    {
        "title": "3.9 TribalChief - resetRewards should emit an event ",
        "body": "  Description  The method resetRewards silently resets a pools tribe allocation.  Examples  code/contracts/staking/TribalChief.sol:L263-L275  /// @notice Reset the given pool's TRIBE allocation to 0 and unlock the pool. Can only be called by the governor or guardian.  /// @param _pid The index of the pool. See `poolInfo`.  function resetRewards(uint256 _pid) public onlyGuardianOrGovernor {  // set the pool's allocation points to zero  totalAllocPoint = (totalAllocPoint - poolInfo[_pid].allocPoint);  poolInfo[_pid].allocPoint = 0;  // unlock all staked tokens in the pool  poolInfo[_pid].unlocked = true;  // erase any IRewarder mapping  rewarder[_pid] = IRewarder(address(0));  Recommendation  For transparency and to create an easily accessible audit trail of events consider emitting an event when resetting a pools allocation.  4 Recommendations  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"
    },
    {
        "title": "4.1 EthCompoundPCVDeposit - stick to upstream interface contract names",
        "body": "  Recommendation  Stick to the original upstream interface names to make clear with which external system the contract interacts with. Rename CEth to CEther. See original upstream interface name.  code/contracts/pcv/compound/EthCompoundPCVDeposit.sol:L6-L8  interface CEth {  function mint() external payable;  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"
    },
    {
        "title": "4.2 CompoundPCVDepositBase - verify provided CToken address is actually a CToken",
        "body": "  Recommendation  The ctoken address provided when deploying a new *CompoundPCVDeposit is never validated. Consider adding the following check: require(_cToken.isCToken, \"not a valid CToken\").  code/contracts/pcv/compound/CompoundPCVDepositBase.sol:L25-L30  constructor(  address _core,  address _cToken  ) CoreRef(_core) {  cToken = CToken(_cToken);  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"
    },
    {
        "title": "4.3 CompoundPCV - documentation & testing",
        "body": "  Recommendation  Currently, the PCV flavor is only unit-tested using a mocked CToken. Consider providing integration tests that actually integrate and operate it in a compound test environment.  Provide a specification. & documentation describing the roles and functionality of the contract. Who deployes the PCVDeposit contract? Who Deploys the CToken and therefore may be in control of certain adminOnly functions of the CToken? What are the requirements for a CToken to be usable with CompoundPCVDeposit (listed/unlisted, \u2026)? Who has the potential power to borrow assets on behalf of the collateral provided?  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"
    },
    {
        "title": "4.4 TribalChief - immutable vs constant",
        "body": "  Recommendation  Constant state variables that are not initialized with the constructor can be constant instead of immutable.  code/contracts/staking/TribalChief.sol:L88-L90  uint256 private immutable ACC_TRIBE_PRECISION = 1e12;  /// exponent for rewards multiplier  uint256 public immutable SCALE_FACTOR = 1e18;  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"
    },
    {
        "title": "4.5 TribalChief - governorAddPoolMultiplier should emit a PoolLocked event",
        "body": "  Description  Users should be notified if the pool gets unlocked during a call to governorAddPoolMultiplier. Consider emitting a PoolLocked(false) event.  code/contracts/staking/TribalChief.sol:L143-L158  function governorAddPoolMultiplier(  uint256 _pid,  uint64 lockLength,  uint64 newRewardsMultiplier  ) external onlyGovernor {  PoolInfo storage pool = poolInfo[_pid];  uint256 currentMultiplier = rewardMultipliers[_pid][lockLength];  // if the new multplier is less than the current multiplier,  // then, you need to unlock the pool to allow users to withdraw  if (newRewardsMultiplier < currentMultiplier) {  pool.unlocked = true;  rewardMultipliers[_pid][lockLength] = newRewardsMultiplier;  emit LogPoolMultiplier(_pid, lockLength, newRewardsMultiplier);  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"
    },
    {
        "title": "4.6 TribalChief - updatePool invocation inside _harvest should be moved to harvest instead",
        "body": "  Description  When TribalChief.withdrawAllAndHarvest is executed, there s a redundant invocation of TribalChief.updatePool that caused by TribalChief._harvest, that can be moved to TribalChief.harvest instead.  Examples  code/contracts/staking/TribalChief.sol:L485-L515  function _harvest(uint256 pid, address to) private {  updatePool(pid);  PoolInfo storage pool = poolInfo[pid];  UserInfo storage user = userInfo[pid][msg.sender];  // assumption here is that we will never go over 2^128 -1  int256 accumulatedTribe = int256( uint256(user.virtualAmount) * uint256(pool.accTribePerShare) ) / int256(ACC_TRIBE_PRECISION);  // this should never happen  require(accumulatedTribe >= 0 || (accumulatedTribe - user.rewardDebt) < 0, \"negative accumulated tribe\");  uint256 pendingTribe = uint256(accumulatedTribe - user.rewardDebt);  // if pending tribe is ever negative, revert as this can cause an underflow when we turn this number to a uint  require(pendingTribe.toInt256() >= 0, \"pendingTribe is less than 0\");  // Effects  user.rewardDebt = int128(accumulatedTribe);  // Interactions  if (pendingTribe != 0) {  TRIBE.safeTransfer(to, pendingTribe);  IRewarder _rewarder = rewarder[pid];  if (address(_rewarder) != address(0)) {  _rewarder.onSushiReward( pid, msg.sender, to, pendingTribe, user.virtualAmount);  emit Harvest(msg.sender, pid, pendingTribe);  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"
    },
    {
        "title": "3.1 GenesisGroup.commit overwrites previously-committed values ",
        "body": "  Resolution   This was addressed in   fei-protocol/fei-protocol-core#16.  Description  commit allows anyone to commit purchased FGEN to a swap that will occur once the genesis group is launched. This commitment may be performed on behalf of other users, as long as the calling account has sufficient allowance:  code/contracts/genesis/GenesisGroup.sol:L87-L94  function commit(address from, address to, uint amount) external override onlyGenesisPeriod {  burnFrom(from, amount);  committedFGEN[to] = amount;  totalCommittedFGEN += amount;  emit Commit(from, to, amount);  The amount stored in the recipient s committedFGEN balance overwrites any previously-committed value. Additionally, this also allows anyone to commit an amount of  0  to any account, deleting their commitment entirely.  Recommendation  Ensure the committed amount is added to the existing commitment.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "3.2 Purchasing and committing still possible after launch ",
        "body": "  Resolution   This was addressed in   fei-protocol/fei-protocol-core#11.  Description  Even after GenesisGroup.launch has successfully been executed, it is still possible to invoke GenesisGroup.purchase and GenesisGroup.commit.  Recommendation  Consider adding validation in GenesisGroup.purchase and GenesisGroup.commit to make sure that these functions cannot be called after the launch.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "3.3 UniswapIncentive overflow on pre-transfer hooks ",
        "body": "  Resolution   This was addressed in   fei-protocol/fei-protocol-core#15.  Description  Before a token transfer is performed, Fei performs some combination of mint/burn operations via UniswapIncentive.incentivize:  code/contracts/token/UniswapIncentive.sol:L49-L65  function incentivize(  address sender,  address receiver,  address operator,  uint amountIn  ) external override onlyFei {  updateOracle();  if (isPair(sender)) {  incentivizeBuy(receiver, amountIn);  if (isPair(receiver)) {  require(isSellAllowlisted(sender) || isSellAllowlisted(operator), \"UniswapIncentive: Blocked Fei sender or operator\");  incentivizeSell(sender, amountIn);  Both incentivizeBuy and incentivizeSell calculate buy/sell incentives using overflow-prone math, then mint / burn from the target according to the results. This may have unintended consequences, like allowing a caller to mint tokens before transferring them, or burn tokens from their recipient.  Examples  incentivizeBuy calls getBuyIncentive to calculate the final minted value:  code/contracts/token/UniswapIncentive.sol:L173-L186  function incentivizeBuy(address target, uint amountIn) internal ifMinterSelf {  if (isExemptAddress(target)) {  return;  (uint incentive, uint32 weight,  Decimal.D256 memory initialDeviation,  Decimal.D256 memory finalDeviation) = getBuyIncentive(amountIn);  updateTimeWeight(initialDeviation, finalDeviation, weight);  if (incentive != 0) {  fei().mint(target, incentive);  getBuyIncentive calculates price deviations after casting amount to an int256, which may overflow:  code/contracts/token/UniswapIncentive.sol:L128-L134  function getBuyIncentive(uint amount) public view override returns(  uint incentive,  uint32 weight,  Decimal.D256 memory initialDeviation,  Decimal.D256 memory finalDeviation  ) {  (initialDeviation, finalDeviation) = getPriceDeviations(-1 * int256(amount));  Recommendation  Ensure casts in getBuyIncentive and getSellPenalty do not overflow.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "3.4 BondingCurve allows users to acquire FEI before launch ",
        "body": "  Resolution   This was addressed in   fei-protocol/fei-protocol-core#59  Description  BondingCurve.allocate allocates the protocol s held PCV, then calls _incentivize, which rewards the caller with FEI if a certain amount of time has passed:  code-update/contracts/bondingcurve/BondingCurve.sol:L180-L186  /// @notice if window has passed, reward caller and reset window  function _incentivize() internal virtual {  if (isTimeEnded()) {  _initTimed(); // reset window  fei().mint(msg.sender, incentiveAmount);  allocate can be called before genesis launch, as long as the contract holds some nonzero PCV. By force-sending the contract 1 wei, anyone can bypass the majority of checks and actions in allocate, and mint themselves FEI each time the timer expires.  Recommendation  Prevent allocate from being called before genesis launch.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "3.5 Timed.isTimeEnded returns true if the timer has not been initialized ",
        "body": "  Resolution   This was addressed in   fei-protocol/fei-protocol-core#62  Description  Timed initialization is a 2-step process:  Timed.duration is set in the constructor: https://github.com/ConsenSys/fei-protocol-audit-2021-01/blob/d31114d834e62b4f3d4fa7b1c0b0c70fbff623a4/code-update/contracts/utils/Timed.sol#L15-L20  Timed.startTime is set when the method _initTimed is called: https://github.com/ConsenSys/fei-protocol-audit-2021-01/blob/d31114d834e62b4f3d4fa7b1c0b0c70fbff623a4/code-update/contracts/utils/Timed.sol#L43-L46  Before this second method is called, isTimeEnded() calculates remaining time using a startTime of 0, resulting in the method returning true for most values, even though the timer has not technically been started.  Recommendation  If Timed has not been initialized, isTimeEnded() should return false, or revert  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "3.6 Overflow/underflow protection ",
        "body": "  Resolution   This was partially addressed in   fei-protocol/fei-protocol-core#17 by using  Description  Having overflow/underflow vulnerabilities is very common for smart contracts. It is usually mitigated by using SafeMath or using solidity version ^0.8 (after solidity 0.8 arithmetical operations already have default overflow/underflow protection).  In this code, many arithmetical operations are used without the  safe  version. The reasoning behind it is that all the values are derived from the actual ETH values, so they can t overflow.  On the other hand, some operations can t be checked for overflow/underflow without going much deeper into the codebase that is out of scope:  code/contracts/genesis/GenesisGroup.sol:L131  uint totalGenesisTribe = tribeBalance() - totalCommittedTribe;  Recommendation  In our opinion, it is still safer to have these operations in a safe mode. So we recommend using SafeMath or solidity version ^0.8 compiler.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "3.7 Unchecked return value for IWETH.transfer call ",
        "body": "  Resolution   This was addressed in   fei-protocol/fei-protocol-core#12.  Description  In EthUniswapPCVController, there is a call to IWETH.transfer that does not check the return value:  code/contracts/pcv/EthUniswapPCVController.sol:L122  weth.transfer(address(pair), amount);  It is usually good to add a require-statement that checks the return value or to use something like safeTransfer; unless one is sure the given token reverts in case of a failure.  Recommendation  Consider adding a require-statement or using safeTransfer.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "3.8 GenesisGroup.emergencyExit remains functional after launch ",
        "body": "  Resolution   This was partially addressed in   fei-protocol/fei-protocol-core#14 and  fei-protocol/fei-protocol-core#13 by addressing the last two recommendations.  Description  emergencyExit is intended as an escape mechanism for users in the event the genesis launch method fails or is frozen. emergencyExit becomes callable 3 days after launch is callable. These two methods are intended to be mutually-exclusive, but are not: either method remains callable after a successful call to the other.  This may result in accounting edge cases. In particular, emergencyExit fails to decrease totalCommittedFGEN by the exiting user s commitment:  code/contracts/genesis/GenesisGroup.sol:L185-L188  burnFrom(from, amountFGEN);  committedFGEN[from] = 0;  payable(to).transfer(total);  As a result, calling launch after a user performs an exit will incorrectly calculate the amount of FEI to swap:  code/contracts/genesis/GenesisGroup.sol:L165-L168  uint amountFei = feiBalance() * totalCommittedFGEN / (totalSupply() + totalCommittedFGEN);  if (amountFei != 0) {  totalCommittedTribe = ido.swapFei(amountFei);  Recommendation  Ensure launch cannot be called if emergencyExit has been called  Ensure emergencyExit cannot be called if launch has been called  In emergencyExit, reduce totalCommittedFGEN by the exiting user s committed amount  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "3.9 Unchecked return value for transferFrom calls ",
        "body": "  Resolution   This was addressed in   fei-protocol/fei-protocol-core#12.  Description  There are two transferFrom calls that do not check the return value (some tokens signal failure by returning false):  code/contracts/pool/Pool.sol:L121  stakedToken.transferFrom(from, address(this), amount);  code/contracts/genesis/IDO.sol:L58  fei().transferFrom(msg.sender, address(pair), amountFei);  It is usually good to add a require-statement that checks the return value or to use something like safeTransferFrom; unless one is sure the given token reverts in case of a failure.  Recommendation  Consider adding a require-statement or using safeTransferFrom.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "3.10 GovernorAlpha proposals may be canceled by the proposer, even after they have been accepted and queued ",
        "body": "  Resolution   This was addressed in   fei-protocol/fei-protocol-core#61  Description  GovernorAlpha allows proposals to be canceled via cancel. To cancel a proposal, two conditions must be met by the proposer:  The proposal should not already have been executed: https://github.com/ConsenSys/fei-protocol-audit-2021-01/blob/d31114d834e62b4f3d4fa7b1c0b0c70fbff623a4/code-update/contracts/dao/GovernorAlpha.sol#L206-L208  The proposer must have under proposalThreshold() TRIBE balance: https://github.com/ConsenSys/fei-protocol-audit-2021-01/blob/d31114d834e62b4f3d4fa7b1c0b0c70fbff623a4/code-update/contracts/dao/GovernorAlpha.sol#L210-L211  Recommendation  Prevent proposals from being canceled unless they are in the Pending or Active states.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "3.11 Pool: claiming to the pool itself causes accounting issues ",
        "body": "  Resolution   This was addressed in   fei-protocol/fei-protocol-core#57  Description  In Pool.sol, claim(address from, address to) is used to claim staking rewards and send them to a destination address to:  code-update/contracts/pool/Pool.sol:L229-L238  function _claim(address from, address to) internal returns (uint256) {  (uint256 amountReward, uint256 amountPool) = redeemableReward(from);  require(amountPool != 0, \"Pool: User has no redeemable pool tokens\");  _burnFrom(from, amountPool);  _incrementClaimed(amountReward);  rewardToken.transfer(to, amountReward);  return amountReward;  If the destination address to is the pool itself, the pool will burn tokens and increment the amount of tokens claimed, then transfer the reward tokens to itself.  Recommendation  Prevent claims from specifying the pool as a destination.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "3.12 Assertions that can fail ",
        "body": "  Description  In UniswapSingleEthRouter there are two assert-statements that may fail:  code/contracts/router/UniswapSingleEthRouter.sol:L21  assert(msg.sender == address(WETH)); // only accept ETH via fallback from the WETH contract  code/contracts/router/UniswapSingleEthRouter.sol:L48  assert(IWETH(WETH).transfer(address(PAIR), amountIn));  Since they do some sort of input validation it might be good to replace them with require-statements. I would only use asserts for checks that should never fail and failure would constitute a bug in the code.  Recommendation  Consider replacing the assert-statements with require-statements. An additional benefit is that this will not result in consuming all the gas in case of a violation.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "3.13 Simplify API of GenesisGroup.purchase ",
        "body": "  Description  The API of GenesisGroup.purchase could be simplified by not including the value parameter that is required to be equivalent to msg.value:  code/contracts/genesis/GenesisGroup.sol:L79  require(msg.value == value, \"GenesisGroup: value mismatch\");  Using msg.value might make the API more explicit and avoid requiring msg.value == value. It can also save some gas due to fewer inputs and fewer checks.  Recommendation  Consider dropping the value parameter and changing the code to use msg.value instead.  4 Infrastructure Security Assessment  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "4.1 Clickjacking and Missing Content Security Policy    ",
        "body": "  Resolution  After multiple iterations, the following Content Security Policy has been put into effect:  The CSP is transmitted through the following headers:  Content-Security-Policy  X-Content-Security-Policy  X-WebKit-CSP  as well as through corresponding meta HTML tags. Additionally, the following frame-busting JavaScript code has been added to prevent Clickjacking attacks in the unlikely event that existing CSP measures fail or are bypassed:  Description  A content security policy (CSP) provides an added layer of protection against cross-site scripting (XSS), clickjacking, and other client-side attacks that rely on executing malicious content in the context of the website.  Specifically, the lack of a content security policy allows an adversary to perform a clickjacking attack by including the target URL (such as app.fei.money) in an iframe element on their site. The attacker then uses one or more transparent layers on top of the embedded site to trick a user into performing a click action on a different element.  This technique can be used to spawn malicious Metamask dialogues, tricking users into thinking that they are signing a legitimate transaction.  Affected Assets  All S3-hosted web sites.  Recommendation  It is recommended to add content security policy headers to the served responses to prevent browsers from embedding Fei-owned sites into malicious parent sites. Furthermore, CSP can be used to limit the permissions of JavaScript and CSS on the page, which can be used to further harden the deployment against a potential compromise of script dependencies.  It should be noted that security headers should not only be served from Cloudfront but any public-facing endpoint. Otherwise, it will be trivial for an attacker to circumvent the security headers added by Cloudfront, e.g. by embedding the index.html file directly from the public-facing S3 bucket URL.  Besides CSP headers, clickjacking can also be mitigated by directly including frame-busting JavaScript code into the served page.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "4.2 S3 Buckets Cleartext Communication    ",
        "body": "  Resolution   Direct access to S3 buckets through   Description  The system s S3 buckets are configured to allow unencrypted traffic:  Affected Assets  arn:aws:s3:::ropsten-app.fei.money/*  arn:aws:s3:::www.fei.money/*  arn:aws:s3:::feiprotocol.com/*  arn:aws:s3:::www.app.fei.money/*  arn:aws:s3:::www.ropsten-app.fei.money/*  arn:aws:s3:::app.fei.money/*  arn:aws:s3:::fei.money/*  Recommendation  It is recommended to enforce encryption of data in transit using TLS certificates. To accomplish this, the aws:SecureTransport can be set in the S3 bucket s policies.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "4.3 Missing Log Aggregation    ",
        "body": "  Resolution  CloudFrond and CloudTrail have been enabled. These components send endpoint-related and organizational log messages into S3 buckets where they can be queried using AWS Athena. The security review process section of this report contains sample queries for Athena.  Description  There is no centralized system that gathers operational events of AWS stack components. This includes S3 server access logs, configuration changes, as well as Cloudfront-related logging.  Recommendation  It is recommended to enable CloudTrail for internal log aggregation as it integrates seamlessly with S3, Cloudfront, and IAM. Furthermore, regular reviews should be set up where system activity is checked to detect suspicious activity as soon as possible.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "4.4 Enforce Strict Transport Security    ",
        "body": "  Resolution  All domains in scope now ship with the following header:  Description  The HTTP Strict-Transport-Security response header (often abbreviated as HSTS) lets a web site tell browsers that it should only be accessed using HTTPS, instead of using HTTP. This prevents attackers from stripping TLS certificates from connections and removing encryption.  Recommendation  It is recommended to deliver all responses with the Strict-Transport-Security header. In an S3-Cloudfront setup, this can be achieved using Lambda@Edge lambda functions.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "4.5 Server Information Leak ",
        "body": "  Description  Responses from the fei.money domain and related assets leak server information in their response headers. This information can be used by an adversary to prepare more sophisticated attacks tailored to the deployed infrastructure.  Note: At the time of reporting, this issue was deemed not possible to fix due to technical limitations on AWS-hosted static sites using S3 and CloudFront.  Examples  Recommendation  It is recommended to remove any headers that hint at server technologies and are not directly required by the frontend.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "4.6 Missing Route53 Domain Lock    ",
        "body": "  Resolution   A transfer lock on both the   Description  Domain registrars often give customers the option to lock a domain. This prevents unauthorized parties from transferring it to another registrar, either through malicious interaction with the registrar itself, or compromised domain owner credentials. No domain currently has a lock enabled.  Affected Assets  fei.money  feiprotocol.com  Recommendation  It is recommended to set a lock for the affected domains, assuming that the registrar allows domain locks:  Sign in to the AWS Management Console and open the Route 53 console at https://console.aws.amazon.com/route53/.  In the navigation pane, choose Registered Domains.  Choose the name of the domain that you want to update.  Choose Enable (to lock the domain) or Disable (to unlock the domain).  Choose Save.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "4.7 Weak IAM Password Policy    ",
        "body": "  Resolution  This has been fixed by the client with the following notes:  Enforced 14 character password length  Enabled 90 day password expiration  Prevent password reuse  Require one uppercase, one lowercase, one number, one non-alphanumeric character  Require 2FA on all users via this doc and this post (Create new Force_MFA policy, attach it to the new Engineers group, and then assign all users (including Dominik) to this group  Also requiring 2FA on command line access. Using src/infra/aws-token.sh for generating the credentials and putting them in ~/.aws/config  Description  The password policy for IAM users currently does not enforce the use of strong passwords, multi-factor authentication, and regular password rotation.  Currently, only a minimum password length of 8 is enforced.  Recommendation  Require a minimum password length of 14  Set a password expiration policy of at most 90 days  Disallow the reuse of passwords  Enable mandatory multi-factor authentication with a virtual app  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "4.8 Review Access Key Expiration    ",
        "body": "  Resolution  This issue is considered resolved with the implementation of regular security review meetings.  Description  It is recommended to only create access keys when absolutely necessary. There should be no access keys given out to root users. Instead, temporary security credentials (IAM Roles) should be created.  Recommendation  It is recommended to read the Best practices for managing AWS access keys and incorporate the security practices where reasonable.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "4.9 Dependency Security    ",
        "body": "  Resolution  This issue has been resolved by implementing Snyk for continuous dependency security scanning. This allows the developers to review potential risks of included packages and receiving automated pull requests with fixes if necessary.  Furthermore, a manual review of select dependencies has been conducted by the penetration tester without significant, actionable results. The following dependencies have been checked:  bignumber  numeral  validator  web-vitals  Description  The Yarn audit feature currently finds two low-severity dependency issues:  Prototype pollution in ini - a dependency of react-scripts  Insecure Credential Storage in web3  Recommendations  It is recommended to apply the ini patch, which is already available. For web3, it is recommended to monitor the repository s Github issue https://github.com/ConsenSys/fei-protocol-audit-2021-01/issues/2739 and upgrade as soon as a fix is available.  For additional dependency security, it is recommended to integrate a security monitoring service. Snyk has a free plan which allows unlimited tests on public repositories, and 200 tests per month for private ones. A bot will automatically add a pull request to bump vulnerable dependency versions.  It should be noted that the quality and reliability of such automated contributions are highly dependent on the quality of the test suite. It is recommended to build strict tests around core functionality and expected dependency behaviour to detect breaking changes as soon as possible.  5 Security Review Process  In an additional effort to achieve security-in-depth, it is recommended to implement a schedule or recurring security review meetings. The goal of these meetings is to complete a checklist to enforce security best-practices, as well as find anomalies in the system as soon as possible to commence mitigation and investigations.  This section outlines recommendations for the contents of such a checklist. It should be noted that security requirements are likely to change, and thus, this list should be treated as a working document as the project s infrastructure and attack surface change.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "5.1 CloudTrail Anomalies",
        "body": "  Event filter query template:  SELECT useridentity.username,  sourceipaddress,  eventtime,  additionaleventdata  FROM cloudtrail_logs  WHERE {{ event filter }}  AND eventtime >= '<yyyy-mm-dd>'  AND eventtime < '<yyyy-mm-dd>';  eventname = 'ConsoleLogin'  Sign-in activity  eventname = 'AddUserToGroup'  User added to group  eventname = 'ChangePassword'  User password change  eventname LIKE '%AccessKey%'  Key management events  eventname LIKE '%MFADevice'  MFA deactivation/deletion/resync  eventname = 'StopLogging'  Logging stopped  eventname LIKE '%BucketPolicy%'  Bucket policy activity  eventname LIKE '%GroupPolicy%'  Group policy activity  eventname LIKE '%UserPolicy%'  User policy activity  eventname LIKE '%RolePolicy%'  Role policy activity  Aggregate statistics about failed authentication and user authorization attempts can be gathered with the following query:  SELECT count (*) AS totalEvents,  useridentity.arn,  eventsource,  eventname,  errorCode,  errorMessage  FROM cloudtrail_logs  WHERE (errorcode LIKE '%Denied%'  OR errorcode LIKE '%Unauthorized%')  AND eventtime >= '2021-02-17'  AND eventtime < '2021-02-17'  GROUP BY  eventsource, eventname, errorCode, errorMessage, useridentity.arn  ORDER BY  eventsource, eventname  For investigative purposes or the goal of covering new infrastructure components, it might be necessary to add more event names to the review process. AWS does not provide a comprehensive list of event names per stack component. An external list of CloudTrail event names is available on the GorillaStack blog.  Note: In case of issues with Athena or ingestion into the database, CloudTrail allows users to view the unfiltered event history for a user-specified time range as well. Particularly notable is the ability to filter by resource types, of which the following are relevant to the Fei AWS infrastructure:  AWS::S3::Bucket  AWS::CloudTrail::Trail  AWS::IAM::AccessKey  AWS::IAM::MfaDevice  AWS::IAM::Group  AWS::IAM::Policy  AWS::IAM::Role  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "5.2 Cloudfront Endpoint Anomalies",
        "body": "  Top 10 endpoints hit in a given time frame:  SELECT uri,  status,  count(*) AS ct  FROM cloudfront_logs_fei_landing  WHERE date >= DATE('2021-02-01')  AND date <= DATE('2021-02-28')  GROUP BY  uri, status  ORDER BY  ct DESC limit 10  This query can be filtered further by adding AND status = 500 or a similar condition to find suspicious response codes.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "5.3 Route53 CNAME Review",
        "body": "  Subdomain takeover vulnerabilities occur when a subdomain is pointing to a service, e.g. a previously deleted CloudFront endpoint or S3 bucket. This allows an attacker to set up a page on the service that was being used and point their page to that subdomain. Especially with wildcard certificates on the system, e.g. *.fei.money, this can lead to an exploitation of user trust and enables attacks that can result in reputational and financial loss.  It is recommended that DNS records in Route53 are reviewed regularly and removed as soon as the underlying resource is decommissioned.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "5.4 External Monitoring and Notifications",
        "body": "  Beyond manual checks, it is recommended that a service such as Assertible is used. This will allow the development team to detect unavailable endpoints and enforce regularly-checked assertions, such as proper return codes or page content. Furthermore, such a service should integrate other means of communication such as Slack notifications, SMS messages, or arbitrary webhook calls to notify an on-duty developer as quickly as possible.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"
    },
    {
        "title": "5.1 Merkle.checkMembership allows existence proofs for the same leaf in multiple locations in the tree    Addressed",
        "body": "  Resolution   This was addressed in   omisego/plasma-contracts#533 by including a check in  omisego/plasma-contracts#547 ensured the passed-in index satisfied the recommended criterion.  Description  checkMembership is used by several contracts to prove that transactions exist in the child chain. The function uses a leaf, an index, and a proof to construct a hypothetical root hash. This constructed hash is compared to the passed in rootHash parameter. If the two are equivalent, the proof is considered valid.  The proof is performed iteratively, and uses a pseudo-index (j) to determine whether the next proof element represents a  left branch  or  right branch :  code/plasma_framework/contracts/src/utils/Merkle.sol:L28-L41  uint256 j = index;  // Note: We're skipping the first 32 bytes of `proof`, which holds the size of the dynamically sized `bytes`  for (uint256 i = 32; i <= proof.length; i += 32) {  // solhint-disable-next-line no-inline-assembly  assembly {  proofElement := mload(add(proof, i))  if (j % 2 == 0) {  computedHash = keccak256(abi.encodePacked(NODE_SALT, computedHash, proofElement));  } else {  computedHash = keccak256(abi.encodePacked(NODE_SALT, proofElement, computedHash));  j = j / 2;  If j is even, the computed hash is placed before the next proof element. If j is odd, the computed hash is placed after the next proof element. After each iteration, j is decremented by j = j / 2.  Because checkMembership makes no requirements on the height of the tree or the size of the proof relative to the provided index, it is possible to pass in invalid values for index that prove a leaf s existence in multiple locations in the tree.  Examples  By modifying existing tests, we showed that for a tree with 3 leaves, leaf 2 can be proven to exist at indices 2, 6, and 10 using the same proof each time. The modified test can be found here: https://gist.github.com/wadeAlexC/01b60099282a026f8dc1ac85d83489fd#file-merkle-test-js-L40-L67  Conclusion  Exit processing is meant to bypass exits processed more than once. This is implemented using an  output id  system, where each exited output should correspond to a unique id that gets flagged in the ExitGameController contract as it s exited. Before an exit is processed, its output id is calculated and checked against ExitGameController. If the output has already been exited, the exit being processed is deleted and skipped. Crucially, output id is calculated differently for standard transactions and deposit transactions: deposit output ids factor in the transaction index.  By using the behavior described in this issue in conjunction with methods discussed in issue 5.8 and issue 5.10, we showed that deposit transactions can be exited twice using indices 0 and 2**16. Because of the distinct output id calculation, these exits have different output ids and can be processed twice, allowing users to exit double their deposited amount.  A modified StandardExit.load.test.js shows that exits are successfully enqueued with a transaction index of 65536: https://gist.github.com/wadeAlexC/4ad459b7510e512bc9556e7c919e0965#file-standardexit-load-test-js-L55  Recommendation  Use the length of the proof to determine the maximum allowed index. The passed-in index should satisfy the following criterion: index < 2**(proof.length/32). Additionally, ensure range checks on transaction position decoding are sufficiently restrictive (see https://github.com/ConsenSys/omisego-morevp-audit-2019-10/issues/20).  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/546  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"
    },
    {
        "title": "5.2 Improper initialization of spending condition abstraction allows  v2 transactions  to exit using PaymentExitGame    Addressed",
        "body": "  Resolution   This was addressed in   omisego/plasma-contracts#478 by requiring that  Description  PaymentOutputToPaymentTxCondition is an abstraction around the transaction signature check needed for many components of the exit games. Its only function, verify, returns true if one transaction (inputTxBytes) is spent by another transaction (spendingTxBytes):  code/plasma_framework/contracts/src/exits/payment/spendingConditions/PaymentOutputToPaymentTxCondition.sol:L40-L69  function verify(  bytes calldata inputTxBytes,  uint16 outputIndex,  uint256 inputTxPos,  bytes calldata spendingTxBytes,  uint16 inputIndex,  bytes calldata signature,  bytes calldata /*optionalArgs*/  external  view  returns (bool)  PaymentTransactionModel.Transaction memory inputTx = PaymentTransactionModel.decode(inputTxBytes);  require(inputTx.txType == supportInputTxType, \"Input tx is an unsupported payment tx type\");  PaymentTransactionModel.Transaction memory spendingTx = PaymentTransactionModel.decode(spendingTxBytes);  require(spendingTx.txType == supportSpendingTxType, \"The spending tx is an unsupported payment tx type\");  UtxoPosLib.UtxoPos memory utxoPos = UtxoPosLib.build(TxPosLib.TxPos(inputTxPos), outputIndex);  require(  spendingTx.inputs[inputIndex] == bytes32(utxoPos.value),  \"Spending tx points to the incorrect output UTXO position\"  );  address payable owner = inputTx.outputs[outputIndex].owner();  require(owner == ECDSA.recover(eip712.hashTx(spendingTx), signature), \"Tx in not signed correctly\");  return true;  Verification process  The verification process is relatively straightforward. The contract performs some basic input validation, checking that the input transaction s txType matches supportInputTxType, and that the spending transaction s txType matches supportSpendingTxType. These values are set during construction.  Next, verify checks that the spending transaction contains an input that matches the position of one of the input transaction s outputs.  Finally, verify performs an EIP-712 hash on the spending transaction, and ensures it is signed by the owner of the output in question.  Implications of the abstraction  code/plasma_framework/contracts/src/framework/registries/ExitGameRegistry.sol:L58-L78  /**  @notice Registers an exit game within the PlasmaFramework. Only the maintainer can call the function.  @dev Emits ExitGameRegistered event to notify clients  @param _txType The tx type where the exit game wants to register  @param _contract Address of the exit game contract  @param _protocol The transaction protocol, either 1 for MVP or 2 for MoreVP  /  function registerExitGame(uint256 _txType, address _contract, uint8 _protocol) public onlyFrom(getMaintainer()) {  require(_txType != 0, \"Should not register with tx type 0\");  require(_contract != address(0), \"Should not register with an empty exit game address\");  require(_exitGames[_txType] == address(0), \"The tx type is already registered\");  require(_exitGameToTxType[_contract] == 0, \"The exit game contract is already registered\");  require(Protocol.isValidProtocol(_protocol), \"Invalid protocol value\");  _exitGames[_txType] = _contract;  _exitGameToTxType[_contract] = _txType;  _protocols[_txType] = _protocol;  _exitGameQuarantine.quarantine(_contract);  emit ExitGameRegistered(_txType, _contract, _protocol);  Migration and initialization  The migration script seems to corroborate this interpretation:  code/plasma_framework/migrations/5_deploy_and_register_payment_exit_game.js:L109-L124  // handle spending condition  await deployer.deploy(  PaymentOutputToPaymentTxCondition,  plasmaFramework.address,  PAYMENT_OUTPUT_TYPE,  PAYMENT_TX_TYPE,  );  const paymentToPaymentCondition = await PaymentOutputToPaymentTxCondition.deployed();  await deployer.deploy(  PaymentOutputToPaymentTxCondition,  plasmaFramework.address,  PAYMENT_OUTPUT_TYPE,  PAYMENT_V2_TX_TYPE,  );  const paymentToPaymentV2Condition = await PaymentOutputToPaymentTxCondition.deployed();  The migration script then registers both of these contracts in SpendingConditionRegistry, and then calls renounceOwnership, freezing the spending conditions registered permanently:  code/plasma_framework/migrations/5_deploy_and_register_payment_exit_game.js:L126-L135  console.log(`Registering paymentToPaymentCondition (${paymentToPaymentCondition.address}) to spendingConditionRegistry`);  await spendingConditionRegistry.registerSpendingCondition(  PAYMENT_OUTPUT_TYPE, PAYMENT_TX_TYPE, paymentToPaymentCondition.address,  );  console.log(`Registering paymentToPaymentV2Condition (${paymentToPaymentV2Condition.address}) to spendingConditionRegistry`);  await spendingConditionRegistry.registerSpendingCondition(  PAYMENT_OUTPUT_TYPE, PAYMENT_V2_TX_TYPE, paymentToPaymentV2Condition.address,  );  await spendingConditionRegistry.renounceOwnership();  Finally, the migration script registers a single exit game contract in PlasmaFramework:  code/plasma_framework/migrations/5_deploy_and_register_payment_exit_game.js:L137-L143  // register the exit game to framework  await plasmaFramework.registerExitGame(  PAYMENT_TX_TYPE,  paymentExitGame.address,  config.frameworks.protocols.moreVp,  { from: maintainerAddress },  );  Note that the associated _txType is permanently associated with the deployed exit game contract:  code/plasma_framework/contracts/src/framework/registries/ExitGameRegistry.sol:L58-L78  /**  @notice Registers an exit game within the PlasmaFramework. Only the maintainer can call the function.  @dev Emits ExitGameRegistered event to notify clients  @param _txType The tx type where the exit game wants to register  @param _contract Address of the exit game contract  @param _protocol The transaction protocol, either 1 for MVP or 2 for MoreVP  /  function registerExitGame(uint256 _txType, address _contract, uint8 _protocol) public onlyFrom(getMaintainer()) {  require(_txType != 0, \"Should not register with tx type 0\");  require(_contract != address(0), \"Should not register with an empty exit game address\");  require(_exitGames[_txType] == address(0), \"The tx type is already registered\");  require(_exitGameToTxType[_contract] == 0, \"The exit game contract is already registered\");  require(Protocol.isValidProtocol(_protocol), \"Invalid protocol value\");  _exitGames[_txType] = _contract;  _exitGameToTxType[_contract] = _txType;  _protocols[_txType] = _protocol;  _exitGameQuarantine.quarantine(_contract);  emit ExitGameRegistered(_txType, _contract, _protocol);  Conclusion  Recommendation  Remove PaymentOutputToPaymentTxCondition and SpendingConditionRegistry  Implement checks for specific spending conditions directly in exit game controllers. Emphasize clarity of function: ensure it is clear when called from the top level that a signature verification check and spending condition check are being performed.  If the inferred relationship between txType and PaymentExitGame is correct, ensure that each PaymentExitGame router checks for its supported txType. Alternatively, the check could be made in PaymentExitGame itself.  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/472  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"
    },
    {
        "title": "5.3 RLPReader - Leading zeroes allow multiple valid encodings and exit / output ids for the same transaction    Addressed",
        "body": "  Resolution   This was addressed in   omisego/plasma-contracts#507 with the addition of checks to ensure primitive decoding functions in  omisego/plasma-contracts#476 rejects leading zeroes in  Description  The current implementation of RLP decoding can take 2 different txBytes and decode them to the same structure. Specifically, the RLPReader.toUint method can decode 2 different types of bytes to the same number. For example:  0x821234 is decoded to uint(0x1234)  0x83001234 is decoded to uint(0x1234)  0xc101 can decode to uint(1), even though the tag specifies a short list  0x01 can decode to uint(1), even though the tag specifies a single byte  As explanation for this encoding:  0x821234 is broken down into 2 parts:  0x82 - represents 0x80 (the string tag) + 0x02 bytes encoded  0x1234 - are the encoded bytes  The same for 0x83001234:  0x83 - represents 0x80 (the string tag) + 0x03 bytes encoded  0x001234 - are the encoded bytes  The current implementation casts the encoded bytes into a uint256, so these different encodings are interpreted by the contracts as the same number:  uint(0x1234) = uint(0x001234)  code/plasma_framework/contracts/src/utils/RLPReader.sol:L112  result := mload(memPtr)  Having different valid encodings for the same data is a problem because the encodings are used to create hashes that are used as unique ids. This means that multiple ids can be created for the same data. The data should only have one possible id.  The encoding is used to create ids in these parts of the code:  Outputid.sol  code/plasma_framework/contracts/src/exits/utils/OutputId.sol:L18  return keccak256(abi.encodePacked(_txBytes, _outputIndex, _utxoPosValue));  code/plasma_framework/contracts/src/exits/utils/OutputId.sol:L32  return keccak256(abi.encodePacked(_txBytes, _outputIndex));  ExitId.sol  code/plasma_framework/contracts/src/exits/utils/ExitId.sol:L41  bytes32 hashData = keccak256(abi.encodePacked(_txBytes, _utxoPos.value));  code/plasma_framework/contracts/src/exits/utils/ExitId.sol:L54  return uint160((uint256(keccak256(_txBytes)) >> 105).setBit(151));  TxFinalizationVerifier.sol  code/plasma_framework/contracts/src/exits/utils/TxFinalizationVerifier.sol:L55  bytes32 leafData = keccak256(data.txBytes);  Other methods that are affected because they rely on the return values of these methods:  ExitId.sol  getStandardExitId getInFlightExitId  OutputId.sol  computeDepositOutputId computeNormalOutputId  PaymentChallengeIFENotCanonical.sol  verifyAndDeterminePositionOfTransactionIncludedInBlock verifyCompetingTxFinalized  PaymentChallengeStandardExit.sol  verifyChallengeTxProtocolFinalized  PaymentStartInFlightExit.sol  verifyInputTransactionIsStandardFinalized  PaymentExitGame.sol  getStandardExitId getInFlightExitId  PaymentOutputToPaymentTxCondition.sol  verify  Recommendation  Enforce strict-length decoding for txBytes, and specify that uint is decoded from a 32-byte short string.  Enforcing a 32-byte length for uint means that 0x1234 should always be encoded as:  0xa00000000000000000000000000000000000000000000000000000000000001234  0xa0 represents the tag + the length: 0x80 + 32  0000000000000000000000000000000000000000000000000000000000001234 is the number 32 bytes long with leading zeroes  Unfortunately, using leading zeroes is against the RLP spec:  https://github.com/ethereum/wiki/wiki/RLP  positive RLP integers must be represented in big endian binary form with no leading zeroes  This means that libraries interacting with OMG contracts which are going to correctly and fully implement the spec will generate  incorrect  encodings for uints; encodings that are not going to be recognized by the OMG contracts.  Fully correct spec encoding: 0x821234. Proposed encoding in this solution: 0xa00000000000000000000000000000000000000000000000000000000000001234.  Similarly enforce restrictions where they can be added; this is possible because of the strict structure format that needs to be encoded.  Some other potential solutions are included below. Note that these solutions are not recommended for reasons included below:  Normalize the encoding that gets passed to methods that hash the transaction for use as an id:  This can be implemented in the methods that call keccak256 on txBytes and should decode and re-encode the passed txBytes in order to normalize the passed encoding.  a txBytes is passed  the txBytes are decoded into structure: tmpDecodedStruct = decode(txBytes)  the tmpDecodedStruct is re-encoded in order to normalize it: normalizedTxBytes = encode(txBytes)  This method is not recommended because it needs a Solidity encoder to be implemented and a lot of gas will be used to decode and re-encode the initial txBytes.  Correctly and fully implement RLP decoding  This is another solution that adds a lot of code and is prone to errors.  The solution would be to enforce all of the restrictions when decoding and not accept any encoding that doesn t fully follow the spec. This for example means that is should not accept uints with leading zeroes.  This is a problem because it needs a lot of code that is not easy to write in Solidity (or EVM).  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"
    },
    {
        "title": "5.4 Recommendation: Remove TxFinalizationModel and TxFinalizationVerifier. Implement stronger checks in Merkle ",
        "body": "  Resolution   This was partially addressed in   omisego/plasma-contracts#503, with the removal of several unneeded branches of logic in  omisego/plasma-contracts#533 added a non-zero proof length check in  Description  TxFinalizationVerifier is an abstraction around the block inclusion check needed for many of the features of plasma exit games. It uses a struct defined in TxFinalizationModel as inputs to its two functions: isStandardFinalized and isProtocolFinalized.  isStandardFinalized returns the result of an inclusion proof. Although there are several branches, only the first is used:  code/plasma_framework/contracts/src/exits/utils/TxFinalizationVerifier.sol:L19-L32  /**  @notice Checks whether a transaction is \"standard finalized\"  @dev MVP: requires that both inclusion proof and confirm signature is checked  @dev MoreVp: checks inclusion proof only  /  function isStandardFinalized(Model.Data memory data) public view returns (bool) {  if (data.protocol == Protocol.MORE_VP()) {  return checkInclusionProof(data);  } else if (data.protocol == Protocol.MVP()) {  revert(\"MVP is not yet supported\");  } else {  revert(\"Invalid protocol value\");  isProtocolFinalized is unused:  code/plasma_framework/contracts/src/exits/utils/TxFinalizationVerifier.sol:L34-L47  /**  @notice Checks whether a transaction is \"protocol finalized\"  @dev MVP: must be standard finalized  @dev MoreVp: allows in-flight tx, so only checks for the existence of the transaction  /  function isProtocolFinalized(Model.Data memory data) public view returns (bool) {  if (data.protocol == Protocol.MORE_VP()) {  return data.txBytes.length > 0;  } else if (data.protocol == Protocol.MVP()) {  revert(\"MVP is not yet supported\");  } else {  revert(\"Invalid protocol value\");  Finally, the abstraction may have ramifications on the safety of Merkle.sol. As it stands now, Merkle.checkMembership should never be called directly by the exit game controllers, as it lacks an important check made in TxFinalizationVerifier.checkInclusionProof:  code/plasma_framework/contracts/src/exits/utils/TxFinalizationVerifier.sol:L49-L59  function checkInclusionProof(Model.Data memory data) private view returns (bool) {  if (data.inclusionProof.length == 0) {  return false;  (bytes32 root,) = data.framework.blocks(data.txPos.blockNum());  bytes32 leafData = keccak256(data.txBytes);  return Merkle.checkMembership(  leafData, data.txPos.txIndex(), root, data.inclusionProof  );  By introducing the abstraction of TxFinalizationVerifier, the input validation performed by Merkle is split across multiple files, and the reasonable-seeming decision of calling Merkle.checkMembership directly becomes unsafe. In fact, this occurs in one location in the contracts:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L187-L204  function verifyAndDeterminePositionOfTransactionIncludedInBlock(  bytes memory txbytes,  UtxoPosLib.UtxoPos memory utxoPos,  bytes32 root,  bytes memory inclusionProof  private  pure  returns(uint256)  bytes32 leaf = keccak256(txbytes);  require(  Merkle.checkMembership(leaf, utxoPos.txIndex(), root, inclusionProof),  \"Transaction is not included in block of Plasma chain\"  );  return utxoPos.value;  Recommendation  Remove TxFinalizationVerifier and TxFinalizationModel  Implement a proof length check in Merkle.sol  Call Merkle.checkMembership directly from exit controller contracts:  PaymentChallengeIFEOutputSpent.verifyInFlightTransactionStandardFinalized:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEOutputSpent.sol:L91  require(controller.txFinalizationVerifier.isStandardFinalized(finalizationData), \"In-flight transaction not finalized\");  PaymentChallengeIFENotCanonical.verifyCompetingTxFinalized:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L244  require(self.txFinalizationVerifier.isStandardFinalized(finalizationData), \"Failed to verify the position of competing tx\");  PaymentStartInFlightExit.verifyInputTransactionIsStandardFinalized:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartInFlightExit.sol:L307-L308  require(exitData.controller.txFinalizationVerifier.isStandardFinalized(finalizationData),  \"Input transaction is not standard finalized\");  If none of the above recommendations are implemented, ensure that PaymentChallengeIFENotCanonical uses the abstraction TxFinalizationVerifier so that a length check is performed on the inclusion proof.  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/471  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"
    },
    {
        "title": "5.5 Merkle - The implementation does not enforce inclusion of leaf nodes.    Addressed",
        "body": "  Resolution   This was addressed in   omisego/plasma-contracts#452 with the addition of leaf and node salts to the  Description  A observation with the current Merkle tree implementation is that it may be possible to validate nodes other than leaves. This is done by providing checkMembership with a reference to a hash within the tree, rather than a leaf.  code/plasma_framework/contracts/src/utils/Merkle.sol:L9-L42  /**  @notice Checks that a leaf hash is contained in a root hash  @param leaf Leaf hash to verify  @param index Position of the leaf hash in the Merkle tree  @param rootHash Root of the Merkle tree  @param proof A Merkle proof demonstrating membership of the leaf hash  @return True, if the leaf hash is in the Merkle tree; otherwise, False  /  function checkMembership(bytes32 leaf, uint256 index, bytes32 rootHash, bytes memory proof)  internal  pure  returns (bool)  require(proof.length % 32 == 0, \"Length of Merkle proof must be a multiple of 32\");  bytes32 proofElement;  bytes32 computedHash = leaf;  uint256 j = index;  // Note: We're skipping the first 32 bytes of `proof`, which holds the size of the dynamically sized `bytes`  for (uint256 i = 32; i <= proof.length; i += 32) {  // solhint-disable-next-line no-inline-assembly  assembly {  proofElement := mload(add(proof, i))  if (j % 2 == 0) {  computedHash = keccak256(abi.encodePacked(computedHash, proofElement));  } else {  computedHash = keccak256(abi.encodePacked(proofElement, computedHash));  j = j / 2;  return computedHash == rootHash;  The current implementation will validate the provided  leaf  and return true. This is a known problem of Merkle trees https://en.wikipedia.org/wiki/Merkle_tree#Second_preimage_attack.  Examples  Provide a hash from within the Merkle tree as the leaf argument. The index has to match the index of that node in regards to its current level in the tree. The rootHash has to be the correct Merkle tree rootHash. The proof has to skip the necessary number of levels because the nodes  underneath  the provided  leaf  will not be processed.  Recommendation  A remediation needs a fixed Merkle tree size as well as the addition of a byte prepended to each node in the tree. Another way would be to create a structure for the Merkle node and mark it as leaf or no leaf.  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/425  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"
    },
    {
        "title": "5.6 Maintainer can bypass exit game quarantine by registering not-yet-deployed contracts    Addressed",
        "body": "  Resolution   This was addressed in   commit 7669076be1dff47473ee877dcebef5989d7617ac by adding a check that registered contracts had nonzero  Description  The plasma framework uses an ExitGameRegistry to allow the maintainer to add new exit games after deployment. An exit game is any arbitrary contract. In order to prevent the maintainer from adding malicious exit games that steal user funds, the framework uses a  quarantine  system whereby newly-registered exit games have restricted permissions until their quarantine period has expired. The quarantine period is by default 3 * minExitPeriod, and is intended to facilitate auditing of the new exit game s functionality by the plasma users.  However, by registering an exit game at a contract which has not yet been deployed, the maintainer can prevent plasma users from auditing the game until the quarantine period has expired. After the quarantine period has expired, the maintainer can deploy the malicious exit game and immediately steal funds.  Explanation  Exit games are registered in the following function, callable only by the plasma contract maintainer:  code/plasma_framework/contracts/src/framework/registries/ExitGameRegistry.sol:L58-L78  /**  @notice Registers an exit game within the PlasmaFramework. Only the maintainer can call the function.  @dev Emits ExitGameRegistered event to notify clients  @param _txType The tx type where the exit game wants to register  @param _contract Address of the exit game contract  @param _protocol The transaction protocol, either 1 for MVP or 2 for MoreVP  /  function registerExitGame(uint256 _txType, address _contract, uint8 _protocol) public onlyFrom(getMaintainer()) {  require(_txType != 0, \"Should not register with tx type 0\");  require(_contract != address(0), \"Should not register with an empty exit game address\");  require(_exitGames[_txType] == address(0), \"The tx type is already registered\");  require(_exitGameToTxType[_contract] == 0, \"The exit game contract is already registered\");  require(Protocol.isValidProtocol(_protocol), \"Invalid protocol value\");  _exitGames[_txType] = _contract;  _exitGameToTxType[_contract] = _txType;  _protocols[_txType] = _protocol;  _exitGameQuarantine.quarantine(_contract);  emit ExitGameRegistered(_txType, _contract, _protocol);  Notably, the function does not check the extcodesize of the submitted contract. As such, the maintainer can submit the address of a contract which does not yet exist and is not auditable.  After at least 3 * minExitPeriod seconds pass, the submitted contract now has full permissions as a registered exit game and can pass all checks using the onlyFromNonQuarantinedExitGame modifier:  code/plasma_framework/contracts/src/framework/registries/ExitGameRegistry.sol:L33-L40  /**  @notice A modifier to verify that the call is from a non-quarantined exit game  /  modifier onlyFromNonQuarantinedExitGame() {  require(_exitGameToTxType[msg.sender] != 0, \"The call is not from a registered exit game contract\");  require(!_exitGameQuarantine.isQuarantined(msg.sender), \"ExitGame is quarantined\");  _;  Additionally, the submitted contract passes checks made by external contracts using the isExitGameSafeToUse function:  code/plasma_framework/contracts/src/framework/registries/ExitGameRegistry.sol:L48-L56  /**  @notice Checks whether the contract is safe to use and is not under quarantine  @dev Exposes information about exit games quarantine  @param _contract Address of the exit game contract  @return boolean Whether the contract is safe to use and is not under quarantine  /  function isExitGameSafeToUse(address _contract) public view returns (bool) {  return _exitGameToTxType[_contract] != 0 && !_exitGameQuarantine.isQuarantined(_contract);  These permissions allow a registered quarantine to:  Withdraw any users  tokens from ERC20Vault:  code/plasma_framework/contracts/src/vaults/Erc20Vault.sol:L52-L55  function withdraw(address payable receiver, address token, uint256 amount) external onlyFromNonQuarantinedExitGame {  IERC20(token).safeTransfer(receiver, amount);  emit Erc20Withdrawn(receiver, token, amount);  Withdraw any users  ETH from EthVault:  code/plasma_framework/contracts/src/vaults/EthVault.sol:L46-L54  function withdraw(address payable receiver, uint256 amount) external onlyFromNonQuarantinedExitGame {  // we do not want to block exit queue if transfer is unucessful  // solhint-disable-next-line avoid-call-value  (bool success, ) = receiver.call.value(amount)(\"\");  if (success) {  emit EthWithdrawn(receiver, amount);  } else {  emit WithdrawFailed(receiver, amount);  Activate and deactivate the ExitGameController reentrancy mutex:  code/plasma_framework/contracts/src/framework/ExitGameController.sol:L63-L66  function activateNonReentrant() external onlyFromNonQuarantinedExitGame() {  require(!mutex, \"Reentrant call\");  mutex = true;  code/plasma_framework/contracts/src/framework/ExitGameController.sol:L72-L75  function deactivateNonReentrant() external onlyFromNonQuarantinedExitGame() {  require(mutex, \"Not locked\");  mutex = false;  enqueue arbitrary exits:  code/plasma_framework/contracts/src/framework/ExitGameController.sol:L115-L138  function enqueue(  uint256 vaultId,  address token,  uint64 exitableAt,  TxPosLib.TxPos calldata txPos,  uint160 exitId,  IExitProcessor exitProcessor  external  onlyFromNonQuarantinedExitGame  returns (uint256)  bytes32 key = exitQueueKey(vaultId, token);  require(hasExitQueue(key), \"The queue for the (vaultId, token) pair is not yet added to the Plasma framework\");  PriorityQueue queue = exitsQueues[key];  uint256 priority = ExitPriority.computePriority(exitableAt, txPos, exitId);  queue.insert(priority);  delegations[priority] = exitProcessor;  emit ExitQueued(exitId, priority);  return priority;  Flag outputs as  spent :  code/plasma_framework/contracts/src/framework/ExitGameController.sol:L210-L213  function flagOutputSpent(bytes32 _outputId) external onlyFromNonQuarantinedExitGame {  require(_outputId != bytes32(\"\"), \"Should not flag with empty outputId\");  isOutputSpent[_outputId] = true;  Recommendation  registerExitGame should check that extcodesize of the submitted contract is non-zero.  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/410  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"
    },
    {
        "title": "5.7 EthVault - Unused state variable    Addressed",
        "body": "  Resolution   This was addressed in   commit ea36f5ff46ab72ec5c281fa0a3dffe3bcc83178b.  Description  The state variable withdrawEntryCounter is not used in the code.  code/plasma_framework/contracts/src/vaults/EthVault.sol:L8  uint256 private withdrawEntryCounter = 0;  Recommendation  Remove it from the contract.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"
    },
    {
        "title": "5.8 Recommendation: Add a tree height limit check to Merkle.sol ",
        "body": "  Description  Each plasma block has a maximum of 2 ** 16 transactions, which corresponds to a maximum Merkle tree height of 16. The Merkle library currently checks that the proof is comprised of 32-byte segments, but neglects to check the maximum height:  code/plasma_framework/contracts/src/utils/Merkle.sol:L17-L23  function checkMembership(bytes32 leaf, uint256 index, bytes32 rootHash, bytes memory proof)  internal  pure  returns (bool)  require(proof.length % 32 == 0, \"Length of Merkle proof must be a multiple of 32\");  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/467  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"
    },
    {
        "title": "5.9 Recommendation: remove IsDeposit and add a similar getter to BlockController    Addressed",
        "body": "  Resolution   This was addressed in   commit 0fee13f7f084983139eb47636ff785ebea8a1c36 by removing the  Description  The IsDeposit library is used to check whether a block number is a deposit or not. The logic is simple - if blockNum % childBlockInterval is nonzero, the block number is a deposit.  By including this check in BlockController instead, the contract can perform an existence check as well. The function in BlockController would return the same result as the IsDeposit library, but would additionally revert if the block in question does not exist:  Note that this check is made at the cost of an external call. If the check needs to be made multiple times in a transaction, the result should be cached.  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/466  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"
    },
    {
        "title": "5.10 Recommendation: Merge TxPosLib into UtxoPosLib and implement a decode function with range checks. ",
        "body": "  Resolution   This was partially addressed in   omisego/plasma-contracts#515 with the merging of  omisego/plasma-contracts#533 implemented stricter range checks for block number and transaction index. Note that the maximum output index in  Description  TxPosLib and UtxoPosLib serve very similar functions. They both provide utility functions to access the block number and tx index of a packed utxo position variable. UtxoPosLib, additionally, provides a function to retrieve the output index of a packed utxo position variable.  What they both lack, though, is sanity checks on the values packed inside a utxo position variable. By implementing a function UtxoPosLib.decode(uint _utxoPos) returns (UtxoPos), each exit controller contract can ensure that the values it is using make logical sense. The decode function should check that:  txIndex is between 0 and 2**16  outputIndex is between 0 and 3  Currently, neither of these restrictions is explicitly enforced. As for blockNum, the best check is that it exists in the PlasmaFramework contract with a nonzero root. Since UtxoPosLib is a pure library, that check is better performed elsewhere (See https://github.com/ConsenSys/omisego-morevp-audit-2019-10/issues/21).  Once implemented, all contracts should avoid casting values directly to the UtxoPos struct, in favor of using the decode function. Merging the two files will help with this.  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/465  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"
    },
    {
        "title": "5.11 Recommendation: Implement additional existence and range checks on inputs and storage reads ",
        "body": "  Resolution   This was partially addressed in   omisego/plasma-contracts#524 and  omisego/plasma-contracts#483. Not all recommended checks were included.  Description  Many input validation and storage read checks are made implicitly, rather than explicitly. The following compilation notes each line of code in the exit controller contracts where an additional check should be added.  Examples  1. PaymentChallengeIFEInputSpent:  Check that inFlightTx has a nonzero input at the provided index:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEInputSpent.sol:L96  require(ife.isInputPiggybacked(args.inFlightTxInputIndex), \"The indexed input has not been piggybacked\");  Check that each transaction is nonzero and is correctly formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEInputSpent.sol:L98-L101  require(  keccak256(args.inFlightTx) != keccak256(args.challengingTx),  \"The challenging transaction is the same as the in-flight transaction\"  );  Check that resulting outputId is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEInputSpent.sol:L123  bytes32 ifeInputOutputId = data.ife.inputs[data.args.inFlightTxInputIndex].outputId;  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEInputSpent.sol:L125  UtxoPosLib.UtxoPos memory utxoPos = UtxoPosLib.UtxoPos(data.args.inputUtxoPos);  See issue 5.9  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEInputSpent.sol:L126  bytes32 challengingTxInputOutputId = data.controller.isDeposit.test(utxoPos.blockNum())  Check that inputTx is nonzero and well-formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEInputSpent.sol:L127-L128  ? OutputId.computeDepositOutputId(data.args.inputTx, utxoPos.outputIndex(), utxoPos.value)  : OutputId.computeNormalOutputId(data.args.inputTx, utxoPos.outputIndex());  Check that output is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEInputSpent.sol:L149  WireTransaction.Output memory output = WireTransaction.getOutput(data.args.challengingTx, data.args.challengingTxInputIndex);  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEInputSpent.sol:L156  UtxoPosLib.UtxoPos memory inputUtxoPos = UtxoPosLib.UtxoPos(data.args.inputUtxoPos);  Check that challengingTx has a nonzero input at provided index:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEInputSpent.sol:L163  data.args.challengingTxInputIndex,  2. PaymentChallengeIFENotCanonical:  Check that each transaction is nonzero and is correctly formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L98-L101  require(  keccak256(args.inFlightTx) != keccak256(args.competingTx),  \"The competitor transaction is the same as transaction in-flight\"  );  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L104  UtxoPosLib.UtxoPos memory inputUtxoPos = UtxoPosLib.UtxoPos(args.inputUtxoPos);  See issue 5.9  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L107  if (self.isDeposit.test(inputUtxoPos.blockNum())) {  Check that inputTx is nonzero and well-formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L108-L110  outputId = OutputId.computeDepositOutputId(args.inputTx, inputUtxoPos.outputIndex(), inputUtxoPos.value);  } else {  outputId = OutputId.computeNormalOutputId(args.inputTx, inputUtxoPos.outputIndex());  Check that inFlightTx has a nonzero input at the provided index:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L112-L113  require(outputId == ife.inputs[args.inFlightTxInputIndex].outputId,  \"Provided inputs data does not point to the same outputId from the in-flight exit\");  Check that output is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L115  WireTransaction.Output memory output = WireTransaction.getOutput(args.inputTx, args.inFlightTxInputIndex);  Check that competingTx has a nonzero input at provided index:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L126  args.competingTxInputIndex,  Check that resulting position is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L133  uint256 competitorPosition = verifyCompetingTxFinalized(self, args, output);  Check that inFlightTxPos is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L171-L173  require(  ife.oldestCompetitorPosition > inFlightTxPos,  \"In-flight transaction must be younger than competitors to respond to non-canonical challenge\");  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L175  UtxoPosLib.UtxoPos memory utxoPos = UtxoPosLib.UtxoPos(inFlightTxPos);  Check that block root is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L176  (bytes32 root, ) = self.framework.blocks(utxoPos.blockNum());  Check that inFlightTx is nonzero and well-formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L178  inFlightTx, utxoPos, root, inFlightTxInclusionProof  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L218  UtxoPosLib.UtxoPos memory competingTxUtxoPos = UtxoPosLib.UtxoPos(args.competingTxPos);  3. PaymentChallengeIFEOutputSpent:  Check that inFlightTx is nonzero and is well-formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEOutputSpent.sol:L54  uint160 exitId = ExitId.getInFlightExitId(args.inFlightTx);  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEOutputSpent.sol:L58  UtxoPosLib.UtxoPos memory utxoPos = UtxoPosLib.UtxoPos(args.outputUtxoPos);  Check that inFlightTx has a nonzero output at the provided index:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEOutputSpent.sol:L60-L63  require(  ife.isOutputPiggybacked(outputIndex),  \"Output is not piggybacked\"  );  Check that bond size is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEOutputSpent.sol:L70  uint256 piggybackBondSize = ife.outputs[outputIndex].piggybackBondSize;  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEOutputSpent.sol:L83  UtxoPosLib.UtxoPos memory utxoPos = UtxoPosLib.UtxoPos(args.outputUtxoPos);  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEOutputSpent.sol:L101  UtxoPosLib.UtxoPos memory utxoPos = UtxoPosLib.UtxoPos(args.outputUtxoPos);  Check that challengingTx is nonzero and is well-formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEOutputSpent.sol:L102  uint256 challengingTxType = WireTransaction.getTransactionType(args.challengingTx);  Check that output is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEOutputSpent.sol:L103  WireTransaction.Output memory output = WireTransaction.getOutput(args.challengingTx, utxoPos.outputIndex());  Check that challengingTx has a nonzero input at provided index:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEOutputSpent.sol:L116  args.challengingTxInputIndex,  4. PaymentChallengeStandardExit:  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeStandardExit.sol:L110  UtxoPosLib.UtxoPos memory utxoPos = UtxoPosLib.UtxoPos(data.exitData.utxoPos);  Check that exitingTx is nonzero and well-formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeStandardExit.sol:L112  .decode(data.args.exitingTx)  Check that output is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeStandardExit.sol:L111-L113  PaymentOutputModel.Output memory output = PaymentTransactionModel  .decode(data.args.exitingTx)  .outputs[utxoPos.outputIndex()];  Check that challengeTx is nonzero and well-formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeStandardExit.sol:L128  uint256 challengeTxType = WireTransaction.getTransactionType(data.args.challengeTx);  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeStandardExit.sol:L134  txPos: TxPosLib.TxPos(data.args.challengeTxPos),  See issue 5.9  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeStandardExit.sol:L157  bytes32 outputId = data.controller.isDeposit.test(utxoPos.blockNum())  Check that challengeTx has a nonzero input at provided index:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeStandardExit.sol:L166  args.inputIndex,  5. PaymentPiggybackInFlightExit:  Check that inFlightTx is nonzero and well-formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentPiggybackInFlightExit.sol:L93  uint160 exitId = ExitId.getInFlightExitId(args.inFlightTx);  Check that inFlightTx has a nonzero input at provided index:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentPiggybackInFlightExit.sol:L99  require(!exit.isInputPiggybacked(args.inputIndex), \"Indexed input already piggybacked\");  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentPiggybackInFlightExit.sol:L108  enqueue(self, withdrawData.token, UtxoPosLib.UtxoPos(exit.position), exitId);  Check that inFlightTx is nonzero and is well-formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentPiggybackInFlightExit.sol:L130  uint160 exitId = ExitId.getInFlightExitId(args.inFlightTx);  Check that inFlightTx has a nonzero output at provided index:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentPiggybackInFlightExit.sol:L136  require(!exit.isOutputPiggybacked(args.outputIndex), \"Indexed output already piggybacked\");  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentPiggybackInFlightExit.sol:L147  enqueue(self, withdrawData.token, UtxoPosLib.UtxoPos(exit.position), exitId);  6. PaymentStartInFlightExit:  Check that inFlightTx is nonzero and is well-formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartInFlightExit.sol:L146  exitData.exitId = ExitId.getInFlightExitId(args.inFlightTx);  Check that the length of inputTxs is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartInFlightExit.sol:L150  exitData.inputTxs = args.inputTxs;  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartInFlightExit.sol:L167  utxosPos[i] = UtxoPosLib.UtxoPos(inputUtxosPos[i]);  See issue 5.9  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartInFlightExit.sol:L180  bool isDepositTx = controller.isDeposit.test(utxoPos[i].blockNum());  Check that each inputTxs is nonzero and well-formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartInFlightExit.sol:L181-L183  outputIds[i] = isDepositTx  ? OutputId.computeDepositOutputId(inputTxs[i], utxoPos[i].outputIndex(), utxoPos[i].value)  : OutputId.computeNormalOutputId(inputTxs[i], utxoPos[i].outputIndex());  Check that each output is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartInFlightExit.sol:L200  WireTransaction.Output memory output = WireTransaction.getOutput(inputTxs[i], outputIndex);  Check that inFlightTx has nonzero inputs for all i:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartInFlightExit.sol:L327-L328  exitData.inFlightTxRaw,  i,  Check that each output is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartInFlightExit.sol:L407  PaymentOutputModel.Output memory output = exitData.inFlightTx.outputs[i];  7. PaymentStartStandardExit:  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartStandardExit.sol:L119  UtxoPosLib.UtxoPos memory utxoPos = UtxoPosLib.UtxoPos(args.utxoPos);  Check that output is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartStandardExit.sol:L121  PaymentOutputModel.Output memory output = outputTx.outputs[utxoPos.outputIndex()];  Check that timestamp is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartStandardExit.sol:L124  (, uint256 blockTimestamp) = controller.framework.blocks(utxoPos.blockNum());  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/463  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"
    },
    {
        "title": "5.12 Recommendation: Remove optional arguments and clean unused code    Addressed",
        "body": "  Resolution   This was addressed in   omisego/plasma-contracts#496 and  omisego/plasma-contracts#503 with the removal of the output guard handler pattern, the simplification of the tx finalization check via  Description  Several locations in the codebase feature unused arguments, functions, return values, and more. There are two primary reasons to remove these artifacts from the codebase:  Mass exits are the primary safeguard against a byzantine operator. The biggest bottleneck of a mass exit is transaction throughput, so plasma rootchain implementations should strive to be as efficient as possible. Many unused features require external calls, memory allocation, unneeded calculation, and more.  The contracts are set up to be extensible by way of the addition of new exit games to the system.  Optional  or unimplemented features in current exit games should be removed for simplicity s sake, as they currently make up a large portion of the codebase.  Examples  Output guard handlers  These offer very little utility in the current contracts. The main contract, PaymentOutputGuardHandler, has three functions:  isValid enforces that some  preimage  value passed in via calldata has a length of zero. This could be removed along with the unused  preimage  parameter. getExitTarget converts a bytes20 to address payable (with the help of AddressPayable.sol). This could be removed in favor of using AddressPayable directly where needed. getConfirmSigAddress simply returns an empty address. This should be removed wherever used - empty fields should be a rare exception or an error, rather than being injected as unused values into critical functions.   The minimal utility offered comes at the price of using an external call to the OutputGuardHandlerRegistry, as well as an external call for each of the functions mentioned above. Overall, the existence of output guard handlers adds thousands of gas to the exit process. Referenced contracts: IOutputGuardHandler, OutputGuardModel, PaymentOutputGuardHandler, OutputGuardHandlerRegistry  Payment router arguments  Several fields in the exit router structs are marked  optional,  and are not used in the contracts. While this is not particularly impactful, it does clutter and confuse the contracts. Many  optional  fields are referenced and passed into functions which do not use them. Of note is the crucially-important signature verification function, PaymentOutputToPaymentTxCondition.verify, where StartExitData.inputSpendingConditionOptionalArgs resolves to an unnamed parameter:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartInFlightExit.sol:L323-L332  bool isSpentByInFlightTx = condition.verify(  exitData.inputTxs[i],  exitData.inputUtxosPos[i].outputIndex(),  exitData.inputUtxosPos[i].txPos().value,  exitData.inFlightTxRaw,  i,  exitData.inFlightTxWitnesses[i],  exitData.inputSpendingConditionOptionalArgs[i]  );  require(isSpentByInFlightTx, \"Spending condition failed\");  code/plasma_framework/contracts/src/exits/payment/spendingConditions/PaymentOutputToPaymentTxCondition.sol:L40-L47  function verify(  bytes calldata inputTxBytes,  uint16 outputIndex,  uint256 inputTxPos,  bytes calldata spendingTxBytes,  uint16 inputIndex,  bytes calldata signature,  bytes calldata /*optionalArgs*/  The additional fields clutter the namespace of each struct, confusing the purpose of the other fields. For example, PaymentInFlightExitRouterArgs.StartExitArgs features two fields, inputTxsConfirmSigs and inFlightTxsWitnesses, the former of which is marked  optional . In fact, the inFlightTxsWitnesses field ends up containing the signatures passed to the spending condition verifier and ECDSA library:  code/plasma_framework/contracts/src/exits/payment/routers/PaymentInFlightExitRouterArgs.sol:L4-L24  /**  @notice Wraps arguments for startInFlightExit.  @param inFlightTx RLP encoded in-flight transaction.  @param inputTxs Transactions that created the inputs to the in-flight transaction. In the same order as in-flight transaction inputs.  @param inputUtxosPos Utxos that represent in-flight transaction inputs. In the same order as input transactions.  @param outputGuardPreimagesForInputs (Optional) Output guard pre-images for in-flight transaction inputs. Length must always match that of the inputTxs  @param inputTxsInclusionProofs Merkle proofs that show the input-creating transactions are valid. In the same order as input transactions.  @param inputTxsConfirmSigs (Optional) Confirm signatures for the input txs. Should be empty bytes if the input tx is MoreVP. Length must always match that of the inputTxs  @param inFlightTxWitnesses Witnesses for in-flight transaction. In the same order as input transactions.  @param inputSpendingConditionOptionalArgs (Optional) Additional args for the spending condition for checking inputs. Should provide empty bytes if nothing is required. Length must always match that of the inputTxs  /  struct StartExitArgs {  bytes inFlightTx;  bytes[] inputTxs;  uint256[] inputUtxosPos;  bytes[] outputGuardPreimagesForInputs;  bytes[] inputTxsInclusionProofs;  bytes[] inputTxsConfirmSigs;  bytes[] inFlightTxWitnesses;  bytes[] inputSpendingConditionOptionalArgs;  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/457  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"
    },
    {
        "title": "5.13 Recommendation: Remove WireTransaction and PaymentOutputModel. Fold functionality into an extended PaymentTransactionModel ",
        "body": "  Description  RLP decoding is performed on transaction bytes in each of WireTransaction, PaymentOutputModel, and PaymentTransactionModel. The latter is the primary decoding function for transactions, while the former two contracts deal with outputs specifically.  Both WireTransaction and PaymentOutputModel make use of RLPReader to decode transaction objects, and both implement very similar features. Rather than having a codebase with two separate definitions for struct Output, PaymentTransactionModel should be extended to implement all required functionality.  Examples  PaymentTransactionModel should include three distinct decoding functions:  decodeDepositTx decodes a deposit transaction, which has no inputs and exactly 1 output. decodeSpendTx decodes a spend transaction, which has exactly 4 inputs and 4 outputs. decodeOutput decodes an output, which is a long list with 4 fields (uint, address, address, uint)  A mock implementation including decodeSpendTx and decodeOutput is shown here: https://gist.github.com/wadeAlexC/7820c0cd82fd5fdc11a0ad58a84165ae  OmiseGo may want to consider enforcing restrictions on the ordering of empty and nonempty fields here as well.  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/456  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"
    },
    {
        "title": "5.14 ECDSA error value is not handled    Addressed",
        "body": "  Resolution   This was addressed in   commit 32288ccff5b867a7477b4eaf3beb0587a4684d7a by adding a check that the returned value is nonzero.  Description  The OpenZeppelin ECDSA library returns address(0x00) for many cases with malformed signatures:  contracts/cryptography/ECDSA.sol:L57-L63  if (uint256(s) > 0x7FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF5D576E7357A4501DDFE92F46681B20A0) {  return address(0);  if (v != 27 && v != 28) {  return address(0);  The PaymentOutputToPaymentTxCondition contract does not explicitly handle this case:  code/plasma_framework/contracts/src/exits/payment/spendingConditions/PaymentOutputToPaymentTxCondition.sol:L65-L68  address payable owner = inputTx.outputs[outputIndex].owner();  require(owner == ECDSA.recover(eip712.hashTx(spendingTx), signature), \"Tx in not signed correctly\");  return true;  Recommendation  Adding a check to handle this case will make it easier to reason about the code.  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/454  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"
    },
    {
        "title": "5.15 No existence checks on framework block and timestamp reads    Addressed",
        "body": "  Resolution   This was addressed in   commit c5e5a460a2082b809a2c45b2d6a69b738b34937a by adding checks that block root and timestamp reads return nonzero values.  Description  The exit game libraries make several queries to the main PlasmaFramework contract where plasma block hashes and timestamps are stored. In multiple locations, the return values of these queries are not checked for existence.  Examples  PaymentStartStandardExit.setupStartStandardExitData:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartStandardExit.sol:L124  (, uint256 blockTimestamp) = controller.framework.blocks(utxoPos.blockNum());  PaymentChallengeIFENotCanonical.respond:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L176  (bytes32 root, ) = self.framework.blocks(utxoPos.blockNum());  PaymentPiggybackInFlightExit.enqueue:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentPiggybackInFlightExit.sol:L167  (, uint256 blockTimestamp) = controller.framework.blocks(utxoPos.blockNum());  TxFinalizationVerifier.checkInclusionProof:  code/plasma_framework/contracts/src/exits/utils/TxFinalizationVerifier.sol:L54  (bytes32 root,) = data.framework.blocks(data.txPos.blockNum());  Recommendation  Although none of these examples seem exploitable, adding existence checks makes it easier to reason about the code. Each query to PlasmaFramework.blocks should be followed with a check that the returned value is nonzero.  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/463  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"
    },
    {
        "title": "5.16 BondSize - effectiveUpdateTime should be uint64 ",
        "body": "  Description  In BondSize, the mechanism to update the size of the bond has a grace period after which the new bond size becomes active.  When updating the bond size, the time is casted as a uint64 and saved in a uint128 variable.  code/plasma_framework/contracts/src/exits/utils/BondSize.sol:L24  uint128 effectiveUpdateTime;  code/plasma_framework/contracts/src/exits/utils/BondSize.sol:L11  uint64 constant public WAITING_PERIOD = 2 days;  code/plasma_framework/contracts/src/exits/utils/BondSize.sol:L57  self.effectiveUpdateTime = uint64(now) + WAITING_PERIOD;  There s no need to use a uint128 to save the time if it never will take up that much space.  Recommendation  Change the type of the effectiveUpdateTime to uint64.  uint128 effectiveUpdateTime;  + uint64 effectiveUpdateTime;  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"
    },
    {
        "title": "5.17 PaymentExitGame contains several redundant plasmaFramework declarations ",
        "body": "  Description  PaymentExitGame inherits from both PaymentInFlightExitRouter and PaymentStandardExitRouter. All three contracts declare and initialize their own PlasmaFramework variable. This pattern can be misleading, and may lead to subtle issues in future versions of the code.  Examples  PaymentExitGame declaration:  code/plasma_framework/contracts/src/exits/payment/PaymentExitGame.sol:L18  PlasmaFramework private plasmaFramework;  PaymentInFlightExitRouter declaration:  code/plasma_framework/contracts/src/exits/payment/routers/PaymentInFlightExitRouter.sol:L53  PlasmaFramework private framework;  PaymentStandardExitRouter declaration:  code/plasma_framework/contracts/src/exits/payment/routers/PaymentStandardExitRouter.sol:L45  PlasmaFramework private framework;  Each variable is initialized in the corresponding file s constructor.  Recommendation  Introduce an inherited contract common to PaymentStandardExitRouter and PaymentInFlightExitRouter with the PlasmaFramework variable. Make the variable internal so it is visible to inheriting contracts.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"
    },
    {
        "title": "5.18 BlockController - inaccurate description of childBlockInterval for submitDepositBlock ",
        "body": "  Description  code/plasma_framework/contracts/src/framework/BlockController.sol:L96-L114  /**  @notice Submits a block for deposit  @dev Block number adds 1 per submission; it's possible to have at most 'childBlockInterval' deposit blocks between two child chain blocks  @param _blockRoot Merkle root of the Plasma block  @return The deposit block number  /  function submitDepositBlock(bytes32 _blockRoot) public onlyFromNonQuarantinedVault returns (uint256) {  require(isChildChainActivated == true, \"Child chain has not been activated by authority address yet\");  require(nextDeposit < childBlockInterval, \"Exceeded limit of deposits per child block interval\");  uint256 blknum = nextDepositBlock();  blocks[blknum] = BlockModel.Block({  root : _blockRoot,  timestamp : block.timestamp  });  nextDeposit++;  return blknum;  However, the comment at line 98 mentions the following:  [..] it s possible to have at most  childBlockInterval  deposit blocks between two child chain blocks [..]  This comment is inaccurate, as a childBlockInterval of 1 would not allow deposits at all (Note how nextDeposit is always >=1).  Remediation  The comment should read: [..] it s possible to have at most  childBlockInterval -1  deposit blocks between two child chain blocks [..]. Make sure to properly validate inputs for these values when deploying the contract to avoid obvious misconfiguration.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"
    },
    {
        "title": "5.19 PlasmaFramework - Can omit inheritance of VaultRegistry ",
        "body": "  Description  The contract PlasmaFramework inherits VaultRegistry even though it does not use any of the methods directly. Also BlockController inherits VaultRegistry effectively adding all of the needed functionality in there.  Remediation  PlasmaFramework does not need to inherit VaultRegistry, thus the import and the inheritance can be removed from PlasmaFramework.sol.  import \"./BlockController.sol\";  import \"./ExitGameController.sol\";  import \"./registries/VaultRegistry.sol\";  import \"./registries/ExitGameRegistry.sol\";  contract PlasmaFramework is VaultRegistry, ExitGameRegistry, ExitGameController, BlockController {  +contract PlasmaFramework is ExitGameRegistry, ExitGameController, BlockController {  uint256 public constant CHILD_BLOCK_INTERVAL = 1000;  /**  All tests still pass after removing the inheritance.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"
    },
    {
        "title": "5.20 BlockController - maintainer should be the only entity to set new authority    Addressed",
        "body": "  Resolution   This was addressed in   commit 25c2560e3b2e40ce9a10c40da97c3f79afc2c641 with the removal of the  Description  code/plasma_framework/contracts/src/framework/BlockController.sol:L69-L72  function setAuthority(address newAuthority) external onlyFrom(authority) {  require(newAuthority != address(0), \"Authority address cannot be zero\");  authority = newAuthority;  security specification notes that the  Authority: EOA used exclusively to submit plasma block hashes to the root chain. The child chain assumes at deployment that the authority account has nonce zero and no transactions have been sent from it.  However, no transactions might not be possible as authority is the only one to activateChildChain. Once activated, the child chain cannot be de-activated but the authority can change.  elixir-omg#managing-the-operator-address notes the following for operator aka authority:  As a consequence, the operator address must never send any other transactions, if it intends to continue submitting blocks. (Workarounds to this limitation are available, if there s such requirement.)  Additionally, setAuthority should emit an event to allow participants to react to this change in the system and have an audit trial.  Remediation  Remove the setAuthority function, or clarify its intended purpose and add an event so it can be detected by users.  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/403  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"
    },
    {
        "title": "4.1 Node Operators Can Stake Validators That Were Not Proposed by Them. ",
        "body": "  In GeodeFi system node operators are meant to add the new validators in two steps:  Proposal step where 1 ETH of the pre-stake deposit is committed.  Stake step, where the 1 ETH pre-stake is reimbursed to the node operator, and the 32ETH user stake is sent to a validator.  The issue itself stems from the fact that node operators are allowed to stake the validators of the other node operators. In the stake() function there is no check of the validator s operatorId against the operator performing the stake. Meaning that node operator A can stake validators of node operator B.  contracts/Portal/modules/StakeModule/libs/StakeModuleLib.sol:L1478-L1558  function stake(  PooledStaking storage self,  DSML.IsolatedStorage storage DATASTORE,  uint256 operatorId,  bytes[] calldata pubkeys  ) external {  _authenticate(DATASTORE, operatorId, false, true, [true, false]);  require(  (pubkeys.length > 0) && (pubkeys.length <= DCL.MAX_DEPOSITS_PER_CALL),  \"SML:1 - 50 validators\"  );  uint256 _verificationIndex = self.VERIFICATION_INDEX;  for (uint256 j = 0; j < pubkeys.length; ) {  require(  _canStake(self, pubkeys[j], _verificationIndex),  \"SML:NOT all pubkeys are stakeable\"  );  unchecked {  j += 1;  bytes32 activeValKey = DSML.getKey(operatorId, rks.activeValidators);  bytes32 proposedValKey = DSML.getKey(operatorId, rks.proposedValidators);  uint256 poolId = self.validators[pubkeys[0]].poolId;  bytes memory withdrawalCredential = DATASTORE.readBytes(poolId, rks.withdrawalCredential);  uint256 lastIdChange = 0;  for (uint256 i = 0; i < pubkeys.length; ) {  uint256 newPoolId = self.validators[pubkeys[i]].poolId;  if (poolId != newPoolId) {  uint256 sinceLastIdChange;  unchecked {  sinceLastIdChange = i - lastIdChange;  DATASTORE.subUint(poolId, rks.secured, (DCL.DEPOSIT_AMOUNT * (sinceLastIdChange)));  DATASTORE.subUint(poolId, proposedValKey, (sinceLastIdChange));  DATASTORE.addUint(poolId, activeValKey, (sinceLastIdChange));  lastIdChange = i;  poolId = newPoolId;  withdrawalCredential = DATASTORE.readBytes(poolId, rks.withdrawalCredential);  DCL.depositValidator(  pubkeys[i],  withdrawalCredential,  self.validators[pubkeys[i]].signature31,  (DCL.DEPOSIT_AMOUNT - DCL.DEPOSIT_AMOUNT_PRESTAKE)  );  self.validators[pubkeys[i]].state = VALIDATOR_STATE.ACTIVE;  unchecked {  i += 1;  uint256 sinceLastIdChange;  unchecked {  sinceLastIdChange = pubkeys.length - lastIdChange;  if (sinceLastIdChange > 0) {  DATASTORE.subUint(poolId, rks.secured, DCL.DEPOSIT_AMOUNT * (sinceLastIdChange));  DATASTORE.subUint(poolId, proposedValKey, (sinceLastIdChange));  DATASTORE.addUint(poolId, activeValKey, (sinceLastIdChange));  _increaseWalletBalance(DATASTORE, operatorId, DCL.DEPOSIT_AMOUNT_PRESTAKE * pubkeys.length);  emit Stake(pubkeys);  This issue can later be escalated to a point where funds can be stolen. Consider the following case:  The attacker creates 10 validators directly through the ETH2 deposit contract using himself as the withdrawal address.  Attacker node operator proposes to add 10 validators and adds the 10ETH as pre-stake deposit. Since validators already exist withdrawal credentials will remain those of the Attacker. As a result of those actions, we have inflated the number of proposed validators the attacker has inside the Geode system.  Attacker then takes the validator keys proposed by someone else and stakes them. Since there is no check described above that is allowed. His proposed validators count will also decrease without a revert due to steps above.  As a result of that step, attacker will receive the pre-stake of the operator that actually proposed those validators. The attacker will immediately proceed to call decreaseWallet() to withdraw the funds.  The attacker will then withdraw the pre-stake he deposited in the initial validators with faulty withdrawal credential.  This way an attacker could profit 10ETH.  This can be prevented by making sure that validator s operatorId is checked on the stake() function call.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/05/geode-liquid-staking/"
    },
    {
        "title": "4.2 Cannot Blame Operator for Proposed Validator ",
        "body": "  In the current code, anyone can blame an operator who does not withdraw in time:  contracts/Portal/modules/StakeModule/libs/StakeModuleLib.sol:L931-L946  function blameOperator(  PooledStaking storage self,  DSML.IsolatedStorage storage DATASTORE,  bytes calldata pk  ) external {  require(  self.validators[pk].state == VALIDATOR_STATE.ACTIVE,  \"SML:validator is never activated\"  );  require(  block.timestamp > self.validators[pk].createdAt + self.validators[pk].period,  \"SML:validator is active\"  );  _imprison(DATASTORE, self.validators[pk].operatorId, pk);  There is one more scenario where the operator should be blamed. When a validator is in the PROPOSED state, only the operator can call the stake function to actually stake the rest of the funds. Before that, the funds of the pool will be locked under the rks.secured variable. So the malicious operator can lock up 31 ETH of the pool indefinitely by locking up only 1 ETH of the attacker. There is currently no way to release these 31 ETH.  We recommend introducing a mechanism that allows one to blame the operator for not staking for a long time after it was approved.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/05/geode-liquid-staking/"
    },
    {
        "title": "4.3 Validators Array Length Has to Be Updated When the Validator Is Alienated. ",
        "body": "  In GeodeFi when the node operator creates a validator with incorrect withdrawal credentials or signatures the Oracle has the ability to alienate this validator. In the process of alienation, the validator status is updated.  contracts/Portal/modules/StakeModule/libs/OracleExtensionLib.sol:L111-L136  function _alienateValidator(  SML.PooledStaking storage STAKE,  DSML.IsolatedStorage storage DATASTORE,  uint256 verificationIndex,  bytes calldata _pk  ) internal {  require(STAKE.validators[_pk].index <= verificationIndex, \"OEL:unexpected index\");  require(  STAKE.validators[_pk].state == VALIDATOR_STATE.PROPOSED,  \"OEL:NOT all pubkeys are pending\"  );  uint256 operatorId = STAKE.validators[_pk].operatorId;  SML._imprison(DATASTORE, operatorId, _pk);  uint256 poolId = STAKE.validators[_pk].poolId;  DATASTORE.subUint(poolId, rks.secured, DCL.DEPOSIT_AMOUNT);  DATASTORE.addUint(poolId, rks.surplus, DCL.DEPOSIT_AMOUNT);  DATASTORE.subUint(poolId, DSML.getKey(operatorId, rks.proposedValidators), 1);  DATASTORE.addUint(poolId, DSML.getKey(operatorId, rks.alienValidators), 1);  STAKE.validators[_pk].state = VALIDATOR_STATE.ALIENATED;  emit Alienated(_pk);  An additional thing that has to be done during the alienation process is that the validator s count should be decreased in order for the monopoly threshold to be calculated correctly. That is because the length of the validators array is used twice in the OpeartorAllowance function:  contracts/Portal/modules/StakeModule/libs/StakeModuleLib.sol:L975  uint256 numOperatorValidators = DATASTORE.readUint(operatorId, rks.validators);  contracts/Portal/modules/StakeModule/libs/StakeModuleLib.sol:L988  uint256 numPoolValidators = DATASTORE.readUint(poolId, rks.validators);  Without the update of the array length, the monopoly threshold as well as the time when the fallback operator will be able to participate is going to be computed incorrectly.  It could be beneficial to not refer to rks.validators in the operator allowance function and instead use the rks.proposedValidators + rks.alienatedValidators + rks.activeValidators. This way allowance function can always rely on the most up to date data.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/05/geode-liquid-staking/"
    },
    {
        "title": "4.4 A Potential Controller Update Issue. ",
        "body": "  We identified a potential issue in the code that is out of our current scope. In the GeodeModuleLib, there is a function that allows a controller of any ID to update the controller address:  contracts/Portal/modules/GeodeModule/libs/GeodeModuleLib.sol:L299-L309  function changeIdCONTROLLER(  DSML.IsolatedStorage storage DATASTORE,  uint256 id,  address newCONTROLLER  ) external onlyController(DATASTORE, id) {  require(newCONTROLLER != address(0), \"GML:CONTROLLER can not be zero\");  DATASTORE.writeAddress(id, rks.CONTROLLER, newCONTROLLER);  emit ControllerChanged(id, newCONTROLLER);  It s becoming tricky with the upgradability mechanism. The current version of any package is stored in the following format: DATASTORE.readAddress(versionId, rks. CONTROLLER). So the address of the current implementation of any package is stored as rks.CONTROLLER. That means if someone can hack the implementation address and make a transaction on its behalf to change the controller, this attacker can change the current implementation to a malicious one.  While this issue may not be exploitable now, many new packages are still to be implemented. So you need to ensure that nobody can get any control over the implementation contract.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/05/geode-liquid-staking/"
    },
    {
        "title": "4.5 The Price Change Limit Could Prevent the Setting of the Correct Price. ",
        "body": "  In the share price update logic of OracleExtensionLib, there is a function called   ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/05/geode-liquid-staking/"
    },
    {
        "title": "4.6 Potential for a Cross-Site-Scripting When Creating a Pool. ",
        "body": "  When creating a new staking pool, the creator has the ability to name it. While it does not present many issues on the chain, if this name is ever displayed on the UI it has to be handled carefully. An attacker could include a malicious script in the name and that could potentially be executed in the victim s browser.  contracts/Portal/modules/StakeModule/libs/StakeModuleLib.sol:L358  DATASTORE.writeBytes(poolId, rks.NAME, name);  We suggest that proper escaping is used when displaying the names of the pool on the UI. We do not recommend adding string validation on the chain.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/05/geode-liquid-staking/"
    },
    {
        "title": "4.1 TransactionManager - User might steal router s locked funds    ",
        "body": "  Resolution  This issue has been fixed.  Description  Recommendation  Consider using a data structure different than issuedShares for storing user deposits. This way, withdrawals by users will only be allowed when calling TransactionManager.cancel.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.2 TransactionManager - Receiver-side check also on sending side    Unverified Fix",
        "body": "  Resolution   The Connext team claims to have fixed this in commit   4adbfd52703441ee5de655130fc2e0252eae4661. We have not reviewed this commit or, generally, the codebase at this point.  Description  The functions prepare, cancel, and fulfill in the TransactionManager all have a  common part  that is executed on both the sending and the receiving chain and side-specific parts that are only executed either on the sending or on the receiving side. The following lines occur in fulfill s common part, but this should only be checked on the receiving chain. In fact, on the sending chain, we might even compare amounts of different assets.  code2/packages/contracts/contracts/TransactionManager.sol:L476-L478  // Sanity check: fee <= amount. Allow `=` in case of only wanting to execute  // 0-value crosschain tx, so only providing the fee amount  require(relayerFee <= txData.amount, \"#F:023\");  This could prevent a legitimate fulfill on the sending chain, causing a loss of funds for the router.  Recommendation  Move these lines to the receiving-side part.  Remark  The callData supplied to fulfill is not used at all on the sending chain, but the check whether its hash matches txData.callDataHash happens in the common part.  code2/packages/contracts/contracts/TransactionManager.sol:L480-L481  // Check provided callData matches stored hash  require(keccak256(callData) == txData.callDataHash, \"#F:024\");  In principle, this check could also be moved to the receiving-chain part, allowing the router to save some gas by calling sending-side fulfill with empty callData and skip the check. Note, however, that the TransactionFulfilled event will then also emit the  wrong  callData on the sending chain, so the off-chain code has to be able to deal with that if you want to employ this optimization.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.3 TransactionManager - Flawed shares arithmetic    ",
        "body": "  Resolution  Comment from Connext:  We removed the shares logic completely, and are instead only focusing on standard tokens (i.e. rebasing, inflationary, and deflationary tokens are not supported directly). If users want to transfer non-standard tokens, they do so at their own risk.  Description  To support a wide variety of tokens, the TransactionManager uses a per-asset shares system to represent fractional ownership of the contract s balance in a token. There are several flaws in the shares-related arithmetic, such as:  addLiquidity and sender-side prepare convert asset amounts 1:1 to shares, instead of taking the current value of a share into account. A 1:1 conversion is only appropriate if the number of shares is 0, for example, for the first deposit.  The WadRayMath library is not used correctly (and maybe not the ideal tool for the task in the first place): rayMul and rayDiv each operate on two rays (decimal numbers with 27 digits) but are not used according to that specification in getAmountFromIssuedShares and getIssuedSharesFromAmount. The scaling errors cancel each other out, though.  The WadRayMath library rounds to the nearest representable number. That might not be desirable for NXTP; for example, converting a token amount to shares and back to tokens might lead to a higher amount than we started with.  The WadRayMath library reverts on overflows, which might not be acceptable behavior. For instance, a receiver-side fulfill might fail due to an overflow in the conversion from shares to a token amount. The corresponding fulfill on the sending chain might very well succeed, though, and it is possible that, at a later point, the receiver-side transaction can be canceled. (Note that canceling does not involve actually converting shares into a token amount, but the calculation is done anyway for the event.)  The amount emitted in the TransactionPrepared event on the receiving chain can, depending on the granularity of the shares, differ considerably from what a user can expect to receive when the shares are converted back into tokens. The reason for this is the double conversion from the initial token amount \u2014 which is emitted \u2014 to shares and, later, back to tokens.  Special cases might have to be taken into account. As a more subtle example, converting a non-zero token amount to shares is not possible (or at least not with the usual semantics) if the contract s balance is zero, but the number of already existing shares is strictly greater than zero, as any number of shares will give you back less than the original amount. Whether this situation is possible depends on the token contract.  Recommendation  The shares logic was added late to the contract and is still in a pretty rough shape. While providing a full-fledged solution is beyond the scope of this review, we hope that the points raised above provide pointers and guidelines to inform a major overhaul.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.4 Router - handleMetaTxRequest - gas griefing / race conditions / missing validations / free meta transactions ",
        "body": "  Description  There s a comment in handleMetaTxRequest that asks whether data needs to be validated before interacting with the contract and the answer is yes, always, or else this opens up a gas griefing vector on the router side.  For example, someone might flood broadcast masses of metaTx requests (*.*.metatx) and all online routers will race to call TransactionManager.fulfill(). Even if only one transaction should be able to successfully go through all the others will loose on gas (until up to the first require failing).  Given that there is no rate limiting and it is a broadcast that is very cheap to perform on the client-side (I can just spawn a lot of nodes spamming messages) this can be very severe, keeping the router busy sending transactions that are deemed to fail until the routers balance falls below the min gas limit configured.  Even if the router would check the contracts current state first (performing read-only calls that can be done offline) to check if the transaction has a chance to succeed, it will still compete in a race for the current block (mempool).  Examples  code/packages/router/src/handler.ts:L459-L477  const fulfillData: MetaTxFulfillPayload = data.data;  // Validate that metatx request matches with known data about fulfill  // Is this needed? Can we just submit to chain without validating?  // Technically this is ok, but perhaps we want to validate only for our own  // logging purposes.  // Would also be bad if router had no gas here  // Next, prepare the tx object  // - Get chainId from data  // - Get fulfill fee from data and validate it covers gas  // - etc.  // Send to txService  // Update metrics  // TODO: make sure fee is something we want to accept  this.logger.info({ method, methodId, requestContext, chainId, data }, \"Submitting tx\");  const res = await this.txManager  .fulfill(  chainId,  Recommendation  For state-changing transactions that actually cost gas there is no way around implementing strict validation whenever possible and avoid performing the transaction in case validation fails. Contract state should always be validated before issuing new online transactions but this might not fix the problem that routers still compete for their transaction to be included in the next block (mempool not monitored). The question therefore is, whether it would be better to change the metaTX flow to have a router confirm that they will send the tx via the messaging service first so others know they do not even have to try to send it. However, even this scenario may allow to DoS the system by maliciously responding with such a method.  In general, there re a lot of ways to craft a message that forces the router to issue an on-chain transaction that may fail with no consequences for the sender of the metaTx message.  Additionally, the relayerFee is currently unchecked which may lead to the router loosing funds because they effectively accept zero-fee relays.  As noted in issue 4.6, the missing return after detecting that the metatx is destined for a TransactionManager that is not supported allows for explicit gas griefing attacks (deploy a fake TransactionManager.fulfill that mines all the gas for a beneficiary).  The contract methods should additionally validate that the sender balance can cover for the gas required to send the transaction.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.5 Router - subgraphLoop may process transactions the router was not configured for (code fragility) ",
        "body": "  Description  subgraphLoop gets all sending transactions for the router, chain, status triplet.  code/packages/router/src/subgraph.ts:L155-L159  allSenderPrepared = await sdk.GetSenderTransactions({  routerId: this.routerAddress.toLowerCase(),  sendingChainId: chainId,  status: TransactionStatus.Prepared,  });  and then sorts the results by receiving chain id. Note that this keeps track of chainID s the router was not configured for.  code/packages/router/src/subgraph.ts:L168-L176  // create list of txIds for each receiving chain  const receivingChains: Record<string, string[]> = {};  allSenderPrepared.router?.transactions.forEach(({ transactionId, receivingChainId }) => {  if (receivingChains[receivingChainId]) {  receivingChains[receivingChainId].push(transactionId);  } else {  receivingChains[receivingChainId] = [transactionId];  });  In a next step, transactions are resolved from the various chains. This filters out chainID s the router was not configured for (and just returns an empty array), however, the GetTransactions query assumes that transactionID s are unique across the subgraph which might not be true!  code/packages/router/src/subgraph.ts:L179-L193  let correspondingReceiverTxs: any[];  try {  const queries = await Promise.all(  Object.entries(receivingChains).map(async ([cId, txIds]) => {  const _sdk = this.sdks[Number(cId)];  if (!_sdk) {  this.logger.error({ chainId: cId, method, methodId }, \"No config for chain, this should not happen\");  return [];  const query = await _sdk.GetTransactions({ transactionIds: txIds.map((t) => t.toLowerCase()) });  return query.transactions;  }),  );  correspondingReceiverTxs = queries.flat();  } catch (err) {  In the last step, all chainID s (even the one s the router was not configured for) are iterated again (which might be unnecessary). TransactionID s are loosely matched from the previously flattened results from all the various chains. Since transactionID s don t necessarily need to be unique across chains or within the chain, it is likely that the subsequent matching of transactionID s (correspondingReceiverTxs.find) returns more than 1 entry. However, find() just returns the first item and covers up the fact that there might be multiple matches. Also, since the code returned an empty array for chains it was not configured for, the find will return undef satisfying the !corresponding branch and fire an SenderTransactionPrepared triggering the handler to perform an on-chain action that will most definitely fail at some point.  Recommendation  The code in this module is generally very fragile. It is based on assumptions that can likely be exploited by a third party re-using transactionID s (or other values). It is highly recommended to rework the code making it more resilient to potential corner cases.  Filter receivingChains for chainID s that are not supported by the router  Avoid having to integrating the allSenderPrepared array twice and use a filtered list instead  Change the very broad query in _sdk.GetTransactions() that assumes transactionID s are unique across all chains to a specific query that selects transactions specific to the chain and this router. The more specific the better!  When matching the transactions also match the source/receiver chains instead of only matching the transactionID. Additionally, check if more than one entry matches the condition instead of silently taking the first result (this is what array.find() does)  Also see issue 5.2  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.6 Router - handler reports an error condition but continues execution instead of aborting it ",
        "body": "  Description  There are some code paths that detect and log an error but then continue the execution flow instead of returning the error condition to the caller. This may allow for a variety of griefing vectors (e.g. gas griefing).  Examples  reports an error because the received address does not match our configured transaction manager, but then proceeds. This means the router would accept a transaction manager it was not configured for.  code/packages/router/src/handler.ts:L448-L458  if (utils.getAddress(data.to) !== utils.getAddress(chainConfig.transactionManagerAddress)) {  const err = new HandlerError(HandlerError.reasons.ConfigError, {  requestContext,  calling: \"chainConfig.transactionManagerAddress\",  methodId,  method,  configError: `Provided transactionManagerAddress does not map to our configured transactionManagerAddress`,  });  this.logger.error({ method, methodId, requestContext, err: err.toJson() }, \"Error in config\");  If chainConfig is undef this should return or else it will bail later  code/packages/router/src/handler.ts:L436-L445  if (!chainConfig) {  const err = new HandlerError(HandlerError.reasons.ConfigError, {  requestContext,  calling: \"getConfig\",  methodId,  method,  configError: `No chainConfig for ${chainId}`,  });  this.logger.error({ method, methodId, requestContext, err: err.toJson() }, \"Error in config\");  if data is not fulfill this silently returns, while it should probably raise an error instead (unexpected message)  code/packages/router/src/handler.ts:L447-L447  if (data.type === \"Fulfill\") {  Recommendation  Implement strict validation of untrusted data.  Be explicit and raise error conditions on unexpected messages (e.g. type is not fulfill) instead of silently skipping the message.  Add the missing returns after reporting an error instead of continuing the execution flow on errors.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.7 Router - spawns unauthenticated admin API endpoint listening on all interfaces ",
        "body": "  Description  unauthenticated  listening on allips  pot. allows any local or remote unpriv user with access to the endpoint to steal the routers liquidity /remove-liquidity -> req.body.recipientAddress  Examples  code/packages/router/src/index.ts:L123-L130  server.listen(8080, \"0.0.0.0\", (err, address) => {  if (err) {  console.error(err);  process.exit(1);  console.log(`Server listening at ${address}`);  });  Recommendation  require authentication  should only bind to localhost by default  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.8 TODO comments should be resolved ",
        "body": "  Description  As part of the process of bringing the application to production readiness, dev comments (especially TODOs) should be resolved. In many cases, these comments indicate a missing functionality that should be implemented, or some missing necessary validation checks.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.9 TransactionManager - Missing nonReentrant modifier on removeLiquidity    ",
        "body": "  Resolution  This issue has been fixed.  Description  The removeLiquidity function does not have a nonReentrant modifier.  code/packages/contracts/contracts/TransactionManager.sol:L274-L329  /**  @notice This is used by any router to decrease their available  liquidity for a given asset.  @param shares The amount of liquidity to remove for the router in shares  @param assetId The address (or `address(0)` if native asset) of the  asset you're removing liquidity for  @param recipient The address that will receive the liquidity being removed  /  function removeLiquidity(  uint256 shares,  address assetId,  address payable recipient  ) external override {  // Sanity check: recipient is sensible  require(recipient != address(0), \"#RL:007\");  // Sanity check: nonzero shares  require(shares > 0, \"#RL:035\");  // Get stored router shares  uint256 routerShares = issuedShares[msg.sender][assetId];  // Get stored outstanding shares  uint256 outstanding = outstandingShares[assetId];  // Sanity check: owns enough shares  require(routerShares >= shares, \"#RL:018\");  // Convert shares to amount  uint256 amount = getAmountFromIssuedShares(  shares,  outstanding,  Asset.getOwnBalance(assetId)  );  // Update router issued shares  // NOTE: unchecked due to require above  unchecked {  issuedShares[msg.sender][assetId] = routerShares - shares;  // Update the total shares for asset  outstandingShares[assetId] = outstanding - shares;  // Transfer from contract to specified recipient  Asset.transferAsset(assetId, recipient, amount);  // Emit event  emit LiquidityRemoved(  msg.sender,  assetId,  shares,  amount,  recipient  );  Assuming we re dealing with a token contract that allows execution of third-party-supplied code, that means it is possible to leave the TransactionManager contract in one of the functions that call into the token contract and then reenter via removeLiquidity. Alternatively, we can leave the contract in removeLiquidity and reenter through an arbitrary external function, even if it has a nonReentrant modifier.  Example  Recommendation  While tokens that behave as described in the example might be rare or not exist at all, caution is advised when integrating with unknown tokens or calling untrusted code in general. We strongly recommend adding a nonReentrant modifier to removeLiquidity.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.10 TransactionManager - Relayer may use user s cancel after expiry signature to steal user s funds by colluding with a router   ",
        "body": "  Resolution   This has been acknowledged by the Connext team. As discussed below in the  Recommendation , it is not a flaw in the contracts   Description  Users that are willing to have a lower trust dependency on a relayer should have the ability to opt-in only for the service that allows the relayer to withdraw back users  funds from the sending chain after expiry. However, in practice, a user is forced to opt-in for the service that refunds the router before the expiry, since the same signature is used for both services (lines 795,817 use the same signature).  Let s consider the case of a user willing to call fulfill on his own, but to use the relayer only to withdraw back his funds from the sending chain after expiry. In this case, the relayer can collude with the router and use the user s cancel signature (meant for withdrawing his only after expiry) as a front-running transaction for a user call to fulfill. This way the router will be able to withdraw both his funds and the user s funds since the user s fulfill signature is now public data residing in the mem-pool.  Examples  code/packages/contracts/contracts/TransactionManager.sol:L795-L817  require(msg.sender == txData.user || recoverSignature(txData.transactionId, relayerFee, \"cancel\", signature) == txData.user, \"#C:022\");  Asset.transferAsset(txData.sendingAssetId, payable(msg.sender), relayerFee);  // Get the amount to refund the user  uint256 toRefund;  unchecked {  toRefund = amount - relayerFee;  // Return locked funds to sending chain fallback  if (toRefund > 0) {  Asset.transferAsset(txData.sendingAssetId, payable(txData.sendingChainFallback), toRefund);  } else {  // Receiver side, router liquidity is returned  if (txData.expiry >= block.timestamp) {  // Timeout has not expired and tx may only be cancelled by user  // Validate signature  require(msg.sender == txData.user || recoverSignature(txData.transactionId, relayerFee, \"cancel\", signature) == txData.user, \"#C:022\");  Recommendation  The crucial point here is that the user must never sign a  cancel  that could be used on the receiving chain while fulfillment on the sending chain is still a possibility. Or, to put it differently: A user may only sign a  cancel  that is valid on the receiving chain after sending-chain expiry or if they never have and won t ever sign a  fulfill  (or at least won t sign until sending-chain expiry \u2014 but it is pointless to sign a  fulfill  after that, so  never  is a reasonable simplification). Or, finally, a more symmetric perspective on this requirement: If a user has signed  fulfill , they must not sign a receiving-chain-valid  cancel  until sending-chain expiry, and if they have signed a receiving-chain-valid  cancel , they must not sign a  fulfill  (until sending-chain expiry).  In this sense,  cancel  signatures that are valid on the receiving chain are dangerous, while sending-side cancellations are not. So the principle stated in the previous paragraph might be easier to follow with different signatures for sending- and receiving-chain cancellations.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.11 Router - handleSenderPrepare - missing validation, unchecked bidExpiry, unchecked expiry, unchecked chainids/swaps, race conidtions ",
        "body": "  Description  This finding highlights a collection of issues with the handleSenderPrepare method. The code and coding style appears fragile. Validation should be strictly enforced and protective measures against potential race conditions should be implemented.  The following list highlights individual findings that contribute risk and therefore broaden the attack surface of this method:  unchecked bidExpiry might allow using bids even after expiration.  code/packages/router/src/handler.ts:L612-L626  .andThen(() => {  // TODO: anything else? seems unnecessary to validate everything  if (!BigNumber.from(bid.amount).eq(amount) || bid.transactionId !== txData.transactionId) {  return err(  new HandlerError(HandlerError.reasons.PrepareValidationError, {  method,  methodId,  calling: \"\",  requestContext,  prepareError: \"Bid params not equal to tx data\",  }),  );  return ok(undefined);  });  unchecked txdata.expiry might lead to router preparing for an already expired prepare. However, this is rather unlikely easily exploitable as the data source is a subgraph.  a bid might not be fulfillable anymore due to changes to the router (e.g. removing a chainconfig or assets) but the router would still attempt it. Make sure to always verify chainid/assets/the configured system parameters.  potential race condition. make sure to lock the txID in the beginning.  code/packages/router/src/handler.ts:L663-L669  // encode the data for contract call  // Send to txService  this.receiverPreparing.set(txData.transactionId, true);  this.logger.info(  { method, methodId, requestContext, transactionId: txData.transactionId },  \"Sending receiver prepare tx\",  );  Note that transactionID s as they are used in the system must be unique across chains.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.12 Router - handleNewAuction - fragile code ",
        "body": "  Description  This finding highlights a collection of issues with the handleNewAuction. The code and coding style appears fragile. Validation should be strictly enforced, debugging code should be removed or disabled in production and protective measures should be taken from abusive clients.  The following list highlights individual findings that contribute risk and therefore broaden the attack surface of this method:  router bids on zero-amount requests (this will fail later when calling the contract, thus a potential gas griefing attack vector)  code/packages/router/src/handler.ts:L197-L201  // validate that assets/chains are supported and there is enough liquidity  // and gas on both sender and receiver side.  // TODO: will need to track this offchain  const amountReceived = mutateAmount(amount);  duplicate constant var assignment (subfunction const shadowing and unchecked initial config!)  code/packages/router/src/handler.ts:L202-L204  const config = getConfig();  const sendingConfig = config.chainConfig[sendingChainId];  const receivingConfig = config.chainConfig[receivingChainId];  code/packages/router/src/handler.ts:L231-L240  // validate config  const config = getConfig();  const sendingConfig = config.chainConfig[sendingChainId];  const receivingConfig = config.chainConfig[receivingChainId];  if (  !sendingConfig.providers ||  sendingConfig.providers.length === 0 ||  !receivingConfig.providers ||  receivingConfig.providers.length === 0  ) {  actual estimated gas required to fuel transaction is never checked. current balance might be outdated, especially in race condition scenarios.  code/packages/router/src/handler.ts:L315-L318  .andThen((balances) => {  const [senderBalance, receiverBalance] = balances as BigNumber[];  if (senderBalance.lt(sendingConfig.minGas) || receiverBalance.lt(receivingConfig.minGas)) {  return errAsync(  remove debug code from production build (dry-run)  code/packages/router/src/handler.ts:L194-L194  dryRun,  code/packages/router/src/handler.ts:L385-L385  this.messagingService.publishAuctionResponse(inbox, { bid, bidSignature: dryRun ? undefined : bidSignature }),  signer address might be different for different chains  code/packages/router/src/handler.ts:L290-L312  return combine([  ResultAsync.fromPromise(  this.txService.getBalance(sendingChainId, this.signer.address),  (err) =>  new HandlerError(HandlerError.reasons.TxServiceError, {  calling: \"txService.getBalance => sending\",  method,  methodId,  requestContext,  txServiceError: jsonifyError(err as NxtpError),  }),  ),  ResultAsync.fromPromise(  this.txService.getBalance(receivingChainId, this.signer.address),  (err) =>  new HandlerError(HandlerError.reasons.TxServiceError, {  calling: \"txService.getBalance => receiving\",  method,  methodId,  requestContext,  txServiceError: jsonifyError(err as NxtpError),  }),  ),  no rate limiting. potential DoS vector when someone floods the node with auction requests (significant work to be done, handler is async, will trigger a reply message). user might force the router to sign the same message multiple times.  missing validation of bid parameters (expiriy within valid range, \u2026)  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.13 Router - Cancel is not implemented ",
        "body": "  Description  Canceling of failed/expired swaps does not seem to be implemented in the router. This may allow a user to trick the router into preparing all its funds which will not automatically be reclaimed after expiration (router DoS).  Examples  cancelExpired is never called  code/packages/sdk/src/sdk.ts:L873-L885  // TODO: this just cancels a transaction, it is misnamed, has nothing to do with expiries  public async cancelExpired(cancelParams: CancelParams, chainId: number): Promise<providers.TransactionResponse> {  const method = this.cancelExpired.name;  const methodId = getRandomBytes32();  this.logger.info({ method, methodId, cancelParams, chainId }, \"Method started\");  const cancelRes = await this.transactionManager.cancel(chainId, cancelParams);  if (cancelRes.isOk()) {  this.logger.info({ method, methodId }, \"Method complete\");  return cancelRes.value;  } else {  throw cancelRes.error;  disabled code  code/packages/router/src/handler.ts:L719-L733  \"Do not cancel ATM, figure out why we are in this case first\",  );  // const cancelRes = await this.txManager.cancel(txData.sendingChainId, {  //   txData,  //   signature: \"0x\",  //   relayerFee: \"0\",  // });  // if (cancelRes.isOk()) {  //   this.logger.warn(  //     { method, methodId, transactionHash: cancelRes.value.transactionHash },  //     \"Cancelled transaction\",  //   );  // } else {  //   this.logger.error({ method, methodId }, \"Could not cancel transaction after error!\");  // }  Recommendation  Implement the cancel flow.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.14 TransactionManager.prepare - Possible griefing/denial of service by front-running    Unverified Fix",
        "body": "  Resolution  Comment from Connext:  We see this as a highly unlikely attack vector and have chosen not to mitigate it, but it is possible. Users can always and easily generate a new key prepare from a new account, and performing this attack will always cost gas and some dust amount. Further, adding in the suggested require(msg.sender == invariantData.user) will lock out many contract-based use cases and requiring an additional signature/user interaction (auth, approve, prepare, fulfill) is not desirable.  The Connext team claims to have implemented this solution in commit 6811bb2681f44f34ce28906cb842db49fb73d797. We have not reviewed this commit or, generally, the codebase at this point.  Description  A call to TransactionManager.prepare might be front-run with a transaction using the same invariantData but with a different amount and/or expiry values. By choosing a tiny amount of assets, the attacker may prevent the user from locking his original desired amount. The attacker can repeat this process for any new transactionId presented by the user, thus effectively denying the service for him.  Recommendation  Consider adding a require(msg.sender == invariantData.user) restriction to TransactionManager.prepare.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.15 Router - Provide and enforce safe defaults (config) ",
        "body": "  Description  Chain confirmations default to 1 which is not safe. In case of a re-org the router might (temporarily) get out of sync with the chain and perform actions it should not perform. This may put funds at risk.  Examples  the schema requires an unsafe minimum of 1 confirmation  code/packages/router/src/config.ts:L33-L36  export const TChainConfig = Type.Object({  providers: Type.Array(Type.String()),  confirmations: Type.Number({ minimum: 1 }),  subgraph: Type.String(),  the default configuration uses 1 confirmation  code/packages/router/config.json.example:L1-L17  \"adminToken\": \"blahblah\",  \"chainConfig\": {  \"4\": {  \"providers\": [\"https://rinkeby.infura.io/v3/\"],  \"confirmations\": 1,  \"subgraph\": \"https://api.thegraph.com/subgraphs/name/connext/nxtp-rinkeby\"  },  \"5\": {  \"providers\": [\"https://goerli.infura.io/v3/\"],  \"confirmations\": 1,  \"subgraph\": \"https://api.thegraph.com/subgraphs/name/connext/nxtp-goerli\"  },  \"logLevel\": \"info\",  \"mnemonic\": \"candy maple cake sugar pudding cream honey rich smooth crumble sweet treat\"  Recommendation  Give guidance, provide and enforce safe defaults.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.16 ProposedOwnable - two-step ownership transfer should be confirmed by the new owner    ",
        "body": "  Resolution  All recommendations given below have been implemented. In addition to that, the privilege to manage assets and the privilege to manage routers can now be renounced separately.  Description  In order to avoid losing control of the contract, the two-step ownership transfer should be confirmed by the new owner s address instead of the current owner.  Examples  acceptProposedOwner is restricted to onlyOwner while ownership should be accepted by the newOwner  code/packages/contracts/contracts/ProposedOwnable.sol:L89-L96  /**  @notice Transfers ownership of the contract to a new account (`newOwner`).  Can only be called by the current owner.  /  function acceptProposedOwner() public virtual onlyOwner {  require((block.timestamp - _proposedTimestamp) > _delay, \"#APO:030\");  _setOwner(_proposed);  move renounced() to ProposedOwnable as this is where it logically belongs to  code/packages/contracts/contracts/TransactionManager.sol:L160-L162  function renounced() public view override returns (bool) {  return owner() == address(0);  onlyOwner can directly access state-var _owner instead of spending more gas on calling owner()  code/packages/contracts/contracts/ProposedOwnable.sol:L76-L79  modifier onlyOwner() {  require(owner() == msg.sender, \"#OO:029\");  _;  Recommendation  onlyOwner can directly access _owner (gas optimization)  add a method to explicitly renounce ownership of the contract  move TransactionManager.renounced() to ProposedOwnable as this is where it logically belongs to  change the access control for acceptProposedOwner from onlyOwner to require(msg.sender == _proposed) (new owner).  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.17 FulfillInterpreter - Wrong order of actions in fallback handling ",
        "body": "  Description  code2/packages/contracts/contracts/interpreters/FulfillInterpreter.sol:L68-L90  bool isNative = LibAsset.isNativeAsset(assetId);  if (!isNative) {  LibAsset.increaseERC20Allowance(assetId, callTo, amount);  // Check if the callTo is a contract  bool success;  bytes memory returnData;  if (Address.isContract(callTo)) {  // Try to execute the callData  // the low level call will return `false` if its execution reverts  (success, returnData) = callTo.call{value: isNative ? amount : 0}(callData);  // Handle failure cases  if (!success) {  // If it fails, transfer to fallback  LibAsset.transferAsset(assetId, fallbackAddress, amount);  // Decrease allowance  if (!isNative) {  LibAsset.decreaseERC20Allowance(assetId, callTo, amount);  For the fallback scenario, i.e., the call isn t executed or fails, the funds are first transferred to fallbackAddress, and the previously increased allowance is decreased after that. If the token supports it, the recipient of the direct transfer could try to exploit that the approval hasn t been revoked yet, so the logically correct order is to decrease the allowance first and transfer the funds later. However, it should be noted that the FulfillInterpreter should, at any point in time, only hold the funds that are supposed to be transferred as part of the current transaction; if there are any excess funds, these are leftovers from a previous failure to withdraw everything that could have been withdrawn, so these can be considered up for grabs. Hence, this is only a minor issue.  Recommendation  We recommend reversing the order of actions for the fallback case: Decrease the allowance first, and transfer later. Moreover, it would be better to increase the allowance only in case a call will actually be made, i.e., if Address.isContract(callTo) is true.  Remark  This issue was already present in the original version of the code but was missed initially and only found during the re-audit.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.18 FulfillInterpreter - Executed event can t be linked to TransactionFulfilled event    ",
        "body": "  Resolution  This issue has been fixed. We d like to point out, though:  Based on the data emitted by the TransactionFulfilled event, it is currently not possible to distinguish between: (A) No call to callTo has been made because the address didn t contain code. (B) Address callTo did contain code, a call was made, and it failed with empty return data. If this distinction seems relevant, an additional bool should be returned from FulfillInterpreter.execute and emitted in TransactionFulfilled, indicating which of the two scenarios were encountered.  The Executed event isn t needed anymore and could be removed.  Description  While it is in the user s best interest not to reuse a transactionId they have used before, unique transaction IDs are not enforced, and a user seeking to wreak havoc might choose to reuse an ID if it helps them accomplish their goal. In this case, event-monitoring software might get confused by several Executed events with the same transactionId and not be able to match the event with its TransactionFulfilled counterpart.  Recommendation  Generally, the following rules apply to transaction IDs:  A user must, in their own best interest, never reuse a transactionId they have used before \u2014 not even across different chains and no matter whether the transaction was successful or not.  This per-user uniqueness of transaction IDs is not enforced, though \u2014 not even per TransactionManager deployment. Hence, the code may not rely on this assumption, and no harm must come from a reused transaction ID for the system or anyone else than the user who reused the ID.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.19 Sdk.finishTransfer - missing validation ",
        "body": "  Description  Sdk.finishTransfer should validate that the router that locks liquidity in the receiving chain, should be the same router the user had committed to in the sending chain.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.20 FulfillInterpreter - Missing check whether callTo address contains code    ",
        "body": "  Resolution  This issue has been fixed.  Description  The receiver-side prepare checks whether the callTo address is either zero or a contract:  code/packages/contracts/contracts/TransactionManager.sol:L466-L470  // Check that the callTo is a contract  // NOTE: This cannot happen on the sending chain (different chain  // contexts), so a user could mistakenly create a transfer that must be  // cancelled if this is incorrect  require(invariantData.callTo == address(0) || Address.isContract(invariantData.callTo), \"#P:031\");  However, as a contract may selfdestruct and the check is not repeated later, there is no guarantee that callTo still contains code when the call to this address (assuming it is non-zero) is actually executed in FulfillInterpreter.execute:  code/packages/contracts/contracts/interpreters/FulfillInterpreter.sol:L71-L82  // Try to execute the callData  // the low level call will return `false` if its execution reverts  (bool success, bytes memory returnData) = callTo.call{value: isEther ? amount : 0}(callData);  if (!success) {  // If it fails, transfer to fallback  Asset.transferAsset(assetId, fallbackAddress, amount);  // Decrease allowance  if (!isEther) {  Asset.decreaseERC20Allowance(assetId, callTo, amount);  As a result, if the contract at callTo self-destructs between prepare and fulfill (both on the receiving chain), success will be true, and the funds will probably be lost to the user.  A user could currently try to avoid this by checking that the contract still exists before calling fulfill on the receiving chain, but even then, they might get front-run by selfdestruct, and the situation is even worse with a relayer, so this provides no reliable protection.  Recommendation  Repeat the Address.isContract check on callTo before making the external call in FulfillInterpreter.execute and send the funds to the fallbackAddress if the result is false.  Remark  It should be noted that an unsuccessful call, i.e., a revert, is the only behavior that is recognized by FulfillInterpreter.execute as failure. While it is prevalent to indicate failure by reverting, this doesn t have to be the case; a well-known example is an ERC20 token that indicates a failing transfer by returning false. A user who wants to utilize this feature has to make sure that the called contract behaves accordingly; if that is not the case, an intermediary contract may be employed, which, for example, reverts for return value false.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.21 TransactionManager - Adherence to EIP-712   ",
        "body": "  Resolution  Comment from Connext:  We did not fully adopt EIP712 because hardware wallet support is still not universal. Additionally, we chose not to address this issue in the recommended fashion (using address(this), block.chainId) because the fulfill signature must be usable across both the sending and receiving chain. Instead, we made sure the transactionManagerReceivingAddress, receivingChainId was signed.  We advise users of the system not to use their key and address for other systems that operate with signed messages unless they can rule out the possibility of replay attacks. Regarding the signed receivingChainId and receivingChainTxManagerAddress, we d like to mention that even for receiver-side fulfillment, these are not verified against the current chain ID and address of the contract.  Description  fulfill function requires the user signature on a transactionId. While currently, the user SDK code is using a cryptographically secured pseudo-random function to generate the transactionId, it should not be counted upon and measures should be placed on the smart-contract level to ensure replay-attack protection.  Examples  code/packages/contracts/contracts/TransactionManager.sol:L918-L933  function recoverSignature(  bytes32 transactionId,  uint256 relayerFee,  string memory functionIdentifier,  bytes calldata signature  ) internal pure returns (address) {  // Create the signed payload  SignedData memory payload = SignedData({  transactionId: transactionId,  relayerFee: relayerFee,  functionIdentifier: functionIdentifier  });  // Recover  return ECDSA.recover(ECDSA.toEthSignedMessageHash(keccak256(abi.encode(payload))), signature);  Recommendation  Consider adhering to EIP-712, or at least including address(this), block.chainId as part of the data signed by the user.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.22 TransactionManager - Hard-coded chain ID might lead to problems after a chain split   Pending",
        "body": "  Resolution  The recommendation below has been implemented, but the current codebase doesn t handle chain splits correctly. On the chain that gets a new chain ID, funds may be lost or frozen.  More specifically, after a chain split, we may find ourselves in the situation that the current chain ID is neither the sendingChainId nor the receivingChainId stored in the invariant transaction data. If that is the case, we re on the chain that got a new chain ID. fulfill should always revert in this situation, but cancellation should be possible to release locked funds. We don t know, however, whether we should send the funds back to the user (that is, we re on a fork of the sending chain) or whether they should be given back to the router (that is, we re on a fork of the receiving chain). Our recommendation to solve this is to store in the variant transaction data explicitly whether this is the sending chain or the receiving chain; with this information, we can disambiguate the situation and implement cancel correctly.  Description  The ID of the chain on which the contract is deployed is supplied as a constructor argument and stored as an immutable state variable:  code/packages/contracts/contracts/TransactionManager.sol:L104-L107  /**  @dev The chain id of the contract, is passed in to avoid any evm issues  /  uint256 public immutable chainId;  code/packages/contracts/contracts/TransactionManager.sol:L125-L128  constructor(uint256 _chainId) {  chainId = _chainId;  interpreter = new FulfillInterpreter(address(this));  Hence, chainId can never change, and even after a chain split, both contracts would continue to use the same chain ID. That can have undesirable consequences. For example, a transaction that was prepared before the split could be fulfilled on both chains.  Recommendation  It would be better to query the chain ID directly from the chain via block.chainId. However, the development team informed us that they had encountered problems with this approach as some chains apparently are not implementing this correctly. They resorted to the method described above, a constructor-supplied, hard-coded value. For chains that do indeed not inform correctly about their chain ID, this is a reasonable solution. However, for the reasons outlined above, we still recommend querying the chain ID via block.chainId for chains that do support that \u2014 which should be the vast majority \u2014 and using the fallback mechanism only when necessary.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.23 Router - handling of native assetID (0x000..00, e.g. ETH) not implemented ",
        "body": "  Description  Additionally, handleSenderPrepare does not manage approvals for ERC20 transfers.  Examples  harcoded zero amount  code/packages/router/src/contract.ts:L137-L147  return ResultAsync.fromPromise(  this.txService.sendTx(  to: this.config.chainConfig[chainId].transactionManagerAddress,  data: encodedData,  value: constants.Zero,  chainId,  from: this.signerAddress,  },  requestContext,  ),  code/packages/router/src/contract.ts:L206-L215  this.txService.sendTx(  chainId,  data: fulfillData,  to: nxtpContractAddress,  value: 0,  from: this.signerAddress,  },  requestContext,  ),  approveTokensIfNeeded will fail when using native assets  code/packages/sdk/src/transactionManager.ts:L329-L333  ).andThen((signerAddress) => {  const erc20 = new Contract(  assetId,  ERC20.abi,  this.signer.provider ? this.signer : this.signer.connect(config.provider),  Recommendation  Remove complexity by requiring ERC20 compliant wrapped native assets (e.g.WETH instead of native ETH).  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.24 Router - config file is missing the swapPools attribute and credentials are leaked to console in case of invalid config ",
        "body": "  Description  Node startup fails due to missing swapPools configuration in config.json.example. Confidential secrets are leaked to console in the event that the config file is invalid.  Examples  yarn workspace @connext/nxtp-router dev  [app] [nodemon] 2.0.12  [app] [nodemon] to restart at any time, enter `rs`  [app] [nodemon] watching path(s): .env dist/**/* ../@connext/nxtp-txservice/dist ../@connext/nxtp-contracts/dist ../@connext/nxtp-utils/dist  [app] [nodemon] watching extensions: js,json  [app] [nodemon] starting `node --enable-source-maps ./dist/index.js | pino-pretty`  [tsc]  [tsc] 13:52:29 - Starting compilation in watch mode...  [tsc]  [tsc]  [tsc] 13:52:29 - Found 0 errors. Watching for file changes.  [app] Found configFile  [app] Invalid config: {  [app]   \"mnemonic\": \"candy maple cake sugar pudding cream honey rich smooth crumble sweet treat\",  [app]   \"authUrl\": \"https://auth.connext.network\",  [app]   \"natsUrl\": \"nats://nats1.connext.provide.network:4222,nats://nats2.connext.provide.network:4222,nats://nats3.connext.provide.network:4222\",  [app]   \"adminToken\": \"blahblah\",  [app]   \"chainConfig\": {  [app]     \"4\": {  [app]       \"providers\": [  [app]         \"https://rinkeby.infura.io/v3/\"  [app]       ],  [app]       \"confirmations\": 1,  [app]       \"subgraph\": \"https://api.thegraph.com/subgraphs/name/connext/nxtp-rinkeby\",  [app]       \"transactionManagerAddress\": \"0x29E81453AAe28A63aE12c7ED7b3F8BC16629A4Fd\",  [app]       \"minGas\": \"100000000000000000\"  [app]     },  [app]     \"5\": {  [app]       \"providers\": [  [app]         \"https://goerli.infura.io/v3/\"  [app]       ],  [app]       \"confirmations\": 1,  [app]       \"subgraph\": \"https://api.thegraph.com/subgraphs/name/connext/nxtp-goerli\",  [app]       \"transactionManagerAddress\": \"0xbF0F4f639cDd010F38CeBEd546783BD71c9e5Ea0\",  [app]       \"minGas\": \"100000000000000000\"  [app]     }  [app]   },  [app]   \"logLevel\": \"info\"  [app] }  [app] Error: must have required property 'swapPools'  [app]     at Object.getEnvConfig (code/packages/router/dist/config.js:135:15)  [app]         -> code/packages/router/src/config.ts:145:11  [app]     at Object.getConfig (code/packages/router/dist/config.js:149:30)  [app]         -> code/packages/router/src/config.ts:161:18  [app]     at Object.<anonymous> (code/packages/router/dist/index.js:19:25)  [app]         -> code/packages/router/src/index.ts:23:16  [app]     at Module._compile (internal/modules/cjs/loader.js:1063:30)  [app]     at Object.Module._extensions..js (internal/modules/cjs/loader.js:1092:10)  [app]     at Module.load (internal/modules/cjs/loader.js:928:32)  [app]     at Function.Module._load (internal/modules/cjs/loader.js:763:16)  [app]     at Function.executeUserEntryPoint [as runMain] (internal/modules/run_main.js:72:12)  [app]     at internal/main/run_main_module.js:17:47  Confidential information is only cleared in case the config file is valid but not in the event of an error  code/packages/router/src/config.ts:L143-L149  if (!valid) {  console.error(`Invalid config: ${JSON.stringify(nxtpConfig, null, 2)}`);  throw new Error(validate.errors?.map((err) => err.message).join(\",\"));  console.log(JSON.stringify({ ...nxtpConfig, mnemonic: \"********\" }, null, 2));  return nxtpConfig;  Recommendation  Provide a valid default example config. Fix integration tests.  Always remove confidential information before logging on screen.  Avoid providing default credentials as it is very likely that someone might end up using them. Consider asking the user to provide missing credentials on first run or autogenerate it for them.  Note that the adminToken is not cleared before it is being printed to screen. If this is a credential it should be blanked out before being printed. Consider separating application-specific configuration from credentials/secrets.  5 Recommendations  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "5.1 Router - Logging Consistency",
        "body": "  Description  Avoid using console.*() in favor of the logger.*() family to provide a consistent timestamped log trail. Note that console.* might have different buffering behavior than logger.log which may mix up output lines.  Examples  code/packages/router/src/index.ts:L124-L131  server.listen(8080, \"0.0.0.0\", (err, address) => {  if (err) {  console.error(err);  process.exit(1);  console.log(`Server listening at ${address}`);  });  code/packages/router/src/contract.ts:L415-L416  const decoded = this.txManagerInterface.decodeFunctionResult(\"getRouterBalance\", encodedData);  console.log(\"decoded: \", decoded);  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "5.2 Router - Always perform strict validation of data received from third-parties or untrusted sources",
        "body": "  Description  For example, in subgraph.ts an external resource is queried to return transactions that match the router s ID, sending Chain, and status. An honest external party will only return items that match this filter. However, in case of the third-party misbehaving (or being breached), it might happen that entries that do not belong to this node or chain configuration are returned.  Examples  code/packages/router/src/subgraph.ts:L153-L175  let allSenderPrepared: GetSenderTransactionsQuery;  try {  allSenderPrepared = await sdk.GetSenderTransactions({  routerId: this.routerAddress.toLowerCase(),  sendingChainId: chainId,  status: TransactionStatus.Prepared,  });  } catch (err) {  this.logger.error(  { method, methodId, error: jsonifyError(err) },  \"Error in sdk.GetSenderTransactions, aborting loop interval\",  );  return;  // create list of txIds for each receiving chain  const receivingChains: Record<string, string[]> = {};  allSenderPrepared.router?.transactions.forEach(({ transactionId, receivingChainId }) => {  if (receivingChains[receivingChainId]) {  receivingChains[receivingChainId].push(transactionId);  } else {  receivingChains[receivingChainId] = [transactionId];  Recommendation  It is recommended to implement a defense-in-depth approach always validating inputs that come from third-parties or untrusted sources. Especially because the resources spent on performing the checks are negligible and significantly reduce the risk posed by third-party data providers.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "5.3 FulfillInterpreter - ReentrancyGuard can be removed  Pending",
        "body": "  Resolution   The   Description and Recommendation  code/packages/contracts/contracts/interpreters/FulfillInterpreter.sol:L22-L28  /**  @notice Errors if the sender is not the transaction manager  /  modifier onlyTransactionManager {  require(msg.sender == _transactionManager, \"#OTM:027\");  _;  code/packages/contracts/contracts/interpreters/FulfillInterpreter.sol:L54-L61  function execute(  bytes32 transactionId,  address payable callTo,  address assetId,  address payable fallbackAddress,  uint256 amount,  bytes calldata callData  ) override external payable nonReentrant onlyTransactionManager {  Consequently, if the TransactionManager contract can t be reentered, the FulfillInterpreter is automatically protected against reentrancy. Hence, if issue 4.9 is fixed, the reentrancy guard can be removed from FulfillInterpreter.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "5.4 FulfillInterpreter - _transactionManager state variable can be immutable   ",
        "body": "  Resolution  This recommendation has been implemented.  Description and Recommendation  The _transactionManager state variable in the FulfillInterpreter is set in the constructor and never changed afterward. Hence, it can be immutable.  code/packages/contracts/contracts/interpreters/FulfillInterpreter.sol:L16-L20  address private _transactionManager;  constructor(address transactionManager) {  _transactionManager = transactionManager;  ",
        "labels": [
            "Consensys",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "5.5 TransactionManager - Risk mitigation for addLiquidity   ",
        "body": "  Resolution  This recommendation has been implemented.  Description and Recommendation  ",
        "labels": [
            "Consensys",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"
    },
    {
        "title": "4.1 _WETH private constant address in UnoswapRouter makes for tricky deployments to other chains ",
        "body": "  Resolution  The 1inch team acknowledged but is unable to implement changes due to stack-too-deep errors that would require a large refactor of this already well-used and tested library.  To interact with WETH, the UnoswapRouter uses a hardcoded _WETH private constant in the contract. Therefore, this currently needs changing every time the contract is deployed to a different chain, as noted by the comment within the contract:  1inch-contract/contracts/routers/UnoswapRouter.sol:L24-L26  /// @dev WETH address is network-specific and needs to be changed before deployment.  /// It can not be moved to immutable as immutables are not supported in assembly  address private constant _WETH = 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2;  As the comment also points out, the choice to not make it an immutable variable is not possible since they are not supported in assembly, and the UnoswapRouter contract is highly efficient and almost entirely written in assembly. However, the other contracts within the scope of this audit, do utilize setting a private immutable variable for WETH in the constructor, and some of them then initialize a new address variable derived from this private immutable variable, thereby allowing the address variable to be used in the assembly blocks instead:  UnoswapV3Router  1inch-contract/contracts/routers/UnoswapV3Router.sol:L33-L37  IWETH private immutable _WETH;  // solhint-disable-line var-name-mixedcase  constructor(IWETH weth) {  _WETH = weth;  ClipperRouter  1inch-contract/contracts/routers/ClipperRouter.sol:L18-L24  IWETH private immutable _WETH;  // solhint-disable-line var-name-mixedcase  IClipperExchangeInterface private immutable _clipperExchange;  constructor(IWETH weth, IClipperExchangeInterface clipperExchange) {  _clipperExchange = clipperExchange;  _WETH = weth;  1inch-contract/contracts/routers/ClipperRouter.sol:L101  address weth = address(_WETH);  1inch-contract/contracts/routers/ClipperRouter.sol:L112  if iszero(call(gas(), weth, 0, ptr, 0x64, 0, 0)) {  OrderMixin  limit-order-protocol/contracts/OrderMixin.sol:L63-L70  IWETH private immutable _WETH;  // solhint-disable-line var-name-mixedcase  /// @notice Stores unfilled amounts for each order plus one.  /// Therefore 0 means order doesn't exist and 1 means order was filled  mapping(bytes32 => uint256) private _remaining;  constructor(IWETH weth) {  _WETH = weth;  Normalizing this process across all smart contracts in the 1inch system could help avoid accidental mistakes when the deployer could forget to first edit the unoswap contract to have the correct address.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"
    },
    {
        "title": "4.2 Selfdestruct may be removed as an opcode in future ",
        "body": "  Resolution  The 1inch team acknowledged and noted.  The AggregationRouterV5 contract implements a function called destroy that calls a selfdestuct on the contract with the msg.sender as the argument, that is checked by the onlyOwner modifier on the function.  1inch-contract/contracts/AggregationRouterV5.sol:L35-L37  function destroy() external onlyOwner {  selfdestruct(payable(msg.sender));  However, there are discussions currently around removing the selfdestruct functionality from the EVM altogether with various motivations and rationale provided, such as this being not possible with Verkle trees and it being a requirement for stateleness. Link to the EIP is below: https://eips.ethereum.org/EIPS/eip-4758  It appears that the suggested remediation of this functionality per the EIP-4758 will not significantly change the results, for example all of the funds will still be sent to the specified address, but the destruction of the actual contract will not occur. So this is just an advisory note for the 1inch team to notify of this potential change in the future.  5 Limit Order Protocol  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"
    },
    {
        "title": "5.1 Malicious maker can take more takers funds than taker expected ",
        "body": "  Resolution   Remediated as per the 1inch team in   1inch/limit-order-protocol@9ddc086 by adding a check that reverts when  OrderMixin contract allows users to match makers(sellers) and takers(buyers) in an orderbook-like manner. One additional feature this contract has is that both makers and takers are allowed to integrate hooks into their orders to better react to market conditions and manage funds on the fly. Two out of many of these hooks are called: _getMakingAmount and _getTakingAmount. Those particular hooks allow the maker to dynamically respond to the making or taking amounts supplied by the taker. Essentially they allow overriding the rate that was initially set by the maker when creating an order up to a certain extent. To make sure that the newly suggested maker rate is reasonable taker also provides a threshold value or in other words the minimum amount of assets the taker is going to be fine receiving.  Generally speaking, the maker can override the taking amount offered to the taker if the buyer passed a specific making amount in the fill transaction and vice versa. But there is one special case where the maker will be able to override both, which when done right will force the taker to spend an amount larger than the one intended. Specifically, this happens when the taker passed the desired taking amount and the maker returns a suggested making amount that is larger than the remaining order size. In this case, the making amount is being set to the remaining order amount and the taking is being recomputed.  limit-order-protocol/contracts/OrderMixin.sol:L214-L217  actualMakingAmount = _getMakingAmount(order.getMakingAmount(), order.takingAmount, actualTakingAmount, order.makingAmount, remainingMakingAmount, orderHash);  if (actualMakingAmount > remainingMakingAmount) {  actualMakingAmount = remainingMakingAmount;  actualTakingAmount = _getTakingAmount(order.getTakingAmount(), order.makingAmount, actualMakingAmount, order.takingAmount, remainingMakingAmount, orderHash);  Essentially this allows the maker to override the taker amount and as long as the maker keeps the price intact or changed within a certain threshold like described in this issue, they can take all taking tokens of the buyer up to an amount of the token balance or approval limit whatever comes first.  Consider the following example scenario:  The maker has a large order to sell 100 ETH on the order book for 100 DAI each.  The taker then wants to partially fill the order and buy as much ETH as 100 DAI will buy. At the same time taker has 100,000 DAI in the wallet.  When taker tries to fill this order taker passes the takingAmount to be 100. Since OrderMixin received the taking amount we go this route:  limit-order-protocol/contracts/OrderMixin.sol:L214  actualMakingAmount = _getMakingAmount(order.getMakingAmount(), order.takingAmount, actualTakingAmount, order.makingAmount, remainingMakingAmount, orderHash);  However, note that when executing the _getMakingAmount() function, it first evaluates the order.getMakingAmount() argument which is evaluated as bytes calldata _getter within the function.  limit-order-protocol/contracts/OrderMixin.sol:L324-L337  function _getMakingAmount(  bytes calldata getter,  uint256 orderTakingAmount,  uint256 requestedTakingAmount,  uint256 orderMakingAmount,  uint256 remainingMakingAmount,  bytes32 orderHash  ) private view returns(uint256) {  if (getter.length == 0) {  // Linear proportion  return getMakingAmount(orderMakingAmount, orderTakingAmount, requestedTakingAmount);  return _callGetter(getter, orderTakingAmount, requestedTakingAmount, orderMakingAmount, remainingMakingAmount, orderHash);  That is because the Order struct that is made and signed by the maker actually contains the necessary bytes within it that can be decoded to construct a target and calldata for static calls, which in this case are supposed to be used to return the making asset amounts that the maker determines to be appropriate, as seen in the comments under the uint256 offsets part of the struct.  limit-order-protocol/contracts/OrderLib.sol:L7-L27  library OrderLib {  struct Order {  uint256 salt;  address makerAsset;  address takerAsset;  address maker;  address receiver;  address allowedSender;  // equals to Zero address on public orders  uint256 makingAmount;  uint256 takingAmount;  uint256 offsets;  // bytes makerAssetData;  // bytes takerAssetData;  // bytes getMakingAmount; // this.staticcall(abi.encodePacked(bytes, swapTakerAmount)) => (swapMakerAmount)  // bytes getTakingAmount; // this.staticcall(abi.encodePacked(bytes, swapMakerAmount)) => (swapTakerAmount)  // bytes predicate;       // this.staticcall(bytes) => (bool)  // bytes permit;          // On first fill: permit.1.call(abi.encodePacked(permit.selector, permit.2))  // bytes preInteraction;  // bytes postInteraction;  bytes interactions; // concat(makerAssetData, takerAssetData, getMakingAmount, getTakingAmount, predicate, permit, preIntercation, postInteraction)  Finally, if these bytes indeed contain data (i.e. length>0), they are passed to the _callGetter() function that asks the previously mentioned target for the data.  limit-order-protocol/contracts/OrderMixin.sol:L354-L377  function _callGetter(  bytes calldata getter,  uint256 orderExpectedAmount,  uint256 requestedAmount,  uint256 orderResultAmount,  uint256 remainingMakingAmount,  bytes32 orderHash  ) private view returns(uint256) {  if (getter.length == 1) {  if (OrderLib.getterIsFrozen(getter)) {  // On \"x\" getter calldata only exact amount is allowed  if (requestedAmount != orderExpectedAmount) revert WrongAmount();  return orderResultAmount;  } else {  revert WrongGetter();  } else {  (address target, bytes calldata data) = getter.decodeTargetAndCalldata();  (bool success, bytes memory result) = target.staticcall(abi.encodePacked(data, requestedAmount, remainingMakingAmount, orderHash));  if (!success || result.length != 32) revert GetAmountCallFailed();  return abi.decode(result, (uint256));  However, since the getter is set in the Order struct, and the Order is set by the maker, the getter itself is entirely under the maker s control and can return whatever the maker wants, with no regard for the taker s passed actualTakingAmount or any arguments at all for that matter. So, in our example, the return value could be 100.1 ETH, i.e. just above the total order size. That will get us on the route of recomputing the taking amount since 100.1 is over the 100ETH remaining in the order.  limit-order-protocol/contracts/OrderMixin.sol:L217  actualTakingAmount = _getTakingAmount(order.getTakingAmount(), order.makingAmount, actualMakingAmount, order.takingAmount, remainingMakingAmount, orderHash);  This branch will set the actualMakingAmount to 100ETH and then the malicious maker will say the actualTakingAmount is 10000 DAI, this can be done via the _getTakingAmount static call in the same exact way as the making amount was manipulated.  Then the threshold check would look like this as defined by its formula:  limit-order-protocol/contracts/OrderMixin.sol:L222  if (actualMakingAmount * takingAmount < thresholdAmount * actualTakingAmount) revert MakingAmountTooLow();  ActualMakingAmount - 100ETH  ActualTakingAmount - 10000DAI  threshold - 1 ETH  takingAmount - 100 DAI  then: 100ETH * 100DAI < 1ETH*10000DAI This condition will be false so we will pass this check.  Then we proceed to taker interaction. Assuming the taker did not pass any interaction, the actualTakingAmount will not change.  Then we proceed to exchange tokens between maker and taker in the amount of actualTakingAmount and actualMakingAmount.  The scenario allows the maker to take the taker s funds up to an amount of taker s approval or balance. Essentially while taker wanted to only spend 100 DAI, potentially they ended up spending much more. This paired with infinite approvals that are currently enabled on the 1inch UI could lead to funds being lost.  While this does not introduce a price discrepancy this attack can be profitable to the malicious actor. The attacker could put an order to sell a large amount of new not trustworthy tokens for sale who s supply the attacker controls. Then after a short marketing campaign when people will cautiously try to buy a small amount of those tokens for let s say a small amount of USDC due to this bug attacker could drain all of their USDC.  We advise that 1inch team treats this issue with extra care since a similar issue is present in a currently deployed production version of 1inch OrderMixin. One potential solution to this bug is introducing a global threshold that would represent by how much the actual taking amount can differ from the taker provided taking amount.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"
    },
    {
        "title": "5.2 Invalidating users orders ",
        "body": "  1inch team has implemented a more streamlined version of the order book that is called OrderRFQMixin. This version has no hooks and is meant to be more straightforward than the main order book contract.  One significant difference between those contracts is that the RFQ version invalidates the orders even after they have been only partially filled.  limit-order-protocol/contracts/OrderRFQMixin.sol:L197-L203  {  // Stack too deep  uint256 info = order.info;  // Check time expiration  uint256 expiration = uint128(info) >> 64;  if (expiration != 0 && block.timestamp > expiration) revert OrderExpired(); // solhint-disable-line not-rely-on-time  _invalidateOrder(maker, info, 0);  Since makers have to sign the orders, only makers can place the remainder of the original order as a new one. Given that information, an attacker could take all the orders and fill them with 1 wei of taking assets. While this will cost an attacker gas, on some chains it would be possible to make the operations of the protocol unreliable and impractical for makers.  One way to fix that without making significant changes to the logic is to introduce a threshold that will determine the smallest taking amount for each order. That could be a percent of the taking amount specified in the order. This change will make the attack more expensive and less likely to happen.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"
    },
    {
        "title": "5.3 Reentrancy potential issue for contracts building on top RFQOrderMixin ",
        "body": "  Resolution   Remediated as per the 1inch team in   1inch/limit-order-protocol@d3957fe by forwarding a limited amount of gas to guard against complex execution at the target but still allow for smart contract receipt that may require a bit more gas than usual EOA receipts.  The RFQOrderMixin contract is used to facilitate transfer of assets in RFQ orders between makers and takers. Naturally, one of such possible assets could be the native coin of the chain, such as ETH. In order to perform these transfers, the contract currently utilizes the target.call(){value:X} method to transfer X ETH to the target address. However, this also calls into the target address and opens up arbitrary code execution that could lead to significant problems, that often times result in a reentrancy attack.  limit-order-protocol/contracts/OrderRFQMixin.sol:L233  (bool success, ) = target.call{value: makingAmount}(\"\");  // solhint-disable-line avoid-low-level-calls  While 1inch s RFQOrderMixin contract doesn t have a clear reentrancy attack vector, other smart contract systems that might utilize 1inch RFQ orders will have to handle a potential reentrancy due to this problem. The impact for downstream systems could be critical.  This could be changed to .transfer() or .send() methods of transferring ETH, or at least heavily noted in documentation for any and all developers who may fork/utilize this code so reentrancy risks are made aware of.  This does not seem to be a general-purpose use library for other systems, so likelihood of this issue happening isn t as high as in issue 6.4, so the severity is lower.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"
    },
    {
        "title": "5.4 Order cancellation event spam in orderMixin ",
        "body": "  The 1inch Limit Order protocol s contracts utilize mechanisms to allow creation of orders without posting the orders on chain. Indeed, the orders are created by signing Order struct hashes off-chain by the maker, and then having takers pass the signatures associated with those hashes to fill in those orders. However, a maker needs to be able to cancel their order if they change their mind, which would require them to execute an on-chain transaction marking that order hash as invalid:  limit-order-protocol/contracts/OrderMixin.sol:L113-L121  function cancelOrder(OrderLib.Order calldata order) external returns(uint256 orderRemaining, bytes32 orderHash) {  if (order.maker != msg.sender) revert AccessDenied();  orderHash = hashOrder(order);  orderRemaining = _remaining[orderHash];  if (orderRemaining == _ORDER_FILLED) revert AlreadyFilled();  emit OrderCanceled(msg.sender, orderHash, orderRemaining);  _remaining[orderHash] = _ORDER_FILLED;  Unfortunately, since the OrderMixin contract is not aware of order hashes before interacting with them for the first time, it can not verify that the order was actually ever seriously present or intended to be executed. As a result, this would allow users to cancel non-existent orders and create event spam. While this would be costly to the spammer, it would nonetheless be possible.  The impact of this would need systems that rely on the OrderCanceled event log to be aware of potential spam attacks with fake order cancellation and not use them, for example, for analytics, potential volume forecasting, tracking order created -> order cancelled metrics and so on.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"
    },
    {
        "title": "5.5 Order book slippage ",
        "body": "  OrderMixin contract allows users to match makers and takers in an orderbook-like manner. One additional feature this contract has is that both makers and takers are allowed to integrate hooks into their orders to better react to market conditions and manage funds on the fly. Two of these hooks are called: _getMakingAmount and _getTakingAmount.  When trying to fill an order taker is required to provide either the making amount or taking amount as well as the threshold or in other words the minimum amount of assets the taker is going to be fine receiving. During the fill transaction, an order maker is given the opportunity to update the offer by the means of the _getMakingAmount and _getTakingAmount. A threshold checks are then used in order to make sure that the updated values are within taker s acceptable bounds:  limit-order-protocol/contracts/OrderMixin.sol:L222  if (actualMakingAmount * takingAmount < thresholdAmount * actualTakingAmount) revert MakingAmountTooLow();  limit-order-protocol/contracts/OrderMixin.sol:L212  if (actualTakingAmount * makingAmount > thresholdAmount * actualMakingAmount) revert TakingAmountTooHigh();  It is reasonable to assume that if the maker knows the threshold the taker selected, the maker will attempt to update the making or taking amount to maximize profits. While _getMakingAmount and _getTakingAmount do not pass the threshold selected by the taker directly, it is still possible for the maker to obtain this information and act accordingly.  A malicious maker could listen to the mempool and wait for a transaction that is meant to fill his order obtaining the threshold value.  Maker would then update the state of the contract that responds to the static call of the _getMakingAmount and _getTakingAmount hooks.  If the maker is using FlashBots or a similar service, the maker can ensure that the above actions are performed before the transaction that would fill the order.  While there is no good way to alleviate this issue given the current design we believe it is important to be aware of this issue and allow the 1inch users to know that some analogy of slippage is still possible even on the orderbook-like system. This will allow them to choose tighter and more secure threshold values.  6 Solidity Utils  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"
    },
    {
        "title": "6.1 ECDSA library has a vulnerability for signature malleability of EIP-2098 compact signatures ",
        "body": "  Resolution   Remediated as per the 1inch team in   1inch/solidity-utils@166353b by adding a warning note in the comments of the library code.  The 1inch ECDSA library supports several types of signatures and forms in which they could be provided. However, for compact signatures there is a recently found malleability attack vector. Specifically, the issue arises when contracts use transaction replay protection through signature uniqueness (i.e. by marking it as used). While this may not be the case in the scope of other contracts of this audit, this ECDSA library is meant to be a general use library so it should be fixed so as to not mislead others who might use this.  For more details and context, find below the advisory notice and fix in the OpenZeppelin s ECDSA library: https://github.com/OpenZeppelin/openzeppelin-contracts/security/advisories/GHSA-4h98-2769-gh6h OpenZeppelin/openzeppelin-contracts@d693d89  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"
    },
    {
        "title": "6.2 Ethereum reimbursements sent to an incorrect address ",
        "body": "  Resolution   Remediated as per the 1inch team as of   1inch/solidity-utils@6b1a3df by adding the correct recipient of the refund.  1inch team has written a library called UniERC20 that extends the traditional ERC20 standard to also support eth transfers seamlessly. In the case of the uniTransferFrom function call, the library checks that the msg.value of the transaction is bigger or equal to the amount passed in the function argument. If the msg.value is larger than the amount required, the difference, or extra funds, should be sent to the sender. In the actual implementation Instead of returning the funds to the sender, extra funds are actually sent to the destination.  solidity-utils/contracts/libraries/UniERC20.sol:L59-L65  if (msg.value > amount) {  // Return remainder if exist  unchecked {  (bool success, ) = to.call{value: msg.value - amount}(\"\");  // solhint-disable-line avoid-low-level-calls  if (!success) revert ETHSendFailed();  Given that this code is packed as a library and allows for easy reusability by the 1inch team and outside developers it is crucial that this logic is written well and well tested.  We recommend reconsidering reimbursing the sender when an incorrect amount is being sent because it introduces an easy-to-oversee reentrancy backdoor with call() that is mentioned in issue 6.4. Reverting was a default behavior in similar cases across the rest of the 1inch contracts.  If this functionality is required, a fix we could recommend is replacing the to with from. We can also suggest running a fuzzing campaign against this library.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"
    },
    {
        "title": "6.3 ECDSA incorrect size provided for calldata in the static call ",
        "body": "  Resolution   Remediated as per the 1inch team in   1inch/solidity-utils@cfdc889 by passing the correct data size.  The ECDSA library implements support for IERC1271 interfaces that verify provided signature for the data through the different isValidSignature functions that depend on the type of signature used.  However, the library passes an incorrect size for the calldata in the static call for signatures that are of the form (bytes32 r, bytes32 vs). It should be 0xa4 (164 bytes) instead of 0xa5 (165 bytes).  solidity-utils/contracts/libraries/ECDSA.sol:L178  if staticcall(gas(), signer, ptr, 0xa5, 0, 0x20) {  The impact could vary and depends on the signature verifier. For example, it could be significant if the signature verifier performs a check on the calldatasize for this specific type of signature and reverts on incorrect sizes, thereby having valid signatures return false when passed to isValidSignature.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"
    },
    {
        "title": "6.4 Re-entrancy risk in UniERC20 ",
        "body": "  Resolution   Remediated as per the 1inch team in   1inch/solidity-utils@6b1a3df by forwarding a limited amount of gas to guard against complex execution at the target but still allow for smart contract receipt that may require a bit more gas than usual EOA receipts.  UniERC20 is a general library for facilitating transfers of any ERC20 or native coin assets. It features gas-efficient code and could be easily integrated into large systems of contract, such as those that are used in this audit   1inch routers and limit order protocol.  However, it also utilizes .call(){value:X} method of transferring chain native assets, such as ETH. This introduces a large risk in the form of re-entrancy attacks, so any system implementing this library would have to handle them. While 1inch s projects in the scope of this audit do not seem to have re-entrancy attack vectors, other projects that could be utilizing this library might. Since this is an especially efficient and convenient library, the likelihood that some other project using this suffers and then sufferring a re-entrancy attack is significant.  solidity-utils/contracts/libraries/UniERC20.sol:L45  (bool success, ) = to.call{value: amount}(\"\");  // solhint-disable-line avoid-low-level-calls  solidity-utils/contracts/libraries/UniERC20.sol:L62  (bool success, ) = to.call{value: msg.value - amount}(\"\");  // solhint-disable-line avoid-low-level-calls  Consider instead implementing transfer() or send() methods for transferring chain native assets, such as ETH, instead of performing a .call()  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"
    },
    {
        "title": "6.1 in3-server - amplified DDoS on incubed requests on proof with signature    ",
        "body": "  Resolution   Mitigated by adding   merge_requests/101. The full extent of this fix is outside the scope of this audit.  Description  It is possible for a client to send a request to each node of the network to request a signature with proof for every other node in the network. This can result in DDoSing the network as there are no costs for the client to request this and client can send the same request to all the nodes in the network, resulting in n^2 requests.  Examples  Client asks each node for in3_nodeList to get all the signer addresses, this could also be done using NodeRegistry contract  Client asks each node for a proof with signature, e.g.:  \"jsonrpc\": \"2.0\",  \"id\": 2,  \"method\": \"eth_getTransactionByHash\",  \"params\": [\"0xf84cfb78971ebd940d7e4375b077244e93db2c3f88443bb93c561812cfed055c\"],  \"in3\": {  \"chainId\": \"0x1\",  \"verification\": \"proofWithSignature\",  \"signatures\":[\"0x784bfa9eb182C3a02DbeB5285e3dBa92d717E07a\", ALL OTHER SIGNERS HERE]  All the nodes are now sending requests to each other with signature required which is an expensive computation. This can go on for more transactions (or blocks, or other Eth_ requests) and can result in DDoS of the network.  Recommendation  Limit the number of signers in proof with signature requests. Also exclude self.signer from the list. This combined with the remediation of issue 6.6 can partially mitigate the attack vector.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.2 BlockProof - Node conviction race condition may trick all but one node into losing funds    ",
        "body": "  Resolution  Mitigated by:  80bb6ecf by checking if blockhash exists and prevent an overwrite, saving gas  Client will blacklist the server if the signature is missing, has a wrong signer or is invalid.  6cc0dbc0 Removing nodes from local available nodes list when the server detects wrong responses  Other commits to mitigate the mentioned vulnerable scenarios  With the new handling, the client will not call convict immediately (as this could be exploited again). Instead, the client will do the calculation whether it s worth convicting the server before even calling convict.  It should be noted that the changes are scattered and modified in the final source code, and this behaviour of IN3-server code is outside the scope of this audit.  Description  TLDR; One node can force all other nodes to convict a specific malicious signer controlled by the attacker and spend gas on something they are not going to be rewarded for. The attacker loses deposit but all other nodes that try to convict and recreate in the same block will lose the fees less or equal to deposit/2. Another variant forces the same node to recreate the blockheaders multiple times within the same block as the node does not check if it is already convicting/recreating blockheaders.  Nodes can request various types of proofs from other nodes. For example, if a node requests a proof when calling one of the eth_getBlock* methods, the in3-server s method handleBlock will be called. The request should contain a list of addresses registered to the NodeRegistry that are requested to sign the blockhash.  code/in3-server/src/modules/eth/EthHandler.ts:L105-L112  // handle special jspn-rpc  if (request.in3.verification.startsWith('proof'))  switch (request.method) {  case 'eth_getBlockByNumber':  case 'eth_getBlockByHash':  case 'eth_getBlockTransactionCountByHash':  case 'eth_getBlockTransactionCountByNumber':  return handleBlock(this, request)  in3-server will subsequently reach out to it s connected blockchain node execute the eth_getBlock* call to get the block data. If the block data is available the in3-server, it will try to collect signatures from the nodes that signature was requested from (request.in3.signatures, collectSignatures())  code/in3-server/src/modules/eth/proof.ts:L237-L243  // create the proof  response.in3 = {  proof: {  type: 'blockProof',  signatures: await collectSignatures(handler, request.in3.signatures, [{ blockNumber: toNumber(blockData.number), hash: blockData.hash }], request.in3.verifiedHashes)  If the node does not find the address it will throw an exception. Note that if this exception is not caught it will actually allow someone to boot nodes off the network - which is critical.  code/in3-server/src/chains/signatures.ts:L58-L60  const config = nodes.nodes.find(_ => _.address.toLowerCase() === adr.toLowerCase())  if (!config) // TODO do we need to throw here or is it ok to simply not deliver the signature?  throw new Error('The ' + adr + ' does not exist within the current registered active nodeList!')  If the address is valid and existent in the NodeRegistry the in3-node will ask the node to sign the blockhash of the requested blocknumber:  code/in3-server/src/chains/signatures.ts:L69-L84  // send the sign-request  let response: RPCResponse  try {  response = (blocksToRequest.length  ? await handler.transport.handle(config.url, { id: handler.counter++ || 1, jsonrpc: '2.0', method: 'in3_sign', params: blocksToRequest })  : { result: [] }) as RPCResponse  if (response.error) {  //throw new Error('Could not get the signature from ' + adr + ' for blocks ' + blocks.map(_ => _.blockNumber).join() + ':' + response.error)  logger.error('Could not get the signature from ' + adr + ' for blocks ' + blocks.map(_ => _.blockNumber).join() + ':' + response.error)  return null  } catch (error) {  logger.error(error.toString())  return null  For all the signed blockhashes that have been returned the in3-server will subsequently check if one of the nodes provided a wrong blockhash.  We note that nodes might:  decided to not follow the in_3sign request and just not provide a signed response  a node might sign with a different key  a node might sign a different blockheader  a node might sign a previous blocknumber  In all these cases, the node will not be convicted, even though it was able to request other nodes to perform work.  If another node signed a wrong blockhash the in3-server will automatically try to convict it. If the block is within the most recent 255 it will directly call convict() on the NodeRegistry (takes less gas). if it is an older block, it will try to recreate the blockchain in the RlockhashRegistry (takes more gas).  code/in3-server/src/chains/signatures.ts:L128-L152  const convictSignature: Buffer = keccak(Buffer.concat([bytes32(s.blockHash), address(singingNode.address), toBuffer(s.v, 1), bytes32(s.r), bytes32(s.s)]))  if (diffBlocks < 255) {  await callContract(handler.config.rpcUrl, nodes.contract, 'convict(uint,bytes32)', [s.block, convictSignature], {  privateKey: handler.config.privateKey,  gas: 500000,  value: 0,  confirm: true                       //  we are not waiting for confirmation, since we want to deliver the answer to the client.  })  handler.watcher.futureConvicts.push({  convictBlockNumber: latestBlockNumber,  signer: singingNode.address,  wrongBlockHash: s.blockHash,  wrongBlockNumber: s.block,  v: s.v,  r: s.r,  s: s.s,  recreationDone: true  })  else {  await handleRecreation(handler, nodes, singingNode, s, diffBlocks)  The recreation and convict is only done if it is profitable for the node. (Note the issue mentioned in issue 6.13)  code/in3-server/src/chains/signatures.ts:L209-L213  const costPerBlock = 86412400000000  const blocksMissing = latestSS - s.block  const costs = blocksMissing * costPerBlock * 1.25  if (costs > (deposit / 2)) {  A malicious node can exploit the hardcoded profit economics and the fact that in3-server implementation will try to auto-convict nodes in the following scenario:  malicious node requests a blockproof with an eth_getBlock* call from the victim node (in3-server) for a block that is not in the most recent 256 blocks (to maximize effort for the node). This equals to spending more gas in order to convict the node (costs <= (deposit / 2)).  the malicious node prepares the BlockhashRegistry to contain a blockhash that would maximize the gas needed to convict the malicious node (can be calculated offline; must fulfill costs <= (deposit /2).  with the blockproof request the malicious node asks the in3-server to get the signature from a specific signer. The signer will also be malicious and is going to sign a wrong blockhash with a valid signature.  the malicious signer is going to lose it s deposit but the deposit also incentivizes other nodes to spend gas on the conviction process. The higher the deposit, the more an in3-server is willing to spend on the conviction.  In this scenario one malicious node tries to trick another node into convicting a malicious signer while having to spend the maximum amount of gas to make it profitable for the node.  The problem is, that the malicious node can ask multiple (or even all other nodes in the registry) to provide a blockproof and ask the malicious signer for a signed blockhash. All nodes will come to the conclusion that the signer returned an invalid hash and will try to convict the node. They will try to recreate the blockchain in the BlockhashRegistry for a barely profitable scenario. Since in3-nodes do not monitor the tx-pool they will not know that other nodes are already trying to convict the node. All nodes are going to spend gas on recreating the same blockchain in the BlockhashRegistry leading to all but the first transaction in the block to lose funds (up to deposit/2 based on the hardcoded costPerBlock)  Another variant of the same issue is that nodes do not check if they already convicted another node (or recreated blockheaders). An attacker can therefore force a specific node to convict a malicious node multiple times before the nodes transactions are actually in a block as the nodes does not check if it is already convicting that node. The node might lose gas on the recreation/conviction process multiple times.  Recommendation  To reduce the impact of multiple nodes trying to update the blockhashRegistry at the same time and avoid nodes losing gas by recreating the same blocks over and over again, the BlockhashRegistry should require that the target blockhash for the blocknumber does not yet exist in the registry (similar to the issue mentioned in https://github.com/ConsenSys/slockit-in3-audit-2019-09/issues/24).  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.3 NodeRegistry Front-running attack on convict()    ",
        "body": "  Resolution  Blocknumber is removed from convict function, which removes any signal for an attacker in the scenario provided. However, the order of the transactions to convict a wrong signed hash is necessary to prevent any front-running attacks:  Convict(_Blockhash)  recreate Blockheaders  RevealConvict (minimum 2 blocks after convict but as soon as recreateBlockheaders is confirmed)  The fixes were introduced in ecf2c6a6 and f4250c9a, although later on NodeRegistry contract was split in two other contracts NodeRegistryLogic and NodeRegistryData and further changes were done in the conviction flow in different commits.  Description  convict(uint _blockNumber, bytes32 _hash) and revealConvict() are designed to prevent front-running and they do so for the purpose they are designed for. However, if the malicious node, is still sending out the wrong blockhash for the convicted block, anyone seeing the initial convict transaction, can check the convicted blocknumber with the nodes and send his own revealConvict before the original sender.  The original sender will be the one updating the block headers recreateBlockheaders(_blockNumber, _blockheaders), and the attacker can just watch for the update headers to perform this attack.  Recommendation  For the first attack vector, remove the blocknumber from the convict(uint _blockNumber, bytes32 _hash) inputs and just use the hash.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.4 NodeRegistry - URL can be arbitrary dns resolvable names, IP s and even localhost or private subnets    ",
        "body": "  Resolution  This issue has been addressed with the following commits:  4c93a10f adding 48 hours delay in the server code before they communicate with the newly registered nodes.  merge_requests/111 adding a whole new smart contract to the IN3 system, IN3WhiteList.sol, and supporting code in the server.  issues/94 To prevent attacker to use nodes as a DoS network, a DNS record verification is discussed to be implemented.  It is a design decision to base the Node registry on URLs (DNS resolvable names). This has the implications outlined in this issue and they cannot easily be mitigated. Adding a delay until nodes can be used after registration only delays the problem. Assuming that an entity curates the registry or a whitelist is in place centralizes the system. Adding DNS record verification still allows an owner of a DNS entry to point its name to any IP address they would like it to point to. It certainly makes it harder to add RPC URLs with DNS names that are not in control of the attacker but it also adds a whole lot more complexity to the system (including manual steps performed by the node operator). In the end, the system allows IP based URLs in the registry which cannot be used for DNS validation.  Note that the server code changes, and the new smart contract IN3WhiteList.sol are outside the scope of the original audit. We strongly recommend to reduce complexity and audit the final codebase before mainnet deployment.  Description  As outlined in issue 6.9 the NodeRegistry allows anyone to register nodes with arbitrary URLs. The url is then used by in3-server or clients to connect to other nodes in the system. Signers can only be convicted if they sign wrong blockhashes. However, if they never provide any signatures they can stay in the registry for as long as they want and sabotage the network. The Registry implements an admin functionality that is available for the first year to remove misbehaving nodes (or spam entries) from the Registry. However, this is insufficient as an attacker might just re-register nodes after the minimum timeout they specify or spend some more finneys on registering more nodes. Depending on the eth-price this will be more or less profitable.  From an attackers perspective the NodeRegistry is a good source of information for reconnaissance, allows to de-anonymize and profile nodes based on dns entries or netblocks or responses to in3_stats (https://github.com/ConsenSys/slockit-in3-audit-2019-09/issues/49), makes a good list of target for DoS attacks on the system or makes it easy to exploit nodes for certain yet unknown security vulnerabilities.  Since nodes and potentially clients (not in scope) do not validate the rpc URL received from the NodeRegistry they will try to connect to whatever is stored in a nodes url entry.  code/in3-server/src/chains/signatures.ts:L58-L75  const config = nodes.nodes.find(_ => _.address.toLowerCase() === adr.toLowerCase())  if (!config) // TODO do we need to throw here or is it ok to simply not deliver the signature?  throw new Error('The ' + adr + ' does not exist within the current registered active nodeList!')  // get cache signatures and remaining blocks that have no signatures  const cachedSignatures: Signature[] = []  const blocksToRequest = blocks.filter(b => {  const s = signatureCaches.get(b.hash) && false  return s ? cachedSignatures.push(s) * 0 : true  })  // send the sign-request  let response: RPCResponse  try {  response = (blocksToRequest.length  ? await handler.transport.handle(config.url, { id: handler.counter++ || 1, jsonrpc: '2.0', method: 'in3_sign', params: blocksToRequest })  : { result: [] }) as RPCResponse  if (response.error) {  This allows for a wide range of attacks not limited to:  An attacker might register a node with an empty or invalid URL. The in3-server does not validate the URL and therefore will attempt to connect to the invalid URL, spending resources (cpu, file-descriptors, ..) to find out that it is invalid.  An attacker might register a node with a URL that is pointing to another node s rpc endpoint and specify weights that suggest that it is capable of service a lot of requests to draw more traffic towards that node in an attempt to cause a DoS situation.  An attacker might register a node for a http/https website at any port in an extortion attempt directed to website owners. The incubed network nodes will have to learn themselves that the URL is invalid and they will at least attempt to connect the website once.  An attacker might update the node information in the NodeRegistry for a specific node every block, providing a new url (or a slightly different URLs issue 6.9) to avoid client/node URL blacklists.  An attacker might provide IP addresses instead of DNS resolvable names with the url in an attempt to draw traffic to targets, avoiding canonicalization and blacklisting features.  An attacker might provide a URL that points to private IP netblocks for IPv4 or IPv6 in various formats. Combined with the ability to ask another node to connect to an attacker defined url (via blockproof, signatures[] -> signer_address -> signer.url) this might allow an attacker to enumerate services in the LAN of node operators.  An attacker might provide the loopback IPv4, IPv6 or resolvable name as the URL in an attempt to make the node connect to local loopback services (service discovery, bypassing authentication for some local running services - however this is very limited to the requests nodes may execute).  URLs may be provided in various formats: resolvable dns names, IPv4, IPv6 and depending on the http handler implementation even in Decimal, Hex or Octal form (i.e. http://2130706433/)  A valid DNS resolvable name might point to a localhost or private IP netblock.  Since none of the rpc endpoints provide signatures they cannot be convicted or removed (unless the unregisterKey does it within the first year. However, that will not solve the problem that someone can re-register the same URLs over and over again)  Recommendation  It is a fundamental design decision of the system architecture to allow rpc urls in the Node Registry, therefore this issue can only be partially mitigated unless the system design is reworked. It is therefore suggested to add checks to both the registry contract (coarse validation to avoid adding invalid urls) and node implementations (rigorous validation of URL s and resolved IP addresses) and filter out any potentially harmful destinations.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.5 Malicious clients can use forks or reorgs to convict honest nodes   ",
        "body": "  Resolution  Default value for past signed blocks is changed to 10 blocks. Slockit plans to use their off-chain channels to notify clients for planned forks. They also looking into using fork oracles in the future releases to detect planned hardforks to mitigate risks.  Description  In case of reorgs it is possible to have more than 6 blocks in a node that gets replaced by a new longer chain. Also for forks, such as upcoming Istanbul fork, it s common to have some nodes taking some time to update and they will be in the wrong chain for the time being. In both cases, in3-nodes are prone to sign blocks that are considered invalid in the main chain. Malicious nodes can catch these instances and convict the honest users in the main chain to get 50% of their deposits.  Recommendation  No perfect solution comes to mind at this time. One possible mitigation method for forks could be to disable the network on the time of the fork but this is most certainly going to be a threat to the system itself.  ",
        "labels": [
            "Consensys",
            "Major",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.6 in3-server - should protect itself from abusive clients    ",
        "body": "  Resolution  Slockit implemented their own DOS protection for incubed server in merge_requests/99. The variant of this implementation adds more complexity to the code base. The benchmark and testing of the new DOS protection is not in scope for this audit.  The incubed server has now an additional DOS-Protection build in. Here we first estimate a Weight of such a request and add them together for all incoming requests per IP of the client per Minute. Since we estimate the execution, we can prevent a client running DOS-Attacks from the same IP with heavy requests (such as eth_getLogs)  Description  The in3-node implementation should provide features for client request throttling to avoid that a client can consume most of the nodes resources by causing a lot of resource intensive requests.  This is a general problem to the system which is designed to make sure that low resource clients can verify blockchain properties. What this means is that almost all of the client requests are very lightweight. Clients can request nodes to sign data for them. A sign request involves cryptographic operations and a http-rpc request to a back-end blockchain node. The imbalance is clearly visible in the case of blockProofs where a client may request another node to interact with a smart contract (NodeRegistry) and ask other nodes to sign blockhashes. All other nodes will have to get the requested block data from their local blockchain nodes and the incubed node requesting the signatures will have to wait for all responses. The client instead only has to send out that request once and may just leave that tcp connection open. It might even consume more resources from a specific node by requesting the same signatures again and again not even waiting for a response but causing a lot of work on the node that has to collect all the signatures. This combined with unbound requests for signatures or other properties can easily be exploited by a powerful client implementation with a mission to stall the whole incubed network.  Recommendation  According to the threat model outlines a general DDoS scenario specific to rpcUrls. It discusses that the nodes are themselves responsible for DDoS protection. However, DDoS protection is a multi-layer approach and it is highly unlikely that every node-operator will hide their nodes behind a DDoS CDN like cloudflare. We therefore suggest to also build in strict limitations for clients that can be checked in code. Similar to checkPerformanceLimits which is just checking for some specific it is suggested to implement a multi-layer throttling mechanism that prevents nodes from being abused by single clients. Methods must be designed with (D)DoS scenarios in mind to avoid that third parties are abusing the network for DDoS campaigns or trying to DoS the incubed network.  code/in3-server/src/modules/eth/EthHandler.ts:L74-L91  private checkPerformanceLimits(request: RPCRequest) {  if (request.method === 'eth_call') {  if (!request.params || request.params.length < 2) throw new Error('eth_call must have a transaction and a block as parameters')  const tx = request.params as TxRequest  if (!tx || (tx.gas && toNumber(tx.gas) > 10000000)) throw new Error('eth_call with a gaslimit > 10M are not allowed')  else if (request.method === 'eth_getLogs') {  if (!request.params || request.params.length < 1) throw new Error('eth_getLogs must have a filter as parameter')  const filter: LogFilter = request.params[0]  let toB = filter && filter.toBlock  if (toB === 'latest' || toB === 'pending' || !toB) toB = this.watcher && this.watcher.block && this.watcher.block.number  let fromB = toB && filter && filter.fromBlock  if (fromB === 'earliest') fromB = 1;  const range = fromB && (toNumber(toB) - toNumber(fromB))  if (range > (request.in3.verification.startsWith('proof') ? 1000 : 10000))  throw new Error('eth_getLogs for a range of ' + range + ' blocks is not allowed. limits: with proof: 1000, without 10000 ')  implement request throttling per client  implement caching mechanism for similar requests if it is expected that the same response is to be delivered multiple times  implement general performance limits and reject further requests if the node is close to exhausting its resources (soft DoS)  make sure the node does not exhaust the systems resources  implement throttling per request method  design methods to prevent (D)DoS in the first place. Methods that allow a client to send one request that causes a node to perform multiple client controlled requests must be avoided or at least bound and throttled (issue 6.7, https://github.com/ConsenSys/slockit-in3-audit-2019-09/issues/50).  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.7 in3-server - DoS on in3.sign and other requests    ",
        "body": "  Resolution  Similar to issue 6.1, Mitigated by adding maxBlocksSigned and maxSignatures for requests of any client.  The Numbers of signatures a client can ask to fetch is now limited to maxSignatures which defaults to 5  in merge_requests/101. The full extent of this fix is outside the scope of this audit.  We have limited the number of block you can ask to sign in the in3_sign-request. The default is 10, because this function is also used for eth_getLogs to provide proof for all events. This limit will also limit the result of logs returned to include only max 10 different blocks.  Description  It is free for the client to ask the nodes to sign block hashes (and also other requests). in3.sign([{\"blockNumber\": 123}]) Takes an array of objects that will result in multiple requests in the node. This sample request has (at least) two internal requests, one eth_getBlockByNumber and signing the block hash.  These requests can be continuously sent out to clients and result in using computation power of the nodes without any expense from the client.  Examples  Request to get and sign the first 200 blocks:  web3.manager.request_blocking(\"in3_sign\", [{'blockNumber':i} for i in range(200)])  Recommendation  Limit the number of blocks (input), or do not accept arrays for input.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.8 in3-server - key management   Pending",
        "body": "  Resolution  The breakdown of the fixes addressed with git.slock.it/PR/13 are as follows:  Keys should never be stored or accepted in plaintext format Keys should only be accepted in an encrypted and protected format  The private key in code/in3-server/config.json has been removed. The repository still contains private keys at least in the following locations:  package.json  vscode/launch.json  example_docker-compose.yml  Note that private keys indexed by a git repository can be restored from the repository history.  The following statement has been provided to address this issue:  We have removed all examples and usage of plain private keys and replaced them with json-keystore files. Also in the documentation we added warnings on how to deal with keys, especially with hints to the bash history or enviroment  A single key should be used for only one purpose. Keys should not be shared.  The following statement has been provided to address this issue:  This is why we seperated the owner and signer-key. This way you can use a multisig to securly protect the owner-key. The signer-key is used to sign blocks (and convict) and is not able to do anything else (not even changing its own url)  The application should support developers in understanding where cryptographic keys are stored within the application as well as in which memory regions they might be accessible for other applications  Addressed by wrapping the private key in an object that stores the key in encrypted form and only decrypts it when signing. The key is cleared after usage. The IN3-server still allows raw private keys to be configured. A warning is printed if that is the case. The loaded raw private key is temporarily assigned to a local variable and not explicitly cleared by the method.  While we used to keep the unlocked key as part of the config, we have now removed the key from the config and store them in a special signer-function. https://git.slock.it/in3/ts/in3-server/merge_requests/113  Keys should be protected in memory and only decrypted for the duration of time they are actively used. Keys should not be stored with the applications source-code repository  see previous remediation note.  After unlocking the signer key, we encrypt it again and keep it encrypted only decrypting it when signing. This way the raw private key only exist for a very short time in memory and will be filled with 0 right after. ( https://git.slock.it/in3/ts/in3-server/merge_requests/113/diffs#653b04fa41e35b55181776b9f14620b661cff64c_54_73 )  Use standard libraries for cryptographic operations  The following statement has been provided to address this issue  We are using ethereumjs-libs.  Use the system keystore and API to sign and avoid to store key material at all  The following statement has been provided to address this issue  We are looking into using different signer-apis, even supporting hardware-modules like HSMs. But this may happen in future releases.  The application should store the keys eth-address (util.getAddress()) instead of re-calculating it multiple times from the private key.  Fixed by generating the address for a private key once and storing it in a private key wrapper object.  Do not leak credentials and key material in debug-mode, to local log-output or external log aggregators.  txArgs still contains a field privateKey as outlined in the issue description. However, this privateKey now represents the wrapper object noted in a previous comment which only provides access to the ETH address generated from the raw private key.  The following statement has been provided to address this issue:  since the private key and the passphrase are actually deleted from the config, logoutputs or even debug will not be able to leak this information.  Description  Secure and efficient key management is a challenge for any cryptographic system. Incubed nodes for example require an account on the ethereum blockchain to actively participate in the incubed network. The account and therefore a private-key is used to sign transactions on the ethereum blockchain and to provide signed proofs to other in3-nodes.  This means that an attacker that is able to discover the keys used by an in3-server by any mechanism may be able to impersonate that node, steal the nodes funds or sign wrong data on behalf of the node which might also lead to a loss of funds.  The private key for the in3-server can be specified in a configuration file called config.json residing in the program working dir. Settings from the config.json can be overridden via command-line options. The application keeps configuration parameters available internally in an IN3RPCConfig object and passes this object as an initialization parameter to other objects.  The key can either be provided in plaintext as a hex-string starting with 0x or within an ethereum keystore format compatible protected keystore file. Either way it is provided it will be held in plaintext in the object.  The application accepts plaintext private keys and the keys are stored unprotected in the applications memory in JavaScript objects. The in3-server might even re-use the nodes private key which may weaken the security provided by the node. The repository leaks a series of presumably  test private keys  and the default config file already comes with a private key set that might be shared across unvary users that fail to override it.  code/in3-server/config.json:L1-L4  \"privateKey\": \"0xc858a0f49ce12df65031ba0eb0b353abc74f93f8ccd43df9682fd2e2293a4db3\",  \"rpcUrl\": \"http://rpc-kovan.slock.it\"  code/in3-server/package.json:L20-L31  \"docker-run\": \"docker run -p 8500:8500 docker.slock.it/slockit/in3-server:latest --privateKey=0x3858a0f49ce12df65031ba0eb0b353abc74f93f8ccd43df9682fd2e2293a4db3 --chain=0x2a --rpcUrl=https://kovan.infura.io/HVtVmCIHVgqHGUgihfhX --minBlockHeight=6 --registry=0x013b82355a066A31427df3140C5326cdE9c64e3A --persistentFile=false --logging-host=logs7.papertrailapp.com --logging-name=Papertrail --logging-port=30571 --logging-type=winston-papertrail\",  \"docker-setup\": \"docker run -p 8500:8500 slockit/in3-server:latest --privateKey=0x3858a0f49ce12df65031ba0eb0b353abc74f93f8ccd43df9682fd2e2293a4db3 --chain=0x2a --rpcUrl=https://kovan.infura.io/HVtVmCIHVgqHGUgihfhX --minBlockHeight=6 --registry=0x013b82355a066A31427df3140C5326cdE9c64e3A --persistentFile=false --autoRegistry-url=https://in3.slock.it/kovan1 --autoRegistry-capabilities-proof=true --autoRegistry-capabilities-multiChain=true --autoRegistry-deposit=1\",  \"local\": \"export NODE_ENV=0 && npm run build && node ./js/src/server/server.js --privateKey=0xD231FCF9349A296F555A060A619235F88650BBA795E5907CFD7F5442876250E4 --chain=0x2a --rpcUrl=https://rpc.slock.it/kovan --minBlockHeight=6 --registry=0x27a37a1210df14f7e058393d026e2fb53b7cf8c1 --persistentFile=false\",  \"ipfs\": \"docker run -d -p 5001:5001 jbenet/go-ipfs  daemon --offline\",  \"linkIn3\": \"cd node_modules; rm -rf in3; ln -s ../../in3 in3; cd ..\",  \"lint:solium\": \"node node_modules/ethlint/bin/solium.js -d contracts/\",  \"lint:solium:fix\": \"node node_modules/ethlint/bin/solium.js -d contracts/ --fix\",  \"lint:solhint\": \"node node_modules/solhint/solhint.js \\\"contracts/**/*.sol\\\" -w 0\",  \"local-env\": \"export NODE_ENV=0 && npm run build && node ./js/src/server/server.js --privateKey=0x9e53e6933d69a28a737943e227ad013c7489e366f33281d350c77f089d8411a6 --chain=0x111 --rpcUrl=http://localhost:8545 --minBlockHeight=6 --registry=0x31636f91297C14A8f1E7Ac271f17947D6A5cE098 --persistentFile=false --autoRegistry-url=http://127.0.0.1:8500 --autoRegistry-capabilities-proof=true --autoRegistry-capabilities-multiChain=true --autoRegistry-deposit=0\",  \"local-env2\": \"export NODE_ENV=0 && npm run build && node ./js/src/server/server.js --privateKey=0xf7db260e6edcdfe396d75f8283aad5aed835815f7d1db4458896310553a8a1a9 --chain=0x111 --rpcUrl=http://localhost:8545 --minBlockHeight=6 --registry=0x31636f91297C14A8f1E7Ac271f17947D6A5cE098 --persistentFile=false --autoRegistry-url=http://127.0.0.1:8501 --autoRegistry-capabilities-proof=true --autoRegistry-capabilities-multiChain=true --autoRegistry-deposit=0\",  \"local-env3\": \"export NODE_ENV=0 && npm run build && node ./js/src/server/server.js --privateKey=0xf7db260e6edcdfe396d75f8283aad5aed835815f7d1db4458896310553a8a1a9 --chain=0x5 --rpcUrl=https://rpc.slock.it/goerli --minBlockHeight=6 --registry=0x85613723dB1Bc29f332A37EeF10b61F8a4225c7e --persistentFile=false\",  \"local-env4\": \"export NODE_ENV=0 && npm run build && node ./js/src/server/server.js --privateKey=0xf7db260e6edcdfe396d75f8283aad5aed835815f7d1db4458896310553a8a1a9 --chain=0x2a --rpcUrl=https://rpc.slock.it/kovan --minBlockHeight=6 --registry=0x27a37a1210df14f7e058393d026e2fb53b7cf8c1 --persistentFile=false\"  The private key is also passed as arguments to other functions. In error cases these may leak the private key to log interfaces or remote log aggregation instances (sentry). See txargs.privateKey in the example below:  code/in3-server/src/util/tx.ts:L100-L100  const key = toBuffer(txargs.privateKey)  code/in3-server/src/util/tx.ts:L134-L140  const txHash = await transport.handle(url, {  jsonrpc: '2.0',  id: idCount++,  method: 'eth_sendRawTransaction',  params: [toHex(tx.serialize())]  }).then((_: RPCResponse) => _.error ? Promise.reject(new SentryError('Error sending tx', 'tx_error', 'Error sending the tx ' + JSON.stringify(txargs) + ':' + JSON.stringify(_.error))) as any : _.result + '')  Recommendation  Keys should never be stored or accepted in plaintext format.  Keys should not be stored in plaintext on the file-system as they might easily be exposed to other users. Credentials on the file-system must be tightly restricted by access control. Keys should not be provided as plaintext via environment variables as this might make them available to other processes sharing the same environment (child-processes, e.g. same shell session) Keys should not be provided as plaintext via command-line arguments as they might persist in the shell s command history or might be available to privileged system accounts that can query other processes startup parameters.  Keys should only be accepted in an encrypted and protected format.  A single key should be used for only one purpose. Keys should not be shared.  The use of the same key for two different cryptographic processes may weaken the security provided by one or both of the processes. The use of the same key for two different applications may weaken the security provided by one or both of the applications. Limiting the use of a key limits the damage that could be done if the key is compromised. Node owners keys should not be re-used as signer keys.  The application should support developers in understanding where cryptographic keys are stored within the application as well as in which memory regions they might be accessible for other applications.  Keys should be protected in memory and only decrypted for the duration of time they are actively used.  Keys should not be stored with the applications source-code repository.  Use standard libraries for cryptographic operations.  Use the system keystore and API to sign and avoid to store key material at all.  The application should store the keys eth-address (util.getAddress()) instead of re-calculating it multiple times from the private key.  Do not leak credentials and key material in debug-mode, to local log-output or external log aggregators.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.9 NodeRegistry - Multiple nodes can share slightly different RPC URL    ",
        "body": "  Resolution   Same mitigation as   issue 6.4.  Description  One of the requirements for Node registration is to have a unique URL which is not already used by a different owner. The uniqueness check is done by hashing the provided _url and checking if someone already registered with that hash of _url.  However, byte-equality checks (via hashing in this case) to enforce uniqueness will not work for URLs. For example, while the following URLs are not equal and will result in different urlHashes they can logically be the same end-point:  https://some-server.com/in3-rpc  https://some-server.com:443/in3-rpc  https://some-server.com/in3-rpc/  https://some-server.com/in3-rpc///  https://some-server.com/in3-rpc?something  https://some-server.com/in3-rpc?something&something  https://www.some-server.com/in3-rpc?something (if www resolves to the same ip)  code/in3-contracts/contracts/NodeRegistry.sol:L547-L553  bytes32 urlHash = keccak256(bytes(_url));  // make sure this url and also this owner was not registered before.  // solium-disable-next-line  require(!urlIndex[urlHash].used && signerIndex[_signer].stage == Stages.NotInUse,  \"a node with the same url or signer is already registered\");  This leads to the following attack vectors:  A user signs up multiple nodes that resolve to the same end-point (URL). A minimum deposit of 0.01 ether is required for each registration. Registering multiple nodes for the same end-point might allow an attacker to increase their chance of being picked to provide proofs. Registering multiple nodes requires unique signer addresses per node.  Also one node can have multiple accounts, hence one node can have slightly different URL and different accounts as the signers.  DoS - A user might register nodes for URLs that do not serve in3-clients in an attempt to DDoS e.g. in an attempt to extort web-site operators. This is kind of a reflection attack where nodes will request other nodes from the contract and try to contact them over RPC. Since it is http-rpc it will consume resources on the receiving end.  DoS - A user might register Nodes with RPC URLs of other nodes, manipulating weights to cause more traffic than the node can actually handle. Nodes will try to communicate with that node. If no proof is requested the node will not even know that someone else signed up other nodes with their RPC URL to cause problems. If they request proof the original signer will return a signed proof and the node will fail due to a signature mismatch. However, the node cannot be convicted and therefore forced to lose the deposit as conviction is bound the signer and the block was not signed by the rogue node entry. There will be no way to remove the node from the registry other than the admin functionality.  Recommendation  Canonicalize URLs, but that will not completely prevent someone from registering nodes for other end-points or websites. Nodes can be removed by an admin in the first year but not after that. Rogue owners cannot be prevented from registering random nodes with high weights and minimum deposit. They cannot be convicted as they do not serve proofs. Rogue owners can still unregister to receive their deposit after messing with the system.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.10 in3-server - should enforce safe settings for minBlockHeight   ",
        "body": "  Resolution  The default block is changed to 10 and minBlockHeight is added to the registry (as part of the properties) in 8c72633e, but allow the user to define a minBlockHeight lower than this number. The client is responsible to review the settings depending on how secure they want their nodes to be.  Client response:  We have discussed this, but decided to keep it flexible. This means:  We have put the minBlockHeight into the registry (as part of the properties). Because these properties indicate the limit and capabilities of the node and give the client a chance to filter out nodes if they don t match the requirements. So each client is able to filter out node who are not willing to take the risk and sign for example latest-6. Of course these nodes will most likely only store a low deposit ( you can not have a signature of a young block and a high deposit), but if you need a high security the nodes with a deposit will propably wait at least 10 or more blocks. In order to protect the owner of a node of using insecure settings, we will use our wizard to check the deposit and minBlockHeights and warn or educate the user. The reason why this flexibility is important, is because there use cases where dapps will not accept the let user wait 10 blocks before confirming a transaction. If the dapp developer needs a signature of a younger block, he will need to live with the fact, that he won t be able to find a high deposit to secure it.  We also changed the default to 10 blocks, but allow the user to define a minBlockHeight lower than this number. In this case the node would write a warning in the logfile, but still accepts the user configuration. This allows to use incubed also on different chains other than the mainnet.  The safeMinBlockHeight is now dependend on different chains, which is one single function, so we don t have hardcoded values in different places anymore.  Description  A node that is signing wrong blockhashes might get their deposit slashed from the registry. The entity that is convicting a node that signs a wrong blockhash is awarded half of the deposit.  A threat to this kind of system is that blocks might constantly be reorganized in the chain, especially with the latest block. Allowing a node to sign the latest block will definitely put the node s deposit at stake with every signature they provide.  A node can configure the minBlockHeight it is about to sign with a configurative option. The option defaults to a minBlockHeight of 6 in the default config:  code/in3-server/src/server/config.ts:L32-L32  minBlockHeight: 6,  And again in the signing function for blockheaders:  code/in3-server/src/chains/signatures.ts:L189-L189  const blockHeight = handler.config.minBlockHeight === undefined ? 6 : handler.config.minBlockHeight  handleSign will refuse to sign any block that is within the last 5 blocks. The 6th block will be signed.  code/in3-server/src/chains/signatures.ts:L190-L193  const tooYoungBlock = blockData.find(block => toNumber(blockNumber) - toNumber(block.number) < blockHeight)  if (tooYoungBlock)  throw new Error(' cannot sign for block ' + tooYoungBlock.number + ', because the blockHeight must be at least ' + blockHeight)  However, a user is not prevented from configuring an insecure minBlockHeight (e.g. 0) which will very likely lead to the loss of funds because the node will be signing the latest block.  Kraken requires at least 30 confirmation (abt. 6 minutes) until a transaction is confirmed. For Bitcoin it is said to be safe to wait more than 6 blocks (abt. 1 hr) for a transaction to be confirmed. ETC even underwent a  deep chain reorg that could have caused many nodes to lose their deposits. The  ethereum whitepaper defines an uncle that can be referenced in a block to have the following property:  Bitfinex requires a minimum of 10 confirmations. Some blockchain explorers and analytics tools also require a minimum of 10 confirmations. Scraped data from  https://etherscan.io/blocks_forked?ps=100 shows 3 forks of depth 3 since they started keeping records 115 days ago, and no forks deeper than 3. So some applications might legitimately pick a number somewhere between 5 and 20, trading some security for better UX. However, it should be re-evaluated whether the current default provides enough security to protect the nodes funds with a trade-off of lag to the network.  Given these values it is suggested to revalidate the default of a minBlockHeight of 6 in favor of a more secure depth to make sure that - with a default setting - nodes will not lose funds in case of re-orgs.  Recommendation  config.minBlockHeight should always be set to a sane value when loading the configuration. There should be no need to reset it to a hardcoded default value of 6 in handleSign. Do not hardcode the values in various places in the config.  normalize and sanitize the settings to make sure that after loading they are always valid and within reasonable bounds. the application should refuse to run with a minBlockHeader set to 0 as this is a guarantee for losing funds. Other nodes can enumerate nodes that are misconfigured (e.g. with minBlockHeight being 0) to request signatures just to convict them on micro-forks.  assume a secure default setting for every chain (note that this might be different for every chain). allow to override the value by the user. warn the user of less secure settings and do not allow to set settings that are obviously leading to the loss of funds.  re-evaluate the minBlockHeight of 6 for the ethereum blockchain and choose a koservative secure default.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.11 in3-server - rpc proof handler specification inconsistency    ",
        "body": "  Resolution   Addressed with   https://git.slock.it/in3/ts/in3-server/issues/100. Checks for  Description  According to the specification incubed requests must specify whether they want to have a proof or not. There are three variants of proofs that can be requested:  never - no proof appended  proof - proof but no signed blockhashes  proofWithSignature- proof and a request to sign blockhashes from the list of addresses provided in signatures.  Note that the name signatures for the array of signers a blockhash signature is requested from is misleading. It is actually signer addresses as listed in the NodeRegistry and not signatures.  Following the in3-server we found at least one inconsistency (and suspect more) with the proof requested by a client. The graceful check for the existence of something starting with proof will pass proof and proofWithSignature but also any other proofXYZ to the blockproof handler.  code/in3-server/src/modules/eth/EthHandler.ts:L106-L112  if (request.in3.verification.startsWith('proof'))  switch (request.method) {  case 'eth_getBlockByNumber':  case 'eth_getBlockByHash':  case 'eth_getBlockTransactionCountByHash':  case 'eth_getBlockTransactionCountByNumber':  return handleBlock(this, request)  Following through handleBlock we cannot find any check for proofWithSignature. The string is not found in the whole codebase which also suggests it is not tested. However, the code assumes that because request.in3.signatures is not empty, signatures were requested. This is inconsistent with the specification and a protocol violation.  code/in3-server/src/modules/eth/proof.ts:L237-L244  // create the proof  response.in3 = {  proof: {  type: 'blockProof',  signatures: await collectSignatures(handler, request.in3.signatures, [{ blockNumber: toNumber(blockData.number), hash: blockData.hash }], request.in3.verifiedHashes)  The same is valid for all other types of proofs. proofWithSignature is never checked and it is assumed that proofWithSignature was requested just because request.in3.signatures is present non-empty.  The same is true for  never  which is actually never handled in code.  Recommendation  The protocol should be strictly enforced without allowing any ambiguities and unsharpness. Ambiguities and gracefulness in the protocol can lead to severe inconsistencies and encourage client authors to not strictly adhere to the protocol. This makes it hard to update and maintain the protocol in the future and may allow potential attackers enough freedom to exploit the protocol. Furthermore the specification must be kept up-to-date at all times. The specification is to lead development and code must always be verified against the specification.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.12 in3-server - hardcoded gas limit could result in failed transactions/requests    ",
        "body": "  Resolution   Fixed by using web3   merge_requests/109 to dynamically price the gas according to the network state.  Description  There are many instances of hardcoded gas limit in in3-server that depending on the complexity of the transaction or gas cost changes in Ethereum could result in failed transactions.  Examples  convict():  code/in3-server/src/chains/signatures.ts:L132-L137  await callContract(handler.config.rpcUrl, nodes.contract, 'convict(uint,bytes32)', [s.block, convictSignature], {  privateKey: handler.config.privateKey,  gas: 500000,  value: 0,  confirm: true                       //  we are not waiting for confirmation, since we want to deliver the answer to the client.  })  recreateBlockheaders():  code/in3-server/src/chains/signatures.ts:L275-L280  await callContract(handler.config.rpcUrl, blockHashRegistry, 'recreateBlockheaders(uint,bytes[])', [latestSS - diffBlock, txArray], {  privateKey: handler.config.privateKey,  gas: 8000000,  value: 0,  confirm: true                       //  we are not waiting for confirmation, since we want to deliver the answer to the client.  })  Other instances of hard coded gasLimit or gasPrice:  code/in3-server/src/modules/eth/EthHandler.ts:L78-L79  if (!tx || (tx.gas && toNumber(tx.gas) > 10000000)) throw new Error('eth_call with a gaslimit > 10M are not allowed')  Recommendation  Use web3 gas estimate instead. To be sure, there can be an additional gas added to the estimated value or max(HARDCODED_GAS, estimated_amount)  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.13 in3-server - handleRecreation tries to recreate blockchain if no block is available to recreate it from    ",
        "body": "  Resolution  Mitigated by 502b5528 by falling back to using the current block in case searchForAvailableBlock returns 0. Costs can be zero, but cannot be negative anymore.  The behaviour of the IN3-server code is outside the scope of this audit. However, while verifying the fixes for this specific issue it was observed that the watch.ts:handleConvict() relies on a static hardcoded cost calculation. We further note that the cost calculation formula has an error and is missing parentheses to avoid that costs can be zero. We did not see a reason for the costs not to be allowed to be zero. Furthermore, costs are calculated based on the difference of the conviction block to the latest block. Actual recreation costs can be less if there is an available block in blockhashRegistry to recreate it from that is other than the latest block.  Description  A node that wants to convict another node for false proof must update the BlockhashRegistry for signatures provided in blocks older than the most recent 256 blocks. Only when the smart contract is able to verify that the signed blockhash is wrong the convicting node will be able to receive half of its deposit.  The in3-server implements an automated mechanism to recreate blockhashes. It first searches for an existing blockhash within a range of blocks. If one is found and it is profitable (gas spend vs. amount awarded) the node will try to recreate the blockchain updating the registry.  The call to searchForAvailableBlock might return 0 (default) because no block is actually found within the range, this will cause costs to be negative and the code will proceed trying to convict the node even though it cannot work.  The call to searchForAvailableBlock might also return the convict block number (latestSS==s.block) in which case costs will be 0 and the code will still proceed trying to recreate the blockheaders and convict the node.  code/in3-server/src/chains/signatures.ts:L207-L231  const [, deposit, , , , , , ,] = await callContract(handler.config.rpcUrl, nodes.contract, 'nodes(uint):(string,uint,uint64,uint64,uint128,uint64,address,bytes32)', [toNumber(singingNode.index)])  const latestSS = toNumber((await callContract(handler.config.rpcUrl, blockHashRegistry, 'searchForAvailableBlock(uint,uint):(uint)', [s.block, diffBlocks]))[0])  const costPerBlock = 86412400000000  const blocksMissing = latestSS - s.block  const costs = blocksMissing * costPerBlock * 1.25  if (costs > (deposit / 2)) {  console.log(\"not worth it\")  //it's not worth it  return  else {  // it's worth convicting the server  const blockrequest = []  for (let i = 0; i < blocksMissing; i++) {  blockrequest.push({  jsonrpc: '2.0',  id: i + 1,  method: 'eth_getBlockByNumber', params: [  toHex(latestSS - i), false  })  Please note that certain parts of the code rely on hardcoded gas values. Gas economics might change with future versions of the evm and have to be re-validated with every version. It is also good practice to provide inline comments about how and on what base certain values were selected.  Recommendation  Verify that the call succeeds and returns valid values. Check if the block already exists in the BlockhashRegistry and avoid recreation. Also note that searchForAvailableBlock can wrap with values close to uint_max even though that is unlikely to happen. In general, return values for external calls should be validated more rigorously.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.14 Impossible to remove malicious nodes after the initial period    ",
        "body": "  Resolution  This issue has been addressed with a large change-set that splits the NodeRegistry into two contracts, which results in a code flow that mitigates this issue by making the logic contract upgradable (after 47 days of notice). The resolution adds more complexity to the system, and this complexity is not covered by the original audit. Splitting up the contracts has the side-effect of events being emitted by two different contracts, requiring nodes to subscribe to both contracts  events.  The need for removing malicious nodes from the registry, arises from the design decision to allow anyone to register any URL. These URLs might not actually belong to the registrar of the URL and might not be IN3 nodes. This is partially mitigated by a centralization feature introduced in the mitigation phase that implements whitelist functionality for adding nodes.  We generally advocate against adding complexity, centralization and upgrading mechanisms that can allow one party to misuse functionalities of the contract system for their benefit (e.g. adminSetNodeDeposit is only used to reset the deposit but allows the Logic contract to set any deposit; the logic contract is set by the owner and there is a 47 day timelock).  We believe the solution to this issue, should have not been this complex. The trust model of the system is changed with this solution, now the logic contract can allow the admin a wide range of control over the system state and data.  The following statement has been provided with the change-set:  During the 1st year, we will keep the current mechanic even though it s a centralized approach. However, we changed the structure of the smart contracts and separated the NodeRegistry into two different smart contracts: NodeRegistryLogic and NodeRegistryData. After a successful deployment only the NodeRegistryLogic-contract is able to write data into the NodeRegistryData-contract. This way, we can keep the stored data (e.g. the nodeList) in the NodeRegistryData-contract while changing the way the data gets added/updated/removed is handled in the NodeRegistryLogic-contract. We also provided a function to update the NodeRegistryLogic-contract, so that we are able to change to a better solution for removing nodes in an updated contract.  Description  The system has centralized power structure for the first year after deployment. An unregisterKey (creator of the contract) is allowed to remove Nodes that are in state Stages.Active from the registry, only in 1st year.  However, there is no possibility to remove malicious nodes from the registry after that.  code/in3-contracts/contracts/NodeRegistry.sol:L249-L264  /// @dev only callable in the 1st year after deployment  function removeNodeFromRegistry(address _signer)  external  onlyActiveState(_signer)  // solium-disable-next-line security/no-block-members  require(block.timestamp < (blockTimeStampDeployment + YEAR_DEFINITION), \"only in 1st year\");// solhint-disable-line not-rely-on-time  require(msg.sender == unregisterKey, \"only unregisterKey is allowed to remove nodes\");  SignerInformation storage si = signerIndex[_signer];  In3Node memory n = nodes[si.index];  unregisterNodeInternal(si, n);  Recommendation  Provide a solution for the network to remove fraudulent node entries. This could be done by voting mechanism (with staking, etc).  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.15 NodeRegistry.registerNodeFor() no replay protection and expiration   ",
        "body": "  Resolution  This issue was addressed with the following statement:  In our understanding of the relationship between node-owner and signer the owner both are controlled by the very same entity, thus the owner should always know the privateKey of the signer. With this in mind a replay-protection would be useless, as the owner could always sign the necessary message. The reason why we separated the signer from the owner was to enable the possibility of owning an in3-node as with a multisig-account, as due to the nature of the exposal of the signer-key the possibility of it being leaked somehow is given (e.g. someone  hacks  the server), making the signer-key more unsecure. In addition, even though it s possible to replay the register as an owner it would unfeasable, as the owner would have to pay for the deposit anyway thus rendering the attack useless as there would be no benefit for an owner to do it.  Description  An owner can register a node with the signer not being the owner by calling registerNodeFor. The owner submits a message signed for the owner including the properties of the node including the url.  The signed data does not include the registryID nor the NodeRegistry s address and can therefore be used by the owner to submit the same node to multiple registries or chains without the signers consent.  The signed data does not expire and can be re-used by the owner indefinitely to submit the same node again to future contracts or the same contract after the node has been removed.  Arguments are not validated in the external function (also see issue 6.17)  code/in3-contracts/contracts/NodeRegistry.sol:L215-L223  bytes32 tempHash = keccak256(  abi.encodePacked(  _url,  _props,  _timeout,  _weight,  msg.sender  );  Recommendation  Include registryID and an expiration timestamp that is checked in the contract with the signed data. Validate function arguments.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.16 BlockhashRegistry - Structure of provided blockheaders should be validated    ",
        "body": "  Resolution  Mitigated by:  99f35fce - validating the block number in the provided RLP encoded input data  79e5a302 - fixes the potential out of bounds access for parentHash by requiring the the input to contain at least data up until including the parentHash in the user provided RLP blob. However, this check does not enforce that the minimum amount of data is available to extract the blockNumber  Additionally we would like to note the following:  While the code decodes the RLPLongList structure that contains the blockheader fields it does not decode the RLPLongString parentHash and just assumes one length-byte for it.  The length of the RLPLongString parentHash is never used but skipped instead.  The decoding is incomplete and fragile. The method does not attempt to decode other fields in the struct to verify that they are indeed valid RLP data. For the blockNumber extraction a fixed offset of 444 is assumed to access the difficulty RLP field (this might through as the minimum input length up to this field is not enforced). difficulty is then skipped and the blockNumber is accessed.  The minimum input data length enforced is shorter than a typical blockheader.  The code relies on implicit exceptions for out of bounds array access instead of verifying early on that enough input bytes are available to extract the required data.  We would also like to note that the commit referenced as mitigation does not appear to be based on the audit code.  Description  getParentAndBlockhash takes an rlp-encoded blockheader blob, extracts the parent parent hash and returns both the parent hash and the calculated blockhash of the provided data. The method is used to add blockhashes to the registry that are older than 256 blocks as they are not available to the evm directly. This is done by establishing a trust-chain from a blockhash that is already in the registry up to an older block  The method assumes that valid rlp encoded data is provided but the structure is not verified (rlp decodes completely; block number is correct; timestamp is younger than prevs, \u2026), giving a wide range of freedom to an attacker with enough hashing power (or exploiting potential future issues with keccak) to forge blocks that would never be accepted by clients, but may be accepted by this smart contract. (threat: mining pool forging arbitrary non-conformant blocks to exploit the BlockhashRegistry)  It is not checked that input was actually provided. However, accessing an array at an invalid index will raise an exception in the EVM. Providing a single byte > 0xf7 will yield a result and succeed even though it would have never been accepted by a real node.  It is assumed that the first byte is the rlp encoded length byte and an offset into the provided _blockheader bytes-array is calculated. Memory is subsequently accessed via a low-level mload at this calculated offset. However, it is never validated that the offset actually lies within the provided range of bytes _blockheader leading to an out-of-bounds memory read access.  The rlp encoded data is only partially decoded. For the first rlp list the number of length bytes is extracted. For the rlp encoded long string a length byte of 1 is assumed. The inline comment appears to be inaccurate or might be misleading. // we also have to add \"2\" = 1 byte to it to skip the length-information  Invalid intermediary blocks (e.g. with parent hash 0x00) will be accepted potentially allowing an attacker to optimize the effort needed to forge invalid blocks skipping to the desired blocknumber overwriting a certain blockhash (see issue 6.18)  With one collisions (very unlikely) an attacker can add arbitrary or even random values to the BlockchainRegistry. The parent-hash of the starting blockheader cannot be verified by the contract ([target_block_random]<--parent_hash--[rnd]<--parent_hash--[rnd]<--parent_hash--...<--parent_hash--[collision]<--parent_hash_collission--[anchor_block]). While nodes can verify block structure and bail on invalid structure and check the first blocks hash and make sure the chain is in-tact the contract can t. Therefore one cannot assume the same trust in the blockchain registry when recreating blocks compared to running a full node.  code/in3-contracts/contracts/BlockhashRegistry.sol:L98-L126  function getParentAndBlockhash(bytes memory _blockheader) public pure returns (bytes32 parentHash, bytes32 bhash) {  /// we need the 1st byte of the blockheader to calculate the position of the parentHash  uint8 first = uint8(_blockheader[0]);  /// calculates the offset  /// by using the 1st byte (usually f9) and substracting f7 to get the start point of the parentHash information  /// we also have to add \"2\" = 1 byte to it to skip the length-information  require(first > 0xf7, \"invalid offset\");  uint8 offset = first - 0xf7 + 2;  /// we are using assembly because it's the most efficent way to access the parent blockhash within the rlp-encoded blockheader  // solium-disable-next-line security/no-inline-assembly  assembly { // solhint-disable-line no-inline-assembly  // mstore to get the memory pointer of the blockheader to 0x20  mstore(0x20, _blockheader)  // we load the pointer we just stored  // then we add 0x20 (32 bytes) to get to the start of the blockheader  // then we add the offset we calculated  // and load it to the parentHash variable  parentHash :=mload(  add(  add(  mload(0x20), 0x20  ), offset)  bhash = keccak256(_blockheader);  Recommendation  Validate that the provided data is within a sane range of bytes that is expected (min/max blockheader sizes).  Validate that the provided data is actually an rlp encoded blockheader.  Validate that the offset for the parent Hash is within the provided data.  Validate that the parent Hash is non zero.  Validate that blockhashes do not repeat.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.17 Registries - Incomplete input validation and inconsistent order of validations   Pending",
        "body": "  Resolution  This issue describes general inconsistencies of the smart contract code base. The inconsistencies have been addressed with multiple change-sets:  Issues that have been addressed by the development team:  BlockhashRegistry.reCalculateBlockheaders - bhash can be zero; blockheaders can be empty  Fixed in 8d2bfa40 by adding the missing checks.  BlockhashRegistry.recreateBlockheaders - blockheaders can be empty; Arguments should be validated before calculating values that depend on them.  Fixed in 8d2bfa40 by adding the missing checks.  NodeRegistry.removeNode - should check require(_nodeIndex < nodes.length) first before any other action.  Fixed in 47255587 by adding the missing checks.  NodeRegistry.registerNodeFor - Signature version v should be checked to be either 27 || 28 before verifying it.  The fix in 47255587 introduced a serious typo (v != _v) that has been fixed with 4a0377c5 .  NodeRegistry.revealConvict - unchecked signer  Addressed with the comment that signer gets checked by ecrecover (slock.it/issue/10).  NodeRegistry.revealConvict - signer status can be checked earlier. Addressed with the following comment (slock.it/issue/10):  Due to the seperation of the contracts we will now check check the signatures and whether the blockhash is right. Only after this steps we will call into the NodeRegistryData contracts, thus potentially saving gas  NodeRegistry.updateNode - the check if the newURL is registered can be done earlier  Fixed in 4786a966.  BlockhashRegistry.getParentAndBlockhash- blockheader structure can be random as long as parenthash can be extracted  This issue has been reviewed as part of issue 6.16 (99f35fce).  Issues that have not been addressed by the development team and still persist:  BlockhashRegistry.searchForAvailableBlock - _startNumber + _numBlocks can be > block.number; _startNumber + _numBlocks can overflow. This issue has not been addressed.  General Notes:  Ideally commits directly reference issues that were raised during the audit. During the review of the mitigations provided with the change-sets for the listed issues we observed that change-sets contain changes that are not directly related to the issues. (e.g. 79e5a302)  Description  Methods and Functions usually live in one of two worlds:  public API - methods declared with visibility public or external exposed for interaction by other parties  internal API - methods declared with visibility internal, private that are not exposed for interaction by other parties  While it is good practice to visually distinguish internal from public API by following commonly accepted naming convention e.g. by prefixing internal functions with an underscore (_doSomething vs. doSomething) or adding the keyword unsafe to unsafe functions that are not performing checks and may have a dramatic effect to the system (_unsafePayout vs. RequestPayout), it is important to properly verify that inputs to methods are within expected ranges for the implementation.  Input validation checks should be explicit and well documented as part of the code s documentation. This is to make sure that smart-contracts are robust against erroneous inputs and reduce the potential attack surface for exploitation.  It is good practice to verify the methods input as early as possible and only perform further actions if the validation succeeds. Methods can be split into an external or public API that performs initial checks and subsequently calls an internal method that performs the action.  The following lists some public API methods that are not properly checking the provided data:  BlockhashRegistry.reCalculateBlockheaders - bhash can be zero; blockheaders can be empty  BlockhashRegistry.getParentAndBlockhash- blockheader structure can be random as long as parenthash can be extracted  BlockhashRegistry.recreateBlockheaders - blockheaders can be empty; Arguments should be validated before calculating values that depend on them:  code/in3-contracts/contracts/BlockhashRegistry.sol:L70-L70  assert(_blockNumber > _blockheaders.length);  BlockhashRegistry.searchForAvailableBlock - _startNumber + _numBlocks can be > block.number; _startNumber + _numBlocks can overflow.  NodeRegistry.removeNode - should check require(_nodeIndex < nodes.length) first before any other action.  code/in3-contracts/contracts/NodeRegistry.sol:L602-L609  function removeNode(uint _nodeIndex) internal {  // trigger event  emit LogNodeRemoved(nodes[_nodeIndex].url, nodes[_nodeIndex].signer);  // deleting the old entry  delete urlIndex[keccak256(bytes(nodes[_nodeIndex].url))];  uint length = nodes.length;  assert(length > 0);  NodeRegistry.registerNodeFor - Signature version v should be checked to be either 27 || 28 before verifying it.  code/in3-contracts/contracts/NodeRegistry.sol:L200-L212  function registerNodeFor(  string calldata _url,  uint64 _props,  uint64 _timeout,  address _signer,  uint64 _weight,  uint8 _v,  bytes32 _r,  bytes32 _s  external  payable  NodeRegistry.revealConvict - unchecked signer  code/in3-contracts/contracts/NodeRegistry.sol:L321-L321  SignerInformation storage si = signerIndex[_signer];  NodeRegistry.revealConvict - signer status can be checked earlier.  code/in3-contracts/contracts/NodeRegistry.sol:L344-L344  require(si.stage != Stages.Convicted, \"node already convicted\");  NodeRegistry.updateNode - the check if the newURL is registered can be done earlier  code/in3-contracts/contracts/NodeRegistry.sol:L444-L444  require(!urlIndex[newURl].used, \"url is already in use\");  Recommendation  Use Checks-Effects-Interactions pattern for all functions.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.18 BlockhashRegistry - recreateBlockheaders allows invalid parent hashes for intermediary blocks    ",
        "body": "  Resolution  Fixed by requiring valid parent hashes for blockheaders.  Description  It is assumed that a blockhash of 0x00 is invalid, but the method accepts intermediary parent hashes extracted from blockheaders that are zero when establishing the trust chain.  This may allow an attacker with enough hashing power to store a blockheader hash that is actually invalid on the real chain but accepted within this smart contract. This may even only be done temporarily to overwrite an existing hash for a short period of time (see https://github.com/ConsenSys/slockit-in3-audit-2019-09/issues/24).  code/in3-contracts/contracts/BlockhashRegistry.sol:L141-L147  for (uint i = 0; i < _blockheaders.length; i++) {  (calcParent, calcBlockhash) = getParentAndBlockhash(_blockheaders[i]);  if (calcBlockhash != currentBlockhash) {  return 0x0;  currentBlockhash = calcParent;  Recommendation  Stop processing the array of _blockheaders immediately if a blockheader is invalid.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.19 BlockhashRegistry - recreateBlockheaders succeeds and emits an event even though no blockheaders have been provided    ",
        "body": "  Resolution  Fixed the vulnerable scenarios by adding proper checks to:  Prevent passing empty _blockheaders in 8d2bfa40  Prevent storing the same blockhash twice in 80bb6ecf  Description  The method is used to re-create blockhashes from a list of rlp-encoded _blockheaders. However, the method never checks if _blockheaders actually contains items. The result is, that the method will unnecessarily store the same value that is already in the blockhashMapping at the same location and wrongly log LogBlockhashAdded even though nothing has been added nor changed.  assume _blockheaders is empty and the registry already knows the blockhash of _blockNumber  code/in3-contracts/contracts/BlockhashRegistry.sol:L61-L67  function recreateBlockheaders(uint _blockNumber, bytes[] memory _blockheaders) public {  bytes32 currentBlockhash = blockhashMapping[_blockNumber];  require(currentBlockhash != 0x0, \"parentBlock is not available\");  bytes32 calculatedHash = reCalculateBlockheaders(_blockheaders, currentBlockhash);  require(calculatedHash != 0x0, \"invalid headers\");  An attempt is made to re-calculate the hash of an empty _blockheaders array (also passing the currentBlockhash from the registry)  code/in3-contracts/contracts/BlockhashRegistry.sol:L66-L66  bytes32 calculatedHash = reCalculateBlockheaders(_blockheaders, currentBlockhash);  The following loop in reCalculateBlockheaders is skipped and the currentBlockhash is returned.  code/in3-contracts/contracts/BlockhashRegistry.sol:L134-L149  function reCalculateBlockheaders(bytes[] memory _blockheaders, bytes32 _bHash) public pure returns (bytes32 bhash) {  bytes32 currentBlockhash = _bHash;  bytes32 calcParent = 0x0;  bytes32 calcBlockhash = 0x0;  /// save to use for up to 200 blocks, exponential increase of gas-usage afterwards  for (uint i = 0; i < _blockheaders.length; i++) {  (calcParent, calcBlockhash) = getParentAndBlockhash(_blockheaders[i]);  if (calcBlockhash != currentBlockhash) {  return 0x0;  currentBlockhash = calcParent;  return currentBlockhash;  The assertion does not fire, the bnr to store the calculatedHash is the same as the one initially provided to the method as an argument.. Nothing has changed but an event is emitted.  code/in3-contracts/contracts/BlockhashRegistry.sol:L69-L74  /// we should never fail this assert, as this would mean that we were able to recreate a invalid blockchain  assert(_blockNumber > _blockheaders.length);  uint bnr = _blockNumber - _blockheaders.length;  blockhashMapping[bnr] = calculatedHash;  emit LogBlockhashAdded(bnr, calculatedHash);  Recommendation  The method is crucial for the system to work correctly and must be tightly controlled by input validation. It should not be allowed to overwrite an existing value in the contract (issue 6.29) or emit an event even though nothing has happened. Therefore validate that user provided input is within safe bounds. In this case, that at least one _blockheader has been provided. Validate that _blockNumber is less than block.number and do not expect that parts of the code will throw and safe the contract from exploitation.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.20 NodeRegistry.updateNode replaces signer with owner and emits inconsistent events    ",
        "body": "  Resolution  Reviewed merged changes at in3-contracts/5cb54165.  The method now emits a distinct event twice when node properties are updated.  The event correctly emits the signer.  When updating a node URL, the new URLInformation now correctly sets the signer.  However, there is a discrepancy between the process of registering a node and updating node s properties. When registering a node the owner has to provide a signed message containing the registration properties from the signer. Once the node is registered it can be unilaterally updated by the owner without requiring the signers permission to do so. According to slock.it it is assumed that the node owner and the signer are in control of the same entity and therefore this is not a concern.  Description  https://github.com/ConsenSys/slockit-in3-audit-2019-09/issues/36).  code/in3-contracts/contracts/NodeRegistry.sol:L438-L452  if (newURl != keccak256(bytes(node.url))) {  // deleting the old entry  delete urlIndex[keccak256(bytes(node.url))];  // make sure the new url is not already in use  require(!urlIndex[newURl].used, \"url is already in use\");  UrlInformation memory ui;  ui.used = true;  ui.signer = msg.sender;  urlIndex[newURl] = ui;  node.url = _url;  Furthermore, the method emits a LogNodeRegistered event when the node structure is updated. However, the event will always emit msg.sender as the signer even though that might not be true. For example, if the url does not change, the signer can still be another account that was previously registered with registerNodeFor and is not necessarily the owner.  code/in3-contracts/contracts/NodeRegistry.sol:L473-L478  emit LogNodeRegistered(  node.url,  _props,  msg.sender,  node.deposit  );  code/in3-contracts/contracts/NodeRegistry.sol:L30-L30  event LogNodeRegistered(string url, uint props, address signer, uint deposit);  Recommendation  The updateNode() function gets the signer as an input used to reference the node structure and this signer should be set for the UrlInformation.  function updateNode(  address _signer,  string calldata _url,  uint64 _props,  uint64 _timeout,  uint64 _weight  The method should actually only allow to change node properties when owner==signer otherwise updateNode is bypassing the strict requirements enforced with registerNodeFor where e.g. the url needs to be signed by the signer in order to register it.  The emitted event should always emit node.signer instead of msg.signer which can be wrong.  The method should emit its own distinct event LogNodeUpdated for audit purposes and to be able to distinguish new node registrations from node structure updates. This might also require software changes to client/node implementations to listen for node updates.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.21 NodeRegistry - In3Node memory n is never used    ",
        "body": "  Resolution   Fixed by removing the modifier and move the node-signer check to functions in   f1fd7943  Description  NodeRegistry In3Node memory n is never used inside the modifier onlyActiveState.  code/in3-contracts/contracts/NodeRegistry.sol:L125-L133  modifier onlyActiveState(address _signer) {  SignerInformation memory si = signerIndex[_signer];  require(si.stage == Stages.Active, \"address is not an in3-signer\");  In3Node memory n = nodes[si.index];  assert(nodes[si.index].signer == _signer);  _;  Recommendation  Use n in the assertion to access the node signer assert(n.signer == _signer);  or directly access it from storage and avoid copying the struct.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.22 NodeRegistry - returnDeposit and transferOwnership should emit an event    ",
        "body": "  Resolution   Fixed in   f1fd7943 by adding new events (  Description  Important state changing functions should emit an event for the purpose of having an audit trail and being able to monitor the smart contract usage and performance.  Recommendation  Emit events for returnDeposit and transferOwnership.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.23 in3-server - in3_stats leaks information   ",
        "body": "  Resolution   There is a config-option   Description  in3_stat shows information from node activities in the currentMonth, currentDay, currentHour which can result in leaking information about the functionality that node is being used for. This information might be valuable when an attacker wants to find out how utilized a node is and if any reflection attacks are successful (https://github.com/ConsenSys/slockit-in3-audit-2019-09/issues/50).  Examples  'profile': AttributeDict({  'name': 'Slockit2',  'icon': 'https://slock.it/assets/slock_logo.png',  'url': 'https://slock.it'  }),  'stats': Attr  ibuteDict({  'upSince': 1568400626355,  'currentMonth': AttributeDict({  'requests': 47618,  'lastRequest': 1569422025347,  'methods': AttributeDict({  'nd-2': 2,  'eth_call': 940,  'eth_blockNumber': 25,  'eth_getBlockByNumber': 45395,  'web3_clientVersion': 386,  'admin_datadir': 7,  'admin_peers': 11,  'shh_version': 7,  'shh_info': 14,  'admin_nodeInfo ': 9, '  txpool_status ': 9, '  personal_listAccounts ': 3, '  eth_chainId ': 12, '  eth_protocolVersion ': 6, '  net_listening ': 6, '  net_peerCount ': 6, '  eth_syncing ': 6,  'eth_mining': 6,  'eth_hashrate': 6,  'eth_gasPrice': 18,  'eth_coinbase': 44,  'eth_accounts': 54,  'eth_getBalance': 321,  'personal_unlockAccount': 61,  'personal_  importRawKey ': 5, '  personal_newAccount ': 8, '  eth_estimateGas ': 16, '  eth_sendRawTransaction ': 9, '  eth_getTransactionReceipt ': 49, '  in3_sign ': 59, '  eth_getCode ': 33,  'eth_getTransactionCount': 15,  'eth_getLogs': 8,  'in3_stats': 16,  'in3_validatorlist': 15,  'in3_nodeList': 15,  'in3_call': 15,  'proof_in3_sign': 1  })  }),  'currentDay': AttributeDict({  'requests': 144,  'lastRequest': 1569422025347,  'methods': AttributeDict({  'eth_getBlockByNumber': 135,  'web3_clientVersion': 6,  'eth_coinbase': 1,  'eth_accounts': 1,  'in3_stats': 1  })  }),  'currentHour': AttributeDict({  'requests': 144,  'lastRequest': 1569422025346,  'methods': AttributeDict({  'eth_getBlockByNumber': 135,  'web3_clientVersion': 6,  'eth_coinbase': 1,  'eth_accounts': 1,  'in3_stats': 1  })  })  })  Recommendation  Make sure if this information is needed, if not enable it just for debugging purposes.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.24 NodeRegistry - removeNode unnecessarily casts the nodeIndex to uint64 potentially truncating its value    ",
        "body": "  Resolution   Fixed as per recommendation   https://git.slock.it/in3/in3-contracts/commit/6c35dd422e27eec1b1d2f70e328268014cadb515.  Description  removeNode removes a node from the Nodes array. This is done by copying the last node of the array to the _nodeIndex of the node that is to be removed. Finally the node array size is decreased.  A Node s index is also referenced in the SignerInformation struct. This index needs to be adjusted when removing a node from the array as the last node is copied to the index of the node that is to be removed.  code/in3-contracts/contracts/NodeRegistry.sol:L60-L69  struct SignerInformation {  uint64 lockedTime;                  /// timestamp until the deposit of an in3-node can not be withdrawn after the node was removed  address owner;                      /// the owner of the node  Stages stage;                       /// state of the address  uint depositAmount;                 /// amount of deposit to be locked, used only after a node had been removed  uint index;                         /// current index-position of the node in the node-array  code/in3-contracts/contracts/NodeRegistry.sol:L614-L620  // move the last entry to the removed one.  In3Node memory m = nodes[length - 1];  nodes[_nodeIndex] = m;  SignerInformation storage si = signerIndex[m.signer];  si.index = uint64(_nodeIndex);  nodes.length--;  Recommendation  Do not cast and therefore truncate the index.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.25 Registries - general inconsistencies   Pending",
        "body": "  Resolution  The breakdown of the fixes are as follows:  NodeRegistry - check for addr(0) being passed. This is anyway only done in the constructor and will not require a lot of gas.  The proper checks for registry addresses are added in 4786a966.  NodeRregistry - unnecessary payable  Removed payable modifier everywhere, as ERC20 support is added to the system. ERC20 support is not part of this audit.  NodeRegistry - deposit checks can be combined into one function to make the code more readable. The min deposit amount could be exposed as public const to allow other entities to query the contracts minimum deposit similar to the max ether amount. MAX_ETHER_LIMIT should make clear that this limit is only applicable in the first year (e.g. MAX_ETHER_LIMIT_FIRST_YEAR) .  Fixed and variables renamed.  NodeRegistry - require(si.owner == msg.sender) can be checked before accessing the nodes array  Added proper checks in c9e75b35.  NodeRegistry - removeNode resets index to a valid node array index of 0. Even though the code will access the index it is good practice to set this to an invalid value to make sure it raises an error condition if it is wrongly accessed in a future revision of the code. This is mainly a safeguard. Another option is to invalidate the 0 index.  Although the index is not set to 0, this issue is not yet fixed (Follow up here).  NodeRegistry - implicitly set defaults are hard to maintain. This should be a constant state variable that can be queried to be transparent about minimum and maximum values. Prefer throwing an exception instead of automatically setting the value to a minimum as this might be unexpected by a client and can cover error conditions.  timeout has been removed, so this is obsolete as it is not in the new code anymore.  NodeRegistry - one year startup period: instead of storing the deployment timestamp the contract should store the end-of-admin-timestamp.  Fixed as recommended timestampAdminKeyActive = block.timestamp + YEAR_DEFINITION;  NodeRegistry - inefficient re-calculation of hash  Fixed (issues/16).  NodeRegistry - weight should be part of proofHash  Added in 9fa5548d.  NodeRegistry - updateNode if the new timeout is smaller than the current timeout it will silently be ignored. This may be unexpected by the caller and cover error conditions where a client provides wrong inputs. Raising an exception should be preferred in such cases instead of gracefully assuming values.  Fixed by removing timeout variable (issues/16).  NodeRegistry - admin functionality should be clearly named as such for transparency reasons (e.g. adminRemovenodeFromRegistry) .  Renamed all admin function in both contracts with prefix admin.  Description  NodeRegistry - check for addr(0) being passed. This is anyway only done in the constructor and will not require a lot of gas.  code/in3-contracts/contracts/NodeRegistry.sol:L138-L139  constructor(BlockhashRegistry _blockRegistry) public {  blockRegistry = _blockRegistry;  NodeRregistry - unnecessary payable  code/in3-contracts/contracts/NodeRegistry.sol:L535-L535  address payable _owner,  NodeRegistry - deposit checks can be combined into one function to make the code more readable. The min deposit amount could be exposed as public const to allow other entities to query the contracts minimum deposit similar to the max ether amount. MAX_ETHER_LIMIT should make clear that this limit is only applicable in the first year (e.g. MAX_ETHER_LIMIT_FIRST_YEAR) .  code/in3-contracts/contracts/NodeRegistry.sol:L543-L545  require(_deposit >= 10 finney, \"not enough deposit\");  checkNodeProperties(_deposit, _timeout);  code/in3-contracts/contracts/NodeRegistry.sol:L120-L120  uint constant internal MAX_ETHER_LIMIT = 50 ether;  NodeRegistry - require(si.owner == msg.sender) can be checked before accessing the nodes array  code/in3-contracts/contracts/NodeRegistry.sol:L402-L404  SignerInformation storage si = signerIndex[_signer];  In3Node memory n = nodes[si.index];  require(si.owner == msg.sender, \"only for the in3-node owner\");  NodeRegistry - removeNode resets index to a valid node array index of 0. Even though the code will access the index it is good practice to set this to an invalid value to make sure it raises an error condition if it is wrongly accessed in a future revision of the code. This is mainly a safeguard. Another option is to invalidate the 0 index.  code/in3-contracts/contracts/NodeRegistry.sol:L612-L612  signerIndex[nodes[_nodeIndex].signer].index = 0;  NodeRegistry - implicitly set defaults are hard to maintain. This should be a constant state variable that can be queried to be transparent about minimum and maximum values. Prefer throwing an exception instead of automatically setting the value to a minimum as this might be unexpected by a client and can cover error conditions.  code/in3-contracts/contracts/NodeRegistry.sol:L565-L565  m.timeout = _timeout > 1 hours ? _timeout : 1 hours;  NodeRegistry - one year startup period: instead of storing the deployment timestamp the contract should store the end-of-admin-timestamp.  code/in3-contracts/contracts/NodeRegistry.sol:L256-L256  require(block.timestamp < (blockTimeStampDeployment + YEAR_DEFINITION), \"only in 1st year\");// solhint-disable-line not-rely-on-time  NodeRegistry - inefficient re-calculation of hash  code/in3-contracts/contracts/NodeRegistry.sol:L438-L441  if (newURl != keccak256(bytes(node.url))) {  // deleting the old entry  delete urlIndex[keccak256(bytes(node.url))];  NodeRegistry - weight should be part of proofHash  code/in3-contracts/contracts/NodeRegistry.sol:L490-L502  function calcProofHash(In3Node memory _node) internal pure returns (bytes32) {  return keccak256(  abi.encodePacked(  _node.deposit,  _node.timeout,  _node.registerTime,  _node.props,  _node.signer,  _node.url  );  NodeRegistry - updateNode if the new timeout is smaller than the current timeout it will silently be ignored. This may be unexpected by the caller and cover error conditions where a client provides wrong inputs. Raising an exception should be preferred in such cases instead of gracefully assuming values.  code/in3-contracts/contracts/NodeRegistry.sol:L463-L465  if (_timeout > node.timeout) {  node.timeout = _timeout;  NodeRegistry - admin functionality should be clearly named as such for transparency reasons (e.g. adminRemovenodeFromRegistry) .  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.26 BlockhashRegistry- assembly code can be optimized    ",
        "body": "  Resolution   Fixed as per recommendation with   https://git.slock.it/in3/in3-contracts/commit/87f02a7c4f5c30d2b4be42f331c1306e85d42ca6.  Description  The following code can be optimized by removing mload and mstore:  code/in3-contracts/contracts/BlockhashRegistry.sol:L106-L125  require(first > 0xf7, \"invalid offset\");  uint8 offset = first - 0xf7 + 2;  /// we are using assembly because it's the most efficent way to access the parent blockhash within the rlp-encoded blockheader  // solium-disable-next-line security/no-inline-assembly  assembly { // solhint-disable-line no-inline-assembly  // mstore to get the memory pointer of the blockheader to 0x20  mstore(0x20, _blockheader)  // we load the pointer we just stored  // then we add 0x20 (32 bytes) to get to the start of the blockheader  // then we add the offset we calculated  // and load it to the parentHash variable  parentHash :=mload(  add(  add(  mload(0x20), 0x20  ), offset)  Recommendation  assembly { // solhint-disable-line no-inline-assembly  // mstore to get the memory pointer of the blockheader to 0x20  //mstore(0x20, _blockheader)  //@audit should assign 0x20ptr to variable first and use it.  // we load the pointer we just stored  // then we add 0x20 (32 bytes) to get to the start of the blockheader  // then we add the offset we calculated  // and load it to the parentHash variable  parentHash :=mload(  add(  add(  _blockheader, 0x20  ), offset)  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.27 Experimental Compiler features are enabled - ABIEncoderV2   ",
        "body": "  Resolution  This issue has been addressed with the following statement:  In order to pass structs between contracts we need that new ABIEncoder. [..] The old NodeRegistry did not require the ABIEncoderV2. [..] But due to the separation of the contracts in Logic and Data we are passing certain data-structures between contracts.  Description  The smart contracts enable experimental compiler features. Please note that these features are experimental for a reason and should be avoided unless explicitly required.  code/in3-contracts/contracts/BlockhashRegistry.sol:L21-L21  pragma experimental ABIEncoderV2;  Seems that NodeRegistry does not require any ABIEncoderV2 specific functionality.  code/in3-contracts/contracts/NodeRegistry.sol:L21-L21  pragma experimental ABIEncoderV2;  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.28 BlockhashRegistry - recreateBlockheaders() should use the evm provided blockhash when applicable   Pending",
        "body": "  Resolution  The provided code-change at 79fa3ef1 is not addressing the raised concerns. As noted in the recommendation it is suggested to completely skip the recreation routine if the target blockhash (_blockNumber.sub(_blockheaders.length)) is available to the evm. The method should call saveBlockNumber(_blockNumber) instead.  The commit attempts to add a verification for extracted blockhashes from the user provided RLP data if the blockhash for the block is available. However, the variable name currentBlock is misleading making it hard to follow the authors intent.  Description  There are different levels of trust attached to blockhashes stored in the BlockhashRegistry. On one side there are blockhashes which data-source is the evm ( blockhash(blocknumber)) and on the other side there are blockhashes that have been fed into the system by recalculating block-headers and establishing a trust chain to an already existing blockhash in the contract. While the contract can trust the result of blockhash(blocknumber) for the most recent 256 blocks because the information is coming directly from the evm, blockhashes that are re-created by calling recreateBlockheaders are manually verified and trust relies on the proper validation of the chain of block-headers provided.  Side-effect: Also saves gas by avoiding unnecessary calculations within the recreateBlockheaders() codepath as blockhash is already available via evm.  Recommendation  recreateBlockheaders() should prefer to use blockhash(number) by calling saveBlockNumber() instead of re-calculating the blockhash from the user provided chain of blockheaders, if the blockhash can easily be accessed by the evm (most recent 256 blocks, except current block). Check if _blockheaders.length > 0 && _blockNumber.sub(_blockheaders.length) < block.number-256.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "6.29 BlockhashRegistry - Existing blockhashes can be overwritten    ",
        "body": "  Resolution   Addressed with   80bb6ecf and  17d450cf by checking if blockhash exists and changing the  Description  Last 256 blocks, that are available in the EVM environment, are stored in BlockhashRegistry by calling snapshot() or saveBlockNumber(uint _blockNumber) functions. Older blocks are recreated by calling recreateBlockheaders.  The methods will overwrite existing blockhashes.  code/in3-contracts/contracts/BlockhashRegistry.sol:L79-L87  function saveBlockNumber(uint _blockNumber) public {  bytes32 bHash = blockhash(_blockNumber);  require(bHash != 0x0, \"block not available\");  blockhashMapping[_blockNumber] = bHash;  emit LogBlockhashAdded(_blockNumber, bHash);  code/in3-contracts/contracts/BlockhashRegistry.sol:L72  blockhashMapping[bnr] = calculatedHash;  Recommendation  If a block is already saved in the smart contract, it can be checked and a SSTORE can be prevented to save gas. Require that blocknumber hash is not stored.  require(blockhashMapping[_blockNumber] == 0x0, \"block already saved\");  7 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "7.1 MythX",
        "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  Below is the raw output of the MythX vulnerability scan:  \"issues\": [  \"swcID\": \"SWC-127\",  \"swcTitle\": \"\",  \"description\": {  \"head\": \"jump to arbitrary destination\",  \"tail\": \"A caller can trigger a jump to an arbitrary destination. Make sure this does not enable unintended control flow.\"  },  \"severity\": \"High\",  \"locations\": [  \"sourceMap\": \"20901:1:1\",  \"sourceType\": \"raw-bytecode\",  \"sourceFormat\": \"evm-byzantium-bytecode\",  \"sourceList\": [  \"0x63ad5e3d2c8551cf64f6d0425940efdeb79801907fcad157d1c82922919c13cb\",  \"0xd8d70c0998b3293c364b1cde922c80081d70d02726e74091c8aae6fa6a10b892\"  },  \"sourceMap\": \"23263:248:-1\",  \"sourceType\": \"solidity-file\",  \"sourceFormat\": \"text\",  \"sourceList\": [  \"NodeRegistry.sol\"  ],  \"extra\": {  \"discoveryTime\": 5593089348,  \"testCases\": [  \"initialState\": {  \"accounts\": {  \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa0\": {  \"nonce\": 0,  \"balance\": \"0x00000000000000000000000000000000000000ffffffffffffffffffffffffff\",  \"code\": \"\",  \"storage\": {}  },  \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa1\": {  \"nonce\": 1,  \"balance\": \"0x0000000000000000000000000000000000000000000000000000000000000000\",  \"code\": \"\",  \"storage\": {}  },  \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa2\": {  \"nonce\": 1,  \"balance\": \"0x00000000000000000000000000000000000000ffffffffffffffffffffffffff\",  \"code\": \"\",  \"storage\": {}  },  \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa3\": {  \"nonce\": 1,  \"balance\": \"0x00000000000000000000000000000000000000ffffffffffffffffffffffffff\",  \"code\": \"0x00\",  \"storage\": {}  },  \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa4\": {  \"nonce\": 1,  \"balance\": \"0x00000000000000000000000000000000000000ffffffffffffffffffffffffff\",  \"code\": \"0xfd\",  \"storage\": {}  },  \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa5\": {  \"nonce\": 1,  \"balance\": \"0x00000000000000000000000000000000000000ffffffffffffffffffffffffff\",  \"code\": \"0x608060405260005600a165627a7a72305820466f8a1bdae15c60b8e998fe04836ef505803cfbd8edd29bd4679531357576530029\",  \"storage\": {}  },  \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa6\": {  \"nonce\": 1,  \"balance\": \"0x00000000000000000000000000000000000000ffffffffffffffffffffffffff\",  \"code\": \"0x608060405273aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa63081146038578073ffffffffffffffffffffffffffffffffffffffff16ff5b5000fea165627a7a723058205e8b906b72ad42c69b05acf4542283b6080ae82562bc74baac467daac2fb0e0e0029\",  \"storage\": {}  },  \"0xaffeaffeaffeaffeaffeaffeaffeaffeaffeaffe\": {  \"nonce\": 0,  \"balance\": \"0x0000000000000000000000000000000000ffffffffffffffffffffffffffffff\",  \"code\": \"\",  \"storage\": {}  },  \"steps\": [  \"address\": \"\",  \"gasLimit\": \"0xffffff\",  \"gasPrice\": \"0x773594000\",  \"input\": REMOVED,  \"origin\": \"0xaffeaffeaffeaffeaffeaffeaffeaffeaffeaffe\",  \"value\": \"0x0\",  \"blockCoinbase\": \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa0\",  \"blockDifficulty\": \"0xa7d7343662e26\",  \"blockGasLimit\": \"0xffffff\",  \"blockNumber\": \"0x661a55\",  \"blockTime\": \"0x5be99aa8\"  },  \"address\": \"0x0901d12ebe1b195e5aa8748e62bd7734ae19b51f\",  \"gasLimit\": \"0x7d00\",  \"gasPrice\": \"0x773594000\",  \"input\": \"0xac48987300000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\",  \"origin\": \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa0\",  \"value\": \"0x9\",  \"blockCoinbase\": \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa0\",  \"blockDifficulty\": \"0x52c054bfb494c\",  \"blockGasLimit\": \"0x7d0000\",  \"blockNumber\": \"0x661a55\",  \"blockTime\": \"0x5be99aa8\"  ],  \"toolName\": \"harvey\"  },  \"swcID\": \"SWC-120\",  \"swcTitle\": \"Weak Sources of Randomness from Chain Attributes\",  \"description\": {  \"head\": \"Potential use of a weak source of randomness \\\"blockhash\\\".\",  \"tail\": \"Using past or the current block hashes through \\\"blockhash\\\" as a source of randomness is predictable. The issue can be ignored if this is unrelated to randomness or if the usage is related to a secure implementation of a commit/reveal scheme.\"  },  \"severity\": \"Medium\",  \"locations\": [  \"sourceMap\": \"6746:25:0\",  \"sourceType\": \"solidity-file\",  \"sourceFormat\": \"text\",  \"sourceList\": [  \"NodeRegistry.sol\"  ],  \"extra\": {  \"discoveryTime\": 1666071716,  \"toolName\": \"maru\"  },  \"swcID\": \"SWC-120\",  \"swcTitle\": \"Weak Sources of Randomness from Chain Attributes\",  \"description\": {  \"head\": \"Potential use of a weak source of randomness \\\"blockhash\\\".\",  \"tail\": \"Using past or the current block hashes through \\\"blockhash\\\" as a source of randomness is predictable. The issue can be ignored if this is unrelated to randomness or if the usage is related to a secure implementation of a commit/reveal scheme.\"  },  \"severity\": \"Medium\",  \"locations\": [  \"sourceMap\": \"13158:23:0\",  \"sourceType\": \"solidity-file\",  \"sourceFormat\": \"text\",  \"sourceList\": [  \"NodeRegistry.sol\"  ],  \"extra\": {  \"discoveryTime\": 1666159516,  \"toolName\": \"maru\"  },  \"swcID\": \"SWC-120\",  \"swcTitle\": \"Weak Sources of Randomness from Chain Attributes\",  \"description\": {  \"head\": \"Potential use of a weak source of randomness \\\"block.number\\\".\",  \"tail\": \"Using past or the current block hashes through \\\"block.number\\\" as a source of randomness is predictable. The issue can be ignored if this is unrelated to randomness or if the usage is related to a secure implementation of a commit/reveal scheme.\"  },  \"severity\": \"Medium\",  \"locations\": [  \"sourceMap\": \"6756:12:0\",  \"sourceType\": \"solidity-file\",  \"sourceFormat\": \"text\",  \"sourceList\": [  \"NodeRegistry.sol\"  ],  \"extra\": {  \"discoveryTime\": 1666984722,  \"toolName\": \"maru\"  },  \"swcID\": \"SWC-120\",  \"swcTitle\": \"Weak Sources of Randomness from Chain Attributes\",  \"description\": {  \"head\": \"Potential use of a weak source of randomness \\\"block.number\\\".\",  \"tail\": \"Using past or the current block hashes through \\\"block.number\\\" as a source of randomness is predictable. The issue can be ignored if this is unrelated to randomness or if the usage is related to a secure implementation of a commit/reveal scheme.\"  },  \"severity\": \"Medium\",  \"locations\": [  \"sourceMap\": \"7532:12:0\",  \"sourceType\": \"solidity-file\",  \"sourceFormat\": \"text\",  \"sourceList\": [  \"NodeRegistry.sol\"  ],  \"extra\": {  \"discoveryTime\": 1667012822,  \"toolName\": \"maru\"  },  \"swcID\": \"SWC-120\",  \"swcTitle\": \"Weak Sources of Randomness from Chain Attributes\",  \"description\": {  \"head\": \"Potential use of a weak source of randomness \\\"block.number\\\".\",  \"tail\": \"Using past or the current block hashes through \\\"block.number\\\" as a source of randomness is predictable. The issue can be ignored if this is unrelated to randomness or if the usage is related to a secure implementation of a commit/reveal scheme.\"  },  \"severity\": \"Medium\",  \"locations\": [  \"sourceMap\": \"13711:12:0\",  \"sourceType\": \"solidity-file\",  \"sourceFormat\": \"text\",  \"sourceList\": [  \"NodeRegistry.sol\"  ],  \"extra\": {  \"discoveryTime\": 1667019122,  \"toolName\": \"maru\"  ],  \"sourceType\": \"raw-bytecode\",  \"sourceFormat\": \"evm-byzantium-bytecode\",  \"sourceList\": [  \"0x63ad5e3d2c8551cf64f6d0425940efdeb79801907fcad157d1c82922919c13cb\",  \"0xd8d70c0998b3293c364b1cde922c80081d70d02726e74091c8aae6fa6a10b892\"  ],  \"meta\": {  \"selectedCompiler\": \"Unknown\",  \"logs\": [],  \"toolName\": \"maru\",  \"coveredPaths\": 91,  \"coveredInstructions\": 7058  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "7.2 Ethlint",
        "body": "  Ethlint is an open source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  contracts/BlockhashRegistry.sol  21:0     warning    Avoid using experimental features in production code    no-experimental  46:12    warning    Line exceeds the limit of 145 characters                max-len  61:1     error      Line contains trailing whitespace                       no-trailing-whitespace  79:4     warning    Line exceeds the limit of 145 characters                max-len  81:1     error      Line contains trailing whitespace                       no-trailing-whitespace  98:4     warning    Line exceeds the limit of 145 characters                max-len  134:4    warning    Line exceeds the limit of 145 characters                max-len  142:1    error      Line contains trailing whitespace                       no-trailing-whitespace  contracts/NodeRegistry.sol  21:0     warning    Avoid using experimental features in production code    no-experimental  117:1    error      Line contains trailing whitespace                       no-trailing-whitespace  123:1    error      Line contains trailing whitespace                       no-trailing-whitespace  128:8    warning    Line exceeds the limit of 145 characters                max-len  143:1    error      Line contains trailing whitespace                       no-trailing-whitespace  143:8    warning    Line exceeds the limit of 145 characters                max-len  152:1    error      Line contains trailing whitespace                       no-trailing-whitespace  152:4    warning    Line exceeds the limit of 145 characters                max-len  197:1    error      Line contains trailing whitespace                       no-trailing-whitespace  200:1    error      Line contains trailing whitespace                       no-trailing-whitespace  215:1    error      Line contains trailing whitespace                       no-trailing-whitespace  224:1    error      Line contains trailing whitespace                       no-trailing-whitespace  324:1    error      Line contains trailing whitespace                       no-trailing-whitespace  342:1    error      Line contains trailing whitespace                       no-trailing-whitespace  448:1    error      Line contains trailing whitespace                       no-trailing-whitespace  555:2    error      Line contains trailing whitespace                       no-trailing-whitespace  555:8    warning    Line exceeds the limit of 145 characters                max-len  568:1    error      Line contains trailing whitespace                       no-trailing-whitespace  571:1    error      Line contains trailing whitespace                       no-trailing-whitespace  602:1    error      Line contains trailing whitespace                       no-trailing-whitespace  615:1    error      Line contains trailing whitespace                       no-trailing-whitespace  \u2716 19 errors, 10 warnings found.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "7.3 Surya",
        "body": "  Surya is an utility tool for smart contract systems. It provides a number of visual outputs and information about structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  Function Name  Visibility  Mutability  Modifiers  NodeRegistry  Implementation  <Constructor>  Public    convict  External    NO   registerNode  External    NO   registerNodeFor  External    NO   removeNodeFromRegistry  External    onlyActiveState  returnDeposit  External    NO   revealConvict  External    NO   transferOwnership  External    onlyActiveState  unregisteringNode  External    onlyActiveState  updateNode  External    onlyActiveState  totalNodes  External    NO   calcProofHash  Internal \ud83d\udd12  checkNodeProperties  Internal \ud83d\udd12  registerNodeInternal  Internal \ud83d\udd12  unregisterNodeInternal  Internal \ud83d\udd12  removeNode  Internal \ud83d\udd12  BlockhashRegistry  Implementation  <Constructor>  Public    searchForAvailableBlock  External    NO   recreateBlockheaders  Public    NO   saveBlockNumber  Public    NO   snapshot  Public    NO   getParentAndBlockhash  Public    NO   reCalculateBlockheaders  Public    NO   Legend  Function can modify state  Function is payable  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "7.4 Other Tools",
        "body": "  Other security tools such as Slither was also used to identify problems in the smart contract.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "7.5 Test Coverage",
        "body": "  Code coverage metrics indicate the amount of lines/statements/branches that are covered by the test-suite. It s important to note that  100% test coverage  does not indicate the code has no vulnerabilities. Be aware that code coverage does not provide information about the individual test-cases quality.  A fork of the Solidity-Coverage tool was used to measure the portion of the code base exercised by the test suite, and identify areas with little or no coverage. Specific sections of the code where necessary test coverage is missing are included in the Issue Details section.  The project is using the automated testing framework provided by Truffle. The test-suite is evaluating 62 individual tests and the test-suite passed without errors. The corresponding console output can be found here.  A code coverage report was generated and is provided along other tool output. The test coverage results for NodeRegistry.sol can be viewed here. The test coverage results for BlockhashRegistry.sol can be viewed here. Please find a summary of the coverage results below.  BlockhashRegistry.sol  100%  30/30  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "92.86%",
        "body": "  13/14  100%  7/7  100%  31/31  NodeRegistry.sol  100%  123/123  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "95.45%",
        "body": "  63/66  100%  17/17  100%  129/129  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"
    },
    {
        "title": "2.1 Accounts that claim incentives immediately before the migration will be stuck ",
        "body": "  Description  For accounts that existed before the migration to the new incentive calculation, the following happens when they claim incentives for the first time after the migration: First, the incentives that are still owed from before the migration are computed according to the old formula; the incentives since the migration are calculated according to the new logic, and the two values are added together. The first part   calculating the pre-migration incentives according to the old formula   happens in function MigrateIncentives.migrateAccountFromPreviousCalculation; the following lines are of particular interest in the current context:  code-582dc37/contracts/external/MigrateIncentives.sol:L39-L50  uint256 timeSinceMigration = finalMigrationTime - lastClaimTime;  // (timeSinceMigration * INTERNAL_TOKEN_PRECISION * finalEmissionRatePerYear) / YEAR  uint256 incentiveRate =  timeSinceMigration  .mul(uint256(Constants.INTERNAL_TOKEN_PRECISION))  // Migration emission rate is stored as is, denominated in whole tokens  .mul(finalEmissionRatePerYear).mul(uint256(Constants.INTERNAL_TOKEN_PRECISION))  .div(Constants.YEAR);  // Returns the average supply using the integral of the total supply.  uint256 avgTotalSupply = finalTotalIntegralSupply.sub(lastClaimIntegralSupply).div(timeSinceMigration);  The division in the last line will throw if finalMigrationTime and lastClaimTime are equal. This will happen if an account claims incentives immediately before the migration happens   where  immediately  means in the same block. In such a case, the account will be stuck as any attempt to claim incentives will revert.  Recommendation  The function should return 0 if finalMigrationTime and lastClaimTime are equal. Moreover, the variable name timeSinceMigration is misleading, as the variable doesn t store the time since the migration but the time between the last incentive claim and the migration.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/notional-protocol-v2.1/"
    },
    {
        "title": "2.2 type(T).max is inclusive ",
        "body": "  Description  Throughout the codebase, there are checks whether a number can be represented by a certain type.  Examples  code-582dc37/contracts/internal/nToken/nTokenSupply.sol:L71  require(accumulatedNOTEPerNToken < type(uint128).max); // dev: accumulated NOTE overflow  code-582dc37/contracts/internal/nToken/nTokenSupply.sol:L134  require(blockTime < type(uint32).max); // dev: block time overflow  code-582dc37/contracts/external/patchfix/MigrateIncentivesFix.sol:L86-L87  require(totalSupply <= type(uint96).max);  require(blockTime <= type(uint32).max);  Sometimes these checks use <=, sometimes they use <.  Recommendation  type(T).max is inclusive, i.e., it is the greatest number that can be represented with type T. Strictly speaking, it can and should therefore be used consistently with <= instead of <.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/notional-protocol-v2.1/"
    },
    {
        "title": "2.3  mathematical mistake in comment ",
        "body": "  Description  In nTokenSupply.sol, there is a comment explaining why 18 decimal places for the accumulation precision is a good choice. There is a minor mistake in the calculation. It does not invalidate the reasoning, but as it is confusing for a reader, we recommend correcting it.  code-582dc37/contracts/internal/nToken/nTokenSupply.sol:L85-L88  // If we use 18 decimal places as the accumulation precision then we will overflow uint128 when  // a single nToken has accumulated 3.4 x 10^20 NOTE tokens. This isn't possible since the max  // NOTE that can accumulate is 10^17 (100 million NOTE in 1e8 precision) so we should be safe  // using 18 decimal places and uint128 storage slot  100 million NOTE in 1e8 precision is 10^16.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/notional-protocol-v2.1/"
    },
    {
        "title": "6.1 Unexpected response in an assimilator s external call can lock-up the whole system    ",
        "body": "  Resolution  Comment from the development team:  When this was brought to our attention, it made the most sense to look at it from a bird s eye view. In the event that an assimilator does seize up either due to smart contract malfunctioning or to some type of governance decision in one of our dependencies, then depending on the severity of the event, it could either make it so that that particular dependency is unable to be transacted with or it could brick the pool altogether.  In the case of the latter severity where the pool is bricked altogether for an extended period of time, then this means the end of that particular pool s life. In this case, we find it prudent to allow for the withdrawal of any asset still functional from the pool. Should such an event transpire, we have instituted functionality to allow users to withdraw individually from the pool s assets according to their Shell balances without being exposed to the inertia of the incapacitated assets.  In such an event, the owner of the pool can now trigger a partitioned state which is an end of life state for the pool in which users send Shells as normal until they decide to redeem any portion of them, after which they will only be able to redeem the portion of individual asset balances their Shell balance held claims on.  Description  The assimilators, being the  middleware  between a shell and all the external DeFi systems it interacts with, perform several external calls within their methods, as would be expected.  An example of such a contract is mainnetSUsdToASUsdAssimilator.sol (the contract can be found here).  The problem outlined in the title arises from the fact that Solidity automatically checks for the successful execution of the underlying message call (i.e., it bubbles up assertions and reverts) and, therefore, if any of these external systems changes in unexpected ways the call to the shell will revert itself.  This problem is immensely magnified by the fact that all the external methods in Loihi dealing with deposits, withdraws, and swaps rebalance the pool and, as a consequence, all of the assimilators for the reserve tokens get called at some point.  In summary, if any of the reserve tokens start, for some reason, refusing to complete a call to some of their methods, the whole protocol stops working, and the tokens are locked in forever (this is assuming the development team removes the safeApprove function from Loihi, v. https://github.com/ConsenSys/shell-protocol-audit-2020-06/issues/10).  Recommendation  There is no easy solution to this problem since calls to these external systems cannot simply be ignored. Shell needs successful responses from the reserve assimilators to be able to function properly.  One possible mitigation is to create a trustless mechanism based on repeated misbehavior by an external system to be able to remove a reserve asset from the pool.  Such a design could consist of an external function accessible to all actors that needs X confirmations over a period of Y blocks (or days, for that matter) with even spacing between them to be able to remove a reserve asset.  This means that no trust to the owners is implied (since this would require the extreme power to take user s tokens) and still maintains the healthy option of being able to remove faulty tokens from the pool.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"
    },
    {
        "title": "6.2 Certain functions lack input validation routines    ",
        "body": "  Resolution  Comment from the development team:  Now all functions in the Orchestrator revert on incorrect arguments.  All functions in Loihi in general revert on incorrect arguments.  Description  The functions should first check if the passed arguments are valid first. The checks-effects-interactions pattern should be implemented throughout the code.  These checks should include, but not be limited to:  uint should be larger than 0 when 0 is considered invalid  uint should be within constraints  int should be positive in some cases  length of arrays should match if more arrays are sent as arguments  addresses should not be 0x0  Examples  The function includeAsset does not do any checks before changing the contract state.  src/Loihi.sol:L59-L61  function includeAsset (address _numeraire, address _nAssim, address _reserve, address _rAssim, uint256 _weight) public onlyOwner {  shell.includeAsset(_numeraire, _nAssim, _reserve, _rAssim, _weight);  The internal function called by the public method includeAsset again doesn t check any of the data.  src/Controller.sol:L77-L97  function includeAsset (Shells.Shell storage shell, address _numeraire, address _numeraireAssim, address _reserve, address _reserveAssim, uint256 _weight) internal {  Assimilators.Assimilator storage _numeraireAssimilator = shell.assimilators[_numeraire];  _numeraireAssimilator.addr = _numeraireAssim;  _numeraireAssimilator.ix = uint8(shell.numeraires.length);  shell.numeraires.push(_numeraireAssimilator);  Assimilators.Assimilator storage _reserveAssimilator = shell.assimilators[_reserve];  _reserveAssimilator.addr = _reserveAssim;  _reserveAssimilator.ix = uint8(shell.reserves.length);  shell.reserves.push(_reserveAssimilator);  shell.weights.push(_weight.divu(1e18).add(uint256(1).divu(1e18)));  Similar with includeAssimilator.  src/Loihi.sol:L63-L65  function includeAssimilator (address _numeraire, address _derivative, address _assimilator) public onlyOwner {  shell.includeAssimilator(_numeraire, _derivative, _assimilator);  Again no checks are done in any function.  src/Controller.sol:L99-L106  function includeAssimilator (Shells.Shell storage shell, address _numeraire, address _derivative, address _assimilator) internal {  Assimilators.Assimilator storage _numeraireAssim = shell.assimilators[_numeraire];  shell.assimilators[_derivative] = Assimilators.Assimilator(_assimilator, _numeraireAssim.ix);  // shell.assimilators[_derivative] = Assimilators.Assimilator(_assimilator, _numeraireAssim.ix, 0, 0);  Not only does the administrator functions not have any checks, but also user facing functions do not check the arguments.  For example swapByOrigin does not check any of the arguments if you consider it calls MainnetDaiToDaiAssimilator.  src/Loihi.sol:L85-L89  function swapByOrigin (address _o, address _t, uint256 _oAmt, uint256 _mTAmt, uint256 _dline) public notFrozen returns (uint256 tAmt_) {  return transferByOrigin(_o, _t, _dline, _mTAmt, _oAmt, msg.sender);  It calls transferByOrigin and we simplify this example and consider we have _o.ix == _t.ix  src/Loihi.sol:L181-L187  function transferByOrigin (address _origin, address _target, uint256 _dline, uint256 _mTAmt, uint256 _oAmt, address _rcpnt) public notFrozen nonReentrant returns (uint256 tAmt_) {  Assimilators.Assimilator memory _o = shell.assimilators[_origin];  Assimilators.Assimilator memory _t = shell.assimilators[_target];  // TODO: how to include min target amount  if (_o.ix == _t.ix) return _t.addr.outputNumeraire(_rcpnt, _o.addr.intakeRaw(_oAmt));  In which case it can call 2 functions on an assimilatior such as MainnetDaiToDaiAssimilator.  The first called function is intakeRaw.  src/assimilators/mainnet/daiReserves/mainnetDaiToDaiAssimilator.sol:L42-L49  // transfers raw amonut of dai in, wraps it in cDai, returns numeraire amount  function intakeRaw (uint256 _amount) public returns (int128 amount_, int128 balance_) {  dai.transferFrom(msg.sender, address(this), _amount);  amount_ = _amount.divu(1e18);  And its result is used in outputNumeraire that again does not have any checks.  src/assimilators/mainnet/daiReserves/mainnetDaiToDaiAssimilator.sol:L83-L92  // takes numeraire amount of dai, unwraps corresponding amount of cDai, transfers that out, returns numeraire amount  function outputNumeraire (address _dst, int128 _amount) public returns (uint256 amount_) {  amount_ = _amount.mulu(1e18);  dai.transfer(_dst, amount_);  return amount_;  Recommendation  Implement the checks-effects-interactions as a pattern to write code. Add tests that check if all of the arguments have been validated.  Consider checking arguments as an important part of writing code and developing the system.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"
    },
    {
        "title": "6.3 Remove Loihi methods that can be used as backdoors by the administrator    ",
        "body": "  Resolution  Issue was partly addressed by the development team. However, the feature to add new assimilators is still present and that ultimately means that the administrators have power to run arbitrary bytecode.  Updated remediation response Since the development team still hadn t fully settled on a strategy for a mainnet launch, this was left as a residue even after the audit mitigation phase. However, at launch time, this issue was no longer present and all the assimilators are now defined at deploy-time, it is fully resolved.  Description  There are several functions in Loihi that give extreme powers to the shell administrator. The most dangerous set of those is the ones granting the capability to add assimilators.  Since assimilators are essentially a proxy architecture to delegate code to several different implementations of the same interface, the administrator could, intentionally or unintentionally, deploy malicious or faulty code in the implementation of an assimilator. This means that the administrator is essentially totally trusted to not run code that, for example, drains the whole pool or locks up the users  and LPs  tokens.  In addition to these, the function safeApprove allows the administrator to move any of the tokens the contract holds to any address regardless of the balances any of the users have.  This can also be used by the owner as a backdoor to completely drain the contract.  src/Loihi.sol:L643-L649  function safeApprove(address _token, address _spender, uint256 _value) public onlyOwner {  (bool success, bytes memory returndata) = _token.call(abi.encodeWithSignature(\"approve(address,uint256)\", _spender, _value));  require(success, \"SafeERC20: low-level call failed\");  Recommendation  Remove the safeApprove function and, instead, use a trustless escape-hatch mechanism like the one suggested in issue 6.1.  For the assimilator addition functions, our recommendation is that they are made completely internal, only callable in the constructor, at deploy time.  Even though this is not a big structural change (in fact, it reduces the attack surface), it is, indeed, a feature loss. However, this is the only way to make each shell a time-invariant system.  This would not only increase Shell s security but also would greatly improve the trust the users have in the protocol since, after deployment, the code is now static and auditable.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"
    },
    {
        "title": "6.4 Assimilators should implement an interface    ",
        "body": "  Resolution  Comment from the development team:  They now implement the interface in  src/interfaces/IAssimilator.sol .  Description  The Assimilators are one of the core components within the application. They are used to move the tokens and can be thought of as a  middleware  between the Shell Protocol application and any other supported tokens.  The methods attached to the assimilators are called throughout the application and they are a critical component of the whole system. Because of this fact, it is extremely important that they behave correctly.  A suggestion to restrict the possibility of errors when implementing them and when using them is to make all of the assimilators implement a unique specific interface. This way, any deviation would be immediately observed, right when the compilation happens.  Examples  Consider this example. The user calls swapByOrigin.  src/Loihi.sol:L85-L89  function swapByOrigin (address _o, address _t, uint256 _oAmt, uint256 _mTAmt, uint256 _dline) public notFrozen returns (uint256 tAmt_) {  return transferByOrigin(_o, _t, _dline, _mTAmt, _oAmt, msg.sender);  Which calls transferByOrigin. In transferByOrigin, if the origin index matches the target index, a different execution branch is activated.  src/Loihi.sol:L187  if (_o.ix == _t.ix) return _t.addr.outputNumeraire(_rcpnt, _o.addr.intakeRaw(_oAmt));  In this case we need the output of _o.addr.intakeRaw(_oAmt).  If we pick a random assimilator and check the implementation, we see the function intakeRaw needs to return the transferred amount.  src/assimilators/mainnet/daiReserves/mainnetCDaiToDaiAssimilator.sol:L52-L67  // takes raw cdai amount, transfers it in, calculates corresponding numeraire amount and returns it  function intakeRaw (uint256 _amount) public returns (int128 amount_) {  bool success = cdai.transferFrom(msg.sender, address(this), _amount);  if (!success) revert(\"CDai/transferFrom-failed\");  uint256 _rate = cdai.exchangeRateStored();  _amount = ( _amount * _rate ) / 1e18;  cdai.redeemUnderlying(_amount);  amount_ = _amount.divu(1e18);  However, with other implementations, the returns do not match. In the case of MainnetDaiToDaiAssimilator, it returns 2 values, which will make the Loihi contract work in this case but can misbehave in other cases, or even fail.  src/assimilators/mainnet/daiReserves/mainnetDaiToDaiAssimilator.sol:L42-L49  // transfers raw amonut of dai in, wraps it in cDai, returns numeraire amount  function intakeRaw (uint256 _amount) public returns (int128 amount_, int128 balance_) {  dai.transferFrom(msg.sender, address(this), _amount);  amount_ = _amount.divu(1e18);  Making all the assimilators implement one unique interface will enforce the functions to look the same from the outside.  Recommendation  Create a unique interface for the assimilators and make all the contracts implement that interface.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"
    },
    {
        "title": "6.5 Assimilators do not conform to the ERC20 specification    ",
        "body": "  Resolution  Comment from the development team:  All calls to compliant ERC20s now check for return booleans. Non compliant ERC20s are called with a function that checks for the success of the call.  Description  The assimilators in the codebase make heavy usage of both the transfer and transferFrom methods in the ERC20 standard.  Quoting the relevant parts of the specification of the standard:  Transfers _value amount of tokens to address _to, and MUST fire the Transfer event. The function SHOULD throw if the message caller s account balance does not have enough tokens to spend.  The transferFrom method is used for a withdraw workflow, allowing contracts to transfer tokens on your behalf. This can be used for example to allow a contract to transfer tokens on your behalf and/or to charge fees in sub-currencies. The function SHOULD throw unless the _from account has deliberately authorized the sender of the message via some mechanism.  We can see that, even though it is suggested that ERC20-compliant tokens do throw on the lack of authorization from the sender or lack of funds to complete the transfer, the standard does not enforce it.  This means that, in order to make the system both more resilient and future-proof, code in each implementation of current and future assimilators should check for the return value of both transfer and transferFrom call instead of just relying on the external contract to revert execution.  The extent of this issue is only mitigated by the fact that new assets are only added by the shell administrator and could, therefore, be audited prior to their addition.  Non-exhaustive Examples  src/assimilators/mainnet/daiReserves/mainnetDaiToDaiAssimilator.sol:L45  dai.transferFrom(msg.sender, address(this), _amount);  src/assimilators/mainnet/daiReserves/mainnetDaiToDaiAssimilator.sol:L64  dai.transfer(_dst, _amount);  Recommendation  Add a check for the return boolean of the function.  Example:  require(dai.transferFrom(msg.sender, address(this), _amount) == true);  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"
    },
    {
        "title": "6.6 Access to assimilators does not check for existence and allows delegation to the zeroth address    ",
        "body": "  Resolution  Comment from the development team:  All retrieval of assimilators now check that the assimilators address is not the zeroth address.  Description  For every method that allows to selectively withdraw, deposit, or swap tokens in Loihi, the user is allowed to specify addresses for the assimilators of said tokens (by inputting the addresses of the tokens themselves).  The shell then performs a lookup on a mapping called assimilators inside its main structure and uses the result of that lookup to delegate call the assimilator deployed by the shell administrator.  However, there are no checks for prior instantiation of a specific, supported token, effectively meaning that we can do a lookup on an all-zeroed-out member of that mapping and delegate call execution to the zeroth address.  The only thing preventing execution from going forward in this unwanted fashion is the fact that the ABI decoder expects a certain return data size from the interface implemented in Assimilator.sol.  For example, the 32 bytes expected as a result of this call:  src/Assimilators.sol:L58-L66  function viewNumeraireAmount (address _assim, uint256 _amt) internal returns (int128 amt_) {  // amount_ = IAssimilator(_assim).viewNumeraireAmount(_amt); // for production  bytes memory data = abi.encodeWithSelector(iAsmltr.viewNumeraireAmount.selector, _amt); // for development  amt_ = abi.decode(_assim.delegate(data), (int128)); // for development  This is definitely an insufficient check since the interface for the assimilators might change in the future to include functions that have no return values.  Recommendation  Check for the prior instantiation of assimilators by including the following requirement:  require(shell.assimilators[<TOKEN_ADDRESS>].ix != 0);  In all the functions that access the assimilators mapping and change the indexes to be 1-based instead pf 0-based.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"
    },
    {
        "title": "6.7 Math library s fork has problematic changes    ",
        "body": "  Description  The math library ABDK Libraries for Solidity was forked and modified to add a few unsafe_* functions.  unsafe_add  unsafe_sub  unsafe_mul  unsafe_div  unsafe_abs  The problem which was introduced is that unsafe_add ironically is not really unsafe, it is as safe as the original add function. It is, in fact, identical to the safe add function.  src/ABDKMath64x64.sol:L102-L113  /**  Calculate x + y.  Revert on overflow.  @param x signed 64.64-bit fixed point number  @param y signed 64.64-bit fixed point number  @return signed 64.64-bit fixed point number  /  function add (int128 x, int128 y) internal pure returns (int128) {  int256 result = int256(x) + y;  require (result >= MIN_64x64 && result <= MAX_64x64);  return int128 (result);  src/ABDKMath64x64.sol:L115-L126  /**  Calculate x + y.  Revert on overflow.  @param x signed 64.64-bit fixed point number  @param y signed 64.64-bit fixed point number  @return signed 64.64-bit fixed point number  /  function unsafe_add (int128 x, int128 y) internal pure returns (int128) {  int256 result = int256(x) + y;  require (result >= MIN_64x64 && result <= MAX_64x64);  return int128 (result);  Fortunately, unsafe_add is not used anywhere in the code.  However, unsafe_abs was changed from this:  src/ABDKMath64x64.sol:L322-L331  /**  Calculate |x|.  Revert on overflow.  @param x signed 64.64-bit fixed point number  @return signed 64.64-bit fixed point number  /  function abs (int128 x) internal pure returns (int128) {  require (x != MIN_64x64);  return x < 0 ? -x : x;  To this:  src/ABDKMath64x64.sol:L333-L341  /**  Calculate |x|.  Revert on overflow.  @param x signed 64.64-bit fixed point number  @return signed 64.64-bit fixed point number  /  function unsafe_abs (int128 x) internal pure returns (int128) {  return x < 0 ? -x : x;  The check that was removed, is actually an important check:  require (x != MIN_64x64);  src/ABDKMath64x64.sol:L19  int128 private constant MIN_64x64 = -0x80000000000000000000000000000000;  The problem is that for an int128 variable that is equal to -0x80000000000000000000000000000000, there is no absolute value within the constraints of int128.  Recommendation  Remove unused unsafe_* functions and try to find other ways of doing unsafe math (if it is fundamentally important) without changing existing, trusted, already audited code.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"
    },
    {
        "title": "6.8 Use one file for each contract or library    ",
        "body": "  Resolution  Issue fixed by the development team.  Description  The repository contains a lot of contracts and libraries that are added in the same file as another contract or library.  Organizing the code in this manner makes it hard to navigate, develop and audit. It is a best practice to have each contract or library in its own file. The file also needs to bear the name of the hosted contract or library.  Examples  src/Shells.sol:L20  library SafeERC20Arithmetic {  src/Shells.sol:L32  library Shells {  src/Loihi.sol:L26-L28  contract ERC20Approve {  function approve (address spender, uint256 amount) public returns (bool);  src/Loihi.sol:L30  contract Loihi is LoihiRoot {  src/Assimilators.sol:L19  library Delegate {  src/Assimilators.sol:L33  library Assimilators {  Recommendation  Split up contracts and libraries in single files.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"
    },
    {
        "title": "6.9 Remove debugging code from the repository    ",
        "body": "  Resolution  Issue fixed but he development team.  Description  Throughout the repository, there is source code from the development stage that was used for debugging the functionality and was not removed.  This should not be present in the source code and even if they are used while functionality is developed, they should be removed after the functionality was implemented.  Examples  src/Shells.sol:L63-L67  event log(bytes32);  event log_int(bytes32, int256);  event log_ints(bytes32, int256[]);  event log_uint(bytes32, uint256);  event log_uints(bytes32, uint256[]);  src/Assimilators.sol:L44-L46  event log(bytes32);  event log_uint(bytes32, uint256);  event log_int(bytes32, int256);  src/Controller.sol:L33-L37  event log(bytes32);  event log_int(bytes32, int128);  event log_int(bytes32, int);  event log_uint(bytes32, uint);  event log_addr(bytes32, address);  src/LoihiRoot.sol:L53  event log(bytes32);  src/Shells.sol:L63-L67  event log(bytes32);  event log_int(bytes32, int256);  event log_ints(bytes32, int256[]);  event log_uint(bytes32, uint256);  event log_uints(bytes32, uint256[]);  src/Loihi.sol:L470-L474  event log_int(bytes32, int);  event log_ints(bytes32, int128[]);  event log_uint(bytes32, uint);  event log_uints(bytes32, uint[]);  event log_addrs(bytes32, address[]);  src/assimilators/mainnet/cdaiReserves/mainnetDaiToCDaiAssimilator.sol:L35-L36  event log_uint(bytes32, uint256);  event log_int(bytes32, int256);  src/assimilators/mainnet/cusdcReserves/mainnetUsdcToCUsdcAssimilator.sol:L38  event log_uint(bytes32, uint256);  src/Loihi.sol:L51  shell.testHalts = true;  src/LoihiRoot.sol:L79-L83  function setTestHalts (bool _testOrNotToTest) public {  shell.testHalts = _testOrNotToTest;  src/Shells.sol:L60  bool testHalts;  Recommendation  Remove the debug functionality at the end of the development cycle of each functionality.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"
    },
    {
        "title": "6.10 Tests should not fail    ",
        "body": "  Resolution  Comment from the development team:  The failing tests are because we made minute changes to our present model (changes in applying the base fee -  epsilon ), so in a sense, rather than failing they just need updating. Many of them are also an artifact of architecting the tests in such a way that they can be run against arbitrary parameter sets - or in different  suites .  Description  The role of the tests should be to make sure the application behaves properly. This should include positive tests (functionality that should be implemented) and negative tests (behavior stopped or limited by the application).  The test suite should pass 100% of the tests. After spending time with the development team, we managed to ask for the changes that allowed us to run the tests suite. This revealed that out of the 555 tests, 206 are failing. This staggering number does not allow us to check what the problem is and makes anybody running tests ignore them completely.  Tests should be an integral part of the codebase, and they should be considered as important (or even more important) than the code itself. One should be able to recreate the whole codebase by just having the tests.  Recommendation  Update tests in order for the whole of the test suite to pass.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"
    },
    {
        "title": "6.11 Remove commented out code from the repository    ",
        "body": "  Description  Having commented out code increases the cognitive load on an already complex system. Also, it hides the important parts of the system that should get the proper attention, but that attention gets to be diluted.  There is no code that is important enough to be left commented out in a repository. Git branching should take care of having different code versions or diffs should show what was before.  If there is commented out code, this also has to be maintained; it will be out of date if other parts of the system are changed, and the tests will not pick that up.  The main problem is that commented code adds confusion with no real benefit. Code should be code, and comments should be comments.  Examples  Commented out code should be removed or dealt with in a separate branch that is later included in the master branch.  src/Assimilators.sol:L48-L56  function viewRawAmount (address _assim, int128 _amt) internal returns (uint256 amount_) {  // amount_ = IAssimilator(_assim).viewRawAmount(_amt); // for production  bytes memory data = abi.encodeWithSelector(iAsmltr.viewRawAmount.selector, _amt.abs()); // for development  amount_ = abi.decode(_assim.delegate(data), (uint256)); // for development  src/Assimilators.sol:L58-L66  function viewNumeraireAmount (address _assim, uint256 _amt) internal returns (int128 amt_) {  // amount_ = IAssimilator(_assim).viewNumeraireAmount(_amt); // for production  bytes memory data = abi.encodeWithSelector(iAsmltr.viewNumeraireAmount.selector, _amt); // for development  amt_ = abi.decode(_assim.delegate(data), (int128)); // for development  src/Assimilators.sol:L58-L66  function viewNumeraireAmount (address _assim, uint256 _amt) internal returns (int128 amt_) {  // amount_ = IAssimilator(_assim).viewNumeraireAmount(_amt); // for production  bytes memory data = abi.encodeWithSelector(iAsmltr.viewNumeraireAmount.selector, _amt); // for development  amt_ = abi.decode(_assim.delegate(data), (int128)); // for development  src/Controller.sol:L99-L106  function includeAssimilator (Shells.Shell storage shell, address _numeraire, address _derivative, address _assimilator) internal {  Assimilators.Assimilator storage _numeraireAssim = shell.assimilators[_numeraire];  shell.assimilators[_derivative] = Assimilators.Assimilator(_assimilator, _numeraireAssim.ix);  // shell.assimilators[_derivative] = Assimilators.Assimilator(_assimilator, _numeraireAssim.ix, 0, 0);  src/Loihi.sol:L596-L618  function transfer (address _recipient, uint256 _amount) public nonReentrant returns (bool) {  // return shell.transfer(_recipient, _amount);  function transferFrom (address _sender, address _recipient, uint256 _amount) public nonReentrant returns (bool) {  // return shell.transferFrom(_sender, _recipient, _amount);  function approve (address _spender, uint256 _amount) public nonReentrant returns (bool success_) {  // return shell.approve(_spender, _amount);  function increaseAllowance(address _spender, uint256 _addedValue) public returns (bool success_) {  // return shell.increaseAllowance(_spender, _addedValue);  function decreaseAllowance(address _spender, uint256 _subtractedValue) public returns (bool success_) {  // return shell.decreaseAllowance(_spender, _subtractedValue);  function balanceOf (address _account) public view returns (uint256) {  // return shell.balances[_account];  src/test/deposits/suiteOne.t.sol:L15-L29  // function test_s1_selectiveDeposit_noSlippage_balanced_10DAI_10USDC_10USDT_2p5SUSD_NO_HACK () public logs_gas {  //     uint256 newShells = super.noSlippage_balanced_10DAI_10USDC_10USDT_2p5SUSD();  //     assertEq(newShells, 32499999216641686631);  // }  // function test_s1_selectiveDeposit_noSlippage_balanced_10DAI_10USDC_10USDT_2p5SUSD_HACK () public logs_gas {  //     uint256 newShells = super.noSlippage_balanced_10DAI_10USDC_10USDT_2p5SUSD_HACK();  //     assertEq(newShells, 32499999216641686631);  // }  src/test/deposits/depositsTemplate.sol:L40-L56  // function noSlippage_balanced_10DAI_10USDC_10USDT_2p5SUSD_HACK () public returns (uint256 shellsMinted_) {  //     uint256 startingShells = l.proportionalDeposit(300e18);  //     uint256 gas = gasleft();  //     shellsMinted_ = l.depositHack(  //         address(dai), 10e18,  //         address(usdc), 10e6,  //         address(usdt), 10e6,  //         address(susd), 2.5e18  //     );  //     emit log_uint(\"gas for deposit\", gas - gasleft());  // }  Recommendation  Remove all the commented out code or transform it into comments.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"
    },
    {
        "title": "6.12 Should check if the asset already exists when adding a new asset    ",
        "body": "  Resolution  Comment from the development team:  We have decided not to have dynamic adding/removing of assets in this release.  Description  The public function includeAsset  src/Loihi.sol:L128-L130  function includeAsset (address _numeraire, address _nAssim, address _reserve, address _rAssim, uint256 _weight) public onlyOwner {  shell.includeAsset(_numeraire, _nAssim, _reserve, _rAssim, _weight);  Calls the internal includeAsset implementation  src/Controller.sol:L72  function includeAsset (Shells.Shell storage shell, address _numeraire, address _numeraireAssim, address _reserve, address _reserveAssim, uint256 _weight) internal {  But there is no check to see if the asset already exists in the list. Because the check was not done, shell.numeraires can contain multiple identical instances.  src/Controller.sol:L80  shell.numeraires.push(_numeraireAssimilator);  Recommendation  Check if the _numeraire already exists before invoking includeAsset.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"
    },
    {
        "title": "6.13 Check return values for both internal and external calls    ",
        "body": "  Resolution  Comment from the development team:  This doesn t seem feasible. Checking how much was transferred to/from the contract would pose unacceptable gas costs. Because of these constraints, the value returned by the assimilator methods never touches the outside world. They are just converted into numeraire format and returned, so checking these values would not provide any previously unknown information.  Description  There are some cases where functions which return values are called throughout the source code but the return values are not processed, nor checked.  The returns should in principle be handled and checked for validity to provide more robustness to the code.  Examples  The function intakeNumeraire receives a number of tokens and returns how many tokens were transferred to the contract.  src/assimilators/mainnet/daiReserves/mainnetDaiToDaiAssimilator.sol:L51-L59  // transfers numeraire amount of dai in, wraps it in cDai, returns raw amount  function intakeNumeraire (int128 _amount) public returns (uint256 amount_) {  // truncate stray decimals caused by conversion  amount_ = _amount.mulu(1e18) / 1e3 * 1e3;  dai.transferFrom(msg.sender, address(this), amount_);  Similarly, the function outputNumeraire receives a destination address and an amount of token for withdrawal and returns a number of transferred tokens to the specified address.  src/assimilators/mainnet/daiReserves/mainnetDaiToDaiAssimilator.sol:L83-L92  // takes numeraire amount of dai, unwraps corresponding amount of cDai, transfers that out, returns numeraire amount  function outputNumeraire (address _dst, int128 _amount) public returns (uint256 amount_) {  amount_ = _amount.mulu(1e18);  dai.transfer(_dst, amount_);  return amount_;  However, the results are not handled in the main contract.  src/Loihi.sol:L497  shell.numeraires[i].addr.intakeNumeraire(_shells.mul(shell.weights[i]));  src/Loihi.sol:L509  shell.numeraires[i].addr.intakeNumeraire(_oBals[i].mul(_multiplier));  src/Loihi.sol:L586  shell.reserves[i].addr.outputNumeraire(msg.sender, _oBals[i].mul(_multiplier));  A sanity check can be done to make sure that more than 0 tokens were transferred to the contract.  unit intakeAmount = shell.numeraires[i].addr.intakeNumeraire(_shells.mul(shell.weights[i]));  require(intakeAmount > 0, \"Must intake a positive number of tokens\");  Recommendation  Handle all return values everywhere returns exist and add checks to make sure an expected value was returned.  If the return values are never used, consider not returning them at all.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"
    },
    {
        "title": "6.14 Interfaces do not need to be implemented for the compiler to access their selectors.    ",
        "body": "  Resolution  Comment from the development team:  This is the case for the version we used, solc 0.5.15. Versions 0.5.17 and 0.6.* do not require it.  Description  In Assimilators.sol the interface for the assimilators is implemented in a state variable constant as an interface to the zeroth address in order to make use of it s selectors.  src/Assimilators.sol:L37  IAssimilator constant iAsmltr = IAssimilator(address(0));  This pattern is unneeded since you can reference selectors by using the imported interface directly without any implementation. It hinders both gas costs and readability of the code.  Examples  Recommendation  Delete line 37 in Assimilators.sol and instead of getting selectors through:  src/Assimilators.sol:L62  bytes memory data = abi.encodeWithSelector(iAsmltr.viewNumeraireAmount.selector, _amt); // for development  use the expression:  IAssimilator.viewRawAmount.selector  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"
    },
    {
        "title": "6.15 Use consistent interfaces for functions in the same group    ",
        "body": "  Description  In the file Shells.sol, there also is a library that is being used internally for safe adds and subtractions.  This library has 2 functions.  add which receives 2 arguments, x and y.  src/Shells.sol:L22-L24  function add(uint x, uint y) internal pure returns (uint z) {  require((z = x + y) >= x, \"add-overflow\");  sub which receives 3 arguments x, y and _errorMessage.  src/Shells.sol:L26-L28  function sub(uint x, uint y, string memory _errorMessage) internal pure returns (uint z) {  require((z = x - y) <= x, _errorMessage);  In order to reduce the cognitive load on the auditors and developers alike, somehow-related functions should have coherent logic and interfaces. Both of the functions either need to have 2 arguments, with an implied error message passed to require, or both functions need to have 3 arguments, with an error message that can be specified.  Recommendation  Update the functions to be coherent with other related functions.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"
    },
    {
        "title": "6.16 Code coverage should be close to 100%    ",
        "body": "  Resolution  Comment from the development team:  This is true for all aspects of the bonding curve.  Things that have been tested on Kovan with the frontend dapp but could use a unit test are things relevant to sending shell tokens - issuing approvals, transfers and transferfroms.  The adding of assets and assimilators are tested by proxy because they are dependencies for the entire behavior of the bonding surface.  For this release, we plan on having the assets and the assimilators frozen at launch, so there is not much to test regarding continuous updating/changing of assets and assimilators.  We have, however, considered allowing for the dynamic adjustment of weights in addition to parameters.  Description  Code coverage is a measure used to describe how much of the source code is executed during the automated test suite. A system with high code coverage, measured as lines of code executed, has a lower chance to contain undiscovered bugs.  The codebase does not have any information about the code coverage.  Recommendation  Make the test suite output code coverage and add more tests to handle the lines of code that are not touched by any tests.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"
    },
    {
        "title": "6.17 Consider emitting an event when changing the frozen state of the contract    ",
        "body": "  Description  The function freeze allows the owner to freeze and unfreeze the contract.  src/Loihi.sol:L144-L146  function freeze (bool _freeze) public onlyOwner {  frozen = _freeze;  The common pattern when doing actions important for the outside of the blockchain is to emit an event when the action is successful.  It s probably a good idea to emit an event stating the contract was frozen or unfrozen.  Recommendation  Create an event that displays the current state of the contract.  event Frozen(bool frozen);  And emit the event when frozen is called.  function freeze (bool _freeze) public onlyOwner {  frozen = _freeze;  emit Frozen(_freeze);  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"
    },
    {
        "title": "6.18 Function supportsInterface can be restricted to pure    ",
        "body": "  Description  The function supportsInterface returns a bool stating that the contract supports one of the defined interfaces.  src/Loihi.sol:L140-L142  function supportsInterface (bytes4 interfaceID) public returns (bool) {  return interfaceID == ERC20ID || interfaceID == ERC165ID;  The function does not access or change the state of the contract, this is why it can be restricted to pure.  Recommendation  Restrict the function definition to pure.  function supportsInterface (bytes4 interfaceID) public pure returns (bool) {  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"
    },
    {
        "title": "6.19 Use more consistent function naming (includeAssimilator / excludeAdapter)    ",
        "body": "  Description  The function includeAssimilator adds a new assimilator to the list  src/Controller.sol:L98  shell.assimilators[_derivative] = Assimilators.Assimilator(_assimilator, _numeraireAssim.ix);  The function excludeAdapter removes the specified assimilator from the list  src/Loihi.sol:L137  delete shell.assimilators[_assimilator];  Recommendation  Consider renaming the function excludeAdapter to removeAssimilator and moving the logic of adding and removing in the same source file.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"
    },
    {
        "title": "6.1 zDAO Token - Specification violation - Snapshots are never taken   Partially Addressed",
        "body": "  Resolution  Addressed with zer0-os/zDAO-Token@81946d4 by exposing the _snapshot() method to a dedicated snapshot role (likely to be a DAO) and the owner of the contract.  We would like to note that we informed the client that depending on how the snapshot method is used and how predictably snapshots are consumed this might open up a frontrunning vector where someone observing that a _snapshot() is about to be taken might sandwich the snapshot call, accumulate a lot of stake (via 2nd markets, lending platforms), and returning it right after it s been taken. The risk of losing funds may be rather low (especially if performed by a miner) and the benefit from a DAO proposal using this snapshot might outweigh it. It is still recommended to increase the number of snapshots taken or take them on a regular basis (e.g. with every first transaction to the contract in a block) to make it harder to sandwich the snapshot taking.  Description  According to the zDAO Token specification the DAO token should implement a snapshot functionality to allow it being used for DAO governance votings.  Any transfer, mint, or burn operation should result in a snapshot of the token balances of involved users being taken.  While the corresponding functionality is implemented and appears to update balances for snapshots, _snapshot() is never called, therefore, the snapshot is never taken. e.g. attempting to call balanceOfAt always results in an error as no snapshot is available.  zDAO-Token/contracts/ZeroDAOToken.sol:L12-L17  contract ZeroDAOToken is  OwnableUpgradeable,  ERC20Upgradeable,  ERC20PausableUpgradeable,  ERC20SnapshotUpgradeable  zDAO-Token/contracts/ZeroDAOToken.sol:L83-L83  _updateAccountSnapshot(sender);  Note that this is an explicit requirement as per specification but unit tests do not seem to attempt calls to balanceOfAt at all.  Recommendation  Actually, take a snapshot by calling _snapshot() once per block when executing the first transaction in a new block. Follow the openzeppeling documentation for ERC20Snapshot.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zdao-token/"
    },
    {
        "title": "6.2 zDAO-Token - Revoking vesting tokens right before cliff period expiration might be delayed/front-runned ",
        "body": "  Description  The owner of TokenVesting contract has the right to revoke the vesting of tokens for any beneficiary. By doing so, the amount of tokens that are already vested and weren t released yet are being transferred to the beneficiary, and the rest are being transferred to the owner. The beneficiary is expected to receive zero tokens in case the revocation transaction was executed before the cliff period is over. Although unlikely, the beneficiary may front run this revocation transaction by delaying the revocation (and) or inserting a release transaction right before that, thus withdrawing the vested amount.  zDAO-Token/contracts/TokenVesting.sol:L69-L109  function release(address beneficiary) public {  uint256 unreleased = getReleasableAmount(beneficiary);  require(unreleased > 0, \"Nothing to release\");  TokenAward storage award = getTokenAwardStorage(beneficiary);  award.released += unreleased;  targetToken.safeTransfer(beneficiary, unreleased);  emit Released(beneficiary, unreleased);  /**  @notice Allows the owner to revoke the vesting. Tokens already vested  are transfered to the beneficiary, the rest are returned to the owner.  @param beneficiary Who the tokens are being released to  /  function revoke(address beneficiary) public onlyOwner {  TokenAward storage award = getTokenAwardStorage(beneficiary);  require(award.revocable, \"Cannot be revoked\");  require(!award.revoked, \"Already revoked\");  // Figure out how many tokens were owed up until revocation  uint256 unreleased = getReleasableAmount(beneficiary);  award.released += unreleased;  uint256 refund = award.amount - award.released;  // Mark award as revoked  award.revoked = true;  award.amount = award.released;  // Transfer owed vested tokens to beneficiary  targetToken.safeTransfer(beneficiary, unreleased);  // Transfer unvested tokens to owner (revoked amount)  targetToken.safeTransfer(owner(), refund);  emit Released(beneficiary, unreleased);  emit Revoked(beneficiary, refund);  Recommendation  The issue described above is possible, but very unlikely. However, the TokenVesting owner should be aware of that, and make sure not to revoke vested tokens closely to cliff period ending.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zdao-token/"
    },
    {
        "title": "6.3 zDAO-Token - Vested tokens revocation depends on claiming state ",
        "body": "  Description  Examples  zDAO-Token/contracts/TokenVesting.sol:L86-L109  function revoke(address beneficiary) public onlyOwner {  TokenAward storage award = getTokenAwardStorage(beneficiary);  require(award.revocable, \"Cannot be revoked\");  require(!award.revoked, \"Already revoked\");  // Figure out how many tokens were owed up until revocation  uint256 unreleased = getReleasableAmount(beneficiary);  award.released += unreleased;  uint256 refund = award.amount - award.released;  // Mark award as revoked  award.revoked = true;  award.amount = award.released;  // Transfer owed vested tokens to beneficiary  targetToken.safeTransfer(beneficiary, unreleased);  // Transfer unvested tokens to owner (revoked amount)  targetToken.safeTransfer(owner(), refund);  emit Released(beneficiary, unreleased);  emit Revoked(beneficiary, refund);  Recommendation  Make sure that the potential owner of a TokenVesting contract is aware of this potential issue, and has the required processes in place to handle it.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zdao-token/"
    },
    {
        "title": "6.4 zDAO-Token - Total amount of claimable tokens is not verifiable    ",
        "body": "  Description  Since both MerkleTokenVesting and MerkleTokenAirdrop use an off-chain Merkle tree to store the accounts that can claim tokens from the underlying contract, there is no way for a user to verify whether the contract token balance is sufficient for all claimers.  Recommendation  Make sure that users are aware of this trust assumption.  7 Document Change Log  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zdao-token/"
    },
    {
        "title": "1.0",
        "body": "  2021-05-20  Initial report  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zdao-token/"
    },
    {
        "title": "1.1",
        "body": "  2021-08-23  Update: added section 3 - WILD Token  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zdao-token/"
    },
    {
        "title": "6.1 Staking node can be inappropriately removed from the tree    ",
        "body": "  Resolution   This is fixed in   OrchidProtocol/orchid@8c586f2.  Description  The following code in OrchidDirectory.pull() is responsible for reattaching a child from a removed tree node:  code/dir-ethereum/directory.sol:L275-L281  if (name(stake.left_) == key) {  current.right_ = stake.right_;  current.after_ = stake.after_;  } else {  current.left_ = stake.left_;  current.before_ = stake.before_;  The condition name(stake.left_) == key can never hold because key is the key for stake itself.  The result of this bug is somewhat catastrophic. The child is not reattached, but it still has a link to the rest of the tree via its  parent_  pointer. This means reducing the stake of that node can underflow the ancestors  before/after amounts, leading to improper random selection or failing altogether.  The node replacing the removed node also ends up with itself as a child, which violates the basic tree structure and is again likely to produce integer underflows and other failures.  Recommendation  As a simple fix, use if(name(stake.left_) == name(last)) as already suggested by the development team when this bug was first shared.  Two suggestions for better long-term fixes:  Use a strict interface for tree operations. It should be impossible to update a node s parent without simultaneously updating that parent s child pointer.  As suggested in (https://github.com/ConsenSys/orchid-audit-2019-10/issues/7), simplify the logic in pull() to avoid this logic altogether.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"
    },
    {
        "title": "6.2 Verifiers need to be pure, but it s very difficult to validate pureness    ",
        "body": "  Resolution  This is addressed in OrchidProtocol/orchid@1b405fb. With this change, the contract checks that the verifier s code doesn t change (via extcodehash). If the code does change, the contract  fails open  by skipping the verifier and allowing all payments.  Because the code can no longer change, the server can use the (relatively) simple method of executing the contract locally and only allowing a whitelist of opcodes that don t depend on or modify state.  The server already has mitigations for denial of service attacks, including limiting the amount of computing resources that can be used for validating code.  Description  After the initial audit, a  verifier  was introduced to the OrchidLottery code. Each Pot can have an associated OrchidVerifier. This is a contract with a good() function that accepts three parameters:  code/lot-ethereum/lottery.sol:L28  function good(bytes calldata shared, address target, bytes calldata receipt) external pure returns (bool);  The verifier returns a boolean indicating whether a given micropayment should be allowed or not. An example use case is a verifier that only allows certain target addresses to be paid. In this case, shared (a single value for a given Pot) is a merkle root, target is (as always) the address being paid, and receipt (specified by the payment recipient) is a merkle proof that the target address is within the merkle tree with the given root.  Unfortunately, this simple scheme is insufficient. As a simple example, a verifier contract could be created with the CREATE2 opcode. It could be demonstrated that it reads no state when good() is called. Then the contract could be destroyed by calling a function that performs a SELFDESTRUCT, and it could be replaced via another CREATE2 call with different code.  This could be mitigated by rejecting any verifier contract that contains the SELFDESTRUCT opcode, but this would also catch harmless occurrences of that particular byte. https://gist.github.com/Arachnid/e8f0638dc9f5687ff8170a95c47eac1e attempts to find SELFDESTRUCT opcodes but fails to account for tricks where the SELFDESTRUCT appears to be data but can actually be executed. (See Recmo s comment.) In general, this approach is difficult to get right and probably requires full data flow analysis to be correct.  Another possible mitigation is to use a factory contract to deploy the verifiers, guaranteeing that they re not created with CREATE2. This should render SELFDESTRUCT harmless, but there s no guarantee that future forks won t introduce new vectors here.  Finally, requiring servers to implement potentially complex contract validation opens up potential for denial-of-service attacks. A server will have to implement mitigations to prevent repeatedly checking the same verifier or spending inordinate resources checking a maliciously crafted contract (e.g. one with high branching factors).  Recommendation  The verifiers add quite a bit of complexity and risk. We recommend looking for an alternative approach, such as including a small number of vetted verifiers (e.g. a merkle proof verifier) or having servers use their own  allow list  for verifiers that they trust.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"
    },
    {
        "title": "6.3 Simplify the logic in OrchidDirectory.pull()    ",
        "body": "  Resolution  This was addressed in the following commits:  OrchidProtocol/orchid@0ad2484  OrchidProtocol/orchid@8b3e821  OrchidProtocol/orchid@affbf93  OrchidProtocol/orchid@e506c0f  OrchidProtocol/orchid@f864e60  Description  pull() is the most complex function in OrchidDirectory, due to its need to handle removing a node altogether when its stake amount reaches 0.  The current logic for removing an interior node is roughly this:  Given a node to be remove called old, walk down the tree, always stepping towards the  heavier  (in terms of total stake) subtree, until you reach a leaf node (called target).  If target is a direct child of old:  Set target to be a child of old.parent. Move the remaining child of old to be under target.  If target is not a direct child of old:  Swap target and old in the tree. Walk up the tree from old (now a leaf node) to target to subtract target s staked amount from the nodes in between. Detach old from the tree.  The code for this is fairly complex, and one serious bug (issue 6.1) was identified in this code.  This logic can be simplified by combining the two cases (direct child and not) and thinking of it as roughly a two-step operation of  detach leaf node  and  replace interior node with leaf node .  Given a node to be removed called old, walk the tree to find target as before.  Walk back up to old, subtracting target s staked amount from the nodes in between.  Detach target from the tree.  Replace old with target.  (Note that in the code,  old  above is called stake and  target  is calledcurrent.)  Recommendation  Replace this code:  code/dir-ethereum/directory.sol:L266-L297  bytes32 direct = current.parent_;  copy(pivot, last);  current.parent_ = stake.parent_;  if (direct == key) {  Primary storage other = stake.before_ > stake.after_ ? stake.right_ : stake.left_;  if (!nope(other))  stakes_[name(other)].parent_ = name(last);  if (name(stake.left_) == key) {  current.right_ = stake.right_;  current.after_ = stake.after_;  } else {  current.left_ = stake.left_;  current.before_ = stake.before_;  } else {  if (!nope(stake.left_))  stakes_[name(stake.left_)].parent_ = name(last);  if (!nope(stake.right_))  stakes_[name(stake.right_)].parent_ = name(last);  current.right_ = stake.right_;  current.after_ = stake.after_;  current.left_ = stake.left_;  current.before_ = stake.before_;  stake.parent_ = direct;  copy(last, staker, stakee);  step(key, stake, -current.amount_, current.parent_);  kill(last);  with something like this code:  // Remember this key so we can update `pivot` later  bytes32 currentKey = name(last);  // Remove `current` from the subtree rooted at `stake`  step(currentKey, current, -current.amount_, stake.parent_);  kill(last);  // Replace `stake` with `current`  current.left_ = stake.left_;  if (!nope(current.left_))  stakes_[name(current.left_)].parent_ = currentKey;  current.right_ = stake.right_;  if (!nope(current.right_))  stakes_[name(current.right_)].parent_ = currentKey;  current.before_ = stake.before_;  current.after_ = stake.after_;  current.parent_ = stake.parent_;  pivot.value_ = currentKey; // `pivot` was parent's pointer to `stake`  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"
    },
    {
        "title": "6.4 Remove unnecessary address payable   ",
        "body": "  Resolution   The development team decided to leave this as-is.   Description  The address payable type is only needed for transferring ether to an address. The OrchidDirectory and OrchidLottery contracts work with tokens, not ether, so there s no need for any parameters to be of type address payable.  Recommendation  Use simply address instead of address payable everywhere.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"
    },
    {
        "title": "6.5 Use consistent staker, stakee ordering in OrchidDirectory    ",
        "body": "  Resolution   This is fixed in   OrchidProtocol/orchid@1cfef88.  Description  code/dir-ethereum/directory.sol:L156  function lift(bytes32 key, Stake storage stake, uint128 amount, address stakee, address staker) private {  OrchidDirectory.lift() has a parameter stakee that precedes staker, while the rest of the code always places staker first. Because Solidity doesn t have named parameters, it s a good idea to use a consistent ordering to avoid mistakes.  Recommendation  Switch lift() to follow the  staker then stakee  ordering convention of the rest of the contract.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"
    },
    {
        "title": "6.6 Use more descriptive function and variable names   ",
        "body": "  Resolution  This issue is about readability. Even though the audit team firmly believes that improved readability would increase trust in Orchid from its clients, this is not a correctness issue.  The Orchid team believes that making this change, particularly this late in their development cycle, would be too risky. The development team is very familiar with the current terminology, and bugs may accidentally be introduced with the change.  Description  Throughout OrchidDirectory and OrchidLottery, function and variable names are quite obscure. This makes it harder for a reader to understand the code.  Examples  OrchidDirectory:  heft() returns the total staked for a given stakee (perhaps totalForStakee()) Primary is a pointer to a tree node (perhaps NodePointer), and its member value_ could be named key name() gives the key for a given (staker, stakee) pair or a Primary (perhaps getKey()) copy() writes a key to a node pointer (probably better to remove this and just do pointer.key = ...) kill() sets a node pointer to zero (probably better to just remove this and use delete pointer) nope() checks whether a node pointer exists (probably better to just do pointer.key == 0) have() returns the total number of staked tokens (perhaps totalStaked) scan() finds a node, given a random 128-bit number (perhaps selectNode(uint128 random)) turn() is only used in one place and is likely better just inlined step() walks up a subtree, adjusting before/after amounts along the way (perhaps propagate() or bubbleUp()) lift() updates the stake for a given node and then calls step() (perhaps updateNodeStake()) more() is really just the body for push(), so it should probably be moved inside push() instead push() is the external method for staking (perhaps increaseStake() or just stake()) wait() increases the withdrawal delay for the sender s stake for a given stakee (increaseDelay()) Pending could be called PendingWithdrawal take() could be called completeWithdrawal() stop() could be called cancelWithdrawal() delay_ could be withdrawalDelay pull() decreases stake and establishes a pending withdrawal (perhaps decreaseStake(), unstake() or startWithdrawal()) Within pull():  pivot could be pointerToStake last could be pointerToLeaf current could be leaf direct could be leafParent other could be sibling  OrchidLottery:  Pot could perhaps be Fund send() just emits an Update event (perhaps log() or logUpdate()) Track is a struct that keeps track of a ticket that has already been redeemed to prevent replay (perhaps RedeemedTicket) kill() is overloaded to delete funds and used tickets (perhaps deleteFund() and forgetTicket()) take() could be called transferTokens() grab() redeems a winning ticket (perhaps redeem() or redeemTicket()) give() and pull() both transfer tokens from a given Pot, but one is used by the signer and one by the funder. Perhaps better would be a single transferFromPot(address funder, address signer, address target, uint128 amount) with require(msg.sender == funder || msg.sender == signer). warn() could be startWithdrawal() lock() could be cancelWithdrawal() pull() could be completeWithdrawal()  Recommendation  Consider using longer, more descriptive names to make it easier to understand the code. Where there s no particularly good name, add comments explaining the meaning.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"
    },
    {
        "title": "6.7 In OrchidDirectory.step() and OrchidDirectory.lift(), use a signed amount   ",
        "body": "  Resolution   The variables in question are now   Description  step() and lift() both accept a uint128 parameter called amount. This amount is added to various struct fields, which are also of type uint128.  The contract intentionally underflows this amount to represent negative numbers. This is roughly equivalent to using a signed integer, except that:  Unsigned integers aren t sign extended when they re cast to a larger integer type, so care must be taken to avoid this.  Tools that look for integer overflow/underflow will detect this possibility as a bug. It s then hard to determine which overflows are intentional and which are not.  Examples  code/dir-ethereum/directory.sol:L247  lift(key, stake, -amount, stakee, staker);  code/dir-ethereum/directory.sol:L296  step(key, stake, -current.amount_, current.parent_);  Recommendation  Use int128 instead, and ensure that amounts can never exceed the maximum int128 value. (This is trivially achieved by limiting the total number of tokens that can exist.)  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"
    },
    {
        "title": "6.8 Document that math in OrchidDirectory assumes a maximum number of tokens    ",
        "body": "  Resolution   This is fixed in   OrchidProtocol/orchid@f2efe42 by using  Description  OrchidDirectory relies on mathematical operations being unable to overflow due to the particular ERC20 token being used being capped at less than 2**128.  Examples  The following code in step() assumes that no before/after amount can reach 2**128:  code/dir-ethereum/directory.sol:L145-L148  if (name(stake.left_) == key)  stake.before_ += amount;  else  stake.after_ += amount;  The following code in lift() assumes that no staked amount (or total amount for a given stakee) can reach 2**128:  code/dir-ethereum/directory.sol:L157-L164  uint128 local = stake.amount_;  local += amount;  stake.amount_ = local;  emit Update(staker, stakee, local);  uint128 global = stakees_[stakee].amount_;  global += amount;  stakees_[stakee].amount_ = global;  The following code in have() assumes that the total amount staked cannot reach 2**128:  code/dir-ethereum/directory.sol:L103  return stake.before_ + stake.after_ + stake.amount_;  Recommendation  Document this assumption in the form of code comments where potential overflows exist.  Consider also asserting the ERC20 token s total supply in the constructor to attempt to block using a token that violates this constraint and/or checking in push() that the total amount staked will remain less than 2**128. This recommendation is in line with the mitigation proposed for issue 6.7.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"
    },
    {
        "title": "6.9 Unneeded named return parameter    ",
        "body": "  Resolution   Fixed in   OrchidProtocol/orchid@21d56d5  Description  In the heft function in the OrchidDirectory contract, there is an unused and unneeded named return parameter (that actually instantiates a new variable in memory which is not used).  Remediation  Change returns (uint128 amount) to returns (uint128).  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"
    },
    {
        "title": "6.10 Improve function visibility    ",
        "body": "  Resolution   Fixed in   OrchidProtocol/orchid@68fb26a  Description  The following methods are not called internally in the token contract and visibility can, therefore, be restricted to external rather than public. This is more gas efficient because less code is emitted and data does not need to be copied into memory. It also makes functions a bit simpler to reason about because there s no need to worry about the possibility of internal calls.  OrchidDirectory.heft()  OrchidDirectory.scan()  OrchidDirectory.push()  OrchidDirectory.wait()  OrchidDirectory.take()  OrchidDirectory.stop()  OrchidDirectory.pull()  OrchidLocation.move()  OrchidLocation.look()  OrchidLottery.size()  OrchidLottery.keys()  OrchidLottery.seek()  OrchidLottery.look()  OrchidLottery.push()  OrchidLottery.move()  OrchidLottery.kill()  OrchidLottery.grab()  OrchidLottery.pull()  OrchidLottery.warn()  OrchidLottery.lock()  OrchidLottery.pull()  OrchidCurator.list()  OrchidCurator.good()  OrchidUntrusted.good()  Recommendation  Change visibility of these methods to external.  7 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"
    },
    {
        "title": "7.1 MythX",
        "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  The output of the MythX Pro vulnerability scan was reviewed by the audit team and no vulnerabilities were identified as part of the process.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"
    },
    {
        "title": "7.2 Ethlint",
        "body": "  Ethlint is an open source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  contracts/curator.sol  35:8    warning    Provide an error message for require().    error-reason  contracts/directory.sol  107:8     warning    Provide an error message for require().            error-reason  141:4     error      \"step\": Avoid assigning to function parameters.    security/no-assign-params  141:4     error      \"step\": Avoid assigning to function parameters.    security/no-assign-params  176:8     warning    Provide an error message for require().            error-reason  180:12    warning    Provide an error message for require().            error-reason  202:8     warning    Provide an error message for require().            error-reason  209:8     warning    Provide an error message for require().            error-reason  211:8     warning    Provide an error message for require().            error-reason  226:8     warning    Provide an error message for require().            error-reason  226:35    warning    Avoid using 'block.timestamp'.                     security/no-block-members  228:8     warning    Provide an error message for require().            error-reason  233:8     warning    Provide an error message for require().            error-reason  233:35    warning    Avoid using 'block.timestamp'.                     security/no-block-members  244:8     warning    Provide an error message for require().            error-reason  245:8     warning    Provide an error message for require().            error-reason  305:8     warning    Provide an error message for require().            error-reason  306:26    warning    Avoid using 'block.timestamp'.                     security/no-block-members  contracts/location.sol  38:24    warning    Avoid using 'block.timestamp'.    security/no-block-members  contracts/lottery.sol  66:8      warning    Provide an error message for require().            error-reason  104:8     warning    Provide an error message for require().            error-reason  111:8     warning    Provide an error message for require().            error-reason  117:8     warning    Provide an error message for require().            error-reason  131:8     warning    Provide an error message for require().            error-reason  131:32    warning    Avoid using 'block.timestamp'.                     security/no-block-members  140:4     error      \"take\": Avoid assigning to function parameters.    security/no-assign-params  153:12    warning    Provide an error message for require().            error-reason  156:4     error      \"grab\": Avoid assigning to function parameters.    security/no-assign-params  156:4     warning    Line exceeds the limit of 145 characters           max-len  157:8     warning    Provide an error message for require().            error-reason  158:8     warning    Provide an error message for require().            error-reason  163:12    error      Only use indent of 8 spaces.                       indentation  165:12    error      Only use indent of 8 spaces.                       indentation  166:12    error      Only use indent of 8 spaces.                       indentation  167:12    error      Only use indent of 8 spaces.                       indentation  167:12    warning    Provide an error message for require().            error-reason  167:28    warning    Avoid using 'block.timestamp'.                     security/no-block-members  168:12    error      Only use indent of 8 spaces.                       indentation  168:12    warning    Provide an error message for require().            error-reason  169:12    error      Only use indent of 8 spaces.                       indentation  171:12    error      Only use indent of 8 spaces.                       indentation  172:0     error      Only use indent of 8 spaces.                       indentation  175:20    warning    Avoid using 'block.timestamp'.                     security/no-block-members  176:64    warning    Avoid using 'block.timestamp'.                     security/no-block-members  182:8     warning    Provide an error message for require().            error-reason  200:22    warning    Avoid using 'block.timestamp'.                     security/no-block-members  214:8     warning    Provide an error message for require().            error-reason  215:8     warning    Provide an error message for require().            error-reason  215:31    warning    Avoid using 'block.timestamp'.                     security/no-block-members  219:8     warning    Provide an error message for require().            error-reason  \u2716 12 errors, 38 warnings found.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"
    },
    {
        "title": "7.3 Surya",
        "body": "  Surya is an utility tool for smart contract systems. It provides a number of visual outputs and information about structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  8 S\u016brya s Description Report  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"
    },
    {
        "title": "8.1 Files Description Table",
        "body": "  contracts/curator.sol  5ea6c8374cec289bcf4dfe1adb3bb157ca9bab74  contracts/directory.sol  811ab3b049d570c4236ee965d76ed1a9f5cb929e  contracts/location.sol  1ea56960f41ca3a299c4fd35fab9ef1fdd494d5b  contracts/lottery.sol  e63f3c86b3abba57d0a7e3ca36436bfee4d9ac1b  contracts/token.sol  faf15f117ac160641adfe56c2a01ad14bff931f3  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"
    },
    {
        "title": "8.2 Contracts Description Table",
        "body": "  Function Name  Visibility  Mutability  Modifiers  OrchidCurator  Implementation  <Constructor>  Public    list  Public    NO   good  Public    NO   OrchidUntrusted  Implementation  good  Public    NO   IOrchidDirectory  Interface  have  External    NO   OrchidDirectory  Implementation  IOrchidDirectory  <Constructor>  Public    heft  Public    NO   name  Public    NO   name  Private \ud83d\udd10  copy  Private \ud83d\udd10  copy  Private \ud83d\udd10  kill  Private \ud83d\udd10  nope  Private \ud83d\udd10  have  Public    NO   scan  Public    NO   turn  Private \ud83d\udd10  step  Private \ud83d\udd10  lift  Private \ud83d\udd10  more  Private \ud83d\udd10  push  Public    NO   wait  Public    NO   take  Public    NO   stop  Public    NO   pull  Public    NO   OrchidLocation  Implementation  move  Public    NO   look  Public    NO   OrchidLottery  Implementation  <Constructor>  Public    send  Private \ud83d\udd10  find  Private \ud83d\udd10  kill  Private \ud83d\udd10  size  Public    NO   keys  Public    NO   seek  Public    NO   page  Public    NO   look  Public    NO   push  Public    NO   move  Public    NO   kill  Private \ud83d\udd10  kill  Public    NO   take  Private \ud83d\udd10  grab  Public    NO   give  Public    NO   pull  Public    NO   warn  Public    NO   lock  Public    NO   pull  Public    NO   OrchidToken  Implementation  ERC20, ERC20Detailed  <Constructor>  Public    ERC20Detailed  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"
    },
    {
        "title": "8.3 Legend",
        "body": "  Function can modify state  Function is payable  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"
    },
    {
        "title": "5.1 zBanc - DynamicLiquidTokenConverter ineffective reentrancy protection    ",
        "body": "  Resolution   Fixed with   zer0-os/zBanc@ff3d913 by following the recommendation.  Description  reduceWeight calls _protected() in an attempt to protect from reentrant calls but this check is insufficient as it will only check for the locked statevar but never set it. A potential for direct reentrancy might be present when an erc-777 token is used as reserve.  It is assumed that the developer actually wanted to use the protected modifier that sets the lock before continuing with the method.  Examples  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L123-L128  function reduceWeight(IERC20Token _reserveToken)  public  validReserve(_reserveToken)  ownerOnly  _protected();  contract ReentrancyGuard {  // true while protected code is being executed, false otherwise  bool private locked = false;  /**  @dev ensures instantiation only by sub-contracts  /  constructor() internal {}  // protects a function against reentrancy attacks  modifier protected() {  _protected();  locked = true;  _;  locked = false;  // error message binary size optimization  function _protected() internal view {  require(!locked, \"ERR_REENTRANCY\");  Recommendation  To mitigate potential attack vectors from reentrant calls remove the call to _protected() and decorate the function with protected instead. This will properly set the lock before executing the function body rejecting reentrant calls.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"
    },
    {
        "title": "5.2 zBanc - DynamicLiquidTokenConverter input validation    ",
        "body": "  Resolution   fixed with   zer0-os/zBanc@ff3d913 by checking that the provided values are at least 0% < p <= 100%.  Description  Check that the value in PPM is within expected bounds before updating system settings that may lead to functionality not working correctly. For example, setting out-of-bounds values for stepWeight or setMinimumWeight may make calls to reduceWeight fail. These values are usually set in the beginning of the lifecycle of the contract and misconfiguration may stay unnoticed until trying to reduce the weights. The settings can be fixed, however, by setting the contract inactive and updating it with valid settings. Setting the contract to inactive may temporarily interrupt the normal operation of the contract which may be unfavorable.  Examples  Both functions allow the full uint32 range to be used, which, interpreted as PPM would range from 0% to 4.294,967295%  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L75-L84  function setMinimumWeight(uint32 _minimumWeight)  public  ownerOnly  inactive  //require(_minimumWeight > 0, \"Min weight 0\");  //_validReserveWeight(_minimumWeight);  minimumWeight = _minimumWeight;  emit MinimumWeightUpdated(_minimumWeight);  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L92-L101  function setStepWeight(uint32 _stepWeight)  public  ownerOnly  inactive  //require(_stepWeight > 0, \"Step weight 0\");  //_validReserveWeight(_stepWeight);  stepWeight = _stepWeight;  emit StepWeightUpdated(_stepWeight);  Recommendation  Reintroduce the checks for _validReserveWeight to check that a percent value denoted in PPM is within valid bounds _weight > 0 && _weight <= PPM_RESOLUTION. There is no need to separately check for the value to be >0 as this is already ensured by _validReserveWeight.  Note that there is still room for misconfiguration (step size too high, min-step too high), however, this would at least allow to catch obviously wrong and often erroneously passed parameters early.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"
    },
    {
        "title": "5.3 zBanc - DynamicLiquidTokenConverter introduces breaking changes to the underlying bancorprotocol base    ",
        "body": "  Resolution  Addressed with zer0-os/zBanc@ff3d913 by removing the modifications in favor of surgical and more simple changes, keeping the factory and upgrade components as close as possible to the forked bancor contracts.  Additionally, the client provided the following statement:  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"
    },
    {
        "title": "5.14 Removed excess functionality from factory and restored the bancor factory pattern.",
        "body": "  Description  Introducing major changes to the complex underlying smart contract system that zBanc was forked from(bancorprotocol) may result in unnecessary complexity to be added. Complexity usually increases the attack surface and potentially introduces software misbehavior. Therefore, it is recommended to focus on reducing the changes to the base system as much as possible and comply with the interfaces and processes of the system instead of introducing diverging behavior.  For example, DynamicLiquidTokenConverterFactory does not implement the ITypedConverterFactory while other converters do. Furthermore, this interface and the behavior may be expected to only perform certain tasks e.g. when called during an upgrade process. Not adhering to the base systems expectations may result in parts of the system failing to function for the new convertertype. Changes introduced to accommodate the custom behavior/interfaces may result in parts of the system failing to operate with existing converters. This risk is best to be avoided.  In the case of DynamicLiquidTokenConverterFactory the interface is imported but not implemented at all (unused import). The reason for this is likely because the function createConverter in DynamicLiquidTokenConverterFactory does not adhere to the bancor-provided interface anymore as it is doing way more than  just  creating and returning a new converter. This can create problems when trying to upgrade the converter as the upgraded expected the shared interface to be exposed unless the update mechanisms are modified as well.  In general, the factories createConverter method appears to perform more tasks than comparable type factories. It is questionable if this is needed but may be required by the design of the system. We would, however, highly recommend to not diverge from how other converters are instantiated unless it is required to provide additional security guarantees (i.e. the token was instantiated by the factory and is therefore trusted).  The ConverterUpgrader changed in a way that it now can only work with the DynamicLiquidTokenconverter instead of the more generalized IConverter interface. This probably breaks the update for all other converter types in the system.  The severity is estimated to be medium based on the fact that the development team seems to be aware of the breaking changes but the direction of the design of the system was not yet decided.  Examples  unused import  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverterFactory.sol:L6-L6  import \"../../interfaces/ITypedConverterFactory.sol\";  converterType should be external as it is not called from within the same or inherited contracts  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverterFactory.sol:L144-L146  function converterType() public pure returns (uint16) {  return 3;  createToken can be external and is actually creating a token and converter that is using that token (the converter is not returned)(consider renaming to createTokenAndConverter)  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverterFactory.sol:L54-L74  DSToken token = new DSToken(_name, _symbol, _decimals);  token.issue(msg.sender, _initialSupply);  emit NewToken(token);  createConverter(  token,  _reserveToken,  _reserveWeight,  _reserveBalance,  _registry,  _maxConversionFee,  _minimumWeight,  _stepWeight,  _marketCapThreshold  );  return token;  the upgrade interface changed and now requires the converter to be a DynamicLiquidTokenConverter. Other converters may potentially fail to upgrade unless they implement the called interfaces.  zBanc/solidity/contracts/converter/ConverterUpgrader.sol:L96-L122  function upgradeOld(DynamicLiquidTokenConverter _converter, bytes32 _version) public {  _version;  DynamicLiquidTokenConverter converter = DynamicLiquidTokenConverter(_converter);  address prevOwner = converter.owner();  acceptConverterOwnership(converter);  DynamicLiquidTokenConverter newConverter = createConverter(converter);  copyReserves(converter, newConverter);  copyConversionFee(converter, newConverter);  transferReserveBalances(converter, newConverter);  IConverterAnchor anchor = converter.token();  // get the activation status before it's being invalidated  bool activate = isV28OrHigherConverter(converter) && converter.isActive();  if (anchor.owner() == address(converter)) {  converter.transferTokenOwnership(address(newConverter));  newConverter.acceptAnchorOwnership();  handleTypeSpecificData(converter, newConverter, activate);  converter.transferOwnership(prevOwner);  newConverter.transferOwnership(prevOwner);  emit ConverterUpgrade(address(converter), address(newConverter));  solidity/contracts/converter/ConverterUpgrader.sol:L95-L101  function upgradeOld(  IConverter _converter,  bytes32 /* _version */  ) public {  // the upgrader doesn't require the version for older converters  upgrade(_converter, 0);  Recommendation  It is a fundamental design decision to either follow the bancorsystems converter API or diverge into a more customized system with a different design, functionality, or even security assumptions. From the current documentation, it is unclear which way the development team wants to go.  However, we highly recommend re-evaluating whether the newly introduced type and components should comply with the bancor API (recommended; avoid unnecessary changes to the underlying system,) instead of changing the API for the new components. Decide if the new factory should adhere to the usually commonly shared ITypedConverterFactory (recommended) and if not, remove the import and provide a new custom shared interface. It is highly recommended to comply and use the bancor systems extensibility mechanisms as intended, keeping the previously audited bancor code in-tact and voiding unnecessary re-assessments of the security impact of changes.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"
    },
    {
        "title": "5.4 zBanc - DynamicLiquidTokenConverter isActive should only be returned if converter is fully configured and converter parameters should only be updateable while converter is inactive    ",
        "body": "  Resolution  Addressed with zer0-os/zBanc@ff3d913 by removing the custom ACL modifier falling back to checking whether the contract is configured (isActive, inactive modifiers). When a new contract is deployed it will be inactive until the main vars are set by the owner (upgrade contract). The upgrade path is now aligned with how the LiquidityPoolV2Converter performs upgrades.  Additionally, the client provided the following statement:  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"
    },
    {
        "title": "5.13 - upgrade path resolved - inactive modifier back on the setters, and upgrade path now mirrors lpv2 path. An important note here is that lastWeightAdjustmentMarketCap setting isn t included in the inActive() override, since it has a valid state of 0. So it must be set before the others settings, or it will revert as inactive",
        "body": "  Description  By default, a converter is active once the anchor ownership was transferred. This is true for converters that do not require to be properly set up with additional parameters before they can be used.  zBanc/solidity/contracts/converter/ConverterBase.sol:L272-L279  /**  @dev returns true if the converter is active, false otherwise  @return true if the converter is active, false otherwise  /  function isActive() public view virtual override returns (bool) {  return anchor.owner() == address(this);  For a simple converter, this might be sufficient. If a converter requires additional setup steps (e.g. setting certain internal variables, an oracle, limits, etc.) it should return inactive until the setup completes. This is to avoid that users are interacting with (or even pot. frontrunning) a partially configured converter as this may have unexpected outcomes.  For example, the LiquidityPoolV2Converter overrides the isActive method to require additional variables be set (oracle) to actually be in active state.  zBanc/solidity/contracts/converter/types/liquidity-pool-v2/LiquidityPoolV2Converter.sol:L79-L85  Additionally, settings can only be updated while the contract is inactive which will be the case during an upgrade. This ensures that the owner cannot adjust settings at will for an active contract.  @dev returns true if the converter is active, false otherwise  @return true if the converter is active, false otherwise  /  function isActive() public view override returns (bool) {  return super.isActive() && address(priceOracle) != address(0);  Additionally, settings can only be updated while the contract is inactive which will be the case during an upgrade. This ensures that the owner cannot adjust settings at will for an active contract.  zBanc/solidity/contracts/converter/types/liquidity-pool-v2/LiquidityPoolV2Converter.sol:L97-L109  function activate(  IERC20Token _primaryReserveToken,  IChainlinkPriceOracle _primaryReserveOracle,  IChainlinkPriceOracle _secondaryReserveOracle)  public  inactive  ownerOnly  validReserve(_primaryReserveToken)  notThis(address(_primaryReserveOracle))  notThis(address(_secondaryReserveOracle))  validAddress(address(_primaryReserveOracle))  validAddress(address(_secondaryReserveOracle))  The DynamicLiquidTokenConverter is following a different approach. It inherits the default isActive which sets the contract active right after anchor ownership is transferred. This kind of breaks the upgrade process for DynamicLiquidTokenConverter as settings cannot be updated while the contract is active (as anchor ownership might be transferred before updating values). To unbreak this behavior a new authentication modifier was added, that allows updates for the upgrade contradict while the contract is active. Now this is a behavior that should be avoided as settings should be predictable while a contract is active. Instead it would make more sense initially set all the custom settings of the converter to zero (uninitialized) and require them to be set and only the return the contract as active. The behavior basically mirrors the upgrade process of LiquidityPoolV2Converter.  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L44-L50  modifier ifActiveOnlyUpgrader(){  if(isActive()){  require(owner == addressOf(CONVERTER_UPGRADER), \"ERR_ACTIVE_NOTUPGRADER\");  _;  Pre initialized variables should be avoided. The marketcap threshold can only be set by the calling entity as it may be very different depending on the type of reserve (eth, token).  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L17-L20  uint32 public minimumWeight = 30000;  uint32 public stepWeight = 10000;  uint256 public marketCapThreshold = 10000 ether;  uint256 public lastWeightAdjustmentMarketCap = 0;  Here s one of the setter functions that can be called while the contract is active (only by the upgrader contract but changing the ACL commonly followed with other converters).  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L67-L74  function setMarketCapThreshold(uint256 _marketCapThreshold)  public  ownerOnly  ifActiveOnlyUpgrader  marketCapThreshold = _marketCapThreshold;  emit MarketCapThresholdUpdated(_marketCapThreshold);  Recommendation  Align the upgrade process as much as possible to how LiquidityPoolV2Converter performs it. Comply with the bancor API.  override isActive and require the contracts main variables to be set.  do not pre initialize the contracts settings to  some  values. Require them to be set by the caller (and perform input validation)  mirror the upgrade process of LiquidityPoolV2Converter and instead of activate call the setter functions that set the variables. After setting the last var and anchor ownership been transferred, the contract should return active.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"
    },
    {
        "title": "5.5 zBanc - DynamicLiquidTokenConverter frontrunner can grief owner when calling reduceWeight   ",
        "body": "  Resolution  The client acknowledged this issue by providing the following statement:  ",
        "labels": [
            "Consensys",
            "Medium",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"
    },
    {
        "title": "5.12 - admin by a DAO will mitigate the owner risks here",
        "body": "  Description  The owner of the converter is allowed to reduce the converters weights once the marketcap surpasses a configured threshhold. The thresshold is configured on first deployment. The marketcap at the beginning of the call is calculated as reserveBalance / reserve.weight and stored as lastWeightAdjustmentMarketCap after reducing the weight.  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L130-L138  function reduceWeight(IERC20Token _reserveToken)  public  validReserve(_reserveToken)  ownerOnly  _protected();  uint256 currentMarketCap = getMarketCap(_reserveToken);  require(currentMarketCap > (lastWeightAdjustmentMarketCap.add(marketCapThreshold)), \"ERR_MARKET_CAP_BELOW_THRESHOLD\");  The reserveBalance can be manipulated by buying (adding reserve token) or selling liquidity tokens (removing reserve token). The success of a call to reduceWeight is highly dependant on the marketcap. A malicious actor may, therefore, attempt to grief calls made by the owner by sandwiching them with buy and sell calls in an attempt to (a) raise the barrier for the next valid payout marketcap or (b) temporarily lower the marketcap if they are a major token holder in an attempt to fail the reduceWeights call.  In both cases the griefer may incur some losses due to conversion errors, bancor fees if they are set, and gas spent. It is, therefore, unlikely that a third party may spend funds on these kinds of activities. However, the owner as a potential major liquid token holder may use this to their own benefit by artificially lowering the marketcap to the absolute minimum (old+threshold) by selling liquidity and buying it back right after reducing weights.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"
    },
    {
        "title": "5.6 zBanc - outdated fork   ",
        "body": "  Description  According to the client the system was forked off bancor v0.6.18 (Oct 2020). The current version 0.6.x is v0.6.36 (Apr 2021).  Recommendation  It is recommended to check if relevant security fixes were released after v0.6.18 and it should be considered to rebase with the current stable release.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"
    },
    {
        "title": "5.7 zBanc - inconsistent DynamicContractRegistry, admin risks    ",
        "body": "  Resolution  The client acknowledged the admin risk and addressed the itemCount concerns by exposing another method that only returns the overridden entries. The following statement was provided:  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"
    },
    {
        "title": "5.10 - keeping this pattern which matches the bancor pattern, and noting the DCR should be owned by a DAO, which is our plan. solved itemCount issue - Added dcrItemCount and made itemCount call the bancor registry s itemCount, so unpredictable behavior due to the count should be eliminated.",
        "body": "  Description  DynamicContractRegistry is a wrapper registry that allows the zBanc to use the custom upgrader contract while still providing access to the normal bancor registry.  For this to work, the registry owner can add or override any registry setting. Settings that don t exist in this contract are attempted to be retrieved from an underlying registry (contractRegistry).  zBanc/solidity/contracts/utility/DynamicContractRegistry.sol:L66-L70  function registerAddress(bytes32 _contractName, address _contractAddress)  public  ownerOnly  validAddress(_contractAddress)  If the item does not exist in the registry, the request is forwarded to the underlying registry.  zBanc/solidity/contracts/utility/DynamicContractRegistry.sol:L52-L58  function addressOf(bytes32 _contractName) public view override returns (address) {  if(items[_contractName].contractAddress != address(0)){  return items[_contractName].contractAddress;  }else{  return contractRegistry.addressOf(_contractName);  According to the documentation this registry is owned by zer0 admins and this means users have to trust zer0 admins to play fair.  To handle this, we deploy our own ConverterUpgrader and ContractRegistry owned by zer0 admins who can register new addresses  The owner of the registry (zer0 admins) can change the underlying registry contract at will. The owner can also add new or override any settings that already exist in the underlying registry. This may for example allow a malicious owner to change the upgrader contract in an attempt to potentially steal funds from a token converter or upgrade to a new malicious contract. The owner can also front-run registry calls changing registry settings and thus influencing the outcome. Such an event will not go unnoticed as events are emitted.  It should also be noted that itemCount will return only the number of items in the wrapper registry but not the number of items in the underlying registry. This may have an unpredictable effect on components consuming this information.  zBanc/solidity/contracts/utility/DynamicContractRegistry.sol:L36-L43  /**  @dev returns the number of items in the registry  @return number of items  /  function itemCount() public view returns (uint256) {  return contractNames.length;  Recommendation  Require the owner/zer0 admins to be a DAO or multisig and enforce 2-step (notify->wait->upgrade) registry updates (e.g. by requiring voting or timelocks in the admin contract). Provide transparency about who is the owner of the registry as this may not be clear for everyone. Evaluate the impact of itemCount only returning the number of settings in the wrapper not taking into account entries in the subcontract (including pot. overlaps).  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"
    },
    {
        "title": "5.8 zBanc - DynamicLiquidTokenConverter consider using PPM_RESOLUTION instead of hardcoding integer literals    ",
        "body": "  Resolution   This issue was present in the initial commit under review (  zer0-os/zBanc@48da0ac) but has since been addressed with  zer0-os/zBanc@3d6943e.  Description  getMarketCap calculates the reserve s market capitalization as reserveBalance * 1e6 / weight where 1e6 should be expressed as the constant PPM_RESOLUTION.  Examples  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L157-L164  function getMarketCap(IERC20Token _reserveToken)  public  view  returns(uint256)  Reserve storage reserve = reserves[_reserveToken];  return reserveBalance(_reserveToken).mul(1e6).div(reserve.weight);  Recommendation  Avoid hardcoding integer literals directly into source code when there is a better expression available. In this case 1e6 is used because weights are denoted in percent to base PPM_RESOLUTION (=100%).  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"
    },
    {
        "title": "5.9 zBanc - DynamicLiquidTokenConverter avoid potential converter type overlap with bancor   ",
        "body": "  Resolution  Acknowledged by providing the following statement:  ",
        "labels": [
            "Consensys",
            "Minor",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"
    },
    {
        "title": "5.24 the converterType relates to an array selector in the test helpers, so would be inconvenient to make a higher value. we will have to maintain the value when rebasing in DynamicLiquidTokenConverter & Factory, ConverterUpgrader, and the ConverterUpgrader.js test file and Converter.js test helper file.",
        "body": "  Description  The system is forked frombancorprotocol/contracts-solidity. As such, it is very likely that security vulnerabilities reported to bancorprotocol upstream need to be merged into the zer0/zBanc fork if they also affect this codebase. There is also a chance that security fixes will only be available with feature releases or that the zer0 development team wants to merge upstream features into the zBanc codebase.  Note that the current master of the bancorprotocol already appears to defined converterType 3 and 4: https://github.com/bancorprotocol/contracts-solidity/blob/5f4c53ebda784751c3a90b06aa2c85e9fdb36295/solidity/test/helpers/Converter.js#L51-L54  Examples  The new custom converter  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L50-L52  function converterType() public pure override returns (uint16) {  return 3;  ConverterTypes from the bancor base system  zBanc/solidity/contracts/converter/types/liquidity-pool-v1/LiquidityPoolV1Converter.sol:L71-L73  function converterType() public pure override returns (uint16) {  return 1;  zBanc/solidity/contracts/converter/types/liquidity-pool-v2/LiquidityPoolV2Converter.sol:L73-L76  Recommendation  /  function converterType() public pure override returns (uint16) {  return 2;  Recommendation  Choose a converterType id for this custom implementation that does not overlap with the codebase the system was forked from. e.g. uint16(-1) or 1001 instead of 3 which might already be used upstream.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"
    },
    {
        "title": "5.10 zBanc - unnecessary contract duplication    ",
        "body": "  Resolution   fixed with   zer0-os/zBanc@ff3d913 by removing the duplicate contract.  Description  DynamicContractRegistryClient is an exact copy of ContractRegistryClient. Avoid unnecessary code duplication.  < contract DynamicContractRegistryClient is Owned, Utils {  ---  > contract ContractRegistryClient is Owned, Utils {  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"
    },
    {
        "title": "3.1 A reverting fallback function will lock up all payouts    ",
        "body": "  Resolution  Replace the push method to pull pattern.  Remove transfer of ETH in the process of execution, and store ETH amount to mapping(address => uint256) ethBalance (contracts/Inheritance/ETHExchange.sol, L21)  Add function withdrawETH to send ethBalance[msg.sender] (contracts/Inheritance/ETHExchange.sol, L158-L162)  Description  In BoxExchange.sol, the internal function _transferEth() reverts if the transfer does not succeed:  code/Fairswap_iDOLvsETH/contracts/BoxExchange.sol:L958-L963  function _transferETH(address _recipient, uint256 _amount) private {  (bool success, ) = _recipient.call{value: _amount}(  abi.encodeWithSignature(\"\")  );  require(success, \"Transfer Failed\");  The _payment() function processes a list of transfers to settle the transactions in an ExchangeBox. If any of the recipients of an Eth transfer is a smart contract that reverts, then the entire payout will fail and will be unrecoverable.  Recommendation  Implement a queuing mechanism to allow buyers/sellers to initiate the withdrawal on their own using a  pull-over-push pattern.   Ignore a failed transfer and leave the responsibility up to users to receive them properly.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"
    },
    {
        "title": "3.2 Force traders to mint gas token    ",
        "body": "  Resolution  Replace push funds with Pull Pattern.  Remove transfer of ETH in the process of execution, and store ETH amount to mapping(address => uint256) ethBalance (contracts/Inheritance/ETHExchange.sol, L21)  Add function withdrawETH to send ethBalance[msg.sender] (contracts/Inheritance/ETHExchange.sol, L158-L162)  Description  Attack scenario:  Alice makes a large trade via the Fairswap_iDOLvsEth exchange. This will tie up her iDOL until the box is executed.  Mallory makes a small trades to buy ETH immediately afterwards, the trades are routed through an attack contract.  Alice needs to execute the box to get her iDOL out.  Because the gas amount is unlimited, when you Mallory s ETH is paid out to her attack contract, mint a lot of GasToken.  If Alice has $100 worth of ETH tied up in the exchange, you can basically ransom her for $99 of gas token or else she ll never see her funds again.  Examples  code/Fairswap_iDOLvsETH/contracts/BoxExchange.sol:L958  function _transferETH(address _recipient, uint256 _amount) private {  Recommendation  When sending ETH, a pull-payment model is generally preferable.  This would require setting up a queue, allowing users to call a function to initiate a withdrawal.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"
    },
    {
        "title": "3.3 Missing Proper Access Control    ",
        "body": "  Resolution  Comment from Lien Protocol:  remove comment out lines to activate time control functions at Auction.sol and AuctionBoard.sol (a contract divided from Auction.sol)  Description  Some functions do not have proper access control and are public, meaning that anyone can call them. This will result in system take over depending on how critical those functionalities are.  Examples  Anyone can set IDOLContract in MainContracts.Auction.sol, which is a critical aspect of the auction contract, and it cannot be changed after it is set:  code/MainContracts/contracts/Auction.sol:L144-L148  Recommendation  /  function setIDOLContract(address contractAddress) public {  require(address(_IDOLContract) == address(0), \"IDOL contract is already registered\");  _setStableCoinContract(contractAddress);  Recommendation  Make the setIDOLContract() function internal and call it from the constructor, or only allow the deployer to set the value.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"
    },
    {
        "title": "3.4 Code is not production-ready    ",
        "body": "  Resolution  Comment from Lien Protocol:  remove comment out lines and update code to activate time control functions at AuctionTimeControl.sol  Description  Similar to other discussed issues, several areas of the code suggest that the system is not production-ready. This results in narrow test scenarios that do not cover production code flow.  Examples  In MainContracts/contracts/AuctionTimeControl.sol the following functions are commented out and replaced with same name functions that simply return True for testing purposes:  isNotStartedAuction  inAcceptingBidsPeriod  inRevealingValuationPeriod  inReceivingBidsPeriod  code/MainContracts/contracts/AuctionTimeControl.sol:L30-L39  /*  // Indicates any auction has never held for a specified BondID  function isNotStartedAuction(bytes32 auctionID) public virtual override returns (bool) {  uint256 closingTime = _auctionClosingTime[auctionID];  return closingTime == 0;  // Indicates if the auctionID is in bid acceptance status  function inAcceptingBidsPeriod(bytes32 auctionID) public virtual override returns (bool) {  uint256 closingTime = _auctionClosingTime[auctionID];  code/MainContracts/contracts/AuctionTimeControl.sol:L67-L78  // TEST  function isNotStartedAuction(bytes32 auctionID)  public  virtual  override  returns (bool)  return true;  // TEST  function inAcceptingBidsPeriod(bytes32 auctionID)  These commented-out functions contain essential functionality for the Auction contract. For example, inRevealingValuationPeriod is used to allow revealing of the bid price publicly:  code/MainContracts/contracts/Auction.sol:L403-L406  require(  inRevealingValuationPeriod(auctionID),  \"it is not the time to reveal the value of bids\"  );  Recommendation  Remove the test functions and use the production code for testing. The tests must have full coverage of the production code to be considered complete.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"
    },
    {
        "title": "3.5 Unable to compile contracts    ",
        "body": "  Resolution   The related code was updated on the 7th day of the audit, and fixed this issue for   Description  In the Fairswap_iDOLvsImmortalOptionsrepository:  Compilation with truffle fails due to a missing file: contracts/testTokens/TestBondMaker.sol. Compilation with solc fails due to an undefined interface function:  In the Fairswap_iDOLvsLien repository:  Compilation with truffle fails due to a missing file: ./ERC20RegularlyRecord.sol. The correct filename is ./TestERC20RegularlyRecord.sol.  Recommendation  Ensure all contracts are easily compilable by following simple instructions in the README.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"
    },
    {
        "title": "3.6 Unreachable code due to checked conditions    ",
        "body": "  Resolution  Comment from Lien Protocol:  Fix Unreachable codes in Auction.sol which were made for the tests.  Description  The code flow in MainContracts.Auction.sol revealBid() is that it first checks if the function has been called during the reveal period, which means  after closing  and  before the end of the reveal period.   code/MainContracts/contracts/Auction.sol:L508-L517  function revealBid(  bytes32 auctionID,  uint256 price,  uint256 targetSBTAmount,  uint256 random  ) public override {  require(  inRevealingValuationPeriod(auctionID),  \"it is not the time to reveal the value of bids\"  );  However, later in the same function, code exists to introduce  Penalties for revealing too early.  This checks to see if the function was called before closing, which should not be possible given the previous check.  code/MainContracts/contracts/Auction.sol:L523-L537  /**  @dev Penalties for revealing too early.  Some participants may not follow the rule and publicate their bid information before the reveal process.  In such a case, the bid price is overwritten by the bid with the strike price (slightly unfavored price).  /  uint256 bidPrice = price;  /**  @dev FOR TEST CODE RUNNING: The following if statement in L533 should be replaced by the comment out  /  if (inAcceptingBidsPeriod(auctionID)) {  // if (false) {  (, , uint256 solidStrikePriceE4, ) = _getBondFromAuctionID(auctionID);  bidPrice = _exchangeSBT2IDOL(solidStrikePriceE4.mul(10**18));  Recommendation  Double-check the logic in these functions. If revealing should be allowed (but penalized in the earlier stage), the first check should be changed. However, based on our understanding, the first check is correct, and the second check for early reveal is redundant and should be removed.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"
    },
    {
        "title": "3.7 TODO tags present in the code    ",
        "body": "  Resolution  Comment from Lien Protocol:  All TODOs in repositories were solved and removed.  Description  There are a few instances of TODO tags in the codebase that must be addressed before production as they correspond to commented-out code that makes up essential parts of the system.  Examples  code/MainContracts/contracts/Auction.sol:L310-L311  // require(strikePriceIDOLAmount > 10**10, 'at least 100 iDOL is required for the bid Amount'); // require $100 for spam protection // TODO  require(  code/MainContracts/contracts/BondMaker.sol:L392-L394  bytes32[] storage bondIDs = bondGroup.bondIDs;  // require(msg.value.mul(998).div(1000) > amount, 'fail to transfer Ether'); // TODO  code/MainContracts/contracts/BondMaker.sol:L402-L404  _issueNewBond(bondID, msg.sender, amount);  // transferETH(bondTokenAddress, msg.value - amount); // TODO  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"
    },
    {
        "title": "3.8 Documented function getERC20TokenDividend() does not exist    ",
        "body": "  Resolution  Comment from Lien Protocol:  Fairswap Fix ReadMe (README.md)  Description  In the README of Fairswap_iDOLvsLien, a function is listed which is not implemented in the codebase:  getERC20TokenDividend() function withdraws ETH and baseToken dividends for the Lien token stored in the exchange.(the dividends are stored in the contract at this moment)  Recommendation  Implement the function, or update the documentation  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"
    },
    {
        "title": "3.9 Fairswap interfaces are inconsistent    ",
        "body": "  Resolution  Comment from Lien Protocol:  Implement interface(contracts/Inheritance/TokenExchange.sol L11, contracts/Inheritance/ETHExchange.sol L12)  Description  There are unexpected inconsistencies between the three Fairswap contract interfaces, which may cause issues for composability with external contracts.  Examples  The function used to submit orders between the base and settlement currency has a different name across the three exchanges:  In Fairswap_iDOLvsETH it is called: orderEThToToken().  In Fairswap_iDOLvsLien it is called: OrderBaseToSettlement() (capitalized).  In Fairswap_iDOLvsImmmortalOptions it is called: orderBaseToSettlement().  Recommendation  Implement the desired interface in a separate file, and inherit it on the exchange contracts to ensure they are implemented as intended.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"
    },
    {
        "title": "3.10 Fairswap: inconsistent checks on _executionOrder()    ",
        "body": "  Resolution  Comment from Lien Protocol:  Fairswap- Integrate if statements about executeUnexecutedBox() (contracts/Inheritance/TokenExchange.sol L161, contracts/Inheritance/ETHExchange.sol L143, contracts/Inheritance/BoxExchange.sol L401-L405)  Description  The _executionOrder() function should only be called under specific conditions. However, these conditions are not always consistently defined.  Examples  code/Fairswap_iDOLvsLien/contracts/BoxExchange.sol:L218  if (nextBoxNumber > 1 && nextBoxNumber > nextExecuteBoxNumber) {  code/Fairswap_iDOLvsLien/contracts/BoxExchange.sol:L312  if (nextBoxNumber > 1 && nextBoxNumber > nextExecuteBoxNumber) {  code/Fairswap_iDOLvsLien/contracts/BoxExchange.sol:L647  if (nextBoxNumber > 1 && nextBoxNumber >= nextExecuteBoxNumber) {  Recommendation  Reduce duplicate code by defining an internal function to perform this check. A clear, descriptive name will help to clarify the intention.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"
    },
    {
        "title": "3.11 Inconsistency in DecimalSafeMath implementations    ",
        "body": "  Resolution  Comment from Lien Protocol:  Integrate and rename DecimalSafeMath to RateMath (contracts/Inheritance/RateMath.sol)  Description  There are two different implementations of DecimalSafeMath in the 3 FairSwap repositories.  Examples  FairSwap_iDOLvsLien/contracts/util/DecimalSafeMath.sol#L4-L11  library DecimalSafeMath {  function decimalDiv(uint256 a, uint256 b)internal pure returns (uint256) {  // assert(b > 0); // Solidity automatically throws when dividing by 0  uint256 a_ = a * 1000000000000000000;  uint256 c = a_ / b;  // assert(a == b * c + a % b); // There is no case in which this doesn't hold  return c;  Fairswap_iDOLvsETH/contracts/util/DecimalSafeMath.sol#L3-L11  library DecimalSafeMath {  function decimalDiv(uint256 a, uint256 b)internal pure returns (uint256) {  // assert(b > 0); // Solidity automatically throws when dividing by 0  uint256 c = (a * 1000000000000000000) / b;  // assert(a == b * c + a % b); // There is no case in which this doesn't hold  return c;  Recommendation  Try removing duplicate code/libraries and using a better inheritance model to include one file in all FairSwaps.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"
    },
    {
        "title": "4.1 ESMS use of sanitized user_amount & user_id values    ",
        "body": "  Resolution   Fixed in   https://github.com/nopslip/gtc-request-signer/pull/4/ , by using the sanitized integer value in the code flow.  Description  In the Signer service, values are properly checked, however the checked values are not preserved and the user input is passed down in the function.  The values are sanitized here:  code/gtc-request-signer-main-5eb22e882e28e6f3192b80f237f7a3bcd15b1ee9/app.py:L98-L108  try:  int(user_id)  except ValueError:  gtc_sig_app.logger.error('Invalid user_id received!')  return Response('{\"message\":\"ESMS error\"}', status=400, mimetype='application/json')  # make sure it's an int  try:  int(user_amount)  except ValueError:  gtc_sig_app.logger.error('Invalid user_amount received!')  return Response('{\"message\":\"ESMS error\"}', status=400, mimetype='application/json')  But the original user inputs are being used here:  code/gtc-request-signer-main-5eb22e882e28e6f3192b80f237f7a3bcd15b1ee9/app.py:L110-L113  try:  leaf = proofs[str(user_id)]['leaf']  proof = proofs[str(user_id)]['proof']  leaf_bytes = Web3.toBytes(hexstr=leaf)  code/gtc-request-signer-main-5eb22e882e28e6f3192b80f237f7a3bcd15b1ee9/app.py:L128-L131  # this is a bit of hack to avoid bug in old web3 on frontend  # this means that user_amount is not converted back to wei before tx is broadcast!  user_amount_in_eth = Web3.fromWei(user_amount, 'ether')  Examples  if a float amount is passed for user_amount, all checks will pass, however the final amount will be slightly different that what it is intended:  >>> print(str(Web3.fromWei(123456789012345, 'ether')))  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"
    },
    {
        "title": "0.000123456789012345",
        "body": "  >>> print(str(Web3.fromWei(123456789012345.123, 'ether')))  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"
    },
    {
        "title": "0.000123456789012345125",
        "body": "  Recommendation  After the sanity check, use the sanitized value for the rest of the code flow.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"
    },
    {
        "title": "4.2 Prefer using abi.encode in TokenDistributor    ",
        "body": "  Resolution   Fixed in   gitcoinco/governance#7  Description  The method _hashLeaf is called when a user claims their airdrop.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L128-L129  // can we repoduce leaf hash included in the claim?  require(_hashLeaf(user_id, user_amount, leaf), 'TokenDistributor: Leaf Hash Mismatch.');  This method receives the user_id and the user_amount as arguments.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L253-L257  /**  @notice hash user_id + claim amount together & compare results to leaf hash  @return boolean true on match  /  function _hashLeaf(uint32 user_id, uint256 user_amount, bytes32 leaf) private returns (bool) {  These arguments are abi encoded and hashed together to produce a unique hash.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L258  bytes32 leaf_hash = keccak256(abi.encodePacked(keccak256(abi.encodePacked(user_id, user_amount))));  This hash is checked against the third argument for equality.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L259  return leaf == leaf_hash;  If the hash matches the third argument, it returns true and considers the provided user_id and user_amount are correct.  However, packing differently sized arguments may produce collisions.  The Solidity documentation states that packing dynamic types will produce collisions, but this is also the case if packing uint32 and uint256.  Examples  Below there s an example showing that packing uint32 and uint256 in both orders can produce collisions with carefully picked values.  library Encode {  function encode32Plus256(uint32 _a, uint256 _b) public pure returns (bytes memory) {  return abi.encodePacked(_a, _b);  function encode256Plus32(uint256 _a, uint32 _b) public pure returns (bytes memory) {  return abi.encodePacked(_a, _b);  contract Hash {  function checkEqual() public pure returns (bytes32, bytes32) {  // Pack 1  uint32  a1 = 0x12345678;  uint256 b1 = 0x99999999999999999999999999999999999999999999999999999999FFFFFFFF;  // Pack 2  uint256 a2 = 0x1234567899999999999999999999999999999999999999999999999999999999;  uint32  b2 = 0xFFFFFFFF;  // Encode these 2 different values  bytes memory packed1 = Encode.encode32Plus256(a1, b1);  bytes memory packed2 = Encode.encode256Plus32(a2, b2);  // Check if the packed encodings match  require(keccak256(packed1) == keccak256(packed2), \"Hash of representation should match\");  // The hashes are the same  // 0x9e46e582607c5c6e05587dacf66d311c4ced0819378a41d4b4c5adf99d72408e  return (  keccak256(packed1),  keccak256(packed2)  );  Changing abi.encodePacked to abi.encode in the library will make the transaction fail with error message Hash of representation should match.  Recommendation  Unless there s a specific use case to use abi.encodePacked, you should always use abi.encode. You might need a few more bytes in the transaction data, but it prevents collisions. Similar fix can be achieved by using unit256 for both values to be packed to prevent any possible collisions.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"
    },
    {
        "title": "4.3 Simplify claim tokens for a gas discount and less code    ",
        "body": "  Resolution  Fixed in gitcoinco/governance#4  Structure Claim can still be removed for further optimization.  Description  The method claimTokens in TokenDistributor needs to do a few checks before it can distribute the tokens.  A few of these checks can be simplified and optimized.  The method hashMatch can be removed because it s only used once and the contents can be moved directly into the parent method.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L125-L126  // can we reproduce the same hash from the raw claim metadata?  require(hashMatch(user_id, user_address, user_amount, delegate_address, leaf, eth_signed_message_hash_hex), 'TokenDistributor: Hash Mismatch.');  Because this method also uses a few other internal calls, they also need to be moved into the parent method.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L211  return getDigest(claim) == eth_signed_message_hash_hex;  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L184  hashClaim(claim)  Moving the code directly in the parent method and removing them will improve gas costs for users.  The structure Claim can also be removed because it s not used anywhere else in the code.  Recommendation  Consider simplifying claimTokens and remove unused methods.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"
    },
    {
        "title": "4.4 ESMS use of environment variable for chain info [Optimization]    ",
        "body": "  Resolution   Fixed in   nopslip/gtc-request-signer#5 by moving the variables to the environment variable.  Description  Variables to create domain separator are hardcoded in the code, and it requires the modify code on different deployments (e.g. testnet, mainnet, etc).  Examples  code/gtc-request-signer-main-5eb22e882e28e6f3192b80f237f7a3bcd15b1ee9/app.py:L203-L208  domain = make_domain(  name='GTA',  version='1.0.0',  chainId=4,  verifyingContract='0xBD2525B5F0B2a663439a78A99A06605549D25cE5')  Recommendation  Use environment variable for these values. This way there is no need to change the source code on different deployments and it can be scripted to prevent any possible errors on the code base.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"
    },
    {
        "title": "4.5 Rename method _hashLeaf to something that represents the validity of the leaf    ",
        "body": "  Resolution   Closed because the method was removed in   gitcoinco/governance#4  Description  The method _hashLeaf accepts 3 arguments.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L257  function _hashLeaf(uint32 user_id, uint256 user_amount, bytes32 leaf) private returns (bool) {  The arguments user_id and user_amount are used to create a keccak256 hash.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L258  bytes32 leaf_hash = keccak256(abi.encodePacked(keccak256(abi.encodePacked(user_id, user_amount))));  This hash is then checked if it matches the third argument.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L259  return leaf == leaf_hash;  The result of the equality is returned by the method.  The name of the method is confusing because it should say that it returns true if the leaf is considered valid.  Recommendation  Consider renaming the method to something like isValidLeafHash.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"
    },
    {
        "title": "4.6 Method returns bool but result is never used in TokenDistributor.claimTokens    ",
        "body": "  Resolution   Removed in   gitcoinco/governance#4  Description  The method _delegateTokens is called when a user claims their tokens to automatically delegate the claimed tokens to their own address or to a different one.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L135  _delegateTokens(user_address, delegate_address);  The method accepts the addresses of the delegator and the delegate and returns a boolean.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L262-L270  /**  @notice execute call on token contract to delegate tokens  @return boolean true on success  /  function _delegateTokens(address delegator, address delegatee) private returns (bool) {  GTCErc20  GTCToken = GTCErc20(token);  GTCToken.delegateOnDist(delegator, delegatee);  return true;  But this boolean is never used.  Recommendation  Remove the returned boolean because it s always returned as true anyway and the transaction will be a bit cheaper.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"
    },
    {
        "title": "4.7 Use a unified compiler version for all contracts    ",
        "body": "  Resolution   Compiler versions updated to   gitcoinco/governance#2  Description  Currently the smart contracts for the Gitcoin token and governance use different versions of Solidity compiler (^0.5.16, 0.6.12 , 0.5.17).  Recommendation  It is suggested to use a unified compiler version for all contracts (e.g. 0.6.12).  Note that it is recommended to use the latest version of Solidity compiler with security patches (currently 0.8.3), although given that these contracts are forks of the battle tested Uniswap governance contracts, the Gitcoin team prefer to keep the modifications to the code at minimum.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"
    },
    {
        "title": "4.8 Improve efficiency by using immutable in TreasuryVester    ",
        "body": "  Resolution   Fixed in   gitcoinco/governance#5  Description  The TreasuryVester contract when deployed has a few fixed storage variables.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TreasuryVester.sol:L30  gtc = gtc_;  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TreasuryVester.sol:L33-L36  vestingAmount = vestingAmount_;  vestingBegin = vestingBegin_;  vestingCliff = vestingCliff_;  vestingEnd = vestingEnd_;  These storage variables are defined in the contract.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TreasuryVester.sol:L8  address public gtc;  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TreasuryVester.sol:L11-L14  uint public vestingAmount;  uint public vestingBegin;  uint public vestingCliff;  uint public vestingEnd;  But they are never changed.  Recommendation  Consider setting storage variables as immutable type for a considerable gas improvement.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"
    },
    {
        "title": "5.1 safeRagequit makes you lose funds     in Pull Pattern",
        "body": "  Resolution  Description  safeRagequit and ragequit functions are used for withdrawing funds from the LAO. The difference between them is that ragequit function tries to withdraw all the allowed tokens and safeRagequit function withdraws only some subset of these tokens, defined by the user. It s needed in case the user or GuildBank is blacklisted in some of the tokens and the transfer reverts. The problem is that even though you can quit in that case, you ll lose the tokens that you exclude from the list.  To be precise, the tokens are not completely lost, they will belong to the LAO and can still potentially be transferred to the user who quit. But that requires a lot of trust, coordination, time and anyone can steal some part of these tokens.  Recommendation  Implementing pull pattern for token withdrawals should solve the issue. Users will be able to quit the LAO and burn their shares but still keep their tokens in the LAO s contract for some time if they can t withdraw them right now.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"
    },
    {
        "title": "5.2 Creating proposal is not trustless     in Pull Pattern",
        "body": "  Resolution  this issue no longer exists in the Pull Pattern update, due to the fact that emergency processing and in function ERC20 transfers are removed.  Description  Usually, if someone submits a proposal and transfers some amount of tribute tokens, these tokens are transferred back if the proposal is rejected. But if the proposal is not processed before the emergency processing, these tokens will not be transferred back to the proposer. This might happen if a tribute token or a deposit token transfers are blocked.  code/contracts/Moloch.sol:L407-L411  if (!emergencyProcessing) {  require(  proposal.tributeToken.transfer(proposal.proposer, proposal.tributeOffered),  \"failing vote token transfer failed\"  );  Tokens are not completely lost in that case, they now belong to the LAO shareholders and they might try to return that money back. But that requires a lot of coordination and time and everyone who ragequits during that time will take a part of that tokens with them.  Recommendation  Pull pattern for token transfers would solve the issue.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"
    },
    {
        "title": "5.3 Emergency processing can be blocked     in Pull Pattern",
        "body": "  Resolution  Emergency Processing no longer exists in the Pull Pattern update.  Description  The main reason for the emergency processing mechanism is that there is a chance that some token transfers might be blocked. For example, a sender or a receiver is in the USDC blacklist. Emergency processing saves from this problem by not transferring tribute token back to the user (if there is some) and rejecting the proposal.  code/contracts/Moloch.sol:L407-L411  if (!emergencyProcessing) {  require(  proposal.tributeToken.transfer(proposal.proposer, proposal.tributeOffered),  \"failing vote token transfer failed\"  );  The problem is that there is still a deposit transfer back to the sponsor and it could be potentially blocked too. If that happens, proposal can t be processed and the LAO is blocked.  Recommendation  Implementing pull pattern for all token withdrawals would solve the problem. The alternative solution would be to also keep the deposit tokens in the LAO, but that makes sponsoring the proposal more risky for the sponsor.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"
    },
    {
        "title": "5.4 Token Overflow might result in system halt or loss of funds    ",
        "body": "  Resolution   Fixed in   fd2da6, and  32ad9b by allowing overflows in most balance calculations (e.g.  Description  If a token overflows, some functionality such as processProposal, cancelProposal will break due to safeMath reverts. The overflow could happen because the supply of the token was artificially inflated to oblivion.  This issue was pointed out by Heiko Fisch in Telegram chat.  Examples  Any function using internalTransfer() can result in an overflow:  contracts/Moloch.sol:L631-L634  function max(uint256 x, uint256 y) internal pure returns (uint256) {  return x >= y ? x : y;  Recommendation  We recommend to allow overflow for broken or malicious tokens. This is to prevent system halt or loss of funds. It should be noted that in case an overflow occurs, the balance of the token will be incorrect for all token holders in the system.  rageKick, rageQuit were fixed by not using safeMath within the function code, however this fix is risky and not recommended, as there are other overflows in other functions that might still result in system halt or loss of funds.  One suggestion is having a function named unsafeInternalTransfer() which does not use safeMath for the cases that overflow should be allowed. This mainly adds better readability to the code.  It is still a risky fix and a better solution should be planned.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"
    },
    {
        "title": "5.5 Whitelisted tokens limit    ",
        "body": "  Resolution  mitigated by having separate limits for number of whitelisted tokens (for non-zero balance and for zero balance) in 486f1b3 and follow up commits. That s helpful because it s much cheaper to process tokens with zero balance in the guild bank and you can have much more whitelisted tokens overall.  uint256 constant MAX_TOKEN_WHITELIST_COUNT = 400; // maximum number of whitelisted tokens  uint256 constant MAX_TOKEN_GUILDBANK_COUNT = 200; // maximum number of tokens with non-zero balance in guildbank  uint256 public totalGuildBankTokens = 0; // total tokens with non-zero balance in guild bank  It should be noted that this is an estimated limit based on the manual calculations and current OP code gas costs. DAO members should consider splitting the DAO into two if more than 100 tokens with non-zero balance are used in the DAO to be safe.  Description  _ragequit function is iterating over all whitelisted tokens:  contracts/Moloch.sol:L507-L513  for (uint256 i = 0; i < tokens.length; i++) {  uint256 amountToRagequit = fairShare(userTokenBalances[GUILD][tokens[i]], sharesAndLootToBurn, initialTotalSharesAndLoot);  // deliberately not using safemath here to keep overflows from preventing the function execution (which would break ragekicks)  // if a token overflows, it is because the supply was artificially inflated to oblivion, so we probably don't care about it anyways  userTokenBalances[GUILD][tokens[i]] -= amountToRagequit;  userTokenBalances[memberAddress][tokens[i]] += amountToRagequit;  If the number of tokens is too big, a transaction can run out of gas and all funds will be blocked forever. Ballpark estimation of this number is around 300 tokens based on the current OpCode gas costs and the block gas limit.  Recommendation  A simple solution would be just limiting the number of whitelisted tokens.  If the intention is to invest in many new tokens over time, and it s not an option to limit the number of whitelisted tokens, it s possible to add a function that removes tokens from the whitelist. For example, it s possible to add a new type of proposals, that is used to vote on token removal if the balance of this token is zero. Before voting for that, shareholders should sell all the balance of that token.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"
    },
    {
        "title": "5.6 Summoner can steal funds using bailout     in Pull Pattern",
        "body": "  Resolution  Description  Currently, there are 2 major reasons for using the bailout function:  Kick someone out of the LAO. If the shareholders vote for kicking somebody, the kicked user goes to jail at first. If the LAO kicks someone, it s important not to steal user s funds, but remove them from profit-sharing as soon as possible. Currently, because the user can potentially block some token transfers, funds can t be transferred and the user is still having loot and is participation in a profit-sharing. In order to avoid that, bailout function was introduced. It allows anyone to transfer kicked user s funds to the summoner if the user does not call safeRagequit (which forces the user to lose some funds). The intention is for the summoner to transfer these funds to the kicked member afterwards. The issue here is that it requires a lot of trust to the summoner on the one hand, and requires more time to kick the member out of the LAO.   lost private key  problem. If someone s private key was lost, shareholders can allow summoner to transfer funds from any user whose keys were lost. The problem is that any member s funds can be stolen by the LAO members and the summoner like that. So every member should keep track of that kind of proposal and is forced to do the ragequit if that proposal passes. That decreases trustlessness because if a user is not tracking the system for some time, the user s money can possibly be stolen.  Recommendation  To solve these issues, these 2 intentions should be split into 2 different mechanisms. By implementing pull pattern for token transfers, kicked member won t be able to block the ragekick and the LAO members would be able to kick anyone much quicker. There is no need to keep the bailout for this intention.  If  lost private key  problem should be addressed in the LAO, the time period for the funds recovery should be big because there is no need to do the recovery asap. Recovery can be done without a preliminary kick and can even cover not only the shares and loot, but also tokens that should be withdrawn (if pull pattern is implemented)  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"
    },
    {
        "title": "5.7 Sponsorship front-running     in Pull Pattern",
        "body": "  Resolution  this issue no longer exists in the Pull Pattern update with Major severity, as mentioned in the recommendation, the front-running vector is still open but no rationale exist for such a behaviour.  Description  If proposal submission and sponsorship are done in 2 different transactions, it s possible to front-run the sponsorProposal function by any member. The incentive to do that is to be able to block the proposal afterwards. It s sometimes possible to block the proposal by getting blacklisted at depositToken. In that case, the proposal won t be accepted and the emergency processing is going to happen next. Currently, if the attacker can become whitelisted again, he might even not lose the deposit tokens. If not, it will block the whole system forever and everyone would have to ragequit (but that s the part of another issue).  Recommendation  Pull pattern for token transfers will solve the issue. Front-running will still be possible but it doesn t affect anything.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"
    },
    {
        "title": "5.8 Delegate assignment front-running   ",
        "body": "  Description  Any member can front-run another member s delegateKey assignment.  if you try to submit an address as your delegateKey, someone else can try to assign your delegate address tp themselves. While incentive of this action is unclear, it s possible to block some address from being a delegate forever. ragekick and ragequit do not free the delegate address and the delegate itself also cannot change the address.  The possible attack could be that a well-known hard-to-replace multisig address is assigned as a delegateKey and someone else take this address to block it. Also, if the malicious member is about to ragequit or be kicked, it s possible to do this attack without losing anything.  The only way to free the delegate is to make it a member, but then it can never be a delegate after.  Recommendation  Make it possible for a delegateKey to approve delegateKey assignment or cancel the current delegation. And additionally, it may be valuable to clear the delegate address in the _ragequit function.  Commit-reveal methods can also be used to mitigate this attack.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"
    },
    {
        "title": "5.9 No votes are still valid after the ragequit/ragekick   ",
        "body": "  Description  Shareholders can vote for the upcoming proposals 2 weeks before they can be executed. If they ragequit or get ragekicked, their votes are still considered valid. And while the LAO does not allow anyone to ragequit before the last proposal with Yes vote is processed, it s still possible to quit the LAO and having active No votes on some proposals.  It s not naturally expected behaviour because by that time a user ragequits, they are not part of the LAO and do not have any voting power. Moreover, there is no incentive not to vote No just to fail all the possible proposals, because the user won t be sharing any consequences of the result of these proposals. And even incentivized to vote No for every proposal just as the act of revenge for the ragekick.  Recommendation  The problem is mitigated by the fact that all rejected proposals can be submitted again and be processed a few weeks after.  It s possible to remove all the No votes from the proposals after user s ragekick/ragequit.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"
    },
    {
        "title": "5.10 Dilution bound should be a fixed-point number   ",
        "body": "  Resolution  a per-proposal dilution bound was considered for the v1, but kept it global in the interest of code simplicity.  Description  The dilution bound is designed to mitigate an issue where a proposal is passed, then many users ragequit from the DAO and the remaining members have to pay more than they initially intended to. Because of that, the proposal will be automatically rejected if the total amount of shares becomes dilutionBound times less than it was before. The problem is that dilutionBound is an integer value and it s impossible to configure it to decimal values such as 1.2, for example.  Recommendation  Make dilutionBound a fixed-point number.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"
    },
    {
        "title": "5.11 Whitelist proposal duplicate   ",
        "body": "  Description  Every time when a whitelist proposal is sponsored, it s checked that there is no other sponsored whitelist proposal with the same token. This is done in order to avoid proposal duplicates.  code/contracts/Moloch.sol:L277-L281  // whitelist proposal  if (proposal.flags[4]) {  require(!tokenWhitelist[address(proposal.tributeToken)], \"cannot already have whitelisted the token\");  require(!proposedToWhitelist[address(proposal.tributeToken)], 'already proposed to whitelist');  proposedToWhitelist[address(proposal.tributeToken)] = true;  The issue is that even though you can t sponsor a duplicate proposal, you can still submit a new proposal with the same token.  Recommendation  Check that there is currently no sponsored proposal with the same token on proposal submission.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"
    },
    {
        "title": "5.12 Moloch - bool[6] flags can be changed to a dedicated structure   ",
        "body": "  Resolution   The Moloch team decided to leave the   Description  The Moloch contract uses a structure that includes an array of bools to store a few flags about the proposal:  code/contracts/Moloch.sol:L88  bool[6] flags; // [sponsored, processed, didPass, cancelled, whitelist, guildkick]  This makes reasoning about the correctness of the code a bit complicated because one needs to remember what each item in the flag list stands for. The make the reader s life simpler a dedicated structure can be created that incorporates all of the required flags.  Examples  bool[6] memory flags; // [sponsored, processed, didPass, cancelled, whitelist, guildkick]  Recommendation  Based on the provided examples change the bool[6] flags to the proposed examples.  Flags as bool array with enum (proposed)  This second contract implements the flags as a defined structure with each named element representing a specific flag. This method makes clear which flag is accessed because they are referred to by the name, not by the index.  This third contract has the least amount of changes to the code and uses an enum structure to handle the index.  pragma solidity 0.5.15;  contract FlagsEnum {  struct Proposal {  address applicant;  uint value;  bool[3] flags; // [sponsored, processed, kicked]  enum ProposalFlags {  SPONSORED,  PROCESSED,  KICKED  uint proposalCount;  mapping(uint256 => Proposal) public proposals;  function addProposal(uint _value, bool _sponsored, bool _processed, bool _kicked) public returns (uint) {  Proposal memory proposal = Proposal({  applicant: msg.sender,  value: _value,  flags: [_sponsored, _processed, _kicked]  });  proposals[proposalCount] = proposal;  proposalCount += 1;  return (proposalCount);  function getProposal(uint _proposalId) public view returns (address, uint, bool, bool, bool) {  return (  proposals[_proposalId].applicant,  proposals[_proposalId].value,  proposals[_proposalId].flags[uint(ProposalFlags.SPONSORED)],  proposals[_proposalId].flags[uint(ProposalFlags.PROCESSED)],  proposals[_proposalId].flags[uint(ProposalFlags.KICKED)]  );  6 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"
    },
    {
        "title": "6.1 MythX",
        "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"
    },
    {
        "title": "6.2 Ethlint",
        "body": "  Ethlint is an open source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  contracts/GuildBank.sol  13:8     warning    Provide an error message for require()                                                     error-reason  23:12    warning    Provide an error message for require()                                                     error-reason  34:8     warning    Provide an error message for require()                                                     error-reason  36:27    warning    There should be no whitespace or comments between the opening brace '{' and first item.    whitespace  36:37    warning    There should be no whitespace or comments between the last item and closing brace '}'.     whitespace  contracts/Moloch.sol  34:4      warning    Line exceeds the limit of 145 characters                                   max-len  41:4      warning    Line exceeds the limit of 145 characters                                   max-len  42:4      warning    Line exceeds the limit of 145 characters                                   max-len  76:8      warning    Line exceeds the limit of 145 characters                                   max-len  169:24    warning    Avoid using 'now' (alias to 'block.timestamp').                            security/no-block-members  262:8     warning    Line exceeds the limit of 145 characters                                   max-len  272:49    error      String literal must be quoted with double quotes.                          quotes  280:74    error      String literal must be quoted with double quotes.                          quotes  285:57    error      String literal must be quoted with double quotes.                          quotes  362:8     warning    Line exceeds the limit of 145 characters                                   max-len  517:8     warning    Line exceeds the limit of 145 characters                                   max-len  540:13    warning    Assignment operator must have exactly single space on both sides of it.    operator-whitespace  559:8     warning    Error message exceeds max length of 76 characters                          error-reason  583:8     warning    Error message exceeds max length of 76 characters                          error-reason  641:15    warning    Avoid using 'now' (alias to 'block.timestamp').                            security/no-block-members  contracts/oz/ERC20.sol  128:8    warning    Provide an error message for require()    error-reason  143:8    warning    Provide an error message for require()    error-reason  157:8    warning    Provide an error message for require()    error-reason  171:8    warning    Provide an error message for require()    error-reason  172:8    warning    Provide an error message for require()    error-reason  contracts/oz/SafeMath.sol  10:8    warning    Provide an error message for require()    error-reason  17:8    warning    Provide an error message for require()    error-reason  24:8    warning    Provide an error message for require()    error-reason  32:8    warning    Provide an error message for require()    error-reason  contracts/test-helpers/Submitter.sol  9:2     error    Only use indent of 4 spaces.    indentation  11:2    error    Only use indent of 4 spaces.    indentation  13:2    error    Only use indent of 4 spaces.    indentation  15:0    error    Only use indent of 4 spaces.    indentation  17:2    error    Only use indent of 4 spaces.    indentation  39:0    error    Only use indent of 4 spaces.    indentation  41:2    error    Only use indent of 4 spaces.    indentation  51:0    error    Only use indent of 4 spaces.    indentation  53:2    error    Only use indent of 4 spaces.    indentation  63:0    error    Only use indent of 4 spaces.    indentation  contracts/tokens/ClaimsToken.sol  95:1     error      Only use indent of 4 spaces.              indentation  98:1     error      Only use indent of 4 spaces.              indentation  100:1    error      Only use indent of 4 spaces.              indentation  102:1    error      Only use indent of 4 spaces.              indentation  105:1    error      Only use indent of 4 spaces.              indentation  112:0    error      Only use indent of 4 spaces.              indentation  121:1    error      Only use indent of 4 spaces.              indentation  129:0    error      Only use indent of 4 spaces.              indentation  140:1    error      Only use indent of 4 spaces.              indentation  148:0    error      Only use indent of 4 spaces.              indentation  154:1    error      Only use indent of 4 spaces.              indentation  160:0    error      Only use indent of 4 spaces.              indentation  167:1    error      Only use indent of 4 spaces.              indentation  173:0    error      Only use indent of 4 spaces.              indentation  180:1    error      Only use indent of 4 spaces.              indentation  184:0    error      Only use indent of 4 spaces.              indentation  190:1    error      Only use indent of 4 spaces.              indentation  197:0    error      Only use indent of 4 spaces.              indentation  203:1    error      Only use indent of 4 spaces.              indentation  208:0    error      Only use indent of 4 spaces.              indentation  216:1    error      Only use indent of 4 spaces.              indentation  226:0    error      Only use indent of 4 spaces.              indentation  232:1    error      Only use indent of 4 spaces.              indentation  234:1    error      Only use indent of 4 spaces.              indentation  237:0    error      Only use indent of 4 spaces.              indentation  239:1    error      Only use indent of 4 spaces.              indentation  243:2    warning    Provide an error message for require()    error-reason  246:0    error      Only use indent of 4 spaces.              indentation  251:1    error      Only use indent of 4 spaces.              indentation  260:0    error      Only use indent of 4 spaces.              indentation  268:1    error      Only use indent of 4 spaces.              indentation  276:0    error      Only use indent of 4 spaces.              indentation  contracts/tokens/Token.sol  25:8    warning    Provide an error message for require()    error-reason  \u2716 44 errors, 28 warnings found.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"
    },
    {
        "title": "6.3 Surya",
        "body": "  Surya is a utility tool for smart contract systems. It provides a number of visual outputs and information about the structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  Files Description Table  contracts/GuildBank.sol  d4329bc7836a1800eb2376da05f76f1783700b9a  contracts/Moloch.sol  8f55cc17fcf0488acdc9dd2261dca1e08f42c4ac  contracts/oz/ERC20.sol  6db943e86683ce536b8e75e79d7fb80a02b855ae  contracts/oz/IERC20.sol  f249341b598ed60fdb987fc6dd05b6cd15da7b6b  contracts/oz/ReentrancyGuard.sol  115a19532af141450ea30ad141aecb76b79035b4  contracts/oz/SafeMath.sol  b86ab5a6679fd597c3a0412d31080893beeb653a  contracts/test-helpers/Submitter.sol  7b29e3178cb4c7848851a8c92661a0e12fee7489  contracts/tokens/ClaimsToken.sol  11bb8b648de195efbca13df15e10b3e6a75fcab6  contracts/tokens/Token.sol  7c193d22ad069e368aba4fa9bc3d4c28e8e1973b  Contracts Description Table  Function Name  Visibility  Mutability  Modifiers  GuildBank  Implementation  <Constructor>  Public    withdraw  Public    onlyOwner  withdrawToken  Public    onlyOwner  fairShare  Internal \ud83d\udd12  Moloch  Implementation  ReentrancyGuard  <Constructor>  Public    submitProposal  Public    nonReentrant  submitWhitelistProposal  Public    nonReentrant  submitGuildKickProposal  Public    nonReentrant  _submitProposal  Internal \ud83d\udd12  sponsorProposal  Public    nonReentrant onlyDelegate  submitVote  Public    nonReentrant onlyDelegate  processProposal  Public    nonReentrant  processWhitelistProposal  Public    nonReentrant  processGuildKickProposal  Public    nonReentrant  _didPass  Internal \ud83d\udd12  _validateProposalForProcessing  Internal \ud83d\udd12  _returnDeposit  Internal \ud83d\udd12  ragequit  Public    nonReentrant onlyMember  safeRagequit  Public    nonReentrant onlyMember  _ragequit  Internal \ud83d\udd12  ragekick  Public    nonReentrant  bailout  Public    nonReentrant  cancelProposal  Public    nonReentrant  updateDelegateKey  Public    nonReentrant onlyShareholder  max  Internal \ud83d\udd12  getCurrentPeriod  Public    NO   getProposalQueueLength  Public    NO   getProposalFlags  Public    NO   canRagequit  Public    NO   canBailout  Public    NO   hasVotingPeriodExpired  Public    NO   getMemberProposalVote  Public    NO   ERC20  Implementation  IERC20  totalSupply  Public    NO   balanceOf  Public    NO   allowance  Public    NO   transfer  Public    NO   approve  Public    NO   transferFrom  Public    NO   increaseAllowance  Public    NO   decreaseAllowance  Public    NO   _transfer  Internal \ud83d\udd12  _mint  Internal \ud83d\udd12  _burn  Internal \ud83d\udd12  _approve  Internal \ud83d\udd12  _burnFrom  Internal \ud83d\udd12  IERC20  Interface  transfer  External    NO   approve  External    NO   transferFrom  External    NO   totalSupply  External    NO   balanceOf  External    NO   allowance  External    NO   ReentrancyGuard  Implementation  <Constructor>  Internal \ud83d\udd12  SafeMath  Library  mul  Internal \ud83d\udd12  div  Internal \ud83d\udd12  sub  Internal \ud83d\udd12  add  Internal \ud83d\udd12  Submitter  Implementation  <Constructor>  Public    submitProposal  Public    NO   submitWhitelistProposal  Public    NO   submitGuildKickProposal  Public    NO   ERC20Detailed  Implementation  IERC20  <Constructor>  Public    name  Public    NO   symbol  Public    NO   decimals  Public    NO   IClaimsToken  Interface  withdrawFunds  External    NO   availableFunds  External    NO   totalReceivedFunds  External    NO   ClaimsToken  Implementation  IClaimsToken, ERC20, ERC20Detailed  <Constructor>  Public    ERC20Detailed  transfer  Public    NO   transferFrom  Public    NO   totalReceivedFunds  External    NO   availableFunds  Public    NO   _registerFunds  Internal \ud83d\udd12  _calcUnprocessedFunds  Internal \ud83d\udd12  _claimFunds  Internal \ud83d\udd12  _prepareWithdraw  Internal \ud83d\udd12  ClaimsTokenERC20Extension  Implementation  IClaimsToken, ClaimsToken  <Constructor>  Public    ClaimsToken  withdrawFunds  External    NO   tokenFallback  Public    onlyFundsToken  Token  Implementation  ERC20  <Constructor>  Public    updateTransfersEnabled  External    NO   updateTransfersReturningFalse  External    NO   transfer  Public    NO   Legend  Function can modify state  Function is payable  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"
    },
    {
        "title": "4.1 Reentrancy vulnerability in MetaSwap.swap()    ",
        "body": "  Resolution   This is fixed in   ConsenSys/metaswap-contracts@8de01f6.  Description  MetaSwap.swap() should have a reentrancy guard.  The adapters use this general process:  Collect the from token (or ether) from the user.  Execute the trade.  Transfer the contract s balance of tokens (from and to) and ether to the user.  If an attacker is able to reenter swap() before step 3, they can execute their own trade using the same tokens and get all the tokens for themselves.  This is partially mitigated by the check against amountTo in CommonAdapter, but note that the amountTo typically allows for slippage, so it may still leave room for an attacker to siphon off some amount while still returning the required minimum to the user.  code/contracts/adapters/CommonAdapter.sol:L57-L62  // Transfer remaining balance of tokenTo to sender  if (address(tokenTo) != Constants.ETH) {  uint256 balance = tokenTo.balanceOf(address(this));  require(balance >= amountTo, \"INSUFFICIENT_AMOUNT\");  _transfer(tokenTo, balance, recipient);  } else {  Examples  As an example of how this could be exploited, 0x supports an  EIP1271Wallet  signature type, which invokes an external contract to check whether a trade is allowed. A malicious maker might front run the swap to reduce their inventory. This way, the taker is sending more of the taker asset than necessary to MetaSwap. The excess can be stolen by the maker during the EIP1271 call.  Recommendation  Use a simple reentrancy guard, such as OpenZeppelin s ReentrancyGuard to prevent reentrancy in MetaSwap.swap(). It might seem more obvious to put this check in Spender.swap(), but the Spender contract intentionally does not use any storage to avoid interference between different adapters.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"
    },
    {
        "title": "4.2 A new malicious adapter can access users  tokens    ",
        "body": "  Resolution   This is fixed in   ConsenSys/metaswap-contracts@8de01f6.  Description  The purpose of the MetaSwap contract is to save users gas costs when dealing with a number of different aggregators. They can just approve() their tokens to be spent by MetaSwap (or in a later architecture, the Spender contract). They can then perform trades with all supported aggregators without having to reapprove anything.  A downside to this design is that a malicious (or buggy) adapter has access to a large collection of valuable assets. Even a user who has diligently checked all existing adapter code before interacting with MetaSwap runs the risk of having their funds intercepted by a new malicious adapter that s added later.  Recommendation  There are a number of designs that could be used to mitigate this type of attack. After discussion and iteration with the client team, we settled on a pattern where the MetaSwap contract is the only contract that receives token approval. It then moves tokens to the Spender contract before that contract DELEGATECALLs to the appropriate adapter. In this model, newly added adapters shouldn t be able to access users  funds.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"
    },
    {
        "title": "4.3 Owner can front-run traders by updating adapters    ",
        "body": "  Resolution   This is fixed in   ConsenSys/metaswap-contracts@8de01f6.  Description  MetaSwap owners can front-run users to swap an adapter implementation. This could be used by a malicious or compromised owner to steal from users.  Because adapters are DELEGATECALLed, they can modify storage. This means any adapter can overwrite the logic of another adapter, regardless of what policies are put in place at the contract level. Users must fully trust every adapter because just one malicious adapter could change the logic of all other adapters.  Recommendation  At a minimum, disallow modification of existing adapters. Instead, simply add new adapters and disable the old ones. (They should be deleted, but the aggregator IDs of deleted adapters should never be reused.)  This is, however, insufficient. A new malicious adapter could still overwrite the adapter mapping to modify existing adapters. To fully address this issue, the adapter registry should be in a separate contract. Through discussion and iteration with the client team, we settled on the following pattern:  MetaSwap contains the adapter registry. It calls into a new Spender contract.  The Spender contract has no storage at all and is just used to DELEGATECALL to the adapter contracts.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"
    },
    {
        "title": "4.4 Simplify fee calculation in WethAdapter    ",
        "body": "  Resolution  ConsenSys/metaswap-contracts@93bf5c6.  Description  WethAdapter does some arithmetic to keep track of how much ether is being provided as a fee versus as funds that should be transferred into WETH:  code/contracts/adapters/WethAdapter.sol:L41-L59  // Some aggregators require ETH fees  uint256 fee = msg.value;  if (address(tokenFrom) == Constants.ETH) {  // If tokenFrom is ETH, msg.value = fee + amountFrom (total fee could be 0)  require(amountFrom <= fee, \"MSG_VAL_INSUFFICIENT\");  fee -= amountFrom;  // Can't deal with ETH, convert to WETH  IWETH weth = getWETH();  weth.deposit{value: amountFrom}();  _approveSpender(weth, spender, amountFrom);  } else {  // Otherwise capture tokens from sender  // tokenFrom.safeTransferFrom(recipient, address(this), amountFrom);  _approveSpender(tokenFrom, spender, amountFrom);  // Perform the swap  aggregator.functionCallWithValue(abi.encodePacked(method, data), fee);  This code can be simplified by using address(this).balance instead.  Recommendation  Consider something like the following code instead:  if (address(tokenFrom) == Constants.ETH) {  getWETH().deposit{value: amountFrom}(); // will revert if the contract has an insufficient balance  _approveSpender(weth, spender, amountFrom);  } else {  tokenFrom.safeTransferFrom(recipient, address(this), amountFrom);  _approveSpender(tokenFrom, spender, amountFrom);  // Send the remaining balance as the fee.  aggregator.functionCallWithValue(abi.encodePacked(method, data), address(this).balance);  Aside from being a little simpler, this way of writing the code makes it obvious that the full balance is being properly consumed. Part is traded, and the rest is sent as a fee.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"
    },
    {
        "title": "4.5 Consider checking adapter existence in MetaSwap    ",
        "body": "  Resolution   The MetaSwap team found that doing the check in   Description  MetaSwap doesn t check that an adapter exists before calling into Spender:  code/contracts/MetaSwap.sol:L87-L100  function swap(  string calldata aggregatorId,  IERC20 tokenFrom,  uint256 amount,  bytes calldata data  ) external payable whenNotPaused nonReentrant {  Adapter storage adapter = adapters[aggregatorId];  if (address(tokenFrom) != Constants.ETH) {  tokenFrom.safeTransferFrom(msg.sender, address(spender), amount);  spender.swap{value: msg.value}(  adapter.addr,  Then Spender performs the check and reverts if it receives address(0).  code/contracts/Spender.sol:L15-L16  function swap(address adapter, bytes calldata data) external payable {  require(adapter != address(0), \"ADAPTER_NOT_SUPPORTED\");  It can be difficult to decide where to put a check like this, especially when the operation spans multiple contracts. Arguments can be made for either choice (or even duplicating the check), but as a general rule it s a good idea to avoid passing invalid parameters internally. Checking for adapter existence in MetaSwap.swap() is a natural place to do input validation, and it means Spender can have a simpler model where it trusts its inputs (which always come from MetaSwap).  Recommendation  Drop the check from Spender.swap() and perform the check instead in MetaSwap.swap().  5 Second Assessment  We performed a second assessment between October 3rd and October 4th, 2020. The engagement was conducted primarily by Steve Marx. The total effort expended was 2 person-days.  This second assessment covered three new features added by the MetaSwap team:  Support for the CHI gas token   This allows users to offset their gas costs by burning gas tokens. These tokens can come from the user or from tokens that are owned by the MetaSwap contract itself.  Uniswap Adapter   This adapter allows swaps to be executed via the Uniswap v2 Router directly, rather than going through some other exchange first.  Fee collection   FeeCommonAdapter and FeeWethAdapter are fee-collecting versions of the original CommonAdapter and WethAdapter. They support an extra parameter fee, indicating the quantity of the from asset to be sent to a fee wallet.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"
    },
    {
        "title": "5.1 Scope for the Second Assessment",
        "body": "  The following files were in scope for the second assessment:  MetaSwap.sol  5d66ea56c131b3ad5246e9fc6c126a0b7ba497fa  adapters/FeeCommonAdapter.sol  1bb0e2b4f7fca8e0d98113cf152eeb6be4ff13c7  adapters/FeeWethAdapter.sol  f844d9e13bd2cbf52a81ae4637b35f214098f3b2  adapters/UniswapAdapter.sol  d0733f6f4567dc58d3caf4af8875e17824a97f2d  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"
    },
    {
        "title": "5.2 Security Specification",
        "body": "  The security specification hasn t change much from the original assessment, so please refer to that. There are two significant changes to the security model: fee collection and gas token ownership.  In the new code, fees are collected, but these fees can be seen as voluntary from the perspective of the smart contracts. Users are free to pass any value for the fee parameter, including 0 to avoid all fees. The assumption is that most users will not bother to change the fee suggested by the MetaSwap API.  The other significant change is the introduction of the CHI gas token. In particular, the ability to use gas tokens held by the MetaSwap contract opens a new potential attack surface. Indeed, we found that an attacker could use contract-held tokens for other purposes.  6 Second Assessment Issues  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"
    },
    {
        "title": "6.1 Attacker can abuse gas tokens stored in MetaSwap    ",
        "body": "  Resolution   This function was removed in   ConsenSys/metaswap-contracts@75c4454.  Description  MetaSwap.swapUsingGasToken() allows users to make use of gas tokens held by the MetaSwap contract itself to reduce the gas cost of trades.  This mechanism is unsafe because any tokens held by the contract can be used by an attacker for other purposes.  Examples  An attack could also be made by using an existing token that makes external calls (e.g. an ERC777 token) or a mechanism in an aggregated exchange that makes external calls (e.g. wallet signatures in 0x).  Recommendation  The simplest way to avoid this vulnerability is to never transfer CHI gas tokens to MetaSwap at all. An alternative would be to only allow gas tokens to be used by approved transactions from the MetaSwap API. A possible mechanism for that would be to require a signature from the MetaSwap API. If such a signature were only provided in known-good situations (which are admittedly hard to define), it wouldn t be possible for an attacker to misuse the tokens.  7 Third Assessment  We performed a third assessment between November 7th and November 10th, 2020. The engagement was conducted primarily by Steve Marx. The total expended effort was 4 person-hours.  This third assessment covered the new FeeDistributor contract, which divides assets among a number of recipients. It s used in the MetaSwap system to distribute fees. Each recipient has a number of  shares , and assets are divided according to each recipients portion of share ownership. Potential assets include ether and ERC20-compatible tokens.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"
    },
    {
        "title": "7.1 Scope for the third assessment",
        "body": "  The only contract in scope was the FeeDistributor:  FeeDistributor.sol  23749a338461db92a96ae87a2fd454d1aa0cbb92  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"
    },
    {
        "title": "7.2 Security Specification",
        "body": "  At setup, the FeeDistributor is initialized with a number of recipients, each with a corresponding number of shares.  Recipients should be able to withdraw their fair share (<recipient's shares> / <total shares>) of any stored asset at any time.  No recipient should receive more than their fair share of an asset.  8 Third Assessment Recommendations  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"
    },
    {
        "title": "8.1 Document assumptions about ERC20 tokens",
        "body": "  Most ERC20-compatible tokens can be used with the FeeDistributor contract, but it s wise to document some assumptions made by the contract:  Token balances will not be too big (relative to the number of shares). Specifically, the total number of token units received by the contract must be able to be multiplied by the largest share amount held by a recipient.  Token balances will not be too small (relative to share amounts). It s impossible to divide a balance of 1 among more than 1 recipient. To be safe, it would be good to make sure that no one cares about losing less than totalShares token units. For example, if there are 1,000,000 total shares, an asset like ether would not be a problem because 1,000,000 wei is a trivial amount.  Token balances will not decrease without an explicit transfer. The contract makes the assumption that it can always compute the total received tokens by adding tokenBalance(token) and _totalWithdrawn[token]. This is not the case if the token balance can be manipulated externally.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"
    },
    {
        "title": "8.2 Only allow full withdrawal",
        "body": "  The current code has both   ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"
    },
    {
        "title": "8.3 Drop the recipient parameter",
        "body": "  Everywhere in the code, the   9 Third Assessment Issues  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"
    },
    {
        "title": "9.1 Simplify accounting and better handle remainders    ",
        "body": "  Resolution   This was fixed in   ConsenSys/metaswap-contracts@f0a62e5. The accounting was reworked according to the recommendation here.  Description  The current code does some fairly complex and redundant calculations during withdrawal to keep track of various pieces of state. In particular, the pair of _available[recipient][token] and _totalOnLastUpdate[recipient][token] is difficult to describe and reason about.  Recommendation  For a given token and recipient, we recommend instead just tracking how much has already been withdrawn. The rest can be easily calculated:  function earned(IERC20 token, address recipient) public view returns (uint256) {  uint256 totalReceived = tokenBalance(token).add(_totalWithdrawn[token]);  return totalReceived.mul(shares[recipient]).div(totalShares);  function available(IERC20 token, address recipient) public view returns (uint256) {  return earned(token, recipient).sub(_withdrawn[token][recipient]);  function withdraw(IERC20[] calldata tokens) external {  for (uint256 i = 0; i < tokens.length; i++) {  IERC20 token = tokens[i];  uint256 amount = available(token, msg.sender);  _withdrawn[token][msg.sender] += amount;  _totalWithdrawn[token] += amount;  _transfer(token, msg.sender, amount);  emit Withdrawal(tokens, msg.sender);  This code is easier to reason about:  It s easy to see that withdrawn[token][msg.sender] is correct because it s only increased when there s a corresponding transfer.  It s easy to see that _totalWithdrawn[token] is correct for the same reason.  It s easy to see that earned() is correct under standard assumptions about ERC20 balances.  It s easy to see that available() is correct, as it s just the earned amount less the already-withdrawn amount.  Remainders are better handled. If 1 token unit is available and you own half the shares, nothing happens on withdrawal, and if there are later 2 token units available, you can withdraw 1. (Under the previous code, if you tried to withdraw when 1 token unit was available, you would be unable to withdraw when 2 were available.)  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"
    },
    {
        "title": "5.1 Code readability - Rename priceDeviation to maxPriceDeviation   ",
        "body": "  Resolution  The variable was renamed.  Description  Improve code readability by renaming the state variable priceDeviation to maxPriceDeviation, distinguishing it from the local variable price_deviation and indicating that the variable is a limit as outlined in the specification (MAX_DEVIATION).  Balancer  code/aave-balancer-3e8367ab/contracts/proxies/BalancerSharedPoolPriceProvider.sol:L124-L129  if (  price_deviation > (BONE + priceDeviation) ||  price_deviation < (BONE - priceDeviation)  ) {  return true;  Uniswapv2  code/aave-uniswapv2-e81cf872/contracts/proxies/UniswapV2PriceProvider.sol:L83-L95  if (  price_deviation > (Math.BONE + priceDeviation) ||  price_deviation < (Math.BONE - priceDeviation)  ) {  return true;  price_deviation = Math.bdiv(ethTotal_1, ethTotal_0);  if (  price_deviation > (Math.BONE + priceDeviation) ||  price_deviation < (Math.BONE - priceDeviation)  ) {  return true;  ",
        "labels": [
            "Consensys",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/08/aave-balancer-and-uniswap-v2-price-providers/"
    },
    {
        "title": "5.2 Improve Input Validation   ",
        "body": "  Resolution  the recommended checks have been added to the constructor.  Description  The constructor does not validate whether the provided price provider arguments actually make sense. In the worst-case someone might be able to deploy the contract that cannot be used. It is recommended to fail the contract creation early if invalid arguments are detected.  Consider implementing the following checks to detect whether a non-viable price provider is being deployed:  tokens.length > 1 and less than the maximum supported tokens (note that hasDeviation requires token.length**2 iterations if no deviation is detected)  _isPeggedToEth.length == tokens.length  _decimals.length == tokens.length  approximationMatrix.length && approximationMatrix[0][0].length == tokens.length +1  _priceDeviation is within bounds (less than 100%, i.e. less than 1 * BONE) otherwise the calculation might underflow.  _powerPrecision is within bounds  address(_priceOracle) != address(0)  Balancer  code/aave-balancer-3e8367ab/contracts/proxies/BalancerSharedPoolPriceProvider.sol:L38-L63  constructor(  BPool _pool,  bool[] memory _isPeggedToEth,  uint8[] memory _decimals,  IPriceOracle _priceOracle,  uint256 _priceDeviation,  uint256 _K,  uint256 _powerPrecision,  uint256[][] memory _approximationMatrix  ) public {  pool = _pool;  //Get token list  tokens = pool.getFinalTokens(); //This already checks for pool finalized  //Get token normalized weights  uint256 length = tokens.length;  for (uint8 i = 0; i < length; i++) {  weights.push(pool.getNormalizedWeight(tokens[i]));  isPeggedToEth = _isPeggedToEth;  decimals = _decimals;  priceOracle = _priceOracle;  priceDeviation = _priceDeviation;  K = _K;  powerPrecision = _powerPrecision;  approximationMatrix = _approximationMatrix;  Uniswapv2  code/aave-uniswapv2-e81cf872/contracts/proxies/UniswapV2PriceProvider.sol:L35-L50  constructor(  IUniswapV2Pair _pair,  bool[] memory _isPeggedToEth,  uint8[] memory _decimals,  IPriceOracle _priceOracle,  uint256 _priceDeviation  ) public {  pair = _pair;  //Get tokens  tokens.push(pair.token0());  tokens.push(pair.token1());  isPeggedToEth = _isPeggedToEth;  decimals = _decimals;  priceOracle = _priceOracle;  priceDeviation = _priceDeviation;  ",
        "labels": [
            "Consensys",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/08/aave-balancer-and-uniswap-v2-price-providers/"
    },
    {
        "title": "5.3 Use SafeMath consistently   ",
        "body": "  Resolution  All arithmetic operations now use SafeMath.  Description  Even though the Uniswap price provider imports the SafeMath library, the SafeMath library functions aren t always used for integer arithmetic operations. Note that plain Solidity arithmetic operators do not check for integer underflows and overflows.  Examples  Example 1:  code/aave-uniswapv2-e81cf872/contracts/proxies/UniswapV2PriceProvider.sol:L66  uint256 missingDecimals = 18 - decimals[index];  Example 2 (same in line 91-92):  code/aave-uniswapv2-e81cf872/contracts/proxies/UniswapV2PriceProvider.sol:L84-L85  price_deviation > (Math.BONE + priceDeviation) ||  price_deviation < (Math.BONE - priceDeviation)  Example 3:  code/aave-uniswapv2-e81cf872/contracts/proxies/UniswapV2PriceProvider.sol:L164-L165  uint256 liquidity = numerator / denominator;  totalSupply += liquidity;  Recommendation  In some cases, this issue is cosmetic because the values are assumed to be within certain ranges. Nevertheless, we recommend accepting the slightly higher gas cost for SafeMath functions for consistency and to prevent potential issues.  6 Issues  The issues are presented in approximate order of priority from highest to lowest.  ",
        "labels": [
            "Consensys",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/08/aave-balancer-and-uniswap-v2-price-providers/"
    },
    {
        "title": "6.1 Unchecked Specification requirement - token limit Closed",
        "body": "  Description  According to the Balancer Shared Pool Price Provider that was provided with the audit code-base the price provide must fulfill the following requirements:  Pool token price cannot be manipulated  Chainlink will be used as the main oracle  It should use as less gas as possible  Limited to Balancer s shared pools where the weights cannot be changed  Limited to a pool containing 2 to 3 tokens  However, the constructor of the price provider does not enforce the limit of 2 to 3 tokens.  Examples  code/aave-balancer-3e8367ab/contracts/proxies/BalancerSharedPoolPriceProvider.sol:L38-L63  constructor(  BPool _pool,  bool[] memory _isPeggedToEth,  uint8[] memory _decimals,  IPriceOracle _priceOracle,  uint256 _priceDeviation,  uint256 _K,  uint256 _powerPrecision,  uint256[][] memory _approximationMatrix  ) public {  pool = _pool;  //Get token list  tokens = pool.getFinalTokens(); //This already checks for pool finalized  //Get token normalized weights  uint256 length = tokens.length;  for (uint8 i = 0; i < length; i++) {  weights.push(pool.getNormalizedWeight(tokens[i]));  isPeggedToEth = _isPeggedToEth;  decimals = _decimals;  priceOracle = _priceOracle;  priceDeviation = _priceDeviation;  K = _K;  powerPrecision = _powerPrecision;  approximationMatrix = _approximationMatrix;  Recommendation  Require that the number of tokens returned by pool.getFinalTokens() is 2<= len <=3.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/08/aave-balancer-and-uniswap-v2-price-providers/"
    },
    {
        "title": "6.2 Integer underflow if a token specifies more than 18 decimals Closed",
        "body": "  Description  Decimals are provided by the account deploying the price provider contract. In getEthBalanceByToken the assumption is made that decimals[index] is less or equal to 18 decimals, however, the deployer may provide decimals that are not within normal operating bounds. Contract creation succeeds, while the contract is not viable.  Examples  The value underflows if the contract is used with a token decimals > 18.  Balancer  code/aave-balancer-3e8367ab/contracts/proxies/BalancerSharedPoolPriceProvider.sol:L69-L78  function getEthBalanceByToken(uint256 index)  internal  view  returns (uint256)  uint256 pi = isPeggedToEth[index]  ? BONE  : uint256(priceOracle.getAssetPrice(tokens[index]));  require(pi > 0, \"ERR_NO_ORACLE_PRICE\");  uint256 missingDecimals = 18 - decimals[index];  Uniswapv2  code/aave-uniswapv2-e81cf872/contracts/proxies/UniswapV2PriceProvider.sol:L57-L66  function getEthBalanceByToken(uint256 index, uint112 reserve)  internal  view  returns (uint256)  uint256 pi = isPeggedToEth[index]  ? Math.BONE  : uint256(priceOracle.getAssetPrice(tokens[index]));  require(pi > 0, \"ERR_NO_ORACLE_PRICE\");  uint256 missingDecimals = 18 - decimals[index];  Recommendation  Add a check to the constructor to ensure that none of the provided decimals is greater than 18.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/08/aave-balancer-and-uniswap-v2-price-providers/"
    },
    {
        "title": "5.1 Warning about ERC20 handling function",
        "body": "  Description  There is something worth bringing up for discussion in the ERC20 disbursement function.  code/BitwaveMultiSend.sol:L55  assert(token.transferFrom(msg.sender, _to[i], _value[i]) == true);  In the above presented line, the external call is being compared to a truthful boolean. And, even though, this is clearly part of the ERC20 specification there have historically been cases where tokens with sizeable market caps and liquidity have erroneously not implemented return values in any of the transfer functions.  The question presents itself as to whether these non-ERC20-conforming tokens are meant to be supported or not.  The audit team believes that the purpose of this smart contract is to disburse OXT tokens and therefore, since its development was under the umbrella of the Orchid team, absolutely no security concerns should arise from this issue.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"
    },
    {
        "title": "5.2 Discussion on the permissioning of send functions",
        "body": "  Description  Since the disbursement of funds is all made atomically (i.e., the Ether funds held by the smart contract are transient) there is no need to permission the function with the restrictedToOwner modifier.  Even in the case of ERC20 tokens, there is no need to permission the function since the smart contract can only spend allowance attributed to it by the caller (msg.sender).  This being said there is value in permissioning this contract, specifically if attribution of the deposited funds in readily available tools like Etherscan is important. Because turning this into a publicly available tool for batch sends of Ether and ERC20 tokens would mean that someone could wrongly attribute some disbursement to Orchid Labs should they be ignorant to this fact.  A possible solution to this problem would be the usage of events to properly attribute the disbursements but it is, indeed, an additional burden to carefully analyse these for proper attribution.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"
    },
    {
        "title": "5.3 Improve function visibility ",
        "body": "  Description  The following methods are not called internally in the token contract and visibility can, therefore, be restricted to external rather than public. This is more gas efficient because less code is emitted and data does not need to be copied into memory. It also makes functions a bit simpler to reason about because there s no need to worry about the possibility of internal calls.  BitwaveMultiSend.sendEth()  BitwaveMultiSend.sendErc20()  Recommendation  Change visibility of these methods to external.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"
    },
    {
        "title": "5.4 Ether send function remainder handling ",
        "body": "  Description  The Ether send function depicted below implements logic to reimburse the sender if an extraneous amount is left in the contract after the disbursement.  code/BitwaveMultiSend.sol:L22-L43  function sendEth(address payable [] memory _to, uint256[] memory _value) public restrictedToOwner payable returns (bool _success) {  // input validation  require(_to.length == _value.length);  require(_to.length <= 255);  // count values for refunding sender  uint256 beforeValue = msg.value;  uint256 afterValue = 0;  // loop through to addresses and send value  for (uint8 i = 0; i < _to.length; i++) {  afterValue = afterValue.add(_value[i]);  assert(_to[i].send(_value[i]));  // send back remaining value to sender  uint256 remainingValue = beforeValue.sub(afterValue);  if (remainingValue > 0) {  assert(msg.sender.send(remainingValue));  return true;  It is also the only place where the SafeMath dependency is being used. More specifically to check there was no underflow in the arithmetic adding up the disbursed amounts.  However, since the individual sends would revert themselves should more Ether than what was available in the balance be specified these protection measures seem unnecessary.  Not only the above is true but the current codebase does not allow to take funds locked within the contract out in the off chance someone forced funds into this smart contract (e.g., by self-destructing some other smart contract containing funds into this one).  Recommendation  The easiest way to handle both retiring SafeMath and returning locked funds would be to phase out all the intra-function arithmetic and just transferring address(this).balance to msg.sender at the end of the disbursement. Since all the funds in there are meant to be from the caller of the function this serves the purpose of returning extraneous funds to him well and, adding to that, it allows for some front-running fun if someone  self-destructed  funds to this smart contract by mistake.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"
    },
    {
        "title": "5.5 Unneeded type cast of contract type ",
        "body": "  Description  The typecast being done on the address parameter in the lien below is unneeded.  code/BitwaveMultiSend.sol:L51  ERC20 token = ERC20(_tokenAddress);  Recommendation  Assign the right type at the function parameter definition like so:  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"
    },
    {
        "title": "5.6 Inadequate use of assert ",
        "body": "  Description  The usage of require vs assert has always been a matter of discussion because of the fine lines distinguishing these transaction-terminating expressions.  However, the usage of the assert syntax in this case is not the most appropriate.  Borrowing the explanation from the latest solidity docs (v. https://solidity.readthedocs.io/en/latest/control-structures.html#id4) :  Since assert-style exceptions (using the 0xfe opcode) consume all gas available to the call and require-style ones (using the 0xfd opcode) do not since the Metropolis release when the REVERT instruction was added, the usage of require in the lines depicted in the examples section would only result in gas savings and the same security assumptions.  In this case, even though the calls are being made to external contracts the supposedly abide to a predefined specification, this is by no means an invariant of the presented system since the component is external to the built system and its integrity cannot be formally verified.  Examples  code/BitwaveMultiSend.sol:L34  assert(_to[i].send(_value[i]));  code/BitwaveMultiSend.sol:L40  assert(msg.sender.send(remainingValue));  code/BitwaveMultiSend.sol:L55  assert(token.transferFrom(msg.sender, _to[i], _value[i]) == true);  Recommendation  Exchange the assert statements for require ones.  6 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"
    },
    {
        "title": "6.1 MythX",
        "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  The output of a MythX Full Mode analysis was reviewed by the audit team and no relevant issues were raised as part of the process.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"
    },
    {
        "title": "6.2 Ethlint",
        "body": "  Ethlint is an open source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  code/BitwaveMultiSend.sol  3:7      error      \"./ERC20.sol\": Import statements must use double quotes only.                              quotes  22:16    error      There should be no whitespace between \"address payable\" and the opening square bracket.    array-declarations  24:8     warning    Provide an error message for require()                                                     error-reason  25:8     warning    Provide an error message for require()                                                     error-reason  34:19    warning    Consider using 'transfer' in place of 'send'.                                              security/no-send  40:19    warning    Consider using 'transfer' in place of 'send'.                                              security/no-send  47:8     warning    Provide an error message for require()                                                     error-reason  48:8     warning    Provide an error message for require()                                                     error-reason  \u2716 2 errors, 6 warnings found.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"
    },
    {
        "title": "6.3 Surya",
        "body": "  Surya is a utility tool for smart contract systems. It provides a number of visual outputs and information about the structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  Function Name  Visibility  Mutability  Modifiers  BitwaveMultiSend  Implementation  Public    NO   sendEth  Public    restrictedToOwner  sendErc20  Public    restrictedToOwner  Legend  Function can modify state  Function is payable  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"
    },
    {
        "title": "5.1 Anyone can remove a maker s pending pool join status    ",
        "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2250 by removing the two-step handshake for a maker to join a pool.  Description  Using behavior described in issue 5.6, it is possible to delete the pending join status of any maker in any pool by passing in NIL_POOL_ID to removeMakerFromStakingPool. Note that the attacker in the following example must not be a confirmed member of any pool:  The attacker calls addMakerToStakingPool(NIL_POOL_ID, makerAddress). In this case, makerAddress can be almost any address, as long as it has not called joinStakingPoolAsMaker (an easy example is address(0)). The key goal of this call is to increment the number of makers in pool 0: code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L262 _poolById[poolId].numberOfMakers = uint256(pool.numberOfMakers).safeAdd(1).downcastToUint32();  The attacker calls removeMakerFromStakingPool(NIL_POOL_ID, targetAddress). This function queries getStakingPoolIdOfMaker(targetAddress) and compares it to the passed-in pool id. Because the target is an unconfirmed maker, their staking pool id is NIL_POOL_ID: code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L166-L173 bytes32 makerPoolId = getStakingPoolIdOfMaker(makerAddress); if (makerPoolId != poolId) {     LibRichErrors.rrevert(LibStakingRichErrors.MakerPoolAssignmentError(         LibStakingRichErrors.MakerPoolAssignmentErrorCodes.MakerAddressNotRegistered,         makerAddress,         makerPoolId     )); }  The check passes, and the target s _poolJoinedByMakerAddress struct is deleted. Additionally, the number of makers in pool 0 is decreased:  code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L176-L177  delete _poolJoinedByMakerAddress[makerAddress];  _poolById[poolId].numberOfMakers = uint256(_poolById[poolId].numberOfMakers).safeSub(1).downcastToUint32();  This can be used to prevent any makers from being confirmed into a pool.  Recommendation  See issue 5.6.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"
    },
    {
        "title": "5.2 Delegated stake weight reduction can be bypassed by using an external contract   ",
        "body": "  Resolution  From the development team:  Although it is possible to bypass the weight reduction via external smart contracts, we believe there is some value to having a lower delegated stake weight as the default behavior. This can still approximate the intended behavior and should give a very slight edge to pool operators that own their stake.  Description  Staking pools allow ZRX holders to delegate their staked ZRX to a market maker in exchange for a configurable percentage of the stake reward (accrued over time through exchange fees). When staking as expected through the 0x contracts, the protocol favors ZRX staked directly by the operator of the pool, assigning a lower weight (90%) to ZRX staked by delegation. In return, delegated members receive a configurable portion of the operator s stake reward.  Using a smart contract, it is possible to represent ZRX owned by any number of parties as ZRX staked by a single party. This contract can serve as the operator of a pool with a single member\u2014itself. The advantages are clear for ZRX holders:  ZRX staked through this contract will be given full (100%) stake weight.  Because stake weight is a factor in reward allocation, the ZRX staked through this contract receives a higher proportion of the stake reward.  Recommendation  Remove stake weight reduction for delegated stake.  ",
        "labels": [
            "Consensys",
            "Major",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"
    },
    {
        "title": "5.3 MixinParams.setParams bypasses safety checks made by standard StakingProxy upgrade path.    ",
        "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2279. Now the parameter validity is asserted in  Description  The staking contracts use a set of configurable parameters to determine the behavior of various parts of the system. The parameters dictate the duration of epochs, the ratio of delegated stake weight vs operator stake, the minimum pool stake, and the Cobb-Douglas numerator and denominator. These parameters can be configured in two ways:  An authorized address can deploy a new Staking contract (perhaps with altered parameters), and configure the StakingProxy to delegate to this new contract. This is done by calling   StakingProxy.detachStakingContract: code/contracts/staking/contracts/src/StakingProxy.sol:L82-L90 /// @dev Detach the current staking contract. /// Note that this is callable only by an authorized address. function detachStakingContract()     external     onlyAuthorized {     stakingContract = NIL_ADDRESS;     emit StakingContractDetachedFromProxy(); }   StakingProxy.attachStakingContract(newContract): code/contracts/staking/contracts/src/StakingProxy.sol:L72-L80 /// @dev Attach a staking contract; future calls will be delegated to the staking contract. /// Note that this is callable only by an authorized address. /// @param _stakingContract Address of staking contract. function attachStakingContract(address _stakingContract)     external     onlyAuthorized {     _attachStakingContract(_stakingContract); }   During the latter call, the StakingProxy performs a delegatecall to Staking.init, then checks the values of the parameters set during initialization: code/contracts/staking/contracts/src/StakingProxy.sol:L208-L219 // Call `init()` on the staking contract to initialize storage. (bool didInitSucceed, bytes memory initReturnData) = stakingContract.delegatecall(     abi.encodeWithSelector(IStorageInit(0).init.selector) ); if (!didInitSucceed) {     assembly {         revert(add(initReturnData, 0x20), mload(initReturnData))     } }  // Assert initialized storage values are valid _assertValidStorageParams();  An authorized address can call MixinParams.setParams at any time and set the contract s parameters to arbitrary values.  The latter method introduces the possibility of setting unsafe or nonsensical values for the contract parameters: epochDurationInSeconds can be set to 0, cobbDouglassAlphaNumerator can be larger than cobbDouglassAlphaDenominator, rewardDelegatedStakeWeight can be set to a value over 100% of the staking reward, and more.  Note, too, that by using MixinParams.setParams to set all parameters to 0, the Staking contract can be re-initialized by way of Staking.init. Additionally, it can be re-attached by way of StakingProxy.attachStakingContract, as the delegatecall to Staking.init will succeed.  Recommendation  Ensure that calls to setParams check that the provided values are within the same range currently enforced by the proxy.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"
    },
    {
        "title": "5.4 Authorized addresses can indefinitely stall ZrxVaultBackstop catastrophic failure mode    ",
        "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2295 by removing the  Description  The ZrxVaultBackstop contract was added to allow anyone to activate the staking system s  catastrophic failure  mode if the StakingProxy is in  read-only  mode for at least 40 days. To enable this behavior, the StakingProxy contract was modified to track the last timestamp at which  read-only  mode was activated. This is done by way of StakingProxy.setReadOnlyMode:  code/contracts/staking/contracts/src/StakingProxy.sol:L92-L104  /// @dev Set read-only mode (state cannot be changed).  function setReadOnlyMode(bool shouldSetReadOnlyMode)  external  onlyAuthorized  // solhint-disable-next-line not-rely-on-time  uint96 timestamp = block.timestamp.downcastToUint96();  if (shouldSetReadOnlyMode) {  stakingContract = readOnlyProxy;  readOnlyState = IStructs.ReadOnlyState({  isReadOnlyModeSet: true,  lastSetTimestamp: timestamp  });  Because the timestamp is updated even if  read-only  mode is already active, any authorized address can prevent ZrxVaultBackstop from activating catastrophic failure mode by repeatedly calling setReadOnlyMode.  Recommendation  If  read-only  mode is already active, setReadOnlyMode(true) should result in a no-op.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"
    },
    {
        "title": "5.5 Pool 0 can be used to temporarily prevent makers from joining another pool    ",
        "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2250. Pool IDs now start at 1.  Description  removeMakerFromStakingPool reverts if the number of makers currently in the pool is 0, due to safeSub catching an underflow:  code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L177  _poolById[poolId].numberOfMakers = uint256(_poolById[poolId].numberOfMakers).safeSub(1).downcastToUint32();  Because of this, edge behavior described in issue 5.6 can allow an attacker to temporarily prevent makers from joining a pool:  The attacker calls addMakerToStakingPool(NIL_POOL_ID, victimAddress). This sets the victim s MakerPoolJoinStatus.confirmed field to true and increases the number of makers in pool 0 to 1: code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L257-L262 poolJoinStatus = IStructs.MakerPoolJoinStatus({     poolId: poolId,     confirmed: true }); _poolJoinedByMakerAddress[makerAddress] = poolJoinStatus; _poolById[poolId].numberOfMakers = uint256(pool.numberOfMakers).safeAdd(1).downcastToUint32();  The attacker calls removeMakerFromStakingPool(NIL_POOL_ID, randomAddress). The net effect of this call simply decreases the number of makers in pool 0 by 1, back to 0: code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L176-L177 delete _poolJoinedByMakerAddress[makerAddress]; _poolById[poolId].numberOfMakers = uint256(_poolById[poolId].numberOfMakers).safeSub(1).downcastToUint32();  Typically, the victim should be able to remove themselves from pool 0 by calling removeMakerFromStakingPool(NIL_POOL_ID, victimAddress), but because the attacker can set the pool s number of makers to 0, the aforementioned underflow causes this call to fail. The victim must first understand what is happening in MixinStakingPool before they are able to remedy the situation:  The victim must call addMakerToStakingPool(NIL_POOL_ID, randomAddress2) to increase pool 0 s number of makers back to 1.  The victim can now call removeMakerFromStakingPool(NIL_POOL_ID, victimAddress), and remove their confirmed status.  Additionally, if the victim in question currently has a pending join, the attacker can use issue 5.1 to first remove their pending status before locking them in pool 0.  Recommendation  See issue 5.1.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"
    },
    {
        "title": "5.6 Recommendation: Fix weak assertions in MixinStakingPool stemming from use of NIL_POOL_ID    ",
        "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2250. Pool IDs now start at 1.  Description  The modifier onlyStakingPoolOperatorOrMaker(poolId) is used to authorize actions taken on a given pool. The sender must be either the operator or a confirmed maker of the pool in question. However, the modifier queries getStakingPoolIdOfMaker(maker), which returns NIL_POOL_ID if the maker s MakerPoolJoinStatus struct is not confirmed. This implicitly makes anyone a maker of the nonexistent  pool 0 :  code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L189-L200  function getStakingPoolIdOfMaker(address makerAddress)  public  view  returns (bytes32)  IStructs.MakerPoolJoinStatus memory poolJoinStatus = _poolJoinedByMakerAddress[makerAddress];  if (poolJoinStatus.confirmed) {  return poolJoinStatus.poolId;  } else {  return NIL_POOL_ID;  joinStakingPoolAsMaker(poolId) makes no existence checks on the provided pool id, and allows makers to become pending makers in nonexistent pools.  addMakerToStakingPool(poolId, maker) makes no existence checks on the provided pool id, allowing makers to be added to nonexistent pools (as long as the sender is an operator or maker in the pool).  Recommendation  Avoid use of 0x00...00 for NIL_POOL_ID. Instead, use 2**256 - 1.  Implement stronger checks for pool existence. Each time a pool id is supplied, it should be checked that the pool id is between 0 and nextPoolId.  onlyStakingPoolOperatorOrMaker should revert if poolId == NIL_POOL_ID or if poolId is not in the valid range: (0, nextPoolId).  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"
    },
    {
        "title": "5.7 LibMath functions fail to catch a number of overflows    ",
        "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2255 and  0xProject/0x-monorepo#2311.  Description  The __add(), __mul(), and __div() functions perform arithmetic on 256-bit signed integers, and they all miss some specific overflows.  Addition Overflows  code/contracts/staking/contracts/src/libs/LibFixedMath.sol:L359-L376  /// @dev Adds two numbers, reverting on overflow.  function _add(int256 a, int256 b) private pure returns (int256 c) {  c = a + b;  if (c > 0 && a < 0 && b < 0) {  LibRichErrors.rrevert(LibFixedMathRichErrors.BinOpError(  LibFixedMathRichErrors.BinOpErrorCodes.SUBTRACTION_OVERFLOW,  a,  ));  if (c < 0 && a > 0 && b > 0) {  LibRichErrors.rrevert(LibFixedMathRichErrors.BinOpError(  LibFixedMathRichErrors.BinOpErrorCodes.ADDITION_OVERFLOW,  a,  ));  The two overflow conditions it tests for are:  Adding two positive numbers shouldn t result in a negative number.  Adding two negative numbers shouldn t result in a positive number.  __add(-2**255, -2**255) returns 0 without reverting because the overflow didn t match either of the above conditions.  Multiplication Overflows  code/contracts/staking/contracts/src/libs/LibFixedMath.sol:L332-L345  /// @dev Returns the multiplication two numbers, reverting on overflow.  function _mul(int256 a, int256 b) private pure returns (int256 c) {  if (a == 0) {  return 0;  c = a * b;  if (c / a != b) {  LibRichErrors.rrevert(LibFixedMathRichErrors.BinOpError(  LibFixedMathRichErrors.BinOpErrorCodes.MULTIPLICATION_OVERFLOW,  a,  ));  The function checks via division for most types of overflows, but it fails to catch one particular case. __mul(-2**255, -1) returns -2**255 without error.  Division Overflows  code/contracts/staking/contracts/src/libs/LibFixedMath.sol:L347-L357  /// @dev Returns the division of two numbers, reverting on division by zero.  function _div(int256 a, int256 b) private pure returns (int256 c) {  if (b == 0) {  LibRichErrors.rrevert(LibFixedMathRichErrors.BinOpError(  LibFixedMathRichErrors.BinOpErrorCodes.DIVISION_BY_ZERO,  a,  ));  c = a / b;  It does not check for overflow. Due to this, __div(-2**255, -1) erroneously returns -2**255.  Recommendation  For addition, the specific case of __add(-2**255, -2**255) can be detected by using a >= 0 check instead of > 0, but the below seems like a clearer check for all cases:  // if b is negative, then the result should be less than a  if (b < 0 && c >= a) { /* subtraction overflow */ }  // if b is positive, then the result should be greater than a  if (b > 0 && c <= a) { /* addition overflow */ }  For multiplication and division, the specific values of -2**255 and -1 are the only missing cases, so that can be explicitly checked in the __mul() and __div() functions.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"
    },
    {
        "title": "5.8 Recommendation: Remove MixinAbstract and fold MixinStakingPoolRewards into MixinFinalizer and MixinStake   ",
        "body": "  Resolution  The development team investigated this suggestion, but they were ultimately uncomfortable making such a large change in this cycle. This can be considered again in a future version of the code.  Description  issue 5.12,  issue 5.11,  issue 5.10, and  issue 5.9,  Move MixinStakingPoolRewards.withdrawDelegatorRewards into MixinStake. As per the comments above this function, its behavior is very similar to functions in MixinStake: code/contracts/staking/contracts/src/staking_pools/MixinStakingPoolRewards.sol:L35-L56 /// @dev Syncs rewards for a delegator. This includes transferring WETH ///      rewards to the delegator, and adding/removing ///      dependencies on cumulative rewards. ///      This is used by a delegator when they want to sync their rewards ///      without delegating/undelegating. It's effectively the same as ///      delegating zero stake. /// @param poolId Unique id of pool. function withdrawDelegatorRewards(bytes32 poolId)     external {     address member = msg.sender;      _withdrawAndSyncDelegatorRewards(         poolId,         member     );      // Update stored balance with synchronized version; this prevents     // redundant withdrawals.     _delegatedStakeToPoolByOwner[member][poolId] =         _loadSyncedBalance(_delegatedStakeToPoolByOwner[member][poolId]); }  Move the rest of the MixinStakingPoolRewards functions into MixinFinalizer. This change allows the MixinStakingPoolRewards and MixinAbstract files to be removed. MixinStakingPool can now inherit directly from MixinFinalizer.  After implementing all recommendations mentioned here, the inheritance graph of the staking contracts is much simpler. The previous graph is pictured here:  The new graph is pictured here:  Further improvements may consider:  Having MixinStorage inherit MixinConstants and IStakingEvents  Moving _loadCurrentBalance into MixinStorage. Currently MixinStakeBalances only inherits from MixinStakeStorage because of this function.  After implementing the above, MixinExchangeFees is no longer dependent on MixinStakingPool and can inherit directly from MixinExchangeManager  A sample inheritance graph including the above is pictured below:  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"
    },
    {
        "title": "5.9 Recommendation: remove confusing access to activePoolsThisEpoch    ",
        "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2276. Along with other state cleanup, these functions and  Description  MixinFinalizer provides two functions to access activePoolsThisEpoch:  _getActivePoolsFromEpoch returns a storage pointer to the mapping: code/contracts/staking/contracts/src/sys/MixinFinalizer.sol:L211-L225 /// @dev Get a mapping of active pools from an epoch. ///      This uses the formula `epoch % 2` as the epoch index in order ///      to reuse state, because we only need to remember, at most, two ///      epochs at once. /// @return activePools The pools that were active in `epoch`. function _getActivePoolsFromEpoch(     uint256 epoch )     internal     view     returns (mapping (bytes32 => IStructs.ActivePool) storage activePools) {     activePools = _activePoolsByEpoch[epoch % 2];     return activePools; }  _getActivePoolFromEpoch invokes _getActivePoolsFromEpoch, then loads an ActivePool struct from a passed-in poolId: code/contracts/staking/contracts/src/sys/MixinFinalizer.sol:L195-L209 /// @dev Get an active pool from an epoch by its ID. /// @param epoch The epoch the pool was/will be active in. /// @param poolId The ID of the pool. /// @return pool The pool with ID `poolId` that was active in `epoch`. function _getActivePoolFromEpoch(     uint256 epoch,     bytes32 poolId )     internal     view     returns (IStructs.ActivePool memory pool) {     pool = _getActivePoolsFromEpoch(epoch)[poolId];     return pool; }  Ultimately, the two functions are syntax sugar for activePoolsThisEpoch[epoch % 2], with the latter also accessing a value within the mapping. Because of the naming similarity, and because one calls the other, this abstraction is more confusing that simply accessing the state variable directly.  Additionally, by removing these functions and adopting the long-form syntax, MixinExchangeFees no longer needs to inherit MixinFinalizer.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"
    },
    {
        "title": "5.10 Recommendation: remove MixinFinalizer._getUnfinalizedPoolRewardsFromState   ",
        "body": "  Resolution  The development team decided to keep this function for its optimization on storage loads. It s will still be used internally by getters that are important for client-side code.  Description  MixinFinalizer._getUnfinalizedPoolRewardsFromState is a simple wrapper around the library function LibCobbDouglas.cobbDouglas:  code/contracts/staking/contracts/src/sys/MixinFinalizer.sol:L250-L286  /// @dev Computes the reward owed to a pool during finalization.  /// @param pool The active pool.  /// @param state The current state of finalization.  /// @return rewards Unfinalized rewards for this pool.  function _getUnfinalizedPoolRewardsFromState(  IStructs.ActivePool memory pool,  IStructs.UnfinalizedState memory state  private  view  returns (uint256 rewards)  // There can't be any rewards if the pool was active or if it has  // no stake.  if (pool.feesCollected == 0) {  return rewards;  // Use the cobb-douglas function to compute the total reward.  rewards = LibCobbDouglas.cobbDouglas(  state.rewardsAvailable,  pool.feesCollected,  state.totalFeesCollected,  pool.weightedStake,  state.totalWeightedStake,  cobbDouglasAlphaNumerator,  cobbDouglasAlphaDenominator  );  // Clip the reward to always be under  // `rewardsAvailable - totalRewardsPaid`,  // in case cobb-douglas overflows, which should be unlikely.  uint256 rewardsRemaining = state.rewardsAvailable.safeSub(state.totalRewardsFinalized);  if (rewardsRemaining < rewards) {  rewards = rewardsRemaining;  After implementing issue 5.11, this function is only called a single time, in MixinFinalizer.finalizePool:  code/contracts/staking/contracts/src/sys/MixinFinalizer.sol:L119-L129  // Noop if the pool was not active or already finalized (has no fees).  if (pool.feesCollected == 0) {  return;  // Clear the pool state so we don't finalize it again, and to recoup  // some gas.  delete _getActivePoolsFromEpoch(prevEpoch)[poolId];  // Compute the rewards.  uint256 rewards = _getUnfinalizedPoolRewardsFromState(pool, state);  Because it is only used a single time, and because it obfuscates an essential library call during the finalization process, the function should be removed and folded into finalizePool. Additionally, the first check for pool.feesCollected == 0 can be removed, as this case is covered in finalizePool already (see above).  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"
    },
    {
        "title": "5.11 Recommendation: remove complicating getters from MixinStakingPoolRewards   ",
        "body": "  Resolution  These getters are useful for client-side code, such as the staking interface.  Description  MixinStakingPoolRewards has two external view functions that contribute complexity to essential functions, as well as the overall inheritance tree:  computeRewardBalanceOfOperator, used to compute the reward balance of a pool s operator on an unfinalized pool: code/contracts/staking/contracts/src/staking_pools/MixinStakingPoolRewards.sol:L55-L69 /// @dev Computes the reward balance in ETH of the operator of a pool. /// @param poolId Unique id of pool. /// @return totalReward Balance in ETH. function computeRewardBalanceOfOperator(bytes32 poolId)     external     view     returns (uint256 reward) {     // Because operator rewards are immediately withdrawn as WETH     // on finalization, the only factor in this function are unfinalized     // rewards.     IStructs.Pool memory pool = _poolById[poolId];     // Get any unfinalized rewards.     (uint256 unfinalizedTotalRewards, uint256 unfinalizedMembersStake) =         _getUnfinalizedPoolRewards(poolId);  computeRewardBalanceOfDelegator, used to compute the reward balance of a delegator for an unfinalized pool: code/contracts/staking/contracts/src/staking_pools/MixinStakingPoolRewards.sol:L80-L99 /// @dev Computes the reward balance in ETH of a specific member of a pool. /// @param poolId Unique id of pool. /// @param member The member of the pool. /// @return totalReward Balance in ETH. function computeRewardBalanceOfDelegator(bytes32 poolId, address member)     external     view     returns (uint256 reward) {     IStructs.Pool memory pool = _poolById[poolId];     // Get any unfinalized rewards.     (uint256 unfinalizedTotalRewards, uint256 unfinalizedMembersStake) =         _getUnfinalizedPoolRewards(poolId);      // Get the members' portion.     (, uint256 unfinalizedMembersReward) = _computePoolRewardsSplit(         pool.operatorShare,         unfinalizedTotalRewards,         unfinalizedMembersStake     );  These two functions are the sole reason for the existence of MixinFinalizer._getUnfinalizedPoolRewards, one of the two functions in MixinAbstract:  code/contracts/staking/contracts/src/sys/MixinAbstract.sol:L40-L52  /// @dev Computes the reward owed to a pool during finalization.  ///      Does nothing if the pool is already finalized.  /// @param poolId The pool's ID.  /// @return totalReward The total reward owed to a pool.  /// @return membersStake The total stake for all non-operator members in  ///         this pool.  function _getUnfinalizedPoolRewards(bytes32 poolId)  internal  view  returns (  uint256 totalReward,  uint256 membersStake  );  These functions also necessitate two additional parameters in MixinStakingPoolRewards._computeDelegatorReward, which are used a single time to call _computeUnfinalizedDelegatorReward:  code/contracts/staking/contracts/src/staking_pools/MixinStakingPoolRewards.sol:L253-L259  // 1/3 Unfinalized rewards earned in `currentEpoch - 1`.  reward = _computeUnfinalizedDelegatorReward(  delegatedStake,  _currentEpoch,  unfinalizedMembersReward,  unfinalizedMembersStake  );  By removing the functions computeRewardBalanceOfOperator and computeRewardBalanceOfDelegator, the following simplifications can be made:  _getUnfinalizedPoolRewards can be removed from both MixinAbstract and MixinFinalizer  The parameters unfinalizedMembersReward and unfinalizedMembersStake can be removed from _computeDelegatorReward  The function _computeUnfinalizedDelegatorReward can be removed  A branch of now-unused logic in _computeDelegatorReward can be removed  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"
    },
    {
        "title": "5.12 Recommendation: remove unneeded dependency on MixinStakeBalances   ",
        "body": "  Resolution  From the development team:  We re going to keep this abstraction to future-proof balance queries.  Description  MixinStakeBalances has two functions used by inheriting contracts:  getStakeDelegatedToPoolByOwner, which provides shorthand to access _delegatedStakeToPoolByOwner: code/contracts/staking/contracts/src/stake/MixinStakeBalances.sol:L84-L95 /// @dev Returns the stake delegated to a specific staking pool, by a given staker. /// @param staker of stake. /// @param poolId Unique Id of pool. /// @return Stake delegated to pool by staker. function getStakeDelegatedToPoolByOwner(address staker, bytes32 poolId)     public     view     returns (IStructs.StoredBalance memory balance) {     balance = _loadCurrentBalance(_delegatedStakeToPoolByOwner[staker][poolId]);     return balance; }  getTotalStakeDelegatedToPool, which provides shorthand to access _delegatedStakeByPoolId: code/contracts/staking/contracts/src/stake/MixinStakeBalances.sol:L97-L108 /// @dev Returns the total stake delegated to a specific staking pool, ///      across all members. /// @param poolId Unique Id of pool. /// @return Total stake delegated to pool. function getTotalStakeDelegatedToPool(bytes32 poolId)     public     view     returns (IStructs.StoredBalance memory balance) {     balance = _loadCurrentBalance(_delegatedStakeByPoolId[poolId]);     return balance; }  Each of these functions is used only a single time:  MixinExchangeFees.payProtocolFee: code/contracts/staking/contracts/src/fees/MixinExchangeFees.sol:L78 uint256 poolStake = getTotalStakeDelegatedToPool(poolId).currentEpochBalance;  MixinExchangeFees._computeMembersAndWeightedStake: code/contracts/staking/contracts/src/fees/MixinExchangeFees.sol:L143-L146 uint256 operatorStake = getStakeDelegatedToPoolByOwner(     _poolById[poolId].operator,     poolId ).currentEpochBalance;  By replacing these function invocations in MixinExchangeFees with the long-form access to each state variable, MixinStakeBalances will no longer need to be included in the inheritance trees for several contracts.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"
    },
    {
        "title": "5.13 Misleading MoveStake event when moving stake from UNDELEGATED to UNDELEGATED    ",
        "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2280. If  Description  Although moving stake between the same status (UNDELEGATED <=> UNDELEGATED) should be a no-op, calls to moveStake succeed even for invalid amount and nonsensical poolId. The resulting MoveStake event can log garbage, potentially confusing those observing events.  Examples  When moving between UNDELEGATED and UNDELEGATED, each check and function call results in a no-op, save the final event:  Neither from nor to are StakeStatus.DELEGATED, so these checks are passed: code/contracts/staking/contracts/src/stake/MixinStake.sol:L115-L129 if (from.status == IStructs.StakeStatus.DELEGATED) {     _undelegateStake(         from.poolId,         staker,         amount     ); }  if (to.status == IStructs.StakeStatus.DELEGATED) {     _delegateStake(         to.poolId,         staker,         amount     ); }  The primary state changing function, _moveStake, immediately returns because the from and to balance pointers are equivalent: code/contracts/staking/contracts/src/stake/MixinStakeStorage.sol:L47-L49 if (_arePointersEqual(fromPtr, toPtr)) {     return; }  Finally, the MoveStake event is invoked, which can log completely invalid values for amount, from.poolId, and to.poolId: code/contracts/staking/contracts/src/stake/MixinStake.sol:L141-L148 emit MoveStake(     staker,     amount,     uint8(from.status),     from.poolId,     uint8(to.status),     to.poolId );  Recommendation  If amount is 0 or if moving between UNDELEGATED and UNDELEGATED, this function should no-op or revert. An explicit check for this case should be made near the start of the function.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"
    },
    {
        "title": "5.14 The staking contracts contain several artifacts of a quickly-changing codebase    ",
        "body": "  Resolution   These issues were addressed in a variety of fixes, most notably   0xProject/0x-monorepo#2262.  Examples  address payable is used repeatedly, but payments use WETH:   MixinStakingPool.createStakingPool: code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L54 address payable operator = msg.sender;   ZrxVault.stakingProxyAddress: code/contracts/staking/contracts/src/ZrxVault.sol:L38 address payable public stakingProxyAddress;   ZrxVault.setStakingProxy: code/contracts/staking/contracts/src/ZrxVault.sol:L76 function setStakingProxy(address payable _stakingProxyAddress)   IZrxVault.setStakingProxy: code/contracts/staking/contracts/src/interfaces/IZrxVault.sol:L53 function setStakingProxy(address payable _stakingProxyAddress)   struct IStructs.Pool: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L114 address payable operator;   MixinStake.stake: code/contracts/staking/contracts/src/stake/MixinStake.sol:L38 address payable staker = msg.sender;   MixinStake.unstake: code/contracts/staking/contracts/src/stake/MixinStake.sol:L63 address payable staker = msg.sender;   MixinStake.moveStake: code/contracts/staking/contracts/src/stake/MixinStake.sol:L119 address payable staker = msg.sender;   MixinStake._delegateStake: code/contracts/staking/contracts/src/stake/MixinStake.sol:L181 address payable staker,   MixinStake._undelegateStake: code/contracts/staking/contracts/src/stake/MixinStake.sol:L210 address payable staker,  Some identifiers are used multiple times for different purposes:  currentEpoch is:   A state variable: code/contracts/staking/contracts/src/immutable/MixinStorage.sol:L86 uint256 public currentEpoch = INITIAL_EPOCH;   A function parameter: code/contracts/staking/contracts/src/staking_pools/MixinStakingPoolRewards.sol:L323 uint256 currentEpoch,   A struct field: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L62 uint32 currentEpoch;  Several comments are out of date:   Many struct comments reference fees and rewards denominated in ETH, while only WETH is used: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L36-L38 /// @param rewardsAvailable Rewards (ETH) available to the epoch ///        being finalized (the previous epoch). This is simply the balance ///        of the contract at the end of the epoch.   UnfinalizedState.totalFeesCollected should specify that it is tracking fees attributed to a pool. Fees not attributed to a pool are still collected, but are not recorded: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L41 /// @param totalFeesCollected The total fees collected for the epoch being finalized.   UnfinalizedState.totalWeightedStake is copy-pasted from totalFeesCollected: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L42 /// @param totalWeightedStake The total fees collected for the epoch being finalized.   Pool.initialized seems to be copy-pasted from an older version of the struct StoredBalance or StakeBalance: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L108 /// @param initialized True iff the balance struct is initialized.  The final contracts produce several compiler warnings:   Several functions are intentionally marked view to allow overriding implementations to read from state. These can be silenced by adding block.timestamp; or similar statements to the functions.   One function is erroneously marked view, and should be changed to pure: code/contracts/staking/contracts/src/staking_pools/MixinStakingPoolRewards.sol:L315-L330 /// @dev Computes the unfinalized rewards earned by a delegator in the last epoch. /// @param unsyncedStake Unsynced delegated stake to pool by staker /// @param currentEpoch The epoch in which this call is executing /// @param unfinalizedMembersReward Unfinalized total members reward (if any). /// @param unfinalizedMembersStake Unfinalized total members stake (if any). /// @return reward Balance in WETH. function _computeUnfinalizedDelegatorReward(     IStructs.StoredBalance memory unsyncedStake,     uint256 currentEpoch,     uint256 unfinalizedMembersReward,     uint256 unfinalizedMembersStake )     private     view     returns (uint256) {  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"
    },
    {
        "title": "5.15 Remove unneeded fields from StoredBalance and Pool structs    ",
        "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2248. As part of a larger refactor, these fields were removed.  Description  Both structs have fields that are only written to, and never read:  StoredBalance.isInitialized: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L61 bool isInitialized;  Pool.initialized: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L113 bool initialized;  Recommendation  The unused fields should be removed.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"
    },
    {
        "title": "5.16 Remove unnecessary fallback function in Staking contract    ",
        "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2277.  Description  The Staking contract has a payable fallback function that is never used. Because it is used with a proxy contract, this pattern introduces silent failures when calls are made to the contract with no matching function selector.  Recommendation  Remove the fallback function from Staking.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"
    },
    {
        "title": "5.17 Pool IDs can just be incrementing integers    ",
        "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2250. Pool IDs now start at 1 and increment by 1 each time.  Description  Pool IDs are currently bytes32 values that increment by 2**128. After discussion with the development team, it seems that this was in preparation for a feature that was ultimately not used. Pool IDs should instead just be incrementing integers.  Examples  code/contracts/staking/contracts/src/immutable/MixinConstants.sol:L30-L34  // The upper 16 bytes represent the pool id, so this would be pool id 1. See MixinStakinPool for more information.  bytes32 constant internal INITIAL_POOL_ID = 0x0000000000000000000000000000000100000000000000000000000000000000;  // The upper 16 bytes represent the pool id, so this would be an increment of 1. See MixinStakinPool for more information.  uint256 constant internal POOL_ID_INCREMENT_AMOUNT = 0x0000000000000000000000000000000100000000000000000000000000000000;  code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L271-L280  /// @dev Computes the unique id that comes after the input pool id.  /// @param poolId Unique id of pool.  /// @return Next pool id after input pool.  function _computeNextStakingPoolId(bytes32 poolId)  internal  pure  returns (bytes32)  return bytes32(uint256(poolId).safeAdd(POOL_ID_INCREMENT_AMOUNT));  Recommendation  Make pool IDs uint256 values and simply add 1 to generate the next ID.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"
    },
    {
        "title": "5.18 LibProxy.proxyCall() may overwrite important memory    ",
        "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2301. This function has been rewritten in Solidity and now avoids manual memory management.  Description  LibProxy.proxyCall() copies from call data to memory, starting at address 0:  code/contracts/staking/contracts/src/libs/LibProxy.sol:L52-L71  assembly {  // store selector of destination function  let freeMemPtr := 0  if gt(customEgressSelector, 0) {  mstore(0x0, customEgressSelector)  freeMemPtr := add(freeMemPtr, 4)  // adjust the calldata offset, if we should ignore the selector  let calldataOffset := 0  if gt(ignoreIngressSelector, 0) {  calldataOffset := 4  // copy calldata to memory  calldatacopy(  freeMemPtr,  calldataOffset,  calldatasize()  The first 64 bytes of memory are treated as  scratch space  by the Solidity compiler. Writing beyond that point is dangerous, as it will overwrite the free memory pointer and the  zero slot  which is where length-0 arrays point.  Although the current callers of proxyCall() don t appear to use any memory after calling proxyCall(), future changes to the code may introduce very serious and subtle bugs due to this unsafe handling of memory.  Recommendation  Use the actual free memory pointer to determine where it s safe to write to memory.  6 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"
    },
    {
        "title": "6.1 MythX",
        "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  The full set of MythX results for both the exchange and staking contracts are available in a separate report.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"
    },
    {
        "title": "6.2 Surya",
        "body": "  Surya is an utility tool for smart contract systems. It provides a number of visual outputs and information about structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  S\u016brya s Description Report  Files Description Table  ReadOnlyProxy.sol  6ec64526446ebff87ec5528ee3b2786338cc4fa0  Staking.sol  67ddcb9ab75e433882e28d9186815990b7084c61  StakingProxy.sol  248f562d014d0b1ca6de3212966af3e52a7deef1  ZrxVault.sol  6c3249314868a2f5d0984122e8ab1413a5b521c9  fees/MixinExchangeFees.sol  9ac3b696baa8ba09305cfc83d3c08f17d9d528e1  fees/MixinExchangeManager.sol  46f48136a49919cdb5588dc1b3d64c977c3367f2  immutable/MixinConstants.sol  97c2ac83ef97a09cfd485cb0d4b119ba0902cc79  immutable/MixinDeploymentConstants.sol  424f22c45df8e494c4a78f239ea07ff0400d694b  immutable/MixinStorage.sol  8ad475b0e424e7a3ff65eedf2e999cba98f414c8  interfaces/IStaking.sol  ec1d7f214e3fd40e14716de412deee9769359bc0  interfaces/IStakingEvents.sol  25f16b814c4df9d2002316831c3f727d858456c4  interfaces/IStakingProxy.sol  02e35c6b51e08235b2a01d30a8082d60d9d61bee  interfaces/IStorage.sol  eeaa798c262b46d1874e904cf7de0423d4132cee  interfaces/IStorageInit.sol  b9899b03e474ea5adc3b4818a4357f71b8d288d4  interfaces/IStructs.sol  fee17d036883d641afb1222b75eec8427f3cdb96  interfaces/IZrxVault.sol  9067154651675317e000cfa92de9741e50c1c809  libs/LibCobbDouglas.sol  242d62d71cf8bc09177d240c0db59b83f9bb4e96  libs/LibFixedMath.sol  36311e7be09a947fa4e6cd8c544cacd13d65833c  libs/LibFixedMathRichErrors.sol  39cb3e07bbce3272bbf090e87002d5834d288ec2  libs/LibProxy.sol  29abe52857a782c8da39b053cc54e02e295c1ae2  libs/LibSafeDowncast.sol  ae16ed2573d64802793320253b060b9507729c3d  libs/LibStakingRichErrors.sol  f5868ef6066a18277c932e59c0a516ec58920b00  stake/MixinStake.sol  ade59ed356fe72521ffd2ef12ff8896c852f11f8  stake/MixinStakeBalances.sol  cde6ca1a6200570ba18dd6d392ffabf68c2bb464  stake/MixinStakeStorage.sol  cadf34d9d341efd2a85dd13ec3cd4ce8383e0f73  staking_pools/MixinCumulativeRewards.sol  664ea3e35376c81492457dc17832a4d0d602c8ae  staking_pools/MixinStakingPool.sol  74ba9cb2db29b8dd6376d112e9452d117a391b18  staking_pools/MixinStakingPoolRewards.sol  a3b4e5c9b1c3568c94923e2dd9a93090ebdf8536  sys/MixinAbstract.sol  99fd4870c20d8fa03cfa30e8055d3dfb348ed5cd  sys/MixinFinalizer.sol  cc658ed07241c1804cec75b12203be3cd8657b9b  sys/MixinParams.sol  7b395f4da7ed787d7aa4eb915f15377725ff8168  sys/MixinScheduler.sol  2fab6b83a6f9e1d0dd1b1bdcea4b129d166aef1d  Contracts Description Table  Function Name  Visibility  Mutability  Modifiers  ReadOnlyProxy  Implementation  MixinStorage  <Fallback>  External    NO   revertDelegateCall  External    NO   Staking  Implementation  IStaking, MixinParams, MixinStake, MixinExchangeFees  <Fallback>  External    NO   init  Public    onlyAuthorized  StakingProxy  Implementation  IStakingProxy, MixinStorage  <Constructor>  Public    MixinStorage  <Fallback>  External    NO   attachStakingContract  External    onlyAuthorized  detachStakingContract  External    onlyAuthorized  setReadOnlyMode  External    onlyAuthorized  batchExecute  External    NO   _assertValidStorageParams  Internal \ud83d\udd12  _attachStakingContract  Internal \ud83d\udd12  ZrxVault  Implementation  Authorizable, IZrxVault  <Constructor>  Public    Authorizable  setStakingProxy  External    onlyAuthorized  enterCatastrophicFailure  External    onlyAuthorized  setZrxProxy  External    onlyAuthorized onlyNotInCatastrophicFailure  depositFrom  External    onlyStakingProxy onlyNotInCatastrophicFailure  withdrawFrom  External    onlyStakingProxy onlyNotInCatastrophicFailure  withdrawAllFrom  External    onlyInCatastrophicFailure  balanceOf  External    NO   _withdrawFrom  Internal \ud83d\udd12  _assertSenderIsStakingProxy  Private \ud83d\udd10  _assertInCatastrophicFailure  Private \ud83d\udd10  _assertNotInCatastrophicFailure  Private \ud83d\udd10  MixinExchangeFees  Implementation  MixinExchangeManager, MixinStakingPool, MixinFinalizer  payProtocolFee  External    onlyExchange  getActiveStakingPoolThisEpoch  External    NO   _computeMembersAndWeightedStake  Private \ud83d\udd10  _assertValidProtocolFee  Private \ud83d\udd10  MixinExchangeManager  Implementation  IStakingEvents, MixinStorage  addExchangeAddress  External    onlyAuthorized  removeExchangeAddress  External    onlyAuthorized  MixinConstants  Implementation  MixinDeploymentConstants  MixinDeploymentConstants  Implementation  getWethContract  Public    NO   getZrxVault  Public    NO   MixinStorage  Implementation  MixinConstants, Authorizable  IStaking  Interface  moveStake  External    NO   payProtocolFee  External    NO   stake  External    NO   IStakingEvents  Interface  IStakingProxy  Interface  <Fallback>  External    NO   attachStakingContract  External    NO   detachStakingContract  External    NO   IStorage  Interface  stakingContract  External    NO   readOnlyProxy  External    NO   readOnlyProxyCallee  External    NO   nextPoolId  External    NO   numMakersByPoolId  External    NO   currentEpoch  External    NO   currentEpochStartTimeInSeconds  External    NO   protocolFeesThisEpochByPool  External    NO   activePoolsThisEpoch  External    NO   validExchanges  External    NO   epochDurationInSeconds  External    NO   rewardDelegatedStakeWeight  External    NO   minimumPoolStake  External    NO   maximumMakersInPool  External    NO   cobbDouglasAlphaNumerator  External    NO   cobbDouglasAlphaDenominator  External    NO   IStorageInit  Interface  init  External    NO   IStructs  Interface  IZrxVault  Interface  setStakingProxy  External    NO   enterCatastrophicFailure  External    NO   setZrxProxy  External    NO   depositFrom  External    NO   withdrawFrom  External    NO   withdrawAllFrom  External    NO   balanceOf  External    NO   LibCobbDouglas  Library  cobbDouglas  Internal \ud83d\udd12  LibFixedMath  Library  one  Internal \ud83d\udd12  add  Internal \ud83d\udd12  sub  Internal \ud83d\udd12  mul  Internal \ud83d\udd12  div  Internal \ud83d\udd12  mulDiv  Internal \ud83d\udd12  uintMul  Internal \ud83d\udd12  abs  Internal \ud83d\udd12  invert  Internal \ud83d\udd12  toFixed  Internal \ud83d\udd12  toFixed  Internal \ud83d\udd12  toFixed  Internal \ud83d\udd12  toFixed  Internal \ud83d\udd12  toInteger  Internal \ud83d\udd12  ln  Internal \ud83d\udd12  exp  Internal \ud83d\udd12  _mul  Private \ud83d\udd10  _div  Private \ud83d\udd10  _add  Private \ud83d\udd10  LibFixedMathRichErrors  Library  SignedValueError  Internal \ud83d\udd12  UnsignedValueError  Internal \ud83d\udd12  BinOpError  Internal \ud83d\udd12  LibProxy  Library  proxyCall  Internal \ud83d\udd12  LibSafeDowncast  Library  downcastToUint96  Internal \ud83d\udd12  downcastToUint64  Internal \ud83d\udd12  downcastToUint32  Internal \ud83d\udd12  LibStakingRichErrors  Library  OnlyCallableByExchangeError  Internal \ud83d\udd12  ExchangeManagerError  Internal \ud83d\udd12  InsufficientBalanceError  Internal \ud83d\udd12  OnlyCallableByPoolOperatorOrMakerError  Internal \ud83d\udd12  MakerPoolAssignmentError  Internal \ud83d\udd12  BlockTimestampTooLowError  Internal \ud83d\udd12  OnlyCallableByStakingContractError  Internal \ud83d\udd12  OnlyCallableIfInCatastrophicFailureError  Internal \ud83d\udd12  OnlyCallableIfNotInCatastrophicFailureError  Internal \ud83d\udd12  OperatorShareError  Internal \ud83d\udd12  PoolExistenceError  Internal \ud83d\udd12  InvalidProtocolFeePaymentError  Internal \ud83d\udd12  InvalidStakeStatusError  Internal \ud83d\udd12  InitializationError  Internal \ud83d\udd12  InvalidParamValueError  Internal \ud83d\udd12  ProxyDestinationCannotBeNilError  Internal \ud83d\udd12  PreviousEpochNotFinalizedError  Internal \ud83d\udd12  MixinStake  Implementation  MixinStakingPool  stake  External    NO   unstake  External    NO   moveStake  External    NO   _delegateStake  Private \ud83d\udd10  _undelegateStake  Private \ud83d\udd10  _getBalancePtrFromStatus  Private \ud83d\udd10  MixinStakeBalances  Implementation  MixinStakeStorage  getGlobalActiveStake  External    NO   getGlobalInactiveStake  External    NO   getGlobalDelegatedStake  External    NO   getTotalStake  External    NO   getActiveStake  External    NO   getInactiveStake  External    NO   getStakeDelegatedByOwner  External    NO   getWithdrawableStake  Public    NO   getStakeDelegatedToPoolByOwner  Public    NO   getTotalStakeDelegatedToPool  Public    NO   _computeWithdrawableStake  Internal \ud83d\udd12  MixinStakeStorage  Implementation  MixinScheduler  _moveStake  Internal \ud83d\udd12  _loadSyncedBalance  Internal \ud83d\udd12  _loadUnsyncedBalance  Internal \ud83d\udd12  _increaseCurrentAndNextBalance  Internal \ud83d\udd12  _decreaseCurrentAndNextBalance  Internal \ud83d\udd12  _increaseNextBalance  Internal \ud83d\udd12  _decreaseNextBalance  Internal \ud83d\udd12  _storeBalance  Private \ud83d\udd10  _arePointersEqual  Private \ud83d\udd10  MixinCumulativeRewards  Implementation  MixinStakeBalances  _initializeCumulativeRewards  Internal \ud83d\udd12  _isCumulativeRewardSet  Internal \ud83d\udd12  _forceSetCumulativeReward  Internal \ud83d\udd12  _computeMemberRewardOverInterval  Internal \ud83d\udd12  _getMostRecentCumulativeReward  Internal \ud83d\udd12  _getCumulativeRewardAtEpoch  Internal \ud83d\udd12  MixinStakingPool  Implementation  MixinAbstract, MixinStakingPoolRewards  createStakingPool  External    NO   decreaseStakingPoolOperatorShare  External    onlyStakingPoolOperatorOrMaker  joinStakingPoolAsMaker  External    NO   addMakerToStakingPool  External    onlyStakingPoolOperatorOrMaker  removeMakerFromStakingPool  External    onlyStakingPoolOperatorOrMaker  getStakingPoolIdOfMaker  Public    NO   getStakingPool  Public    NO   _addMakerToStakingPool  Internal \ud83d\udd12  _computeNextStakingPoolId  Internal \ud83d\udd12  _assertStakingPoolExists  Internal \ud83d\udd12  _assertNewOperatorShare  Private \ud83d\udd10  _assertSenderIsPoolOperatorOrMaker  Private \ud83d\udd10  MixinStakingPoolRewards  Implementation  MixinAbstract, MixinCumulativeRewards  withdrawDelegatorRewards  External    NO   computeRewardBalanceOfOperator  External    NO   computeRewardBalanceOfDelegator  External    NO   _withdrawAndSyncDelegatorRewards  Internal \ud83d\udd12  _syncPoolRewards  Internal \ud83d\udd12  _computePoolRewardsSplit  Internal \ud83d\udd12  _computeDelegatorReward  Private \ud83d\udd10  _computeUnfinalizedDelegatorReward  Private \ud83d\udd10  _increasePoolRewards  Private \ud83d\udd10  _decreasePoolRewards  Private \ud83d\udd10  MixinAbstract  Implementation  finalizePool  Public    NO   _getUnfinalizedPoolRewards  Internal \ud83d\udd12  MixinFinalizer  Implementation  MixinStakingPoolRewards  endEpoch  External    NO   finalizePool  Public    NO   _getUnfinalizedPoolRewards  Internal \ud83d\udd12  _getActivePoolFromEpoch  Internal \ud83d\udd12  _getActivePoolsFromEpoch  Internal \ud83d\udd12  _wrapEth  Internal \ud83d\udd12  _getAvailableWethBalance  Internal \ud83d\udd12  _getUnfinalizedPoolRewardsFromState  Private \ud83d\udd10  _creditRewardsToPool  Private \ud83d\udd10  MixinParams  Implementation  IStakingEvents, MixinStorage  setParams  External    onlyAuthorized  getParams  External    NO   _initMixinParams  Internal \ud83d\udd12  _assertParamsNotInitialized  Internal \ud83d\udd12  _setParams  Private \ud83d\udd10  MixinScheduler  Implementation  IStakingEvents, MixinStorage  getCurrentEpochEarliestEndTimeInSeconds  Public    NO   _initMixinScheduler  Internal \ud83d\udd12  _goToNextEpoch  Internal \ud83d\udd12  _assertSchedulerNotInitialized  Internal \ud83d\udd12  Legend  Function can modify state  Function is payable  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"
    },
    {
        "title": "5.1 InfinityPool Contract Authorization Bypass Attack    ",
        "body": "  Resolution  Addressed by not allowing the  Description  An attacker could create their own credential and set the Agent ID to 0, which would bypass the subjectIsAgentCaller modifier. The attacker could use this attack to borrow funds from the pool, draining any available liquidity. For example, only an Agent should be able to borrow funds from the pool and call the borrow function:  src/Pool/InfinityPool.sol:L302-L325  function borrow(VerifiableCredential memory vc) external isOpen subjectIsAgentCaller(vc) {  // 1e18 => 1 FIL, can't borrow less than 1 FIL  if (vc.value < WAD) revert InvalidParams();  // can't borrow more than the pool has  if (totalBorrowableAssets() < vc.value) revert InsufficientLiquidity();  Account memory account = _getAccount(vc.subject);  // fresh account, set start epoch and epochsPaid to beginning of current window  if (account.principal == 0) {  uint256 currentEpoch = block.number;  account.startEpoch = currentEpoch;  account.epochsPaid = currentEpoch;  GetRoute.agentPolice(router).addPoolToList(vc.subject, id);  account.principal += vc.value;  account.save(router, vc.subject, id);  totalBorrowed += vc.value;  emit Borrow(vc.subject, vc.value);  // interact - here `msg.sender` must be the Agent bc of the `subjectIsAgentCaller` modifier  asset.transfer(msg.sender, vc.value);  The following modifier checks that the caller is an Agent:  src/Pool/InfinityPool.sol:L96-L101  modifier subjectIsAgentCaller(VerifiableCredential memory vc) {  if (  GetRoute.agentFactory(router).agents(msg.sender) != vc.subject  ) revert Unauthorized();  _;  Recommendation  Ensure only an Agent can call borrow and pass the subjectIsAgentCaller modifier.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"
    },
    {
        "title": "5.2 Agent Data Oracle Signed Credential Front-Running Attack    ",
        "body": "  Resolution  Mitigated by allowing only the  Description  Recommendation  Ensure an Agent can always have new credentials that are needed. One solution would be to allow only an Agent s owner to request the credentials. The problem is that the beneficiary is also supposed to do that, but the beneficiary may also be a contract.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"
    },
    {
        "title": "5.3 Wrong Accounting for totalBorrowed in the InfinityPool.writeOff Function    ",
        "body": "  Resolution  Fixed.  Description  Here is a part of the InfinityPool.writeOff function:  src/Pool/InfinityPool.sol:L271-L287  // transfer the assets into the pool  // whatever we couldn't pay back  uint256 lostAmt = principalOwed > recoveredFunds ? principalOwed - recoveredFunds : 0;  uint256 totalOwed = interestPaid + principalOwed;  asset.transferFrom(  msg.sender,  address(this),  totalOwed > recoveredFunds ? recoveredFunds : totalOwed  );  // write off only what we lost  totalBorrowed -= lostAmt;  // set the account with the funds the pool lost  account.principal = lostAmt;  account.save(router, agentID, id);  The totalBorrowed is decreased by the lostAmt value. Instead, it should be decreased by the original account.principal value to acknowledge the loss.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"
    },
    {
        "title": "5.4 Wrong Accounting for totalBorrowed in the InfinityPool.pay Function    ",
        "body": "  Resolution   Addressed as recommended in two pull rquests:   1,  2.  Description  If the Agent pays more than the current interest debt, the remaining payment will be accounted as repayment of the principal debt:  src/Pool/InfinityPool.sol:L382-L401  // pay interest and principal  principalPaid = vc.value - interestOwed;  // the fee basis only applies to the interest payment  feeBasis = interestOwed;  // protect against underflow  totalBorrowed -= (principalPaid > totalBorrowed) ? 0 : principalPaid;  // fully paid off  if (principalPaid >= account.principal) {  // remove the account from the pool's list of accounts  GetRoute.agentPolice(router).removePoolFromList(vc.subject, id);  // return the amount of funds overpaid  refund = principalPaid - account.principal;  // reset the account  account.reset();  } else {  // interest and partial principal payment  account.principal -= principalPaid;  // move the `epochsPaid` cursor to mark the account as \"current\"  account.epochsPaid = block.number;  Let s focus on the totalBorrowed changes:  src/Pool/InfinityPool.sol:L387  totalBorrowed -= (principalPaid > totalBorrowed) ? 0 : principalPaid;  This value is supposed to be decreased by the principal that is repaid. So there are 2 mistakes in the calculation:  Should be totalBorrowed instead of 0.  The principalPaid cannot be larger than the account.principal in that calculation.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"
    },
    {
        "title": "5.5 The beneficiaryWithdrawable Function Can Be Called by Anyone    ",
        "body": "  Resolution  Fixed by removing beneficiary logic completely.  Description  The beneficiaryWithdrawable function is supposed to be called by the Agent when a beneficiary is trying to withdraw funds:  src/Agent/AgentPolice.sol:L320-L341  function beneficiaryWithdrawable(  address recipient,  address sender,  uint256 agentID,  uint256 proposedAmount  ) external returns (  uint256 amount  ) {  AgentBeneficiary memory beneficiary = _agentBeneficiaries[agentID];  address benneficiaryAddress = beneficiary.active.beneficiary;  // If the sender is not the owner of the Agent or the beneficiary, revert  if(  !(benneficiaryAddress == sender || (IAuth(msg.sender).owner() == sender && recipient == benneficiaryAddress) )) {  revert Unauthorized();  beneficiary,  amount  ) = beneficiary.withdraw(proposedAmount);  // update the beneficiary in storage  _agentBeneficiaries[agentID] = beneficiary;  This function reduces the quota that is supposed to be transferred during the withdraw call:  src/Agent/Agent.sol:L343-L352  sendAmount = agentPolice.beneficiaryWithdrawable(receiver, msg.sender, id, sendAmount);  else if (msg.sender != owner()) {  revert Unauthorized();  // unwrap any wfil needed to withdraw  _poolFundsInFIL(sendAmount);  // transfer funds  payable(receiver).sendValue(sendAmount);  The issue is that anyone can call this function directly, and the quota will be reduced without funds being transferred.  Recommendation  Ensure only the Agent can call this function.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"
    },
    {
        "title": "5.6 An Agent Can Borrow Even With Existing Debt in Interest Payments    ",
        "body": "  Resolution  Mitigated by adding a limit to the remaining interest debt when borrowing. So an agent should have an interest debt that is no larger than 1 day.  Description  To borrow funds, an Agent has to call the borrow function of the pool:  src/Pool/InfinityPool.sol:L302-L325  function borrow(VerifiableCredential memory vc) external isOpen subjectIsAgentCaller(vc) {  // 1e18 => 1 FIL, can't borrow less than 1 FIL  if (vc.value < WAD) revert InvalidParams();  // can't borrow more than the pool has  if (totalBorrowableAssets() < vc.value) revert InsufficientLiquidity();  Account memory account = _getAccount(vc.subject);  // fresh account, set start epoch and epochsPaid to beginning of current window  if (account.principal == 0) {  uint256 currentEpoch = block.number;  account.startEpoch = currentEpoch;  account.epochsPaid = currentEpoch;  GetRoute.agentPolice(router).addPoolToList(vc.subject, id);  account.principal += vc.value;  account.save(router, vc.subject, id);  totalBorrowed += vc.value;  emit Borrow(vc.subject, vc.value);  // interact - here `msg.sender` must be the Agent bc of the `subjectIsAgentCaller` modifier  asset.transfer(msg.sender, vc.value);  Let s assume that the Agent already had some funds borrowed. During this function execution, the current debt status is not checked. The principal debt increases after borrowing, but account.epochsPaid remains the same. So the pending debt will instantly increase as if the borrowing happened on account.epochsPaid.  Recommendation  Ensure the debt is paid when borrowing more funds.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"
    },
    {
        "title": "5.7 The AgentPolice.distributeLiquidatedFunds() Function Can Have Undistributed Residual Funds    ",
        "body": "  Resolution  Mitigated by returning the excess funds in  Description  When an Agent is liquidated, the liquidator (owner of the protocol) is supposed to try to redeem as many funds as possible and re-distribute them to the pools:  src/Agent/AgentPolice.sol:L185-L191  function distributeLiquidatedFunds(uint256 agentID, uint256 amount) external {  if (!liquidated[agentID]) revert Unauthorized();  // transfer the assets into the pool  GetRoute.wFIL(router).transferFrom(msg.sender, address(this), amount);  _writeOffPools(agentID, amount);  The problem is that in the pool, it s accounted that the amount of funds can be larger than the debt. In that case, the pool won t transfer more funds than the pool needs:  src/Pool/InfinityPool.sol:L275-L289  uint256 totalOwed = interestPaid + principalOwed;  asset.transferFrom(  msg.sender,  address(this),  totalOwed > recoveredFunds ? recoveredFunds : totalOwed  );  // write off only what we lost  totalBorrowed -= lostAmt;  // set the account with the funds the pool lost  account.principal = lostAmt;  account.save(router, agentID, id);  emit WriteOff(agentID, recoveredFunds, lostAmt, interestPaid);  If that happens, the remaining funds will be stuck in the AgentPolice contract.  Recommendation  Return the residual funds to the Agent s owner or process them in some way so they are not lost.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"
    },
    {
        "title": "5.8 An Agent Can Be Upgraded Even if There Is No New Implementation    ",
        "body": "  Resolution  Mitigated by introducing a new version control mechanism. This solution also adds centralized power. The owner can create a new deployer with an arbitrary (even lower) version number, while agents can only upgrade to a higher version. Also, agents are forced to upgrade to a new version in another  pull request.  Description  Agents can be upgraded to a new implementation, and only the Agent s owner can call the upgrade function:  src/Agent/AgentFactory.sol:L51-L72  function upgradeAgent(  address agent  ) external returns (address newAgent) {  IAgent oldAgent = IAgent(agent);  address owner = IAuth(address(oldAgent)).owner();  uint256 agentId = agents[agent];  // only the Agent's owner can upgrade, and only a registered agent can be upgraded  if (owner != msg.sender || agentId == 0) revert Unauthorized();  // deploy a new instance of Agent with the same ID and auth  newAgent = GetRoute.agentDeployer(router).deploy(  router,  agentId,  owner,  IAuth(address(oldAgent)).operator()  );  // Register the new agent and unregister the old agent  agents[newAgent] = agentId;  // transfer funds from old agent to new agent and mark old agent as decommissioning  oldAgent.decommissionAgent(newAgent);  // delete the old agent from the registry  agents[agent] = 0;  The issue is that the owner can trigger the upgrade even if no new implementation exists. Multiple possible problems derive from it.  Upgrading to the current implementation of the Agent will break the logic because the current version is not calling the migrateMiner function, so all the miners will stay with the old Agent, and their funds will be lost.  The owner can accidentally trigger multiple upgrades simultaneously, leading to a loss of funds (https://github.com/ConsenSysDiligence/glif-audit-2023-04/issues/2).  The owner also has no control over the new version of the Agent. To increase decentralization, it s better to pass the deployer s address as a parameter additionally.  Recommendation  Ensure the upgrades can only happen when there is a new version of an Agent, and the owner controls this version.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"
    },
    {
        "title": "5.9 Potential Re-Entrancy Issues When Upgrading the Contracts    ",
        "body": "  Resolution   The issue is   mitigated by removing the old agent before the potential re-entrancy.  Description  The protocol doesn t have any built-in re-entrancy protection mechanisms. That mainly explains by using the wFIL token, which is not supposed to give that opportunity. And also by carefully using FIL transfers.  However, there are some places in the code where things may go wrong in the future. For example, when upgrading an Agent:  src/Agent/AgentFactory.sol:L51-L72  function upgradeAgent(  address agent  ) external returns (address newAgent) {  IAgent oldAgent = IAgent(agent);  address owner = IAuth(address(oldAgent)).owner();  uint256 agentId = agents[agent];  // only the Agent's owner can upgrade, and only a registered agent can be upgraded  if (owner != msg.sender || agentId == 0) revert Unauthorized();  // deploy a new instance of Agent with the same ID and auth  newAgent = GetRoute.agentDeployer(router).deploy(  router,  agentId,  owner,  IAuth(address(oldAgent)).operator()  );  // Register the new agent and unregister the old agent  agents[newAgent] = agentId;  // transfer funds from old agent to new agent and mark old agent as decommissioning  oldAgent.decommissionAgent(newAgent);  // delete the old agent from the registry  agents[agent] = 0;  Here, we see the oldAgent.decommissionAgent(newAgent); call happens before the oldAgent is deleted. Inside this function, we see:  src/Agent/Agent.sol:L200-L212  function decommissionAgent(address _newAgent) external {  // only the agent factory can decommission an agent  AuthController.onlyAgentFactory(router, msg.sender);  // if the newAgent has a mismatching ID, revert  if(IAgent(_newAgent).id() != id) revert Unauthorized();  // set the newAgent in storage, which marks the upgrade process as starting  newAgent = _newAgent;  uint256 _liquidAssets = liquidAssets();  // Withdraw all liquid funds from the Agent to the newAgent  _poolFundsInFIL(_liquidAssets);  // transfer funds to new agent  payable(_newAgent).sendValue(_liquidAssets);  Here, the FIL is transferred to a new contract which is currently unimplemented and unknown. Potentially, the fallback function of this contract could trigger a re-entrancy attack. If that s the case, during the execution of this function, there will be two contracts that are active agents with the same ID, and the attacker can try to use that maliciously.  Recommendation  Be very cautious with further implementations of agents and pools. Also, consider using reentrancy protection in public functions.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"
    },
    {
        "title": "5.10 InfinityPool Is Subject to a Donation With Inflation Attack if Emtied.    ",
        "body": "  Resolution  this issue will not be fixed in the current version of the contracts since some of the shares were already minted. The next iteration of the pool will have a more generic fix to this issue.  Description  Since InfinityPool is an implementation of the ERC4626 vault, it is too susceptible to inflation attacks. An attacker could front-run the first deposit and inflate the share price to an extent where the following deposit will be less than the value of 1 wei of share resulting in 0 shares minted. The attacker could conduct the inflation by means of self-destructing of another contract. In the case of GLIF this attack is less likely on the first pool since GLIF team accepts predeposits so some amount of shares was already minted. We do suggest fixing this issue before the next pool is deployed and no pre-stake is generated.  Examples  src/Pool/InfinityPool.sol:L491-L516  /*//////////////////////////////////////////////////////////////  4626 LOGIC  //////////////////////////////////////////////////////////////*/  /**  @dev Converts `assets` to shares  @param assets The amount of assets to convert  @return shares - The amount of shares converted from assets  /  function convertToShares(uint256 assets) public view returns (uint256) {  uint256 supply = liquidStakingToken.totalSupply(); // Saves an extra SLOAD if totalSupply is non-zero.  return supply == 0 ? assets : assets * supply / totalAssets();  /**  @dev Converts `shares` to assets  @param shares The amount of shares to convert  @return assets - The amount of assets converted from shares  /  function convertToAssets(uint256 shares) public view returns (uint256) {  uint256 supply = liquidStakingToken.totalSupply(); // Saves an extra SLOAD if totalSupply is non-zero.  return supply == 0 ? shares : shares * totalAssets() / supply;  Recommendation  Since the pool does not need to accept donations, the easiest way to handle this case is to use virtual price, where the balance of the contract is duplicated in a separate variable.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"
    },
    {
        "title": "5.11 MaxWithdraw Should Potentially Account for the Funds Available in the Ramp.    ",
        "body": "  Resolution   Partially fixed in   https://github.com/glif-confidential/pools/issues/462 but the ramp balance is still not accounted for.  Description  Since InfinityPool is ERC4626 it should also support the MaxWithdraw method. According to the EIP it should include any withdrawal limitation that the participant could encounter. At the moment the MaxWithdraw function returns the maximum amount of IOU tokens rather than WFIL. Since IOU token is not the asset token of the vault, this behavior is not ideal.  Examples  src/Pool/InfinityPool.sol:L569-L571  function maxWithdraw(address owner) public view returns (uint256) {  return convertToAssets(liquidStakingToken.balanceOf(owner));  Recommendation  We suggest considering returning the maximum amount of WFIL withdrawal which should account for Ramp balance.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"
    },
    {
        "title": "5.12 The Upgradeability of MinerRegistry, AgentPolice, and Agent Is Overcomplicated and Has a Hight Chance of Errors.   ",
        "body": "  Description  During the engagement, we have identified a few places that signify that the Agent, MinerRegistry and AgentPolice can be upgraded, for example:  Ability to migrate the miner from one version of the Agent to another inside the migrateMiner.  Ability to refreshRoutes that would update the AgentPolice and MinerRegistry addresses for a given Agent.  Ability to decommission pool. We believe that this functionality is present it is not very well thought through. For example, both MinerRegistry and AgentPolice are not upgradable but have mappings inside of them.  src/Agent/AgentPolice.sol:L51-L60  mapping(uint256 => bool) public liquidated;  /// @notice `_poolIDs` maps agentID to the pools they have actively borrowed from  mapping(uint256 => uint256[]) private _poolIDs;  /// @notice `_credentialUseBlock` maps signature bytes to when a credential was used  mapping(bytes32 => uint256) private _credentialUseBlock;  /// @notice `_agentBeneficiaries` maps an Agent ID to its Beneficiary struct  mapping(uint256 => AgentBeneficiary) private _agentBeneficiaries;  src/Agent/MinerRegistry.sol:L18-L20  mapping(bytes32 => bool) private _minerRegistered;  mapping(uint256 => uint64[]) private _minersByAgent;  That means that any time these contracts would need to be upgraded, the contents of those mappings will need to be somehow recreated in the new contract. That is not trivial since it is not easy to obtain all values of a mapping. This will also require an additional protocol-controlled setter ala kickstart mapping functions that are not ideal.  In the case of Agent if the contract was upgradable there would be no need for a process of migrating miners that can be tedious and opens possibilities for errors. Since protocol has a lot of centralization and trust assumptions already, having upgradability will not contribute to it a lot.  We also believe that during the upgrade of the pool, the PoolToken will stay the same in the new pool. That means that the minting and burning permissions of the share tokens have to be carefully updated or checked in a manner that does not require the address of the pool to be constant. Since we did not have access to this file, we can not check if that is done correctly.  Recommendation  Consider using upgradable contracts or have a solid upgrade plan that is well-tested before an emergency situation occurs.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"
    },
    {
        "title": "5.13 Mint Function in the Infinity Pool Will Emit the Incorrect Value.    ",
        "body": "  Resolution  Fixed by emitting the right value.  Description  Examples  src/Pool/InfinityPool.sol:L449-L457  function mint(uint256 shares, address receiver) public isOpen returns (uint256 assets) {  if(shares == 0) revert InvalidParams();  // These transfers need to happen before the mint, and this is forcing a higher degree of coupling than is ideal  assets = previewMint(shares);  asset.transferFrom(msg.sender, address(this), assets);  liquidStakingToken.mint(receiver, shares);  assets = convertToAssets(shares);  emit Deposit(msg.sender, receiver, assets, shares);  Recommendation  Use the assets value computed by the previewMint when emitting the event.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"
    },
    {
        "title": "5.14 Incorrect Operator Used    ",
        "body": "  Resolution  Fixed.  Description  Minor typo in the InfinityPool where the -= should be replaced with -.  Examples  src/Pool/InfinityPool.sol:L200  return balance -= feesCollected;  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"
    },
    {
        "title": "5.15 Potential Overpayment Due to Rounding Imprecision   ",
        "body": "  Resolution  The issue is acknowledged and the potential loss is considered tolerable.  Description  Inside the InifintyPool the pay function might accept unaccounted files. Imagine a situation where an Agent is trying to repay only the fees portion of the debt. In that case, the following branch will be executed:  src/Pool/InfinityPool.sol:L373-L381  if (vc.value <= interestOwed) {  // compute the amount of epochs this payment covers  // vc.value is not WAD yet, so divWadDown cancels the extra WAD in interestPerEpoch  uint256 epochsForward = vc.value.divWadDown(interestPerEpoch);  // update the account's `epochsPaid` cursor  account.epochsPaid += epochsForward;  // since the entire payment is interest, the entire payment is used to compute the fee (principal payments are fee-free)  feeBasis = vc.value;  } else {  The issue is if the value does not divide by the interestPerEpoch exactly, any remainder will remain in the InfinityPool.  src/Pool/InfinityPool.sol:L376  uint256 epochsForward = vc.value.divWadDown(interestPerEpoch);  Recommendation  Since the remainder will most likely not be too large this is not critical, but ideally, those remaining funds would be included in the refund variable.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"
    },
    {
        "title": "5.16 jumpStartAccount Should Be Subject to the Same Approval Checks as Regular Borrow.    ",
        "body": "  Resolution  Will not be fixed due to the complexity of the fix which will require passing verified credentials to be executed.  Description  InfinityPool contract has the ability to kick start an account that will have a debt position in this pool.  Examples  src/Pool/InfinityPool.sol:L673-L689  function jumpStartAccount(address receiver, uint256 agentID, uint256 accountPrincipal) external onlyOwner {  Account memory account = _getAccount(agentID);  // if the account is already initialized, revert  if (account.principal != 0) revert InvalidState();  // create the account  account.principal = accountPrincipal;  account.startEpoch = block.number;  account.epochsPaid = block.number;  // save the account  account.save(router, agentID, id);  // add the pool to the agent's list of borrowed pools  GetRoute.agentPolice(router).addPoolToList(agentID, id);  // mint the iFIL to the receiver, using principal as the deposit amount  liquidStakingToken.mint(receiver, convertToShares(accountPrincipal));  // account for the new principal in the total borrowed of the pool  totalBorrowed += accountPrincipal;  Recommendation  We suggest that this action is subject to the same rules as the standard borrow action. Thus checks on DTE, LTV and DTI should be done if possible.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"
    },
    {
        "title": "5.17 No Miner Migration Is Happening in the Current Implementation of the Agent  ",
        "body": "  Description  All miners should be transferred from the old Agent to a new one when upgrading an Agent. To do so, the new Agent is supposed to call the migrateMiner function for every miner:  src/Agent/Agent.sol:L219-L235  function migrateMiner(uint64 miner) external {  if (newAgent != msg.sender) revert Unauthorized();  uint256 newId = IAgent(newAgent).id();  if (  // first check to make sure the agentFactory knows about this \"agent\"  GetRoute.agentFactory(router).agents(newAgent) != newId ||  // then make sure this is the same agent, just upgraded  newId != id ||  // check to ensure this miner was registered to the original agent  !minerRegistry.minerRegistered(id, miner)  ) revert Unauthorized();  // propose an ownership change (must be accepted in v2 agent)  miner.changeOwnerAddress(newAgent);  emit MigrateMiner(msg.sender, newAgent, miner);  The problem is that this function is not called in the current Agent implementation. Since it s just the first version of an Agent contract, it s not a big issue. There is only one edge case where this may be a vulnerability. That may happen if the owner of an Agent decides to upgrade the contract to the same version. It is possible to do, and in that case, the miners  funds will be lost.  Recommendation  It s important to remember to call migrateMiner in a new version and not allow upgrading to the same implementation.  ",
        "labels": [
            "Consensys",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"
    },
    {
        "title": "3.1 The Hypervisor.deposit function does not check the msg.sender    ",
        "body": "  Resolution   Partially fixed in   GammaStrategies/hypervisor@9a7a3dd, by allowing only  Description  Hypervisor.deposit pulls pre-approved ERC20 tokens from the from address to the contract. Later it mints shares to the to address. Attackers can determine both the from and to addresses as they wish, and thus steal shares (that can be redeemed to tokens immediately) from users that pre-approved the contract to spend ERC20 tokens on their behalf.  Recommendation  As described in issue 3.5, we recommend restricting access to this function only for UniProxy. Moreover, the UniProxy contract should validate that from == msg.sender.  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"
    },
    {
        "title": "3.2 UniProxy.depositSwap - Tokens are not approved before calling Router.exactInput    ",
        "body": "  Resolution   Fixed in   GammaStrategies/hypervisor@9a7a3dd by deleting the  Description  the call to Router.exactInputrequires the sender to pre-approve the tokens. We could not find any reference for that, thus we assume that a call to UniProxy.depositSwap will always revert.  Examples  code/contracts/UniProxy.sol:L202-L234  router = ISwapRouter(_router);  uint256 amountOut;  uint256 swap;  if(swapAmount < 0) {  //swap token1 for token0  swap = uint256(swapAmount * -1);  IHypervisor(pos).token1().transferFrom(msg.sender, address(this), deposit1+swap);  amountOut = router.exactInput(  ISwapRouter.ExactInputParams(  path,  address(this),  block.timestamp + swapLife,  swap,  deposit0  );  else{  //swap token1 for token0  swap = uint256(swapAmount);  IHypervisor(pos).token0().transferFrom(msg.sender, address(this), deposit0+swap);  amountOut = router.exactInput(  ISwapRouter.ExactInputParams(  path,  address(this),  block.timestamp + swapLife,  swap,  deposit1  );  Recommendation  Consider approving the exact amount of input tokens before the swap.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"
    },
    {
        "title": "3.3 Uniproxy.depositSwap - _router should not be determined by the caller    ",
        "body": "  Resolution   Fixed in   GammaStrategies/hypervisor@9a7a3dd by deleting the  Description  Uniproxy.depositSwap uses _router that is determined by the caller, which in turn might inject a  fake  contract, and thus may steal funds stuck in the UniProxy contract.  The UniProxy contract has certain trust assumptions regarding the router. The router is supposed to return not less than deposit1(or deposit0) amount of tokens but that fact is never checked.  Examples  code/contracts/UniProxy.sol:L168-L177  function depositSwap(  int256 swapAmount, // (-) token1, (+) token0 for token1; amount to swap  uint256 deposit0,  uint256 deposit1,  address to,  address from,  bytes memory path,  address pos,  address _router  ) external returns (uint256 shares) {  Recommendation  Consider removing the _router parameter from the function, and instead, use a storage variable that will be initialized in the constructor.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"
    },
    {
        "title": "3.4 Re-entrancy + flash loan attack can invalidate price check    ",
        "body": "  Resolution   Fixed in   GammaStrategies/hypervisor@9a7a3dd by implementing the auditor s recommendation.  Description  The UniProxy contract has a price manipulation protection:  code/contracts/UniProxy.sol:L75-L82  if (twapCheck || positions[pos].twapOverride) {  // check twap  checkPriceChange(  pos,  (positions[pos].twapOverride ? positions[pos].twapInterval : twapInterval),  (positions[pos].twapOverride ? positions[pos].priceThreshold : priceThreshold)  );  But after that, the tokens are transferred from the user, if the token transfer allows an attacker to hijack the call-flow of the transaction inside, the attacker can manipulate the Uniswap price there, after the check happened. The Hypervisor s deposit function itself is vulnerable to the flash-loan attack.  Recommendation  Make sure the price does not change before the Hypervisor.deposit call. For example, the token transfers can be made at the beginning of the UniProxy.deposit function.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"
    },
    {
        "title": "3.5 The deposit function of the Hypervisor contract should only be called from UniProxy    ",
        "body": "  Resolution   Partially fixed in   GammaStrategies/hypervisor@9a7a3dd, by allowing only  Description  The deposit function is designed to be called only from the UniProxy contract, but everyone can call it. This function does not have any protection against price manipulation in the Uniswap pair. A deposit can be frontrunned, and the depositor s funds may be  stolen .  Recommendation  Make sure only UniProxy can call the deposit function.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"
    },
    {
        "title": "3.6 UniProxy.properDepositRatio - Proper ratio will not prevent liquidity imbalance for all possible scenarios    ",
        "body": "  Resolution   Fixed in   GammaStrategies/hypervisor@9a7a3dd by deleting the  Description  Examples  code/contracts/UniProxy.sol:L258-L275  function properDepositRatio(  address pos,  uint256 deposit0,  uint256 deposit1  ) public view returns (bool) {  (uint256 hype0, uint256 hype1) = IHypervisor(pos).getTotalAmounts();  if (IHypervisor(pos).totalSupply() != 0) {  uint256 depositRatio = deposit0 == 0 ? 10e18 : deposit1.mul(1e18).div(deposit0);  depositRatio = depositRatio > 10e18 ? 10e18 : depositRatio;  depositRatio = depositRatio < 10e16 ? 10e16 : depositRatio;  uint256 hypeRatio = hype0 == 0 ? 10e18 : hype1.mul(1e18).div(hype0);  hypeRatio = hypeRatio > 10e18 ? 10e18 : hypeRatio;  hypeRatio = hypeRatio < 10e16 ? 10e16 : hypeRatio;  return (FullMath.mulDiv(depositRatio, deltaScale, hypeRatio) < depositDelta &&  FullMath.mulDiv(hypeRatio, deltaScale, depositRatio) < depositDelta);  return true;  Recommendation  Consider removing the cap of [0.1,10] both for depositRatio and for hypeRatio.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"
    },
    {
        "title": "3.7 UniProxy - SafeERC20 is declared but safe functions are not used    ",
        "body": "  Resolution   fixed in   GammaStrategies/hypervisor@9a7a3dd by implementing the auditor s recommendation.  Description  The UniProxy contract declares the usage of the SafeERC20 library for functions of the IERC20 type. However, unsafe functions are used instead of safe ones.  Examples  Usage of approve instead of safeApprove  Usage of transferFrom instead of safeTransferFrom.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"
    },
    {
        "title": "3.8 Missing/wrong implementation    ",
        "body": "  Resolution  Fixed in GammaStrategies/hypervisor@9a7a3dd by introducing two new functions: toggleDepositOverride, setPriceThresholdPos.  Fixed in GammaStrategies/hypervisor@9a7a3dd by keeping only the version of deposit function with 4 parameters.  Fixed in GammaStrategies/hypervisor@9a7a3dd by removing the unreachable code.  Examples  The UniProxy contract has different functions used for setting the properties of a position. However, Position.priceThreshold, and Position.depositOverride are never assigned to, even though they are being used.  UniProxy.deposit is calling IHypervisor.deposit multiple times with different function signatures (3 and 4 parameters), while the Hypervisor contract only implements the version with 4 parameters, and does not implement the IHypervisor interface.  Hypervisor.uniswapV3MintCallback | uniswapV3SwapCallback - both these functions contain unreachable code, namely the case where payer != address(this).  Recommendations  Consider adding functions to set these properties, or alternatively, a single function to set the properties of a position.  Consider supporting a single deposit function for IHypervisor, and make sure that the actual implementation adheres to this interface.  Consider deleting these lines.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"
    },
    {
        "title": "3.9 Hypervisor.withdraw - Possible reentrancy    ",
        "body": "  Resolution   Fixed in   GammaStrategies/hypervisor@9a7a3dd by implementing the auditor s recommendation.  Description  Recommendation  Consider adding a ReentrancyGuard both to Hypervisor.withdraw and Hypervisor.deposit  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"
    },
    {
        "title": "3.10 UniProxy.depositSwap doesn t deposit all the users  funds    ",
        "body": "  Resolution   Fixed in   GammaStrategies/hypervisor@9a7a3dd by deleting the  Description  When executing the swap, the minimal amount out is passed to the router (deposit1 in this example), but the actual swap amount will be amountOut. But after the trade, instead of depositing amountOut, the contract tries to deposit deposit1, which is lower. This may result in some users  funds staying in the UniProxy contract.  code/contracts/UniProxy.sol:L220-L242  else{  //swap token1 for token0  swap = uint256(swapAmount);  IHypervisor(pos).token0().transferFrom(msg.sender, address(this), deposit0+swap);  amountOut = router.exactInput(  ISwapRouter.ExactInputParams(  path,  address(this),  block.timestamp + swapLife,  swap,  deposit1  );  require(amountOut > 0, \"Swap failed\");  if (positions[pos].version < 2) {  // requires lp token transfer from proxy to msg.sender  shares = IHypervisor(pos).deposit(deposit0, deposit1, address(this));  IHypervisor(pos).transfer(to, shares);  Recommendation  Deposit all the user s funds to the Hypervisor.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"
    },
    {
        "title": "3.11 Hypervisor - Multiple  sandwiching  front running vectors    ",
        "body": "  Resolution   Fixed in   GammaStrategies/hypervisor@9a7a3dd by removing the call to  Description  The amount of tokens received from UniswapV3Pool functions might be manipulated by front-runners due to the decentralized nature of AMMs, where the order of transactions can not be pre-determined. A potential  sandwicher  may insert a buying order before the user s call to Hypervisor.rebalance for instance, and a sell order after.  More specifically, calls to pool.swap, pool.mint, pool.burn are susceptible to  sandwiching  vectors.  Examples  Hypervisor.rebalance  code/contracts/Hypervisor.sol:L278-L286  if (swapQuantity != 0) {  pool.swap(  address(this),  swapQuantity > 0,  swapQuantity > 0 ? swapQuantity : -swapQuantity,  swapQuantity > 0 ? TickMath.MIN_SQRT_RATIO + 1 : TickMath.MAX_SQRT_RATIO - 1,  abi.encode(address(this))  );  code/contracts/Hypervisor.sol:L348-L363  function _mintLiquidity(  int24 tickLower,  int24 tickUpper,  uint128 liquidity,  address payer  ) internal returns (uint256 amount0, uint256 amount1) {  if (liquidity > 0) {  (amount0, amount1) = pool.mint(  address(this),  tickLower,  tickUpper,  liquidity,  abi.encode(payer)  );  code/contracts/Hypervisor.sol:L365-L383  function _burnLiquidity(  int24 tickLower,  int24 tickUpper,  uint128 liquidity,  address to,  bool collectAll  ) internal returns (uint256 amount0, uint256 amount1) {  if (liquidity > 0) {  // Burn liquidity  (uint256 owed0, uint256 owed1) = pool.burn(tickLower, tickUpper, liquidity);  // Collect amount owed  uint128 collect0 = collectAll ? type(uint128).max : _uint128Safe(owed0);  uint128 collect1 = collectAll ? type(uint128).max : _uint128Safe(owed1);  if (collect0 > 0 || collect1 > 0) {  (amount0, amount1) = pool.collect(to, tickLower, tickUpper, collect0, collect1);  Recommendation  Consider adding an amountMin parameter(s) to ensure that at least the amountMin of tokens was received.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"
    },
    {
        "title": "3.12 Full test suite is necessary ",
        "body": "  Description  The test suite at this stage is not complete. It is crucial to have a full test coverage that includes the edge cases and failure scenarios, especially for complex system like Gamma.  As we ve seen in some smart contract incidents, a complete test suite can prevent issues that might be hard to find with manual reviews.  Some issues such as issue 3.8, issue 3.2 could be caught by a full-coverage test suite.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"
    },
    {
        "title": "3.13 Uniswap v3 callbacks access control should be hardened    ",
        "body": "  Resolution   Fixed in   GammaStrategies/hypervisor@9a7a3dd by implementing the auditor s recommendation for  Description  Examples  code/contracts/Hypervisor.sol:L407-L445  function uniswapV3MintCallback(  uint256 amount0,  uint256 amount1,  bytes calldata data  ) external override {  require(msg.sender == address(pool));  address payer = abi.decode(data, (address));  if (payer == address(this)) {  if (amount0 > 0) token0.safeTransfer(msg.sender, amount0);  if (amount1 > 0) token1.safeTransfer(msg.sender, amount1);  } else {  if (amount0 > 0) token0.safeTransferFrom(payer, msg.sender, amount0);  if (amount1 > 0) token1.safeTransferFrom(payer, msg.sender, amount1);  function uniswapV3SwapCallback(  int256 amount0Delta,  int256 amount1Delta,  bytes calldata data  ) external override {  require(msg.sender == address(pool));  address payer = abi.decode(data, (address));  if (amount0Delta > 0) {  if (payer == address(this)) {  token0.safeTransfer(msg.sender, uint256(amount0Delta));  } else {  token0.safeTransferFrom(payer, msg.sender, uint256(amount0Delta));  } else if (amount1Delta > 0) {  if (payer == address(this)) {  token1.safeTransfer(msg.sender, uint256(amount1Delta));  } else {  token1.safeTransferFrom(payer, msg.sender, uint256(amount1Delta));  Recommendation  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"
    },
    {
        "title": "3.14 Code quality comments    ",
        "body": "  Resolution  Fixed in GammaStrategies/hypervisor@9a7a3dd by removing the from parameter.  Fixed in GammaStrategies/hypervisor@9a7a3dd by implementing the auditor s recommendation.  Fixed in GammaStrategies/hypervisor@9a7a3dd by deleting depositSwap.  Examples  UniProxy.deposit - from parameter is never used.  UniProxy - MAX_INT should be changed to MAX_UINT.  Consider using compiler version >= 0.8.0, and make sure that the compiler version is specified explicitly for every .sol file in the repo.  UniProxy - Minimize code duplication in deposit and depositSwap.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"
    },
    {
        "title": "6.1 Ether temporarily held during transactions can be stolen via reentrancy    ",
        "body": "  Resolution  This is addressed in 0xProject/protocol@437a3b0 by transferring exactly msg.value in sellToLiquidityProvider(). This adequately protects against this specific vulnerability.  The client team decided to leave the accounting in MetaTransactionsFeature as-is due to the complexity/expense of tracking ether consumption more strictly.  Description  The exchange proxy typically holds no ether balance, but it can temporarily hold a balance during a transaction. This balance is vulnerable to theft if the following conditions are met:  No check at the end of the transaction reverts if ether goes missing,  reentrancy is possible during the transaction, and  a mechanism exists to spend ether held by the exchange proxy.  We found one example where these conditions are met, but it s possible that more exist.  Example  MetaTransactionsFeature.executeMetaTransaction() accepts ether, which is used to pay protocol fees. It s possible for less than the full amount in msg.value to be consumed, which is why the function uses the refundsAttachedEth modifier to return any remaining ether to the caller:  code/contracts/zero-ex/contracts/src/features/MetaTransactionsFeature.sol:L98-L106  /// @dev Refunds up to `msg.value` leftover ETH at the end of the call.  modifier refundsAttachedEth() {  _;  uint256 remainingBalance =  LibSafeMathV06.min256(msg.value, address(this).balance);  if (remainingBalance > 0) {  msg.sender.transfer(remainingBalance);  Notice that this modifier just returns the remaining ether balance (up to msg.value). It does not check for a specific amount of remaining ether. This meets condition (1) above.  It s impossible to reenter the system with a second metatransaction because executeMetaTransaction() uses the modifier nonReentrant, but there s nothing preventing reentrancy via a different feature. We can achieve reentrancy by trading a token that uses callbacks (e.g. ERC777 s hooks) during transfers. This meets condition (2).  LiquidityProviderFeature.sellToLiquidityProvider() provides such a mechanism. By passing  code/contracts/zero-ex/contracts/src/features/LiquidityProviderFeature.sol:L114-L115  if (inputToken == ETH_TOKEN_ADDRESS) {  provider.transfer(sellAmount);  This meets condition (3).  The full steps to exploit this vulnerability are as follows:  A maker/attacker signs a trade where one of the tokens will invoke a callback during the trade.  A taker signs a metatransaction to take this trade.  A relayer sends in the metatransaction, providing more ether than is necessary to pay the protocol fee. (It s unclear how likely this situation is.)  During the token callback, the attacker invokes LiquidityProviderFeature.sellToLiquidityProvider() to transfer the excess ether to their account.  The metatransaction feature returns the remaining ether balance, which is now zero.  Recommendation  In general, we recommend using strict accounting of ether throughout the system. If there s ever a temporary balance, it should be accurately resolved at the end of the transaction, after any potential reentrancy opportunities.  For the example we specifically found, we recommend doing strict accounting in the metatransactions feature. This means features called via a metatransaction would need to return how much ether was consumed. The metatransactions feature could then refund exactly msg.value - <consumed ether>. The transaction should be reverted if this fails because it means ether went missing during the transaction.  We also recommend limiting sellToLiquidityProvider() to only transfer up to msg.value. This is a form of defense in depth in case other vectors for a similar attack exist.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/12/0x-exchange-v4/"
    },
    {
        "title": "6.2 UniswapFeature: Non-static call to ERC20.allowance()    ",
        "body": "  Resolution   This is fixed in   0xProject/protocol@437a3b0.  Description  In the case where a token is possibly  greedy  (consumes all gas on failure), UniswapFeature makes a call to the token s allowance() function to check whether the user has provided a token allowance to the protocol proxy or to the AllowanceTarget. This call is made using call(), potentially allowing state-changing operations to take place before control of the execution returns to UniswapFeature.  code/contracts/zero-ex/contracts/src/features/UniswapFeature.sol:L373-L377  // `token.allowance()``  mstore(0xB00, ALLOWANCE_CALL_SELECTOR_32)  mstore(0xB04, caller())  mstore(0xB24, address())  let success := call(gas(), token, 0, 0xB00, 0x44, 0xC00, 0x20)  Recommendation  Replace the call() with a staticcall().  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/12/0x-exchange-v4/"
    },
    {
        "title": "6.3 UniswapFeature: Unchecked returndatasize in low-level external calls    ",
        "body": "  Resolution   This is fixed in   0xProject/protocol@437a3b0.  Description  UniswapFeature makes a number of external calls from low-level assembly code. Two of these calls rely on the CALL opcode to copy the returndata to memory without checking that the call returned the expected amount of data. Because the CALL opcode does not zero memory if the call returns less data than expected, this can lead to usage of dirty memory under the assumption that it is data returned from the most recent call.  Examples  Call to UniswapV2Pair.getReserves()  code/contracts/zero-ex/contracts/src/features/UniswapFeature.sol:L201-L205  // Call pair.getReserves(), store the results at `0xC00`  mstore(0xB00, UNISWAP_PAIR_RESERVES_CALL_SELECTOR_32)  if iszero(staticcall(gas(), pair, 0xB00, 0x4, 0xC00, 0x40)) {  bubbleRevert()  Call to ERC20.allowance()  code/contracts/zero-ex/contracts/src/features/UniswapFeature.sol:L372-L377  // Check if we have enough direct allowance by calling  // `token.allowance()``  mstore(0xB00, ALLOWANCE_CALL_SELECTOR_32)  mstore(0xB04, caller())  mstore(0xB24, address())  let success := call(gas(), token, 0, 0xB00, 0x44, 0xC00, 0x20)  Recommendation  Instead of providing a memory range for call() to write returndata to, explicitly check returndatasize() after the call is made and then copy the data into memory using returndatacopy().  if lt(returndatasize(), EXPECTED_SIZE) {  revert(0, 0)  returndatacopy(0xC00, 0x00, EXPECTED_SIZE)  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/12/0x-exchange-v4/"
    },
    {
        "title": "6.4 Rollback functionality can lead to untested combinations   ",
        "body": "  Resolution  From the client team:  Just like our migrations, we batch our rollbacks by release, which enforces rolling back to known good configurations.  The documentation now includes an emergency playbook that describes how rollbacks should be done.  Description  SimpleFunctionRegistry maps individual function selectors to implementation contracts. As features are newly deployed or upgraded, functions are registered in logical groups after a timelock enforced by the owning multisig wallet. This gives users time to evaluate upcoming changes and stop using the contract if they don t like the changes.  Once deployed, however, any function can individually be rolled back without a timelock to any previous version of that function. Users are given no warning, functions can be rolled back to any previous implementation (regardless of how old), and the per-function granularity means that the configuration after rollback may be a never-before-seen combination of functions.  The combinatorics makes it impossible for a user (or auditor) to be comfortable with all the possible outcomes of rollbacks. If there are n versions each of m functions, there are n^m combinations that could be in effect at any moment. Some functions depend on other onlySelf functions, so the behavior of those combinations is not at all obvious.  This presents a trust problem for users.  Recommendation  Rollback makes sense as a way to rapidly recover from a bad deployment, but we recommend limiting its scope. The following ideas are in preferred order (our favorite first):  Disallow rollback altogether except to an implementation of address(0). This way broken functionality can be immediately disabled, but no old version of a function can be reinstated.  Limit rollback by number of versions, e.g. only allowing rollback to the immediately previous version of a function.  Limit rollback by time, e.g. only allowing rollback to versions in the past n weeks.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/12/0x-exchange-v4/"
    },
    {
        "title": "4.1 Reuse of CHAINID from contract deployment    ",
        "body": "  Resolution   This is addressed in   ScopeLift/umbra-protocol@7cfdc81.  Description  The internal function _validateWithdrawSignature() is used to check whether a sponsored token withdrawal is approved by the owner of the stealth address that received the tokens. Among other data, the chain ID is signed over to prevent replay of signatures on other EVM-compatible chains.  contracts/contracts/Umbra.sol:L307-L329  function _validateWithdrawSignature(  address _stealthAddr,  address _acceptor,  address _tokenAddr,  address _sponsor,  uint256 _sponsorFee,  IUmbraHookReceiver _hook,  bytes memory _data,  uint8 _v,  bytes32 _r,  bytes32 _s  ) internal view {  bytes32 _digest =  keccak256(  abi.encodePacked(  \"\\x19Ethereum Signed Message:\\n32\",  keccak256(abi.encode(chainId, version, _acceptor, _tokenAddr, _sponsor, _sponsorFee, address(_hook), _data))  );  address _recoveredAddress = ecrecover(_digest, _v, _r, _s);  require(_recoveredAddress != address(0) && _recoveredAddress == _stealthAddr, \"Umbra: Invalid Signature\");  However, this chain ID is set as an immutable value in the contract constructor. In the case of a future contentious hard fork of the Ethereum network, the same Umbra contract would exist on both of the resulting chains. One of these two chains would be expected to change the network s chain ID, but the Umbra contracts would not be aware of this change. As a result, signatures to the Umbra contract on either chain would be replayable on the other chain.  This is a common pattern in contracts that implement EIP-712 signatures. Presumably, the motivation in most cases for committing to the chain ID at deployment time is to avoid recomputing the EIP-712 domain separator for every signature verification. In this case, the chain ID is a direct input to the generation of the signed digest, so this should not be a concern.  Recommendation  Replace the use of the chainId immutable value with the CHAINID opcode in _validateWithdrawSignature(). Note that CHAINID is only available using Solidity s inline assembly, so this would need to be accessed in the same way as it is currently accessed in the contract s constructor:  contracts/contracts/Umbra.sol:L68-L72  uint256 _chainId;  assembly {  _chainId := chainid()  5 Recommendations  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/umbra-smart-contracts/"
    },
    {
        "title": "5.1 Use separate mappings for keys in StealthKeyResolver",
        "body": "  Description  The StealthKeyResolver currently stores keys in a mapping bytes32 => uint256 => uint256 that maps nodes => prefixes => keys. The prefixes are offset in the setStealthKeys() function to differentiate between viewing public keys and spending public keys, and these offsets are reversed in the stealthKeys() view function.  contracts/profiles/StealthKeyResolver.sol:L37-L56  function setStealthKeys(bytes32 node, uint256 spendingPubKeyPrefix, uint256 spendingPubKey, uint256 viewingPubKeyPrefix, uint256 viewingPubKey) external authorised(node) {  require(  (spendingPubKeyPrefix == 2 || spendingPubKeyPrefix == 3) &&  (viewingPubKeyPrefix == 2 || viewingPubKeyPrefix == 3),  \"StealthKeyResolver: Invalid Prefix\"  );  emit StealthKeyChanged(node, spendingPubKeyPrefix, spendingPubKey, viewingPubKeyPrefix, viewingPubKey);  // Shift the spending key prefix down by 2, making it the appropriate index of 0 or 1  spendingPubKeyPrefix -= 2;  // Ensure the opposite prefix indices are empty  delete _stealthKeys[node][1 - spendingPubKeyPrefix];  delete _stealthKeys[node][5 - viewingPubKeyPrefix];  // Set the appropriate indices to the new key values  _stealthKeys[node][spendingPubKeyPrefix] = spendingPubKey;  _stealthKeys[node][viewingPubKeyPrefix] = viewingPubKey;  This manual adjustment of prefixes adds complexity to an otherwise simple function. To avoid this, consider splitting this into two separate mappings   one for viewing keys and one for spending keys. For clarity, also specify the visibility of these mappings explicitly.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/umbra-smart-contracts/"
    },
    {
        "title": "5.2 Document potential edge cases for hook receiver contracts",
        "body": "  Description  The functions withdrawTokenAndCall() and withdrawTokenAndCallOnBehalf() make a call to a hook contract designated by the owner of the withdrawing stealth address.  contracts/contracts/Umbra.sol:L289-L291  if (address(_hook) != address(0)) {  _hook.tokensWithdrawn(_withdrawalAmount, _stealthAddr, _acceptor, _tokenAddr, _sponsor, _sponsorFee, _data);  There are very few constraints on the parameters to these calls in the Umbra contract itself. Anyone can force a call to a hook contract by transferring a small amount of tokens to an address that they control and withdrawing these tokens, passing the target address as the hook receiver. Developers of these UmbraHookReceiver contracts should be sure to validate both the caller of the tokensWithdrawn() function and the function parameters. There are a number of possible edge cases that should be handled when relevant. These include, but are not limited to, the following:  The _amount may not have been transferred to the hook receiver itself.  All four addresses passed to tokensWithdrawn() could be the same. Most of these address parameters could also be any arbitrary address. This includes the token contract address, the address of the hook receiver, or the address of the Umbra contract itself.  The token received may be valueless.  The token received may be malicious. The only requirements are that the token contract address contains code and accepts calls to the ERC20 methods transfer() and transferFrom().  While it is difficult to determine a feasible exploit without knowledge of what hook receiver contracts may do in the future, a slightly contrived example follows.  Suppose a user builds a hook receiver contract that accepts an arbitrary token, TOK, and immediately provides liquidity to the ETH-TOK Uniswap pair when tokensWithdrawn() is called by the Umbra contract. An attacker could create a malicious token that can not be transferred out of its own Uniswap Pair contract and force a call to the hook receiver contract from Umbra. The hook receiver would be able to provide liquidity to the pool but would be unable to remove it, losing any ETH that was provided.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/umbra-smart-contracts/"
    },
    {
        "title": "5.3 Document token behavior restrictions",
        "body": "  As with any protocol that interacts with arbitrary ERC20 tokens, it is important to clearly document which tokens are supported. Often this is best done by providing a specification for the behavior of the expected ERC20 tokens and only relaxing this specification after careful review of a particular class of tokens and their interactions with the protocol.  In the absence of this, the following is a necessarily incomplete list of some known deviations from  normal  ERC20 behavior that should be explicitly noted as NOT supported by the Umbra Protocol:  Deflationary or fee-on-transfer tokens: These are tokens in which the balance of the recipient of a transfer may not be increased by the amount of the transfer. There may also be some alternative mechanism by which balances are unexpectedly decreased. While these tokens can be successfully sent via the sendToken() function, the internal accounting of the Umbra contract will be out of sync with the balance as recorded in the token contract, resulting in loss of funds.  Inflationary tokens: The opposite of deflationary tokens. The Umbra contract provides no mechanism for claiming positive balance adjustments.  Rebasing tokens: A combination of the above cases, these are tokens in which an account s balance increases or decreases along with expansions or contractions in supply. The contract provides no mechanism to update its internal accounting in response to these unexpected balance adjustments, and funds may be lost as a result.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/umbra-smart-contracts/"
    },
    {
        "title": "5.4 Add an address parameter to withdrawal signatures   ",
        "body": "  Resolution   This is addressed in   ScopeLift/umbra-protocol@d6e4235, which replaces the  Description  As discussed above, the _validateWithdrawSignature() function checks the signer of a digest consisting of the keccak-256 hash of the following preimage:  abi.encodePacked(  \"\\x19Ethereum Signed Message:\\n32\",  keccak256(abi.encode(chainId, version, _acceptor, _tokenAddr, _sponsor, _sponsorFee, address(_hook), _data))  Consider adding the address of the contract itself to this signed message. Currently, it is possible to deploy any number of contracts with the same version to the same chain, and signatures would be replayable across all of these contracts. While users are likely to only have balances for the same stealth address in one of these contracts, adding an address parameter provides some additional replay protection. Because the contract can not be self-destructed, a given address can only ever contain a single version of the Umbra contract.  ",
        "labels": [
            "Consensys",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/03/umbra-smart-contracts/"
    },
    {
        "title": "5.1 Reactivated gauges can t queue up rewards    ",
        "body": "  Resolution   Fixed in   fei-protocol/flywheel-v2@e765d24 by making it so all gauges are always included in cycles, thus keeping in sync their  Description  Active gauges as set in ERC20Gauges.addGauge() function by authorised users get their rewards queued up in the FlywheelGaugeRewards._queueRewards() function. As part of it, their associated struct QueuedRewards updates its storedCycle value to the cycle in which they get queued up:  code-flywheel-v2/src/rewards/FlywheelGaugeRewards.sol:L202-L206  gaugeQueuedRewards[gauge] = QueuedRewards({  priorCycleRewards: queuedRewards.priorCycleRewards + completedRewards,  cycleRewards: uint112(nextRewards),  storedCycle: currentCycle  });  Once reactivated later with at least 1 full cycle being done without it, it will produce issues. It will now be returned by gaugeToken.gauges() to be processed in either FlywheelGaugeRewards.queueRewardsForCycle()or FlywheelGaugeRewards.queueRewardsForCyclePaginated(), but, once the reactivated gauge is passed to _queueRewards(), it will fail an assert:  code-flywheel-v2/src/rewards/FlywheelGaugeRewards.sol:L196  assert(queuedRewards.storedCycle == 0 || queuedRewards.storedCycle >= lastCycle);  This is because it already has a set value from the cycle it was processed in previously (i.e. storedCycle>0), and, since that cycle is at least 1 full cycle behind the state contract, it will also not pass the second condition queuedRewards.storedCycle >= lastCycle.  The result is that this gauge is locked out of queuing up for rewards because queuedRewards.storedCycle is only synchronised with the contract s cycle later in _queueRewards() which will now always fail for this gauge.  Recommendation  Account for the reactivated gauges that previously went through the rewards queue process, such as introducing a separate flow for newly activated gauges. However, any changes such as removing the above mentioned assert() should be carefully validated for other downstream logic that may use the QueuedRewards.storedCycle value. Therefore, it is recommended to review the state transitions as opposed to only passing this specific check.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/04/tribe-dao-flywheel-v2-xtribe-xerc4626/"
    },
    {
        "title": "5.2 Reactivated gauges have incorrect accounting for the last cycle s rewards    ",
        "body": "  Resolution   Fixed in   fei-protocol/flywheel-v2@e765d24 by making it so all gauges are always included in cycles, thus keeping in sync their  Description  As described in issue 5.1, reactivated gauges that previously had queued up rewards have a mismatch between their storedCycle and contract s gaugeCycle state variable.  Due to this mismatch, there is also a resulting issue with the accounting logic for its completed rewards:  code-flywheel-v2/src/rewards/FlywheelGaugeRewards.sol:L198  uint112 completedRewards = queuedRewards.storedCycle == lastCycle ? queuedRewards.cycleRewards : 0;  Consequently, this then produces an incorrect value for QueuedRewards.priorCycleRewards:  code-flywheel-v2/src/rewards/FlywheelGaugeRewards.sol:L203  priorCycleRewards: queuedRewards.priorCycleRewards + completedRewards,  As now completedRewards will be equal to 0 instead of the previous cycle s rewards for that gauge. This may cause a loss of rewards accounted for this gauge as this value is later used in getAccruedRewards().  Recommendation  Consider changing the logic of the check so that storedCycle values further in the past than lastCycle may produce the right rewards return for this expression, such as using <= instead of == and adding an explicit check for storedCycle == 0 to account for the initial scenario.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/04/tribe-dao-flywheel-v2-xtribe-xerc4626/"
    },
    {
        "title": "5.3 Lack of input validation in delegateBySig    ",
        "body": "  Resolution   Fixed in   fei-protocol/flywheel-v2@e765d24 by reverting for  Description  ERC20MultiVotes.sol makes use of ecrecover() in delegateBySig to return the address of the message signer. ecrecover() typically returns address(0x0) to indicate an error; however, there s no zero address check in the function logic. This might not be exploitable though, as delegate(0x0, arbitraryAddress) might always return zero votes (in freeVotes). Additionally, ecrecover() can be forced to return a random address by messing with the parameters. Although this is extremely rare and will likely resolve to zero free votes most times, this might return a random address and delegate someone else s votes.  Examples  code-flywheel-v2/src/token/ERC20MultiVotes.sol:L364-L387  function delegateBySig(  address delegatee,  uint256 nonce,  uint256 expiry,  uint8 v,  bytes32 r,  bytes32 s  ) public {  require(block.timestamp <= expiry, \"ERC20MultiVotes: signature expired\");  address signer = ecrecover(  keccak256(  abi.encodePacked(  \"\\x19\\x01\",  DOMAIN_SEPARATOR(),  keccak256(abi.encode(DELEGATION_TYPEHASH, delegatee, nonce, expiry))  ),  v,  r,  );  require(nonce == nonces[signer]++, \"ERC20MultiVotes: invalid nonce\");  _delegate(signer, delegatee);  Recommendation  Introduce a zero address check i.e require signer!=address(0) and check if the recovered signer is an expected address. Refer to ERC20 s permit for inspiration.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/04/tribe-dao-flywheel-v2-xtribe-xerc4626/"
    },
    {
        "title": "5.4 Decreasing maxGauges does not account for users  previous gauge list size.    ",
        "body": "  Resolution   Fixed in   fei-protocol/flywheel-v2@e765d24 by documenting.  Description  ERC20Gauges contract has a maxGauges state variable meant to represent the maximum amount of gauges a user can allocate to. As per the natspec, it is meant to protect against gas DOS attacks upon token transfer to allow complicated transactions to fit in a block. There is also a function setMaxGauges for authorised users to decrease or increase this state variable.  code-flywheel-v2/src/token/ERC20Gauges.sol:L499-L504  function setMaxGauges(uint256 newMax) external requiresAuth {  uint256 oldMax = maxGauges;  maxGauges = newMax;  emit MaxGaugesUpdate(oldMax, newMax);  Recommendation  Either document the potential discrepancy between the user gauges size and the maxGauges state variable, or limit maxGauges to be only called within the contract thereby forcing other contracts to retrieve user gauge list size through numUserGauges().  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/04/tribe-dao-flywheel-v2-xtribe-xerc4626/"
    },
    {
        "title": "5.5 Decrementing a gauge by 0 that is not in the user gauge list will fail an assert.    ",
        "body": "  Resolution   Fixed in   fei-protocol/flywheel-v2@e765d24 by implementing auditor s recommendation.  Description  ERC20Gauges._decrementGaugeWeight has an edge case scenario where a user can attempt to decrement a gauge that is not in the user gauge list by 0 weight, which would trigger a failure in an assert.  code-flywheel-v2/src/token/ERC20Gauges.sol:L333-L345  function _decrementGaugeWeight(  address user,  address gauge,  uint112 weight,  uint32 cycle  ) internal {  uint112 oldWeight = getUserGaugeWeight[user][gauge];  getUserGaugeWeight[user][gauge] = oldWeight - weight;  if (oldWeight == weight) {  // If removing all weight, remove gauge from user list.  assert(_userGauges[user].remove(gauge));  code-flywheel-v2/src/token/ERC20Gauges.sol:L339-L341  uint112 oldWeight = getUserGaugeWeight[user][gauge];  getUserGaugeWeight[user][gauge] = oldWeight - weight;  However, passing a weight=0 parameter with a gauge that doesn t belong to the user, would successfully process that line. This would then be followed by an evaluation if (oldWeight == weight), which would also succeed since both are 0, to finally reach an assert that will verify a remove of that gauge from the user gauge list. However, it will fail since it was never there in the first place.  code-flywheel-v2/src/token/ERC20Gauges.sol:L344  assert(_userGauges[user].remove(gauge));  Although an edge case with no effect on contract state s health, it may happen with front end bugs or incorrect user transactions, and it is best not to have asserts fail.  Recommendation  Replace assert() with a require() or verify that the gauge belongs to the user prior to performing any operations.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/04/tribe-dao-flywheel-v2-xtribe-xerc4626/"
    },
    {
        "title": "5.6 Undelegating 0 votes from an address who is not a delegate of a user will fail an assert.    ",
        "body": "  Resolution   Fixed in   fei-protocol/flywheel-v2@e765d24 by implementing auditor s recommendation.  Description  Similar scenario with issue 5.5. ERC20MultiVotes._undelegate has an edge case scenario where a user can attempt to undelegate from a delegatee that is not in the user delegates list by 0 amount, which would trigger a failure in an assert.  code-flywheel-v2/src/token/ERC20MultiVotes.sol:L251-L260  function _undelegate(  address delegator,  address delegatee,  uint256 amount  ) internal virtual {  uint256 newDelegates = _delegatesVotesCount[delegator][delegatee] - amount;  if (newDelegates == 0) {  assert(_delegates[delegator].remove(delegatee)); // Should never fail.  code-flywheel-v2/src/token/ERC20MultiVotes.sol:L256  uint256 newDelegates = _delegatesVotesCount[delegator][delegatee] - amount;  However, passing a amount=0 parameter with a delegatee that doesn t belong to the user, would successfully process that line. This would then be followed by an evaluation if (newDelegates == 0), which would succeed, to finally reach an assert that will verify a remove of that delegatee from the user delegates list. However, it will fail since it was never there in the first place.  code-flywheel-v2/src/token/ERC20MultiVotes.sol:L259  assert(_delegates[delegator].remove(delegatee)); // Should never fail.  Although an edge case with no effect on contract state s health, it may happen with front end bugs or incorrect user transactions, and it is best not to have asserts fail, as per the dev comment in that line  // Should never fail .  Recommendation  Replace assert() with a require() or verify that the delegatee belongs to the user prior to performing any operations.  6 Findings: xTRIBE  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/04/tribe-dao-flywheel-v2-xtribe-xerc4626/"
    },
    {
        "title": "6.1 xTRIBE.emitVotingBalances - DelegateVotesChanged event can be emitted by anyone    ",
        "body": "  Resolution   Fixed in   fei-protocol/xTRIBE@ea9705b by adding authentication.  Description  xTRIBE.emitVotingBalances is an external function without authentication constraints. It means anyone can call it and emit DelegateVotesChanged which may impact other layers of code that rely on these events.  Examples  code-xTRIBE/src/xTRIBE.sol:L89-L99  function emitVotingBalances(address[] calldata accounts) external {  uint256 size = accounts.length;  for (uint256 i = 0; i < size; ) {  emit DelegateVotesChanged(accounts[i], 0, getVotes(accounts[i]));  unchecked {  i++;  Recommendation  Consider restricting access to this function for allowed accounts only.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/04/tribe-dao-flywheel-v2-xtribe-xerc4626/"
    },
    {
        "title": "6.1 IdleCDO._deposit() allows re-entrancy from hookable tokens. ",
        "body": "  Resolution   The development team has addressed this concern in commit   5fbdc0506c94a172abbd4122276ed2bd489d1964. This change has not been reviewed by the audit team.  Description  The function IdleCDO._deposit() updates the system s internal accounting and mints shares to the caller, then transfers the deposited funds from the user. Some token standards, such as ERC777, allow a callback to the source of the funds before the balances are updated in transferFrom(). This callback could be used to re-enter the protocol while already holding the minted tranche tokens and at a point where the system accounting reflects a receipt of funds that has not yet occurred.  While an attacker could not interact with IdleCDO.withdraw() within this callback because of the _checkSameTx() restriction, they would be able to interact with the rest of the protocol.  code/contracts/IdleCDO.sol:L230-L245  function _deposit(uint256 _amount, address _tranche) internal returns (uint256 _minted) {  // check that we are not depositing more than the contract available limit  _guarded(_amount);  // set _lastCallerBlock hash  _updateCallerBlock();  // check if strategyPrice decreased  _checkDefault();  // interest accrued since last depositXX/withdrawXX/harvest is splitted between AA and BB  // according to trancheAPRSplitRatio. NAVs of AA and BB are updated and tranche  // prices adjusted accordingly  _updateAccounting();  // mint tranche tokens according to the current tranche price  _minted = _mintShares(_amount, msg.sender, _tranche);  // get underlyings from sender  IERC20Detailed(token).safeTransferFrom(msg.sender, address(this), _amount);  Recommendation  Move the transferFrom() action in _deposit() to immediately after _updateCallerBlock().  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/06/idle-finance/"
    },
    {
        "title": "6.2 IdleCDO.virtualPrice() and _updatePrices() yield different prices in a number of cases ",
        "body": "  Resolution  The development team implemented a new version of both functions using a third method, virtualPricesAux(), to perform the primary price calculation. Additionally, _updatePrices() was renamed to _updateAccounting().  This change was incorporated in commit ff0b69380828657f16df8683c35703b325a6b656.  Description  The function IdleCDO.virtualPrice() is used to determine the current price of a tranche. Similarly, IdleCDO._updatePrices() is used to store the latest price of a tranche, as well as update other parts of the system accounting. There are a number of cases where the prices yielded by these two functions differ. While these are primarily corner cases that are not obviously exploitable in practice, potential violations of key accounting invariants should always be considered serious.  Additionally, the use of two separate implementations of the same calculation suggest the potential for more undiscovered discrepancies, possibly of higher consequence.  As an example, in _updatePrices() the precision loss from splitting the strategy returns favors BB tranche holders. In virtualPrice() both branches of the price calculation incur precision loss, favoring the IdleCDO contract itself.  _updatePrices()  code/contracts/IdleCDO.sol:L331-L341  if (BBTotSupply == 0) {  // if there are no BB holders, all gain to AA  AAGain = gain;  } else if (AATotSupply == 0) {  // if there are no AA holders, all gain to BB  BBGain = gain;  } else {  // split the gain between AA and BB holders according to trancheAPRSplitRatio  AAGain = gain * trancheAPRSplitRatio / FULL_ALLOC;  BBGain = gain - AAGain;  virtualPrice()  code/contracts/IdleCDO.sol:L237-L245  if (_tranche == AATranche) {  // calculate gain for AA tranche  // trancheGain (AAGain) = gain * trancheAPRSplitRatio / FULL_ALLOC;  trancheNAV = lastNAVAA + (gain * _trancheAPRSplitRatio / FULL_ALLOC);  } else {  // calculate gain for BB tranche  // trancheGain (BBGain) = gain * (FULL_ALLOC - trancheAPRSplitRatio) / FULL_ALLOC;  trancheNAV = lastNAVBB + (gain * (FULL_ALLOC - _trancheAPRSplitRatio) / FULL_ALLOC);  Recommendation  Implement a single method that determines the current price for a tranche, and use this same implementation anywhere the price is needed.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/06/idle-finance/"
    },
    {
        "title": "6.3 IdleCDO.harvest() allows price manipulation in certain circumstances ",
        "body": "  Resolution   The development team has addressed this concern in a pull request with a final commit hash of   5341a9391f9c42cadf26d72c9f804ca75a15f0fb. This change has not been reviewed by the audit team.  Description  The function IdleCDO.harvest() uses Uniswap to liquidate rewards earned by the contract s strategy, then updates the relevant positions and internal accounting. This function can only be called by the contract owner or the designated rebalancer address, and it accepts an array which indicates the minimum buy amounts for the liquidation of each reward token.  The purpose of permissioning this method and specifying minimum buy amounts is to prevent a sandwiching attack from manipulating the reserves of the Uniswap pools and forcing the IdleCDO contract to incur loss due to price slippage.  However, this does not effectively prevent price manipulation in all cases. Because the contract sells it s entire balance of redeemed rewards for the specified minimum buy amount, this approach does not enforce a minimum price for the executed trades. If the balance of IdleCDO or the amount of claimable rewards increases between the submission of the harvest() transaction and its execution, it may be possible to perform a profitable sandwiching attack while still satisfying the required minimum buy amounts.  The viability of this exploit depends on how effectively an attacker can increase the amount of rewards tokens to be sold without incurring an offsetting loss. The strategy contracts used by IdleCDO are expected to vary widely in their implementations, and this manipulation could potentially be done either through direct interaction with the protocol or as part of a flashbots bundle containing a large position adjustment from an honest user.  code/contracts/IdleCDO.sol:L564-L565  function harvest(bool _skipRedeem, bool _skipIncentivesUpdate, bool[] calldata _skipReward, uint256[] calldata _minAmount) external {  require(msg.sender == rebalancer || msg.sender == owner(), \"IDLE:!AUTH\");  code/contracts/IdleCDO.sol:L590-L599  // approve the uniswap router to spend our reward  IERC20Detailed(rewardToken).safeIncreaseAllowance(address(_uniRouter), _currentBalance);  // do the uniswap trade  _uniRouter.swapExactTokensForTokensSupportingFeeOnTransferTokens(  _currentBalance,  _minAmount[i],  _path,  address(this),  block.timestamp + 1  );  Recommendation  Update IdleCDO.harvest() to enforce a minimum price rather than a minimum buy amount. One method of doing so would be taking an additional array parameter indicating the amount of each token to sell in exchange for the respective buy amount.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/06/idle-finance/"
    },
    {
        "title": "6.4 Prevent zero amount transfers/minting ",
        "body": "  Resolution   The development team has addressed this concern in commit   a72747da8c0ca71274f3a1506c6faf724cf82dd2. This change has not been reviewed by the audit team.  Description  Many of the functions in the system can be called with amount = 0. This is not a security issue, however a  defense in depth  approach in this and similar cases may prevent an undiscovered bug from being exploitable. Most of the functionalities that were reviewed in this audit won t create an exploitable state transition in these cases, however they will trigger a 0 token transfer or minting.  Examples  depositAA()  depositBB()  stake()  unstake()  Recommendation  Check and return early (or revert) on requests with zero amount.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/06/idle-finance/"
    },
    {
        "title": "6.5 Missing Sanity checks ",
        "body": "  Resolution   The development team has addressed this concern in commit   a1d5dac0ad5f562d4c75bff99e770d92bcc2a72f. This change has not been reviewed by the audit team.  Description  The implementation of initialize() functions are missing some sanity checks. The proper checks are implemented in some of the setter functions but missing in some others.  Examples  Missing sanity check for != address(0)  code/contracts/IdleCDO.sol:L54-L57  token = _guardedToken;  strategy = _strategy;  strategyToken = IIdleCDOStrategy(_strategy).strategyToken();  rebalancer = _rebalancer;  code/contracts/IdleCDO.sol:L84-L84  guardian = _owner;  code/contracts/IdleCDO.sol:L672-L673  address _currAAStaking = AAStaking;  address _currBBStaking = BBStaking;  code/contracts/IdleCDOTrancheRewards.sol:L50-L53  idleCDO = _idleCDO;  tranche = _trancheToken;  rewards = _rewards;  governanceRecoveryFund = _governanceRecoveryFund;  Recommendation  Add sanity checks before assigning system variables.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/06/idle-finance/"
    },
    {
        "title": "6.6 IdleCDO.virtualPrice() & _updatePrices() too complicated to verify ",
        "body": "  Resolution  These methods were revisited in the continuation of the original review and more time was allotted to them than was possible previously. Some refactoring also occurred during that time (see 6.2). However, the development team elected to maintain the general approach used in these functions.  The primary challenge in verifying their correctness remains, which is their heavy reliance on external interactions with contracts whose expected semantics are poorly defined.  Description  IdleCDO.virtualPrice() and _updatePrices() functions are used for many important functionality in the Idle system. They also have nested external calls to many other contracts (e.g. IdleTokenGovernance, IdleCDOStrategy and strategy token, IdleCDOTranche on both Tranche tokens, etc). This level of complexity for a vital function is not recommended and is considered dangerous implementation.  Examples  Recommendation  Consider refactoring the code to use less complicated logic and code flow.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/06/idle-finance/"
    },
    {
        "title": "3.1 Owners can never be removed    ",
        "body": "  Resolution   This has been fixed in   paxosglobal/simple-multisig#5, and appropriate tests have been added.  Description  The intention of setOwners() is to replace the current set of owners with a new set of owners. However, the isOwner mapping is never updated, which means any address that was ever considered an owner is permanently considered an owner for purposes of signing transactions.  Recommendation  In setOwners_(), before adding new owners, loop through the current set of owners and clear their isOwner booleans, as in the following code:  for (uint256 i = 0; i < ownersArr.length; i++) {  isOwner[ownersArr[i]] = false;  ",
        "labels": [
            "Consensys",
            "Critical",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/paxos/"
    },
    {
        "title": "6.1 Swap fees can be bypassed using redeemMasset    Addressed",
        "body": "  Resolution   This issue was reported independently via the bug bounty program and was   fixed early during the audit. The fix has already been deployed on mainnet using the upgrade mechanism  Description  Part of the value proposition for liquidity providers is earning fees incurred for swapping between assets. However, traders can perform fee-less swaps by providing liquidity in one bAsset, followed by calling redeemMasset() to convert the resulting mAssets back into a proportional amount of bAssets. Since removing liquidity via redeemMasset() does not incur a fee this is equivalent to doing a swap with zero fees.  As a very simple example, assuming a pool with 2 bAssets (say, DAI and USDT), it would be possible to swap 10 DAI to USDT as follows:  Add 20 DAI to the pool, receive 20 mUSD  call redeemMasset() to redeem 10 DAI and 10 USDT  Examples  The boolean argument applyFee is set to false in _redeemMasset:  code/contracts/masset/Masset.sol:L569  _settleRedemption(_recipient, _mAssetQuantity, props.bAssets, bAssetQuantities, props.indexes, props.integrators, false);  Recommendation  Charge a small redemption fee in redeemMasset().  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"
    },
    {
        "title": "6.2 Users can collect interest from SavingsContract by only staking mTokens momentarily    Addressed",
        "body": "  Resolution  The blocker on collecting interest more than once in 30 minute period. A new APY bounds check has been added to verify that supply isn t inflated by more than 0.1% within a 30 minutes window.  Description  The SAVE contract allows users to deposit mAssets in return for lending yield and swap fees. When depositing mAsset, users receive a  credit  tokens at the momentary credit/mAsset exchange rate which is updated at every deposit. However, the smart contract enforces a minimum timeframe of 30 minutes in which the interest rate will not be updated. A user who deposits shortly before the end of the timeframe will receive credits at the stale interest rate and can immediately trigger and update of the rate and withdraw at the updated (more favorable) rate after the 30 minutes window. As a result, it would be possible for users to benefit from interest payouts by only staking mAssets momentarily and using them for other purposes the rest of the time.  Examples  code/contracts/savings/SavingsManager.sol:L141-L143  // 1. Only collect interest if it has been 30 mins  uint256 timeSinceLastCollection = now.sub(previousCollection);  if(timeSinceLastCollection > THIRTY_MINUTES) {  Recommendation  Remove the 30 minutes window such that every deposit also updates the exchange rate between credits and tokens. Note that this issue was reported independently during the bug bounty program and a fix is currently being worked on.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"
    },
    {
        "title": "6.3 Internal accounting of vault balance may diverge from actual token balance in lending pool   ",
        "body": "  Resolution  After discussion with the team the risk of this invariant violation was considered negligible as the gas cost increase for querying constantly querying the lending pool would outweigh the size of the accounting error of only 1 base unit.  Description  It is possible that the vault balance for a given bAsset is greater than the corresponding balance in the lending pool. This violates one of the correctness properties stated in the audit brief. Our Harvey fuzzer was able to generate a transaction that mints a small amount (0xf500) of mAsset. Due to the way that the lending pool integration (Compound in this case) updates the vault balance it ends up greater than the available balance in the lending pool.  More specifically, the integration contract assumes that the amount deposited into the pool is equal to the amount received by the mAsset contract for the case where no transaction fees are charged for token transfers:  code/contracts/masset/platform-integrations/CompoundIntegration.sol:L45-L58  quantityDeposited = _amount;  if(_isTokenFeeCharged) {  // If we charge a fee, account for it  uint256 prevBal = _checkBalance(cToken);  require(cToken.mint(_amount) == 0, \"cToken mint failed\");  uint256 newBal = _checkBalance(cToken);  quantityDeposited = _min(quantityDeposited, newBal.sub(prevBal));  } else {  // Else just execute the mint  require(cToken.mint(_amount) == 0, \"cToken mint failed\");  emit Deposit(_bAsset, address(cToken), quantityDeposited);  For illustration, consider the following scenario: assume your current balance in a lending pool is 0. When you deposit some amount X into the lending pool your balance after the deposit may be less than X (even if the underlying token does not charge transfer fees). One reason for this is rounding, but, in theory, a lending pool could also charge fees, etc.  The vault balance is updated in function Masset._mintTo based on the amount returned by the integration.  code/contracts/masset/Masset.sol:L189  basketManager.increaseVaultBalance(bInfo.index, integrator, quantityDeposited);  code/contracts/masset/Masset.sol:L274  uint256 deposited = IPlatformIntegration(_integrator).deposit(_bAsset, quantityTransferred, _erc20TransferFeeCharged);  This violation of the correctness property is temporary since the vault balance is readjusted when interest is collected. However, the time frame of ca. 30 minutes between interest collections (may be longer if no continuous interest is distributed) means that it may be violated for substantial periods of time.  code/contracts/masset/BasketManager.sol:L243-L249  uint256 balance = IPlatformIntegration(integrations[i]).checkBalance(b.addr);  uint256 oldVaultBalance = b.vaultBalance;  // accumulate interest (ratioed bAsset)  if(balance > oldVaultBalance && b.status == BassetStatus.Normal) {  // Update balance  basket.bassets[i].vaultBalance = balance;  The regular updates due to interest collection should ensure that the difference stays relatively small. However, note that the following scenarios is feasible: assuming there is 0 DAI in the basket, a user mints X mUSD by depositing X DAI. While the interest collection hasn t been triggered yet, the user tries to redeem X mUSD for DAI. This may fail since the amount of DAI in the lending pool is smaller than X.  Recommendation  It seems like this issue could be fixed by using the balance increase from the lending pool to update the vault balance (much like for the scenario where transfer fees are charged) instead of using the amount received.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"
    },
    {
        "title": "6.4 Missing validation in Masset._redeemTo   ",
        "body": "  Resolution  An explicit check will be added with the next Masset proxy upgrade.  Description  In function _redeemTo the collateralisation ratio is not taken into account unlike in _redeemMasset:  code/contracts/masset/Masset.sol:L558-L561  uint256 colRatio = StableMath.min(props.colRatio, StableMath.getFullScale());  // Ensure payout is related to the collateralised mAsset quantity  uint256 collateralisedMassetQuantity = _mAssetQuantity.mulTruncate(colRatio);  It seems like _redeemTo should not be executed if the collateralisation ratio is below 100%. However, the contracts (that is, Masset and ForgeValidator) themselves don t seem to enforce this explicitly. Instead, the governor needs to ensure that the collateralisation ratio is only set to a value below 100% when the basket is not  healthy  (for instance, if it is considered  failed ). Failing to ensure this may allow an attacker to redeem a disproportionate amount of assets. Note that the functionality for setting the collateralisation ratio is not currently implemented in the audited code.  Recommendation  Consider enforcing the intended use of _redeemTo more explicitly. For instance, it might be possible to introduce additional input validation by requiring that the collateralisation ratio is not below 100%.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"
    },
    {
        "title": "6.5 Removing a bAsset might leave some tokens stuck in the vault   ",
        "body": "  Resolution  The issue was acknowledged and downgraded to  minor  risk as only very small token amounts can be affected. A fix will be triaged for a future update.  Description  In function _removeBasset there is existing validation to make sure only  empty  vaults are removed:  code/contracts/masset/BasketManager.sol:L464  require(bAsset.vaultBalance == 0, \"bAsset vault must be empty\");  However, this is not necessarily sufficient since the lending pool balance may be higher than the vault balance. The reason is that the vault balance is usually slightly out-of-date due to the 30 minutes time span between interest collections. Consider the scenario: (1) a user swaps out an asset 29 minutes after the last interest collection to reduce its vault balance from 100 USD to 0, and (2) the governor subsequently remove the asset. During those 29 minutes the asset was collecting interest (according to the lending pool the balance was higher than 100 USD at the time of the swap) that is now  stuck  in the vault.  Recommendation  Consider adding additional input validation (for instance, by requiring that the lending pool balance to be 0) or triggering a swap directly when removing an asset from the basket.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"
    },
    {
        "title": "6.6 Unused parameter in BasketManager._addBasset   ",
        "body": "  Resolution  While the parameter is not currently used it will be used in future mAssets such as mGOLD.  Description  It seems like the _measurementMultiple parameter is always StableMath.getRatioScale() (1e8). There is also some range validation code that seems unnecessary if the parameter is always 1e8.  code/contracts/masset/BasketManager.sol:L310  require(_measurementMultiple >= 1e6 && _measurementMultiple <= 1e10, \"MM out of range\");  Recommendation  Consider removing the parameter and the input validation to improve the readability of the code.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"
    },
    {
        "title": "6.7 Unused event BasketStatusChanged   ",
        "body": "  Resolution  This event will be used in future releases.  Description  It seems like the event BasketManager.BasketStatusChanged event is unused.  Recommendation  Consider removing the event declaration to improve the readability of the code.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"
    },
    {
        "title": "6.8 Assumptions are made about interest distribution   ",
        "body": "  Description  There is a mechanism that prevents interest collection if the extrapolated APY exceeds a threshold (MAX_APY).  code/contracts/savings/SavingsManager.sol:L174  require(extrapolatedAPY < MAX_APY, \"Interest protected from inflating past maxAPY\");  The extrapolation seems to assume that the interest is payed out frequently and continuously. It seems like a less frequent payout (for instance, once a month/year) could be rejected since the extrapolation considers the interest since the last time that collectAndDistributeInterest was called (potentially without interest being collected).  Recommendation  Consider revisiting or documenting this assumption. For instance, one could consider extrapolating between the current time and the last time that (non-zero) interest was actually collected.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"
    },
    {
        "title": "6.9 Assumptions are made about Aave and Compound integrations   ",
        "body": "  Resolution  it was acknowledged that unexpected changes in behaviour by the integrated lending pools could potentially cause issues; However, it was decided that the risk is minor since the current lending pool behaviour is known and the fact that lending pools might introduce severe changes is accounted for by keeping the integrations separate and upgradable such that governance can react these changes in time.  Description  The code makes several assumptions about the Aave and Compound integrations. A malicious or malfunctioning integration (or lending pool) might violate those assumptions. This might lead to unintended behavior in the system. Below are three such assumptions:  function checkBalance reverts if the token hasn t been added:  code/contracts/masset/BasketManager.sol:L317  IPlatformIntegration(_integration).checkBalance(_bAsset);  function withdraw is trusted to not fail when it shouldn t:  code/contracts/masset/Masset.sol:L611  IPlatformIntegration(_integrators[i]).withdraw(_recipient, bAsset, q, _bAssets[i].isTransferFeeCharged);  the mapping from mAssets to pTokens is fixed:  code/contracts/masset/platform-integrations/InitializableAbstractIntegration.sol:L119  require(bAssetToPToken[_bAsset] == address(0), \"pToken already set\");  The first assumption could be avoided by adding a designated function to check if the token was added.  The second assumption is more difficult to avoid, but should be considered when adding new integrations. The system needs to trust the lending pools to work properly; for instance, if the lending pool would blacklist the integration contract the system may behave in unintended ways.  The third assumption could be avoided, but it comes at a cost.  Recommendation  Consider revisiting or avoiding these assumptions. For any assumptions that are there by design it would be good to document them to facilitate future changes. One should also be careful to avoid coupling between external systems. For instance, if withdrawing from Aave fails this should not prevent withdrawing from Compound.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"
    },
    {
        "title": "6.10 Assumptions are made about bAssets   ",
        "body": "  Description  The code makes several assumptions about the bAssets that can be used. A malicious or malfunctioning asset contract might violate those assumptions. This might lead to unintended behavior in the system. Below there are several such assumptions:  Decimals of a bAsset are constant where the decimals are used to derive the asset s ratio:  code/contracts/masset/BasketManager.sol:L319  uint256 bAsset_decimals = CommonHelpers.getDecimals(_bAsset);  Decimals must be in a range from 4 to 18:  code/contracts/shared/CommonHelpers.sol:L23  require(decimals >= 4 && decimals <= 18, \"Token must have sufficient decimal places\");  The governor is able to foresee when transfer fees are charged (which needs to be called if anything changes); in theory, assets could be much more flexible in when transfer fees are charged (for instance, during certain periods or for certain users)  code/contracts/masset/BasketManager.sol:L425  function setTransferFeesFlag(address _bAsset, bool _flag)  It seems like some of these assumptions could be avoided, but there might be a cost. For instance, one could retrieve the decimals directly instead of  caching  them and one could always enable the setting where transfer fees may be charged.  Recommendation  Consider revisiting or avoiding these assumptions. For any assumptions that are there by design it would be good to document them to facilitate future changes.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"
    },
    {
        "title": "6.11 Unused field in ForgePropsMulti struct   ",
        "body": "  Resolution  The field is currently used but will be used in a future version.  Description  The ForgePropsMulti struct defines the field isValid which always seems to be true:  code/contracts/masset/shared/MassetStructs.sol:L78-L84  /** @dev All details needed to Forge with multiple bAssets */  struct ForgePropsMulti {  bool isValid; // Flag to signify that forge bAssets have passed validity check  Basset[] bAssets;  address[] integrators;  uint8[] indexes;  If it is indeed always true, one could remove the following line:  code/contracts/masset/Masset.sol:L518  if(!props.isValid) return 0;  Recommendation  If the field is indeed always true please consider removing it to simplify the code.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"
    },
    {
        "title": "6.12 BassetStatus enum defines multiple unused states   ",
        "body": "  Resolution  The states will potentially be used in future releases.  Description  The BassetStatus enum defines several values that do not seem to be assigned in the code:  Default (different from  Normal ?)  Blacklisted  Liquidating  Liquidated  Failed  code/contracts/masset/shared/MassetStructs.sol:L59-L69  /** @dev Status of the Basset - has it broken its peg? */  enum BassetStatus {  Default,  Normal,  BrokenBelowPeg,  BrokenAbovePeg,  Blacklisted,  Liquidating,  Liquidated,  Failed  Since some of these are used in the code there might be some dead code that can be removed as a result. For example:  code/contracts/masset/forge-validator/ForgeValidator.sol:L46-L47  _bAsset.status == BassetStatus.Liquidating ||  _bAsset.status == BassetStatus.Blacklisted  Recommendation  If those values are indeed never used please consider removing them to simplify the code.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"
    },
    {
        "title": "6.13 Potential gas savings by terminating early   ",
        "body": "  Resolution  acknowledged that gas savings are possible, might be moved changed in a future version.  Description  If a function invocation is bound to revert, one should try to revert as soon as possible to save gas. In ForgeValidator.validateRedemption it is possible to terminate more early:  code/contracts/masset/forge-validator/ForgeValidator.sol:L264  if(atLeastOneBecameOverweight) return (false, \"bAssets must remain below max weight\", false);  Recommendation  Consider moving the require-statement a few lines up (for instance, after assigning to atLeastOneBecameOverweight).  ",
        "labels": [
            "Consensys",
            "Minor",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"
    },
    {
        "title": "6.14 Discrepancy between code and comments    Addressed",
        "body": "  Resolution  The comments have been updated.  Description  There is a discrepancy between the code at:  code/contracts/masset/BasketManager.sol:L417  require(weightSum >= 1e18 && weightSum <= 4e18, \"Basket weight must be >= 100 && <= 400%\");  And the comment at:  code/contracts/masset/BasketManager.sol:L409  Recommendation  @dev Throws if the total Basket weight does not sum to 100  Recommendation  Update the code or the comment to be consistent.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"
    },
    {
        "title": "6.15 Outdated Solidity version   ",
        "body": "  Resolution   the issue was deemed acceptable because an update to solc 0.5.17   would not fix any relevant security bugs.  Description  The codebase is using an outdated version of the Solidity compiler.  Recommendation  Please consider using an up-to-date version (ideally 0.6.12 or at least 0.5.17).  ",
        "labels": [
            "Consensys",
            "Minor",
            "Won't Fix"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"
    },
    {
        "title": "5.1 zAuction - incomplete / dead code zWithdraw and zDeposit    ",
        "body": "  Resolution   obsolete with changes from   zer0-os/zAuction@135b2aa removing the  Description  Examples  zAuction/contracts/zAuctionAccountant.sol:L44-L52  function zDeposit(address to) external payable onlyZauction {  ethbalance[to] = SafeMath.add(ethbalance[to], msg.value);  emit zDeposited(to, msg.value);  function zWithdraw(address from, uint256 amount) external onlyZauction {  ethbalance[from] = SafeMath.sub(ethbalance[from], amount);  emit zWithdrew(from, amount);  Recommendation  The methods do not seem to be used by the zAuction contract. It is highly discouraged from shipping incomplete implementations in productive code. Remove dead/unreachable code. Fix the implementations to perform proper accounting before reintroducing them if they are called by zAuction.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zauction/"
    },
    {
        "title": "5.2 zAuction - Unpredictable behavior for users due to admin front running or general bad timing    ",
        "body": "  Resolution  obsolete with changes from zer0-os/zAuction@135b2aa removing the zAccountAccountant. The client provided the following remark:  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zauction/"
    },
    {
        "title": "5.20 accountant deprecated",
        "body": "  Description  An administrator of zAuctionAccountant contract can update the zAuction contract without warning. This has the potential to violate a security goal of the system.  Specifically, privileged roles could use front running to make malicious changes just ahead of incoming transactions, or purely accidental negative effects could occur due to the unfortunate timing of changes.  In general users of the system should have assurances about the behavior of the action they re about to take.  Examples  updating the zAuction takes effect immediately. This has the potential to fail acceptance of bids by sellers on the now outdated zAuction contract as interaction with the accountant contract is now rejected. This forces bidders to reissue their bids in order for the seller to be able to accept them using the Accountant contract. This may also be used by admins to selectively censor the acceptance of accountant based bids by changing the active zAuction address.  zAuction/contracts/zAuctionAccountant.sol:L60-L68  function SetZauction(address zauctionaddress) external onlyAdmin{  zauction = zauctionaddress;  emit ZauctionSet(zauctionaddress);  function SetAdmin(address newadmin) external onlyAdmin{  admin = newadmin;  emit AdminSet(msg.sender, newadmin);  Upgradeable contracts may introduce the same unpredictability issues where the proxyUpgradeable owner may divert execution to a new zNS registrar implementation selectively for certain transactions or without prior notice to users.  Recommendation  The underlying issue is that users of the system can t be sure what the behavior of a function call will be, and this is because the behavior can change at any time.  We recommend giving the user advance notice of changes with a time lock. For example, make all system-parameter and upgrades require two steps with a mandatory time window between them. The first step merely broadcasts to users that a particular change is coming, and the second step commits that change after a suitable waiting period. This allows users that do not accept the change to withdraw immediately.  Validate arguments before updating contract addresses (at least != current/0x0). Consider implementing a 2-step admin ownership transfer (transfer+accept) to avoid losing control of the contract by providing the wrong ETH address.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zauction/"
    },
    {
        "title": "5.3 zAuction, zNS - Bids cannot be cancelled, never expire, and the auction lifecycle is unclear    ",
        "body": "  Resolution  Addressed with zer0-os/zNS@ab7d62a by refactoring the StakingController to control the lifecycle of bids instead of handling this off-chain.  Addressed with zer0-os/zAuction@135b2aa for zAuction by adding a bid/saleOffer expiration for bids. The client also provided the following statement:  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zauction/"
    },
    {
        "title": "5.6 added expireblock and startblock to zauction, expireblock to zsale",
        "body": " Decided not to add a cancel function. Paying gas to cancel isn t ideal, and it can be used as a griefing function. though that s still possible to do by moving weth but differently  The stateless nature of auctions may make it hard to enforce bid/sale expirations and it is not possible to cancel a bid/offer that should not be valid anymore. The expiration reduces the risk of old offers being used as they now automatically invalidate after time, however, it is still likely that multiple valid offers may be present at the same time. As outlined in the recommendation, one option would be to allow someone who signed a commitment to explicitly cancel it in the contract. Another option would be to create a stateful auction where the entity that puts up something for  starts  an auction, creating an auction id, requiring bidders to bid on that auction id. Once a bid is accepted the auction id is invalidated which invalidates all bids that might be floating around.  zer0-os/zAuction@2f92aa1 for  Description  The lifecycle of a bid both for zAuction and zNS is not clear, and has many flaws.  zAuction - Consider the case where a bid is placed, then the underlying asset in being transferred to a new owner. The new owner can now force to sell the asset even though it s might not be relevant anymore.  zAuction - Once a bid was accepted and the asset was transferred, all other bids need to be invalidated automatically, otherwise and old bid might be accepted even after the formal auction is over.  zAuction, zNS - There is no way for the bidder to cancel an old bid. That might be useful in the event of a significant change in market trend, where the old pricing is no longer relevant. Currently, in order to cancel a bid, the bidder can either withdraw his ether balance from the zAuctionAccountant, or disapprove WETH which requires an extra transaction that might be front-runned by the seller.  Examples  zAuction/contracts/zAuction.sol:L35-L45  function acceptBid(bytes memory signature, uint256 rand, address bidder, uint256 bid, address nftaddress, uint256 tokenid) external {  address recoveredbidder = recover(toEthSignedMessageHash(keccak256(abi.encode(rand, address(this), block.chainid, bid, nftaddress, tokenid))), signature);  require(bidder == recoveredbidder, 'zAuction: incorrect bidder');  require(!randUsed[rand], 'Random nonce already used');  randUsed[rand] = true;  IERC721 nftcontract = IERC721(nftaddress);  accountant.Exchange(bidder, msg.sender, bid);  nftcontract.transferFrom(msg.sender, bidder, tokenid);  emit BidAccepted(bidder, msg.sender, bid, nftaddress, tokenid);  zNS/contracts/StakingController.sol:L120-L152  function fulfillDomainBid(  uint256 parentId,  uint256 bidAmount,  uint256 royaltyAmount,  string memory bidIPFSHash,  string memory name,  string memory metadata,  bytes memory signature,  bool lockOnCreation,  address recipient  ) external {  bytes32 recoveredBidHash = createBid(parentId, bidAmount, bidIPFSHash, name);  address recoveredBidder = recover(recoveredBidHash, signature);  require(recipient == recoveredBidder, \"ZNS: bid info doesnt match/exist\");  bytes32 hashOfSig = keccak256(abi.encode(signature));  require(approvedBids[hashOfSig] == true, \"ZNS: has been fullfilled\");  infinity.safeTransferFrom(recoveredBidder, controller, bidAmount);  uint256 id = registrar.registerDomain(parentId, name, controller, recoveredBidder);  registrar.setDomainMetadataUri(id, metadata);  registrar.setDomainRoyaltyAmount(id, royaltyAmount);  registrar.transferFrom(controller, recoveredBidder, id);  if (lockOnCreation) {  registrar.lockDomainMetadataForOwner(id);  approvedBids[hashOfSig] = false;  emit DomainBidFulfilled(  metadata,  name,  recoveredBidder,  id,  parentId  );  Recommendation  Consider adding an expiration field to the message signed by the bidder both for zAuction and zNS. Consider adding auction control, creating an auctionId, and have users bid on specific auctions. By adding this id to the signed message, all other bids are invalidated automatically and users would have to place new bids for a new auction. Optionally allow users to cancel bids explicitly.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zauction/"
    },
    {
        "title": "5.4 zAuction - pot. initialization fronrunning and unnecessary init function    ",
        "body": "  Resolution  Addressed with zer0-os/zAuction@135b2aa and the following statement:  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zauction/"
    },
    {
        "title": "5.21 init deprecated, constructor added",
        "body": "  Description  The zAuction initialization method is unprotected and while only being executable once, can be called by anyone. This might allow someone to monitor the mempool for new deployments of this contract and fron-run the initialization to initialize it with different parameters.  A mitigating factor is that this condition can be detected by the deployer as subsequent calls to init() will fail.  Note: this doesn t adhere to common interface naming convention/oz naming convention where this method would be called initialize.  Note: that zNS in contrast relies on ou/initializable pattern with proper naming.  Note: that this function might not be necessary at all and should be replaced by a constructor instead, as the contract is not used with a proxy pattern.  Examples  zAuction/contracts/zAuction.sol:L22-L26  function init(address accountantaddress) external {  require(!initialized);  initialized = true;  accountant = zAuctionAccountant(accountantaddress);  Recommendation  The contract is not used in a proxy pattern, hence, the initialization should be performed in the constructor instead.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zauction/"
    },
    {
        "title": "5.5 zAuction - unclear upgrade path    ",
        "body": "  Resolution   obsolete with changes from   zer0-os/zAuction@135b2aa removing the  Description  https://github.com/ConsenSys/zer0-zauction-audit-2021-05/issues/7).  Acceptance of bids via the accountant on the old contract immediately fail after an admin updates the referenced zAuction contract while WETH bids may still continue. This may create an unfavorable scenario where two contracts may be active in parallel accepting WETH bids.  It should also be noted that 2nd layer bids (signed data) using the accountant for the old contract will not be acceptable anymore.  Examples  zAuction/contracts/zAuctionAccountant.sol:L60-L63  function SetZauction(address zauctionaddress) external onlyAdmin{  zauction = zauctionaddress;  emit ZauctionSet(zauctionaddress);  Recommendation  Consider re-thinking the upgrade path. Avoid keeping multiple versions of the auction contact active.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zauction/"
    },
    {
        "title": "5.6 zAuction, zNS - gas griefing by spamming offchain fake bids   ",
        "body": "  Resolution  Addressed and acknowledged with changes from zer0-os/zAuction@135b2aa. The client provided the following remark:  ",
        "labels": [
            "Consensys",
            "Medium",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zauction/"
    },
    {
        "title": "5.19 I have attempted to order the requires sensibly, putting the least expensive first. Please advise if the ordering is optimal. gas griefing will be mitigated in the dapp with off-client checks",
        "body": "  Description  The execution status of both zAuction.acceptBid and StakingController.fulfillDomainBid transactions depend on the bidder, as his approval is needed, his signature is being validated, etc. However, these transactions can be submitted by accounts that are different from the bidder account, or for accounts that do not have the required funds/deposits available, luring the account that has to perform the on-chain call into spending gas on a transaction that is deemed to fail (gas griefing). E.g. posting high-value fake bids for zAuction without having funds deposited or WETH approved.  Examples  zNS/contracts/StakingController.sol:L120-L152  function fulfillDomainBid(  uint256 parentId,  uint256 bidAmount,  uint256 royaltyAmount,  string memory bidIPFSHash,  string memory name,  string memory metadata,  bytes memory signature,  bool lockOnCreation,  address recipient  ) external {  bytes32 recoveredBidHash = createBid(parentId, bidAmount, bidIPFSHash, name);  address recoveredBidder = recover(recoveredBidHash, signature);  require(recipient == recoveredBidder, \"ZNS: bid info doesnt match/exist\");  bytes32 hashOfSig = keccak256(abi.encode(signature));  require(approvedBids[hashOfSig] == true, \"ZNS: has been fullfilled\");  infinity.safeTransferFrom(recoveredBidder, controller, bidAmount);  uint256 id = registrar.registerDomain(parentId, name, controller, recoveredBidder);  registrar.setDomainMetadataUri(id, metadata);  registrar.setDomainRoyaltyAmount(id, royaltyAmount);  registrar.transferFrom(controller, recoveredBidder, id);  if (lockOnCreation) {  registrar.lockDomainMetadataForOwner(id);  approvedBids[hashOfSig] = false;  emit DomainBidFulfilled(  metadata,  name,  recoveredBidder,  id,  parentId  );  zAuction/contracts/zAuction.sol:L35-L44  function acceptBid(bytes memory signature, uint256 rand, address bidder, uint256 bid, address nftaddress, uint256 tokenid) external {  address recoveredbidder = recover(toEthSignedMessageHash(keccak256(abi.encode(rand, address(this), block.chainid, bid, nftaddress, tokenid))), signature);  require(bidder == recoveredbidder, 'zAuction: incorrect bidder');  require(!randUsed[rand], 'Random nonce already used');  randUsed[rand] = true;  IERC721 nftcontract = IERC721(nftaddress);  accountant.Exchange(bidder, msg.sender, bid);  nftcontract.transferFrom(msg.sender, bidder, tokenid);  emit BidAccepted(bidder, msg.sender, bid, nftaddress, tokenid);  Recommendation  Revert early for checks that depend on the bidder before performing gas-intensive computations.  Consider adding a dry-run validation for off-chain components before transaction submission.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zauction/"
    },
    {
        "title": "5.7 zAuction - functionality outlined in specification that is not implemented yet    ",
        "body": "  Resolution   implemented as   zer0-os/zAuction@135b2aa.  Description  The specification outlines three main user journeys of which one does not seem to be implemented.  Users will be able to do simple transfer of NFTs. - which does not require functionality in the smart contract  Users will be able to post NFTs at a sale price, and buy at that price. - does not seem to be implemented  Users will be able to post NFTs for auction, bid on auctions, and accept bids - is implemented  Recommendation  User flow (2) is not implemented in the smart contract system. Consider updating the spec or clearly highlighting functionality that is still in development for it to be excluded from security testing.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zauction/"
    },
    {
        "title": "5.8 zAuction - auctions/offers can be terminated by reusing the auction id    ",
        "body": "  Resolution  zer0-os/zAuction@8ff0eab by binding  In the zSale case the saleId is chosen by the seller. The offer (signed offer parameters including saleid) is shared on an off-chain channel. The buyer calls zSale.purchase to buy the token from the offer. The offer and all offers containing the same seller+saleid are then invalidated.  In zAuction there is no seller or someone who initiates an auction. Anyone can bid for nft s held by anyone else. The bidder chooses an auction id. There might be multiple bidders. Since the auctionId is an individual choice and the smart contract does not enforce an auction to be started there may be multiple auctions for the same token but using different auction ids. The current mechanism automatically invalidates all current bids for the token+auctionId combination for the winning bidder. Bids by other holders are not automatically invalidated but they can be invalidated manually via cancelBidsUnderPrice for an auctionId. Note that the winning bid is chosen by the nftowner/seller. The new owner of the nft may be able to immediately accept another bid and transfer the token [seller]--acceptBid-->[newOwner-A]--acceptBid-->[newOwner-B].  Description  zer0-os/zAuction@2f92aa1 introduced a way of tracking auctions/sales by using an auctionId/saleId. The id s are unique and the same id cannot be used for multiple auctions/offers.  Two different auctions/offers may pick the same id, the first auction/offer will go through while the latter cannot be fulfilled anymore. This may happen accidentally or intentionally be forced by a malicious actor to terminate active auctions/sales (griefing, front-running).  Examples  Alice puts out an offer for someone to buy nft X at a specific price. Bob decides to accept that offer and buy the nft by calling zSale.purchase(saleid, price, token, ...). Mallory monitors the mempool, sees this transaction, front-runs it to fulfill its own sale (for a random nft he owns) reusing the saleid from Bobs transaction. Since Mallories transaction marks the saleid as consumed it terminates Alie s offer and hence Bob cannot buy the token as the transaction will revert.  Recommendation  Consider using keccak(saleid+nftcontract+nfttokenid) as the unique sale/auction identifier instead, or alternatively associate the bidder address with the auctionId (require that consumed[bidder][auctionId]== false)  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zauction/"
    },
    {
        "title": "5.9 zAuction - hardcoded ropsten token address    ",
        "body": "  Resolution  Addressed with zer0-os/zAuction@135b2aa and the following statement:  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zauction/"
    },
    {
        "title": "5.30 weth address in constructor",
        "body": "  Note: does not perform input validation as recommended  Description  The auction contract hardcodes the WETH ERC20 token address. this address will not be functional when deploying to mainnet.  Examples  zAuction/contracts/zAuction.sol:L15-L16  IERC20 weth = IERC20(address(0xc778417E063141139Fce010982780140Aa0cD5Ab)); // rinkeby weth  Recommendation  Consider taking the used WETH token address as a constructor argument. Avoid code changes to facilitate testing! Perform input validation on arguments rejecting address(0x0) to facilitate the detection of potential misconfiguration in the deployment pipeline.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zauction/"
    },
    {
        "title": "5.10 zAuction - accountant allows zero value withdrawals/deposits/exchange    ",
        "body": "  Resolution   Obsolete. The affected component has been removed from the system with   zer0-os/zAuction@135b2aa.  Description  Zero value transfers effectively perform a no-operation sometimes followed by calling out to the recipient of the withdrawal.  A transfer where from==to or where the value is 0 is ineffective.  Examples  zAuction/contracts/zAuctionAccountant.sol:L38-L42  function Withdraw(uint256 amount) external {  ethbalance[msg.sender] = SafeMath.sub(ethbalance[msg.sender], amount);  payable(msg.sender).transfer(amount);  emit Withdrew(msg.sender, amount);  zAuction/contracts/zAuctionAccountant.sol:L33-L36  function Deposit() external payable {  ethbalance[msg.sender] = SafeMath.add(ethbalance[msg.sender], msg.value);  emit Deposited(msg.sender, msg.value);  zAuction/contracts/zAuctionAccountant.sol:L44-L58  function zDeposit(address to) external payable onlyZauction {  ethbalance[to] = SafeMath.add(ethbalance[to], msg.value);  emit zDeposited(to, msg.value);  function zWithdraw(address from, uint256 amount) external onlyZauction {  ethbalance[from] = SafeMath.sub(ethbalance[from], amount);  emit zWithdrew(from, amount);  function Exchange(address from, address to, uint256 amount) external onlyZauction {  ethbalance[from] = SafeMath.sub(ethbalance[from], amount);  ethbalance[to] = SafeMath.add(ethbalance[to], amount);  emit zExchanged(from, to, amount);  Recommendation  Consider rejecting ineffective withdrawals (zero value) or at least avoid issuing a zero value ETH transfers. Avoid emitting successful events for ineffective calls to not trigger 3rd party components on noop s.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zauction/"
    },
    {
        "title": "5.11 zAuction - seller should not be able to accept their own bid    ",
        "body": "  Resolution  Addressed with zer0-os/zAuction@135b2aa by disallowing the seller to accept their own bid. The client provided the following note:  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zauction/"
    },
    {
        "title": "5.28 seller != buyer required",
        "body": "  Description  A seller can accept their own bid which is an ineffective action that is emitting an event.  Examples  zAuction/contracts/zAuction.sol:L35-L56  function acceptBid(bytes memory signature, uint256 rand, address bidder, uint256 bid, address nftaddress, uint256 tokenid) external {  address recoveredbidder = recover(toEthSignedMessageHash(keccak256(abi.encode(rand, address(this), block.chainid, bid, nftaddress, tokenid))), signature);  require(bidder == recoveredbidder, 'zAuction: incorrect bidder');  require(!randUsed[rand], 'Random nonce already used');  randUsed[rand] = true;  IERC721 nftcontract = IERC721(nftaddress);  accountant.Exchange(bidder, msg.sender, bid);  nftcontract.transferFrom(msg.sender, bidder, tokenid);  emit BidAccepted(bidder, msg.sender, bid, nftaddress, tokenid);  /// @dev 'true' in the hash here is the eth/weth switch  function acceptWethBid(bytes memory signature, uint256 rand, address bidder, uint256 bid, address nftaddress, uint256 tokenid) external {  address recoveredbidder = recover(toEthSignedMessageHash(keccak256(abi.encode(rand, address(this), block.chainid, bid, nftaddress, tokenid, true))), signature);  require(bidder == recoveredbidder, 'zAuction: incorrect bidder');  require(!randUsed[rand], 'Random nonce already used');  randUsed[rand] = true;  IERC721 nftcontract = IERC721(nftaddress);  weth.transferFrom(bidder, msg.sender, bid);  nftcontract.transferFrom(msg.sender, bidder, tokenid);  emit WethBidAccepted(bidder, msg.sender, bid, nftaddress, tokenid);  Recommendation  Disallow transfers to self.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zauction/"
    },
    {
        "title": "5.1 Similar token-to-token swap methods can yield very different results ",
        "body": "  Description  BPool s interface exposes several methods to perform token swaps. Because the formula used to calculate trade values varies depending on the method, we compared token swaps performed using two different methods:  BPool.swapExactAmountIn performs a direct token-to-token swap between two bound assets within the pool. Some amount tokenAmountIn of tokenIn is directly traded for some minimum amount minAmountOut of tokenOut. An additional parameter, maxPrice, allows the trader to specify the maximum amount of slippage allowed during the trade.  BPool.joinswapExternAmountIn allows a trader to exchange an amount tokenAmountIn of tokenIn for a minimum amount minPoolAmountOut of the pool s token. A subsequent call to BPool.exitswapPoolAmountIn allows a trader to exchange amount poolAmountIn of the pool s tokens for a minimum amount minAmountOut of tokenOut.  While the latter method performs a swap by way of the pool s token as an intermediary, both methods can be used in order to perform a token-to-token swap. Our comparison between the two tested the relative amount tokenAmountOut of tokenOut between the two methods with a variety of different parameters.  Examples  Each example made use of a testing contract, found here: https://gist.github.com/wadeAlexC/12ee22438e8028f5439c5f0faaf9b7f7  Additionally, BPool was modified; unneeded functions were removed so that deployment did not exceed the block gas limit.  1. tokenIn weight: 25 BONE  tokenOut weight: 25 BONE  tokenIn, tokenOut at equal balances (50 BONE)  tokenAmountIn: 1 BONE  swapExactAmountIn tokenAmountOut: 980391195693945000  joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 980391186207949598  Result: swapExactAmountIn gives 1.00000001x more tokens  2. tokenIn weight: 1 BONE  tokenOut weight: 49 BONE  tokenIn, tokenOut at equal balances (50 BONE)  tokenAmountIn: 1 BONE  swapExactAmountIn tokenAmountOut: 20202659955287800  joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 20202659970818843  Result: joinswap/exitswap gives 1.00000001x more tokens  3. tokenIn weight: 25 BONE  tokenOut weight: 25 BONE  tokenIn, tokenOut at equal balances (1 BONE)  tokenAmountIn: 0.5 BONE  swapExactAmountIn tokenAmountOut: 333333111111037037  joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 333333055579388951  Result: swapExactAmountIn gives 1.000000167x more tokens  4. tokenIn weight: 25 BONE  tokenOut weight: 25 BONE  tokenIn, tokenOut at equal balances (30 BONE)  tokenAmountIn: 15 BONE  swapExactAmountIn tokenAmountOut: 9999993333331111110  joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 9999991667381668530  Result: swapExactAmountIn gives 1.000000167x more tokens  The final test raised the swap fee from MIN_FEE (0.0001%) to MAX_FEE (10%):    tokenIn weight: 25 BONE  tokenOut weight: 25 BONE  tokenIn, tokenOut at equal balances (30 BONE)  tokenAmountIn: 15 BONE  swapExactAmountIn tokenAmountOut: 9310344827586206910  joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 9177966102628338740  Result: swapExactAmountIn gives 1.014423536x more tokens  Recommendation  Our final test showed that with equivalent balances and weights, raising the swap fee to 10% had a drastic effect on relative tokenAmountOut received, with swapExactAmountIn yielding >1.44% more tokens than the joinswap/exitswap method.  Reading through Balancer s provided documentation, our assumption was that these two swap methods were roughly equivalent. Discussion with Balancer clarified that the joinswap/exitswap method applied two swap fees: one for single asset deposit, and one for single asset withdrawal. With the minimum swap fee, this double application proved to have relatively little impact on the difference between the two methods. In fact, some parameters resulted in higher relative yield from the joinswap/exitswap method. With the maximum swap fee, the double application was distinctly noticeable.  Given the relative complexity of the math behind BPools, there is much that remains to be tested. There are alternative swap methods, as well as numerous additional permutations of parameters that could be used; these tests were relatively narrow in scope.  We recommend increasing the intensity of unit testing to cover a more broad range of interactions with BPool s various swap methods. In particular, the double application of the swap fee should be examined, as well as the differences between low and high swap fees.  Those using BPool should endeavor to understand as much of the underlying math as they can, ensuring awareness of the various options available for performing trades.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/balancer-finance/"
    },
    {
        "title": "5.2 Commented code exists in BMath ",
        "body": "  Description  There are some instances of code being commented out in the BMath.sol that should be removed. It seems that most of the commented code is related to exit fee, however this is in contrast to BPool.sol code base that still has the exit fee code flow, but uses 0 as the fee.  Examples  code/contracts/BMath.sol:L137-L140  uint tokenInRatio = bdiv(newTokenBalanceIn, tokenBalanceIn);  // uint newPoolSupply = (ratioTi ^ weightTi) * poolSupply;  uint poolRatio = bpow(tokenInRatio, normalizedWeight);  code/contracts/BMath.sol:L206-L209  uint normalizedWeight = bdiv(tokenWeightOut, totalWeight);  // charge exit fee on the pool token side  // pAiAfterExitFee = pAi*(1-exitFee)  uint poolAmountInAfterExitFee = bmul(poolAmountIn, bsub(BONE, EXIT_FEE));  And many more examples.  Recommendation  Remove the commented code, or address them properly. If the code is related to exit fee, which is considered to be 0 in this version, this style should be persistent in other contracts as well.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/balancer-finance/"
    },
    {
        "title": "5.3 Max weight requirement in rebind is inaccurate ",
        "body": "  Description  BPool.rebind enforces MIN_WEIGHT and MAX_WEIGHT bounds on the passed-in denorm value:  code/contracts/BPool.sol:L262-L274  function rebind(address token, uint balance, uint denorm)  public  _logs_  _lock_  require(msg.sender == _controller, \"ERR_NOT_CONTROLLER\");  require(_records[token].bound, \"ERR_NOT_BOUND\");  require(!_finalized, \"ERR_IS_FINALIZED\");  require(denorm >= MIN_WEIGHT, \"ERR_MIN_WEIGHT\");  require(denorm <= MAX_WEIGHT, \"ERR_MAX_WEIGHT\");  require(balance >= MIN_BALANCE, \"ERR_MIN_BALANCE\");  MIN_WEIGHT is 1 BONE, and MAX_WEIGHT is 50 BONE.  Though a token weight of 50 BONE may make sense in a single-token system, BPool is intended to be used with two to eight tokens. The sum of the weights of all tokens must not be greater than 50 BONE.  This implies that a weight of 50 BONE for any single token is incorrect, given that at least one other token must be present.  Recommendation  MAX_WEIGHT for any single token should be MAX_WEIGHT - MIN_WEIGHT, or 49 BONE.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/balancer-finance/"
    },
    {
        "title": "5.4 Switch modifier order in BPool ",
        "body": "  Description  BPool functions often use modifiers in the following order: _logs_, _lock_. Because _lock_ is a reentrancy guard, it should take precedence over _logs_. See example:  Recommendation  Place _lock_ before other modifiers; ensuring it is the very first and very last thing to run when a function is called.  6 Document Change Log  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/balancer-finance/"
    },
    {
        "title": "1.0",
        "body": "0000001x more tokens  2. tokenIn weight: 1 BONE  tokenOut weight: 49 BONE  tokenIn, tokenOut at equal balances (50 BONE)  tokenAmountIn: 1 BONE  swapExactAmountIn tokenAmountOut: 20202659955287800  joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 20202659970818843  Result: joinswap/exitswap gives 1.00000001x more tokens  3. tokenIn weight: 25 BONE  tokenOut weight: 25 BONE  tokenIn, tokenOut at equal balances (1 BONE)  tokenAmountIn: 0.5 BONE  swapExactAmountIn tokenAmountOut: 333333111111037037  joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 333333055579388951  Result: swapExactAmountIn gives 1.000000167x more tokens  4. tokenIn weight: 25 BONE  tokenOut weight: 25 BONE  tokenIn, tokenOut at equal balances (30 BONE)  tokenAmountIn: 15 BONE  swapExactAmountIn tokenAmountOut: 9999993333331111110  joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 9999991667381668530  Result: swapExactAmountIn gives 1.000000167x more tokens  The final test raised the swap fee from MIN_FEE (0.0001%) to MAX_FEE (10%):    tokenIn weight: 25 BONE  tokenOut weight: 25 BONE  tokenIn, tokenOut at equal balances (30 BONE)  tokenAmountIn: 15 BONE  swapExactAmountIn tokenAmountOut: 9310344827586206910  joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 9177966102628338740  Result: swapExactAmountIn gives 1.014423536x more tokens  Recommendation  Our final test showed that with equivalent balances and weights, raising the swap fee to 10% had a drastic effect on relative tokenAmountOut received, with swapExactAmountIn yielding >1.44% more tokens than the joinswap/exitswap method.  Reading through Balancer s provided documentation, our assumption was that these two swap methods were roughly equivalent. Discussion with Balancer clarified that the joinswap/exitswap method applied two swap fees: one for single asset deposit, and one for single asset withdrawal. With the minimum swap fee, this double application proved to have relatively little impact on the difference between the two methods. In fact, some parameters resulted in higher relative yield from the joinswap/exitswap method. With the maximum swap fee, the double application was distinctly noticeable.  Given the relative complexity of the math behind BPools, there is much that remains to be tested. There are alternative swap methods, as well as numerous additional permutations of parameters that could be used; these tests were relatively narrow in scope.  We recommend increasing the intensity of unit testing to cover a more broad range of interactions with BPool s various swap methods. In particular, the double application of the swap fee should be examined, as well as the differences between low and high swap fees.  Those using BPool should endeavor to understand as much of the underlying math as they can, ensuring awareness of the various options available for performing trades.  5.2 Commented code exists in BMath Minor  Description  There are some instances of code being commented out in the BMath.sol that should be removed. It seems that most of the commented code is related to exit fee, however this is in contrast to BPool.sol code base that still has the exit fee code flow, but uses 0 as the fee.  Examples  code/contracts/BMath.sol:L137-L140  uint tokenInRatio = bdiv(newTokenBalanceIn, tokenBalanceIn);  // uint newPoolSupply = (ratioTi ^ weightTi) * poolSupply;  uint poolRatio = bpow(tokenInRatio, normalizedWeight);  code/contracts/BMath.sol:L206-L209  uint normalizedWeight = bdiv(tokenWeightOut, totalWeight);  // charge exit fee on the pool token side  // pAiAfterExitFee = pAi*(1-exitFee)  uint poolAmountInAfterExitFee = bmul(poolAmountIn, bsub(BONE, EXIT_FEE));  And many more examples.  Recommendation  Remove the commented code, or address them properly. If the code is related to exit fee, which is considered to be 0 in this version, this style should be persistent in other contracts as well.  5.3 Max weight requirement in rebind is inaccurate Minor  Description  BPool.rebind enforces MIN_WEIGHT and MAX_WEIGHT bounds on the passed-in denorm value:  code/contracts/BPool.sol:L262-L274  function rebind(address token, uint balance, uint denorm)  public  _logs_  _lock_  require(msg.sender == _controller, \"ERR_NOT_CONTROLLER\");  require(_records[token].bound, \"ERR_NOT_BOUND\");  require(!_finalized, \"ERR_IS_FINALIZED\");  require(denorm >= MIN_WEIGHT, \"ERR_MIN_WEIGHT\");  require(denorm <= MAX_WEIGHT, \"ERR_MAX_WEIGHT\");  require(balance >= MIN_BALANCE, \"ERR_MIN_BALANCE\");  MIN_WEIGHT is 1 BONE, and MAX_WEIGHT is 50 BONE.  Though a token weight of 50 BONE may make sense in a single-token system, BPool is intended to be used with two to eight tokens. The sum of the weights of all tokens must not be greater than 50 BONE.  This implies that a weight of 50 BONE for any single token is incorrect, given that at least one other token must be present.  Recommendation  MAX_WEIGHT for any single token should be MAX_WEIGHT - MIN_WEIGHT, or 49 BONE.  5.4 Switch modifier order in BPool Minor  Description  BPool functions often use modifiers in the following order: _logs_, _lock_. Because _lock_ is a reentrancy guard, it should take precedence over _logs_. See example:  Recommendation  Place _lock_ before other modifiers; ensuring it is the very first and very last thing to run when a function is called.  6 Document Change Log  1.0  2020-05-15  Initial report  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/05/balancer-finance/"
    },
    {
        "title": "5.1 ERC20 tokens with no return value will fail to transfer    ",
        "body": "  Resolution  This issue was addressed using OpenZeppelin s SafeERC20.  Description  Although the ERC20 standard suggests that a transfer should return true on success, many tokens are non-compliant in this regard.  In that case, the .transfer() call here will revert even if the transfer is successful, because solidity will check that the RETURNDATASIZE matches the ERC20 interface.  code/contracts/ExchangeDeposit.sol:L229-L231  if (!instance.transfer(getSendAddress(), forwarderBalance)) {  revert('Could not gather ERC20');  Recommendation  Consider using OpenZeppelin s SafeERC20.  ",
        "labels": [
            "Consensys",
            "Major",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/bitbank/"
    },
    {
        "title": "5.1 PeriodicPrizeStrategy - RNG failure can lock user funds ",
        "body": "  Description  To prevent manipulation of the SortitionSumTree after a requested random number enters the mempool, users are unable to withdraw funds while the strategy contract waits on a random number request between execution of startAward() and completeAward().  If an rng request fails, however, there is no way to exit this locked state. After an rng request times out, only startAward() can be called, which will make another rng request and re-enter the same locked state. The rng provider can also not be updated while the contract is in this state. If the rng provider fails permanently, user funds are permanently locked.  Examples  requireNotLocked() prevents transfers, deposits, or withdrawals when there is a pending award.  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L282-L285  function beforeTokenTransfer(address from, address to, uint256 amount, address controlledToken) external override onlyPrizePool {  if (controlledToken == address(ticket)) {  _requireNotLocked();  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L528-L531  function _requireNotLocked() internal view {  uint256 currentBlock = _currentBlock();  require(rngRequest.lockBlock == 0 || currentBlock < rngRequest.lockBlock, \"PeriodicPrizeStrategy/rng-in-flight\");  setRngService() reverts if there is a pending or timed-out rng request  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L413-L414  function setRngService(RNGInterface rngService) external onlyOwner {  require(!isRngRequested(), \"PeriodicPrizeStrategy/rng-in-flight\");  Recommendation  Instead of forcing the pending award phase to be re-entered in the event of an rng request time-out, provide an exitAwardPhase() function that ends the award phase without paying out the award. This will at least allow users to withdraw their funds in the event of a catastrophic failure of the rng service. It may also be prudent to allow the rng service to be updated in the event of an rng request time out.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pooltogether-lootbox-and-multiplewinners-strategy/"
    },
    {
        "title": "5.2 LootBox - Unprotected selfdestruct in proxy implementation ",
        "body": "  Description  When the LootBoxController is deployed, it also deploys an instance of LootBox. When someone calls LootBoxController.plunder() or LootBoxController.executeCall() the controller actually deploys a temporary proxy contract to a deterministic address using create2, then calls out to it to collect the loot.  The LootBox implementation contract is completely unprotected, exposing all its functionality to any actor on the blockchain. The most critical functionality is actually the LootBox.destroy() method that calls selfdestruct() on the implementation contract.  Therefore, an unauthenticated user can selfdestruct the LootBox proxy implementation and cause the complete system to become dysfunctional. As an effect, none of the AirDrops that were delivered based on this contract will be redeemable (Note: create2 deploy address is calculated from the current contract address and salt). Funds may be lost.  Examples  code/loot-box/contracts/LootBoxController.sol:L28-L31  constructor () public {  lootBoxActionInstance = new LootBox();  lootBoxActionBytecode = MinimalProxyLibrary.minimalProxy(address(lootBoxActionInstance));  code/loot-box/contracts/LootBox.sol:L86-L90  /// @notice Destroys this contract using `selfdestruct`  /// @param to The address to send remaining Ether to  function destroy(address payable to) external {  selfdestruct(to);  not in scope but listed for completeness  code/pool/contracts/counterfactual-action/CounterfactualAction.sol:L7-L21  contract CounterfactualAction {  function depositTo(address payable user, PrizePool prizePool, address output, address referrer) external {  IERC20 token = IERC20(prizePool.token());  uint256 amount = token.balanceOf(address(this));  token.approve(address(prizePool), amount);  prizePool.depositTo(user, amount, output, referrer);  selfdestruct(user);  function cancel(address payable user, PrizePool prizePool) external {  IERC20 token = IERC20(prizePool.token());  token.transfer(user, token.balanceOf(address(this)));  selfdestruct(user);  Recommendation  Enforce that only the deployer of the contract can call functionality in the contract. Make sure that nobody can destroy the implementation of proxy contracts.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pooltogether-lootbox-and-multiplewinners-strategy/"
    },
    {
        "title": "5.3 Ticket duplication ",
        "body": "  Description  Ticket._beforeTokenTransfer() contains logic to update the SortitionSumTree from which prize winners are drawn. In the case where the from address is the same as the to address, tickets are duplicated rather than left unchanged. This allows any attacker to duplicate their tickets with no limit and virtually guarantee that they will win all awarded prizes.  code/pool/contracts/token/Ticket.sol:L71-L79  if (from != address(0)) {  uint256 fromBalance = balanceOf(from).sub(amount);  sortitionSumTrees.set(TREE_KEY, fromBalance, bytes32(uint256(from)));  if (to != address(0)) {  uint256 toBalance = balanceOf(to).add(amount);  sortitionSumTrees.set(TREE_KEY, toBalance, bytes32(uint256(to)));  This code was outside the scope of our review but was live on mainnet at the time the issue was disovered. We immediately made the client aware of the issue and an effort was made to mitigate the impact on the existing deployment.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pooltogether-lootbox-and-multiplewinners-strategy/"
    },
    {
        "title": "5.4 PeriodicPriceStrategy - trustedForwarder can impersonate any msg.sender ",
        "body": "  Description  The centralization of power to allow one account to impersonate other components and roles (owner, listener, prizePool) in the system is a concern by itself and may give users pause when deciding whether to trust the contract system. The fact that the trustedForwarder can spoof events for any msg.sender may also make it hard to keep an accurate log trail of events in case of a security incident.  Note: The same functionality seems to be used in ControlledToken and other contracts which allows the trustedForwarder to assume any tokenholder in ERC20UpgradeSafe. There is practically no guarantee to ControlledToken holders.  Note: The trustedForwarder/msgSender() pattern is used in multiple contracts, many of which are not in the scope of this assessment.  Examples  access control modifiers that can be impersonated  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L588-L591  modifier onlyPrizePool() {  require(_msgSender() == address(prizePool), \"PeriodicPrizeStrategy/only-prize-pool\");  _;  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L565-L568  modifier onlyOwnerOrListener() {  require(_msgSender() == owner() || _msgSender() == address(periodicPrizeStrategyListener), \"PeriodicPrizeStrategy/only-owner-or-listener\");  _;  event msg.sender that can be spoofed because the actual msg.sender can be trustedForwarder  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L164-L164  emit PrizePoolOpened(_msgSender(), prizePeriodStartedAt);  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L340-L340  emit PrizePoolAwardStarted(_msgSender(), address(prizePool), requestId, lockBlock);  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L356-L357  emit PrizePoolAwarded(_msgSender(), randomNumber);  emit PrizePoolOpened(_msgSender(), prizePeriodStartedAt);  _msgSender() implementation allows the trustedForwarder to impersonate any msg.sender address  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L541-L551  /// @dev Provides information about the current execution context for GSN Meta-Txs.  /// @return The payable address of the message sender  function _msgSender()  internal  override(BaseRelayRecipient, ContextUpgradeSafe)  virtual  view  returns (address payable)  return BaseRelayRecipient._msgSender();  // File: @opengsn/gsn/contracts/BaseRelayRecipient.sol  ...  /**  return the sender of this call.  if the call came through our trusted forwarder, return the original sender.  otherwise, return `msg.sender`.  should be used in the contract anywhere instead of msg.sender  /  function _msgSender() internal override virtual view returns (address payable ret) {  if (msg.data.length >= 24 && isTrustedForwarder(msg.sender)) {  // At this point we know that the sender is a trusted forwarder,  // so we trust that the last bytes of msg.data are the verified sender address.  // extract sender address from the end of msg.data  assembly {  ret := shr(96,calldataload(sub(calldatasize(),20)))  } else {  return msg.sender;  Recommendation  Remove the trustedForwarder or restrict the type of actions the forwarder can perform and don t allow it to impersonate other components in the system. Make sure users understand the trust assumptions and who has what powers in the system. Make sure to keep an accurate log trail of who performed which action on whom s behalf.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pooltogether-lootbox-and-multiplewinners-strategy/"
    },
    {
        "title": "5.5 Unpredictable behavior for users due to admin front running or general bad timing ",
        "body": "  Description  In a number of cases, administrators of contracts can update or upgrade things in the system without warning. This has the potential to violate a security goal of the system.  Specifically, privileged roles could use front running to make malicious changes just ahead of incoming transactions, or purely accidental negative effects could occur due to unfortunate timing of changes.  In general users of the system should have assurances about the behavior of the action they re about to take.  Examples  An administrator (deployer) of MultipleWinners can change the number of winners in the system without warning. This has the potential to violate a security goal of the system.  admin can change the number of winners during a prize-draw period  code/pool/contracts/prize-strategy/multiple-winners/MultipleWinners.sol:L38-L42  function setNumberOfWinners(uint256 count) external onlyOwner {  __numberOfWinners = count;  emit NumberOfWinnersSet(count);  PeriodicPriceStrategy - admin may switch-out RNG service at any time (when RNG is not in inflight or timed-out)  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L413-L418  function setRngService(RNGInterface rngService) external onlyOwner {  require(!isRngRequested(), \"PeriodicPrizeStrategy/rng-in-flight\");  rng = rngService;  emit RngServiceUpdated(address(rngService));  PeriodicPriceStrategy - admin can effectively disable the rng request timeout by setting a high value during a prize-draw (e.g. to indefinitely block payouts)  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L420-L422  function setRngRequestTimeout(uint32 _rngRequestTimeout) external onlyOwner {  _setRngRequestTimeout(_rngRequestTimeout);  PeriodicPriceStrategy - admin may set new tokenListener which might intentionally block token-transfers  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L175-L179  function setTokenListener(TokenListenerInterface _tokenListener) external onlyOwner {  tokenListener = _tokenListener;  emit TokenListenerUpdated(address(tokenListener));  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L360-L364  function setPeriodicPrizeStrategyListener(address _periodicPrizeStrategyListener) external onlyOwner {  periodicPrizeStrategyListener = PeriodicPrizeStrategyListener(_periodicPrizeStrategyListener);  emit PeriodicPrizeStrategyListenerSet(_periodicPrizeStrategyListener);  out of scope but mentioned as a relevant example: PrizePool owner can set new PrizeStrategy at any time  code/pool/contracts/prize-pool/PrizePool.sol:L1003-L1008  /// @notice Sets the prize strategy of the prize pool.  Only callable by the owner.  /// @param _prizeStrategy The new prize strategy  function setPrizeStrategy(address _prizeStrategy) external override onlyOwner {  _setPrizeStrategy(TokenListenerInterface(_prizeStrategy));  a malicious admin may remove all external ERC20/ERC721 token awards prior to the user claiming them (admin front-running opportunity)  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L461-L464  function removeExternalErc20Award(address _externalErc20, address _prevExternalErc20) external onlyOwner {  externalErc20s.removeAddress(_prevExternalErc20, _externalErc20);  emit ExternalErc20AwardRemoved(_externalErc20);  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L506-L510  function removeExternalErc721Award(address _externalErc721, address _prevExternalErc721) external onlyOwner {  externalErc721s.removeAddress(_prevExternalErc721, _externalErc721);  delete externalErc721TokenIds[_externalErc721];  emit ExternalErc721AwardRemoved(_externalErc721);  the PeriodicPrizeStrategy owner (also see concerns outlined in issue 5.4) can transfer external ERC20 at any time to avoid them being awarded to users. there is no guarantee to the user.  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L517-L526  function transferExternalERC20(  address to,  address externalToken,  uint256 amount  external  onlyOwner  prizePool.transferExternalERC20(to, externalToken, amount);  Recommendation  The underlying issue is that users of the system can t be sure what the behavior of a function call will be, and this is because the behavior can change at any time.  We recommend giving the user advance notice of changes with a time lock. For example, make all system-parameter and upgrades require two steps with a mandatory time window between them. The first step merely broadcasts to users that a particular change is coming, and the second step commits that change after a suitable waiting period. This allows users that do not accept the change to withdraw immediately.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pooltogether-lootbox-and-multiplewinners-strategy/"
    },
    {
        "title": "5.6 PeriodicPriceStrategy - addExternalErc721Award duplicate or invalid tokenIds may block award phase ",
        "body": "  Description  The prize-strategy owner (or a listener) can add ERC721 token awards by calling addExternalErc721Award providing the ERC721 token address and a list of tokenIds owned by the prizePool.  The method does not check if duplicate tokenIds or tokenIds that are not owned by the contract are provided. This may cause an exception when _awardExternalErc721s calls prizePool.awardExternalERC721 to transfer an invalid or previously transferred token, blocking the award phase.  Note: An admin can recover from this situation by removing and re-adding the ERC721 token from the awards list.  Examples  adding tokenIds  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L478-L499  /// @notice Adds an external ERC721 token as an additional prize that can be awarded  /// @dev Only the Prize-Strategy owner/creator can assign external tokens,  /// and they must be approved by the Prize-Pool  /// NOTE: The NFT must already be owned by the Prize-Pool  /// @param _externalErc721 The address of an ERC721 token to be awarded  /// @param _tokenIds An array of token IDs of the ERC721 to be awarded  function addExternalErc721Award(address _externalErc721, uint256[] calldata _tokenIds) external onlyOwnerOrListener {  // require(_externalErc721.isContract(), \"PeriodicPrizeStrategy/external-erc721-not-contract\");  require(prizePool.canAwardExternal(_externalErc721), \"PeriodicPrizeStrategy/cannot-award-external\");  if (!externalErc721s.contains(_externalErc721)) {  externalErc721s.addAddress(_externalErc721);  for (uint256 i = 0; i < _tokenIds.length; i++) {  uint256 tokenId = _tokenIds[i];  require(IERC721(_externalErc721).ownerOf(tokenId) == address(prizePool), \"PeriodicPrizeStrategy/unavailable-token\");  externalErc721TokenIds[_externalErc721].push(tokenId);  emit ExternalErc721AwardAdded(_externalErc721, _tokenIds);  awarding tokens  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L248-L263  /// @notice Awards all external ERC721 tokens to the given user.  /// The external tokens must be held by the PrizePool contract.  /// @dev The list of ERC721s is reset after every award  /// @param winner The user to transfer the tokens to  function _awardExternalErc721s(address winner) internal {  address currentToken = externalErc721s.start();  while (currentToken != address(0) && currentToken != externalErc721s.end()) {  uint256 balance = IERC721(currentToken).balanceOf(address(prizePool));  if (balance > 0) {  prizePool.awardExternalERC721(winner, currentToken, externalErc721TokenIds[currentToken]);  delete externalErc721TokenIds[currentToken];  currentToken = externalErc721s.next(currentToken);  externalErc721s.clearAll();  transferring the tokens  code/pool/contracts/prize-pool/PrizePool.sol:L582-L606  /// @notice Called by the prize strategy to award external ERC721 prizes  /// @dev Used to award any arbitrary NFTs held by the Prize Pool  /// @param to The address of the winner that receives the award  /// @param externalToken The address of the external NFT token being awarded  /// @param tokenIds An array of NFT Token IDs to be transferred  function awardExternalERC721(  address to,  address externalToken,  uint256[] calldata tokenIds  external override  onlyPrizeStrategy  require(_canAwardExternal(externalToken), \"PrizePool/invalid-external-token\");  if (tokenIds.length == 0) {  return;  for (uint256 i = 0; i < tokenIds.length; i++) {  IERC721(externalToken).transferFrom(address(this), to, tokenIds[i]);  emit AwardedExternalERC721(to, externalToken, tokenIds);  Recommendation  Ensure that no duplicate token-ids were provided or skip over token-ids that are not owned by prize-pool (anymore).  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pooltogether-lootbox-and-multiplewinners-strategy/"
    },
    {
        "title": "5.7 PeriodicPrizeStrategy - Token with callback related warnings (ERC777 a.o.) ",
        "body": "  Description  This issue is highly dependent on the configuration of the system. If an admin decides to allow callback enabled token (e.g. ERC20 compliant ERC777 or other ERC721/ERC20 extensions) as awards then one recipient may be able to  block the payout for everyone by forcing a revert in the callback when accepting token awards  use the callback to siphon gas, mint gas token, or similar activities  potentially re-enter the PrizeStrategy contract in an attempt to manipulate the payout (e.g. by immediately withdrawing from the pool to manipulate the 2nd ticket.draw())  Examples  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L252-L263  function _awardExternalErc721s(address winner) internal {  address currentToken = externalErc721s.start();  while (currentToken != address(0) && currentToken != externalErc721s.end()) {  uint256 balance = IERC721(currentToken).balanceOf(address(prizePool));  if (balance > 0) {  prizePool.awardExternalERC721(winner, currentToken, externalErc721TokenIds[currentToken]);  delete externalErc721TokenIds[currentToken];  currentToken = externalErc721s.next(currentToken);  externalErc721s.clearAll();  Recommendation  It is highly recommended to not allow tokens with callback functionality into the system. Document and/or implement safeguards that disallow the use of callback enabled tokens. Consider implementing means for the  other winners  to withdraw their share of the rewards independently from others.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pooltogether-lootbox-and-multiplewinners-strategy/"
    },
    {
        "title": "5.8 PeriodicPrizeStrategy - unbounded external tokens linked list may be used to force a gas DoS ",
        "body": "  Description  The size of the linked list of ERC20/ERC721 token awards is not limited. This fact may be exploited by an administrative account by adding an excessive number of external token addresses.  The winning user might want to claim their win by calling completeAward() which fails in one of the _distribute() -> _awardAllExternalTokens() -> _awardExternalErc20s/_awardExternalErc721s while loops if too many token addresses are configured and gas consumption hits the block gas limit (or it just gets too expensive for the user to call).  Note: an admin can recover from this situation by removing items from the list.  Examples  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L436-L448  /// @notice Adds an external ERC20 token type as an additional prize that can be awarded  /// @dev Only the Prize-Strategy owner/creator can assign external tokens,  /// and they must be approved by the Prize-Pool  /// @param _externalErc20 The address of an ERC20 token to be awarded  function addExternalErc20Award(address _externalErc20) external onlyOwnerOrListener {  _addExternalErc20Award(_externalErc20);  function _addExternalErc20Award(address _externalErc20) internal {  require(prizePool.canAwardExternal(_externalErc20), \"PeriodicPrizeStrategy/cannot-award-external\");  externalErc20s.addAddress(_externalErc20);  emit ExternalErc20AwardAdded(_externalErc20);  code/pool/contracts/utils/MappedSinglyLinkedList.sol:L46-L53  /// @param newAddress The address to shift to the front of the list  function addAddress(Mapping storage self, address newAddress) internal {  require(newAddress != SENTINEL && newAddress != address(0), \"Invalid address\");  require(self.addressMap[newAddress] == address(0), \"Already added\");  self.addressMap[newAddress] = self.addressMap[SENTINEL];  self.addressMap[SENTINEL] = newAddress;  self.count = self.count + 1;  awarding the tokens loops through the linked list of configured tokens  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L248-L263  /// @notice Awards all external ERC721 tokens to the given user.  /// The external tokens must be held by the PrizePool contract.  /// @dev The list of ERC721s is reset after every award  /// @param winner The user to transfer the tokens to  function _awardExternalErc721s(address winner) internal {  address currentToken = externalErc721s.start();  while (currentToken != address(0) && currentToken != externalErc721s.end()) {  uint256 balance = IERC721(currentToken).balanceOf(address(prizePool));  if (balance > 0) {  prizePool.awardExternalERC721(winner, currentToken, externalErc721TokenIds[currentToken]);  delete externalErc721TokenIds[currentToken];  currentToken = externalErc721s.next(currentToken);  externalErc721s.clearAll();  Recommendation  Limit the number of tokens an admin can add. Consider implementing an interface that allows the user to claim tokens one-by-one or in user-configured batches.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pooltogether-lootbox-and-multiplewinners-strategy/"
    },
    {
        "title": "5.9 MultipleWinners - setNumberOfWinners does not enforce count>0 ",
        "body": "  Description  The constructor of MultipleWinners enforces that the argument _numberOfWinners > 0 while setNumberOfWinners does not. A careless or malicious admin might set __numberOfWinners to zero to cause the distribute() method to throw and not pay out any winners.  Examples  enforced in the constructor  code/pool/contracts/prize-strategy/multiple-winners/MultipleWinners.sol:L34-L34  require(_numberOfWinners > 0, \"MultipleWinners/num-gt-zero\");  not enforced when updating the value at a later stage  code/pool/contracts/prize-strategy/multiple-winners/MultipleWinners.sol:L38-L42  function setNumberOfWinners(uint256 count) external onlyOwner {  __numberOfWinners = count;  emit NumberOfWinnersSet(count);  Recommendation  Require that numberOfWinners > 0.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pooltogether-lootbox-and-multiplewinners-strategy/"
    },
    {
        "title": "5.10 LootBox - plunder should disallow plundering to address(0) ",
        "body": "  Description  Note:  Depending on the token implementation, transfers may or may not revert if the toAddress == address(0), while burning the ETH will succeed.  This might allow anyone to forcefully burn received ETH that would otherwise be available to the future beneficiary  If the airdrop and transfer of LootBox ownership are not done within one transaction, this might open up a front-running window that allows a third party to burn air-dropped ETH before it can be claimed by the owner.  consider one component issues the airdrop in one transaction (or block) and setting the owner in a later transaction (or block). The owner is unset for a short duration of time which might allow anyone to burn ETH held by the LootBox proxy instance.  Examples  plunder() receiving the owner of an ERC721.tokenId  code/loot-box/contracts/LootBoxController.sol:L49-L56  function plunder(  address erc721,  uint256 tokenId,  IERC20[] calldata erc20s,  LootBox.WithdrawERC721[] calldata erc721s,  LootBox.WithdrawERC1155[] calldata erc1155s  ) external {  address payable owner = payable(IERC721(erc721).ownerOf(tokenId));  The modified ERC721 returns address(0) if the owner is not known  code/loot-box/contracts/external/openzeppelin/ERC721.sol:L102-L107  While withdraw[ERC20|ERC721|ERC1155] fail with to == address(0), transferEther() succeeds and burns the eth by sending it to address(0)  @dev See {IERC721-ownerOf}.  /  function ownerOf(uint256 tokenId) public view override returns (address) {  return _tokenOwners[tokenId];  While withdraw[ERC20|ERC721|ERC1155] fail with to == address(0), transferEther() succeeds and burns the eth by sending it to address(0)  code/loot-box/contracts/LootBox.sol:L74-L84  function plunder(  IERC20[] memory erc20,  WithdrawERC721[] memory erc721,  WithdrawERC1155[] memory erc1155,  address payable to  ) external {  _withdrawERC20(erc20, to);  _withdrawERC721(erc721, to);  _withdrawERC1155(erc1155, to);  transferEther(to, address(this).balance);  Recommendation  Require that the destination address to in plunder() and transferEther() is not address(0).  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pooltogether-lootbox-and-multiplewinners-strategy/"
    },
    {
        "title": "5.11 PeriodicPrizeStrategy - Inconsistent behavior between award-phase modifiers and view functions ",
        "body": "  Description  The logic in the canStartAward() function is inconsistent with that of the requireCanStartAward modifier, and the logic in the canCompleteAward() function is inconsistent with that of the requireCanCompleteAward modifier. Neither of these view functions appear to be used elsewhere in the codebase, but the similarities between the function names and the corresponding modifiers is highly misleading.  Examples  canStartAward() is inconsistent with requireCanStartAward  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L377-L379  function canStartAward() external view returns (bool) {  return _isPrizePeriodOver() && !isRngRequested();  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L575-L579  modifier requireCanStartAward() {  require(_isPrizePeriodOver(), \"PeriodicPrizeStrategy/prize-period-not-over\");  require(!isRngRequested() || isRngTimedOut(), \"PeriodicPrizeStrategy/rng-already-requested\");  _;  canCompleteAward() is inconsistent with requireCanCompleteAward  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L383-L385  function canCompleteAward() external view returns (bool) {  return isRngRequested() && isRngCompleted();  code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L581-L586  modifier requireCanCompleteAward() {  require(_isPrizePeriodOver(), \"PeriodicPrizeStrategy/prize-period-not-over\");  require(isRngRequested(), \"PeriodicPrizeStrategy/rng-not-requested\");  require(isRngCompleted(), \"PeriodicPrizeStrategy/rng-not-complete\");  _;  Recommendation  Make the logic consistent between the view functions and the modifiers of the same name or remove the functions.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pooltogether-lootbox-and-multiplewinners-strategy/"
    },
    {
        "title": "5.12 MultipleWinners - Awards can be guaranteed with a set number of tickets ",
        "body": "  Description  Because additional award drawings are distributed at a constant interval in the SortitionSumTree by MultipleWinners._distribute(), any user that holds a number of tickets >= floor(totalSupply / __numberOfWinners) can guarantee at least one award regardless of the initial drawing.  MultipleWinners._distribute():  code/pool/contracts/prize-strategy/multiple-winners/MultipleWinners.sol:L59-L65  uint256 ticketSplit = totalSupply.div(__numberOfWinners);  uint256 nextRandom = randomNumber.add(ticketSplit);  // the other winners receive their prizeShares  for (uint256 winnerCount = 1; winnerCount < __numberOfWinners; winnerCount++) {  winners[winnerCount] = ticket.draw(nextRandom);  nextRandom = nextRandom.add(ticketSplit);  Recommendation  Do not distribute awards at fixed intervals from the initial drawing, but instead randomize the additional drawings as well.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pooltogether-lootbox-and-multiplewinners-strategy/"
    },
    {
        "title": "5.13 MultipleWinners - Inconsistent behavior compared to SingleRandomWinner ",
        "body": "  Description  The MultipleWinners strategy carries out award distribution to the zero address if ticket.draw() returns address(0) (indicating an error condition) while SingleRandomWinner does not.  Examples  SingleRandomWinner silently skips award distribution if ticket.draw() returns address(0).  code/pool/contracts/prize-strategy/single-random-winner/SingleRandomWinner.sol:L8-L17  contract SingleRandomWinner is PeriodicPrizeStrategy {  function _distribute(uint256 randomNumber) internal override {  uint256 prize = prizePool.captureAwardBalance();  address winner = ticket.draw(randomNumber);  if (winner != address(0)) {  _awardTickets(winner, prize);  _awardAllExternalTokens(winner);  MultipleWinners still attempts to distribute awards if ticket.draw() returns address(0). This may or may not succeed depending on the implementation of the tokens included in the externalErc20s and externalErc721s linked lists.  code/pool/contracts/prize-strategy/multiple-winners/MultipleWinners.sol:L48-L57  function _distribute(uint256 randomNumber) internal override {  uint256 prize = prizePool.captureAwardBalance();  // main winner gets all external tokens  address mainWinner = ticket.draw(randomNumber);  _awardAllExternalTokens(mainWinner);  address[] memory winners = new address[](__numberOfWinners);  winners[0] = mainWinner;  Recommendation  Implement consistent behavior. Avoid hiding error conditions and consider throwing an exception instead.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pooltogether-lootbox-and-multiplewinners-strategy/"
    },
    {
        "title": "5.14 Initialize implementations for proxy contracts and protect initialization methods ",
        "body": "  Description  Any situation where the implementation of proxy contracts can be initialized by third parties should be avoided. This can be the case if the initialize function is unprotected or not initialized immediately after deployment. Since the implementation contract is not meant to be used directly without a proxy delegate-calling to it, it is recommended to protect the initialization method of the implementation by initializing on deployment.  This affects all proxy implementations (the delegatecall target contract) deployed in the system.  Examples  The implementation for MultipleWinners is not initialized. Even though not directly used by the system it may be initialized by a third party.  code/pool/contracts/prize-strategy/multiple-winners/MultipleWinnersProxyFactory.sol:L13-L15  constructor () public {  instance = new MultipleWinners();  The deployed ERC721Contract is not initialized.  code/loot-box/contracts/ERC721ControlledFactory.sol:L25-L29  constructor () public {  erc721ControlledInstance = new ERC721Controlled();  erc721ControlledBytecode = MinimalProxyLibrary.minimalProxy(address(erc721ControlledInstance));  The deployed LootBox is not initialized.  code/loot-box/contracts/LootBoxController.sol:L28-L31  constructor () public {  lootBoxActionInstance = new LootBox();  lootBoxActionBytecode = MinimalProxyLibrary.minimalProxy(address(lootBoxActionInstance));  Recommendation  Initialize unprotected implementation contracts in the implementation s constructor. Protect initialization methods from being called by unauthorized parties or ensure that deployment of the proxy and initialization is performed in the same transaction.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pooltogether-lootbox-and-multiplewinners-strategy/"
    },
    {
        "title": "5.15 LootBox - transferEther should be internal ",
        "body": "  Description  LootBox.transferEther() can be internal as it is only called from LootBox.plunder() and the LootBox(proxy) instances are generally very short-living (created and destroyed within one transaction).  Examples  code/loot-box/contracts/LootBox.sol:L63-L67  function transferEther(address payable to, uint256 amount) public {  to.transfer(amount);  emit TransferredEther(to, amount);  Recommendation  Restrict transferEther() s visibility to internal.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pooltogether-lootbox-and-multiplewinners-strategy/"
    },
    {
        "title": "5.16 LootBox - executeCalls can be misused to relay calls ",
        "body": "  Description  Note: allows non-value and value calls (deposits can be forces via selfdestruct)  Examples  code/loot-box/contracts/LootBox.sol:L52-L58  function executeCalls(Call[] calldata calls) external returns (bytes[] memory) {  bytes[] memory response = new bytes[](calls.length);  for (uint256 i = 0; i < calls.length; i++) {  response[i] = _executeCall(calls[i].to, calls[i].value, calls[i].data);  return response;  Recommendation  Restrict access to call forwarding functionality to trusted entities. Consider implementing the Ownable pattern allowing access to functionality to the owner only.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/11/pooltogether-lootbox-and-multiplewinners-strategy/"
    },
    {
        "title": "4.1 FlasherFTM - Unsolicited invocation of the callback (CREAM auth bypass) ",
        "body": "  Description  TL;DR: Anyone can call ICTokenFlashloan(crToken).flashLoan(address(FlasherFTM), address(FlasherFTM), info.amount, params) directly and pass validation checks in onFlashLoan(). This call forces it to accept unsolicited flash loans and execute the actions provided under the attacker s FlashLoan.Info.  receiver.onFlashLoan(initiator, token, amount, ...) is called when receiving a flash loan. According to EIP-3156, the initiator is msg.sender so that one can use it to check if the call to receiver.onFlashLoan() was unsolicited or not.  Third-party Flash Loan provider contracts are often upgradeable.  For example, the Geist lending contract configured with this system is upgradeable. Upgradeable contracts bear the risk that one cannot assume that the contract is always running the same code. In the worst case, for example, a malicious proxy admin (leaked keys, insider, \u2026) could upgrade the contract and perform unsolicited calls with arbitrary data to Flash Loan consumers in an attempt to exploit them. It, therefore, is highly recommended to verify that flash loan callbacks in the system can only be called if the contract was calling out to the provider to provide a Flash Loan and that the conditions of the flash loan (returned data, amount) are correct.  Not all Flash Loan providers implement EIP-3156 correctly.  Cream Finance, for example, allows users to set an arbitrary initiator when requesting a flash loan. This deviates from EIP-3156 and was reported to the Cream development team as a security issue. Hence, anyone can spoof that initiator and potentially bypass authentication checks in the consumers  receiver.onFlashLoan(). Depending on the third-party application consuming the flash loan is doing with the funds, the impact might range from medium to critical with funds at risk. For example, projects might assume that the flash loan always originates from their trusted components, e.g., because they use them to refinance switching funds between pools or protocols.  Examples  The FlasherFTM contract assumes that flash loans for the Flasher can only be initiated by authorized callers (isAuthorized) - for a reason - because it is vital that the FlashLoan.Info calldata info parameter only contains trusted data:  code/contracts/fantom/flashloans/FlasherFTM.sol:L66-L79  /**  @dev Routing Function for Flashloan Provider  @param info: struct information for flashLoan  @param _flashnum: integer identifier of flashloan provider  /  function initiateFlashloan(FlashLoan.Info calldata info, uint8 _flashnum) external isAuthorized override {  if (_flashnum == 0) {  _initiateGeistFlashLoan(info);  } else if (_flashnum == 2) {  _initiateCreamFlashLoan(info);  } else {  revert(Errors.VL_INVALID_FLASH_NUMBER);  code/contracts/fantom/flashloans/FlasherFTM.sol:L46-L55  modifier isAuthorized() {  require(  msg.sender == _fujiAdmin.getController() ||  msg.sender == _fujiAdmin.getFliquidator() ||  msg.sender == owner(),  Errors.VL_NOT_AUTHORIZED  );  _;  The Cream Flash Loan initiation code requests the flash loan via ICTokenFlashloan(crToken).flashLoan(receiver=address(this), initiator=address(this), ...):  code/contracts/fantom/flashloans/FlasherFTM.sol:L144-L158  /**  @dev Initiates an CreamFinance flashloan.  @param info: data to be passed between functions executing flashloan logic  /  function _initiateCreamFlashLoan(FlashLoan.Info calldata info) internal {  address crToken = info.asset == _FTM  ? 0xd528697008aC67A21818751A5e3c58C8daE54696  : _crMappings.addressMapping(info.asset);  // Prepara data for flashloan execution  bytes memory params = abi.encode(info);  // Initialize Instance of Cream crLendingContract  ICTokenFlashloan(crToken).flashLoan(address(this), address(this), info.amount, params);  contracts/CCollateralCapErc20.sol:L187  address initiator,  code/contracts/fantom/flashloans/FlasherFTM.sol:L162-L175  Recommendation  /  function onFlashLoan(  address sender,  address underlying,  uint256 amount,  uint256 fee,  bytes calldata params  ) external override returns (bytes32) {  // Check Msg. Sender is crToken Lending Contract  // from IronBank because ETH on Cream cannot perform a flashloan  address crToken = underlying == _WFTM  ? 0xd528697008aC67A21818751A5e3c58C8daE54696  : _crMappings.addressMapping(underlying);  require(msg.sender == crToken && address(this) == sender, Errors.VL_NOT_AUTHORIZED);  Recommendation  Cream Finance  We ve reached out to the Cream developer team, who have confirmed the issue. They are planning to implement countermeasures. Our recommendation can be summarized as follows:  Implement the EIP-3156 compliant version of flashLoan() with initiator hardcoded to msg.sender.  FujiDAO (and other flash loan consumers)  We recommend not assuming that FlashLoan.Info contains trusted or even validated data when a third-party flash loan provider provides it! Developers should ensure that the data received was provided when the flash loan was requested.  The contract should reject unsolicited flash loans. In the scenario where a flash loan provider is exploited, the risk of an exploited trust relationship is less likely to spread to the rest of the system.  The Cream initiator provided to the onFlashLoan() callback cannot be trusted until the Cream developers fix this issue. The initiator can easily be spoofed to perform unsolicited flash loans. We, therefore, suggest:  Validate that the initiator value is the flashLoan() caller. This conforms to the standard and is hopefully how the Cream team is fixing this, and  Ensure the implementation tracks its own calls to flashLoan() in a state-variable semaphore, i.e. store the flash loan data/hash in a temporary state-variable that is only set just before calling flashLoan() until being called back in onFlashLoan(). The received data can then be verified against the stored artifact. This is a safe way of authenticating and verifying callbacks.  Values received from untrusted third parties should always be validated with the utmost scrutiny.  Smart contract upgrades are risky, so we recommend implementing the means to pause certain flash loan providers.  Ensure that flash loan handler functions should never re-enter the system. This provides additional security guarantees in case a flash loan provider gets breached.  Note: The Fuji development team implemented a hotfix to prevent unsolicited calls from Cream by storing the hash(FlashLoan.info) in a state variable just before requesting the flash loan. Inside the onFlashLoan callback, this state is validated and cleared accordingly.  An improvement to this hotfix would be, to check _paramsHash before any external calls are made and clear it right after validation at the beginning of the function. Additionally, hash==0x0 should be explicitly disallowed. By doing so, the check also serves as a reentrancy guard and helps further reduce the risk of a potentially malicious flash loan re-entering the function.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.2 Lack of reentrancy protection in token interactions ",
        "body": "  Description  Therefore, it is crucial to strictly adhere to the checks-effects pattern and safeguard affected methods using a mutex.  Examples  code/contracts/fantom/libraries/LibUniversalERC20FTM.sol:L26-L40  function univTransfer(  IERC20 token,  address payable to,  uint256 amount  ) internal {  if (amount > 0) {  if (isFTM(token)) {  (bool sent, ) = to.call{ value: amount }(\"\");  require(sent, \"Failed to send Ether\");  } else {  token.safeTransfer(to, amount);  withdraw is nonReentrant while paybackAndWithdraw is not, which appears to be inconsistent  code/contracts/fantom/FujiVaultFTM.sol:L172-L182  /**  @dev Paybacks the underlying asset and withdraws collateral in a single function call from activeProvider  @param _paybackAmount: amount of underlying asset to be payback, pass -1 to pay full amount  @param _collateralAmount: amount of collateral to be withdrawn, pass -1 to withdraw maximum amount  /  function paybackAndWithdraw(int256 _paybackAmount, int256 _collateralAmount) external payable {  updateF1155Balances();  _internalPayback(_paybackAmount);  _internalWithdraw(_collateralAmount);  code/contracts/fantom/FujiVaultFTM.sol:L232-L241  /**  @dev Paybacks Vault's type underlying to activeProvider - called by users  @param _repayAmount: token amount of underlying to repay, or  pass any 'negative number' to repay full ammount  Emits a {Repay} event.  /  function payback(int256 _repayAmount) public payable override {  updateF1155Balances();  _internalPayback(_repayAmount);  depositAndBorrow is not nonReentrant while borrow() is which appears to be inconsistent  code/contracts/fantom/FujiVaultFTM.sol:L161-L171  /**  @dev Deposits collateral and borrows underlying in a single function call from activeProvider  @param _collateralAmount: amount to be deposited  @param _borrowAmount: amount to be borrowed  /  function depositAndBorrow(uint256 _collateralAmount, uint256 _borrowAmount) external payable {  updateF1155Balances();  _internalDeposit(_collateralAmount);  _internalBorrow(_borrowAmount);  code/contracts/fantom/FujiVaultFTM.sol:L222-L230  /**  @dev Borrows Vault's type underlying amount from activeProvider  @param _borrowAmount: token amount of underlying to borrow  Emits a {Borrow} event.  /  function borrow(uint256 _borrowAmount) public override nonReentrant {  updateF1155Balances();  _internalBorrow(_borrowAmount);  depositAndBorrow  updateBalances  internalDeposit ->  ERC777(collateralAsset).safeTransferFrom()  ---> calls back!  ---callback:beforeTokenTransfer---->  !! depositAndBorrow  updateBalances  internalDeposit  --> ERC777.safeTransferFrom()  <--  _deposit  mint  internalBorrow  mint  _borrow  ERC777(borrowAsset).univTransfer(msg.sender) --> might call back  <-------------------------------  _deposit  mint  internalBorrow  mint  _borrow  --> ERC777(borrowAsset).univTransfer(msg.sender) --> might call back  <--  Recommendation  Consider decorating methods that may call back to untrusted sources (i.e., native token transfers, callback token operations) as nonReentrant and strictly follow the checks-effects pattern for all contracts in the code-base.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.3 Lack of segregation of duties, excessive owner permissions, misleading authentication modifiers ",
        "body": "  Descriptio  In the FujiERC1155 contract, the onlyPermit modifier should not include owner.  code/contracts/abstracts/fujiERC1155/F1155Manager.sol:L34-L37  modifier onlyPermit() {  require(addrPermit[_msgSender()] || msg.sender == owner(), Errors.VL_NOT_AUTHORIZED);  _;  However, the owner can also wholly mess up accounting as they are permitted to call updateState(), which should only be callable by vaults:  code/contracts/FujiERC1155.sol:L53-L59  function updateState(uint256 _assetID, uint256 newBalance) external override onlyPermit {  uint256 total = totalSupply(_assetID);  if (newBalance > 0 && total > 0 && newBalance > total) {  uint256 newIndex = (indexes[_assetID] * newBalance) / total;  indexes[_assetID] = uint128(newIndex);  The same is true for FujiERC1155.{mint|mintBatch|burn|burnBatch|addInitializeAsset} unless there is a reason to allow owner to freely burn/mint/initialize tokens and updateState for borrowed assets to arbitrary values.  FujiVault - owner is part of isAuthorized and can change the system out-of-band. controller does not implement means to call functions it has permissions to.  Multiple methods in FujiVault are decorated with the access control isAuthorized that grants the owner and the currently configured controller access. The controller, however, does not implement any means to call some of the methods on the Vault.  Furthermore, the owner is part of isAuthorized, too, and can switch out the debt-management token while one is already configured without any migration. This is likely to create an inconsistent state with the Vault, and no one will be able to withdraw their now non-existent token.  code/contracts/fantom/FujiVaultFTM.sol:L65-L74  /**  @dev Throws if caller is not the 'owner' or the '_controller' address stored in {FujiAdmin}  /  modifier isAuthorized() {  require(  msg.sender == owner() || msg.sender == _fujiAdmin.getController(),  Errors.VL_NOT_AUTHORIZED  );  _;  The owner can call methods  out of band,  bypassing steps the contract system would enforce otherwise, e.g. controller calling setActiveProvider.  It is assumed that setOracle, setFactor should probably be onlyOwner instead.  code/contracts/fantom/FujiVaultFTM.sol:L354-L367  function setFujiERC1155(address _fujiERC1155) external isAuthorized {  require(_fujiERC1155 != address(0), Errors.VL_ZERO_ADDR);  fujiERC1155 = _fujiERC1155;  vAssets.collateralID = IFujiERC1155(_fujiERC1155).addInitializeAsset(  IFujiERC1155.AssetType.collateralToken,  address(this)  );  vAssets.borrowID = IFujiERC1155(_fujiERC1155).addInitializeAsset(  IFujiERC1155.AssetType.debtToken,  address(this)  );  emit F1155Changed(_fujiERC1155);  Note ensure that setProviders can only ever be set by a trusted entity or multi-sig as the Vault delegatecalls the provider logic (via VaultControlUpgradeable) and, hence, the provider has total control over the Vault storage!  FliquidatorFTM - Unnecessary and confusing modifier FliquidatorFTM.isAuthorized  The contract is already Claimable; therefore, use the already existing modifier Claimable.onlyOwner instead.  code/contracts/fantom/FliquidatorFTM.sol:L86-L91  code/contracts/abstracts/claimable/Claimable.sol:L48-L51  /  modifier isAuthorized() {  require(msg.sender == owner(), Errors.VL_NOT_AUTHORIZED);  _;  code/contracts/abstracts/claimable/Claimable.sol:L48-L51  modifier onlyOwner() {  require(_msgSender() == owner(), \"Ownable: caller is not the owner\");  _;  Use Claimable.onlyOwner instead.  FlasherFTM - owner should not be able to call initiateFlashloan directly; misleading comment.  code/contracts/fantom/flashloans/FlasherFTM.sol:L42-L54  /**  @dev Throws if caller is not 'owner'.  /  modifier isAuthorized() {  require(  msg.sender == _fujiAdmin.getController() ||  msg.sender == _fujiAdmin.getFliquidator() ||  msg.sender == owner(),  Errors.VL_NOT_AUTHORIZED  );  _;  FujiERC1155 - All vaults have equal permission to mint/burn/initializeAssets for every vault  All vaults need to be in the onlyPermit ACL whitelist. No additional checks enforce that the calling vault can only modify its token balances. Furthermore, FujiVaultFTM is upgradeable; thus, the contract logic may be altered to allow the vault to modify any other token id s balance. To reduce this risk and the potential of an exploited contract affecting other token balances in the system, it is suggested to change the coarse onlyPermit ACL to one that checks that the calling vault can only manage their token IDs.  Recommendation  Reconsider the authentication concept and make it more transparent. Segregate duties and clearly define roles and capabilities. Avoid having overly powerful actors and reduce their capabilities to the bare minimum needed to segregate risk. If an actor is part of an ACL in a third-party contract, they also should have the means to call that method in a controlled way or else remove them from the ACL. To avoid conveying a false sense of trust towards certain actors within the smart contract system, it is suggested to use the centralized onlyOwner decorator for methods only the owner can call. This more accurately depicts  who can do what  in the system and makes it easier to trust the project team managing it.  Avoid excessively powerful owners that can change/mint/burn anything in the system as this is a risk for the general consistency.  Remove owner from methods/modifiers they don t need to be part of/have access to.  Ensure owner is a time-locked multi-sig or governance contract. Rename authentication modifiers to describe better what callers they allow.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.4 Unchecked Return Values - ICErc20 repayBorrow ",
        "body": "  Description  ICErc20.repayBorrow returns a non-zero uint on error. Multiple providers do not check for this error condition and might return success even though repayBorrow failed, returning an error code.  This can potentially allow a malicious user to call paybackAndWithdraw() while not repaying by causing an error in the sub-call to Compound.repayBorrow(), which ends up being silently ignored. Due to the missing success condition check, execution continues normally with _internalWithdraw().  Also, see issue 4.5.  code/contracts/interfaces/compound/ICErc20.sol:L11-L12  function repayBorrow(uint256 repayAmount) external returns (uint256);  The method may return an error due to multiple reasons:  contracts/CToken.sol:L808-L816  function repayBorrowInternal(uint repayAmount) internal nonReentrant returns (uint, uint) {  uint error = accrueInterest();  if (error != uint(Error.NO_ERROR)) {  // accrueInterest emits logs on errors, but we still want to log the fact that an attempted borrow failed  return (fail(Error(error), FailureInfo.REPAY_BORROW_ACCRUE_INTEREST_FAILED), 0);  // repayBorrowFresh emits repay-borrow-specific logs on errors, so we don't need to  return repayBorrowFresh(msg.sender, msg.sender, repayAmount);  contracts/CToken.sol:L855-L873  if (allowed != 0) {  return (failOpaque(Error.COMPTROLLER_REJECTION, FailureInfo.REPAY_BORROW_COMPTROLLER_REJECTION, allowed), 0);  /* Verify market's block number equals current block number */  if (accrualBlockNumber != getBlockNumber()) {  return (fail(Error.MARKET_NOT_FRESH, FailureInfo.REPAY_BORROW_FRESHNESS_CHECK), 0);  RepayBorrowLocalVars memory vars;  /* We remember the original borrowerIndex for verification purposes */  vars.borrowerIndex = accountBorrows[borrower].interestIndex;  /* We fetch the amount the borrower owes, with accumulated interest */  (vars.mathErr, vars.accountBorrows) = borrowBalanceStoredInternal(borrower);  if (vars.mathErr != MathError.NO_ERROR) {  return (failOpaque(Error.MATH_ERROR, FailureInfo.REPAY_BORROW_ACCUMULATED_BALANCE_CALCULATION_FAILED, uint(vars.mathErr)), 0);  Examples  Multiple providers, here are some examples:  code/contracts/fantom/providers/ProviderCream.sol:L168-L173  // Check there is enough balance to pay  require(erc20token.balanceOf(address(this)) >= _amount, \"Not-enough-token\");  erc20token.univApprove(address(cyTokenAddr), _amount);  cyToken.repayBorrow(_amount);  code/contracts/fantom/providers/ProviderScream.sol:L170-L172  require(erc20token.balanceOf(address(this)) >= _amount, \"Not-enough-token\");  erc20token.univApprove(address(cyTokenAddr), _amount);  cyToken.repayBorrow(_amount);  code/contracts/mainnet/providers/ProviderCompound.sol:L139-L155  if (_isETH(_asset)) {  // Create a reference to the corresponding cToken contract  ICEth cToken = ICEth(cTokenAddr);  cToken.repayBorrow{ value: msg.value }();  } else {  // Create reference to the ERC20 contract  IERC20 erc20token = IERC20(_asset);  // Create a reference to the corresponding cToken contract  ICErc20 cToken = ICErc20(cTokenAddr);  // Check there is enough balance to pay  require(erc20token.balanceOf(address(this)) >= _amount, \"Not-enough-token\");  erc20token.univApprove(address(cTokenAddr), _amount);  cToken.repayBorrow(_amount);  Recommendation  Check for cyToken.repayBorrow(_amount) != 0 or Error.NO_ERROR.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.5 Unchecked Return Values - IComptroller exitMarket, enterMarket ",
        "body": "  Description  IComptroller.exitMarket(), IComptroller.enterMarkets() may return a non-zero uint on error but none of the Providers check for this error condition. Together with issue 4.10, this might suggest that unchecked return values may be a systemic problem.  Here s the upstream implementation:  contracts/Comptroller.sol:L179-L187  if (amountOwed != 0) {  return fail(Error.NONZERO_BORROW_BALANCE, FailureInfo.EXIT_MARKET_BALANCE_OWED);  /* Fail if the sender is not permitted to redeem all of their tokens */  uint allowed = redeemAllowedInternal(cTokenAddress, msg.sender, tokensHeld);  if (allowed != 0) {  return failOpaque(Error.REJECTION, FailureInfo.EXIT_MARKET_REJECTION, allowed);  /**  @notice Removes asset from sender's account liquidity calculation  @dev Sender must not have an outstanding borrow balance in the asset,  or be providing necessary collateral for an outstanding borrow.  @param cTokenAddress The address of the asset to be removed  @return Whether or not the account successfully exited the market  /  function exitMarket(address cTokenAddress) external returns (uint) {  CToken cToken = CToken(cTokenAddress);  /* Get sender tokensHeld and amountOwed underlying from the cToken */  (uint oErr, uint tokensHeld, uint amountOwed, ) = cToken.getAccountSnapshot(msg.sender);  require(oErr == 0, \"exitMarket: getAccountSnapshot failed\"); // semi-opaque error code  /* Fail if the sender has a borrow balance */  if (amountOwed != 0) {  return fail(Error.NONZERO_BORROW_BALANCE, FailureInfo.EXIT_MARKET_BALANCE_OWED);  /* Fail if the sender is not permitted to redeem all of their tokens */  uint allowed = redeemAllowedInternal(cTokenAddress, msg.sender, tokensHeld);  if (allowed != 0) {  return failOpaque(Error.REJECTION, FailureInfo.EXIT_MARKET_REJECTION, allowed);  Examples  Unchecked return value exitMarket  All Providers exhibit the same issue, probably due to code reuse. (also see https://github.com/ConsenSysDiligence/fuji-protocol-audit-2022-02/issues/19). Some examples:  code/contracts/fantom/providers/ProviderCream.sol:L52-L57  function _exitCollatMarket(address _cyTokenAddress) internal {  // Create a reference to the corresponding network Comptroller  IComptroller comptroller = IComptroller(_getComptrollerAddress());  comptroller.exitMarket(_cyTokenAddress);  code/contracts/fantom/providers/ProviderScream.sol:L52-L57  function _exitCollatMarket(address _cyTokenAddress) internal {  // Create a reference to the corresponding network Comptroller  IComptroller comptroller = IComptroller(_getComptrollerAddress());  comptroller.exitMarket(_cyTokenAddress);  code/contracts/mainnet/providers/ProviderCompound.sol:L46-L51  function _exitCollatMarket(address _cTokenAddress) internal {  // Create a reference to the corresponding network Comptroller  IComptroller comptroller = IComptroller(_getComptrollerAddress());  comptroller.exitMarket(_cTokenAddress);  code/contracts/mainnet/providers/ProviderIronBank.sol:L52-L57  function _exitCollatMarket(address _cyTokenAddress) internal {  // Create a reference to the corresponding network Comptroller  IComptroller comptroller = IComptroller(_getComptrollerAddress());  comptroller.exitMarket(_cyTokenAddress);  Unchecked return value enterMarkets (Note that IComptroller returns NO_ERROR when already joined to enterMarkets.  All Providers exhibit the same issue, probably due to code reuse. (also see https://github.com/ConsenSysDiligence/fuji-protocol-audit-2022-02/issues/19). For example:  code/contracts/fantom/providers/ProviderCream.sol:L39-L46  function _enterCollatMarket(address _cyTokenAddress) internal {  // Create a reference to the corresponding network Comptroller  IComptroller comptroller = IComptroller(_getComptrollerAddress());  address[] memory cyTokenMarkets = new address[](1);  cyTokenMarkets[0] = _cyTokenAddress;  comptroller.enterMarkets(cyTokenMarkets);  Recommendation  Require that return value is ERROR.NO_ERROR or 0.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.6 Fliquidator - excess funds of native tokens are not returned ",
        "body": "  Description  Examples  code/contracts/fantom/FliquidatorFTM.sol:L148-L150  if (vAssets.borrowAsset == FTM) {  require(msg.value >= debtTotal, Errors.VL_AMOUNT_ERROR);  } else {  Recommendation  Consider returning excess funds. Consider making _constructParams public to allow the caller to pre-calculate the debtTotal that needs to be provided with the call.  Consider removing support for native token FTM entirely to reduce the overall code complexity. The wrapped equivalent can be used instead.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.7 Unsafe arithmetic casts ",
        "body": "  Description  The reason for using signed integers in some situations appears to be to use negative values as an indicator to withdraw everything. Using a whole bit of uint256 for this is quite a lot when using type(uint256).max would equal or better serve as a flag to withdraw everything.  Furthermore, even though the code uses solidity 0.8.x, which safeguards arithmetic operations against under/overflows, arithmetic typecast is not protected.  Also, see issue 4.9 for a related issue.  \u21d2  solidity-shell  \ud83d\ude80 Entering interactive Solidity ^0.8.11 shell. '.help' and '.exit' are your friends.  \u00bb  \u2139\ufe0f  ganache-mgr: starting temp. ganache instance ...  \u00bb  uint(int(-100))  115792089237316195423570985008687907853269984665640564039457584007913129639836  \u00bb  int256(uint(2**256-100))  100  Examples  code/contracts/fantom/FliquidatorFTM.sol:L167-L178  // Compute how much collateral needs to be swapt  uint256 collateralInPlay = _getCollateralInPlay(  vAssets.collateralAsset,  vAssets.borrowAsset,  debtTotal + bonus  );  // Burn f1155  _burnMulti(addrs, borrowBals, vAssets, _vault, f1155);  // Withdraw collateral  IVault(_vault).withdrawLiq(int256(collateralInPlay));  code/contracts/fantom/FliquidatorFTM.sol:L264-L276  // Compute how much collateral needs to be swapt for all liquidated users  uint256 collateralInPlay = _getCollateralInPlay(  vAssets.collateralAsset,  vAssets.borrowAsset,  _amount + _flashloanFee + bonus  );  // Burn f1155  _burnMulti(_addrs, _borrowBals, vAssets, _vault, f1155);  // Withdraw collateral  IVault(_vault).withdrawLiq(int256(collateralInPlay));  code/contracts/fantom/FliquidatorFTM.sol:L334-L334  uint256 amount = _amount < 0 ? debtTotal : uint256(_amount);  code/contracts/fantom/FujiVaultFTM.sol:L213-L220  function withdrawLiq(int256 _withdrawAmount) external override nonReentrant onlyFliquidator {  // Logic used when called by Fliquidator  _withdraw(uint256(_withdrawAmount), address(activeProvider));  IERC20Upgradeable(vAssets.collateralAsset).univTransfer(  payable(msg.sender),  uint256(_withdrawAmount)  );  pot. unsafe truncation (unlikely)  code/contracts/FujiERC1155.sol:L53-L59  function updateState(uint256 _assetID, uint256 newBalance) external override onlyPermit {  uint256 total = totalSupply(_assetID);  if (newBalance > 0 && total > 0 && newBalance > total) {  uint256 newIndex = (indexes[_assetID] * newBalance) / total;  indexes[_assetID] = uint128(newIndex);  Recommendation  If negative values are only used as a flag to indicate that all funds should be used for an operation, use type(uint256).max instead. It is wasting less value-space for a simple flag than using the uint256 high-bit range. Avoid typecast where possible. Use SafeCast instead or verify that the casts are safe because the values they operate on cannot under- or overflow. Add inline code comments if that s the case.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.8 Missing input validation on flash close fee factors ",
        "body": "  Description  The FliquidatorFTM contract allows authorized parties to set the flash close fee factor. The factor is provided as two integers denoting numerator and denominator. Due to a lack of boundary checks, it is possible to set unrealistically high factors, which go well above 1. This can have unexpected effects on internal accounting and the impact of flashloan balances.  Examples  code/contracts/fantom/FliquidatorFTM.sol:L657-L659  function setFlashCloseFee(uint64 _newFactorA, uint64 _newFactorB) external isAuthorized {  flashCloseF.a = _newFactorA;  flashCloseF.b = _newFactorB;  Recommendation  Add a requirement making sure that flashCloseF.a <= flashCloseF.b.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.9 Separation of concerns and consistency in vaults ",
        "body": "  Description  The FujiVaultFTM contract contains multiple balance-changing functions. Most notably, withdraw is passed an int256 denoted amount parameter. Negative values of this parameter are given to the _internalWithdraw function, where they trigger the withdrawal of all collateral. This approach can result in accounting mistakes in the future as beyond a certain point in the vault s accounting; amounts are expected to be only positive. Furthermore, the concerns of withdrawing and entirely withdrawing are not separated.  The above issue applies analogously to the payback function and its dependency on _internalPayback.  For consistency, withdrawLiq also takes an int256 amount parameter. This function is only accessible to the Fliquidator contract and withdraws collateral from the active provider. However, all occurrences of the _withdrawAmount parameter are cast to uint256.  Examples  The withdraw entry point:  code/contracts/fantom/FujiVaultFTM.sol:L201-L204  function withdraw(int256 _withdrawAmount) public override nonReentrant {  updateF1155Balances();  _internalWithdraw(_withdrawAmount);  _internalWithdraw s negative amount check:  code/contracts/fantom/FujiVaultFTM.sol:L654-L657  uint256 amountToWithdraw = _withdrawAmount < 0  ? providedCollateral - neededCollateral  : uint256(_withdrawAmount);  The withdrawLiq entry point for the Fliquidator:  code/contracts/fantom/FujiVaultFTM.sol:L213-L220  function withdrawLiq(int256 _withdrawAmount) external override nonReentrant onlyFliquidator {  // Logic used when called by Fliquidator  _withdraw(uint256(_withdrawAmount), address(activeProvider));  IERC20Upgradeable(vAssets.collateralAsset).univTransfer(  payable(msg.sender),  uint256(_withdrawAmount)  );  Recommendation  Similarly, withdrawLiq s parameter should be a uint256 to prevent unnecessary casts.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.10 Aave/Geist Interface declaration mismatch and unchecked return values ",
        "body": "  Description  The two lending providers, Geist & Aave, do not seem to be directly affiliated even though one is a fork of the other. However, the interfaces may likely diverge in the future. Using the same interface declaration for both protocols might become problematic with future upgrades to either protocol. The interface declaration does not seem to come from the original upstream project. The interface IAaveLendingPool does not declare any return values while some of the functions called in Geist or Aave return them.  Note: that we have not verified all interfaces for correctness. However, we urge the client to only use official interface declarations from the upstream projects and verify that all other interfaces match.  Examples  The ILendingPool configured in ProviderAave (0xB53C1a33016B2DC2fF3653530bfF1848a515c8c5 -> implementation: 0xc6845a5c768bf8d7681249f8927877efda425baf)  code/contracts/mainnet/providers/ProviderAave.sol:L19-L21  function _getAaveProvider() internal pure returns (IAaveLendingPoolProvider) {  return IAaveLendingPoolProvider(0xB53C1a33016B2DC2fF3653530bfF1848a515c8c5);  The IAaveLendingPool does not declare return values for any function, while upstream does.  code/contracts/interfaces/aave/IAaveLendingPool.sol:L1-L46  // SPDX-License-Identifier: MIT  pragma solidity ^0.8.0;  interface IAaveLendingPool {  function flashLoan(  address receiverAddress,  address[] calldata assets,  uint256[] calldata amounts,  uint256[] calldata modes,  address onBehalfOf,  bytes calldata params,  uint16 referralCode  ) external;  function deposit(  address _asset,  uint256 _amount,  address _onBehalfOf,  uint16 _referralCode  ) external;  function withdraw(  address _asset,  uint256 _amount,  address _to  ) external;  function borrow(  address _asset,  uint256 _amount,  uint256 _interestRateMode,  uint16 _referralCode,  address _onBehalfOf  ) external;  function repay(  address _asset,  uint256 _amount,  uint256 _rateMode,  address _onBehalfOf  ) external;  function setUserUseReserveAsCollateral(address _asset, bool _useAsCollateral) external;  Methods: withdraw(), repay() return uint256 in the original implementation for Aave, see:  https://etherscan.io/address/0xc6845a5c768bf8d7681249f8927877efda425baf#code  The ILendingPool configured for Geist:  Methods withdraw(), repay() return uint256 in the original implementation for Geist, see:  https://ftmscan.com/address/0x3104ad2aadb6fe9df166948a5e3a547004862f90#code  Note: that the actual amount withdrawn does not necessarily need to match the amount provided with the function argument. Here s an excerpt of the upstream LendingProvider.withdraw():  ...  if (amount == type(uint256).max) {  amountToWithdraw = userBalance;  ...  return amountToWithdraw;  And here s the code in Fuji that calls that method. This will break the withdrawAll functionality of LendingProvider if token isFTM.  code/contracts/fantom/providers/ProviderGeist.sol:L151-L165  function withdraw(address _asset, uint256 _amount) external payable override {  IAaveLendingPool aave = IAaveLendingPool(_getAaveProvider().getLendingPool());  bool isFtm = _asset == _getFtmAddr();  address _tokenAddr = isFtm ? _getWftmAddr() : _asset;  aave.withdraw(_tokenAddr, _amount, address(this));  // convert WFTM to FTM  if (isFtm)  {  address unwrapper = _getUnwrapper();  IERC20(_tokenAddr).univTransfer(payable(unwrapper), _amount);  IUnwrapper(unwrapper).withdraw(_amount);  Similar for repay(), which returns the actual amount repaid.  Recommendation  Always use the original interface unless only a minimal subset of functions is used.  Use the original upstream interfaces of the corresponding project (link via the respective npm packages if available).  Avoid omitting parts of the function declaration! Especially when it comes to return values.  Check return values. Use the value returned from withdraw() AND repay()  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.11 Missing slippage protection for rewards swap ",
        "body": "  Description  In FujiVaultFTM.harvestRewards a swap transaction is generated using a call to SwapperFTM.getSwapTransaction. In all relevant scenarios, this call uses a minimum output amount of zero, which de-facto deactivates slippage checks. Most values from harvesting rewards can thus be siphoned off by sandwiching such calls.  Examples  amountOutMin is 0, effectively disabling slippage control in the swap method.  code/contracts/fantom/SwapperFTM.sol:L49-L55  transaction.data = abi.encodeWithSelector(  IUniswapV2Router01.swapExactETHForTokens.selector,  0,  path,  msg.sender,  type(uint256).max  );  Only success required  code/contracts/fantom/FujiVaultFTM.sol:L565-L567  // Swap rewards -> collateralAsset  (success, ) = swapTransaction.to.call{ value: swapTransaction.value }(swapTransaction.data);  require(success, \"failed to swap rewards\");  Recommendation  Use a slippage check such as for liquidator swaps:  code/contracts/fantom/FliquidatorFTM.sol:L476-L479  require(  (priceDelta * SLIPPAGE_LIMIT_DENOMINATOR) / priceFromOracle < SLIPPAGE_LIMIT_NUMERATOR,  Errors.VL_SWAP_SLIPPAGE_LIMIT_EXCEED  );  Or specify a non-zero amountOutMin argument in calls to IUniswapV2Router01.swapExactETHForTokens.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.12 Unpredictable behavior due to admin front running or general bad timing ",
        "body": "  Description  In several cases, the owner of deployed contracts can update or upgrade things in the system without warning. This has the potential to violate a security goal of the system.  Specifically, contract owners (a 2/3 EOA Gnosis Multisig) could use front running to make malicious changes just ahead of incoming transactions, or purely accidental adverse effects could occur due to unfortunate timing of changes.  Some instances of this are more important than others, but in general, users of the system should have assurances about the behavior of the action they re about to take.  Examples  FujiAdmin  The owner of FujiAdmin is 0x0e1484c9a9f9b31ff19300f082e843415a575f4f and this address is a proxy to a Gnosis Safe: Mastercopy 1.2.0 implementation, requiring 2/3 signatures to execute transactions. All three signees are EOA s.  code/artifacts/1-core.deploy:L958-L960  \"FujiAdmin\": {  \"address\": \"0x4cB46032e2790D8CA10be6d0001e8c6362a76adA\",  \"abi\": [  Controller, FujiOracle  The owner of controller seems to be a single EOA:  https://etherscan.io/address/0x3f366802F4e7576FC5DAA82890Cc6e04c85f3736#readContract  The owner of FujiOracle seems to be a single EOA:  https://etherscan.io/address/0xadF849079d415157CbBdb21BB7542b47077734A8#readContract  The owner of FujiERC1155 seems to be a single EOA:  https://etherscan.io/address/0xa2d62f8b02225fbFA1cf8bF206C8106bDF4c692b#readProxyContract  FujiAdmin (fantom)  Deployer is 0xb98d4D4e205afF4d4755E9Df19BD0B8BD4e0f148 which is an EOA.  code/artifacts/250-core.deploy:L1-L5  \"FujiAdmin\": {  \"address\": \"0xaAb2AAfBFf7419Ff85181d3A846bA9045803dd67\",  \"deployer\": \"0xb98d4D4e205afF4d4755E9Df19BD0B8BD4e0f148\",  \"abi\": [  FujiAdmin.owner is 0x40578f7902304e0e34d7069fb487ee57f841342e which is a GnosisSafeProxy  Recommendation  The underlying issue is that users of the system can t be sure what the behavior of a function call will be, and this is because the behavior can change at any time.  We recommend giving the user advance notice of changes with a time lock. For example, all onlyOwner functionality requires two steps with a mandatory time window between them. The first step merely tells users that a particular change is coming, and the second step commits that change after a reasonable waiting period.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.13 FujiOracle - _getUSDPrice does not detect stale oracle prices; General Oracle Risks ",
        "body": "  Description  The external Chainlink oracle, which provides index price information to the system, introduces risk inherent to any dependency on third-party data sources. For example, the oracle could fall behind or otherwise fail to be maintained, resulting in outdated data being fed to the index price calculations. Oracle reliance has historically resulted in crippled on-chain systems, and complications that lead to these outcomes can arise from things as simple as network congestion.  This is more extreme in lesser-known tokens with fewer ChainLink Price feeds to update the price frequently.  Ensuring that unexpected oracle return values are correctly handled will reduce reliance on off-chain components and increase the resiliency of the smart contract system that depends on them.  The codebase, as is, relies on chainLinkOracle.latestRoundData() and does not check the timestamp or answeredIn round of the returned price.  Examples  Here s how the oracle is consumed, skipping any fields that would allow checking for stale data:  code/contracts/FujiOracle.sol:L66-L77  /**  @dev Calculates the USD price of asset.  @param _asset: the asset address.  Returns the USD price of the given asset  /  function _getUSDPrice(address _asset) internal view returns (uint256 price) {  require(usdPriceFeeds[_asset] != address(0), Errors.ORACLE_NONE_PRICE_FEED);  (, int256 latestPrice, , , ) = AggregatorV3Interface(usdPriceFeeds[_asset]).latestRoundData();  price = uint256(latestPrice);  Here s the implementation of the v0.6 FluxAggregator Chainlink feed with a note that timestamps should be checked.  contracts/src/v0.6/FluxAggregator.sol:L489-L490  Recommendation  @return updatedAt is the timestamp when the round last was updated (i.e.  answer was last computed)  Recommendation  Perform sanity checks on the price returned by the oracle. If the price is older, not within configured limits, revert or handle in other means.  The oracle does not provide any means to remove a potentially broken price-feed (e.g., by updating its address to address(0) or by pausing specific feeds or the complete oracle). The only way to pause an oracle right now is to deploy a new oracle contract. Therefore, consider adding minimally invasive functionality to pause the price-feeds if the oracle becomes unreliable.  Monitor the oracle data off-chain and intervene if it becomes unreliable.  On-chain, realistically, both answeredInRound and updatedAt must be checked within acceptable bounds.  answeredInRound == latestRound - in this case, data may be assumed to be fresh while it might not be because the feed was entirely abandoned by nodes (no one starting a new round). Also, there s a good chance that many feeds won t always be super up-to-date (it might be acceptable to allow a threshold). A strict check might lead to transactions failing (race; e.g., round just timed out).  roundId + threshold >= answeredInRound - would allow a deviation of threshold rounds. This check alone might still result in stale data to be used if there are no more rounds. Therefore, this should be combined with updatedAt + threshold >= block.timestamp.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.14 Unclaimed or front-runnable proxy implementations ",
        "body": "  Description  Various smart contracts in the system require initialization functions to be called. The point when these calls happen is up to the deploying address. Deployment and initialization in one transaction are typically safe, but it can potentially be front-run if the initialization is done in a separate transaction.  A frontrunner can call these functions to silently take over the contracts and provide malicious parameters or plant a backdoor during the deployment.  Leaving proxy implementations uninitialized further aides potential phishing attacks where users might claim that - just because a contract address is listed in the official documentation/code-repo - a contract is a legitimate component of the system. At the same time, it is  only  a proxy implementation that an attacker claimed. For the end-user, it might be hard to distinguish whether this contract is part of the system or was a maliciously appropriated implementation.  Examples  code/contracts/mainnet/FujiVault.sol:L97-L102  function initialize(  address _fujiadmin,  address _oracle,  address _collateralAsset,  address _borrowAsset  ) external initializer {  FujiVault was initialized many days after deployment, and FujiVault inherits VaultBaseUpgradeable, which exposes a delegatecall that can be used to selfdestruct the contract s implementation.  Another FujiVault was deployed by deployer initialized in a 2-step approach that can theoretically silently be front-run.  code/artifacts/250-core.deploy:L2079-L2079  \"deployer\": \"0xb98d4D4e205afF4d4755E9Df19BD0B8BD4e0f148\",  Transactions of deployer:  https://ftmscan.com/txs?a=0xb98d4D4e205afF4d4755E9Df19BD0B8BD4e0f148&p=2  The specific contract was initialized 19 blocks after deployment.  https://ftmscan.com/address/0x8513c2db99df213887f63300b23c6dd31f1d14b0  FujiAdminFTM (and others) don t seem to be initialized. (low prior; no risk other than pot. reputational damage)  code/artifacts/250-core.deploy:L1-L7  \"FujiAdmin\": {  \"address\": \"0xaAb2AAfBFf7419Ff85181d3A846bA9045803dd67\",  \"deployer\": \"0xb98d4D4e205afF4d4755E9Df19BD0B8BD4e0f148\",  \"abi\": [  \"anonymous\": false,  Recommendation  It is recommended to use constructors wherever possible to immediately initialize proxy implementations during deploy-time. The code is only run when the implementation is deployed and affects the proxy initializations. If other initialization functions are used, we recommend enforcing deployer access restrictions or a standardized, top-level initialized boolean, set to true on the first deployment and used to prevent future initialization.  Using constructors and locked-down initialization functions will significantly reduce potential developer errors and the possibility of attackers re-initializing vital system components.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.15 Unused Import ",
        "body": "  Description  The following dependency is imported but never used:  code/contracts/mainnet/flashloans/Flasher.sol:L13-L13  import \"../../interfaces/IFujiMappings.sol\";  Recommendation  Remove the unused import.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.16 WFTM - Use of incorrect interface declarations ",
        "body": "  Description  WETH and  WFTM implementations are different.  code/contracts/fantom/WFTMUnwrapper.sol:L7-L23  contract WFTMUnwrapper {  address constant wftm = 0x21be370D5312f44cB42ce377BC9b8a0cEF1A4C83;  receive() external payable {}  /**  @notice Convert WFTM to FTM and transfer to msg.sender  @dev msg.sender needs to send WFTM before calling this withdraw  @param _amount amount to withdraw.  /  function withdraw(uint256 _amount) external {  IWETH(wftm).withdraw(_amount);  (bool sent, ) = msg.sender.call{ value: _amount }(\"\");  require(sent, \"Failed to send FTM\");  code/contracts/fantom/providers/ProviderGeist.sol:L115-L116  // convert FTM to WFTM  if (isFtm) IWETH(_tokenAddr).deposit{ value: _amount }();  Also see issues: issue 4.4, issue 4.5, issue 4.10  Recommendation  We recommend using the correct interfaces for all contracts instead of partial stubs. Do not modify the original function declarations, e.g., by omitting return value declarations. The codebase should also check return values where possible or explicitly state why values can safely be ignored in inline comments or the function s natspec documentation block.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.17 Inconsistent isFTM, isETH checks ",
        "body": "  Description  LibUniversalERC20FTM.isFTM() and LibUniversalERC20.isETH() identifies native assets by matching against two distinct addresses while some components only check for one.  Examples  The same is true for FTM.  Flasher only identifies a native asset transfer by matching asset against _ETH = 0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE while univTransfer() identifies it using 0x0 || 0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE  code/contracts/mainnet/flashloans/Flasher.sol:L122-L141  function callFunction(  address sender,  Account.Info calldata account,  bytes calldata data  ) external override {  require(msg.sender == _dydxSoloMargin && sender == address(this), Errors.VL_NOT_AUTHORIZED);  account;  FlashLoan.Info memory info = abi.decode(data, (FlashLoan.Info));  uint256 _value;  if (info.asset == _ETH) {  // Convert WETH to ETH and assign amount to be set as msg.value  _convertWethToEth(info.amount);  _value = info.amount;  } else {  // Transfer to Vault the flashloan Amount  // _value is 0  IERC20(info.asset).univTransfer(payable(info.vault), info.amount);  LibUniversalERC20  code/contracts/mainnet/libraries/LibUniversalERC20.sol:L8-L16  library LibUniversalERC20 {  using SafeERC20 for IERC20;  IERC20 private constant _ETH_ADDRESS = IERC20(0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE);  IERC20 private constant _ZERO_ADDRESS = IERC20(0x0000000000000000000000000000000000000000);  function isETH(IERC20 token) internal pure returns (bool) {  return (token == _ZERO_ADDRESS || token == _ETH_ADDRESS);  code/contracts/mainnet/libraries/LibUniversalERC20.sol:L26-L40  function univTransfer(  IERC20 token,  address payable to,  uint256 amount  ) internal {  if (amount > 0) {  if (isETH(token)) {  (bool sent, ) = to.call{ value: amount }(\"\");  require(sent, \"Failed to send Ether\");  } else {  token.safeTransfer(to, amount);  There are multiple other instances of this  code/contracts/mainnet/Fliquidator.sol:L162-L162  uint256 _value = vAssets.borrowAsset == ETH ? debtTotal : 0;  Recommendation  Consider using a consistent way to identify native asset transfers (i.e. ETH, FTM) by using LibUniversalERC20.isETH(). Alternatively, the system can be greatly simplified by expecting WFTM and only working with it. This simplification will remove all special cases where the library must handle non-ERC20 interfaces.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.18 FujiOracle - setPriceFeed should check asset and priceFeed decimals ",
        "body": "  Description  getPriceOf() assumes that all price feeds return prices with identical decimals, but setPriceFeed does not enforce this. Potential misconfigurations can have severe effects on the system s internal accounting.  Examples  code/contracts/FujiOracle.sol:L27-L36  /**  @dev Sets '_priceFeed' address for a '_asset'.  Can only be called by the contract owner.  Emits a {AssetPriceFeedChanged} event.  /  function setPriceFeed(address _asset, address _priceFeed) public onlyOwner {  require(_priceFeed != address(0), Errors.VL_ZERO_ADDR);  usdPriceFeeds[_asset] = _priceFeed;  emit AssetPriceFeedChanged(_asset, _priceFeed);  Recommendation  We recommend adding additional checks to detect unexpected changes in assets  properties. Safeguard price feeds by enforcing priceFeed == address(0) || priceFeed.decimals() == 8. This allows the owner to disable a priceFeed (setting it to zero) and otherwise ensure that the feed is compatible and indeed returns 8 decimals.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.19 Unchecked function return values for low-level calls",
        "body": "  Description  It should be noted that the swapping and harvesting transactions sometimes return values to the function caller. While the low-level call is checked for  success , the return values are not actively handled. This can be intentional but should be verified.  Before calling the external contract, there is no check whether a contract is deployed at that address. Since destinations seem to be hardcoded in the Swapper/Harvester modules, we assume this has been ensured before deploying the contract. However, we suggest checking that code is deployed at the destination address, especially for upgradeable contracts.  We raise this as an informational finding as both the Harvester and Swapper flows using token.balanceOf(this), which might make this check obsolete. However, potential future third-party Swapper/Harvester additions to the protocol might return error codes that need to be checked for.  Examples  Geist/Uniswap and WFTM methods may return amounts or error codes  code/contracts/fantom/FujiVaultFTM.sol:L549-L551  // Claim rewards  (bool success, ) = harvestTransaction.to.call(harvestTransaction.data);  require(success, \"failed to harvest rewards\");  code/contracts/fantom/FujiVaultFTM.sol:L565-L567  // Swap rewards -> collateralAsset  (success, ) = swapTransaction.to.call{ value: swapTransaction.value }(swapTransaction.data);  require(success, \"failed to swap rewards\");  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.20 Use the compiler to resolve function selectors for interfaces",
        "body": "  Description  Function signatures of known contract and interface types are available to the compiler. We recommend using abi.encodeWithSelector(IProvider.withdraw.selector, ...) instead of the more error prone abi.encodeWithSignature(\"withdraw(address,uint256)\", ...) equivalent. Using the former method avoids hard-to-detect errors stemming from typos, interface changes, etc.  Examples  code/contracts/abstracts/vault/VaultBaseUpgradeable.sol:L57-L84  /**  @dev Executes withdraw operation with delegatecall.  @param _amount: amount to be withdrawn  @param _provider: address of provider to be used  /  function _withdraw(uint256 _amount, address _provider) internal {  bytes memory data = abi.encodeWithSignature(  \"withdraw(address,uint256)\",  vAssets.collateralAsset,  _amount  );  _execute(_provider, data);  /**  @dev Executes borrow operation with delegatecall.  @param _amount: amount to be borrowed  @param _provider: address of provider to be used  /  function _borrow(uint256 _amount, address _provider) internal {  bytes memory data = abi.encodeWithSignature(  \"borrow(address,uint256)\",  vAssets.borrowAsset,  _amount  );  _execute(_provider, data);  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.21 Reduce code complexity",
        "body": "  Description  Throughout the codebase, snippets of code and whole functions have been copy-pasted. This duplication significantly increases code complexity and the potential for bugs. We recommend re-using code across modules or providing library contracts that implement re-usable code fragments.  Examples  Providers should use LibUniversalERC20FTM.isFTM instead of re-implementing Helper.isFTM.  code/contracts/fantom/providers/ProviderCream.sol:L17-L19  function _isFTM(address token) internal pure returns (bool) {  return (token == address(0) || token == address(0xFFfFfFffFFfffFFfFFfFFFFFffFFFffffFfFFFfF));  code/contracts/fantom/providers/ProviderScream.sol:L17-L19  function _isFTM(address token) internal pure returns (bool) {  return (token == address(0) || token == address(0xFFfFfFffFFfffFFfFFfFFFFFffFFFffffFfFFFfF));  ProviderGeist should provide an internal method instead of implementing multiple variants of the isFtm to token address mapping. E.g., both calls do the same thing. They select a different return value from the external call. Avoid re-implementing an inconsistent isFtm variant. Require that isFtm && amount != 0 on deposit/payback.  code/contracts/fantom/providers/ProviderGeist.sol:L57-L67  function getBorrowBalance(address _asset) external view override returns (uint256) {  IAaveDataProvider aaveData = _getAaveDataProvider();  bool isFtm = _asset == _getFtmAddr();  address _tokenAddr = isFtm ? _getWftmAddr() : _asset;  (, , uint256 variableDebt, , , , , , ) = aaveData.getUserReserveData(_tokenAddr, msg.sender);  return variableDebt;  code/contracts/fantom/providers/ProviderGeist.sol:L43-L52  function getBorrowRateFor(address _asset) external view override returns (uint256) {  IAaveDataProvider aaveData = _getAaveDataProvider();  (, , , , uint256 variableBorrowRate, , , , , ) = IAaveDataProvider(aaveData).getReserveData(  _asset == _getFtmAddr() ? _getWftmAddr() : _asset  );  return variableBorrowRate;  Also, note the unnecessary double cast IAaveDataProvider.  code/contracts/fantom/providers/ProviderGeist.sol:L73-L87  function getBorrowBalanceOf(address _asset, address _who)  external  view  override  returns (uint256)  IAaveDataProvider aaveData = _getAaveDataProvider();  bool isFtm = _asset == _getFtmAddr();  address _tokenAddr = isFtm ? _getWftmAddr() : _asset;  (, , uint256 variableDebt, , , , , , ) = aaveData.getUserReserveData(_tokenAddr, _who);  return variableDebt;  Consider removing support for the native currency altogether in favor of only accepting pre-wrapped WFTM (WETH). This should remove a lot of glue code currently implemented to auto-wrap/unwrap native currency.  Unused functionality  code/contracts/fantom/providers/ProviderCream.sol:L52-L57  function _exitCollatMarket(address _cyTokenAddress) internal {  // Create a reference to the corresponding network Comptroller  IComptroller comptroller = IComptroller(_getComptrollerAddress());  comptroller.exitMarket(_cyTokenAddress);  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.22 Unusable state variable in dYdX provider",
        "body": "  Description  Remove the state variable donothing . Providers are always called via staticcall or delegatecall and should not hold any state.  code/contracts/mainnet/providers/ProviderDYDX.sol:L93-L95  bool public donothing = true;  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.23 Use enums instead of hardcoded integer literals",
        "body": "  Description  Hardcoded integers are used throughout the codebase to denote states and distinguish between states. The code s complexity can be significantly reduced by using descriptive enum values.  Examples  2 should be InterestRateMode.VARIABLE  code/contracts/fantom/providers/ProviderGeist.sol:L184-L184  aave.repay(_tokenAddr, _amount, 2, address(this));  code/contracts/fantom/providers/ProviderGeist.sol:L136-L136  aave.borrow(_tokenAddr, _amount, 2, 0, address(this));  _farmProtocolNum and harvestType should be refactored to their enum equivalents:  code/contracts/mainnet/Harvester.sol:L20-L32  if (_farmProtocolNum == 0) {  transaction.to = 0x3d9819210A31b4961b30EF54bE2aeD79B9c9Cd3B;  transaction.data = abi.encodeWithSelector(  bytes4(keccak256(\"claimComp(address)\")),  msg.sender  );  claimedToken = 0xc00e94Cb662C3520282E6f5717214004A7f26888;  } else if (_farmProtocolNum == 1) {  uint256 harvestType = abi.decode(_data, (uint256));  if (harvestType == 0) {  // claim  (, address[] memory assets) = abi.decode(_data, (uint256, address[]));  label the flashloan providers with an enum representing their name  code/contracts/fantom/flashloans/FlasherFTM.sol:L72-L78  if (_flashnum == 0) {  _initiateGeistFlashLoan(info);  } else if (_flashnum == 2) {  _initiateCreamFlashLoan(info);  } else {  revert(Errors.VL_INVALID_FLASH_NUMBER);  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.24 Redundant harvest check in vault",
        "body": "  Description  In the FujiVaultFTM.harvestRewards function, the check for a returned token s address in the if condition and require statement overlap with tokenReturned != address(0).  Examples  code/contracts/mainnet/FujiVault.sol:L553-L555  if (tokenReturned != address(0)) {  uint256 tokenBal = IERC20Upgradeable(tokenReturned).univBalanceOf(address(this));  require(tokenReturned != address(0) && tokenBal > 0, Errors.VL_HARVESTING_FAILED);  Recommendation  We recommend removing one of the statements for gas savings and increased readability.  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.25 Redundant use of immutable for constants",
        "body": "  Description  The FlasherFTM contract declares immutable state variables even though they are never set in the constructor. Consider declaring them as constant instead unless they are to be set on construction time. See the Solidity Documentation for further details:  [\u2026] For constant variables, the value has to be fixed at compile-time, while for immutable, it can still be assigned at construction time. [\u2026]  Examples  code/contracts/mainnet/flashloans/Flasher.sol:L37-L44  address private immutable _aaveLendingPool = 0x7d2768dE32b0b80b7a3454c06BdAc94A69DDc7A9;  address private immutable _dydxSoloMargin = 0x1E0447b19BB6EcFdAe1e4AE1694b0C3659614e4e;  // IronBank  address private immutable _cyFlashloanLender = 0x1a21Ab52d1Ca1312232a72f4cf4389361A479829;  address private immutable _cyComptroller = 0xAB1c342C7bf5Ec5F02ADEA1c2270670bCa144CbB;  // need to be payable because of the conversion ETH <> WETH  code/contracts/fantom/flashloans/FlasherFTM.sol:L36-L39  address private immutable _geistLendingPool = 0x9FAD24f572045c7869117160A571B2e50b10d068;  IFujiMappings private immutable _crMappings =  IFujiMappings(0x1eEdE44b91750933C96d2125b6757C4F89e63E20);  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.26 Redeclaration of constant values in multiple contracts",
        "body": "  Description  Throughout the codebase, constant values are redeclared in various contracts. This duplication makes the code harder to maintain and increases the risk for bugs. A central contract, e.g., Constants.sol, ConstantsFTM.sol, and ConstantsETH.sol, to declare the constants used throughout the codebase instead of redeclaring them in multiple source units can fix this issue. Ideally, for example, an address constant for an external component is only configured in a single place but consumed by multiple contracts. This will significantly reduce the potential for misconfiguration.  Avoid hardcoded addresses and use meaningful, constant names for them.  Note that the solidity compiler is going to inline constants where possible.  Examples  code/contracts/mainnet/WETHUnwrapper.sol:L7-L9  contract WETHUnwrapper {  address constant weth = 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2;  code/contracts/mainnet/Swapper.sol:L16-L19  address public constant ETH = 0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE;  address public constant WETH = 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2;  address public constant SUSHI_ROUTER_ADDR = 0xd9e1cE17f2641f24aE83637ab66a2cca9C378B9F;  code/contracts/mainnet/FujiVault.sol:L32-L34  address public constant ETH = 0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE;  code/contracts/mainnet/Fliquidator.sol:L31-L31  address public constant ETH = 0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE;  code/contracts/mainnet/providers/ProviderCompound.sol:L14-L18  contract HelperFunct {  function _isETH(address token) internal pure returns (bool) {  return (token == address(0) || token == address(0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE));  code/contracts/mainnet/libraries/LibUniversalERC20.sol:L10-L14  IERC20 private constant _ETH_ADDRESS = IERC20(0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE);  IERC20 private constant _ZERO_ADDRESS = IERC20(0x0000000000000000000000000000000000000000);  function isETH(IERC20 token) internal pure returns (bool) {  code/contracts/mainnet/flashloans/Flasher.sol:L34-L36  address private constant _ETH = 0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE;  address private constant _WETH = 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2;  Use meaningful names instead of hardcoded addresses  code/contracts/mainnet/Harvester.sol:L20-L29  if (_farmProtocolNum == 0) {  transaction.to = 0x3d9819210A31b4961b30EF54bE2aeD79B9c9Cd3B;  transaction.data = abi.encodeWithSelector(  bytes4(keccak256(\"claimComp(address)\")),  msg.sender  );  claimedToken = 0xc00e94Cb662C3520282E6f5717214004A7f26888;  } else if (_farmProtocolNum == 1) {  uint256 harvestType = abi.decode(_data, (uint256));  Avoid unnamed hardcoded inlined addresses  code/contracts/fantom/providers/ProviderCream.sol:L157-L162  if (_isFTM(_asset)) {  // Transform FTM to WFTM  IWETH(0x21be370D5312f44cB42ce377BC9b8a0cEF1A4C83).deposit{ value: _amount }();  _asset = address(0x21be370D5312f44cB42ce377BC9b8a0cEF1A4C83);  comptroller address - can also be private constant state variables as the compiler/preprocessor will inline them.  code/contracts/fantom/providers/ProviderCream.sol:L21-L31  function _getMappingAddr() internal pure returns (address) {  return 0x1eEdE44b91750933C96d2125b6757C4F89e63E20; // Cream fantom mapper  function _getComptrollerAddress() internal pure returns (address) {  return 0x4250A6D3BD57455d7C6821eECb6206F507576cD2; // Cream fantom  function _getUnwrapper() internal pure returns(address) {  return 0xee94A39D185329d8c46dEA726E01F91641E57346;  WFTM multiple re-declarations  code/contracts/fantom/WFTMUnwrapper.sol:L7-L9  contract WFTMUnwrapper {  address constant wftm = 0x21be370D5312f44cB42ce377BC9b8a0cEF1A4C83;  code/contracts/fantom/providers/ProviderGeist.sol:L27-L29  function _getWftmAddr() internal pure returns (address) {  return 0x21be370D5312f44cB42ce377BC9b8a0cEF1A4C83;  code/contracts/fantom/providers/ProviderCream.sol:L79-L81  IWETH(0x21be370D5312f44cB42ce377BC9b8a0cEF1A4C83).deposit{ value: _amount }();  _asset = address(0x21be370D5312f44cB42ce377BC9b8a0cEF1A4C83);  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.27 Always use the best available type",
        "body": "  Description  Declare state variables with the best type available and downcast to address if needed. Typecasting inside the corpus of a function is unneeded when the parameter s type is known beforehand. Declare the best type in function arguments, state vars. Always return the best type available instead of falling back to address.  Examples  There are many more instances of this, but here s a list of samples:  Should be declared with the correct types/interfaces instead of address  code/contracts/FujiAdmin.sol:L14-L20  address private _flasher;  address private _fliquidator;  address payable private _ftreasury;  address private _controller;  address private _vaultHarvester;  Should return the correct type/interfaces instead of address  code/contracts/FujiAdmin.sol:L144-L147  Should declare the argument with the correct type instead of casting in the function body.  /  function getSwapper() external view override returns (address) {  return _swapper;  Should declare the argument with the correct type instead of casting in the function body.  code/contracts/Controller.sol:L73-L80  function doRefinancing(  address _vaultAddr,  address _newProvider,  uint8 _flashNum  ) external isValidVault(_vaultAddr) onlyOwnerOrExecutor {  IVault vault = IVault(_vaultAddr);  Should make the FujiVaultFTM.fujiERC1155 state variable of type IFujiERC1155  code/contracts/fantom/FujiVaultFTM.sol:L438-L445  IFujiERC1155(fujiERC1155).updateState(  vAssets.borrowID,  IProvider(activeProvider).getBorrowBalance(vAssets.borrowAsset)  );  IFujiERC1155(fujiERC1155).updateState(  vAssets.collateralID,  IProvider(activeProvider).getDepositBalance(vAssets.collateralAsset)  );  Return the best type available  code/contracts/fantom/providers/ProviderCream.sol:L25-L31  function _getComptrollerAddress() internal pure returns (address) {  return 0x4250A6D3BD57455d7C6821eECb6206F507576cD2; // Cream fantom  function _getUnwrapper() internal pure returns(address) {  return 0xee94A39D185329d8c46dEA726E01F91641E57346;  ",
        "labels": [
            "Consensys"
        ],
        "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"
    },
    {
        "title": "4.1 Unhandled return values of transfer and transferFrom    ",
        "body": "  Resolution   The issue was fixed by using OpenZeppelin s   ERC20 implementations are not always consistent. Some implementations of transfer and transferFrom could return  false  on failure instead of reverting. It is safer to wrap such calls into require() statements to these failures.  code/contracts/stake/StakedToken.sol:L92  IERC20(STAKED_TOKEN).transferFrom(msg.sender, address(this), amount);  code/contracts/stake/StakedToken.sol:L156  REWARD_TOKEN.transferFrom(REWARDS_VAULT, to, amountToWithdraw);  code/contracts/stake/StakedToken.sol:L125  IERC20(STAKED_TOKEN).transfer(to, amount);  ",
        "labels": [
            "Consensys",
            "Medium",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/09/aave-safety-module/"
    },
    {
        "title": "4.2 Staking cooldown can be avoided for a part of the funds    ",
        "body": "  Resolution  The cooldown window will be set to much higher value (to the order of days) in production. The mechanism is sufficient to prevent stakers from withdrawing if the cooldown window is long enough while also being larger than the withdrawal window.  Aave is planning to introduce a slashing mechanism for the staking system in the future. In order to prevent stakers from withdrawing their stake immediately, the team has added a  cooldown  mechanism. The idea is that whenever stakers want to redeem the stake, they should call the cooldown function and wait for COOLDOWN_SECONDS. After that, a time period called UNSTAKE_WINDOW starts during which the stake can be withdrawn.  However, depending on the settings ( COOLDOWN_SECONDS  and  UNSTAKE_WINDOW  values), various algorithms exist that would allow users to optimize their withdrawal tactics. By using such tactics, stakers may be able to withdraw at least a part of the stake immediately.  Let s assume that the values are the same as in tests: COOLDOWN_SECONDS == 1 hour and UNSTAKE_WINDOW == 30 minutes. Stakers can split their stake into 3 parts and call cooldown for one of them every 30 minutes. That would ensure that at least 1/3 of the stake can be withdrawn immediately at any time. And on average, more than 1/2 of the stake can be withdrawn immediately.  Remediation:  Make sure that the COOLDOWN_SECONDS value is much larger than the UNSTAKE_WINDOW. This will make any cooldown optimization techniques less effective.  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/09/aave-safety-module/"
    },
    {
        "title": "4.3  code quality issues    ",
        "body": "  Resolution  all issues have been fixed in production.  We recommend the following improvements:  Fix todos  Clean up all TODOs before going into production:  code/contracts/stake/AaveDistributionManager.sol:L44-L46  function configureAssets(DistributionTypes.AssetConfigInput[] calldata assetsConfigInput)  external  //    override TODO: create interface  Fix incorrect NatSpec comments  Clean up NatSpec comments to improve readability.  The function claimRewards() in StakedToken has the same description as the stake() function:  code/contracts/stake/StakedToken.sol:L141-L145  One function argument is missing from the docstrings for claimRewards() in AaveIncentivesController:  @dev Stakes tokens to start earning rewards  @param to Address to stake for  @param amount Amount to stake  **/  function claimRewards(address to, uint256 amount) external override {  One function argument is missing from the docstrings for claimRewards() in AaveIncentivesController:  code/contracts/stake/AaveIncentivesController.sol:L97-L107  /**  @dev Claims reward for an user, on all the assets of the lending pool, accumulating the pending rewards  @param amount Amount of rewards to claim  @param to Address that will be receiving the rewards  @return Rewards claimed  **/  function claimRewards(  uint256 amount,  address to,  bool stake  ) external override returns (uint256) {  ",
        "labels": [
            "Consensys",
            "Minor",
            "Fixed"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/09/aave-safety-module/"
    },
    {
        "title": "4.1 Token approvals can be stolen in DAOfiV1Router01.addLiquidity() ",
        "body": "  Description  DAOfiV1Router01.addLiquidity() creates the desired pair contract if it does not already exist, then transfers tokens into the pair and calls DAOfiV1Pair.deposit(). There is no validation of the address to transfer tokens from, so an attacker could pass in any address with nonzero token approvals to DAOfiV1Router. This could be used to add liquidity to a pair contract for which the attacker is the pairOwner, allowing the stolen funds to be retrieved using DAOfiV1Pair.withdraw().  code/daofi-v1-periphery/contracts/DAOfiV1Router01.sol:L57-L85  function addLiquidity(  LiquidityParams calldata lp,  uint deadline  ) external override ensure(deadline) returns (uint256 amountBase) {  if (IDAOfiV1Factory(factory).getPair(  lp.tokenBase,  lp.tokenQuote,  lp.slopeNumerator,  lp.n,  lp.fee  ) == address(0)) {  IDAOfiV1Factory(factory).createPair(  address(this),  lp.tokenBase,  lp.tokenQuote,  msg.sender,  lp.slopeNumerator,  lp.n,  lp.fee  );  address pair = DAOfiV1Library.pairFor(  factory, lp.tokenBase, lp.tokenQuote, lp.slopeNumerator, lp.n, lp.fee  );  TransferHelper.safeTransferFrom(lp.tokenBase, lp.sender, pair, lp.amountBase);  TransferHelper.safeTransferFrom(lp.tokenQuote, lp.sender, pair, lp.amountQuote);  amountBase = IDAOfiV1Pair(pair).deposit(lp.to);  Recommendation  Transfer tokens from msg.sender instead of lp.sender.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/02/daofi/"
    },
    {
        "title": "4.2 The deposit of a new pair can be stolen ",
        "body": "  Description  To create a new pair, a user is expected to call the same addLiquidity() (or the addLiquidityETH()) function of the router contract seen above:  code/daofi-v1-periphery/contracts/DAOfiV1Router01.sol:L57-L85  function addLiquidity(  LiquidityParams calldata lp,  uint deadline  ) external override ensure(deadline) returns (uint256 amountBase) {  if (IDAOfiV1Factory(factory).getPair(  lp.tokenBase,  lp.tokenQuote,  lp.slopeNumerator,  lp.n,  lp.fee  ) == address(0)) {  IDAOfiV1Factory(factory).createPair(  address(this),  lp.tokenBase,  lp.tokenQuote,  msg.sender,  lp.slopeNumerator,  lp.n,  lp.fee  );  address pair = DAOfiV1Library.pairFor(  factory, lp.tokenBase, lp.tokenQuote, lp.slopeNumerator, lp.n, lp.fee  );  TransferHelper.safeTransferFrom(lp.tokenBase, lp.sender, pair, lp.amountBase);  TransferHelper.safeTransferFrom(lp.tokenQuote, lp.sender, pair, lp.amountQuote);  amountBase = IDAOfiV1Pair(pair).deposit(lp.to);  This function checks if the pair already exists and creates a new one if it does not. After that, the first and only deposit is made to that pair.  The attacker can front-run that call and create a pair with the same parameters (thus, with the same address) by calling the createPair function of the DAOfiV1Factory contract. By calling that function directly, the attacker does not have to make the deposit when creating a new pair. The initial user will make this deposit, whose funds can now be withdrawn by the attacker.  Recommendation  There are a few factors/bugs that allowed this attack. All or some of them should be fixed:  The createPair function of the DAOfiV1Factory contract can be called directly by anyone without depositing with any router address as the parameter. The solution could be to allow only the router to create a pair.  The addLiquidity function checks that the pair does not exist yet. If the pair exists already, a deposit should only be made by the owner of the pair. But in general, a new pair shouldn t be deployed without depositing in the same transaction.  The pair s address does not depend on the owner/creator. It might make sense to add that information to the salt.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/02/daofi/"
    },
    {
        "title": "4.3 Incorrect token decimal conversions can lead to loss of funds ",
        "body": "  Description  The _convert() function in DAOfiV1Pair is used to accommodate tokens with varying decimals() values. There are three cases in which it implicitly returns 0 for any amount, the most notable of which is when token.decimals() == resolution.  As a result of this, getQuoteOut() reverts any time either baseToken or quoteToken have decimals == INTERNAL_DECIMALS (currently hardcoded to 8).  The result of this is that no swaps can be performed in one of these pools, and the deposit() function will return an incorrect amountBaseOut of baseToken to the depositor, the balance of which can then be withdrawn by the pairOwner.  code/daofi-v1-core/contracts/DAOfiV1Pair.sol:L108-L130  function _convert(address token, uint256 amount, uint8 resolution, bool to) private view returns (uint256 converted) {  uint8 decimals = IERC20(token).decimals();  uint256 diff = 0;  uint256 factor = 0;  converted = 0;  if (decimals > resolution) {  diff = uint256(decimals.sub(resolution));  factor = 10 ** diff;  if (to && amount >= factor) {  converted = amount.div(factor);  } else if (!to) {  converted = amount.mul(factor);  } else if (decimals < resolution) {  diff = uint256(resolution.sub(decimals));  factor = 10 ** diff;  if (to) {  converted = amount.mul(factor);  } else if (!to && amount >= factor) {  converted = amount.div(factor);  Recommendation  The _convert() function should return amount when token.decimals() == resolution. Additionally, implicit return values should be avoided whenever possible, especially in functions that implement complex mathematical operations.  BancorFormula.power(baseN, baseD, _, _) does not support baseN < baseD, and checks should be added to ensure that any call to the BancorFormula conforms to the expected input ranges.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/02/daofi/"
    },
    {
        "title": "4.4 The swapExactTokensForETH checks the wrong return value ",
        "body": "  Description  The following lines are intended to check that the amount of tokens received from a swap is greater than the minimum amount expected from this swap (sp.amountOut):  code/daofi-v1-periphery/contracts/DAOfiV1Router01.sol:L341-L345  uint amountOut = IWETH10(WETH).balanceOf(address(this));  require(  IWETH10(sp.tokenOut).balanceOf(address(this)).sub(balanceBefore) >= sp.amountOut,  'DAOfiV1Router: INSUFFICIENT_OUTPUT_AMOUNT'  );  Instead, it calculates the difference between the initial receiver s balance and the balance of the router.  Recommendation  Check the intended value.  ",
        "labels": [
            "Consensys",
            "Major"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/02/daofi/"
    },
    {
        "title": "4.5 DAOfiV1Pair.deposit() accepts deposits of zero, blocking the pool ",
        "body": "  Description  code/daofi-v1-core/contracts/DAOfiV1Pair.sol:L223-L239  function deposit(address to) external override lock returns (uint256 amountBaseOut) {  require(msg.sender == router, 'DAOfiV1: FORBIDDEN_DEPOSIT');  require(deposited == false, 'DAOfiV1: DOUBLE_DEPOSIT');  reserveBase = IERC20(baseToken).balanceOf(address(this));  reserveQuote = IERC20(quoteToken).balanceOf(address(this));  // this function is locked and the contract can not reset reserves  deposited = true;  if (reserveQuote > 0) {  // set initial supply from reserveQuote  supply = amountBaseOut = getBaseOut(reserveQuote);  if (amountBaseOut > 0) {  _safeTransfer(baseToken, to, amountBaseOut);  reserveBase = reserveBase.sub(amountBaseOut);  emit Deposit(msg.sender, reserveBase, reserveQuote, amountBaseOut, to);  Recommendation  Require a minimum deposit amount in both baseToken and quoteToken, and do not rely on any assumptions about the distribution of baseToken as part of the security model.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/02/daofi/"
    },
    {
        "title": "4.6 Restricting DAOfiV1Pair functions to calls from router makes DAOfiV1Router01 security critical ",
        "body": "  Description  The DAOfiV1Pair functions deposit(), withdraw(), and swap() are all restricted to calls from the router in order to avoid losses from user error. However, this means that any unidentified issue in the Router could render all pair contracts unusable, potentially locking the pair owner s funds.  Additionally, DAOfiV1Factory.createPair() allows any nonzero address to be provided as the router, so pairs can be initialized with a malicious router that users would be forced to interact with to utilize the pair contract.  code/daofi-v1-core/contracts/DAOfiV1Pair.sol:L223-L224  function deposit(address to) external override lock returns (uint256 amountBaseOut) {  require(msg.sender == router, 'DAOfiV1: FORBIDDEN_DEPOSIT');  code/daofi-v1-core/contracts/DAOfiV1Pair.sol:L250-L251  function withdraw(address to) external override lock returns (uint256 amountBase, uint256 amountQuote) {  require(msg.sender == router, 'DAOfiV1: FORBIDDEN_WITHDRAW');  code/daofi-v1-core/contracts/DAOfiV1Pair.sol:L292-L293  function swap(address tokenIn, address tokenOut, uint256 amountIn, uint256 amountOut, address to) external override lock {  require(msg.sender == router, 'DAOfiV1: FORBIDDEN_SWAP');  Recommendation  Do not restrict DAOfiV1Pair functions to calls from router, but encourage users to use a trusted router to avoid losses from user error. If this restriction is kept, consider including the router address in the deployment salt for the pair or hardcoding the address of a trusted router in DAOfiV1Factory instead of taking the router as a parameter to createPair().  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/02/daofi/"
    },
    {
        "title": "4.7 Pair contracts can be easily blocked ",
        "body": "  Description  The existing mitigation for this issue is to create a new pool with slightly different parameters. This creates significant cost for the creator of a pair, forces them to deploy a pair with sub-optimal parameters, and could potentially block all interesting pools for a token pair.  The salt used to determine unique pair contracts in DAOfiV1Factory.createPair():  code/daofi-v1-core/contracts/DAOfiV1Factory.sol:L77-L84  require(getPair(baseToken, quoteToken, slopeNumerator, n, fee) == address(0), 'DAOfiV1: PAIR_EXISTS'); // single check is sufficient  bytes memory bytecode = type(DAOfiV1Pair).creationCode;  bytes32 salt = keccak256(abi.encodePacked(baseToken, quoteToken, slopeNumerator, n, fee));  assembly {  pair := create2(0, add(bytecode, 32), mload(bytecode), salt)  IDAOfiV1Pair(pair).initialize(router, baseToken, quoteToken, pairOwner, slopeNumerator, n, fee);  pairs[salt] = pair;  Recommendation  Consider adding additional parameters to the salt that defines a unique pair, such as the pairOwner. Modifying the parameters included in the salt can also be used to partially mitigate other security concerns raised in this report.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/02/daofi/"
    },
    {
        "title": "4.8 DAOfiV1Router01.removeLiquidityETH() does not support tokens with no return value ",
        "body": "  Description  While the rest of the system uses the safeTransfer* pattern, allowing tokens that do not return a boolean value on transfer() or transferFrom(), DAOfiV1Router01.removeLiquidityETH() throws and consumes all remaining gas if the base token does not return true.  Note that the deposit in this case can still be withdrawn without unwrapping the Eth using removeLiquidity().  code/daofi-v1-periphery/contracts/DAOfiV1Router01.sol:L157-L167  function removeLiquidityETH(  LiquidityParams calldata lp,  uint deadline  ) external override ensure(deadline) returns (uint amountToken, uint amountETH) {  IDAOfiV1Pair pair = IDAOfiV1Pair(DAOfiV1Library.pairFor(factory, lp.tokenBase, WETH, lp.slopeNumerator, lp.n, lp.fee));  require(msg.sender == pair.pairOwner(), 'DAOfiV1Router: FORBIDDEN');  (amountToken, amountETH) = pair.withdraw(address(this));  assert(IERC20(lp.tokenBase).transfer(lp.to, amountToken));  IWETH10(WETH).withdraw(amountETH);  TransferHelper.safeTransferETH(lp.to, amountETH);  Recommendation  Be consistent with the use of safeTransfer*, and do not use assert() in cases where the condition can be false.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2021/02/daofi/"
    },
    {
        "title": "5.1 [Out of Scope] ReferralFeeReceiver - anyone can steal all the funds that belong to ReferralFeeReceiver    Fix Unverified",
        "body": "  Resolution  According to the client, this issue is addressed in 1inch-exchange/1inch-liquidity-protocol#2 and the reentrancy in FeeReceiver in 1inch-exchange/1inch-liquidity-protocol@e9c6a03  (This fix is as reported by the developer team, but has not been verified by Diligence).  Description  Note: This issue was raised in components that were being affected by the scope reduction as outlined in the section  Scope  and are, therefore, only shallowly validated. Nevertheless, we find it important to communicate such potential findings and ask the client to further investigate.  The ReferralFeeReceiver receives pool shares when users swap() tokens in the pool. A ReferralFeeReceiver may be used with multiple pools and, therefore, be a lucrative target as it is holding pool shares.  Any token or ETH that belongs to the ReferralFeeReceiver is at risk and can be drained by any user by providing a custom mooniswap pool contract that references existing token holdings.  It should be noted that none of the functions in ReferralFeeReceiver verify that the user-provided mooniswap pool address was actually deployed by the linked MooniswapFactory. The factory provides certain security guarantees about mooniswap pool contracts (e.g. valid mooniswap contract, token deduplication, tokenA!=tokenB, enforced token sorting, \u2026), however, since the ReferralFeeReceiver does not verify the user-provided mooniswap address they are left unchecked.  Additional Notes  freezeEpoch - (callable by anyone) performs a pool.withdraw() with the minAmounts check being disabled. This may allow someone to call this function at a time where the contract actually gets a bad deal.  trade - (callable by anyone) can intentionally be used to perform bad trades (front-runnable)  trade - (callable by anyone) appears to implement inconsistent behavior when sending out availableBalance. ETH is sent to tx.origin (the caller) while tokens are sent to the user-provided mooniswap address.  code/contracts/ReferralFeeReceiver.sol:L91-L95  if (path[0].isETH()) {  tx.origin.transfer(availableBalance);  // solhint-disable-line avoid-tx-origin  } else {  path[0].safeTransfer(address(mooniswap), availableBalance);  multiple methods - since mooniswap is a user-provided address there are a lot of opportunities to reenter the contract. Consider adding reentrancy guards as another security layer (e.g. claimCurrentEpoch and others).  multiple methods - do not validate the amount of tokens that are returned, causing an evm assertion due to out of bounds index access.  code/contracts/ReferralFeeReceiver.sol:L57-L59  IERC20[] memory tokens = mooniswap.getTokens();  uint256 token0Balance = tokens[0].uniBalanceOf(address(this));  uint256 token1Balance = tokens[1].uniBalanceOf(address(this));  in GovernanceFeeReceiver anyone can intentionally force unwrapping of pool tokens or perform swaps in the worst time possible. e.g. The checks for withdraw(..., minAmounts) is disabled.  code/contracts/governance/GovernanceFeeReceiver.sol:L18-L26  function unwrapLPTokens(Mooniswap mooniswap) external validSpread(mooniswap) {  mooniswap.withdraw(mooniswap.balanceOf(address(this)), new uint256[](0));  function swap(IERC20[] memory path) external validPath(path) {  (uint256 amount,) = _maxAmountForSwap(path, path[0].uniBalanceOf(address(this)));  uint256 result = _swap(path, amount, payable(address(rewards)));  rewards.notifyRewardAmount(result);  Examples  Let s assume the following scenario:  ReferralFeeReceiver holds DAI token and we want to steal them.  An attacker may be able to drain the contract from DAI token via claimFrozenToken if  they control the mooniswap address argument and provide a malicious contract  user.share[mooniswap][firstUnprocessedEpoch] > 0 - this can be arbitrarily set in updateReward  token.epochBalance[currentEpoch].token0Balance > 0 - this can be manipulated in freezeEpoch by providing a malicious mooniswap contract  they own a worthless ERC20 token e.g. named ATTK  The following steps outline the attack:  The attacker calls into updateReward to set user.share[mooniswap][currentEpoch] to a value that is greater than zero to make sure that share in claimFrozenEpoch takes the _transferTokenShare path.  code/contracts/ReferralFeeReceiver.sol:L38-L50  function updateReward(address referral, uint256 amount) external override {  Mooniswap mooniswap = Mooniswap(msg.sender);  TokenInfo storage token = tokenInfo[mooniswap];  UserInfo storage user = userInfo[referral];  uint256 currentEpoch = token.currentEpoch;  // Add new reward to current epoch  user.share[mooniswap][currentEpoch] = user.share[mooniswap][currentEpoch].add(amount);  token.epochBalance[currentEpoch].totalSupply = token.epochBalance[currentEpoch].totalSupply.add(amount);  // Collect all processed epochs and advance user token epoch  _collectProcessedEpochs(user, token, mooniswap, currentEpoch);  The attacker then calls freezeEpoch() providing the malicious mooniswap contract address controlled by the attacker.  The malicious contract returns token that is controlled by the attacker (e.g. ATTK) in a call to mooniswap.getTokens();  The contract then stores the current balance of the attacker-controlled token in token0Balance/token1Balance. Note that the token being returned here by the malicious contract can be different from the one we re checking out in the last step (balance manipulation via ATTK, checkout of DAI in the last step).  Then the contract calls out to the malicious mooniswap contract. This gives the malicious contract an easy opportunity to send some attacker-controlled token (ATTK) to the ReferralFeeReceiver in order to freely manipulate the frozen tokenbalances (tokens[0].uniBalanceOf(address(this)).sub(token0Balance);).  Note that the used token addresses are never stored anywhere. The balances recorded here are for an attacker-controlled token (ATTK), not the actual one that we re about to steal (e.g. DAI)  The token balances are now set-up for checkout in the last step (claimFrozenEpoch).  code/contracts/ReferralFeeReceiver.sol:L52-L64  function freezeEpoch(Mooniswap mooniswap) external validSpread(mooniswap) {  TokenInfo storage token = tokenInfo[mooniswap];  uint256 currentEpoch = token.currentEpoch;  require(token.firstUnprocessedEpoch == currentEpoch, \"Previous epoch is not finalized\");  IERC20[] memory tokens = mooniswap.getTokens();  uint256 token0Balance = tokens[0].uniBalanceOf(address(this));  uint256 token1Balance = tokens[1].uniBalanceOf(address(this));  mooniswap.withdraw(mooniswap.balanceOf(address(this)), new uint256[](0));  token.epochBalance[currentEpoch].token0Balance = tokens[0].uniBalanceOf(address(this)).sub(token0Balance);  token.epochBalance[currentEpoch].token1Balance = tokens[1].uniBalanceOf(address(this)).sub(token1Balance);  token.currentEpoch = currentEpoch.add(1);  A call to claimFrozenEpoch checks-out the previously frozen token balance.  The claim > 0 requirement was fulfilled in step 1.  The token balance was prepared for the attacker-controlled token (ATTK) in step 2, but we re now checking out DAI.  When the contract calls out to the attackers mooniswap contract the call to IERC20[] memory tokens = mooniswap.getTokens(); returns the address of the token to be stolen (e.g. DAI) instead of the attacker-controlled token (ATTK) that was used to set-up the balance records.  Subsequently, the valuable target tokens (DAI) are sent out to the caller in _transferTokenShare.  code/contracts/ReferralFeeReceiver.sol:L153-L162  if (share > 0) {  EpochBalance storage epochBalance = token.epochBalance[firstUnprocessedEpoch];  uint256 totalSupply = epochBalance.totalSupply;  user.share[mooniswap][firstUnprocessedEpoch] = 0;  epochBalance.totalSupply = totalSupply.sub(share);  IERC20[] memory tokens = mooniswap.getTokens();  epochBalance.token0Balance = _transferTokenShare(tokens[0], epochBalance.token0Balance, share, totalSupply);  epochBalance.token1Balance = _transferTokenShare(tokens[1], epochBalance.token1Balance, share, totalSupply);  epochBalance.inchBalance = _transferTokenShare(inchToken, epochBalance.inchBalance, share, totalSupply);  Recommendation  Enforce that the user-provided mooniswap contract was actually deployed by the linked factory. Other contracts cannot be trusted. Consider implementing token sorting and de-duplication (tokenA!=tokenB) in the pool contract constructor as well. Consider employing a reentrancy guard to safeguard the contract from reentrancy attacks.  Improve testing. The methods mentioned here are not covered at all. Improve documentation and provide a specification that outlines how this contract is supposed to be used.  Review the  additional notes  provided with this issue.  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/12/1inch-liquidity-protocol/"
    },
    {
        "title": "5.2 GovernanceMothership - notifyFor allows to arbitrarily create new or override other users stake in governance modules    Fix Unverified",
        "body": "  Resolution  According to the client, this issue is addressed in 1inch-exchange/1inch-liquidity-protocol@2ce549d and added tests with 1inch-exchange/1inch-liquidity-protocol@e0dc46b  (This fix is as reported by the developer team, but has not been verified by Diligence).  Description  The notify* methods are called to update linked governance modules when an accounts stake changes in the Mothership. The linked modules then update their own balances of the user to accurately reflect the account s real stake in the Mothership.  Besides notify there s also a method named notifyFor which is publicly accessible. It is assumed that the method should be used similar to notify to force an update for another account s balance.  However, invoking the method forces an update in the linked modules for the provided address, but takes balanceOf(msg.sender) instead of balanceOf(account). This allows malicious actors to:  Arbitrarily change other accounts stake in linked governance modules (e.g. zeroing stake, increasing stake) based on the callers stake in the mothership  Duplicate stake out of thin air to arbitrary addresses (e.g. staking in mothership once and calling notifyFor many other account addresses)  Examples  publicly accessible method allows forcing stake updates for arbitrary users  code/contracts/inch/GovernanceMothership.sol:L48-L50  function notifyFor(address account) external {  _notifyFor(account, balanceOf(msg.sender));  the method calls the linked governance modules  code/contracts/inch/GovernanceMothership.sol:L73-L78  function _notifyFor(address account, uint256 balance) private {  uint256 modulesLength = _modules.length();  for (uint256 i = 0; i < modulesLength; ++i) {  IGovernanceModule(_modules.at(i)).notifyStakeChanged(account, balance);  which will arbitrarily mint or burn stake in the BalanceAccounting of Factory or Reward (or other linked governance modules)  code/contracts/governance/BaseGovernanceModule.sol:L29-L31  function notifyStakeChanged(address account, uint256 newBalance) external override onlyMothership {  _notifyStakeChanged(account, newBalance);  code/contracts/governance/MooniswapFactoryGovernance.sol:L144-L160  function _notifyStakeChanged(address account, uint256 newBalance) internal override {  uint256 balance = balanceOf(account);  if (newBalance > balance) {  _mint(account, newBalance.sub(balance));  } else if (newBalance < balance) {  _burn(account, balance.sub(newBalance));  } else {  return;  uint256 newTotalSupply = totalSupply();  _defaultFee.updateBalance(account, _defaultFee.votes[account], balance, newBalance, newTotalSupply, _DEFAULT_FEE, _emitDefaultFeeVoteUpdate);  _defaultSlippageFee.updateBalance(account, _defaultSlippageFee.votes[account], balance, newBalance, newTotalSupply, _DEFAULT_SLIPPAGE_FEE, _emitDefaultSlippageFeeVoteUpdate);  _defaultDecayPeriod.updateBalance(account, _defaultDecayPeriod.votes[account], balance, newBalance, newTotalSupply, _DEFAULT_DECAY_PERIOD, _emitDefaultDecayPeriodVoteUpdate);  _referralShare.updateBalance(account, _referralShare.votes[account], balance, newBalance, newTotalSupply, _DEFAULT_REFERRAL_SHARE, _emitReferralShareVoteUpdate);  _governanceShare.updateBalance(account, _governanceShare.votes[account], balance, newBalance, newTotalSupply, _DEFAULT_GOVERNANCE_SHARE, _emitGovernanceShareVoteUpdate);  code/contracts/governance/GovernanceRewards.sol:L72-L79  function _notifyStakeChanged(address account, uint256 newBalance) internal override updateReward(account) {  uint256 balance = balanceOf(account);  if (newBalance > balance) {  _mint(account, newBalance.sub(balance));  } else if (newBalance < balance) {  _burn(account, balance.sub(newBalance));  Recommendation  Remove notifyFor or change it to take the balance of the correct account _notifyFor(account, balanceOf(msg.sender)).  ",
        "labels": [
            "Consensys",
            "Critical"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/12/1inch-liquidity-protocol/"
    },
    {
        "title": "5.3 Users can  increase  their voting power by voting for the max/min values ",
        "body": "  Description  Many parameters in the system are determined by the complicated governance mechanism. These parameters are calculated as a result of the voting process and are equal to the weighted average of all the votes that stakeholders make. The idea is that every user is voting for the desired value. But if the result value is smaller (larger) than the desired, the user can change the vote for the max (min) possible value. That would shift the result towards the desired one and basically  increase  this stakeholder s voting power. So every user is more incentivized to vote for the min/max value than for the desired one.  The issue s severity is not high because all parameters have reasonable max value limitations, so it s hard to manipulate the system too much.  Recommendation  Reconsider the voting mechanism.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/12/1inch-liquidity-protocol/"
    },
    {
        "title": "5.4 The uniTransferFrom function can potentially be used with invalid params    Fix Unverified",
        "body": "  Resolution  According to the client, this issue is addressed in 1inch-exchange/1inch-liquidity-protocol@d0ffb6f.  (This fix is as reported by the developer team, but has not been verified by Diligence).  Description  The system is using the UniERC20 contract to incapsulate transfers of both ERC-20 tokens and ETH. This contract has uniTransferFrom function that can be used for any ERC-20 or ETH:  code/contracts/libraries/UniERC20.sol:L36-L48  function uniTransferFrom(IERC20 token, address payable from, address to, uint256 amount) internal {  if (amount > 0) {  if (isETH(token)) {  require(msg.value >= amount, \"UniERC20: not enough value\");  if (msg.value > amount) {  // Return remainder if exist  from.transfer(msg.value.sub(amount));  } else {  token.safeTransferFrom(from, to, amount);  This issue s severity is not high because the function is always called with the proper parameters in the current codebase.  Recommendation  Make sure that the uniTransferFrom function is always called with expected parameters.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/12/1inch-liquidity-protocol/"
    },
    {
        "title": "5.5 MooniswapGovernance - votingpower is not accurately reflected when minting pool tokens    Fix Unverified",
        "body": "  Resolution  According to the client, this issue is addressed in 1inch-exchange/1inch-liquidity-protocol@eb869fd  (This fix is as reported by the developer team, but has not been verified by Diligence).  Description  When a user provides liquidity to the pool, pool-tokens are minted. The minting event triggers the _beforeTokenTransfer callback in MooniswapGovernance which updates voting power reflecting the newly minted stake for the user.  There seems to be a copy-paste error in the way balanceTo is determined that sets balanceTo to zero if new token were minted (from==address(0)). This means, that in a later call to _updateOnTransfer only the newly minted amount is considered when adjusting voting power.  Examples  If tokens are newly minted from==address(0) and therefore balanceTo -> 0.  code/contracts/governance/MooniswapGovernance.sol:L100-L114  function _beforeTokenTransfer(address from, address to, uint256 amount) internal override {  uint256 balanceFrom = (from != address(0)) ? balanceOf(from) : 0;  uint256 balanceTo = (from != address(0)) ? balanceOf(to) : 0;  uint256 newTotalSupply = totalSupply()  .add(from == address(0) ? amount : 0)  .sub(to == address(0) ? amount : 0);  ParamsHelper memory params = ParamsHelper({  from: from,  to: to,  amount: amount,  balanceFrom: balanceFrom,  balanceTo: balanceTo,  newTotalSupply: newTotalSupply  });  now, balanceTo is zero which would adjust voting power to amount instead of the user s actual balance + the newly minted token.  code/contracts/governance/MooniswapGovernance.sol:L150-L153  if (params.to != address(0)) {  votingData.updateBalance(params.to, voteTo, params.balanceTo, params.balanceTo.add(params.amount), params.newTotalSupply, defaultValue, emitEvent);  Recommendation  balanceTo should be zero when burning (to == address(0)) and balanceOf(to) when minting.  e.g. like this:  uint256 balanceTo = (to != address(0)) ? balanceOf(to) : 0;  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/12/1inch-liquidity-protocol/"
    },
    {
        "title": "5.6 MooniswapGovernance - _beforeTokenTransfer should not update voting power on transfers to self    Fix Unverified",
        "body": "  Resolution  Addressed 1inch-exchange/1inch-liquidity-protocol@7c7126d  (This fix is as reported by the developer team, but has not been verified by Diligence).  Description  Mooniswap governance is based on the liquidity voting system that is also employed by the mothership or for factory governance. In contrast to traditional voting systems where users vote for discrete values, the liquidity voting system derives a continuous weighted averaged  consensus  value from all the votes. Thus it is required that whenever stake changes in the system, all the parameters that can be voted upon are updated with the new weights for a specific user.  The Mooniswap pool is governed by liquidity providers and liquidity tokens are the stake that gives voting rights in MooniswapGovernance. Thus whenever liquidity tokens are transferred to another address, stake and voting values need to be updated. This is handled by MooniswapGovernance._beforeTokenTransfer().  In the special case where someone triggers a token transfer where the from address equals the to address, effectively sending the token to themselves, no update on voting power should be performed. Instead, voting power is first updated with balance - amount and then with balance + amount which in the worst case means it is updating first to a zero balance and then to 2x the balance.  Ultimately this should not have an effect on the overall outcome but is unnecessary and wasting gas.  Examples  beforeTokenTransfer callback in Mooniswap does not check for the NOP case where from==to  code/contracts/governance/MooniswapGovernance.sol:L100-L119  function _beforeTokenTransfer(address from, address to, uint256 amount) internal override {  uint256 balanceFrom = (from != address(0)) ? balanceOf(from) : 0;  uint256 balanceTo = (from != address(0)) ? balanceOf(to) : 0;  uint256 newTotalSupply = totalSupply()  .add(from == address(0) ? amount : 0)  .sub(to == address(0) ? amount : 0);  ParamsHelper memory params = ParamsHelper({  from: from,  to: to,  amount: amount,  balanceFrom: balanceFrom,  balanceTo: balanceTo,  newTotalSupply: newTotalSupply  });  _updateOnTransfer(params, mooniswapFactoryGovernance.defaultFee, _emitFeeVoteUpdate, _fee);  _updateOnTransfer(params, mooniswapFactoryGovernance.defaultSlippageFee, _emitSlippageFeeVoteUpdate, _slippageFee);  _updateOnTransfer(params, mooniswapFactoryGovernance.defaultDecayPeriod, _emitDecayPeriodVoteUpdate, _decayPeriod);  which leads to updateBalance being called on the same address twice, first with currentBalance - amountTransferred and then with currentBalance + amountTransferred.  code/contracts/governance/MooniswapGovernance.sol:L147-L153  if (params.from != address(0)) {  votingData.updateBalance(params.from, voteFrom, params.balanceFrom, params.balanceFrom.sub(params.amount), params.newTotalSupply, defaultValue, emitEvent);  if (params.to != address(0)) {  votingData.updateBalance(params.to, voteTo, params.balanceTo, params.balanceTo.add(params.amount), params.newTotalSupply, defaultValue, emitEvent);  Recommendation  Do not update voting power on LP token transfers where from == to.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/12/1inch-liquidity-protocol/"
    },
    {
        "title": "5.7 Unpredictable behavior for users due to admin front running or general bad timing ",
        "body": "  Description  In a number of cases, administrators of contracts can update or upgrade things in the system without warning. This has the potential to violate a security goal of the system.  Specifically, privileged roles could use front running to make malicious changes just ahead of incoming transactions, or purely accidental negative effects could occur due to the unfortunate timing of changes.  In general users of the system should have assurances about the behavior of the action they re about to take.  Examples  MooniswapFactoryGovernance - Admin opportunity to lock swapFor with a referral when setting an invalid referralFeeReceiver  setReferralFeeReceiver and setGovernanceFeeReceiver takes effect immediately.  code/contracts/governance/MooniswapFactoryGovernance.sol:L92-L95  function setReferralFeeReceiver(address newReferralFeeReceiver) external onlyOwner {  referralFeeReceiver = newReferralFeeReceiver;  emit ReferralFeeReceiverUpdate(newReferralFeeReceiver);  setReferralFeeReceiver can be used to set an invalid receiver address (or one that reverts on every call) effectively rendering Mooniswap.swapFor unusable if a referral was specified in the swap.  code/contracts/Mooniswap.sol:L281-L286  if (referral != address(0)) {  referralShare = invIncrease.mul(referralShare).div(_FEE_DENOMINATOR);  if (referralShare > 0) {  if (referralFeeReceiver != address(0)) {  _mint(referralFeeReceiver, referralShare);  IReferralFeeReceiver(referralFeeReceiver).updateReward(referral, referralShare);  Locking staked token  At any point in time and without prior notice to users an admin may accidentally or intentionally add a broken governance sub-module to the system that blocks all users from unstaking their 1INCH token. An admin can recover from this by removing the broken sub-module, however, with malicious intent tokens may be locked forever.  Since 1INCH token gives voting power in the system, tokens are considered to hold value for other users and may be traded on exchanges. This raises concerns if tokens can be locked in a contract by one actor.  An admin adds an invalid address or a malicious sub-module to the governance contract that always reverts on calls to notifyStakeChanged.  code/contracts/inch/GovernanceMothership.sol:L63-L66  function addModule(address module) external onlyOwner {  require(_modules.add(module), \"Module already registered\");  emit AddModule(module);  code/contracts/inch/GovernanceMothership.sol:L73-L78  function _notifyFor(address account, uint256 balance) private {  uint256 modulesLength = _modules.length();  for (uint256 i = 0; i < modulesLength; ++i) {  IGovernanceModule(_modules.at(i)).notifyStakeChanged(account, balance);  Admin front-running to prevent user stake sync  An admin may front-run users while staking in an attempt to prevent submodules from being notified of the stake update. This is unlikely to happen as it incurs costs for the attacker (front-back-running) to normal users but may be an interesting attack scenario to exclude a whale s stake from voting.  For example, an admin may front-run stake() or notoify*() by briefly removing all governance submodules from the mothership and re-adding them after the users call succeeded. The stake-update will not be propagated to the sub-modules. A user may only detect this when they are voting (if they had no stake before) or when they actually check their stake. Such an attack might likely stay unnoticed unless someone listens for addmodule removemodule events on the contract.  An admin front-runs a transaction by removing all modules and re-adding them afterwards to prevent the stake from propagating to the submodules.  code/contracts/inch/GovernanceMothership.sol:L68-L71  function removeModule(address module) external onlyOwner {  require(_modules.remove(module), \"Module was not registered\");  emit RemoveModule(module);  Admin front-running to prevent unstake from propagating  An admin may choose to front-run their own unstake(), temporarily removing all governance sub-modules, preventing unstake() from syncing the action to sub-modules while still getting their previously staked tokens out. The governance sub-modules can be re-added right after unstaking. Due to double-accounting of the stake (in governance and in every sub-module) their stake will still be exercisable in the sub-module even though it was removed from the mothership. Users can only prevent this by manually calling a state-sync on the affected account(s).  Recommendation  The underlying issue is that users of the system can t be sure what the behavior of a function call will be, and this is because the behavior can change at any time.  We recommend giving the user advance notice of changes with a time lock. For example, make all system-parameter and upgrades require two steps with a mandatory time window between them. The first step merely broadcasts to users that a particular change is coming, and the second step commits that change after a suitable waiting period. This allows users that do not accept the change to withdraw immediately.  Furthermore, users should be guaranteed to be able to redeem their staked tokens. An entity - even though trusted - in the system should not be able to lock tokens indefinitely.  ",
        "labels": [
            "Consensys",
            "Medium"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/12/1inch-liquidity-protocol/"
    },
    {
        "title": "5.8 The owner can borrow token0/token1 in the rescueFunds ",
        "body": "  Description  If some random tokens/funds are accidentally transferred to the pool, the owner can call the rescueFunds function to withdraw any funds manually:  code/contracts/Mooniswap.sol:L331-L340  function rescueFunds(IERC20 token, uint256 amount) external nonReentrant onlyOwner {  uint256 balance0 = token0.uniBalanceOf(address(this));  uint256 balance1 = token1.uniBalanceOf(address(this));  token.uniTransfer(msg.sender, amount);  require(token0.uniBalanceOf(address(this)) >= balance0, \"Mooniswap: access denied\");  require(token1.uniBalanceOf(address(this)) >= balance1, \"Mooniswap: access denied\");  require(balanceOf(address(this)) >= _BASE_SUPPLY, \"Mooniswap: access denied\");  There s no restriction on which funds the owner can try to withdraw and which token to call. It s theoretically possible to transfer pool tokens and then return them to the contract (e.g. in the case of ERC-777). That action would be similar to a free flash loan.  Recommendation  Explicitly check that the token is not equal to any of the pool tokens.  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/12/1inch-liquidity-protocol/"
    },
    {
        "title": "7.1 Permissionless nature of proxy factory might cause confusion when parsing events  ",
        "body": "  Resolution  Update from the iExec team:  The iExec offchain platform does not listen to GenericFactory. This factory is intended to be public and available to anyone and is just a tool used for deployment.  Description  The permissionless nature of the factory (the GenericFactory contract) meant to deploy the ERC1538Proxy and the instances of its several delegates might create confusion when parsing events.  Since there is no access control being enforced through the use of modifiers on said factory, any account can use its deployment public methods to deploy a contract. This means that the supporting off-chain infrastructure making use of the fired events to look for deployed instances of either the iExec proxies or its delegates might get hindered by an ill-intended actor that abuses its functions.  Recommendation  Use a modifier enforcing some sort of access control (easily done through the inherited Ownable contract) to make sure only iExec can deploy from the factory and, therefore, increase the readability of logged events.  This becomes more important as time goes by and updates to the architecture are performed or any past analysis needs to be done on deployed modules.  ",
        "labels": [
            "Consensys",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/03/iexec-poco/"
    },
    {
        "title": "7.2 System deployer is fully trusted in this version of the PoCo system   ",
        "body": "  Resolution  Update from the iExec team:  After deployment, ownership is planned to be transferred to a multisig. This is just the first step towards a more decentralised governance on the protocol. We will consider adding an intermediary contract that enforces the lock period. This would however, prevent us from any kind of  emergency  update. The long term goal is it involve the community in the process, using a DAO or a similar solution.  Description  The introduction of ERC1538-compliant proxies to construct the PoCo system has many benefits. It heightens modularity, reduces the number of external calls between the system s components and allows for easy expansion of the system s capabilities without disruption of the service or need for off-chain infrastructure upgrade. However, the last enumerated benefit is in fact a double-edged sword.  Even though ERC1538 enables easy upgradeability it also completely strips the PoCo system of all of its prior trustless nature. In this version the iExec development team should be entirely trusted by every actor in the system not to change the deployed on-chain delegates for new ones.  Also the deployer, owner, has permission to change some of the system variables, such as m_callbackgas for Oracle callback gas limit. This indirectly can lock the system, for example it could result in IexecPocoDelegate.executeCallback() reverting which prevents the finalization of corresponding task.  Recommendation  The best, easiest solution for the trust issue would be to immediately revoke ownership of the proxy right after deployment. This way the modular deployment would still be possible but no power to change the deployed on-chain code would exist.  A second best solution would be to force a timespan period before any change to the proxy methods (and its delegates) is made effective. This way any actor in the system can still monitor for possible changes and  leave  the system before they are implemented.  In this last option the  lock  period should, obviously, be greater than the amount of time it takes to verify a Task of the bigger category but it is advisable to decide on it by anthropomorphic rules and use a longer,  human-friendly  time lock of, for example, 72 hours.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/03/iexec-poco/"
    },
    {
        "title": "7.3 importScore() in IexecMaintenanceDelegate can be used to wrongfully reset worker scores   ",
        "body": "  Resolution  Update from the iExec team:  In order to perform this attack, one would first have to gain reputation on the new version, and lose it. They would then be able to restore its score from the old version.  We feel the risk is acceptable for a few reasons:  It can only be done once per worker  Considering the score dynamics discussed in the  Trust in the PoCo  document, it is more interesting for a worker to import its reputation in the beginning rather then creating a new one, since bad contributions only remove part of the reputation  Only a handful of workers have reputation in the old system (180), and their score is low (average 7, max 22)  We might force the import all 180 workers with reputation >0. A script to identify the relevant addresses is already available.  Description  The import of worker scores from the previous PoCo system deployed on chain is made to be asynchronous. And, even though the pull pattern usually makes a system much more resilient, in this case, it opens up the possibility for an attack that undermines the trust-based game-theoretical balance the PoCo system relies on. As can be seen in the following function:  code/poco-dev/contracts/modules/delegates/IexecMaintenanceDelegate.sol:L51-L57  function importScore(address _worker)  external override  require(!m_v3_scoreImported[_worker], \"score-already-imported\");  m_workerScores[_worker] = m_workerScores[_worker].max(m_v3_iexecHub.viewScore(_worker));  m_v3_scoreImported[_worker] = true;  A motivated attacker could attack the system providing bogus results for computation tasks therefore reducing his own reputation (mirrored by the low worker score that would follow).  After the fact, the attacker could reset its score to the previous high value attained in the previously deployed PoCo system (v3) and undo all the wrongdoings he had done at no reputational cost.  Recommendation  Check that each worker interacting with the PoCo system has already imported his score. Otherwise import it synchronously with a call at the time of their first interaction.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/03/iexec-poco/"
    },
    {
        "title": "7.4 Outdated documentation   ",
        "body": "  Resolution   Update from the iExec team:   Description  There are many changes within the system from the initial version that are not reflected in the documentation.  It is necessary to have updated documentation for the time of the audit, as the specification dictates the correct behaviour of the code base.  Examples  Entities such as iExecClerk are the main point of entry in the documentation, however they have been replaced by proxy implementation in the code base (V5).  Recommendation  Up date documentation to reflect the recent changes and design in the code base.  ",
        "labels": [
            "Consensys",
            "Medium",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/03/iexec-poco/"
    },
    {
        "title": "7.5 Domain separator in iExecMaintenanceDelegate has a wrong version field   ",
        "body": "  Resolution   Issue was fixed in   iExecBlockchainComputing/PoCo-dev@ebee370  Description  The domain separator used to comply with the EIP712 standard in iExecMaintenanceDelegate has a wrong version field.  code/poco-dev/contracts/modules/delegates/IexecMaintenanceDelegate.sol:L77-L86  function _domain()  internal view returns (IexecLibOrders_v5.EIP712Domain memory)  return IexecLibOrders_v5.EIP712Domain({  name:              \"iExecODB\"  , version:           \"3.0-alpha\"  , chainId:           _chainId()  , verifyingContract: address(this)  });  In the above snippet we can see the code is still using the version field from an old version of the PoCo protocol, \"3.0-alpha\".  Recommendation  Change the version field to: \"5.0-alpha\"  ",
        "labels": [
            "Consensys",
            "Medium",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/03/iexec-poco/"
    },
    {
        "title": "7.6 Limit the length of task.contributors to prevent reaching gasBlockLimit   ",
        "body": "  Resolution  Update from the iExec team:  Any hardcoded lock would be a restriction in the future if thee block size increases. In addition to that, workers are strongly incentivised to not contribute if it would result in a deadlocked task. Schedulers are incentivised to not authorise too many workers to contribute (they also lose stake if a task get deadlocked). So the development team has assessed the risk as low.  In the unlikely event the described flaw still happens, the task will get in a deadlocked state, until at some point the block size limit is increased and a claim becomes possible. Because in a world where block size increases are possible, deadlocks are not eternal.  Description  It is recommended to limit the length of arrays that the contract iterates through to prevent system halts. task.contributors is used within iExec contract in many functions, and main functions such as claim(), reOpen(), and most importantly contribute() (through calling checkConsensus()) iterate through this list.  Given that contributions are not free and they could only block the task they are contributing to, this is a low impact issue.  Recommendation  The fix is trivial to implement and only requires to limit the number of items in task.contributors to the maximum imagined for the system (based on client communication this number could be 20, although further testing should be done to make sure with this number does not reach the blockGasLimit, possibly with future changes in the opcode pricing).  ",
        "labels": [
            "Consensys",
            "Minor",
            "Acknowledged"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/03/iexec-poco/"
    },
    {
        "title": "7.7 The updateContract() method in ERC1538UpdateDelegate is incorrectly implemented ",
        "body": "  Resolution   Issue was fixed in   iExecBlockchainComputing/iexec-solidity@e6be083  Description  The updateContract() method in ERC1538UpdateDelegate does not behave as intended for some specific streams of bytes (meant to be parsed as function signatures).  The mentioned function takes as input, among other things, a string (which is, canonically, a dynamically-sized bytes array) and tries to parse it as a conjunction of function signatures.  As is evident in:  code/iexec-solidity/contracts/ERC1538/ERC1538Update.sol:L39  if (char == 0x3B) // 0x3B = ';'  Inside the function, ; is being used as a  reserved  character, serving as a delimiter between each function signature.  However, if two semicolons are used in succession, the second one will not be checked and will be made part of the function signature being sent into the _setFunc() method.  Example of faulty input  someFunc;;someOtherFuncWithSemiColon;  Recommendation  Replace the line that increases the pos counter at the end of the function:  code/iexec-solidity/contracts/ERC1538/ERC1538Update.sol:L47  start = ++pos;  WIth this line of code:  start = pos + 1;  ",
        "labels": [
            "Consensys",
            "Minor"
        ],
        "html_url": "https://consensys.io/diligence/audits/2020/03/iexec-poco/"
    }
]